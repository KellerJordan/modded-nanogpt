import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:32:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     56908      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56909      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56910      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56911      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56912      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56913      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56914      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     56915      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     56909      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     56910      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     56911      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     56912      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     56913      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     56914      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     56915      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8310 train_time:0ms step_avg:0.18ms
step:1/1900 train_time:68ms step_avg:68.42ms
step:2/1900 train_time:91ms step_avg:45.45ms
step:3/1900 train_time:125ms step_avg:41.75ms
step:4/1900 train_time:159ms step_avg:39.75ms
step:5/1900 train_time:193ms step_avg:38.58ms
step:6/1900 train_time:251ms step_avg:41.78ms
step:7/1900 train_time:297ms step_avg:42.39ms
step:8/1900 train_time:331ms step_avg:41.33ms
step:9/1900 train_time:364ms step_avg:40.48ms
step:10/1900 train_time:398ms step_avg:39.81ms
step:11/1900 train_time:432ms step_avg:39.28ms
step:12/1900 train_time:466ms step_avg:38.82ms
step:13/1900 train_time:500ms step_avg:38.48ms
step:14/1900 train_time:534ms step_avg:38.15ms
step:15/1900 train_time:568ms step_avg:37.87ms
step:16/1900 train_time:602ms step_avg:37.62ms
step:17/1900 train_time:636ms step_avg:37.41ms
step:18/1900 train_time:670ms step_avg:37.21ms
step:19/1900 train_time:704ms step_avg:37.04ms
step:20/1900 train_time:738ms step_avg:36.88ms
step:21/1900 train_time:772ms step_avg:36.75ms
step:22/1900 train_time:806ms step_avg:36.62ms
step:23/1900 train_time:840ms step_avg:36.52ms
step:24/1900 train_time:874ms step_avg:36.41ms
step:25/1900 train_time:908ms step_avg:36.32ms
step:26/1900 train_time:942ms step_avg:36.23ms
step:27/1900 train_time:976ms step_avg:36.14ms
step:28/1900 train_time:1010ms step_avg:36.07ms
step:29/1900 train_time:1044ms step_avg:36.00ms
step:30/1900 train_time:1078ms step_avg:35.93ms
step:31/1900 train_time:1112ms step_avg:35.87ms
step:32/1900 train_time:1146ms step_avg:35.81ms
step:33/1900 train_time:1180ms step_avg:35.76ms
step:34/1900 train_time:1214ms step_avg:35.71ms
step:35/1900 train_time:1249ms step_avg:35.68ms
step:36/1900 train_time:1283ms step_avg:35.63ms
step:37/1900 train_time:1317ms step_avg:35.61ms
step:38/1900 train_time:1351ms step_avg:35.57ms
step:39/1900 train_time:1386ms step_avg:35.54ms
step:40/1900 train_time:1420ms step_avg:35.50ms
step:41/1900 train_time:1454ms step_avg:35.47ms
step:42/1900 train_time:1488ms step_avg:35.44ms
step:43/1900 train_time:1523ms step_avg:35.41ms
step:44/1900 train_time:1557ms step_avg:35.38ms
step:45/1900 train_time:1591ms step_avg:35.35ms
step:46/1900 train_time:1625ms step_avg:35.32ms
step:47/1900 train_time:1659ms step_avg:35.30ms
step:48/1900 train_time:1693ms step_avg:35.28ms
step:49/1900 train_time:1727ms step_avg:35.25ms
step:50/1900 train_time:1761ms step_avg:35.22ms
step:51/1900 train_time:1795ms step_avg:35.20ms
step:52/1900 train_time:1829ms step_avg:35.17ms
step:53/1900 train_time:1863ms step_avg:35.15ms
step:54/1900 train_time:1897ms step_avg:35.13ms
step:55/1900 train_time:1931ms step_avg:35.10ms
step:56/1900 train_time:1965ms step_avg:35.08ms
step:57/1900 train_time:1998ms step_avg:35.06ms
step:58/1900 train_time:2032ms step_avg:35.04ms
step:59/1900 train_time:2066ms step_avg:35.03ms
step:60/1900 train_time:2100ms step_avg:35.01ms
step:61/1900 train_time:2134ms step_avg:34.99ms
step:62/1900 train_time:2168ms step_avg:34.97ms
step:63/1900 train_time:2202ms step_avg:34.96ms
step:64/1900 train_time:2236ms step_avg:34.94ms
step:65/1900 train_time:2270ms step_avg:34.92ms
step:66/1900 train_time:2304ms step_avg:34.91ms
step:67/1900 train_time:2338ms step_avg:34.90ms
step:68/1900 train_time:2372ms step_avg:34.89ms
step:69/1900 train_time:2407ms step_avg:34.88ms
step:70/1900 train_time:2441ms step_avg:34.87ms
step:71/1900 train_time:2475ms step_avg:34.86ms
step:72/1900 train_time:2509ms step_avg:34.84ms
step:73/1900 train_time:2543ms step_avg:34.84ms
step:74/1900 train_time:2577ms step_avg:34.83ms
step:75/1900 train_time:2612ms step_avg:34.82ms
step:76/1900 train_time:2645ms step_avg:34.81ms
step:77/1900 train_time:2680ms step_avg:34.80ms
step:78/1900 train_time:2713ms step_avg:34.79ms
step:79/1900 train_time:2748ms step_avg:34.78ms
step:80/1900 train_time:2781ms step_avg:34.77ms
step:81/1900 train_time:2816ms step_avg:34.76ms
step:82/1900 train_time:2850ms step_avg:34.75ms
step:83/1900 train_time:2884ms step_avg:34.74ms
step:84/1900 train_time:2918ms step_avg:34.73ms
step:85/1900 train_time:2952ms step_avg:34.73ms
step:86/1900 train_time:2986ms step_avg:34.72ms
step:87/1900 train_time:3020ms step_avg:34.71ms
step:88/1900 train_time:3054ms step_avg:34.70ms
step:89/1900 train_time:3088ms step_avg:34.69ms
step:90/1900 train_time:3122ms step_avg:34.68ms
step:91/1900 train_time:3156ms step_avg:34.68ms
step:92/1900 train_time:3190ms step_avg:34.67ms
step:93/1900 train_time:3224ms step_avg:34.66ms
step:94/1900 train_time:3258ms step_avg:34.66ms
step:95/1900 train_time:3291ms step_avg:34.65ms
step:96/1900 train_time:3325ms step_avg:34.64ms
step:97/1900 train_time:3359ms step_avg:34.63ms
step:98/1900 train_time:3393ms step_avg:34.63ms
step:99/1900 train_time:3428ms step_avg:34.62ms
step:100/1900 train_time:3462ms step_avg:34.62ms
step:101/1900 train_time:3496ms step_avg:34.61ms
step:102/1900 train_time:3530ms step_avg:34.61ms
step:103/1900 train_time:3564ms step_avg:34.60ms
step:104/1900 train_time:3598ms step_avg:34.59ms
step:105/1900 train_time:3632ms step_avg:34.59ms
step:106/1900 train_time:3666ms step_avg:34.58ms
step:107/1900 train_time:3700ms step_avg:34.58ms
step:108/1900 train_time:3734ms step_avg:34.57ms
step:109/1900 train_time:3768ms step_avg:34.57ms
step:110/1900 train_time:3802ms step_avg:34.56ms
step:111/1900 train_time:3836ms step_avg:34.56ms
step:112/1900 train_time:3870ms step_avg:34.55ms
step:113/1900 train_time:3904ms step_avg:34.55ms
step:114/1900 train_time:3938ms step_avg:34.54ms
step:115/1900 train_time:3972ms step_avg:34.54ms
step:116/1900 train_time:4006ms step_avg:34.53ms
step:117/1900 train_time:4040ms step_avg:34.53ms
step:118/1900 train_time:4074ms step_avg:34.52ms
step:119/1900 train_time:4108ms step_avg:34.52ms
step:120/1900 train_time:4142ms step_avg:34.51ms
step:121/1900 train_time:4176ms step_avg:34.51ms
step:122/1900 train_time:4210ms step_avg:34.51ms
step:123/1900 train_time:4244ms step_avg:34.50ms
step:124/1900 train_time:4278ms step_avg:34.50ms
step:125/1900 train_time:4312ms step_avg:34.50ms
step:126/1900 train_time:4346ms step_avg:34.49ms
step:127/1900 train_time:4380ms step_avg:34.49ms
step:128/1900 train_time:4414ms step_avg:34.48ms
step:129/1900 train_time:4448ms step_avg:34.48ms
step:130/1900 train_time:4482ms step_avg:34.48ms
step:131/1900 train_time:4516ms step_avg:34.47ms
step:132/1900 train_time:4550ms step_avg:34.47ms
step:133/1900 train_time:4584ms step_avg:34.47ms
step:134/1900 train_time:4618ms step_avg:34.46ms
step:135/1900 train_time:4652ms step_avg:34.46ms
step:136/1900 train_time:4686ms step_avg:34.46ms
step:137/1900 train_time:4720ms step_avg:34.45ms
step:138/1900 train_time:4754ms step_avg:34.45ms
step:139/1900 train_time:4788ms step_avg:34.44ms
step:140/1900 train_time:4822ms step_avg:34.44ms
step:141/1900 train_time:4856ms step_avg:34.44ms
step:142/1900 train_time:4889ms step_avg:34.43ms
step:143/1900 train_time:4923ms step_avg:34.43ms
step:144/1900 train_time:4957ms step_avg:34.43ms
step:145/1900 train_time:4992ms step_avg:34.43ms
step:146/1900 train_time:5025ms step_avg:34.42ms
step:147/1900 train_time:5060ms step_avg:34.42ms
step:148/1900 train_time:5093ms step_avg:34.42ms
step:149/1900 train_time:5128ms step_avg:34.41ms
step:150/1900 train_time:5161ms step_avg:34.41ms
step:151/1900 train_time:5195ms step_avg:34.41ms
step:152/1900 train_time:5229ms step_avg:34.40ms
step:153/1900 train_time:5263ms step_avg:34.40ms
step:154/1900 train_time:5297ms step_avg:34.40ms
step:155/1900 train_time:5331ms step_avg:34.39ms
step:156/1900 train_time:5365ms step_avg:34.39ms
step:157/1900 train_time:5399ms step_avg:34.39ms
step:158/1900 train_time:5432ms step_avg:34.38ms
step:159/1900 train_time:5467ms step_avg:34.38ms
step:160/1900 train_time:5500ms step_avg:34.38ms
step:161/1900 train_time:5535ms step_avg:34.38ms
step:162/1900 train_time:5568ms step_avg:34.37ms
step:163/1900 train_time:5602ms step_avg:34.37ms
step:164/1900 train_time:5636ms step_avg:34.37ms
step:165/1900 train_time:5670ms step_avg:34.36ms
step:166/1900 train_time:5704ms step_avg:34.36ms
step:167/1900 train_time:5738ms step_avg:34.36ms
step:168/1900 train_time:5772ms step_avg:34.36ms
step:169/1900 train_time:5806ms step_avg:34.36ms
step:170/1900 train_time:5840ms step_avg:34.35ms
step:171/1900 train_time:5874ms step_avg:34.35ms
step:172/1900 train_time:5908ms step_avg:34.35ms
step:173/1900 train_time:5942ms step_avg:34.35ms
step:174/1900 train_time:5976ms step_avg:34.34ms
step:175/1900 train_time:6010ms step_avg:34.34ms
step:176/1900 train_time:6043ms step_avg:34.34ms
step:177/1900 train_time:6077ms step_avg:34.33ms
step:178/1900 train_time:6111ms step_avg:34.33ms
step:179/1900 train_time:6145ms step_avg:34.33ms
step:180/1900 train_time:6179ms step_avg:34.33ms
step:181/1900 train_time:6213ms step_avg:34.33ms
step:182/1900 train_time:6247ms step_avg:34.32ms
step:183/1900 train_time:6281ms step_avg:34.32ms
step:184/1900 train_time:6315ms step_avg:34.32ms
step:185/1900 train_time:6349ms step_avg:34.32ms
step:186/1900 train_time:6383ms step_avg:34.32ms
step:187/1900 train_time:6418ms step_avg:34.32ms
step:188/1900 train_time:6452ms step_avg:34.32ms
step:189/1900 train_time:6486ms step_avg:34.32ms
step:190/1900 train_time:6520ms step_avg:34.31ms
step:191/1900 train_time:6554ms step_avg:34.31ms
step:192/1900 train_time:6587ms step_avg:34.31ms
step:193/1900 train_time:6621ms step_avg:34.31ms
step:194/1900 train_time:6655ms step_avg:34.30ms
step:195/1900 train_time:6689ms step_avg:34.30ms
step:196/1900 train_time:6723ms step_avg:34.30ms
step:197/1900 train_time:6757ms step_avg:34.30ms
step:198/1900 train_time:6791ms step_avg:34.30ms
step:199/1900 train_time:6825ms step_avg:34.30ms
step:200/1900 train_time:6859ms step_avg:34.30ms
step:201/1900 train_time:6893ms step_avg:34.29ms
step:202/1900 train_time:6927ms step_avg:34.29ms
step:203/1900 train_time:6961ms step_avg:34.29ms
step:204/1900 train_time:6995ms step_avg:34.29ms
step:205/1900 train_time:7029ms step_avg:34.29ms
step:206/1900 train_time:7063ms step_avg:34.29ms
step:207/1900 train_time:7097ms step_avg:34.28ms
step:208/1900 train_time:7130ms step_avg:34.28ms
step:209/1900 train_time:7164ms step_avg:34.28ms
step:210/1900 train_time:7198ms step_avg:34.28ms
step:211/1900 train_time:7232ms step_avg:34.27ms
step:212/1900 train_time:7266ms step_avg:34.27ms
step:213/1900 train_time:7300ms step_avg:34.27ms
step:214/1900 train_time:7333ms step_avg:34.27ms
step:215/1900 train_time:7367ms step_avg:34.27ms
step:216/1900 train_time:7401ms step_avg:34.27ms
step:217/1900 train_time:7435ms step_avg:34.26ms
step:218/1900 train_time:7469ms step_avg:34.26ms
step:219/1900 train_time:7503ms step_avg:34.26ms
step:220/1900 train_time:7537ms step_avg:34.26ms
step:221/1900 train_time:7571ms step_avg:34.26ms
step:222/1900 train_time:7605ms step_avg:34.26ms
step:223/1900 train_time:7639ms step_avg:34.26ms
step:224/1900 train_time:7673ms step_avg:34.25ms
step:225/1900 train_time:7707ms step_avg:34.25ms
step:226/1900 train_time:7741ms step_avg:34.25ms
step:227/1900 train_time:7775ms step_avg:34.25ms
step:228/1900 train_time:7809ms step_avg:34.25ms
step:229/1900 train_time:7843ms step_avg:34.25ms
step:230/1900 train_time:7877ms step_avg:34.25ms
step:231/1900 train_time:7911ms step_avg:34.24ms
step:232/1900 train_time:7944ms step_avg:34.24ms
step:233/1900 train_time:7978ms step_avg:34.24ms
step:234/1900 train_time:8012ms step_avg:34.24ms
step:235/1900 train_time:8046ms step_avg:34.24ms
step:236/1900 train_time:8080ms step_avg:34.24ms
step:237/1900 train_time:8114ms step_avg:34.24ms
step:238/1900 train_time:8148ms step_avg:34.24ms
step:239/1900 train_time:8182ms step_avg:34.24ms
step:240/1900 train_time:8216ms step_avg:34.23ms
step:241/1900 train_time:8250ms step_avg:34.23ms
step:242/1900 train_time:8284ms step_avg:34.23ms
step:243/1900 train_time:8318ms step_avg:34.23ms
step:244/1900 train_time:8352ms step_avg:34.23ms
step:245/1900 train_time:8386ms step_avg:34.23ms
step:246/1900 train_time:8420ms step_avg:34.23ms
step:247/1900 train_time:8454ms step_avg:34.23ms
step:248/1900 train_time:8488ms step_avg:34.23ms
step:249/1900 train_time:8522ms step_avg:34.23ms
step:250/1900 train_time:8556ms step_avg:34.22ms
step:250/1900 val_loss:4.5981 train_time:8593ms step_avg:34.37ms
step:251/1900 train_time:8611ms step_avg:34.31ms
step:252/1900 train_time:8630ms step_avg:34.25ms
step:253/1900 train_time:8661ms step_avg:34.23ms
step:254/1900 train_time:8695ms step_avg:34.23ms
step:255/1900 train_time:8729ms step_avg:34.23ms
step:256/1900 train_time:8764ms step_avg:34.23ms
step:257/1900 train_time:8798ms step_avg:34.23ms
step:258/1900 train_time:8832ms step_avg:34.23ms
step:259/1900 train_time:8866ms step_avg:34.23ms
step:260/1900 train_time:8900ms step_avg:34.23ms
step:261/1900 train_time:8934ms step_avg:34.23ms
step:262/1900 train_time:8968ms step_avg:34.23ms
step:263/1900 train_time:9001ms step_avg:34.23ms
step:264/1900 train_time:9035ms step_avg:34.22ms
step:265/1900 train_time:9069ms step_avg:34.22ms
step:266/1900 train_time:9103ms step_avg:34.22ms
step:267/1900 train_time:9137ms step_avg:34.22ms
step:268/1900 train_time:9171ms step_avg:34.22ms
step:269/1900 train_time:9205ms step_avg:34.22ms
step:270/1900 train_time:9238ms step_avg:34.22ms
step:271/1900 train_time:9272ms step_avg:34.22ms
step:272/1900 train_time:9306ms step_avg:34.21ms
step:273/1900 train_time:9340ms step_avg:34.21ms
step:274/1900 train_time:9374ms step_avg:34.21ms
step:275/1900 train_time:9408ms step_avg:34.21ms
step:276/1900 train_time:9442ms step_avg:34.21ms
step:277/1900 train_time:9476ms step_avg:34.21ms
step:278/1900 train_time:9510ms step_avg:34.21ms
step:279/1900 train_time:9544ms step_avg:34.21ms
step:280/1900 train_time:9577ms step_avg:34.21ms
step:281/1900 train_time:9611ms step_avg:34.20ms
step:282/1900 train_time:9645ms step_avg:34.20ms
step:283/1900 train_time:9680ms step_avg:34.20ms
step:284/1900 train_time:9714ms step_avg:34.20ms
step:285/1900 train_time:9748ms step_avg:34.20ms
step:286/1900 train_time:9782ms step_avg:34.20ms
step:287/1900 train_time:9816ms step_avg:34.20ms
step:288/1900 train_time:9850ms step_avg:34.20ms
step:289/1900 train_time:9884ms step_avg:34.20ms
step:290/1900 train_time:9918ms step_avg:34.20ms
step:291/1900 train_time:9952ms step_avg:34.20ms
step:292/1900 train_time:9985ms step_avg:34.20ms
step:293/1900 train_time:10020ms step_avg:34.20ms
step:294/1900 train_time:10054ms step_avg:34.20ms
step:295/1900 train_time:10088ms step_avg:34.20ms
step:296/1900 train_time:10122ms step_avg:34.20ms
step:297/1900 train_time:10156ms step_avg:34.19ms
step:298/1900 train_time:10190ms step_avg:34.19ms
step:299/1900 train_time:10223ms step_avg:34.19ms
step:300/1900 train_time:10257ms step_avg:34.19ms
step:301/1900 train_time:10291ms step_avg:34.19ms
step:302/1900 train_time:10325ms step_avg:34.19ms
step:303/1900 train_time:10359ms step_avg:34.19ms
step:304/1900 train_time:10392ms step_avg:34.19ms
step:305/1900 train_time:10426ms step_avg:34.18ms
step:306/1900 train_time:10460ms step_avg:34.18ms
step:307/1900 train_time:10494ms step_avg:34.18ms
step:308/1900 train_time:10528ms step_avg:34.18ms
step:309/1900 train_time:10562ms step_avg:34.18ms
step:310/1900 train_time:10596ms step_avg:34.18ms
step:311/1900 train_time:10630ms step_avg:34.18ms
step:312/1900 train_time:10664ms step_avg:34.18ms
step:313/1900 train_time:10698ms step_avg:34.18ms
step:314/1900 train_time:10732ms step_avg:34.18ms
step:315/1900 train_time:10766ms step_avg:34.18ms
step:316/1900 train_time:10800ms step_avg:34.18ms
step:317/1900 train_time:10833ms step_avg:34.17ms
step:318/1900 train_time:10867ms step_avg:34.17ms
step:319/1900 train_time:10901ms step_avg:34.17ms
step:320/1900 train_time:10935ms step_avg:34.17ms
step:321/1900 train_time:10969ms step_avg:34.17ms
step:322/1900 train_time:11003ms step_avg:34.17ms
step:323/1900 train_time:11037ms step_avg:34.17ms
step:324/1900 train_time:11071ms step_avg:34.17ms
step:325/1900 train_time:11105ms step_avg:34.17ms
step:326/1900 train_time:11139ms step_avg:34.17ms
step:327/1900 train_time:11173ms step_avg:34.17ms
step:328/1900 train_time:11207ms step_avg:34.17ms
step:329/1900 train_time:11241ms step_avg:34.17ms
step:330/1900 train_time:11275ms step_avg:34.17ms
step:331/1900 train_time:11309ms step_avg:34.17ms
step:332/1900 train_time:11343ms step_avg:34.16ms
step:333/1900 train_time:11377ms step_avg:34.16ms
step:334/1900 train_time:11411ms step_avg:34.16ms
step:335/1900 train_time:11444ms step_avg:34.16ms
step:336/1900 train_time:11478ms step_avg:34.16ms
step:337/1900 train_time:11513ms step_avg:34.16ms
step:338/1900 train_time:11546ms step_avg:34.16ms
step:339/1900 train_time:11580ms step_avg:34.16ms
step:340/1900 train_time:11614ms step_avg:34.16ms
step:341/1900 train_time:11648ms step_avg:34.16ms
step:342/1900 train_time:11682ms step_avg:34.16ms
step:343/1900 train_time:11716ms step_avg:34.16ms
step:344/1900 train_time:11750ms step_avg:34.16ms
step:345/1900 train_time:11784ms step_avg:34.16ms
step:346/1900 train_time:11818ms step_avg:34.16ms
step:347/1900 train_time:11852ms step_avg:34.16ms
step:348/1900 train_time:11886ms step_avg:34.15ms
step:349/1900 train_time:11920ms step_avg:34.15ms
step:350/1900 train_time:11954ms step_avg:34.15ms
step:351/1900 train_time:11987ms step_avg:34.15ms
step:352/1900 train_time:12021ms step_avg:34.15ms
step:353/1900 train_time:12055ms step_avg:34.15ms
step:354/1900 train_time:12089ms step_avg:34.15ms
step:355/1900 train_time:12123ms step_avg:34.15ms
step:356/1900 train_time:12157ms step_avg:34.15ms
step:357/1900 train_time:12191ms step_avg:34.15ms
step:358/1900 train_time:12225ms step_avg:34.15ms
step:359/1900 train_time:12259ms step_avg:34.15ms
step:360/1900 train_time:12292ms step_avg:34.15ms
step:361/1900 train_time:12326ms step_avg:34.14ms
step:362/1900 train_time:12360ms step_avg:34.14ms
step:363/1900 train_time:12394ms step_avg:34.14ms
step:364/1900 train_time:12428ms step_avg:34.14ms
step:365/1900 train_time:12462ms step_avg:34.14ms
step:366/1900 train_time:12496ms step_avg:34.14ms
step:367/1900 train_time:12530ms step_avg:34.14ms
step:368/1900 train_time:12564ms step_avg:34.14ms
step:369/1900 train_time:12598ms step_avg:34.14ms
step:370/1900 train_time:12631ms step_avg:34.14ms
step:371/1900 train_time:12665ms step_avg:34.14ms
step:372/1900 train_time:12699ms step_avg:34.14ms
step:373/1900 train_time:12733ms step_avg:34.14ms
step:374/1900 train_time:12767ms step_avg:34.14ms
step:375/1900 train_time:12801ms step_avg:34.14ms
step:376/1900 train_time:12835ms step_avg:34.13ms
step:377/1900 train_time:12868ms step_avg:34.13ms
step:378/1900 train_time:12902ms step_avg:34.13ms
step:379/1900 train_time:12936ms step_avg:34.13ms
step:380/1900 train_time:12970ms step_avg:34.13ms
step:381/1900 train_time:13004ms step_avg:34.13ms
step:382/1900 train_time:13038ms step_avg:34.13ms
step:383/1900 train_time:13071ms step_avg:34.13ms
step:384/1900 train_time:13105ms step_avg:34.13ms
step:385/1900 train_time:13139ms step_avg:34.13ms
step:386/1900 train_time:13173ms step_avg:34.13ms
step:387/1900 train_time:13207ms step_avg:34.13ms
step:388/1900 train_time:13241ms step_avg:34.13ms
step:389/1900 train_time:13275ms step_avg:34.13ms
step:390/1900 train_time:13309ms step_avg:34.13ms
step:391/1900 train_time:13343ms step_avg:34.13ms
step:392/1900 train_time:13377ms step_avg:34.12ms
step:393/1900 train_time:13411ms step_avg:34.13ms
step:394/1900 train_time:13445ms step_avg:34.12ms
step:395/1900 train_time:13479ms step_avg:34.12ms
step:396/1900 train_time:13513ms step_avg:34.12ms
step:397/1900 train_time:13547ms step_avg:34.12ms
step:398/1900 train_time:13580ms step_avg:34.12ms
step:399/1900 train_time:13614ms step_avg:34.12ms
step:400/1900 train_time:13648ms step_avg:34.12ms
step:401/1900 train_time:13682ms step_avg:34.12ms
step:402/1900 train_time:13716ms step_avg:34.12ms
step:403/1900 train_time:13750ms step_avg:34.12ms
step:404/1900 train_time:13784ms step_avg:34.12ms
step:405/1900 train_time:13818ms step_avg:34.12ms
step:406/1900 train_time:13852ms step_avg:34.12ms
step:407/1900 train_time:13886ms step_avg:34.12ms
step:408/1900 train_time:13919ms step_avg:34.12ms
step:409/1900 train_time:13953ms step_avg:34.12ms
step:410/1900 train_time:13987ms step_avg:34.12ms
step:411/1900 train_time:14021ms step_avg:34.11ms
step:412/1900 train_time:14055ms step_avg:34.11ms
step:413/1900 train_time:14089ms step_avg:34.11ms
step:414/1900 train_time:14123ms step_avg:34.11ms
step:415/1900 train_time:14157ms step_avg:34.11ms
step:416/1900 train_time:14191ms step_avg:34.11ms
step:417/1900 train_time:14224ms step_avg:34.11ms
step:418/1900 train_time:14258ms step_avg:34.11ms
step:419/1900 train_time:14292ms step_avg:34.11ms
step:420/1900 train_time:14326ms step_avg:34.11ms
step:421/1900 train_time:14360ms step_avg:34.11ms
step:422/1900 train_time:14394ms step_avg:34.11ms
step:423/1900 train_time:14428ms step_avg:34.11ms
step:424/1900 train_time:14462ms step_avg:34.11ms
step:425/1900 train_time:14496ms step_avg:34.11ms
step:426/1900 train_time:14529ms step_avg:34.11ms
step:427/1900 train_time:14563ms step_avg:34.11ms
step:428/1900 train_time:14597ms step_avg:34.11ms
step:429/1900 train_time:14631ms step_avg:34.10ms
step:430/1900 train_time:14665ms step_avg:34.10ms
step:431/1900 train_time:14699ms step_avg:34.10ms
step:432/1900 train_time:14733ms step_avg:34.10ms
step:433/1900 train_time:14767ms step_avg:34.10ms
step:434/1900 train_time:14800ms step_avg:34.10ms
step:435/1900 train_time:14834ms step_avg:34.10ms
step:436/1900 train_time:14868ms step_avg:34.10ms
step:437/1900 train_time:14902ms step_avg:34.10ms
step:438/1900 train_time:14936ms step_avg:34.10ms
step:439/1900 train_time:14970ms step_avg:34.10ms
step:440/1900 train_time:15004ms step_avg:34.10ms
step:441/1900 train_time:15038ms step_avg:34.10ms
step:442/1900 train_time:15072ms step_avg:34.10ms
step:443/1900 train_time:15105ms step_avg:34.10ms
step:444/1900 train_time:15139ms step_avg:34.10ms
step:445/1900 train_time:15174ms step_avg:34.10ms
step:446/1900 train_time:15207ms step_avg:34.10ms
step:447/1900 train_time:15241ms step_avg:34.10ms
step:448/1900 train_time:15275ms step_avg:34.10ms
step:449/1900 train_time:15309ms step_avg:34.10ms
step:450/1900 train_time:15343ms step_avg:34.10ms
step:451/1900 train_time:15377ms step_avg:34.10ms
step:452/1900 train_time:15411ms step_avg:34.10ms
step:453/1900 train_time:15445ms step_avg:34.09ms
step:454/1900 train_time:15479ms step_avg:34.09ms
step:455/1900 train_time:15513ms step_avg:34.09ms
step:456/1900 train_time:15546ms step_avg:34.09ms
step:457/1900 train_time:15580ms step_avg:34.09ms
step:458/1900 train_time:15614ms step_avg:34.09ms
step:459/1900 train_time:15648ms step_avg:34.09ms
step:460/1900 train_time:15682ms step_avg:34.09ms
step:461/1900 train_time:15716ms step_avg:34.09ms
step:462/1900 train_time:15750ms step_avg:34.09ms
step:463/1900 train_time:15784ms step_avg:34.09ms
step:464/1900 train_time:15818ms step_avg:34.09ms
step:465/1900 train_time:15852ms step_avg:34.09ms
step:466/1900 train_time:15886ms step_avg:34.09ms
step:467/1900 train_time:15920ms step_avg:34.09ms
step:468/1900 train_time:15954ms step_avg:34.09ms
step:469/1900 train_time:15988ms step_avg:34.09ms
step:470/1900 train_time:16022ms step_avg:34.09ms
step:471/1900 train_time:16056ms step_avg:34.09ms
step:472/1900 train_time:16089ms step_avg:34.09ms
step:473/1900 train_time:16124ms step_avg:34.09ms
step:474/1900 train_time:16157ms step_avg:34.09ms
step:475/1900 train_time:16191ms step_avg:34.09ms
step:476/1900 train_time:16225ms step_avg:34.09ms
step:477/1900 train_time:16259ms step_avg:34.09ms
step:478/1900 train_time:16293ms step_avg:34.09ms
step:479/1900 train_time:16328ms step_avg:34.09ms
step:480/1900 train_time:16361ms step_avg:34.09ms
step:481/1900 train_time:16395ms step_avg:34.09ms
step:482/1900 train_time:16429ms step_avg:34.09ms
step:483/1900 train_time:16463ms step_avg:34.09ms
step:484/1900 train_time:16497ms step_avg:34.08ms
step:485/1900 train_time:16531ms step_avg:34.08ms
step:486/1900 train_time:16565ms step_avg:34.08ms
step:487/1900 train_time:16599ms step_avg:34.08ms
step:488/1900 train_time:16633ms step_avg:34.08ms
step:489/1900 train_time:16666ms step_avg:34.08ms
step:490/1900 train_time:16700ms step_avg:34.08ms
step:491/1900 train_time:16734ms step_avg:34.08ms
step:492/1900 train_time:16768ms step_avg:34.08ms
step:493/1900 train_time:16802ms step_avg:34.08ms
step:494/1900 train_time:16835ms step_avg:34.08ms
step:495/1900 train_time:16869ms step_avg:34.08ms
step:496/1900 train_time:16903ms step_avg:34.08ms
step:497/1900 train_time:16937ms step_avg:34.08ms
step:498/1900 train_time:16971ms step_avg:34.08ms
step:499/1900 train_time:17005ms step_avg:34.08ms
step:500/1900 train_time:17039ms step_avg:34.08ms
step:500/1900 val_loss:4.2870 train_time:17076ms step_avg:34.15ms
step:501/1900 train_time:17095ms step_avg:34.12ms
step:502/1900 train_time:17114ms step_avg:34.09ms
step:503/1900 train_time:17144ms step_avg:34.08ms
step:504/1900 train_time:17178ms step_avg:34.08ms
step:505/1900 train_time:17213ms step_avg:34.09ms
step:506/1900 train_time:17247ms step_avg:34.08ms
step:507/1900 train_time:17282ms step_avg:34.09ms
step:508/1900 train_time:17315ms step_avg:34.09ms
step:509/1900 train_time:17350ms step_avg:34.09ms
step:510/1900 train_time:17383ms step_avg:34.09ms
step:511/1900 train_time:17418ms step_avg:34.09ms
step:512/1900 train_time:17451ms step_avg:34.08ms
step:513/1900 train_time:17485ms step_avg:34.08ms
step:514/1900 train_time:17519ms step_avg:34.08ms
step:515/1900 train_time:17553ms step_avg:34.08ms
step:516/1900 train_time:17586ms step_avg:34.08ms
step:517/1900 train_time:17620ms step_avg:34.08ms
step:518/1900 train_time:17654ms step_avg:34.08ms
step:519/1900 train_time:17688ms step_avg:34.08ms
step:520/1900 train_time:17721ms step_avg:34.08ms
step:521/1900 train_time:17755ms step_avg:34.08ms
step:522/1900 train_time:17789ms step_avg:34.08ms
step:523/1900 train_time:17823ms step_avg:34.08ms
step:524/1900 train_time:17857ms step_avg:34.08ms
step:525/1900 train_time:17891ms step_avg:34.08ms
step:526/1900 train_time:17924ms step_avg:34.08ms
step:527/1900 train_time:17958ms step_avg:34.08ms
step:528/1900 train_time:17992ms step_avg:34.08ms
step:529/1900 train_time:18026ms step_avg:34.08ms
step:530/1900 train_time:18060ms step_avg:34.08ms
step:531/1900 train_time:18094ms step_avg:34.08ms
step:532/1900 train_time:18128ms step_avg:34.08ms
step:533/1900 train_time:18163ms step_avg:34.08ms
step:534/1900 train_time:18197ms step_avg:34.08ms
step:535/1900 train_time:18231ms step_avg:34.08ms
step:536/1900 train_time:18265ms step_avg:34.08ms
step:537/1900 train_time:18299ms step_avg:34.08ms
step:538/1900 train_time:18333ms step_avg:34.08ms
step:539/1900 train_time:18367ms step_avg:34.08ms
step:540/1900 train_time:18401ms step_avg:34.08ms
step:541/1900 train_time:18435ms step_avg:34.08ms
step:542/1900 train_time:18469ms step_avg:34.08ms
step:543/1900 train_time:18503ms step_avg:34.08ms
step:544/1900 train_time:18537ms step_avg:34.08ms
step:545/1900 train_time:18571ms step_avg:34.07ms
step:546/1900 train_time:18605ms step_avg:34.07ms
step:547/1900 train_time:18638ms step_avg:34.07ms
step:548/1900 train_time:18672ms step_avg:34.07ms
step:549/1900 train_time:18706ms step_avg:34.07ms
step:550/1900 train_time:18740ms step_avg:34.07ms
step:551/1900 train_time:18773ms step_avg:34.07ms
step:552/1900 train_time:18807ms step_avg:34.07ms
step:553/1900 train_time:18841ms step_avg:34.07ms
step:554/1900 train_time:18875ms step_avg:34.07ms
step:555/1900 train_time:18909ms step_avg:34.07ms
step:556/1900 train_time:18943ms step_avg:34.07ms
step:557/1900 train_time:18976ms step_avg:34.07ms
step:558/1900 train_time:19011ms step_avg:34.07ms
step:559/1900 train_time:19044ms step_avg:34.07ms
step:560/1900 train_time:19078ms step_avg:34.07ms
step:561/1900 train_time:19112ms step_avg:34.07ms
step:562/1900 train_time:19146ms step_avg:34.07ms
step:563/1900 train_time:19180ms step_avg:34.07ms
step:564/1900 train_time:19214ms step_avg:34.07ms
step:565/1900 train_time:19248ms step_avg:34.07ms
step:566/1900 train_time:19282ms step_avg:34.07ms
step:567/1900 train_time:19316ms step_avg:34.07ms
step:568/1900 train_time:19350ms step_avg:34.07ms
step:569/1900 train_time:19384ms step_avg:34.07ms
step:570/1900 train_time:19418ms step_avg:34.07ms
step:571/1900 train_time:19452ms step_avg:34.07ms
step:572/1900 train_time:19486ms step_avg:34.07ms
step:573/1900 train_time:19520ms step_avg:34.07ms
step:574/1900 train_time:19554ms step_avg:34.07ms
step:575/1900 train_time:19588ms step_avg:34.07ms
step:576/1900 train_time:19622ms step_avg:34.07ms
step:577/1900 train_time:19656ms step_avg:34.07ms
step:578/1900 train_time:19690ms step_avg:34.07ms
step:579/1900 train_time:19724ms step_avg:34.07ms
step:580/1900 train_time:19758ms step_avg:34.06ms
step:581/1900 train_time:19792ms step_avg:34.07ms
step:582/1900 train_time:19826ms step_avg:34.06ms
step:583/1900 train_time:19860ms step_avg:34.06ms
step:584/1900 train_time:19893ms step_avg:34.06ms
step:585/1900 train_time:19927ms step_avg:34.06ms
step:586/1900 train_time:19961ms step_avg:34.06ms
step:587/1900 train_time:19995ms step_avg:34.06ms
step:588/1900 train_time:20029ms step_avg:34.06ms
step:589/1900 train_time:20063ms step_avg:34.06ms
step:590/1900 train_time:20097ms step_avg:34.06ms
step:591/1900 train_time:20131ms step_avg:34.06ms
step:592/1900 train_time:20165ms step_avg:34.06ms
step:593/1900 train_time:20199ms step_avg:34.06ms
step:594/1900 train_time:20232ms step_avg:34.06ms
step:595/1900 train_time:20267ms step_avg:34.06ms
step:596/1900 train_time:20300ms step_avg:34.06ms
step:597/1900 train_time:20335ms step_avg:34.06ms
step:598/1900 train_time:20369ms step_avg:34.06ms
step:599/1900 train_time:20403ms step_avg:34.06ms
step:600/1900 train_time:20437ms step_avg:34.06ms
step:601/1900 train_time:20470ms step_avg:34.06ms
step:602/1900 train_time:20505ms step_avg:34.06ms
step:603/1900 train_time:20539ms step_avg:34.06ms
step:604/1900 train_time:20572ms step_avg:34.06ms
step:605/1900 train_time:20606ms step_avg:34.06ms
step:606/1900 train_time:20640ms step_avg:34.06ms
step:607/1900 train_time:20674ms step_avg:34.06ms
step:608/1900 train_time:20708ms step_avg:34.06ms
step:609/1900 train_time:20742ms step_avg:34.06ms
step:610/1900 train_time:20776ms step_avg:34.06ms
step:611/1900 train_time:20810ms step_avg:34.06ms
step:612/1900 train_time:20844ms step_avg:34.06ms
step:613/1900 train_time:20878ms step_avg:34.06ms
step:614/1900 train_time:20912ms step_avg:34.06ms
step:615/1900 train_time:20946ms step_avg:34.06ms
step:616/1900 train_time:20979ms step_avg:34.06ms
step:617/1900 train_time:21013ms step_avg:34.06ms
step:618/1900 train_time:21047ms step_avg:34.06ms
step:619/1900 train_time:21081ms step_avg:34.06ms
step:620/1900 train_time:21115ms step_avg:34.06ms
step:621/1900 train_time:21150ms step_avg:34.06ms
step:622/1900 train_time:21210ms step_avg:34.10ms
step:623/1900 train_time:21272ms step_avg:34.14ms
step:624/1900 train_time:21333ms step_avg:34.19ms
step:625/1900 train_time:21395ms step_avg:34.23ms
step:626/1900 train_time:21456ms step_avg:34.27ms
step:627/1900 train_time:21517ms step_avg:34.32ms
step:628/1900 train_time:21578ms step_avg:34.36ms
step:629/1900 train_time:21640ms step_avg:34.40ms
step:630/1900 train_time:21701ms step_avg:34.45ms
step:631/1900 train_time:21763ms step_avg:34.49ms
step:632/1900 train_time:21825ms step_avg:34.53ms
step:633/1900 train_time:21887ms step_avg:34.58ms
step:634/1900 train_time:21948ms step_avg:34.62ms
step:635/1900 train_time:22010ms step_avg:34.66ms
step:636/1900 train_time:22071ms step_avg:34.70ms
step:637/1900 train_time:22132ms step_avg:34.74ms
step:638/1900 train_time:22193ms step_avg:34.79ms
step:639/1900 train_time:22254ms step_avg:34.83ms
step:640/1900 train_time:22315ms step_avg:34.87ms
step:641/1900 train_time:22377ms step_avg:34.91ms
step:642/1900 train_time:22438ms step_avg:34.95ms
step:643/1900 train_time:22500ms step_avg:34.99ms
step:644/1900 train_time:22561ms step_avg:35.03ms
step:645/1900 train_time:22623ms step_avg:35.07ms
step:646/1900 train_time:22684ms step_avg:35.11ms
step:647/1900 train_time:22746ms step_avg:35.16ms
step:648/1900 train_time:22807ms step_avg:35.20ms
step:649/1900 train_time:22869ms step_avg:35.24ms
step:650/1900 train_time:22929ms step_avg:35.28ms
step:651/1900 train_time:22992ms step_avg:35.32ms
step:652/1900 train_time:23052ms step_avg:35.36ms
step:653/1900 train_time:23114ms step_avg:35.40ms
step:654/1900 train_time:23175ms step_avg:35.44ms
step:655/1900 train_time:23236ms step_avg:35.48ms
step:656/1900 train_time:23297ms step_avg:35.51ms
step:657/1900 train_time:23359ms step_avg:35.55ms
step:658/1900 train_time:23420ms step_avg:35.59ms
step:659/1900 train_time:23482ms step_avg:35.63ms
step:660/1900 train_time:23543ms step_avg:35.67ms
step:661/1900 train_time:23605ms step_avg:35.71ms
step:662/1900 train_time:23666ms step_avg:35.75ms
step:663/1900 train_time:23728ms step_avg:35.79ms
step:664/1900 train_time:23789ms step_avg:35.83ms
step:665/1900 train_time:23850ms step_avg:35.87ms
step:666/1900 train_time:23911ms step_avg:35.90ms
step:667/1900 train_time:23973ms step_avg:35.94ms
step:668/1900 train_time:24034ms step_avg:35.98ms
step:669/1900 train_time:24096ms step_avg:36.02ms
step:670/1900 train_time:24157ms step_avg:36.06ms
step:671/1900 train_time:24219ms step_avg:36.09ms
step:672/1900 train_time:24280ms step_avg:36.13ms
step:673/1900 train_time:24341ms step_avg:36.17ms
step:674/1900 train_time:24403ms step_avg:36.21ms
step:675/1900 train_time:24464ms step_avg:36.24ms
step:676/1900 train_time:24525ms step_avg:36.28ms
step:677/1900 train_time:24587ms step_avg:36.32ms
step:678/1900 train_time:24648ms step_avg:36.35ms
step:679/1900 train_time:24710ms step_avg:36.39ms
step:680/1900 train_time:24771ms step_avg:36.43ms
step:681/1900 train_time:24833ms step_avg:36.47ms
step:682/1900 train_time:24894ms step_avg:36.50ms
step:683/1900 train_time:24956ms step_avg:36.54ms
step:684/1900 train_time:25017ms step_avg:36.57ms
step:685/1900 train_time:25079ms step_avg:36.61ms
step:686/1900 train_time:25140ms step_avg:36.65ms
step:687/1900 train_time:25202ms step_avg:36.68ms
step:688/1900 train_time:25263ms step_avg:36.72ms
step:689/1900 train_time:25325ms step_avg:36.76ms
step:690/1900 train_time:25386ms step_avg:36.79ms
step:691/1900 train_time:25447ms step_avg:36.83ms
step:692/1900 train_time:25509ms step_avg:36.86ms
step:693/1900 train_time:25570ms step_avg:36.90ms
step:694/1900 train_time:25630ms step_avg:36.93ms
step:695/1900 train_time:25692ms step_avg:36.97ms
step:696/1900 train_time:25753ms step_avg:37.00ms
step:697/1900 train_time:25815ms step_avg:37.04ms
step:698/1900 train_time:25876ms step_avg:37.07ms
step:699/1900 train_time:25938ms step_avg:37.11ms
step:700/1900 train_time:25998ms step_avg:37.14ms
step:701/1900 train_time:26060ms step_avg:37.18ms
step:702/1900 train_time:26121ms step_avg:37.21ms
step:703/1900 train_time:26183ms step_avg:37.24ms
step:704/1900 train_time:26244ms step_avg:37.28ms
step:705/1900 train_time:26306ms step_avg:37.31ms
step:706/1900 train_time:26367ms step_avg:37.35ms
step:707/1900 train_time:26428ms step_avg:37.38ms
step:708/1900 train_time:26490ms step_avg:37.41ms
step:709/1900 train_time:26551ms step_avg:37.45ms
step:710/1900 train_time:26612ms step_avg:37.48ms
step:711/1900 train_time:26674ms step_avg:37.52ms
step:712/1900 train_time:26735ms step_avg:37.55ms
step:713/1900 train_time:26797ms step_avg:37.58ms
step:714/1900 train_time:26858ms step_avg:37.62ms
step:715/1900 train_time:26919ms step_avg:37.65ms
step:716/1900 train_time:26980ms step_avg:37.68ms
step:717/1900 train_time:27042ms step_avg:37.72ms
step:718/1900 train_time:27103ms step_avg:37.75ms
step:719/1900 train_time:27166ms step_avg:37.78ms
step:720/1900 train_time:27227ms step_avg:37.82ms
step:721/1900 train_time:27289ms step_avg:37.85ms
step:722/1900 train_time:27350ms step_avg:37.88ms
step:723/1900 train_time:27412ms step_avg:37.91ms
step:724/1900 train_time:27472ms step_avg:37.95ms
step:725/1900 train_time:27534ms step_avg:37.98ms
step:726/1900 train_time:27595ms step_avg:38.01ms
step:727/1900 train_time:27657ms step_avg:38.04ms
step:728/1900 train_time:27717ms step_avg:38.07ms
step:729/1900 train_time:27779ms step_avg:38.11ms
step:730/1900 train_time:27840ms step_avg:38.14ms
step:731/1900 train_time:27902ms step_avg:38.17ms
step:732/1900 train_time:27962ms step_avg:38.20ms
step:733/1900 train_time:28024ms step_avg:38.23ms
step:734/1900 train_time:28085ms step_avg:38.26ms
step:735/1900 train_time:28148ms step_avg:38.30ms
step:736/1900 train_time:28209ms step_avg:38.33ms
step:737/1900 train_time:28271ms step_avg:38.36ms
step:738/1900 train_time:28331ms step_avg:38.39ms
step:739/1900 train_time:28393ms step_avg:38.42ms
step:740/1900 train_time:28454ms step_avg:38.45ms
step:741/1900 train_time:28516ms step_avg:38.48ms
step:742/1900 train_time:28577ms step_avg:38.51ms
step:743/1900 train_time:28639ms step_avg:38.54ms
step:744/1900 train_time:28699ms step_avg:38.57ms
step:745/1900 train_time:28761ms step_avg:38.61ms
step:746/1900 train_time:28822ms step_avg:38.64ms
step:747/1900 train_time:28884ms step_avg:38.67ms
step:748/1900 train_time:28945ms step_avg:38.70ms
step:749/1900 train_time:29006ms step_avg:38.73ms
step:750/1900 train_time:29067ms step_avg:38.76ms
step:750/1900 val_loss:4.0186 train_time:29132ms step_avg:38.84ms
step:751/1900 train_time:29153ms step_avg:38.82ms
step:752/1900 train_time:29193ms step_avg:38.82ms
step:753/1900 train_time:29256ms step_avg:38.85ms
step:754/1900 train_time:29318ms step_avg:38.88ms
step:755/1900 train_time:29380ms step_avg:38.91ms
step:756/1900 train_time:29441ms step_avg:38.94ms
step:757/1900 train_time:29502ms step_avg:38.97ms
step:758/1900 train_time:29563ms step_avg:39.00ms
step:759/1900 train_time:29624ms step_avg:39.03ms
step:760/1900 train_time:29685ms step_avg:39.06ms
step:761/1900 train_time:29746ms step_avg:39.09ms
step:762/1900 train_time:29807ms step_avg:39.12ms
step:763/1900 train_time:29868ms step_avg:39.15ms
step:764/1900 train_time:29929ms step_avg:39.17ms
step:765/1900 train_time:29990ms step_avg:39.20ms
step:766/1900 train_time:30051ms step_avg:39.23ms
step:767/1900 train_time:30114ms step_avg:39.26ms
step:768/1900 train_time:30175ms step_avg:39.29ms
step:769/1900 train_time:30238ms step_avg:39.32ms
step:770/1900 train_time:30299ms step_avg:39.35ms
step:771/1900 train_time:30362ms step_avg:39.38ms
step:772/1900 train_time:30423ms step_avg:39.41ms
step:773/1900 train_time:30485ms step_avg:39.44ms
step:774/1900 train_time:30546ms step_avg:39.46ms
step:775/1900 train_time:30607ms step_avg:39.49ms
step:776/1900 train_time:30668ms step_avg:39.52ms
step:777/1900 train_time:30729ms step_avg:39.55ms
step:778/1900 train_time:30790ms step_avg:39.58ms
step:779/1900 train_time:30851ms step_avg:39.60ms
step:780/1900 train_time:30912ms step_avg:39.63ms
step:781/1900 train_time:30973ms step_avg:39.66ms
step:782/1900 train_time:31034ms step_avg:39.69ms
step:783/1900 train_time:31096ms step_avg:39.71ms
step:784/1900 train_time:31157ms step_avg:39.74ms
step:785/1900 train_time:31219ms step_avg:39.77ms
step:786/1900 train_time:31280ms step_avg:39.80ms
step:787/1900 train_time:31343ms step_avg:39.83ms
step:788/1900 train_time:31404ms step_avg:39.85ms
step:789/1900 train_time:31465ms step_avg:39.88ms
step:790/1900 train_time:31526ms step_avg:39.91ms
step:791/1900 train_time:31588ms step_avg:39.93ms
step:792/1900 train_time:31649ms step_avg:39.96ms
step:793/1900 train_time:31710ms step_avg:39.99ms
step:794/1900 train_time:31771ms step_avg:40.01ms
step:795/1900 train_time:31832ms step_avg:40.04ms
step:796/1900 train_time:31893ms step_avg:40.07ms
step:797/1900 train_time:31954ms step_avg:40.09ms
step:798/1900 train_time:32015ms step_avg:40.12ms
step:799/1900 train_time:32077ms step_avg:40.15ms
step:800/1900 train_time:32138ms step_avg:40.17ms
step:801/1900 train_time:32200ms step_avg:40.20ms
step:802/1900 train_time:32261ms step_avg:40.23ms
step:803/1900 train_time:32323ms step_avg:40.25ms
step:804/1900 train_time:32384ms step_avg:40.28ms
step:805/1900 train_time:32446ms step_avg:40.31ms
step:806/1900 train_time:32507ms step_avg:40.33ms
step:807/1900 train_time:32570ms step_avg:40.36ms
step:808/1900 train_time:32630ms step_avg:40.38ms
step:809/1900 train_time:32692ms step_avg:40.41ms
step:810/1900 train_time:32752ms step_avg:40.43ms
step:811/1900 train_time:32814ms step_avg:40.46ms
step:812/1900 train_time:32874ms step_avg:40.49ms
step:813/1900 train_time:32936ms step_avg:40.51ms
step:814/1900 train_time:32997ms step_avg:40.54ms
step:815/1900 train_time:33059ms step_avg:40.56ms
step:816/1900 train_time:33120ms step_avg:40.59ms
step:817/1900 train_time:33182ms step_avg:40.61ms
step:818/1900 train_time:33242ms step_avg:40.64ms
step:819/1900 train_time:33304ms step_avg:40.66ms
step:820/1900 train_time:33365ms step_avg:40.69ms
step:821/1900 train_time:33427ms step_avg:40.72ms
step:822/1900 train_time:33488ms step_avg:40.74ms
step:823/1900 train_time:33550ms step_avg:40.77ms
step:824/1900 train_time:33611ms step_avg:40.79ms
step:825/1900 train_time:33673ms step_avg:40.82ms
step:826/1900 train_time:33734ms step_avg:40.84ms
step:827/1900 train_time:33795ms step_avg:40.86ms
step:828/1900 train_time:33855ms step_avg:40.89ms
step:829/1900 train_time:33917ms step_avg:40.91ms
step:830/1900 train_time:33977ms step_avg:40.94ms
step:831/1900 train_time:34039ms step_avg:40.96ms
step:832/1900 train_time:34100ms step_avg:40.99ms
step:833/1900 train_time:34162ms step_avg:41.01ms
step:834/1900 train_time:34223ms step_avg:41.04ms
step:835/1900 train_time:34285ms step_avg:41.06ms
step:836/1900 train_time:34346ms step_avg:41.08ms
step:837/1900 train_time:34408ms step_avg:41.11ms
step:838/1900 train_time:34469ms step_avg:41.13ms
step:839/1900 train_time:34531ms step_avg:41.16ms
step:840/1900 train_time:34592ms step_avg:41.18ms
step:841/1900 train_time:34654ms step_avg:41.21ms
step:842/1900 train_time:34714ms step_avg:41.23ms
step:843/1900 train_time:34776ms step_avg:41.25ms
step:844/1900 train_time:34837ms step_avg:41.28ms
step:845/1900 train_time:34899ms step_avg:41.30ms
step:846/1900 train_time:34959ms step_avg:41.32ms
step:847/1900 train_time:35021ms step_avg:41.35ms
step:848/1900 train_time:35082ms step_avg:41.37ms
step:849/1900 train_time:35144ms step_avg:41.39ms
step:850/1900 train_time:35205ms step_avg:41.42ms
step:851/1900 train_time:35267ms step_avg:41.44ms
step:852/1900 train_time:35328ms step_avg:41.46ms
step:853/1900 train_time:35390ms step_avg:41.49ms
step:854/1900 train_time:35451ms step_avg:41.51ms
step:855/1900 train_time:35513ms step_avg:41.54ms
step:856/1900 train_time:35573ms step_avg:41.56ms
step:857/1900 train_time:35635ms step_avg:41.58ms
step:858/1900 train_time:35696ms step_avg:41.60ms
step:859/1900 train_time:35757ms step_avg:41.63ms
step:860/1900 train_time:35818ms step_avg:41.65ms
step:861/1900 train_time:35880ms step_avg:41.67ms
step:862/1900 train_time:35941ms step_avg:41.69ms
step:863/1900 train_time:36002ms step_avg:41.72ms
step:864/1900 train_time:36063ms step_avg:41.74ms
step:865/1900 train_time:36125ms step_avg:41.76ms
step:866/1900 train_time:36185ms step_avg:41.78ms
step:867/1900 train_time:36247ms step_avg:41.81ms
step:868/1900 train_time:36308ms step_avg:41.83ms
step:869/1900 train_time:36370ms step_avg:41.85ms
step:870/1900 train_time:36431ms step_avg:41.87ms
step:871/1900 train_time:36492ms step_avg:41.90ms
step:872/1900 train_time:36553ms step_avg:41.92ms
step:873/1900 train_time:36616ms step_avg:41.94ms
step:874/1900 train_time:36677ms step_avg:41.96ms
step:875/1900 train_time:36739ms step_avg:41.99ms
step:876/1900 train_time:36799ms step_avg:42.01ms
step:877/1900 train_time:36861ms step_avg:42.03ms
step:878/1900 train_time:36922ms step_avg:42.05ms
step:879/1900 train_time:36983ms step_avg:42.07ms
step:880/1900 train_time:37044ms step_avg:42.10ms
step:881/1900 train_time:37106ms step_avg:42.12ms
step:882/1900 train_time:37167ms step_avg:42.14ms
step:883/1900 train_time:37229ms step_avg:42.16ms
step:884/1900 train_time:37289ms step_avg:42.18ms
step:885/1900 train_time:37351ms step_avg:42.20ms
step:886/1900 train_time:37412ms step_avg:42.23ms
step:887/1900 train_time:37474ms step_avg:42.25ms
step:888/1900 train_time:37535ms step_avg:42.27ms
step:889/1900 train_time:37596ms step_avg:42.29ms
step:890/1900 train_time:37657ms step_avg:42.31ms
step:891/1900 train_time:37719ms step_avg:42.33ms
step:892/1900 train_time:37780ms step_avg:42.35ms
step:893/1900 train_time:37842ms step_avg:42.38ms
step:894/1900 train_time:37903ms step_avg:42.40ms
step:895/1900 train_time:37965ms step_avg:42.42ms
step:896/1900 train_time:38026ms step_avg:42.44ms
step:897/1900 train_time:38088ms step_avg:42.46ms
step:898/1900 train_time:38149ms step_avg:42.48ms
step:899/1900 train_time:38211ms step_avg:42.50ms
step:900/1900 train_time:38272ms step_avg:42.52ms
step:901/1900 train_time:38334ms step_avg:42.55ms
step:902/1900 train_time:38395ms step_avg:42.57ms
step:903/1900 train_time:38456ms step_avg:42.59ms
step:904/1900 train_time:38517ms step_avg:42.61ms
step:905/1900 train_time:38579ms step_avg:42.63ms
step:906/1900 train_time:38640ms step_avg:42.65ms
step:907/1900 train_time:38702ms step_avg:42.67ms
step:908/1900 train_time:38763ms step_avg:42.69ms
step:909/1900 train_time:38825ms step_avg:42.71ms
step:910/1900 train_time:38886ms step_avg:42.73ms
step:911/1900 train_time:38947ms step_avg:42.75ms
step:912/1900 train_time:39008ms step_avg:42.77ms
step:913/1900 train_time:39070ms step_avg:42.79ms
step:914/1900 train_time:39131ms step_avg:42.81ms
step:915/1900 train_time:39193ms step_avg:42.83ms
step:916/1900 train_time:39254ms step_avg:42.85ms
step:917/1900 train_time:39315ms step_avg:42.87ms
step:918/1900 train_time:39377ms step_avg:42.89ms
step:919/1900 train_time:39438ms step_avg:42.91ms
step:920/1900 train_time:39499ms step_avg:42.93ms
step:921/1900 train_time:39561ms step_avg:42.95ms
step:922/1900 train_time:39622ms step_avg:42.97ms
step:923/1900 train_time:39684ms step_avg:42.99ms
step:924/1900 train_time:39745ms step_avg:43.01ms
step:925/1900 train_time:39807ms step_avg:43.03ms
step:926/1900 train_time:39867ms step_avg:43.05ms
step:927/1900 train_time:39929ms step_avg:43.07ms
step:928/1900 train_time:39990ms step_avg:43.09ms
step:929/1900 train_time:40052ms step_avg:43.11ms
step:930/1900 train_time:40112ms step_avg:43.13ms
step:931/1900 train_time:40174ms step_avg:43.15ms
step:932/1900 train_time:40235ms step_avg:43.17ms
step:933/1900 train_time:40297ms step_avg:43.19ms
step:934/1900 train_time:40358ms step_avg:43.21ms
step:935/1900 train_time:40419ms step_avg:43.23ms
step:936/1900 train_time:40480ms step_avg:43.25ms
step:937/1900 train_time:40542ms step_avg:43.27ms
step:938/1900 train_time:40603ms step_avg:43.29ms
step:939/1900 train_time:40666ms step_avg:43.31ms
step:940/1900 train_time:40727ms step_avg:43.33ms
step:941/1900 train_time:40789ms step_avg:43.35ms
step:942/1900 train_time:40849ms step_avg:43.36ms
step:943/1900 train_time:40911ms step_avg:43.38ms
step:944/1900 train_time:40971ms step_avg:43.40ms
step:945/1900 train_time:41034ms step_avg:43.42ms
step:946/1900 train_time:41095ms step_avg:43.44ms
step:947/1900 train_time:41156ms step_avg:43.46ms
step:948/1900 train_time:41217ms step_avg:43.48ms
step:949/1900 train_time:41279ms step_avg:43.50ms
step:950/1900 train_time:41340ms step_avg:43.52ms
step:951/1900 train_time:41403ms step_avg:43.54ms
step:952/1900 train_time:41464ms step_avg:43.55ms
step:953/1900 train_time:41525ms step_avg:43.57ms
step:954/1900 train_time:41586ms step_avg:43.59ms
step:955/1900 train_time:41648ms step_avg:43.61ms
step:956/1900 train_time:41709ms step_avg:43.63ms
step:957/1900 train_time:41771ms step_avg:43.65ms
step:958/1900 train_time:41831ms step_avg:43.67ms
step:959/1900 train_time:41894ms step_avg:43.68ms
step:960/1900 train_time:41955ms step_avg:43.70ms
step:961/1900 train_time:42016ms step_avg:43.72ms
step:962/1900 train_time:42078ms step_avg:43.74ms
step:963/1900 train_time:42139ms step_avg:43.76ms
step:964/1900 train_time:42200ms step_avg:43.78ms
step:965/1900 train_time:42262ms step_avg:43.79ms
step:966/1900 train_time:42323ms step_avg:43.81ms
step:967/1900 train_time:42385ms step_avg:43.83ms
step:968/1900 train_time:42446ms step_avg:43.85ms
step:969/1900 train_time:42508ms step_avg:43.87ms
step:970/1900 train_time:42569ms step_avg:43.89ms
step:971/1900 train_time:42631ms step_avg:43.90ms
step:972/1900 train_time:42692ms step_avg:43.92ms
step:973/1900 train_time:42753ms step_avg:43.94ms
step:974/1900 train_time:42814ms step_avg:43.96ms
step:975/1900 train_time:42876ms step_avg:43.98ms
step:976/1900 train_time:42937ms step_avg:43.99ms
step:977/1900 train_time:42999ms step_avg:44.01ms
step:978/1900 train_time:43059ms step_avg:44.03ms
step:979/1900 train_time:43122ms step_avg:44.05ms
step:980/1900 train_time:43183ms step_avg:44.06ms
step:981/1900 train_time:43244ms step_avg:44.08ms
step:982/1900 train_time:43305ms step_avg:44.10ms
step:983/1900 train_time:43366ms step_avg:44.12ms
step:984/1900 train_time:43427ms step_avg:44.13ms
step:985/1900 train_time:43489ms step_avg:44.15ms
step:986/1900 train_time:43549ms step_avg:44.17ms
step:987/1900 train_time:43611ms step_avg:44.19ms
step:988/1900 train_time:43671ms step_avg:44.20ms
step:989/1900 train_time:43733ms step_avg:44.22ms
step:990/1900 train_time:43794ms step_avg:44.24ms
step:991/1900 train_time:43856ms step_avg:44.25ms
step:992/1900 train_time:43917ms step_avg:44.27ms
step:993/1900 train_time:43979ms step_avg:44.29ms
step:994/1900 train_time:44040ms step_avg:44.31ms
step:995/1900 train_time:44101ms step_avg:44.32ms
step:996/1900 train_time:44162ms step_avg:44.34ms
step:997/1900 train_time:44223ms step_avg:44.36ms
step:998/1900 train_time:44284ms step_avg:44.37ms
step:999/1900 train_time:44345ms step_avg:44.39ms
step:1000/1900 train_time:44406ms step_avg:44.41ms
step:1000/1900 val_loss:3.7711 train_time:44471ms step_avg:44.47ms
step:1001/1900 train_time:44491ms step_avg:44.45ms
step:1002/1900 train_time:44533ms step_avg:44.44ms
step:1003/1900 train_time:44598ms step_avg:44.46ms
step:1004/1900 train_time:44661ms step_avg:44.48ms
step:1005/1900 train_time:44722ms step_avg:44.50ms
step:1006/1900 train_time:44783ms step_avg:44.52ms
step:1007/1900 train_time:44844ms step_avg:44.53ms
step:1008/1900 train_time:44904ms step_avg:44.55ms
step:1009/1900 train_time:44966ms step_avg:44.56ms
step:1010/1900 train_time:45026ms step_avg:44.58ms
step:1011/1900 train_time:45087ms step_avg:44.60ms
step:1012/1900 train_time:45148ms step_avg:44.61ms
step:1013/1900 train_time:45209ms step_avg:44.63ms
step:1014/1900 train_time:45269ms step_avg:44.64ms
step:1015/1900 train_time:45331ms step_avg:44.66ms
step:1016/1900 train_time:45393ms step_avg:44.68ms
step:1017/1900 train_time:45456ms step_avg:44.70ms
step:1018/1900 train_time:45518ms step_avg:44.71ms
step:1019/1900 train_time:45581ms step_avg:44.73ms
step:1020/1900 train_time:45643ms step_avg:44.75ms
step:1021/1900 train_time:45706ms step_avg:44.77ms
step:1022/1900 train_time:45767ms step_avg:44.78ms
step:1023/1900 train_time:45829ms step_avg:44.80ms
step:1024/1900 train_time:45889ms step_avg:44.81ms
step:1025/1900 train_time:45951ms step_avg:44.83ms
step:1026/1900 train_time:46012ms step_avg:44.85ms
step:1027/1900 train_time:46073ms step_avg:44.86ms
step:1028/1900 train_time:46134ms step_avg:44.88ms
step:1029/1900 train_time:46196ms step_avg:44.89ms
step:1030/1900 train_time:46257ms step_avg:44.91ms
step:1031/1900 train_time:46318ms step_avg:44.93ms
step:1032/1900 train_time:46378ms step_avg:44.94ms
step:1033/1900 train_time:46441ms step_avg:44.96ms
step:1034/1900 train_time:46502ms step_avg:44.97ms
step:1035/1900 train_time:46565ms step_avg:44.99ms
step:1036/1900 train_time:46627ms step_avg:45.01ms
step:1037/1900 train_time:46689ms step_avg:45.02ms
step:1038/1900 train_time:46750ms step_avg:45.04ms
step:1039/1900 train_time:46812ms step_avg:45.06ms
step:1040/1900 train_time:46873ms step_avg:45.07ms
step:1041/1900 train_time:46935ms step_avg:45.09ms
step:1042/1900 train_time:46996ms step_avg:45.10ms
step:1043/1900 train_time:47057ms step_avg:45.12ms
step:1044/1900 train_time:47118ms step_avg:45.13ms
step:1045/1900 train_time:47180ms step_avg:45.15ms
step:1046/1900 train_time:47240ms step_avg:45.16ms
step:1047/1900 train_time:47301ms step_avg:45.18ms
step:1048/1900 train_time:47362ms step_avg:45.19ms
step:1049/1900 train_time:47424ms step_avg:45.21ms
step:1050/1900 train_time:47485ms step_avg:45.22ms
step:1051/1900 train_time:47548ms step_avg:45.24ms
step:1052/1900 train_time:47609ms step_avg:45.26ms
step:1053/1900 train_time:47672ms step_avg:45.27ms
step:1054/1900 train_time:47733ms step_avg:45.29ms
step:1055/1900 train_time:47794ms step_avg:45.30ms
step:1056/1900 train_time:47855ms step_avg:45.32ms
step:1057/1900 train_time:47917ms step_avg:45.33ms
step:1058/1900 train_time:47978ms step_avg:45.35ms
step:1059/1900 train_time:48039ms step_avg:45.36ms
step:1060/1900 train_time:48100ms step_avg:45.38ms
step:1061/1900 train_time:48161ms step_avg:45.39ms
step:1062/1900 train_time:48222ms step_avg:45.41ms
step:1063/1900 train_time:48284ms step_avg:45.42ms
step:1064/1900 train_time:48344ms step_avg:45.44ms
step:1065/1900 train_time:48407ms step_avg:45.45ms
step:1066/1900 train_time:48468ms step_avg:45.47ms
step:1067/1900 train_time:48530ms step_avg:45.48ms
step:1068/1900 train_time:48591ms step_avg:45.50ms
step:1069/1900 train_time:48653ms step_avg:45.51ms
step:1070/1900 train_time:48714ms step_avg:45.53ms
step:1071/1900 train_time:48776ms step_avg:45.54ms
step:1072/1900 train_time:48837ms step_avg:45.56ms
step:1073/1900 train_time:48899ms step_avg:45.57ms
step:1074/1900 train_time:48960ms step_avg:45.59ms
step:1075/1900 train_time:49021ms step_avg:45.60ms
step:1076/1900 train_time:49082ms step_avg:45.62ms
step:1077/1900 train_time:49144ms step_avg:45.63ms
step:1078/1900 train_time:49205ms step_avg:45.64ms
step:1079/1900 train_time:49266ms step_avg:45.66ms
step:1080/1900 train_time:49327ms step_avg:45.67ms
step:1081/1900 train_time:49388ms step_avg:45.69ms
step:1082/1900 train_time:49449ms step_avg:45.70ms
step:1083/1900 train_time:49511ms step_avg:45.72ms
step:1084/1900 train_time:49571ms step_avg:45.73ms
step:1085/1900 train_time:49633ms step_avg:45.74ms
step:1086/1900 train_time:49694ms step_avg:45.76ms
step:1087/1900 train_time:49757ms step_avg:45.77ms
step:1088/1900 train_time:49818ms step_avg:45.79ms
step:1089/1900 train_time:49880ms step_avg:45.80ms
step:1090/1900 train_time:49941ms step_avg:45.82ms
step:1091/1900 train_time:50003ms step_avg:45.83ms
step:1092/1900 train_time:50063ms step_avg:45.85ms
step:1093/1900 train_time:50125ms step_avg:45.86ms
step:1094/1900 train_time:50186ms step_avg:45.87ms
step:1095/1900 train_time:50248ms step_avg:45.89ms
step:1096/1900 train_time:50309ms step_avg:45.90ms
step:1097/1900 train_time:50370ms step_avg:45.92ms
step:1098/1900 train_time:50431ms step_avg:45.93ms
step:1099/1900 train_time:50493ms step_avg:45.94ms
step:1100/1900 train_time:50554ms step_avg:45.96ms
step:1101/1900 train_time:50616ms step_avg:45.97ms
step:1102/1900 train_time:50677ms step_avg:45.99ms
step:1103/1900 train_time:50739ms step_avg:46.00ms
step:1104/1900 train_time:50800ms step_avg:46.01ms
step:1105/1900 train_time:50862ms step_avg:46.03ms
step:1106/1900 train_time:50923ms step_avg:46.04ms
step:1107/1900 train_time:50985ms step_avg:46.06ms
step:1108/1900 train_time:51046ms step_avg:46.07ms
step:1109/1900 train_time:51108ms step_avg:46.08ms
step:1110/1900 train_time:51169ms step_avg:46.10ms
step:1111/1900 train_time:51230ms step_avg:46.11ms
step:1112/1900 train_time:51292ms step_avg:46.13ms
step:1113/1900 train_time:51353ms step_avg:46.14ms
step:1114/1900 train_time:51414ms step_avg:46.15ms
step:1115/1900 train_time:51475ms step_avg:46.17ms
step:1116/1900 train_time:51536ms step_avg:46.18ms
step:1117/1900 train_time:51598ms step_avg:46.19ms
step:1118/1900 train_time:51659ms step_avg:46.21ms
step:1119/1900 train_time:51721ms step_avg:46.22ms
step:1120/1900 train_time:51782ms step_avg:46.23ms
step:1121/1900 train_time:51844ms step_avg:46.25ms
step:1122/1900 train_time:51905ms step_avg:46.26ms
step:1123/1900 train_time:51967ms step_avg:46.27ms
step:1124/1900 train_time:52028ms step_avg:46.29ms
step:1125/1900 train_time:52090ms step_avg:46.30ms
step:1126/1900 train_time:52150ms step_avg:46.31ms
step:1127/1900 train_time:52212ms step_avg:46.33ms
step:1128/1900 train_time:52273ms step_avg:46.34ms
step:1129/1900 train_time:52335ms step_avg:46.35ms
step:1130/1900 train_time:52395ms step_avg:46.37ms
step:1131/1900 train_time:52457ms step_avg:46.38ms
step:1132/1900 train_time:52518ms step_avg:46.39ms
step:1133/1900 train_time:52580ms step_avg:46.41ms
step:1134/1900 train_time:52641ms step_avg:46.42ms
step:1135/1900 train_time:52702ms step_avg:46.43ms
step:1136/1900 train_time:52764ms step_avg:46.45ms
step:1137/1900 train_time:52826ms step_avg:46.46ms
step:1138/1900 train_time:52886ms step_avg:46.47ms
step:1139/1900 train_time:52948ms step_avg:46.49ms
step:1140/1900 train_time:53009ms step_avg:46.50ms
step:1141/1900 train_time:53070ms step_avg:46.51ms
step:1142/1900 train_time:53131ms step_avg:46.52ms
step:1143/1900 train_time:53193ms step_avg:46.54ms
step:1144/1900 train_time:53254ms step_avg:46.55ms
step:1145/1900 train_time:53315ms step_avg:46.56ms
step:1146/1900 train_time:53377ms step_avg:46.58ms
step:1147/1900 train_time:53439ms step_avg:46.59ms
step:1148/1900 train_time:53499ms step_avg:46.60ms
step:1149/1900 train_time:53561ms step_avg:46.62ms
step:1150/1900 train_time:53621ms step_avg:46.63ms
step:1151/1900 train_time:53684ms step_avg:46.64ms
step:1152/1900 train_time:53745ms step_avg:46.65ms
step:1153/1900 train_time:53807ms step_avg:46.67ms
step:1154/1900 train_time:53868ms step_avg:46.68ms
step:1155/1900 train_time:53930ms step_avg:46.69ms
step:1156/1900 train_time:53990ms step_avg:46.70ms
step:1157/1900 train_time:54052ms step_avg:46.72ms
step:1158/1900 train_time:54113ms step_avg:46.73ms
step:1159/1900 train_time:54174ms step_avg:46.74ms
step:1160/1900 train_time:54235ms step_avg:46.75ms
step:1161/1900 train_time:54297ms step_avg:46.77ms
step:1162/1900 train_time:54358ms step_avg:46.78ms
step:1163/1900 train_time:54420ms step_avg:46.79ms
step:1164/1900 train_time:54480ms step_avg:46.80ms
step:1165/1900 train_time:54542ms step_avg:46.82ms
step:1166/1900 train_time:54603ms step_avg:46.83ms
step:1167/1900 train_time:54665ms step_avg:46.84ms
step:1168/1900 train_time:54726ms step_avg:46.85ms
step:1169/1900 train_time:54788ms step_avg:46.87ms
step:1170/1900 train_time:54849ms step_avg:46.88ms
step:1171/1900 train_time:54911ms step_avg:46.89ms
step:1172/1900 train_time:54972ms step_avg:46.90ms
step:1173/1900 train_time:55034ms step_avg:46.92ms
step:1174/1900 train_time:55095ms step_avg:46.93ms
step:1175/1900 train_time:55157ms step_avg:46.94ms
step:1176/1900 train_time:55218ms step_avg:46.95ms
step:1177/1900 train_time:55280ms step_avg:46.97ms
step:1178/1900 train_time:55341ms step_avg:46.98ms
step:1179/1900 train_time:55403ms step_avg:46.99ms
step:1180/1900 train_time:55464ms step_avg:47.00ms
step:1181/1900 train_time:55525ms step_avg:47.02ms
step:1182/1900 train_time:55587ms step_avg:47.03ms
step:1183/1900 train_time:55649ms step_avg:47.04ms
step:1184/1900 train_time:55710ms step_avg:47.05ms
step:1185/1900 train_time:55771ms step_avg:47.06ms
step:1186/1900 train_time:55833ms step_avg:47.08ms
step:1187/1900 train_time:55894ms step_avg:47.09ms
step:1188/1900 train_time:55955ms step_avg:47.10ms
step:1189/1900 train_time:56016ms step_avg:47.11ms
step:1190/1900 train_time:56077ms step_avg:47.12ms
step:1191/1900 train_time:56139ms step_avg:47.14ms
step:1192/1900 train_time:56200ms step_avg:47.15ms
step:1193/1900 train_time:56261ms step_avg:47.16ms
step:1194/1900 train_time:56322ms step_avg:47.17ms
step:1195/1900 train_time:56384ms step_avg:47.18ms
step:1196/1900 train_time:56445ms step_avg:47.19ms
step:1197/1900 train_time:56507ms step_avg:47.21ms
step:1198/1900 train_time:56568ms step_avg:47.22ms
step:1199/1900 train_time:56630ms step_avg:47.23ms
step:1200/1900 train_time:56691ms step_avg:47.24ms
step:1201/1900 train_time:56753ms step_avg:47.26ms
step:1202/1900 train_time:56815ms step_avg:47.27ms
step:1203/1900 train_time:56877ms step_avg:47.28ms
step:1204/1900 train_time:56938ms step_avg:47.29ms
step:1205/1900 train_time:57000ms step_avg:47.30ms
step:1206/1900 train_time:57061ms step_avg:47.31ms
step:1207/1900 train_time:57123ms step_avg:47.33ms
step:1208/1900 train_time:57184ms step_avg:47.34ms
step:1209/1900 train_time:57245ms step_avg:47.35ms
step:1210/1900 train_time:57306ms step_avg:47.36ms
step:1211/1900 train_time:57368ms step_avg:47.37ms
step:1212/1900 train_time:57429ms step_avg:47.38ms
step:1213/1900 train_time:57491ms step_avg:47.40ms
step:1214/1900 train_time:57552ms step_avg:47.41ms
step:1215/1900 train_time:57614ms step_avg:47.42ms
step:1216/1900 train_time:57675ms step_avg:47.43ms
step:1217/1900 train_time:57736ms step_avg:47.44ms
step:1218/1900 train_time:57797ms step_avg:47.45ms
step:1219/1900 train_time:57859ms step_avg:47.46ms
step:1220/1900 train_time:57920ms step_avg:47.48ms
step:1221/1900 train_time:57982ms step_avg:47.49ms
step:1222/1900 train_time:58043ms step_avg:47.50ms
step:1223/1900 train_time:58106ms step_avg:47.51ms
step:1224/1900 train_time:58167ms step_avg:47.52ms
step:1225/1900 train_time:58229ms step_avg:47.53ms
step:1226/1900 train_time:58290ms step_avg:47.54ms
step:1227/1900 train_time:58351ms step_avg:47.56ms
step:1228/1900 train_time:58412ms step_avg:47.57ms
step:1229/1900 train_time:58474ms step_avg:47.58ms
step:1230/1900 train_time:58536ms step_avg:47.59ms
step:1231/1900 train_time:58597ms step_avg:47.60ms
step:1232/1900 train_time:58658ms step_avg:47.61ms
step:1233/1900 train_time:58720ms step_avg:47.62ms
step:1234/1900 train_time:58780ms step_avg:47.63ms
step:1235/1900 train_time:58842ms step_avg:47.65ms
step:1236/1900 train_time:58903ms step_avg:47.66ms
step:1237/1900 train_time:58964ms step_avg:47.67ms
step:1238/1900 train_time:59025ms step_avg:47.68ms
step:1239/1900 train_time:59087ms step_avg:47.69ms
step:1240/1900 train_time:59148ms step_avg:47.70ms
step:1241/1900 train_time:59210ms step_avg:47.71ms
step:1242/1900 train_time:59298ms step_avg:47.74ms
step:1243/1900 train_time:59386ms step_avg:47.78ms
step:1244/1900 train_time:59474ms step_avg:47.81ms
step:1245/1900 train_time:59563ms step_avg:47.84ms
step:1246/1900 train_time:59650ms step_avg:47.87ms
step:1247/1900 train_time:59739ms step_avg:47.91ms
step:1248/1900 train_time:59827ms step_avg:47.94ms
step:1249/1900 train_time:59915ms step_avg:47.97ms
step:1250/1900 train_time:60003ms step_avg:48.00ms
step:1250/1900 val_loss:3.5412 train_time:60094ms step_avg:48.08ms
step:1251/1900 train_time:60115ms step_avg:48.05ms
step:1252/1900 train_time:60182ms step_avg:48.07ms
step:1253/1900 train_time:60273ms step_avg:48.10ms
step:1254/1900 train_time:60361ms step_avg:48.14ms
step:1255/1900 train_time:60449ms step_avg:48.17ms
step:1256/1900 train_time:60536ms step_avg:48.20ms
step:1257/1900 train_time:60623ms step_avg:48.23ms
step:1258/1900 train_time:60710ms step_avg:48.26ms
step:1259/1900 train_time:60798ms step_avg:48.29ms
step:1260/1900 train_time:60886ms step_avg:48.32ms
step:1261/1900 train_time:60973ms step_avg:48.35ms
step:1262/1900 train_time:61063ms step_avg:48.39ms
step:1263/1900 train_time:61152ms step_avg:48.42ms
step:1264/1900 train_time:61241ms step_avg:48.45ms
step:1265/1900 train_time:61330ms step_avg:48.48ms
step:1266/1900 train_time:61418ms step_avg:48.51ms
step:1267/1900 train_time:61506ms step_avg:48.54ms
step:1268/1900 train_time:61592ms step_avg:48.57ms
step:1269/1900 train_time:61680ms step_avg:48.61ms
step:1270/1900 train_time:61767ms step_avg:48.64ms
step:1271/1900 train_time:61856ms step_avg:48.67ms
step:1272/1900 train_time:61943ms step_avg:48.70ms
step:1273/1900 train_time:62032ms step_avg:48.73ms
step:1274/1900 train_time:62121ms step_avg:48.76ms
step:1275/1900 train_time:62210ms step_avg:48.79ms
step:1276/1900 train_time:62299ms step_avg:48.82ms
step:1277/1900 train_time:62388ms step_avg:48.85ms
step:1278/1900 train_time:62475ms step_avg:48.88ms
step:1279/1900 train_time:62563ms step_avg:48.92ms
step:1280/1900 train_time:62650ms step_avg:48.95ms
step:1281/1900 train_time:62738ms step_avg:48.98ms
step:1282/1900 train_time:62825ms step_avg:49.01ms
step:1283/1900 train_time:62913ms step_avg:49.04ms
step:1284/1900 train_time:63001ms step_avg:49.07ms
step:1285/1900 train_time:63089ms step_avg:49.10ms
step:1286/1900 train_time:63177ms step_avg:49.13ms
step:1287/1900 train_time:63269ms step_avg:49.16ms
step:1288/1900 train_time:63357ms step_avg:49.19ms
step:1289/1900 train_time:63446ms step_avg:49.22ms
step:1290/1900 train_time:63533ms step_avg:49.25ms
step:1291/1900 train_time:63620ms step_avg:49.28ms
step:1292/1900 train_time:63707ms step_avg:49.31ms
step:1293/1900 train_time:63795ms step_avg:49.34ms
step:1294/1900 train_time:63881ms step_avg:49.37ms
step:1295/1900 train_time:63969ms step_avg:49.40ms
step:1296/1900 train_time:64057ms step_avg:49.43ms
step:1297/1900 train_time:64146ms step_avg:49.46ms
step:1298/1900 train_time:64236ms step_avg:49.49ms
step:1299/1900 train_time:64326ms step_avg:49.52ms
step:1300/1900 train_time:64413ms step_avg:49.55ms
step:1301/1900 train_time:64502ms step_avg:49.58ms
step:1302/1900 train_time:64589ms step_avg:49.61ms
step:1303/1900 train_time:64677ms step_avg:49.64ms
step:1304/1900 train_time:64765ms step_avg:49.67ms
step:1305/1900 train_time:64852ms step_avg:49.70ms
step:1306/1900 train_time:64939ms step_avg:49.72ms
step:1307/1900 train_time:65028ms step_avg:49.75ms
step:1308/1900 train_time:65115ms step_avg:49.78ms
step:1309/1900 train_time:65204ms step_avg:49.81ms
step:1310/1900 train_time:65292ms step_avg:49.84ms
step:1311/1900 train_time:65380ms step_avg:49.87ms
step:1312/1900 train_time:65468ms step_avg:49.90ms
step:1313/1900 train_time:65556ms step_avg:49.93ms
step:1314/1900 train_time:65644ms step_avg:49.96ms
step:1315/1900 train_time:65732ms step_avg:49.99ms
step:1316/1900 train_time:65820ms step_avg:50.02ms
step:1317/1900 train_time:65908ms step_avg:50.04ms
step:1318/1900 train_time:65996ms step_avg:50.07ms
step:1319/1900 train_time:66084ms step_avg:50.10ms
step:1320/1900 train_time:66171ms step_avg:50.13ms
step:1321/1900 train_time:66261ms step_avg:50.16ms
step:1322/1900 train_time:66348ms step_avg:50.19ms
step:1323/1900 train_time:66436ms step_avg:50.22ms
step:1324/1900 train_time:66525ms step_avg:50.25ms
step:1325/1900 train_time:66613ms step_avg:50.27ms
step:1326/1900 train_time:66700ms step_avg:50.30ms
step:1327/1900 train_time:66789ms step_avg:50.33ms
step:1328/1900 train_time:66876ms step_avg:50.36ms
step:1329/1900 train_time:66965ms step_avg:50.39ms
step:1330/1900 train_time:67053ms step_avg:50.42ms
step:1331/1900 train_time:67141ms step_avg:50.44ms
step:1332/1900 train_time:67229ms step_avg:50.47ms
step:1333/1900 train_time:67318ms step_avg:50.50ms
step:1334/1900 train_time:67406ms step_avg:50.53ms
step:1335/1900 train_time:67496ms step_avg:50.56ms
step:1336/1900 train_time:67582ms step_avg:50.59ms
step:1337/1900 train_time:67670ms step_avg:50.61ms
step:1338/1900 train_time:67757ms step_avg:50.64ms
step:1339/1900 train_time:67847ms step_avg:50.67ms
step:1340/1900 train_time:67934ms step_avg:50.70ms
step:1341/1900 train_time:68023ms step_avg:50.73ms
step:1342/1900 train_time:68110ms step_avg:50.75ms
step:1343/1900 train_time:68198ms step_avg:50.78ms
step:1344/1900 train_time:68285ms step_avg:50.81ms
step:1345/1900 train_time:68375ms step_avg:50.84ms
step:1346/1900 train_time:68462ms step_avg:50.86ms
step:1347/1900 train_time:68550ms step_avg:50.89ms
step:1348/1900 train_time:68638ms step_avg:50.92ms
step:1349/1900 train_time:68726ms step_avg:50.95ms
step:1350/1900 train_time:68814ms step_avg:50.97ms
step:1351/1900 train_time:68903ms step_avg:51.00ms
step:1352/1900 train_time:68991ms step_avg:51.03ms
step:1353/1900 train_time:69079ms step_avg:51.06ms
step:1354/1900 train_time:69167ms step_avg:51.08ms
step:1355/1900 train_time:69255ms step_avg:51.11ms
step:1356/1900 train_time:69344ms step_avg:51.14ms
step:1357/1900 train_time:69431ms step_avg:51.17ms
step:1358/1900 train_time:69519ms step_avg:51.19ms
step:1359/1900 train_time:69608ms step_avg:51.22ms
step:1360/1900 train_time:69696ms step_avg:51.25ms
step:1361/1900 train_time:69784ms step_avg:51.27ms
step:1362/1900 train_time:69871ms step_avg:51.30ms
step:1363/1900 train_time:69959ms step_avg:51.33ms
step:1364/1900 train_time:70047ms step_avg:51.35ms
step:1365/1900 train_time:70135ms step_avg:51.38ms
step:1366/1900 train_time:70223ms step_avg:51.41ms
step:1367/1900 train_time:70312ms step_avg:51.44ms
step:1368/1900 train_time:70399ms step_avg:51.46ms
step:1369/1900 train_time:70488ms step_avg:51.49ms
step:1370/1900 train_time:70576ms step_avg:51.52ms
step:1371/1900 train_time:70664ms step_avg:51.54ms
step:1372/1900 train_time:70752ms step_avg:51.57ms
step:1373/1900 train_time:70840ms step_avg:51.60ms
step:1374/1900 train_time:70928ms step_avg:51.62ms
step:1375/1900 train_time:71016ms step_avg:51.65ms
step:1376/1900 train_time:71103ms step_avg:51.67ms
step:1377/1900 train_time:71191ms step_avg:51.70ms
step:1378/1900 train_time:71279ms step_avg:51.73ms
step:1379/1900 train_time:71367ms step_avg:51.75ms
step:1380/1900 train_time:71455ms step_avg:51.78ms
step:1381/1900 train_time:71543ms step_avg:51.80ms
step:1382/1900 train_time:71630ms step_avg:51.83ms
step:1383/1900 train_time:71718ms step_avg:51.86ms
step:1384/1900 train_time:71806ms step_avg:51.88ms
step:1385/1900 train_time:71894ms step_avg:51.91ms
step:1386/1900 train_time:71981ms step_avg:51.93ms
step:1387/1900 train_time:72070ms step_avg:51.96ms
step:1388/1900 train_time:72157ms step_avg:51.99ms
step:1389/1900 train_time:72247ms step_avg:52.01ms
step:1390/1900 train_time:72334ms step_avg:52.04ms
step:1391/1900 train_time:72423ms step_avg:52.07ms
step:1392/1900 train_time:72510ms step_avg:52.09ms
step:1393/1900 train_time:72599ms step_avg:52.12ms
step:1394/1900 train_time:72686ms step_avg:52.14ms
step:1395/1900 train_time:72774ms step_avg:52.17ms
step:1396/1900 train_time:72863ms step_avg:52.19ms
step:1397/1900 train_time:72951ms step_avg:52.22ms
step:1398/1900 train_time:73038ms step_avg:52.24ms
step:1399/1900 train_time:73127ms step_avg:52.27ms
step:1400/1900 train_time:73215ms step_avg:52.30ms
step:1401/1900 train_time:73303ms step_avg:52.32ms
step:1402/1900 train_time:73391ms step_avg:52.35ms
step:1403/1900 train_time:73479ms step_avg:52.37ms
step:1404/1900 train_time:73566ms step_avg:52.40ms
step:1405/1900 train_time:73655ms step_avg:52.42ms
step:1406/1900 train_time:73742ms step_avg:52.45ms
step:1407/1900 train_time:73830ms step_avg:52.47ms
step:1408/1900 train_time:73918ms step_avg:52.50ms
step:1409/1900 train_time:74006ms step_avg:52.52ms
step:1410/1900 train_time:74094ms step_avg:52.55ms
step:1411/1900 train_time:74182ms step_avg:52.57ms
step:1412/1900 train_time:74270ms step_avg:52.60ms
step:1413/1900 train_time:74358ms step_avg:52.62ms
step:1414/1900 train_time:74446ms step_avg:52.65ms
step:1415/1900 train_time:74535ms step_avg:52.67ms
step:1416/1900 train_time:74622ms step_avg:52.70ms
step:1417/1900 train_time:74710ms step_avg:52.72ms
step:1418/1900 train_time:74798ms step_avg:52.75ms
step:1419/1900 train_time:74886ms step_avg:52.77ms
step:1420/1900 train_time:74974ms step_avg:52.80ms
step:1421/1900 train_time:75062ms step_avg:52.82ms
step:1422/1900 train_time:75149ms step_avg:52.85ms
step:1423/1900 train_time:75238ms step_avg:52.87ms
step:1424/1900 train_time:75326ms step_avg:52.90ms
step:1425/1900 train_time:75414ms step_avg:52.92ms
step:1426/1900 train_time:75502ms step_avg:52.95ms
step:1427/1900 train_time:75590ms step_avg:52.97ms
step:1428/1900 train_time:75678ms step_avg:53.00ms
step:1429/1900 train_time:75767ms step_avg:53.02ms
step:1430/1900 train_time:75855ms step_avg:53.05ms
step:1431/1900 train_time:75943ms step_avg:53.07ms
step:1432/1900 train_time:76030ms step_avg:53.09ms
step:1433/1900 train_time:76119ms step_avg:53.12ms
step:1434/1900 train_time:76206ms step_avg:53.14ms
step:1435/1900 train_time:76295ms step_avg:53.17ms
step:1436/1900 train_time:76382ms step_avg:53.19ms
step:1437/1900 train_time:76471ms step_avg:53.22ms
step:1438/1900 train_time:76558ms step_avg:53.24ms
step:1439/1900 train_time:76646ms step_avg:53.26ms
step:1440/1900 train_time:76734ms step_avg:53.29ms
step:1441/1900 train_time:76824ms step_avg:53.31ms
step:1442/1900 train_time:76911ms step_avg:53.34ms
step:1443/1900 train_time:77000ms step_avg:53.36ms
step:1444/1900 train_time:77087ms step_avg:53.38ms
step:1445/1900 train_time:77176ms step_avg:53.41ms
step:1446/1900 train_time:77263ms step_avg:53.43ms
step:1447/1900 train_time:77352ms step_avg:53.46ms
step:1448/1900 train_time:77439ms step_avg:53.48ms
step:1449/1900 train_time:77529ms step_avg:53.51ms
step:1450/1900 train_time:77617ms step_avg:53.53ms
step:1451/1900 train_time:77705ms step_avg:53.55ms
step:1452/1900 train_time:77792ms step_avg:53.58ms
step:1453/1900 train_time:77881ms step_avg:53.60ms
step:1454/1900 train_time:77968ms step_avg:53.62ms
step:1455/1900 train_time:78057ms step_avg:53.65ms
step:1456/1900 train_time:78145ms step_avg:53.67ms
step:1457/1900 train_time:78233ms step_avg:53.69ms
step:1458/1900 train_time:78321ms step_avg:53.72ms
step:1459/1900 train_time:78410ms step_avg:53.74ms
step:1460/1900 train_time:78498ms step_avg:53.77ms
step:1461/1900 train_time:78586ms step_avg:53.79ms
step:1462/1900 train_time:78673ms step_avg:53.81ms
step:1463/1900 train_time:78762ms step_avg:53.84ms
step:1464/1900 train_time:78850ms step_avg:53.86ms
step:1465/1900 train_time:78939ms step_avg:53.88ms
step:1466/1900 train_time:79026ms step_avg:53.91ms
step:1467/1900 train_time:79114ms step_avg:53.93ms
step:1468/1900 train_time:79201ms step_avg:53.95ms
step:1469/1900 train_time:79289ms step_avg:53.97ms
step:1470/1900 train_time:79377ms step_avg:54.00ms
step:1471/1900 train_time:79467ms step_avg:54.02ms
step:1472/1900 train_time:79555ms step_avg:54.05ms
step:1473/1900 train_time:79644ms step_avg:54.07ms
step:1474/1900 train_time:79731ms step_avg:54.09ms
step:1475/1900 train_time:79819ms step_avg:54.11ms
step:1476/1900 train_time:79907ms step_avg:54.14ms
step:1477/1900 train_time:79995ms step_avg:54.16ms
step:1478/1900 train_time:80082ms step_avg:54.18ms
step:1479/1900 train_time:80171ms step_avg:54.21ms
step:1480/1900 train_time:80258ms step_avg:54.23ms
step:1481/1900 train_time:80347ms step_avg:54.25ms
step:1482/1900 train_time:80434ms step_avg:54.27ms
step:1483/1900 train_time:80522ms step_avg:54.30ms
step:1484/1900 train_time:80609ms step_avg:54.32ms
step:1485/1900 train_time:80699ms step_avg:54.34ms
step:1486/1900 train_time:80786ms step_avg:54.36ms
step:1487/1900 train_time:80875ms step_avg:54.39ms
step:1488/1900 train_time:80963ms step_avg:54.41ms
step:1489/1900 train_time:81051ms step_avg:54.43ms
step:1490/1900 train_time:81138ms step_avg:54.46ms
step:1491/1900 train_time:81227ms step_avg:54.48ms
step:1492/1900 train_time:81315ms step_avg:54.50ms
step:1493/1900 train_time:81403ms step_avg:54.52ms
step:1494/1900 train_time:81490ms step_avg:54.54ms
step:1495/1900 train_time:81578ms step_avg:54.57ms
step:1496/1900 train_time:81666ms step_avg:54.59ms
step:1497/1900 train_time:81756ms step_avg:54.61ms
step:1498/1900 train_time:81842ms step_avg:54.63ms
step:1499/1900 train_time:81932ms step_avg:54.66ms
step:1500/1900 train_time:82019ms step_avg:54.68ms
step:1500/1900 val_loss:3.4122 train_time:82109ms step_avg:54.74ms
step:1501/1900 train_time:82129ms step_avg:54.72ms
step:1502/1900 train_time:82198ms step_avg:54.73ms
step:1503/1900 train_time:82290ms step_avg:54.75ms
step:1504/1900 train_time:82378ms step_avg:54.77ms
step:1505/1900 train_time:82467ms step_avg:54.80ms
step:1506/1900 train_time:82553ms step_avg:54.82ms
step:1507/1900 train_time:82641ms step_avg:54.84ms
step:1508/1900 train_time:82727ms step_avg:54.86ms
step:1509/1900 train_time:82815ms step_avg:54.88ms
step:1510/1900 train_time:82902ms step_avg:54.90ms
step:1511/1900 train_time:82989ms step_avg:54.92ms
step:1512/1900 train_time:83078ms step_avg:54.95ms
step:1513/1900 train_time:83170ms step_avg:54.97ms
step:1514/1900 train_time:83259ms step_avg:54.99ms
step:1515/1900 train_time:83348ms step_avg:55.01ms
step:1516/1900 train_time:83435ms step_avg:55.04ms
step:1517/1900 train_time:83524ms step_avg:55.06ms
step:1518/1900 train_time:83611ms step_avg:55.08ms
step:1519/1900 train_time:83698ms step_avg:55.10ms
step:1520/1900 train_time:83785ms step_avg:55.12ms
step:1521/1900 train_time:83873ms step_avg:55.14ms
step:1522/1900 train_time:83960ms step_avg:55.16ms
step:1523/1900 train_time:84048ms step_avg:55.19ms
step:1524/1900 train_time:84137ms step_avg:55.21ms
step:1525/1900 train_time:84228ms step_avg:55.23ms
step:1526/1900 train_time:84316ms step_avg:55.25ms
step:1527/1900 train_time:84405ms step_avg:55.27ms
step:1528/1900 train_time:84492ms step_avg:55.30ms
step:1529/1900 train_time:84581ms step_avg:55.32ms
step:1530/1900 train_time:84668ms step_avg:55.34ms
step:1531/1900 train_time:84755ms step_avg:55.36ms
step:1532/1900 train_time:84843ms step_avg:55.38ms
step:1533/1900 train_time:84931ms step_avg:55.40ms
step:1534/1900 train_time:85018ms step_avg:55.42ms
step:1535/1900 train_time:85107ms step_avg:55.44ms
step:1536/1900 train_time:85196ms step_avg:55.47ms
step:1537/1900 train_time:85286ms step_avg:55.49ms
step:1538/1900 train_time:85374ms step_avg:55.51ms
step:1539/1900 train_time:85463ms step_avg:55.53ms
step:1540/1900 train_time:85549ms step_avg:55.55ms
step:1541/1900 train_time:85637ms step_avg:55.57ms
step:1542/1900 train_time:85724ms step_avg:55.59ms
step:1543/1900 train_time:85813ms step_avg:55.61ms
step:1544/1900 train_time:85899ms step_avg:55.63ms
step:1545/1900 train_time:85988ms step_avg:55.66ms
step:1546/1900 train_time:86076ms step_avg:55.68ms
step:1547/1900 train_time:86165ms step_avg:55.70ms
step:1548/1900 train_time:86254ms step_avg:55.72ms
step:1549/1900 train_time:86343ms step_avg:55.74ms
step:1550/1900 train_time:86431ms step_avg:55.76ms
step:1551/1900 train_time:86519ms step_avg:55.78ms
step:1552/1900 train_time:86606ms step_avg:55.80ms
step:1553/1900 train_time:86694ms step_avg:55.82ms
step:1554/1900 train_time:86781ms step_avg:55.84ms
step:1555/1900 train_time:86869ms step_avg:55.86ms
step:1556/1900 train_time:86956ms step_avg:55.88ms
step:1557/1900 train_time:87045ms step_avg:55.91ms
step:1558/1900 train_time:87133ms step_avg:55.93ms
step:1559/1900 train_time:87221ms step_avg:55.95ms
step:1560/1900 train_time:87309ms step_avg:55.97ms
step:1561/1900 train_time:87398ms step_avg:55.99ms
step:1562/1900 train_time:87486ms step_avg:56.01ms
step:1563/1900 train_time:87575ms step_avg:56.03ms
step:1564/1900 train_time:87663ms step_avg:56.05ms
step:1565/1900 train_time:87751ms step_avg:56.07ms
step:1566/1900 train_time:87838ms step_avg:56.09ms
step:1567/1900 train_time:87926ms step_avg:56.11ms
step:1568/1900 train_time:88013ms step_avg:56.13ms
step:1569/1900 train_time:88102ms step_avg:56.15ms
step:1570/1900 train_time:88189ms step_avg:56.17ms
step:1571/1900 train_time:88278ms step_avg:56.19ms
step:1572/1900 train_time:88366ms step_avg:56.21ms
step:1573/1900 train_time:88455ms step_avg:56.23ms
step:1574/1900 train_time:88542ms step_avg:56.25ms
step:1575/1900 train_time:88631ms step_avg:56.27ms
step:1576/1900 train_time:88719ms step_avg:56.29ms
step:1577/1900 train_time:88807ms step_avg:56.31ms
step:1578/1900 train_time:88894ms step_avg:56.33ms
step:1579/1900 train_time:88982ms step_avg:56.35ms
step:1580/1900 train_time:89070ms step_avg:56.37ms
step:1581/1900 train_time:89159ms step_avg:56.39ms
step:1582/1900 train_time:89247ms step_avg:56.41ms
step:1583/1900 train_time:89336ms step_avg:56.43ms
step:1584/1900 train_time:89424ms step_avg:56.45ms
step:1585/1900 train_time:89513ms step_avg:56.48ms
step:1586/1900 train_time:89601ms step_avg:56.49ms
step:1587/1900 train_time:89689ms step_avg:56.51ms
step:1588/1900 train_time:89777ms step_avg:56.53ms
step:1589/1900 train_time:89865ms step_avg:56.55ms
step:1590/1900 train_time:89953ms step_avg:56.57ms
step:1591/1900 train_time:90041ms step_avg:56.59ms
step:1592/1900 train_time:90129ms step_avg:56.61ms
step:1593/1900 train_time:90217ms step_avg:56.63ms
step:1594/1900 train_time:90305ms step_avg:56.65ms
step:1595/1900 train_time:90393ms step_avg:56.67ms
step:1596/1900 train_time:90482ms step_avg:56.69ms
step:1597/1900 train_time:90570ms step_avg:56.71ms
step:1598/1900 train_time:90658ms step_avg:56.73ms
step:1599/1900 train_time:90746ms step_avg:56.75ms
step:1600/1900 train_time:90834ms step_avg:56.77ms
step:1601/1900 train_time:90923ms step_avg:56.79ms
step:1602/1900 train_time:91010ms step_avg:56.81ms
step:1603/1900 train_time:91099ms step_avg:56.83ms
step:1604/1900 train_time:91186ms step_avg:56.85ms
step:1605/1900 train_time:91275ms step_avg:56.87ms
step:1606/1900 train_time:91363ms step_avg:56.89ms
step:1607/1900 train_time:91452ms step_avg:56.91ms
step:1608/1900 train_time:91540ms step_avg:56.93ms
step:1609/1900 train_time:91628ms step_avg:56.95ms
step:1610/1900 train_time:91715ms step_avg:56.97ms
step:1611/1900 train_time:91804ms step_avg:56.99ms
step:1612/1900 train_time:91892ms step_avg:57.00ms
step:1613/1900 train_time:91980ms step_avg:57.02ms
step:1614/1900 train_time:92067ms step_avg:57.04ms
step:1615/1900 train_time:92156ms step_avg:57.06ms
step:1616/1900 train_time:92245ms step_avg:57.08ms
step:1617/1900 train_time:92334ms step_avg:57.10ms
step:1618/1900 train_time:92421ms step_avg:57.12ms
step:1619/1900 train_time:92510ms step_avg:57.14ms
step:1620/1900 train_time:92597ms step_avg:57.16ms
step:1621/1900 train_time:92686ms step_avg:57.18ms
step:1622/1900 train_time:92774ms step_avg:57.20ms
step:1623/1900 train_time:92863ms step_avg:57.22ms
step:1624/1900 train_time:92950ms step_avg:57.24ms
step:1625/1900 train_time:93038ms step_avg:57.25ms
step:1626/1900 train_time:93126ms step_avg:57.27ms
step:1627/1900 train_time:93215ms step_avg:57.29ms
step:1628/1900 train_time:93302ms step_avg:57.31ms
step:1629/1900 train_time:93391ms step_avg:57.33ms
step:1630/1900 train_time:93478ms step_avg:57.35ms
step:1631/1900 train_time:93566ms step_avg:57.37ms
step:1632/1900 train_time:93654ms step_avg:57.39ms
step:1633/1900 train_time:93742ms step_avg:57.40ms
step:1634/1900 train_time:93829ms step_avg:57.42ms
step:1635/1900 train_time:93917ms step_avg:57.44ms
step:1636/1900 train_time:94005ms step_avg:57.46ms
step:1637/1900 train_time:94094ms step_avg:57.48ms
step:1638/1900 train_time:94181ms step_avg:57.50ms
step:1639/1900 train_time:94270ms step_avg:57.52ms
step:1640/1900 train_time:94358ms step_avg:57.54ms
step:1641/1900 train_time:94446ms step_avg:57.55ms
step:1642/1900 train_time:94534ms step_avg:57.57ms
step:1643/1900 train_time:94623ms step_avg:57.59ms
step:1644/1900 train_time:94710ms step_avg:57.61ms
step:1645/1900 train_time:94799ms step_avg:57.63ms
step:1646/1900 train_time:94886ms step_avg:57.65ms
step:1647/1900 train_time:94976ms step_avg:57.67ms
step:1648/1900 train_time:95064ms step_avg:57.68ms
step:1649/1900 train_time:95153ms step_avg:57.70ms
step:1650/1900 train_time:95240ms step_avg:57.72ms
step:1651/1900 train_time:95329ms step_avg:57.74ms
step:1652/1900 train_time:95416ms step_avg:57.76ms
step:1653/1900 train_time:95504ms step_avg:57.78ms
step:1654/1900 train_time:95591ms step_avg:57.79ms
step:1655/1900 train_time:95679ms step_avg:57.81ms
step:1656/1900 train_time:95767ms step_avg:57.83ms
step:1657/1900 train_time:95856ms step_avg:57.85ms
step:1658/1900 train_time:95944ms step_avg:57.87ms
step:1659/1900 train_time:96033ms step_avg:57.89ms
step:1660/1900 train_time:96121ms step_avg:57.90ms
step:1661/1900 train_time:96210ms step_avg:57.92ms
step:1662/1900 train_time:96297ms step_avg:57.94ms
step:1663/1900 train_time:96386ms step_avg:57.96ms
step:1664/1900 train_time:96474ms step_avg:57.98ms
step:1665/1900 train_time:96564ms step_avg:58.00ms
step:1666/1900 train_time:96651ms step_avg:58.01ms
step:1667/1900 train_time:96739ms step_avg:58.03ms
step:1668/1900 train_time:96827ms step_avg:58.05ms
step:1669/1900 train_time:96916ms step_avg:58.07ms
step:1670/1900 train_time:97004ms step_avg:58.09ms
step:1671/1900 train_time:97094ms step_avg:58.11ms
step:1672/1900 train_time:97182ms step_avg:58.12ms
step:1673/1900 train_time:97270ms step_avg:58.14ms
step:1674/1900 train_time:97357ms step_avg:58.16ms
step:1675/1900 train_time:97447ms step_avg:58.18ms
step:1676/1900 train_time:97535ms step_avg:58.19ms
step:1677/1900 train_time:97623ms step_avg:58.21ms
step:1678/1900 train_time:97710ms step_avg:58.23ms
step:1679/1900 train_time:97799ms step_avg:58.25ms
step:1680/1900 train_time:97886ms step_avg:58.27ms
step:1681/1900 train_time:97976ms step_avg:58.28ms
step:1682/1900 train_time:98064ms step_avg:58.30ms
step:1683/1900 train_time:98152ms step_avg:58.32ms
step:1684/1900 train_time:98239ms step_avg:58.34ms
step:1685/1900 train_time:98327ms step_avg:58.35ms
step:1686/1900 train_time:98415ms step_avg:58.37ms
step:1687/1900 train_time:98504ms step_avg:58.39ms
step:1688/1900 train_time:98592ms step_avg:58.41ms
step:1689/1900 train_time:98680ms step_avg:58.43ms
step:1690/1900 train_time:98767ms step_avg:58.44ms
step:1691/1900 train_time:98856ms step_avg:58.46ms
step:1692/1900 train_time:98944ms step_avg:58.48ms
step:1693/1900 train_time:99032ms step_avg:58.49ms
step:1694/1900 train_time:99120ms step_avg:58.51ms
step:1695/1900 train_time:99208ms step_avg:58.53ms
step:1696/1900 train_time:99296ms step_avg:58.55ms
step:1697/1900 train_time:99385ms step_avg:58.56ms
step:1698/1900 train_time:99473ms step_avg:58.58ms
step:1699/1900 train_time:99561ms step_avg:58.60ms
step:1700/1900 train_time:99649ms step_avg:58.62ms
step:1701/1900 train_time:99738ms step_avg:58.63ms
step:1702/1900 train_time:99825ms step_avg:58.65ms
step:1703/1900 train_time:99914ms step_avg:58.67ms
step:1704/1900 train_time:100001ms step_avg:58.69ms
step:1705/1900 train_time:100090ms step_avg:58.70ms
step:1706/1900 train_time:100177ms step_avg:58.72ms
step:1707/1900 train_time:100265ms step_avg:58.74ms
step:1708/1900 train_time:100354ms step_avg:58.76ms
step:1709/1900 train_time:100442ms step_avg:58.77ms
step:1710/1900 train_time:100530ms step_avg:58.79ms
step:1711/1900 train_time:100618ms step_avg:58.81ms
step:1712/1900 train_time:100706ms step_avg:58.82ms
step:1713/1900 train_time:100794ms step_avg:58.84ms
step:1714/1900 train_time:100882ms step_avg:58.86ms
step:1715/1900 train_time:100971ms step_avg:58.88ms
step:1716/1900 train_time:101059ms step_avg:58.89ms
step:1717/1900 train_time:101147ms step_avg:58.91ms
step:1718/1900 train_time:101234ms step_avg:58.93ms
step:1719/1900 train_time:101323ms step_avg:58.94ms
step:1720/1900 train_time:101411ms step_avg:58.96ms
step:1721/1900 train_time:101498ms step_avg:58.98ms
step:1722/1900 train_time:101586ms step_avg:58.99ms
step:1723/1900 train_time:101676ms step_avg:59.01ms
step:1724/1900 train_time:101764ms step_avg:59.03ms
step:1725/1900 train_time:101852ms step_avg:59.04ms
step:1726/1900 train_time:101940ms step_avg:59.06ms
step:1727/1900 train_time:102028ms step_avg:59.08ms
step:1728/1900 train_time:102116ms step_avg:59.09ms
step:1729/1900 train_time:102206ms step_avg:59.11ms
step:1730/1900 train_time:102293ms step_avg:59.13ms
step:1731/1900 train_time:102382ms step_avg:59.15ms
step:1732/1900 train_time:102469ms step_avg:59.16ms
step:1733/1900 train_time:102557ms step_avg:59.18ms
step:1734/1900 train_time:102646ms step_avg:59.20ms
step:1735/1900 train_time:102735ms step_avg:59.21ms
step:1736/1900 train_time:102822ms step_avg:59.23ms
step:1737/1900 train_time:102911ms step_avg:59.25ms
step:1738/1900 train_time:102998ms step_avg:59.26ms
step:1739/1900 train_time:103086ms step_avg:59.28ms
step:1740/1900 train_time:103174ms step_avg:59.30ms
step:1741/1900 train_time:103262ms step_avg:59.31ms
step:1742/1900 train_time:103350ms step_avg:59.33ms
step:1743/1900 train_time:103439ms step_avg:59.35ms
step:1744/1900 train_time:103527ms step_avg:59.36ms
step:1745/1900 train_time:103615ms step_avg:59.38ms
step:1746/1900 train_time:103703ms step_avg:59.39ms
step:1747/1900 train_time:103792ms step_avg:59.41ms
step:1748/1900 train_time:103879ms step_avg:59.43ms
step:1749/1900 train_time:103967ms step_avg:59.44ms
step:1750/1900 train_time:104054ms step_avg:59.46ms
step:1750/1900 val_loss:3.3171 train_time:104145ms step_avg:59.51ms
step:1751/1900 train_time:104166ms step_avg:59.49ms
step:1752/1900 train_time:104233ms step_avg:59.49ms
step:1753/1900 train_time:104326ms step_avg:59.51ms
step:1754/1900 train_time:104414ms step_avg:59.53ms
step:1755/1900 train_time:104503ms step_avg:59.55ms
step:1756/1900 train_time:104589ms step_avg:59.56ms
step:1757/1900 train_time:104677ms step_avg:59.58ms
step:1758/1900 train_time:104764ms step_avg:59.59ms
step:1759/1900 train_time:104852ms step_avg:59.61ms
step:1760/1900 train_time:104939ms step_avg:59.62ms
step:1761/1900 train_time:105027ms step_avg:59.64ms
step:1762/1900 train_time:105116ms step_avg:59.66ms
step:1763/1900 train_time:105206ms step_avg:59.67ms
step:1764/1900 train_time:105295ms step_avg:59.69ms
step:1765/1900 train_time:105385ms step_avg:59.71ms
step:1766/1900 train_time:105472ms step_avg:59.72ms
step:1767/1900 train_time:105561ms step_avg:59.74ms
step:1768/1900 train_time:105647ms step_avg:59.76ms
step:1769/1900 train_time:105735ms step_avg:59.77ms
step:1770/1900 train_time:105822ms step_avg:59.79ms
step:1771/1900 train_time:105910ms step_avg:59.80ms
step:1772/1900 train_time:105998ms step_avg:59.82ms
step:1773/1900 train_time:106086ms step_avg:59.83ms
step:1774/1900 train_time:106176ms step_avg:59.85ms
step:1775/1900 train_time:106265ms step_avg:59.87ms
step:1776/1900 train_time:106353ms step_avg:59.88ms
step:1777/1900 train_time:106442ms step_avg:59.90ms
step:1778/1900 train_time:106529ms step_avg:59.92ms
step:1779/1900 train_time:106618ms step_avg:59.93ms
step:1780/1900 train_time:106705ms step_avg:59.95ms
step:1781/1900 train_time:106793ms step_avg:59.96ms
step:1782/1900 train_time:106880ms step_avg:59.98ms
step:1783/1900 train_time:106968ms step_avg:59.99ms
step:1784/1900 train_time:107056ms step_avg:60.01ms
step:1785/1900 train_time:107145ms step_avg:60.03ms
step:1786/1900 train_time:107234ms step_avg:60.04ms
step:1787/1900 train_time:107324ms step_avg:60.06ms
step:1788/1900 train_time:107412ms step_avg:60.07ms
step:1789/1900 train_time:107500ms step_avg:60.09ms
step:1790/1900 train_time:107588ms step_avg:60.10ms
step:1791/1900 train_time:107676ms step_avg:60.12ms
step:1792/1900 train_time:107764ms step_avg:60.14ms
step:1793/1900 train_time:107852ms step_avg:60.15ms
step:1794/1900 train_time:107940ms step_avg:60.17ms
step:1795/1900 train_time:108028ms step_avg:60.18ms
step:1796/1900 train_time:108115ms step_avg:60.20ms
step:1797/1900 train_time:108205ms step_avg:60.21ms
step:1798/1900 train_time:108294ms step_avg:60.23ms
step:1799/1900 train_time:108382ms step_avg:60.25ms
step:1800/1900 train_time:108470ms step_avg:60.26ms
step:1801/1900 train_time:108559ms step_avg:60.28ms
step:1802/1900 train_time:108646ms step_avg:60.29ms
step:1803/1900 train_time:108734ms step_avg:60.31ms
step:1804/1900 train_time:108821ms step_avg:60.32ms
step:1805/1900 train_time:108910ms step_avg:60.34ms
step:1806/1900 train_time:108997ms step_avg:60.35ms
step:1807/1900 train_time:109086ms step_avg:60.37ms
step:1808/1900 train_time:109173ms step_avg:60.38ms
step:1809/1900 train_time:109263ms step_avg:60.40ms
step:1810/1900 train_time:109350ms step_avg:60.41ms
step:1811/1900 train_time:109439ms step_avg:60.43ms
step:1812/1900 train_time:109526ms step_avg:60.44ms
step:1813/1900 train_time:109615ms step_avg:60.46ms
step:1814/1900 train_time:109702ms step_avg:60.48ms
step:1815/1900 train_time:109790ms step_avg:60.49ms
step:1816/1900 train_time:109878ms step_avg:60.51ms
step:1817/1900 train_time:109967ms step_avg:60.52ms
step:1818/1900 train_time:110055ms step_avg:60.54ms
step:1819/1900 train_time:110143ms step_avg:60.55ms
step:1820/1900 train_time:110231ms step_avg:60.57ms
step:1821/1900 train_time:110319ms step_avg:60.58ms
step:1822/1900 train_time:110406ms step_avg:60.60ms
step:1823/1900 train_time:110495ms step_avg:60.61ms
step:1824/1900 train_time:110582ms step_avg:60.63ms
step:1825/1900 train_time:110671ms step_avg:60.64ms
step:1826/1900 train_time:110759ms step_avg:60.66ms
step:1827/1900 train_time:110848ms step_avg:60.67ms
step:1828/1900 train_time:110934ms step_avg:60.69ms
step:1829/1900 train_time:111023ms step_avg:60.70ms
step:1830/1900 train_time:111111ms step_avg:60.72ms
step:1831/1900 train_time:111199ms step_avg:60.73ms
step:1832/1900 train_time:111287ms step_avg:60.75ms
step:1833/1900 train_time:111376ms step_avg:60.76ms
step:1834/1900 train_time:111464ms step_avg:60.78ms
step:1835/1900 train_time:111552ms step_avg:60.79ms
step:1836/1900 train_time:111640ms step_avg:60.81ms
step:1837/1900 train_time:111728ms step_avg:60.82ms
step:1838/1900 train_time:111815ms step_avg:60.84ms
step:1839/1900 train_time:111903ms step_avg:60.85ms
step:1840/1900 train_time:111991ms step_avg:60.86ms
step:1841/1900 train_time:112079ms step_avg:60.88ms
step:1842/1900 train_time:112166ms step_avg:60.89ms
step:1843/1900 train_time:112255ms step_avg:60.91ms
step:1844/1900 train_time:112343ms step_avg:60.92ms
step:1845/1900 train_time:112432ms step_avg:60.94ms
step:1846/1900 train_time:112519ms step_avg:60.95ms
step:1847/1900 train_time:112608ms step_avg:60.97ms
step:1848/1900 train_time:112695ms step_avg:60.98ms
step:1849/1900 train_time:112783ms step_avg:61.00ms
step:1850/1900 train_time:112871ms step_avg:61.01ms
step:1851/1900 train_time:112959ms step_avg:61.03ms
step:1852/1900 train_time:113046ms step_avg:61.04ms
step:1853/1900 train_time:113135ms step_avg:61.05ms
step:1854/1900 train_time:113222ms step_avg:61.07ms
step:1855/1900 train_time:113311ms step_avg:61.08ms
step:1856/1900 train_time:113398ms step_avg:61.10ms
step:1857/1900 train_time:113488ms step_avg:61.11ms
step:1858/1900 train_time:113576ms step_avg:61.13ms
step:1859/1900 train_time:113664ms step_avg:61.14ms
step:1860/1900 train_time:113752ms step_avg:61.16ms
step:1861/1900 train_time:113841ms step_avg:61.17ms
step:1862/1900 train_time:113928ms step_avg:61.19ms
step:1863/1900 train_time:114017ms step_avg:61.20ms
step:1864/1900 train_time:114105ms step_avg:61.22ms
step:1865/1900 train_time:114194ms step_avg:61.23ms
step:1866/1900 train_time:114282ms step_avg:61.24ms
step:1867/1900 train_time:114371ms step_avg:61.26ms
step:1868/1900 train_time:114460ms step_avg:61.27ms
step:1869/1900 train_time:114548ms step_avg:61.29ms
step:1870/1900 train_time:114636ms step_avg:61.30ms
step:1871/1900 train_time:114725ms step_avg:61.32ms
step:1872/1900 train_time:114813ms step_avg:61.33ms
step:1873/1900 train_time:114902ms step_avg:61.35ms
step:1874/1900 train_time:114989ms step_avg:61.36ms
step:1875/1900 train_time:115078ms step_avg:61.37ms
step:1876/1900 train_time:115165ms step_avg:61.39ms
step:1877/1900 train_time:115254ms step_avg:61.40ms
step:1878/1900 train_time:115342ms step_avg:61.42ms
step:1879/1900 train_time:115431ms step_avg:61.43ms
step:1880/1900 train_time:115519ms step_avg:61.45ms
step:1881/1900 train_time:115608ms step_avg:61.46ms
step:1882/1900 train_time:115695ms step_avg:61.47ms
step:1883/1900 train_time:115784ms step_avg:61.49ms
step:1884/1900 train_time:115872ms step_avg:61.50ms
step:1885/1900 train_time:115961ms step_avg:61.52ms
step:1886/1900 train_time:116048ms step_avg:61.53ms
step:1887/1900 train_time:116136ms step_avg:61.55ms
step:1888/1900 train_time:116225ms step_avg:61.56ms
step:1889/1900 train_time:116315ms step_avg:61.57ms
step:1890/1900 train_time:116403ms step_avg:61.59ms
step:1891/1900 train_time:116493ms step_avg:61.60ms
step:1892/1900 train_time:116580ms step_avg:61.62ms
step:1893/1900 train_time:116668ms step_avg:61.63ms
step:1894/1900 train_time:116756ms step_avg:61.65ms
step:1895/1900 train_time:116844ms step_avg:61.66ms
step:1896/1900 train_time:116932ms step_avg:61.67ms
step:1897/1900 train_time:117021ms step_avg:61.69ms
step:1898/1900 train_time:117108ms step_avg:61.70ms
step:1899/1900 train_time:117197ms step_avg:61.71ms
step:1900/1900 train_time:117284ms step_avg:61.73ms
step:1900/1900 val_loss:3.2771 train_time:117376ms step_avg:61.78ms
peak memory allocated: 29709 MiB reserved: 44998 MiB

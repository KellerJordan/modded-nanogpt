import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 07:58:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     38029      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38030      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38031      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38032      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38033      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38034      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38035      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     38036      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     38030      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     38031      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     38032      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     38033      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     38034      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     38035      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     38036      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8349 train_time:0ms step_avg:0.21ms
step:1/1900 train_time:76ms step_avg:76.14ms
step:2/1900 train_time:100ms step_avg:50.14ms
step:3/1900 train_time:121ms step_avg:40.42ms
step:4/1900 train_time:154ms step_avg:38.48ms
step:5/1900 train_time:188ms step_avg:37.55ms
step:6/1900 train_time:271ms step_avg:45.16ms
step:7/1900 train_time:290ms step_avg:41.43ms
step:8/1900 train_time:317ms step_avg:39.60ms
step:9/1900 train_time:351ms step_avg:38.95ms
step:10/1900 train_time:384ms step_avg:38.44ms
step:11/1900 train_time:418ms step_avg:38.04ms
step:12/1900 train_time:452ms step_avg:37.69ms
step:13/1900 train_time:486ms step_avg:37.42ms
step:14/1900 train_time:520ms step_avg:37.16ms
step:15/1900 train_time:555ms step_avg:37.00ms
step:16/1900 train_time:589ms step_avg:36.80ms
step:17/1900 train_time:623ms step_avg:36.64ms
step:18/1900 train_time:657ms step_avg:36.49ms
step:19/1900 train_time:691ms step_avg:36.36ms
step:20/1900 train_time:725ms step_avg:36.24ms
step:21/1900 train_time:759ms step_avg:36.14ms
step:22/1900 train_time:793ms step_avg:36.04ms
step:23/1900 train_time:827ms step_avg:35.95ms
step:24/1900 train_time:861ms step_avg:35.87ms
step:25/1900 train_time:895ms step_avg:35.79ms
step:26/1900 train_time:929ms step_avg:35.72ms
step:27/1900 train_time:963ms step_avg:35.66ms
step:28/1900 train_time:997ms step_avg:35.60ms
step:29/1900 train_time:1031ms step_avg:35.55ms
step:30/1900 train_time:1065ms step_avg:35.49ms
step:31/1900 train_time:1099ms step_avg:35.44ms
step:32/1900 train_time:1133ms step_avg:35.40ms
step:33/1900 train_time:1167ms step_avg:35.37ms
step:34/1900 train_time:1201ms step_avg:35.33ms
step:35/1900 train_time:1236ms step_avg:35.31ms
step:36/1900 train_time:1270ms step_avg:35.28ms
step:37/1900 train_time:1305ms step_avg:35.27ms
step:38/1900 train_time:1339ms step_avg:35.23ms
step:39/1900 train_time:1373ms step_avg:35.21ms
step:40/1900 train_time:1407ms step_avg:35.18ms
step:41/1900 train_time:1441ms step_avg:35.16ms
step:42/1900 train_time:1475ms step_avg:35.13ms
step:43/1900 train_time:1510ms step_avg:35.11ms
step:44/1900 train_time:1543ms step_avg:35.08ms
step:45/1900 train_time:1578ms step_avg:35.06ms
step:46/1900 train_time:1612ms step_avg:35.03ms
step:47/1900 train_time:1646ms step_avg:35.02ms
step:48/1900 train_time:1680ms step_avg:34.99ms
step:49/1900 train_time:1714ms step_avg:34.98ms
step:50/1900 train_time:1748ms step_avg:34.96ms
step:51/1900 train_time:1782ms step_avg:34.94ms
step:52/1900 train_time:1816ms step_avg:34.92ms
step:53/1900 train_time:1850ms step_avg:34.91ms
step:54/1900 train_time:1884ms step_avg:34.89ms
step:55/1900 train_time:1918ms step_avg:34.87ms
step:56/1900 train_time:1952ms step_avg:34.86ms
step:57/1900 train_time:1986ms step_avg:34.84ms
step:58/1900 train_time:2020ms step_avg:34.83ms
step:59/1900 train_time:2054ms step_avg:34.81ms
step:60/1900 train_time:2088ms step_avg:34.80ms
step:61/1900 train_time:2122ms step_avg:34.78ms
step:62/1900 train_time:2156ms step_avg:34.77ms
step:63/1900 train_time:2190ms step_avg:34.76ms
step:64/1900 train_time:2224ms step_avg:34.75ms
step:65/1900 train_time:2259ms step_avg:34.75ms
step:66/1900 train_time:2293ms step_avg:34.74ms
step:67/1900 train_time:2327ms step_avg:34.73ms
step:68/1900 train_time:2361ms step_avg:34.72ms
step:69/1900 train_time:2395ms step_avg:34.71ms
step:70/1900 train_time:2429ms step_avg:34.70ms
step:71/1900 train_time:2463ms step_avg:34.69ms
step:72/1900 train_time:2497ms step_avg:34.68ms
step:73/1900 train_time:2531ms step_avg:34.68ms
step:74/1900 train_time:2565ms step_avg:34.66ms
step:75/1900 train_time:2599ms step_avg:34.66ms
step:76/1900 train_time:2633ms step_avg:34.65ms
step:77/1900 train_time:2668ms step_avg:34.65ms
step:78/1900 train_time:2702ms step_avg:34.64ms
step:79/1900 train_time:2736ms step_avg:34.63ms
step:80/1900 train_time:2770ms step_avg:34.62ms
step:81/1900 train_time:2804ms step_avg:34.62ms
step:82/1900 train_time:2838ms step_avg:34.61ms
step:83/1900 train_time:2872ms step_avg:34.60ms
step:84/1900 train_time:2906ms step_avg:34.59ms
step:85/1900 train_time:2940ms step_avg:34.58ms
step:86/1900 train_time:2974ms step_avg:34.58ms
step:87/1900 train_time:3008ms step_avg:34.57ms
step:88/1900 train_time:3041ms step_avg:34.56ms
step:89/1900 train_time:3076ms step_avg:34.56ms
step:90/1900 train_time:3110ms step_avg:34.55ms
step:91/1900 train_time:3144ms step_avg:34.55ms
step:92/1900 train_time:3178ms step_avg:34.54ms
step:93/1900 train_time:3212ms step_avg:34.54ms
step:94/1900 train_time:3246ms step_avg:34.53ms
step:95/1900 train_time:3280ms step_avg:34.53ms
step:96/1900 train_time:3314ms step_avg:34.52ms
step:97/1900 train_time:3348ms step_avg:34.52ms
step:98/1900 train_time:3382ms step_avg:34.51ms
step:99/1900 train_time:3416ms step_avg:34.50ms
step:100/1900 train_time:3450ms step_avg:34.50ms
step:101/1900 train_time:3484ms step_avg:34.49ms
step:102/1900 train_time:3518ms step_avg:34.49ms
step:103/1900 train_time:3552ms step_avg:34.48ms
step:104/1900 train_time:3586ms step_avg:34.48ms
step:105/1900 train_time:3620ms step_avg:34.48ms
step:106/1900 train_time:3654ms step_avg:34.47ms
step:107/1900 train_time:3688ms step_avg:34.47ms
step:108/1900 train_time:3722ms step_avg:34.46ms
step:109/1900 train_time:3756ms step_avg:34.46ms
step:110/1900 train_time:3790ms step_avg:34.45ms
step:111/1900 train_time:3824ms step_avg:34.45ms
step:112/1900 train_time:3858ms step_avg:34.44ms
step:113/1900 train_time:3892ms step_avg:34.44ms
step:114/1900 train_time:3926ms step_avg:34.44ms
step:115/1900 train_time:3960ms step_avg:34.44ms
step:116/1900 train_time:3994ms step_avg:34.43ms
step:117/1900 train_time:4028ms step_avg:34.43ms
step:118/1900 train_time:4062ms step_avg:34.42ms
step:119/1900 train_time:4096ms step_avg:34.42ms
step:120/1900 train_time:4130ms step_avg:34.42ms
step:121/1900 train_time:4165ms step_avg:34.42ms
step:122/1900 train_time:4199ms step_avg:34.41ms
step:123/1900 train_time:4233ms step_avg:34.41ms
step:124/1900 train_time:4266ms step_avg:34.41ms
step:125/1900 train_time:4301ms step_avg:34.41ms
step:126/1900 train_time:4335ms step_avg:34.40ms
step:127/1900 train_time:4369ms step_avg:34.40ms
step:128/1900 train_time:4403ms step_avg:34.40ms
step:129/1900 train_time:4436ms step_avg:34.39ms
step:130/1900 train_time:4470ms step_avg:34.39ms
step:131/1900 train_time:4505ms step_avg:34.39ms
step:132/1900 train_time:4539ms step_avg:34.38ms
step:133/1900 train_time:4573ms step_avg:34.38ms
step:134/1900 train_time:4607ms step_avg:34.38ms
step:135/1900 train_time:4641ms step_avg:34.38ms
step:136/1900 train_time:4675ms step_avg:34.37ms
step:137/1900 train_time:4709ms step_avg:34.37ms
step:138/1900 train_time:4742ms step_avg:34.37ms
step:139/1900 train_time:4776ms step_avg:34.36ms
step:140/1900 train_time:4810ms step_avg:34.36ms
step:141/1900 train_time:4844ms step_avg:34.36ms
step:142/1900 train_time:4878ms step_avg:34.35ms
step:143/1900 train_time:4913ms step_avg:34.35ms
step:144/1900 train_time:4946ms step_avg:34.35ms
step:145/1900 train_time:4981ms step_avg:34.35ms
step:146/1900 train_time:5014ms step_avg:34.34ms
step:147/1900 train_time:5048ms step_avg:34.34ms
step:148/1900 train_time:5082ms step_avg:34.34ms
step:149/1900 train_time:5116ms step_avg:34.34ms
step:150/1900 train_time:5151ms step_avg:34.34ms
step:151/1900 train_time:5185ms step_avg:34.34ms
step:152/1900 train_time:5219ms step_avg:34.33ms
step:153/1900 train_time:5253ms step_avg:34.33ms
step:154/1900 train_time:5287ms step_avg:34.33ms
step:155/1900 train_time:5321ms step_avg:34.33ms
step:156/1900 train_time:5355ms step_avg:34.33ms
step:157/1900 train_time:5389ms step_avg:34.32ms
step:158/1900 train_time:5422ms step_avg:34.32ms
step:159/1900 train_time:5457ms step_avg:34.32ms
step:160/1900 train_time:5490ms step_avg:34.32ms
step:161/1900 train_time:5524ms step_avg:34.31ms
step:162/1900 train_time:5558ms step_avg:34.31ms
step:163/1900 train_time:5592ms step_avg:34.31ms
step:164/1900 train_time:5626ms step_avg:34.31ms
step:165/1900 train_time:5661ms step_avg:34.31ms
step:166/1900 train_time:5694ms step_avg:34.30ms
step:167/1900 train_time:5729ms step_avg:34.30ms
step:168/1900 train_time:5763ms step_avg:34.30ms
step:169/1900 train_time:5797ms step_avg:34.30ms
step:170/1900 train_time:5831ms step_avg:34.30ms
step:171/1900 train_time:5864ms step_avg:34.30ms
step:172/1900 train_time:5898ms step_avg:34.29ms
step:173/1900 train_time:5933ms step_avg:34.29ms
step:174/1900 train_time:5967ms step_avg:34.29ms
step:175/1900 train_time:6001ms step_avg:34.29ms
step:176/1900 train_time:6035ms step_avg:34.29ms
step:177/1900 train_time:6069ms step_avg:34.29ms
step:178/1900 train_time:6102ms step_avg:34.28ms
step:179/1900 train_time:6137ms step_avg:34.28ms
step:180/1900 train_time:6171ms step_avg:34.28ms
step:181/1900 train_time:6205ms step_avg:34.28ms
step:182/1900 train_time:6239ms step_avg:34.28ms
step:183/1900 train_time:6273ms step_avg:34.28ms
step:184/1900 train_time:6307ms step_avg:34.28ms
step:185/1900 train_time:6341ms step_avg:34.27ms
step:186/1900 train_time:6375ms step_avg:34.27ms
step:187/1900 train_time:6409ms step_avg:34.27ms
step:188/1900 train_time:6443ms step_avg:34.27ms
step:189/1900 train_time:6477ms step_avg:34.27ms
step:190/1900 train_time:6510ms step_avg:34.27ms
step:191/1900 train_time:6545ms step_avg:34.26ms
step:192/1900 train_time:6578ms step_avg:34.26ms
step:193/1900 train_time:6612ms step_avg:34.26ms
step:194/1900 train_time:6646ms step_avg:34.26ms
step:195/1900 train_time:6680ms step_avg:34.26ms
step:196/1900 train_time:6714ms step_avg:34.26ms
step:197/1900 train_time:6748ms step_avg:34.26ms
step:198/1900 train_time:6782ms step_avg:34.25ms
step:199/1900 train_time:6816ms step_avg:34.25ms
step:200/1900 train_time:6850ms step_avg:34.25ms
step:201/1900 train_time:6884ms step_avg:34.25ms
step:202/1900 train_time:6918ms step_avg:34.25ms
step:203/1900 train_time:6952ms step_avg:34.25ms
step:204/1900 train_time:6986ms step_avg:34.24ms
step:205/1900 train_time:7020ms step_avg:34.25ms
step:206/1900 train_time:7054ms step_avg:34.24ms
step:207/1900 train_time:7089ms step_avg:34.24ms
step:208/1900 train_time:7122ms step_avg:34.24ms
step:209/1900 train_time:7156ms step_avg:34.24ms
step:210/1900 train_time:7190ms step_avg:34.24ms
step:211/1900 train_time:7224ms step_avg:34.24ms
step:212/1900 train_time:7258ms step_avg:34.24ms
step:213/1900 train_time:7293ms step_avg:34.24ms
step:214/1900 train_time:7327ms step_avg:34.24ms
step:215/1900 train_time:7361ms step_avg:34.24ms
step:216/1900 train_time:7395ms step_avg:34.24ms
step:217/1900 train_time:7429ms step_avg:34.23ms
step:218/1900 train_time:7463ms step_avg:34.23ms
step:219/1900 train_time:7497ms step_avg:34.23ms
step:220/1900 train_time:7531ms step_avg:34.23ms
step:221/1900 train_time:7565ms step_avg:34.23ms
step:222/1900 train_time:7599ms step_avg:34.23ms
step:223/1900 train_time:7633ms step_avg:34.23ms
step:224/1900 train_time:7667ms step_avg:34.23ms
step:225/1900 train_time:7701ms step_avg:34.23ms
step:226/1900 train_time:7735ms step_avg:34.23ms
step:227/1900 train_time:7769ms step_avg:34.23ms
step:228/1900 train_time:7803ms step_avg:34.22ms
step:229/1900 train_time:7837ms step_avg:34.22ms
step:230/1900 train_time:7871ms step_avg:34.22ms
step:231/1900 train_time:7905ms step_avg:34.22ms
step:232/1900 train_time:7939ms step_avg:34.22ms
step:233/1900 train_time:7973ms step_avg:34.22ms
step:234/1900 train_time:8007ms step_avg:34.22ms
step:235/1900 train_time:8041ms step_avg:34.22ms
step:236/1900 train_time:8074ms step_avg:34.21ms
step:237/1900 train_time:8108ms step_avg:34.21ms
step:238/1900 train_time:8142ms step_avg:34.21ms
step:239/1900 train_time:8176ms step_avg:34.21ms
step:240/1900 train_time:8210ms step_avg:34.21ms
step:241/1900 train_time:8245ms step_avg:34.21ms
step:242/1900 train_time:8278ms step_avg:34.21ms
step:243/1900 train_time:8312ms step_avg:34.21ms
step:244/1900 train_time:8346ms step_avg:34.21ms
step:245/1900 train_time:8380ms step_avg:34.20ms
step:246/1900 train_time:8414ms step_avg:34.20ms
step:247/1900 train_time:8448ms step_avg:34.20ms
step:248/1900 train_time:8482ms step_avg:34.20ms
step:249/1900 train_time:8516ms step_avg:34.20ms
step:250/1900 train_time:8550ms step_avg:34.20ms
step:250/1900 val_loss:4.6091 train_time:8587ms step_avg:34.35ms
step:251/1900 train_time:8608ms step_avg:34.29ms
step:252/1900 train_time:8628ms step_avg:34.24ms
step:253/1900 train_time:8655ms step_avg:34.21ms
step:254/1900 train_time:8689ms step_avg:34.21ms
step:255/1900 train_time:8725ms step_avg:34.21ms
step:256/1900 train_time:8759ms step_avg:34.21ms
step:257/1900 train_time:8793ms step_avg:34.21ms
step:258/1900 train_time:8827ms step_avg:34.21ms
step:259/1900 train_time:8861ms step_avg:34.21ms
step:260/1900 train_time:8895ms step_avg:34.21ms
step:261/1900 train_time:8929ms step_avg:34.21ms
step:262/1900 train_time:8962ms step_avg:34.21ms
step:263/1900 train_time:8996ms step_avg:34.21ms
step:264/1900 train_time:9030ms step_avg:34.20ms
step:265/1900 train_time:9064ms step_avg:34.20ms
step:266/1900 train_time:9098ms step_avg:34.20ms
step:267/1900 train_time:9132ms step_avg:34.20ms
step:268/1900 train_time:9165ms step_avg:34.20ms
step:269/1900 train_time:9199ms step_avg:34.20ms
step:270/1900 train_time:9233ms step_avg:34.20ms
step:271/1900 train_time:9267ms step_avg:34.20ms
step:272/1900 train_time:9301ms step_avg:34.19ms
step:273/1900 train_time:9335ms step_avg:34.19ms
step:274/1900 train_time:9369ms step_avg:34.19ms
step:275/1900 train_time:9403ms step_avg:34.19ms
step:276/1900 train_time:9436ms step_avg:34.19ms
step:277/1900 train_time:9470ms step_avg:34.19ms
step:278/1900 train_time:9504ms step_avg:34.19ms
step:279/1900 train_time:9538ms step_avg:34.19ms
step:280/1900 train_time:9572ms step_avg:34.19ms
step:281/1900 train_time:9606ms step_avg:34.19ms
step:282/1900 train_time:9640ms step_avg:34.19ms
step:283/1900 train_time:9675ms step_avg:34.19ms
step:284/1900 train_time:9709ms step_avg:34.19ms
step:285/1900 train_time:9743ms step_avg:34.19ms
step:286/1900 train_time:9777ms step_avg:34.19ms
step:287/1900 train_time:9811ms step_avg:34.19ms
step:288/1900 train_time:9845ms step_avg:34.19ms
step:289/1900 train_time:9880ms step_avg:34.19ms
step:290/1900 train_time:9913ms step_avg:34.18ms
step:291/1900 train_time:9948ms step_avg:34.18ms
step:292/1900 train_time:9981ms step_avg:34.18ms
step:293/1900 train_time:10015ms step_avg:34.18ms
step:294/1900 train_time:10049ms step_avg:34.18ms
step:295/1900 train_time:10083ms step_avg:34.18ms
step:296/1900 train_time:10117ms step_avg:34.18ms
step:297/1900 train_time:10151ms step_avg:34.18ms
step:298/1900 train_time:10185ms step_avg:34.18ms
step:299/1900 train_time:10219ms step_avg:34.18ms
step:300/1900 train_time:10253ms step_avg:34.18ms
step:301/1900 train_time:10287ms step_avg:34.18ms
step:302/1900 train_time:10321ms step_avg:34.17ms
step:303/1900 train_time:10355ms step_avg:34.17ms
step:304/1900 train_time:10388ms step_avg:34.17ms
step:305/1900 train_time:10422ms step_avg:34.17ms
step:306/1900 train_time:10456ms step_avg:34.17ms
step:307/1900 train_time:10490ms step_avg:34.17ms
step:308/1900 train_time:10524ms step_avg:34.17ms
step:309/1900 train_time:10558ms step_avg:34.17ms
step:310/1900 train_time:10592ms step_avg:34.17ms
step:311/1900 train_time:10626ms step_avg:34.17ms
step:312/1900 train_time:10660ms step_avg:34.17ms
step:313/1900 train_time:10694ms step_avg:34.17ms
step:314/1900 train_time:10728ms step_avg:34.16ms
step:315/1900 train_time:10762ms step_avg:34.17ms
step:316/1900 train_time:10796ms step_avg:34.16ms
step:317/1900 train_time:10830ms step_avg:34.16ms
step:318/1900 train_time:10864ms step_avg:34.16ms
step:319/1900 train_time:10898ms step_avg:34.16ms
step:320/1900 train_time:10933ms step_avg:34.16ms
step:321/1900 train_time:10966ms step_avg:34.16ms
step:322/1900 train_time:11000ms step_avg:34.16ms
step:323/1900 train_time:11034ms step_avg:34.16ms
step:324/1900 train_time:11068ms step_avg:34.16ms
step:325/1900 train_time:11102ms step_avg:34.16ms
step:326/1900 train_time:11136ms step_avg:34.16ms
step:327/1900 train_time:11170ms step_avg:34.16ms
step:328/1900 train_time:11204ms step_avg:34.16ms
step:329/1900 train_time:11238ms step_avg:34.16ms
step:330/1900 train_time:11272ms step_avg:34.16ms
step:331/1900 train_time:11305ms step_avg:34.16ms
step:332/1900 train_time:11339ms step_avg:34.15ms
step:333/1900 train_time:11373ms step_avg:34.15ms
step:334/1900 train_time:11407ms step_avg:34.15ms
step:335/1900 train_time:11441ms step_avg:34.15ms
step:336/1900 train_time:11475ms step_avg:34.15ms
step:337/1900 train_time:11509ms step_avg:34.15ms
step:338/1900 train_time:11543ms step_avg:34.15ms
step:339/1900 train_time:11577ms step_avg:34.15ms
step:340/1900 train_time:11610ms step_avg:34.15ms
step:341/1900 train_time:11644ms step_avg:34.15ms
step:342/1900 train_time:11678ms step_avg:34.15ms
step:343/1900 train_time:11712ms step_avg:34.15ms
step:344/1900 train_time:11746ms step_avg:34.15ms
step:345/1900 train_time:11781ms step_avg:34.15ms
step:346/1900 train_time:11815ms step_avg:34.15ms
step:347/1900 train_time:11849ms step_avg:34.15ms
step:348/1900 train_time:11883ms step_avg:34.15ms
step:349/1900 train_time:11917ms step_avg:34.15ms
step:350/1900 train_time:11950ms step_avg:34.14ms
step:351/1900 train_time:11985ms step_avg:34.14ms
step:352/1900 train_time:12018ms step_avg:34.14ms
step:353/1900 train_time:12053ms step_avg:34.14ms
step:354/1900 train_time:12086ms step_avg:34.14ms
step:355/1900 train_time:12120ms step_avg:34.14ms
step:356/1900 train_time:12154ms step_avg:34.14ms
step:357/1900 train_time:12188ms step_avg:34.14ms
step:358/1900 train_time:12222ms step_avg:34.14ms
step:359/1900 train_time:12256ms step_avg:34.14ms
step:360/1900 train_time:12290ms step_avg:34.14ms
step:361/1900 train_time:12324ms step_avg:34.14ms
step:362/1900 train_time:12358ms step_avg:34.14ms
step:363/1900 train_time:12392ms step_avg:34.14ms
step:364/1900 train_time:12425ms step_avg:34.14ms
step:365/1900 train_time:12459ms step_avg:34.14ms
step:366/1900 train_time:12493ms step_avg:34.13ms
step:367/1900 train_time:12527ms step_avg:34.13ms
step:368/1900 train_time:12561ms step_avg:34.13ms
step:369/1900 train_time:12595ms step_avg:34.13ms
step:370/1900 train_time:12629ms step_avg:34.13ms
step:371/1900 train_time:12663ms step_avg:34.13ms
step:372/1900 train_time:12697ms step_avg:34.13ms
step:373/1900 train_time:12731ms step_avg:34.13ms
step:374/1900 train_time:12764ms step_avg:34.13ms
step:375/1900 train_time:12799ms step_avg:34.13ms
step:376/1900 train_time:12833ms step_avg:34.13ms
step:377/1900 train_time:12867ms step_avg:34.13ms
step:378/1900 train_time:12900ms step_avg:34.13ms
step:379/1900 train_time:12935ms step_avg:34.13ms
step:380/1900 train_time:12968ms step_avg:34.13ms
step:381/1900 train_time:13003ms step_avg:34.13ms
step:382/1900 train_time:13036ms step_avg:34.13ms
step:383/1900 train_time:13070ms step_avg:34.13ms
step:384/1900 train_time:13104ms step_avg:34.13ms
step:385/1900 train_time:13138ms step_avg:34.13ms
step:386/1900 train_time:13172ms step_avg:34.12ms
step:387/1900 train_time:13206ms step_avg:34.12ms
step:388/1900 train_time:13240ms step_avg:34.12ms
step:389/1900 train_time:13274ms step_avg:34.12ms
step:390/1900 train_time:13308ms step_avg:34.12ms
step:391/1900 train_time:13342ms step_avg:34.12ms
step:392/1900 train_time:13375ms step_avg:34.12ms
step:393/1900 train_time:13409ms step_avg:34.12ms
step:394/1900 train_time:13443ms step_avg:34.12ms
step:395/1900 train_time:13477ms step_avg:34.12ms
step:396/1900 train_time:13511ms step_avg:34.12ms
step:397/1900 train_time:13545ms step_avg:34.12ms
step:398/1900 train_time:13579ms step_avg:34.12ms
step:399/1900 train_time:13613ms step_avg:34.12ms
step:400/1900 train_time:13647ms step_avg:34.12ms
step:401/1900 train_time:13680ms step_avg:34.12ms
step:402/1900 train_time:13714ms step_avg:34.11ms
step:403/1900 train_time:13748ms step_avg:34.11ms
step:404/1900 train_time:13782ms step_avg:34.11ms
step:405/1900 train_time:13816ms step_avg:34.11ms
step:406/1900 train_time:13850ms step_avg:34.11ms
step:407/1900 train_time:13884ms step_avg:34.11ms
step:408/1900 train_time:13918ms step_avg:34.11ms
step:409/1900 train_time:13952ms step_avg:34.11ms
step:410/1900 train_time:13986ms step_avg:34.11ms
step:411/1900 train_time:14020ms step_avg:34.11ms
step:412/1900 train_time:14054ms step_avg:34.11ms
step:413/1900 train_time:14088ms step_avg:34.11ms
step:414/1900 train_time:14122ms step_avg:34.11ms
step:415/1900 train_time:14156ms step_avg:34.11ms
step:416/1900 train_time:14190ms step_avg:34.11ms
step:417/1900 train_time:14224ms step_avg:34.11ms
step:418/1900 train_time:14258ms step_avg:34.11ms
step:419/1900 train_time:14291ms step_avg:34.11ms
step:420/1900 train_time:14325ms step_avg:34.11ms
step:421/1900 train_time:14359ms step_avg:34.11ms
step:422/1900 train_time:14393ms step_avg:34.11ms
step:423/1900 train_time:14427ms step_avg:34.11ms
step:424/1900 train_time:14461ms step_avg:34.11ms
step:425/1900 train_time:14495ms step_avg:34.11ms
step:426/1900 train_time:14529ms step_avg:34.10ms
step:427/1900 train_time:14562ms step_avg:34.10ms
step:428/1900 train_time:14596ms step_avg:34.10ms
step:429/1900 train_time:14630ms step_avg:34.10ms
step:430/1900 train_time:14664ms step_avg:34.10ms
step:431/1900 train_time:14698ms step_avg:34.10ms
step:432/1900 train_time:14732ms step_avg:34.10ms
step:433/1900 train_time:14766ms step_avg:34.10ms
step:434/1900 train_time:14799ms step_avg:34.10ms
step:435/1900 train_time:14833ms step_avg:34.10ms
step:436/1900 train_time:14867ms step_avg:34.10ms
step:437/1900 train_time:14901ms step_avg:34.10ms
step:438/1900 train_time:14935ms step_avg:34.10ms
step:439/1900 train_time:14969ms step_avg:34.10ms
step:440/1900 train_time:15003ms step_avg:34.10ms
step:441/1900 train_time:15037ms step_avg:34.10ms
step:442/1900 train_time:15071ms step_avg:34.10ms
step:443/1900 train_time:15105ms step_avg:34.10ms
step:444/1900 train_time:15139ms step_avg:34.10ms
step:445/1900 train_time:15173ms step_avg:34.10ms
step:446/1900 train_time:15207ms step_avg:34.10ms
step:447/1900 train_time:15241ms step_avg:34.10ms
step:448/1900 train_time:15275ms step_avg:34.10ms
step:449/1900 train_time:15309ms step_avg:34.10ms
step:450/1900 train_time:15343ms step_avg:34.10ms
step:451/1900 train_time:15377ms step_avg:34.10ms
step:452/1900 train_time:15411ms step_avg:34.10ms
step:453/1900 train_time:15445ms step_avg:34.09ms
step:454/1900 train_time:15479ms step_avg:34.09ms
step:455/1900 train_time:15513ms step_avg:34.09ms
step:456/1900 train_time:15546ms step_avg:34.09ms
step:457/1900 train_time:15581ms step_avg:34.09ms
step:458/1900 train_time:15615ms step_avg:34.09ms
step:459/1900 train_time:15649ms step_avg:34.09ms
step:460/1900 train_time:15683ms step_avg:34.09ms
step:461/1900 train_time:15717ms step_avg:34.09ms
step:462/1900 train_time:15751ms step_avg:34.09ms
step:463/1900 train_time:15785ms step_avg:34.09ms
step:464/1900 train_time:15819ms step_avg:34.09ms
step:465/1900 train_time:15853ms step_avg:34.09ms
step:466/1900 train_time:15887ms step_avg:34.09ms
step:467/1900 train_time:15921ms step_avg:34.09ms
step:468/1900 train_time:15955ms step_avg:34.09ms
step:469/1900 train_time:15989ms step_avg:34.09ms
step:470/1900 train_time:16022ms step_avg:34.09ms
step:471/1900 train_time:16057ms step_avg:34.09ms
step:472/1900 train_time:16091ms step_avg:34.09ms
step:473/1900 train_time:16125ms step_avg:34.09ms
step:474/1900 train_time:16159ms step_avg:34.09ms
step:475/1900 train_time:16193ms step_avg:34.09ms
step:476/1900 train_time:16227ms step_avg:34.09ms
step:477/1900 train_time:16260ms step_avg:34.09ms
step:478/1900 train_time:16294ms step_avg:34.09ms
step:479/1900 train_time:16328ms step_avg:34.09ms
step:480/1900 train_time:16362ms step_avg:34.09ms
step:481/1900 train_time:16396ms step_avg:34.09ms
step:482/1900 train_time:16430ms step_avg:34.09ms
step:483/1900 train_time:16464ms step_avg:34.09ms
step:484/1900 train_time:16498ms step_avg:34.09ms
step:485/1900 train_time:16532ms step_avg:34.09ms
step:486/1900 train_time:16565ms step_avg:34.09ms
step:487/1900 train_time:16599ms step_avg:34.08ms
step:488/1900 train_time:16633ms step_avg:34.08ms
step:489/1900 train_time:16667ms step_avg:34.08ms
step:490/1900 train_time:16701ms step_avg:34.08ms
step:491/1900 train_time:16735ms step_avg:34.08ms
step:492/1900 train_time:16769ms step_avg:34.08ms
step:493/1900 train_time:16803ms step_avg:34.08ms
step:494/1900 train_time:16837ms step_avg:34.08ms
step:495/1900 train_time:16871ms step_avg:34.08ms
step:496/1900 train_time:16905ms step_avg:34.08ms
step:497/1900 train_time:16938ms step_avg:34.08ms
step:498/1900 train_time:16973ms step_avg:34.08ms
step:499/1900 train_time:17006ms step_avg:34.08ms
step:500/1900 train_time:17040ms step_avg:34.08ms
step:500/1900 val_loss:4.2772 train_time:17077ms step_avg:34.15ms
step:501/1900 train_time:17097ms step_avg:34.13ms
step:502/1900 train_time:17117ms step_avg:34.10ms
step:503/1900 train_time:17144ms step_avg:34.08ms
step:504/1900 train_time:17179ms step_avg:34.08ms
step:505/1900 train_time:17214ms step_avg:34.09ms
step:506/1900 train_time:17248ms step_avg:34.09ms
step:507/1900 train_time:17283ms step_avg:34.09ms
step:508/1900 train_time:17316ms step_avg:34.09ms
step:509/1900 train_time:17350ms step_avg:34.09ms
step:510/1900 train_time:17384ms step_avg:34.09ms
step:511/1900 train_time:17418ms step_avg:34.09ms
step:512/1900 train_time:17452ms step_avg:34.09ms
step:513/1900 train_time:17486ms step_avg:34.08ms
step:514/1900 train_time:17519ms step_avg:34.08ms
step:515/1900 train_time:17553ms step_avg:34.08ms
step:516/1900 train_time:17587ms step_avg:34.08ms
step:517/1900 train_time:17621ms step_avg:34.08ms
step:518/1900 train_time:17655ms step_avg:34.08ms
step:519/1900 train_time:17688ms step_avg:34.08ms
step:520/1900 train_time:17722ms step_avg:34.08ms
step:521/1900 train_time:17756ms step_avg:34.08ms
step:522/1900 train_time:17790ms step_avg:34.08ms
step:523/1900 train_time:17824ms step_avg:34.08ms
step:524/1900 train_time:17858ms step_avg:34.08ms
step:525/1900 train_time:17892ms step_avg:34.08ms
step:526/1900 train_time:17925ms step_avg:34.08ms
step:527/1900 train_time:17959ms step_avg:34.08ms
step:528/1900 train_time:17993ms step_avg:34.08ms
step:529/1900 train_time:18027ms step_avg:34.08ms
step:530/1900 train_time:18061ms step_avg:34.08ms
step:531/1900 train_time:18095ms step_avg:34.08ms
step:532/1900 train_time:18129ms step_avg:34.08ms
step:533/1900 train_time:18164ms step_avg:34.08ms
step:534/1900 train_time:18197ms step_avg:34.08ms
step:535/1900 train_time:18231ms step_avg:34.08ms
step:536/1900 train_time:18265ms step_avg:34.08ms
step:537/1900 train_time:18300ms step_avg:34.08ms
step:538/1900 train_time:18334ms step_avg:34.08ms
step:539/1900 train_time:18368ms step_avg:34.08ms
step:540/1900 train_time:18402ms step_avg:34.08ms
step:541/1900 train_time:18436ms step_avg:34.08ms
step:542/1900 train_time:18470ms step_avg:34.08ms
step:543/1900 train_time:18504ms step_avg:34.08ms
step:544/1900 train_time:18538ms step_avg:34.08ms
step:545/1900 train_time:18572ms step_avg:34.08ms
step:546/1900 train_time:18606ms step_avg:34.08ms
step:547/1900 train_time:18640ms step_avg:34.08ms
step:548/1900 train_time:18674ms step_avg:34.08ms
step:549/1900 train_time:18708ms step_avg:34.08ms
step:550/1900 train_time:18742ms step_avg:34.08ms
step:551/1900 train_time:18776ms step_avg:34.08ms
step:552/1900 train_time:18810ms step_avg:34.08ms
step:553/1900 train_time:18844ms step_avg:34.08ms
step:554/1900 train_time:18877ms step_avg:34.07ms
step:555/1900 train_time:18911ms step_avg:34.07ms
step:556/1900 train_time:18945ms step_avg:34.07ms
step:557/1900 train_time:18979ms step_avg:34.07ms
step:558/1900 train_time:19013ms step_avg:34.07ms
step:559/1900 train_time:19047ms step_avg:34.07ms
step:560/1900 train_time:19080ms step_avg:34.07ms
step:561/1900 train_time:19115ms step_avg:34.07ms
step:562/1900 train_time:19148ms step_avg:34.07ms
step:563/1900 train_time:19182ms step_avg:34.07ms
step:564/1900 train_time:19216ms step_avg:34.07ms
step:565/1900 train_time:19250ms step_avg:34.07ms
step:566/1900 train_time:19284ms step_avg:34.07ms
step:567/1900 train_time:19318ms step_avg:34.07ms
step:568/1900 train_time:19352ms step_avg:34.07ms
step:569/1900 train_time:19386ms step_avg:34.07ms
step:570/1900 train_time:19420ms step_avg:34.07ms
step:571/1900 train_time:19454ms step_avg:34.07ms
step:572/1900 train_time:19488ms step_avg:34.07ms
step:573/1900 train_time:19522ms step_avg:34.07ms
step:574/1900 train_time:19556ms step_avg:34.07ms
step:575/1900 train_time:19590ms step_avg:34.07ms
step:576/1900 train_time:19624ms step_avg:34.07ms
step:577/1900 train_time:19658ms step_avg:34.07ms
step:578/1900 train_time:19692ms step_avg:34.07ms
step:579/1900 train_time:19725ms step_avg:34.07ms
step:580/1900 train_time:19760ms step_avg:34.07ms
step:581/1900 train_time:19794ms step_avg:34.07ms
step:582/1900 train_time:19828ms step_avg:34.07ms
step:583/1900 train_time:19862ms step_avg:34.07ms
step:584/1900 train_time:19895ms step_avg:34.07ms
step:585/1900 train_time:19929ms step_avg:34.07ms
step:586/1900 train_time:19963ms step_avg:34.07ms
step:587/1900 train_time:19998ms step_avg:34.07ms
step:588/1900 train_time:20031ms step_avg:34.07ms
step:589/1900 train_time:20065ms step_avg:34.07ms
step:590/1900 train_time:20099ms step_avg:34.07ms
step:591/1900 train_time:20133ms step_avg:34.07ms
step:592/1900 train_time:20167ms step_avg:34.07ms
step:593/1900 train_time:20201ms step_avg:34.07ms
step:594/1900 train_time:20235ms step_avg:34.06ms
step:595/1900 train_time:20268ms step_avg:34.06ms
step:596/1900 train_time:20302ms step_avg:34.06ms
step:597/1900 train_time:20336ms step_avg:34.06ms
step:598/1900 train_time:20370ms step_avg:34.06ms
step:599/1900 train_time:20404ms step_avg:34.06ms
step:600/1900 train_time:20438ms step_avg:34.06ms
step:601/1900 train_time:20472ms step_avg:34.06ms
step:602/1900 train_time:20506ms step_avg:34.06ms
step:603/1900 train_time:20540ms step_avg:34.06ms
step:604/1900 train_time:20574ms step_avg:34.06ms
step:605/1900 train_time:20608ms step_avg:34.06ms
step:606/1900 train_time:20641ms step_avg:34.06ms
step:607/1900 train_time:20676ms step_avg:34.06ms
step:608/1900 train_time:20709ms step_avg:34.06ms
step:609/1900 train_time:20743ms step_avg:34.06ms
step:610/1900 train_time:20777ms step_avg:34.06ms
step:611/1900 train_time:20811ms step_avg:34.06ms
step:612/1900 train_time:20845ms step_avg:34.06ms
step:613/1900 train_time:20880ms step_avg:34.06ms
step:614/1900 train_time:20914ms step_avg:34.06ms
step:615/1900 train_time:20947ms step_avg:34.06ms
step:616/1900 train_time:20981ms step_avg:34.06ms
step:617/1900 train_time:21015ms step_avg:34.06ms
step:618/1900 train_time:21049ms step_avg:34.06ms
step:619/1900 train_time:21083ms step_avg:34.06ms
step:620/1900 train_time:21117ms step_avg:34.06ms
step:621/1900 train_time:21152ms step_avg:34.06ms
step:622/1900 train_time:21212ms step_avg:34.10ms
step:623/1900 train_time:21274ms step_avg:34.15ms
step:624/1900 train_time:21335ms step_avg:34.19ms
step:625/1900 train_time:21397ms step_avg:34.23ms
step:626/1900 train_time:21458ms step_avg:34.28ms
step:627/1900 train_time:21520ms step_avg:34.32ms
step:628/1900 train_time:21581ms step_avg:34.36ms
step:629/1900 train_time:21643ms step_avg:34.41ms
step:630/1900 train_time:21704ms step_avg:34.45ms
step:631/1900 train_time:21766ms step_avg:34.49ms
step:632/1900 train_time:21827ms step_avg:34.54ms
step:633/1900 train_time:21888ms step_avg:34.58ms
step:634/1900 train_time:21949ms step_avg:34.62ms
step:635/1900 train_time:22011ms step_avg:34.66ms
step:636/1900 train_time:22073ms step_avg:34.71ms
step:637/1900 train_time:22134ms step_avg:34.75ms
step:638/1900 train_time:22195ms step_avg:34.79ms
step:639/1900 train_time:22256ms step_avg:34.83ms
step:640/1900 train_time:22317ms step_avg:34.87ms
step:641/1900 train_time:22379ms step_avg:34.91ms
step:642/1900 train_time:22440ms step_avg:34.95ms
step:643/1900 train_time:22502ms step_avg:35.00ms
step:644/1900 train_time:22564ms step_avg:35.04ms
step:645/1900 train_time:22626ms step_avg:35.08ms
step:646/1900 train_time:22687ms step_avg:35.12ms
step:647/1900 train_time:22749ms step_avg:35.16ms
step:648/1900 train_time:22810ms step_avg:35.20ms
step:649/1900 train_time:22872ms step_avg:35.24ms
step:650/1900 train_time:22933ms step_avg:35.28ms
step:651/1900 train_time:22995ms step_avg:35.32ms
step:652/1900 train_time:23056ms step_avg:35.36ms
step:653/1900 train_time:23117ms step_avg:35.40ms
step:654/1900 train_time:23179ms step_avg:35.44ms
step:655/1900 train_time:23240ms step_avg:35.48ms
step:656/1900 train_time:23301ms step_avg:35.52ms
step:657/1900 train_time:23363ms step_avg:35.56ms
step:658/1900 train_time:23423ms step_avg:35.60ms
step:659/1900 train_time:23485ms step_avg:35.64ms
step:660/1900 train_time:23546ms step_avg:35.68ms
step:661/1900 train_time:23607ms step_avg:35.71ms
step:662/1900 train_time:23668ms step_avg:35.75ms
step:663/1900 train_time:23730ms step_avg:35.79ms
step:664/1900 train_time:23791ms step_avg:35.83ms
step:665/1900 train_time:23853ms step_avg:35.87ms
step:666/1900 train_time:23914ms step_avg:35.91ms
step:667/1900 train_time:23975ms step_avg:35.95ms
step:668/1900 train_time:24036ms step_avg:35.98ms
step:669/1900 train_time:24098ms step_avg:36.02ms
step:670/1900 train_time:24159ms step_avg:36.06ms
step:671/1900 train_time:24221ms step_avg:36.10ms
step:672/1900 train_time:24281ms step_avg:36.13ms
step:673/1900 train_time:24343ms step_avg:36.17ms
step:674/1900 train_time:24404ms step_avg:36.21ms
step:675/1900 train_time:24465ms step_avg:36.25ms
step:676/1900 train_time:24527ms step_avg:36.28ms
step:677/1900 train_time:24588ms step_avg:36.32ms
step:678/1900 train_time:24649ms step_avg:36.36ms
step:679/1900 train_time:24711ms step_avg:36.39ms
step:680/1900 train_time:24772ms step_avg:36.43ms
step:681/1900 train_time:24834ms step_avg:36.47ms
step:682/1900 train_time:24895ms step_avg:36.50ms
step:683/1900 train_time:24957ms step_avg:36.54ms
step:684/1900 train_time:25018ms step_avg:36.58ms
step:685/1900 train_time:25079ms step_avg:36.61ms
step:686/1900 train_time:25140ms step_avg:36.65ms
step:687/1900 train_time:25202ms step_avg:36.68ms
step:688/1900 train_time:25263ms step_avg:36.72ms
step:689/1900 train_time:25325ms step_avg:36.76ms
step:690/1900 train_time:25385ms step_avg:36.79ms
step:691/1900 train_time:25447ms step_avg:36.83ms
step:692/1900 train_time:25508ms step_avg:36.86ms
step:693/1900 train_time:25570ms step_avg:36.90ms
step:694/1900 train_time:25630ms step_avg:36.93ms
step:695/1900 train_time:25692ms step_avg:36.97ms
step:696/1900 train_time:25753ms step_avg:37.00ms
step:697/1900 train_time:25815ms step_avg:37.04ms
step:698/1900 train_time:25876ms step_avg:37.07ms
step:699/1900 train_time:25938ms step_avg:37.11ms
step:700/1900 train_time:25998ms step_avg:37.14ms
step:701/1900 train_time:26061ms step_avg:37.18ms
step:702/1900 train_time:26121ms step_avg:37.21ms
step:703/1900 train_time:26183ms step_avg:37.24ms
step:704/1900 train_time:26244ms step_avg:37.28ms
step:705/1900 train_time:26306ms step_avg:37.31ms
step:706/1900 train_time:26366ms step_avg:37.35ms
step:707/1900 train_time:26428ms step_avg:37.38ms
step:708/1900 train_time:26489ms step_avg:37.41ms
step:709/1900 train_time:26550ms step_avg:37.45ms
step:710/1900 train_time:26612ms step_avg:37.48ms
step:711/1900 train_time:26673ms step_avg:37.52ms
step:712/1900 train_time:26734ms step_avg:37.55ms
step:713/1900 train_time:26796ms step_avg:37.58ms
step:714/1900 train_time:26857ms step_avg:37.61ms
step:715/1900 train_time:26919ms step_avg:37.65ms
step:716/1900 train_time:26979ms step_avg:37.68ms
step:717/1900 train_time:27041ms step_avg:37.71ms
step:718/1900 train_time:27102ms step_avg:37.75ms
step:719/1900 train_time:27164ms step_avg:37.78ms
step:720/1900 train_time:27225ms step_avg:37.81ms
step:721/1900 train_time:27287ms step_avg:37.85ms
step:722/1900 train_time:27348ms step_avg:37.88ms
step:723/1900 train_time:27410ms step_avg:37.91ms
step:724/1900 train_time:27471ms step_avg:37.94ms
step:725/1900 train_time:27532ms step_avg:37.98ms
step:726/1900 train_time:27593ms step_avg:38.01ms
step:727/1900 train_time:27655ms step_avg:38.04ms
step:728/1900 train_time:27716ms step_avg:38.07ms
step:729/1900 train_time:27778ms step_avg:38.10ms
step:730/1900 train_time:27839ms step_avg:38.14ms
step:731/1900 train_time:27901ms step_avg:38.17ms
step:732/1900 train_time:27962ms step_avg:38.20ms
step:733/1900 train_time:28024ms step_avg:38.23ms
step:734/1900 train_time:28085ms step_avg:38.26ms
step:735/1900 train_time:28146ms step_avg:38.29ms
step:736/1900 train_time:28207ms step_avg:38.32ms
step:737/1900 train_time:28269ms step_avg:38.36ms
step:738/1900 train_time:28330ms step_avg:38.39ms
step:739/1900 train_time:28392ms step_avg:38.42ms
step:740/1900 train_time:28453ms step_avg:38.45ms
step:741/1900 train_time:28514ms step_avg:38.48ms
step:742/1900 train_time:28575ms step_avg:38.51ms
step:743/1900 train_time:28637ms step_avg:38.54ms
step:744/1900 train_time:28698ms step_avg:38.57ms
step:745/1900 train_time:28760ms step_avg:38.60ms
step:746/1900 train_time:28821ms step_avg:38.63ms
step:747/1900 train_time:28884ms step_avg:38.67ms
step:748/1900 train_time:28944ms step_avg:38.70ms
step:749/1900 train_time:29007ms step_avg:38.73ms
step:750/1900 train_time:29068ms step_avg:38.76ms
step:750/1900 val_loss:4.0232 train_time:29132ms step_avg:38.84ms
step:751/1900 train_time:29151ms step_avg:38.82ms
step:752/1900 train_time:29192ms step_avg:38.82ms
step:753/1900 train_time:29256ms step_avg:38.85ms
step:754/1900 train_time:29319ms step_avg:38.88ms
step:755/1900 train_time:29381ms step_avg:38.91ms
step:756/1900 train_time:29442ms step_avg:38.94ms
step:757/1900 train_time:29503ms step_avg:38.97ms
step:758/1900 train_time:29563ms step_avg:39.00ms
step:759/1900 train_time:29625ms step_avg:39.03ms
step:760/1900 train_time:29685ms step_avg:39.06ms
step:761/1900 train_time:29747ms step_avg:39.09ms
step:762/1900 train_time:29807ms step_avg:39.12ms
step:763/1900 train_time:29868ms step_avg:39.15ms
step:764/1900 train_time:29929ms step_avg:39.17ms
step:765/1900 train_time:29990ms step_avg:39.20ms
step:766/1900 train_time:30051ms step_avg:39.23ms
step:767/1900 train_time:30114ms step_avg:39.26ms
step:768/1900 train_time:30175ms step_avg:39.29ms
step:769/1900 train_time:30238ms step_avg:39.32ms
step:770/1900 train_time:30299ms step_avg:39.35ms
step:771/1900 train_time:30361ms step_avg:39.38ms
step:772/1900 train_time:30423ms step_avg:39.41ms
step:773/1900 train_time:30485ms step_avg:39.44ms
step:774/1900 train_time:30546ms step_avg:39.46ms
step:775/1900 train_time:30608ms step_avg:39.49ms
step:776/1900 train_time:30668ms step_avg:39.52ms
step:777/1900 train_time:30729ms step_avg:39.55ms
step:778/1900 train_time:30789ms step_avg:39.58ms
step:779/1900 train_time:30851ms step_avg:39.60ms
step:780/1900 train_time:30911ms step_avg:39.63ms
step:781/1900 train_time:30972ms step_avg:39.66ms
step:782/1900 train_time:31033ms step_avg:39.68ms
step:783/1900 train_time:31095ms step_avg:39.71ms
step:784/1900 train_time:31156ms step_avg:39.74ms
step:785/1900 train_time:31219ms step_avg:39.77ms
step:786/1900 train_time:31280ms step_avg:39.80ms
step:787/1900 train_time:31343ms step_avg:39.83ms
step:788/1900 train_time:31404ms step_avg:39.85ms
step:789/1900 train_time:31466ms step_avg:39.88ms
step:790/1900 train_time:31527ms step_avg:39.91ms
step:791/1900 train_time:31589ms step_avg:39.94ms
step:792/1900 train_time:31650ms step_avg:39.96ms
step:793/1900 train_time:31712ms step_avg:39.99ms
step:794/1900 train_time:31772ms step_avg:40.02ms
step:795/1900 train_time:31834ms step_avg:40.04ms
step:796/1900 train_time:31894ms step_avg:40.07ms
step:797/1900 train_time:31956ms step_avg:40.09ms
step:798/1900 train_time:32017ms step_avg:40.12ms
step:799/1900 train_time:32078ms step_avg:40.15ms
step:800/1900 train_time:32140ms step_avg:40.17ms
step:801/1900 train_time:32202ms step_avg:40.20ms
step:802/1900 train_time:32263ms step_avg:40.23ms
step:803/1900 train_time:32325ms step_avg:40.26ms
step:804/1900 train_time:32386ms step_avg:40.28ms
step:805/1900 train_time:32447ms step_avg:40.31ms
step:806/1900 train_time:32508ms step_avg:40.33ms
step:807/1900 train_time:32569ms step_avg:40.36ms
step:808/1900 train_time:32630ms step_avg:40.38ms
step:809/1900 train_time:32692ms step_avg:40.41ms
step:810/1900 train_time:32752ms step_avg:40.43ms
step:811/1900 train_time:32814ms step_avg:40.46ms
step:812/1900 train_time:32874ms step_avg:40.49ms
step:813/1900 train_time:32936ms step_avg:40.51ms
step:814/1900 train_time:32997ms step_avg:40.54ms
step:815/1900 train_time:33058ms step_avg:40.56ms
step:816/1900 train_time:33119ms step_avg:40.59ms
step:817/1900 train_time:33181ms step_avg:40.61ms
step:818/1900 train_time:33242ms step_avg:40.64ms
step:819/1900 train_time:33305ms step_avg:40.67ms
step:820/1900 train_time:33366ms step_avg:40.69ms
step:821/1900 train_time:33428ms step_avg:40.72ms
step:822/1900 train_time:33488ms step_avg:40.74ms
step:823/1900 train_time:33550ms step_avg:40.77ms
step:824/1900 train_time:33611ms step_avg:40.79ms
step:825/1900 train_time:33673ms step_avg:40.82ms
step:826/1900 train_time:33733ms step_avg:40.84ms
step:827/1900 train_time:33795ms step_avg:40.86ms
step:828/1900 train_time:33855ms step_avg:40.89ms
step:829/1900 train_time:33917ms step_avg:40.91ms
step:830/1900 train_time:33978ms step_avg:40.94ms
step:831/1900 train_time:34040ms step_avg:40.96ms
step:832/1900 train_time:34100ms step_avg:40.99ms
step:833/1900 train_time:34162ms step_avg:41.01ms
step:834/1900 train_time:34223ms step_avg:41.04ms
step:835/1900 train_time:34286ms step_avg:41.06ms
step:836/1900 train_time:34347ms step_avg:41.08ms
step:837/1900 train_time:34408ms step_avg:41.11ms
step:838/1900 train_time:34469ms step_avg:41.13ms
step:839/1900 train_time:34532ms step_avg:41.16ms
step:840/1900 train_time:34593ms step_avg:41.18ms
step:841/1900 train_time:34655ms step_avg:41.21ms
step:842/1900 train_time:34716ms step_avg:41.23ms
step:843/1900 train_time:34777ms step_avg:41.25ms
step:844/1900 train_time:34838ms step_avg:41.28ms
step:845/1900 train_time:34900ms step_avg:41.30ms
step:846/1900 train_time:34961ms step_avg:41.33ms
step:847/1900 train_time:35023ms step_avg:41.35ms
step:848/1900 train_time:35083ms step_avg:41.37ms
step:849/1900 train_time:35145ms step_avg:41.40ms
step:850/1900 train_time:35206ms step_avg:41.42ms
step:851/1900 train_time:35267ms step_avg:41.44ms
step:852/1900 train_time:35329ms step_avg:41.47ms
step:853/1900 train_time:35391ms step_avg:41.49ms
step:854/1900 train_time:35452ms step_avg:41.51ms
step:855/1900 train_time:35515ms step_avg:41.54ms
step:856/1900 train_time:35575ms step_avg:41.56ms
step:857/1900 train_time:35637ms step_avg:41.58ms
step:858/1900 train_time:35698ms step_avg:41.61ms
step:859/1900 train_time:35760ms step_avg:41.63ms
step:860/1900 train_time:35820ms step_avg:41.65ms
step:861/1900 train_time:35882ms step_avg:41.68ms
step:862/1900 train_time:35943ms step_avg:41.70ms
step:863/1900 train_time:36006ms step_avg:41.72ms
step:864/1900 train_time:36066ms step_avg:41.74ms
step:865/1900 train_time:36129ms step_avg:41.77ms
step:866/1900 train_time:36189ms step_avg:41.79ms
step:867/1900 train_time:36250ms step_avg:41.81ms
step:868/1900 train_time:36312ms step_avg:41.83ms
step:869/1900 train_time:36373ms step_avg:41.86ms
step:870/1900 train_time:36434ms step_avg:41.88ms
step:871/1900 train_time:36496ms step_avg:41.90ms
step:872/1900 train_time:36557ms step_avg:41.92ms
step:873/1900 train_time:36619ms step_avg:41.95ms
step:874/1900 train_time:36680ms step_avg:41.97ms
step:875/1900 train_time:36742ms step_avg:41.99ms
step:876/1900 train_time:36803ms step_avg:42.01ms
step:877/1900 train_time:36865ms step_avg:42.04ms
step:878/1900 train_time:36926ms step_avg:42.06ms
step:879/1900 train_time:36987ms step_avg:42.08ms
step:880/1900 train_time:37048ms step_avg:42.10ms
step:881/1900 train_time:37110ms step_avg:42.12ms
step:882/1900 train_time:37170ms step_avg:42.14ms
step:883/1900 train_time:37232ms step_avg:42.17ms
step:884/1900 train_time:37293ms step_avg:42.19ms
step:885/1900 train_time:37356ms step_avg:42.21ms
step:886/1900 train_time:37416ms step_avg:42.23ms
step:887/1900 train_time:37478ms step_avg:42.25ms
step:888/1900 train_time:37539ms step_avg:42.27ms
step:889/1900 train_time:37601ms step_avg:42.30ms
step:890/1900 train_time:37662ms step_avg:42.32ms
step:891/1900 train_time:37724ms step_avg:42.34ms
step:892/1900 train_time:37785ms step_avg:42.36ms
step:893/1900 train_time:37846ms step_avg:42.38ms
step:894/1900 train_time:37907ms step_avg:42.40ms
step:895/1900 train_time:37969ms step_avg:42.42ms
step:896/1900 train_time:38030ms step_avg:42.44ms
step:897/1900 train_time:38091ms step_avg:42.47ms
step:898/1900 train_time:38152ms step_avg:42.49ms
step:899/1900 train_time:38214ms step_avg:42.51ms
step:900/1900 train_time:38275ms step_avg:42.53ms
step:901/1900 train_time:38337ms step_avg:42.55ms
step:902/1900 train_time:38398ms step_avg:42.57ms
step:903/1900 train_time:38460ms step_avg:42.59ms
step:904/1900 train_time:38521ms step_avg:42.61ms
step:905/1900 train_time:38583ms step_avg:42.63ms
step:906/1900 train_time:38644ms step_avg:42.65ms
step:907/1900 train_time:38705ms step_avg:42.67ms
step:908/1900 train_time:38766ms step_avg:42.69ms
step:909/1900 train_time:38828ms step_avg:42.72ms
step:910/1900 train_time:38889ms step_avg:42.73ms
step:911/1900 train_time:38951ms step_avg:42.76ms
step:912/1900 train_time:39011ms step_avg:42.78ms
step:913/1900 train_time:39073ms step_avg:42.80ms
step:914/1900 train_time:39133ms step_avg:42.82ms
step:915/1900 train_time:39196ms step_avg:42.84ms
step:916/1900 train_time:39256ms step_avg:42.86ms
step:917/1900 train_time:39318ms step_avg:42.88ms
step:918/1900 train_time:39379ms step_avg:42.90ms
step:919/1900 train_time:39441ms step_avg:42.92ms
step:920/1900 train_time:39502ms step_avg:42.94ms
step:921/1900 train_time:39564ms step_avg:42.96ms
step:922/1900 train_time:39624ms step_avg:42.98ms
step:923/1900 train_time:39686ms step_avg:43.00ms
step:924/1900 train_time:39747ms step_avg:43.02ms
step:925/1900 train_time:39809ms step_avg:43.04ms
step:926/1900 train_time:39870ms step_avg:43.06ms
step:927/1900 train_time:39932ms step_avg:43.08ms
step:928/1900 train_time:39993ms step_avg:43.10ms
step:929/1900 train_time:40055ms step_avg:43.12ms
step:930/1900 train_time:40116ms step_avg:43.14ms
step:931/1900 train_time:40178ms step_avg:43.16ms
step:932/1900 train_time:40239ms step_avg:43.18ms
step:933/1900 train_time:40301ms step_avg:43.20ms
step:934/1900 train_time:40362ms step_avg:43.21ms
step:935/1900 train_time:40424ms step_avg:43.23ms
step:936/1900 train_time:40485ms step_avg:43.25ms
step:937/1900 train_time:40546ms step_avg:43.27ms
step:938/1900 train_time:40607ms step_avg:43.29ms
step:939/1900 train_time:40669ms step_avg:43.31ms
step:940/1900 train_time:40730ms step_avg:43.33ms
step:941/1900 train_time:40792ms step_avg:43.35ms
step:942/1900 train_time:40853ms step_avg:43.37ms
step:943/1900 train_time:40915ms step_avg:43.39ms
step:944/1900 train_time:40976ms step_avg:43.41ms
step:945/1900 train_time:41038ms step_avg:43.43ms
step:946/1900 train_time:41099ms step_avg:43.45ms
step:947/1900 train_time:41162ms step_avg:43.47ms
step:948/1900 train_time:41223ms step_avg:43.48ms
step:949/1900 train_time:41285ms step_avg:43.50ms
step:950/1900 train_time:41346ms step_avg:43.52ms
step:951/1900 train_time:41407ms step_avg:43.54ms
step:952/1900 train_time:41468ms step_avg:43.56ms
step:953/1900 train_time:41530ms step_avg:43.58ms
step:954/1900 train_time:41591ms step_avg:43.60ms
step:955/1900 train_time:41652ms step_avg:43.61ms
step:956/1900 train_time:41713ms step_avg:43.63ms
step:957/1900 train_time:41775ms step_avg:43.65ms
step:958/1900 train_time:41835ms step_avg:43.67ms
step:959/1900 train_time:41897ms step_avg:43.69ms
step:960/1900 train_time:41958ms step_avg:43.71ms
step:961/1900 train_time:42020ms step_avg:43.73ms
step:962/1900 train_time:42081ms step_avg:43.74ms
step:963/1900 train_time:42143ms step_avg:43.76ms
step:964/1900 train_time:42204ms step_avg:43.78ms
step:965/1900 train_time:42266ms step_avg:43.80ms
step:966/1900 train_time:42327ms step_avg:43.82ms
step:967/1900 train_time:42389ms step_avg:43.84ms
step:968/1900 train_time:42450ms step_avg:43.85ms
step:969/1900 train_time:42512ms step_avg:43.87ms
step:970/1900 train_time:42572ms step_avg:43.89ms
step:971/1900 train_time:42634ms step_avg:43.91ms
step:972/1900 train_time:42695ms step_avg:43.92ms
step:973/1900 train_time:42757ms step_avg:43.94ms
step:974/1900 train_time:42818ms step_avg:43.96ms
step:975/1900 train_time:42880ms step_avg:43.98ms
step:976/1900 train_time:42941ms step_avg:44.00ms
step:977/1900 train_time:43003ms step_avg:44.02ms
step:978/1900 train_time:43064ms step_avg:44.03ms
step:979/1900 train_time:43126ms step_avg:44.05ms
step:980/1900 train_time:43187ms step_avg:44.07ms
step:981/1900 train_time:43249ms step_avg:44.09ms
step:982/1900 train_time:43309ms step_avg:44.10ms
step:983/1900 train_time:43371ms step_avg:44.12ms
step:984/1900 train_time:43432ms step_avg:44.14ms
step:985/1900 train_time:43494ms step_avg:44.16ms
step:986/1900 train_time:43555ms step_avg:44.17ms
step:987/1900 train_time:43617ms step_avg:44.19ms
step:988/1900 train_time:43678ms step_avg:44.21ms
step:989/1900 train_time:43739ms step_avg:44.23ms
step:990/1900 train_time:43800ms step_avg:44.24ms
step:991/1900 train_time:43862ms step_avg:44.26ms
step:992/1900 train_time:43924ms step_avg:44.28ms
step:993/1900 train_time:43986ms step_avg:44.30ms
step:994/1900 train_time:44047ms step_avg:44.31ms
step:995/1900 train_time:44109ms step_avg:44.33ms
step:996/1900 train_time:44169ms step_avg:44.35ms
step:997/1900 train_time:44231ms step_avg:44.36ms
step:998/1900 train_time:44291ms step_avg:44.38ms
step:999/1900 train_time:44353ms step_avg:44.40ms
step:1000/1900 train_time:44414ms step_avg:44.41ms
step:1000/1900 val_loss:3.7842 train_time:44479ms step_avg:44.48ms
step:1001/1900 train_time:44499ms step_avg:44.45ms
step:1002/1900 train_time:44540ms step_avg:44.45ms
step:1003/1900 train_time:44604ms step_avg:44.47ms
step:1004/1900 train_time:44668ms step_avg:44.49ms
step:1005/1900 train_time:44730ms step_avg:44.51ms
step:1006/1900 train_time:44791ms step_avg:44.52ms
step:1007/1900 train_time:44853ms step_avg:44.54ms
step:1008/1900 train_time:44913ms step_avg:44.56ms
step:1009/1900 train_time:44975ms step_avg:44.57ms
step:1010/1900 train_time:45035ms step_avg:44.59ms
step:1011/1900 train_time:45096ms step_avg:44.61ms
step:1012/1900 train_time:45157ms step_avg:44.62ms
step:1013/1900 train_time:45219ms step_avg:44.64ms
step:1014/1900 train_time:45279ms step_avg:44.65ms
step:1015/1900 train_time:45340ms step_avg:44.67ms
step:1016/1900 train_time:45401ms step_avg:44.69ms
step:1017/1900 train_time:45464ms step_avg:44.70ms
step:1018/1900 train_time:45526ms step_avg:44.72ms
step:1019/1900 train_time:45589ms step_avg:44.74ms
step:1020/1900 train_time:45651ms step_avg:44.76ms
step:1021/1900 train_time:45713ms step_avg:44.77ms
step:1022/1900 train_time:45774ms step_avg:44.79ms
step:1023/1900 train_time:45835ms step_avg:44.80ms
step:1024/1900 train_time:45896ms step_avg:44.82ms
step:1025/1900 train_time:45958ms step_avg:44.84ms
step:1026/1900 train_time:46019ms step_avg:44.85ms
step:1027/1900 train_time:46080ms step_avg:44.87ms
step:1028/1900 train_time:46141ms step_avg:44.88ms
step:1029/1900 train_time:46203ms step_avg:44.90ms
step:1030/1900 train_time:46263ms step_avg:44.92ms
step:1031/1900 train_time:46325ms step_avg:44.93ms
step:1032/1900 train_time:46386ms step_avg:44.95ms
step:1033/1900 train_time:46448ms step_avg:44.96ms
step:1034/1900 train_time:46509ms step_avg:44.98ms
step:1035/1900 train_time:46572ms step_avg:45.00ms
step:1036/1900 train_time:46633ms step_avg:45.01ms
step:1037/1900 train_time:46696ms step_avg:45.03ms
step:1038/1900 train_time:46758ms step_avg:45.05ms
step:1039/1900 train_time:46820ms step_avg:45.06ms
step:1040/1900 train_time:46881ms step_avg:45.08ms
step:1041/1900 train_time:46943ms step_avg:45.09ms
step:1042/1900 train_time:47003ms step_avg:45.11ms
step:1043/1900 train_time:47065ms step_avg:45.12ms
step:1044/1900 train_time:47126ms step_avg:45.14ms
step:1045/1900 train_time:47187ms step_avg:45.15ms
step:1046/1900 train_time:47248ms step_avg:45.17ms
step:1047/1900 train_time:47308ms step_avg:45.18ms
step:1048/1900 train_time:47369ms step_avg:45.20ms
step:1049/1900 train_time:47431ms step_avg:45.22ms
step:1050/1900 train_time:47492ms step_avg:45.23ms
step:1051/1900 train_time:47554ms step_avg:45.25ms
step:1052/1900 train_time:47615ms step_avg:45.26ms
step:1053/1900 train_time:47678ms step_avg:45.28ms
step:1054/1900 train_time:47739ms step_avg:45.29ms
step:1055/1900 train_time:47801ms step_avg:45.31ms
step:1056/1900 train_time:47862ms step_avg:45.32ms
step:1057/1900 train_time:47924ms step_avg:45.34ms
step:1058/1900 train_time:47986ms step_avg:45.36ms
step:1059/1900 train_time:48047ms step_avg:45.37ms
step:1060/1900 train_time:48108ms step_avg:45.38ms
step:1061/1900 train_time:48170ms step_avg:45.40ms
step:1062/1900 train_time:48231ms step_avg:45.41ms
step:1063/1900 train_time:48292ms step_avg:45.43ms
step:1064/1900 train_time:48352ms step_avg:45.44ms
step:1065/1900 train_time:48414ms step_avg:45.46ms
step:1066/1900 train_time:48475ms step_avg:45.47ms
step:1067/1900 train_time:48537ms step_avg:45.49ms
step:1068/1900 train_time:48598ms step_avg:45.50ms
step:1069/1900 train_time:48661ms step_avg:45.52ms
step:1070/1900 train_time:48722ms step_avg:45.53ms
step:1071/1900 train_time:48784ms step_avg:45.55ms
step:1072/1900 train_time:48846ms step_avg:45.57ms
step:1073/1900 train_time:48908ms step_avg:45.58ms
step:1074/1900 train_time:48969ms step_avg:45.59ms
step:1075/1900 train_time:49030ms step_avg:45.61ms
step:1076/1900 train_time:49091ms step_avg:45.62ms
step:1077/1900 train_time:49152ms step_avg:45.64ms
step:1078/1900 train_time:49213ms step_avg:45.65ms
step:1079/1900 train_time:49274ms step_avg:45.67ms
step:1080/1900 train_time:49335ms step_avg:45.68ms
step:1081/1900 train_time:49396ms step_avg:45.69ms
step:1082/1900 train_time:49458ms step_avg:45.71ms
step:1083/1900 train_time:49520ms step_avg:45.72ms
step:1084/1900 train_time:49581ms step_avg:45.74ms
step:1085/1900 train_time:49643ms step_avg:45.75ms
step:1086/1900 train_time:49704ms step_avg:45.77ms
step:1087/1900 train_time:49766ms step_avg:45.78ms
step:1088/1900 train_time:49827ms step_avg:45.80ms
step:1089/1900 train_time:49889ms step_avg:45.81ms
step:1090/1900 train_time:49950ms step_avg:45.83ms
step:1091/1900 train_time:50011ms step_avg:45.84ms
step:1092/1900 train_time:50072ms step_avg:45.85ms
step:1093/1900 train_time:50133ms step_avg:45.87ms
step:1094/1900 train_time:50194ms step_avg:45.88ms
step:1095/1900 train_time:50256ms step_avg:45.90ms
step:1096/1900 train_time:50317ms step_avg:45.91ms
step:1097/1900 train_time:50378ms step_avg:45.92ms
step:1098/1900 train_time:50440ms step_avg:45.94ms
step:1099/1900 train_time:50502ms step_avg:45.95ms
step:1100/1900 train_time:50562ms step_avg:45.97ms
step:1101/1900 train_time:50625ms step_avg:45.98ms
step:1102/1900 train_time:50686ms step_avg:45.99ms
step:1103/1900 train_time:50748ms step_avg:46.01ms
step:1104/1900 train_time:50809ms step_avg:46.02ms
step:1105/1900 train_time:50871ms step_avg:46.04ms
step:1106/1900 train_time:50932ms step_avg:46.05ms
step:1107/1900 train_time:50994ms step_avg:46.07ms
step:1108/1900 train_time:51055ms step_avg:46.08ms
step:1109/1900 train_time:51116ms step_avg:46.09ms
step:1110/1900 train_time:51177ms step_avg:46.11ms
step:1111/1900 train_time:51239ms step_avg:46.12ms
step:1112/1900 train_time:51300ms step_avg:46.13ms
step:1113/1900 train_time:51362ms step_avg:46.15ms
step:1114/1900 train_time:51423ms step_avg:46.16ms
step:1115/1900 train_time:51484ms step_avg:46.17ms
step:1116/1900 train_time:51545ms step_avg:46.19ms
step:1117/1900 train_time:51607ms step_avg:46.20ms
step:1118/1900 train_time:51668ms step_avg:46.21ms
step:1119/1900 train_time:51730ms step_avg:46.23ms
step:1120/1900 train_time:51791ms step_avg:46.24ms
step:1121/1900 train_time:51852ms step_avg:46.26ms
step:1122/1900 train_time:51913ms step_avg:46.27ms
step:1123/1900 train_time:51976ms step_avg:46.28ms
step:1124/1900 train_time:52036ms step_avg:46.30ms
step:1125/1900 train_time:52098ms step_avg:46.31ms
step:1126/1900 train_time:52159ms step_avg:46.32ms
step:1127/1900 train_time:52221ms step_avg:46.34ms
step:1128/1900 train_time:52282ms step_avg:46.35ms
step:1129/1900 train_time:52344ms step_avg:46.36ms
step:1130/1900 train_time:52405ms step_avg:46.38ms
step:1131/1900 train_time:52467ms step_avg:46.39ms
step:1132/1900 train_time:52528ms step_avg:46.40ms
step:1133/1900 train_time:52590ms step_avg:46.42ms
step:1134/1900 train_time:52651ms step_avg:46.43ms
step:1135/1900 train_time:52713ms step_avg:46.44ms
step:1136/1900 train_time:52773ms step_avg:46.46ms
step:1137/1900 train_time:52835ms step_avg:46.47ms
step:1138/1900 train_time:52896ms step_avg:46.48ms
step:1139/1900 train_time:52958ms step_avg:46.50ms
step:1140/1900 train_time:53019ms step_avg:46.51ms
step:1141/1900 train_time:53081ms step_avg:46.52ms
step:1142/1900 train_time:53142ms step_avg:46.53ms
step:1143/1900 train_time:53204ms step_avg:46.55ms
step:1144/1900 train_time:53265ms step_avg:46.56ms
step:1145/1900 train_time:53327ms step_avg:46.57ms
step:1146/1900 train_time:53388ms step_avg:46.59ms
step:1147/1900 train_time:53450ms step_avg:46.60ms
step:1148/1900 train_time:53510ms step_avg:46.61ms
step:1149/1900 train_time:53572ms step_avg:46.62ms
step:1150/1900 train_time:53633ms step_avg:46.64ms
step:1151/1900 train_time:53694ms step_avg:46.65ms
step:1152/1900 train_time:53755ms step_avg:46.66ms
step:1153/1900 train_time:53817ms step_avg:46.68ms
step:1154/1900 train_time:53878ms step_avg:46.69ms
step:1155/1900 train_time:53940ms step_avg:46.70ms
step:1156/1900 train_time:54001ms step_avg:46.71ms
step:1157/1900 train_time:54063ms step_avg:46.73ms
step:1158/1900 train_time:54124ms step_avg:46.74ms
step:1159/1900 train_time:54186ms step_avg:46.75ms
step:1160/1900 train_time:54247ms step_avg:46.76ms
step:1161/1900 train_time:54308ms step_avg:46.78ms
step:1162/1900 train_time:54369ms step_avg:46.79ms
step:1163/1900 train_time:54431ms step_avg:46.80ms
step:1164/1900 train_time:54492ms step_avg:46.81ms
step:1165/1900 train_time:54554ms step_avg:46.83ms
step:1166/1900 train_time:54615ms step_avg:46.84ms
step:1167/1900 train_time:54677ms step_avg:46.85ms
step:1168/1900 train_time:54738ms step_avg:46.86ms
step:1169/1900 train_time:54800ms step_avg:46.88ms
step:1170/1900 train_time:54861ms step_avg:46.89ms
step:1171/1900 train_time:54923ms step_avg:46.90ms
step:1172/1900 train_time:54984ms step_avg:46.91ms
step:1173/1900 train_time:55046ms step_avg:46.93ms
step:1174/1900 train_time:55107ms step_avg:46.94ms
step:1175/1900 train_time:55169ms step_avg:46.95ms
step:1176/1900 train_time:55230ms step_avg:46.96ms
step:1177/1900 train_time:55291ms step_avg:46.98ms
step:1178/1900 train_time:55352ms step_avg:46.99ms
step:1179/1900 train_time:55414ms step_avg:47.00ms
step:1180/1900 train_time:55474ms step_avg:47.01ms
step:1181/1900 train_time:55536ms step_avg:47.02ms
step:1182/1900 train_time:55598ms step_avg:47.04ms
step:1183/1900 train_time:55660ms step_avg:47.05ms
step:1184/1900 train_time:55721ms step_avg:47.06ms
step:1185/1900 train_time:55783ms step_avg:47.07ms
step:1186/1900 train_time:55844ms step_avg:47.09ms
step:1187/1900 train_time:55906ms step_avg:47.10ms
step:1188/1900 train_time:55967ms step_avg:47.11ms
step:1189/1900 train_time:56029ms step_avg:47.12ms
step:1190/1900 train_time:56090ms step_avg:47.13ms
step:1191/1900 train_time:56151ms step_avg:47.15ms
step:1192/1900 train_time:56213ms step_avg:47.16ms
step:1193/1900 train_time:56274ms step_avg:47.17ms
step:1194/1900 train_time:56335ms step_avg:47.18ms
step:1195/1900 train_time:56397ms step_avg:47.19ms
step:1196/1900 train_time:56458ms step_avg:47.21ms
step:1197/1900 train_time:56520ms step_avg:47.22ms
step:1198/1900 train_time:56581ms step_avg:47.23ms
step:1199/1900 train_time:56643ms step_avg:47.24ms
step:1200/1900 train_time:56705ms step_avg:47.25ms
step:1201/1900 train_time:56767ms step_avg:47.27ms
step:1202/1900 train_time:56827ms step_avg:47.28ms
step:1203/1900 train_time:56889ms step_avg:47.29ms
step:1204/1900 train_time:56950ms step_avg:47.30ms
step:1205/1900 train_time:57012ms step_avg:47.31ms
step:1206/1900 train_time:57073ms step_avg:47.32ms
step:1207/1900 train_time:57134ms step_avg:47.34ms
step:1208/1900 train_time:57196ms step_avg:47.35ms
step:1209/1900 train_time:57258ms step_avg:47.36ms
step:1210/1900 train_time:57318ms step_avg:47.37ms
step:1211/1900 train_time:57380ms step_avg:47.38ms
step:1212/1900 train_time:57441ms step_avg:47.39ms
step:1213/1900 train_time:57503ms step_avg:47.41ms
step:1214/1900 train_time:57564ms step_avg:47.42ms
step:1215/1900 train_time:57626ms step_avg:47.43ms
step:1216/1900 train_time:57687ms step_avg:47.44ms
step:1217/1900 train_time:57749ms step_avg:47.45ms
step:1218/1900 train_time:57810ms step_avg:47.46ms
step:1219/1900 train_time:57871ms step_avg:47.47ms
step:1220/1900 train_time:57932ms step_avg:47.49ms
step:1221/1900 train_time:57994ms step_avg:47.50ms
step:1222/1900 train_time:58055ms step_avg:47.51ms
step:1223/1900 train_time:58117ms step_avg:47.52ms
step:1224/1900 train_time:58178ms step_avg:47.53ms
step:1225/1900 train_time:58240ms step_avg:47.54ms
step:1226/1900 train_time:58301ms step_avg:47.55ms
step:1227/1900 train_time:58363ms step_avg:47.57ms
step:1228/1900 train_time:58424ms step_avg:47.58ms
step:1229/1900 train_time:58486ms step_avg:47.59ms
step:1230/1900 train_time:58547ms step_avg:47.60ms
step:1231/1900 train_time:58608ms step_avg:47.61ms
step:1232/1900 train_time:58669ms step_avg:47.62ms
step:1233/1900 train_time:58731ms step_avg:47.63ms
step:1234/1900 train_time:58792ms step_avg:47.64ms
step:1235/1900 train_time:58854ms step_avg:47.65ms
step:1236/1900 train_time:58915ms step_avg:47.67ms
step:1237/1900 train_time:58976ms step_avg:47.68ms
step:1238/1900 train_time:59038ms step_avg:47.69ms
step:1239/1900 train_time:59100ms step_avg:47.70ms
step:1240/1900 train_time:59161ms step_avg:47.71ms
step:1241/1900 train_time:59223ms step_avg:47.72ms
step:1242/1900 train_time:59310ms step_avg:47.75ms
step:1243/1900 train_time:59399ms step_avg:47.79ms
step:1244/1900 train_time:59487ms step_avg:47.82ms
step:1245/1900 train_time:59576ms step_avg:47.85ms
step:1246/1900 train_time:59663ms step_avg:47.88ms
step:1247/1900 train_time:59751ms step_avg:47.92ms
step:1248/1900 train_time:59839ms step_avg:47.95ms
step:1249/1900 train_time:59927ms step_avg:47.98ms
step:1250/1900 train_time:60015ms step_avg:48.01ms
step:1250/1900 val_loss:3.5465 train_time:60105ms step_avg:48.08ms
step:1251/1900 train_time:60126ms step_avg:48.06ms
step:1252/1900 train_time:60195ms step_avg:48.08ms
step:1253/1900 train_time:60289ms step_avg:48.12ms
step:1254/1900 train_time:60376ms step_avg:48.15ms
step:1255/1900 train_time:60465ms step_avg:48.18ms
step:1256/1900 train_time:60551ms step_avg:48.21ms
step:1257/1900 train_time:60639ms step_avg:48.24ms
step:1258/1900 train_time:60725ms step_avg:48.27ms
step:1259/1900 train_time:60813ms step_avg:48.30ms
step:1260/1900 train_time:60900ms step_avg:48.33ms
step:1261/1900 train_time:60988ms step_avg:48.36ms
step:1262/1900 train_time:61077ms step_avg:48.40ms
step:1263/1900 train_time:61167ms step_avg:48.43ms
step:1264/1900 train_time:61259ms step_avg:48.46ms
step:1265/1900 train_time:61349ms step_avg:48.50ms
step:1266/1900 train_time:61438ms step_avg:48.53ms
step:1267/1900 train_time:61526ms step_avg:48.56ms
step:1268/1900 train_time:61612ms step_avg:48.59ms
step:1269/1900 train_time:61699ms step_avg:48.62ms
step:1270/1900 train_time:61786ms step_avg:48.65ms
step:1271/1900 train_time:61875ms step_avg:48.68ms
step:1272/1900 train_time:61962ms step_avg:48.71ms
step:1273/1900 train_time:62052ms step_avg:48.74ms
step:1274/1900 train_time:62139ms step_avg:48.77ms
step:1275/1900 train_time:62229ms step_avg:48.81ms
step:1276/1900 train_time:62318ms step_avg:48.84ms
step:1277/1900 train_time:62407ms step_avg:48.87ms
step:1278/1900 train_time:62495ms step_avg:48.90ms
step:1279/1900 train_time:62583ms step_avg:48.93ms
step:1280/1900 train_time:62669ms step_avg:48.96ms
step:1281/1900 train_time:62757ms step_avg:48.99ms
step:1282/1900 train_time:62844ms step_avg:49.02ms
step:1283/1900 train_time:62933ms step_avg:49.05ms
step:1284/1900 train_time:63020ms step_avg:49.08ms
step:1285/1900 train_time:63109ms step_avg:49.11ms
step:1286/1900 train_time:63197ms step_avg:49.14ms
step:1287/1900 train_time:63286ms step_avg:49.17ms
step:1288/1900 train_time:63374ms step_avg:49.20ms
step:1289/1900 train_time:63462ms step_avg:49.23ms
step:1290/1900 train_time:63550ms step_avg:49.26ms
step:1291/1900 train_time:63638ms step_avg:49.29ms
step:1292/1900 train_time:63725ms step_avg:49.32ms
step:1293/1900 train_time:63813ms step_avg:49.35ms
step:1294/1900 train_time:63899ms step_avg:49.38ms
step:1295/1900 train_time:63988ms step_avg:49.41ms
step:1296/1900 train_time:64075ms step_avg:49.44ms
step:1297/1900 train_time:64163ms step_avg:49.47ms
step:1298/1900 train_time:64252ms step_avg:49.50ms
step:1299/1900 train_time:64341ms step_avg:49.53ms
step:1300/1900 train_time:64428ms step_avg:49.56ms
step:1301/1900 train_time:64517ms step_avg:49.59ms
step:1302/1900 train_time:64605ms step_avg:49.62ms
step:1303/1900 train_time:64692ms step_avg:49.65ms
step:1304/1900 train_time:64780ms step_avg:49.68ms
step:1305/1900 train_time:64868ms step_avg:49.71ms
step:1306/1900 train_time:64955ms step_avg:49.74ms
step:1307/1900 train_time:65044ms step_avg:49.77ms
step:1308/1900 train_time:65131ms step_avg:49.79ms
step:1309/1900 train_time:65221ms step_avg:49.82ms
step:1310/1900 train_time:65309ms step_avg:49.85ms
step:1311/1900 train_time:65398ms step_avg:49.88ms
step:1312/1900 train_time:65487ms step_avg:49.91ms
step:1313/1900 train_time:65576ms step_avg:49.94ms
step:1314/1900 train_time:65663ms step_avg:49.97ms
step:1315/1900 train_time:65750ms step_avg:50.00ms
step:1316/1900 train_time:65838ms step_avg:50.03ms
step:1317/1900 train_time:65926ms step_avg:50.06ms
step:1318/1900 train_time:66014ms step_avg:50.09ms
step:1319/1900 train_time:66102ms step_avg:50.12ms
step:1320/1900 train_time:66190ms step_avg:50.14ms
step:1321/1900 train_time:66279ms step_avg:50.17ms
step:1322/1900 train_time:66368ms step_avg:50.20ms
step:1323/1900 train_time:66458ms step_avg:50.23ms
step:1324/1900 train_time:66545ms step_avg:50.26ms
step:1325/1900 train_time:66634ms step_avg:50.29ms
step:1326/1900 train_time:66721ms step_avg:50.32ms
step:1327/1900 train_time:66809ms step_avg:50.35ms
step:1328/1900 train_time:66896ms step_avg:50.37ms
step:1329/1900 train_time:66984ms step_avg:50.40ms
step:1330/1900 train_time:67072ms step_avg:50.43ms
step:1331/1900 train_time:67160ms step_avg:50.46ms
step:1332/1900 train_time:67248ms step_avg:50.49ms
step:1333/1900 train_time:67337ms step_avg:50.52ms
step:1334/1900 train_time:67426ms step_avg:50.54ms
step:1335/1900 train_time:67516ms step_avg:50.57ms
step:1336/1900 train_time:67603ms step_avg:50.60ms
step:1337/1900 train_time:67692ms step_avg:50.63ms
step:1338/1900 train_time:67778ms step_avg:50.66ms
step:1339/1900 train_time:67866ms step_avg:50.68ms
step:1340/1900 train_time:67955ms step_avg:50.71ms
step:1341/1900 train_time:68044ms step_avg:50.74ms
step:1342/1900 train_time:68131ms step_avg:50.77ms
step:1343/1900 train_time:68220ms step_avg:50.80ms
step:1344/1900 train_time:68308ms step_avg:50.82ms
step:1345/1900 train_time:68397ms step_avg:50.85ms
step:1346/1900 train_time:68485ms step_avg:50.88ms
step:1347/1900 train_time:68575ms step_avg:50.91ms
step:1348/1900 train_time:68662ms step_avg:50.94ms
step:1349/1900 train_time:68751ms step_avg:50.96ms
step:1350/1900 train_time:68837ms step_avg:50.99ms
step:1351/1900 train_time:68926ms step_avg:51.02ms
step:1352/1900 train_time:69014ms step_avg:51.05ms
step:1353/1900 train_time:69101ms step_avg:51.07ms
step:1354/1900 train_time:69189ms step_avg:51.10ms
step:1355/1900 train_time:69277ms step_avg:51.13ms
step:1356/1900 train_time:69366ms step_avg:51.15ms
step:1357/1900 train_time:69455ms step_avg:51.18ms
step:1358/1900 train_time:69543ms step_avg:51.21ms
step:1359/1900 train_time:69632ms step_avg:51.24ms
step:1360/1900 train_time:69719ms step_avg:51.26ms
step:1361/1900 train_time:69807ms step_avg:51.29ms
step:1362/1900 train_time:69895ms step_avg:51.32ms
step:1363/1900 train_time:69983ms step_avg:51.34ms
step:1364/1900 train_time:70070ms step_avg:51.37ms
step:1365/1900 train_time:70159ms step_avg:51.40ms
step:1366/1900 train_time:70247ms step_avg:51.43ms
step:1367/1900 train_time:70335ms step_avg:51.45ms
step:1368/1900 train_time:70423ms step_avg:51.48ms
step:1369/1900 train_time:70512ms step_avg:51.51ms
step:1370/1900 train_time:70599ms step_avg:51.53ms
step:1371/1900 train_time:70688ms step_avg:51.56ms
step:1372/1900 train_time:70776ms step_avg:51.59ms
step:1373/1900 train_time:70865ms step_avg:51.61ms
step:1374/1900 train_time:70953ms step_avg:51.64ms
step:1375/1900 train_time:71041ms step_avg:51.67ms
step:1376/1900 train_time:71128ms step_avg:51.69ms
step:1377/1900 train_time:71218ms step_avg:51.72ms
step:1378/1900 train_time:71306ms step_avg:51.75ms
step:1379/1900 train_time:71395ms step_avg:51.77ms
step:1380/1900 train_time:71483ms step_avg:51.80ms
step:1381/1900 train_time:71571ms step_avg:51.83ms
step:1382/1900 train_time:71658ms step_avg:51.85ms
step:1383/1900 train_time:71746ms step_avg:51.88ms
step:1384/1900 train_time:71834ms step_avg:51.90ms
step:1385/1900 train_time:71922ms step_avg:51.93ms
step:1386/1900 train_time:72009ms step_avg:51.95ms
step:1387/1900 train_time:72097ms step_avg:51.98ms
step:1388/1900 train_time:72185ms step_avg:52.01ms
step:1389/1900 train_time:72274ms step_avg:52.03ms
step:1390/1900 train_time:72361ms step_avg:52.06ms
step:1391/1900 train_time:72450ms step_avg:52.08ms
step:1392/1900 train_time:72538ms step_avg:52.11ms
step:1393/1900 train_time:72627ms step_avg:52.14ms
step:1394/1900 train_time:72714ms step_avg:52.16ms
step:1395/1900 train_time:72802ms step_avg:52.19ms
step:1396/1900 train_time:72890ms step_avg:52.21ms
step:1397/1900 train_time:72978ms step_avg:52.24ms
step:1398/1900 train_time:73066ms step_avg:52.26ms
step:1399/1900 train_time:73155ms step_avg:52.29ms
step:1400/1900 train_time:73242ms step_avg:52.32ms
step:1401/1900 train_time:73331ms step_avg:52.34ms
step:1402/1900 train_time:73418ms step_avg:52.37ms
step:1403/1900 train_time:73506ms step_avg:52.39ms
step:1404/1900 train_time:73594ms step_avg:52.42ms
step:1405/1900 train_time:73682ms step_avg:52.44ms
step:1406/1900 train_time:73769ms step_avg:52.47ms
step:1407/1900 train_time:73858ms step_avg:52.49ms
step:1408/1900 train_time:73946ms step_avg:52.52ms
step:1409/1900 train_time:74034ms step_avg:52.54ms
step:1410/1900 train_time:74121ms step_avg:52.57ms
step:1411/1900 train_time:74209ms step_avg:52.59ms
step:1412/1900 train_time:74297ms step_avg:52.62ms
step:1413/1900 train_time:74385ms step_avg:52.64ms
step:1414/1900 train_time:74474ms step_avg:52.67ms
step:1415/1900 train_time:74562ms step_avg:52.69ms
step:1416/1900 train_time:74649ms step_avg:52.72ms
step:1417/1900 train_time:74737ms step_avg:52.74ms
step:1418/1900 train_time:74826ms step_avg:52.77ms
step:1419/1900 train_time:74915ms step_avg:52.79ms
step:1420/1900 train_time:75003ms step_avg:52.82ms
step:1421/1900 train_time:75092ms step_avg:52.84ms
step:1422/1900 train_time:75179ms step_avg:52.87ms
step:1423/1900 train_time:75268ms step_avg:52.89ms
step:1424/1900 train_time:75355ms step_avg:52.92ms
step:1425/1900 train_time:75443ms step_avg:52.94ms
step:1426/1900 train_time:75530ms step_avg:52.97ms
step:1427/1900 train_time:75619ms step_avg:52.99ms
step:1428/1900 train_time:75706ms step_avg:53.02ms
step:1429/1900 train_time:75796ms step_avg:53.04ms
step:1430/1900 train_time:75884ms step_avg:53.07ms
step:1431/1900 train_time:75972ms step_avg:53.09ms
step:1432/1900 train_time:76059ms step_avg:53.11ms
step:1433/1900 train_time:76147ms step_avg:53.14ms
step:1434/1900 train_time:76235ms step_avg:53.16ms
step:1435/1900 train_time:76324ms step_avg:53.19ms
step:1436/1900 train_time:76411ms step_avg:53.21ms
step:1437/1900 train_time:76500ms step_avg:53.24ms
step:1438/1900 train_time:76587ms step_avg:53.26ms
step:1439/1900 train_time:76676ms step_avg:53.28ms
step:1440/1900 train_time:76763ms step_avg:53.31ms
step:1441/1900 train_time:76851ms step_avg:53.33ms
step:1442/1900 train_time:76939ms step_avg:53.36ms
step:1443/1900 train_time:77028ms step_avg:53.38ms
step:1444/1900 train_time:77115ms step_avg:53.40ms
step:1445/1900 train_time:77204ms step_avg:53.43ms
step:1446/1900 train_time:77293ms step_avg:53.45ms
step:1447/1900 train_time:77381ms step_avg:53.48ms
step:1448/1900 train_time:77469ms step_avg:53.50ms
step:1449/1900 train_time:77558ms step_avg:53.52ms
step:1450/1900 train_time:77645ms step_avg:53.55ms
step:1451/1900 train_time:77733ms step_avg:53.57ms
step:1452/1900 train_time:77821ms step_avg:53.60ms
step:1453/1900 train_time:77909ms step_avg:53.62ms
step:1454/1900 train_time:77996ms step_avg:53.64ms
step:1455/1900 train_time:78085ms step_avg:53.67ms
step:1456/1900 train_time:78173ms step_avg:53.69ms
step:1457/1900 train_time:78261ms step_avg:53.71ms
step:1458/1900 train_time:78350ms step_avg:53.74ms
step:1459/1900 train_time:78439ms step_avg:53.76ms
step:1460/1900 train_time:78527ms step_avg:53.79ms
step:1461/1900 train_time:78615ms step_avg:53.81ms
step:1462/1900 train_time:78701ms step_avg:53.83ms
step:1463/1900 train_time:78790ms step_avg:53.86ms
step:1464/1900 train_time:78878ms step_avg:53.88ms
step:1465/1900 train_time:78966ms step_avg:53.90ms
step:1466/1900 train_time:79055ms step_avg:53.93ms
step:1467/1900 train_time:79143ms step_avg:53.95ms
step:1468/1900 train_time:79230ms step_avg:53.97ms
step:1469/1900 train_time:79319ms step_avg:54.00ms
step:1470/1900 train_time:79407ms step_avg:54.02ms
step:1471/1900 train_time:79497ms step_avg:54.04ms
step:1472/1900 train_time:79584ms step_avg:54.07ms
step:1473/1900 train_time:79673ms step_avg:54.09ms
step:1474/1900 train_time:79760ms step_avg:54.11ms
step:1475/1900 train_time:79848ms step_avg:54.13ms
step:1476/1900 train_time:79936ms step_avg:54.16ms
step:1477/1900 train_time:80024ms step_avg:54.18ms
step:1478/1900 train_time:80112ms step_avg:54.20ms
step:1479/1900 train_time:80200ms step_avg:54.23ms
step:1480/1900 train_time:80287ms step_avg:54.25ms
step:1481/1900 train_time:80377ms step_avg:54.27ms
step:1482/1900 train_time:80464ms step_avg:54.29ms
step:1483/1900 train_time:80554ms step_avg:54.32ms
step:1484/1900 train_time:80641ms step_avg:54.34ms
step:1485/1900 train_time:80729ms step_avg:54.36ms
step:1486/1900 train_time:80817ms step_avg:54.39ms
step:1487/1900 train_time:80906ms step_avg:54.41ms
step:1488/1900 train_time:80993ms step_avg:54.43ms
step:1489/1900 train_time:81081ms step_avg:54.45ms
step:1490/1900 train_time:81170ms step_avg:54.48ms
step:1491/1900 train_time:81258ms step_avg:54.50ms
step:1492/1900 train_time:81346ms step_avg:54.52ms
step:1493/1900 train_time:81436ms step_avg:54.55ms
step:1494/1900 train_time:81523ms step_avg:54.57ms
step:1495/1900 train_time:81611ms step_avg:54.59ms
step:1496/1900 train_time:81698ms step_avg:54.61ms
step:1497/1900 train_time:81787ms step_avg:54.63ms
step:1498/1900 train_time:81874ms step_avg:54.66ms
step:1499/1900 train_time:81962ms step_avg:54.68ms
step:1500/1900 train_time:82049ms step_avg:54.70ms
step:1500/1900 val_loss:3.4149 train_time:82141ms step_avg:54.76ms
step:1501/1900 train_time:82161ms step_avg:54.74ms
step:1502/1900 train_time:82228ms step_avg:54.75ms
step:1503/1900 train_time:82321ms step_avg:54.77ms
step:1504/1900 train_time:82410ms step_avg:54.79ms
step:1505/1900 train_time:82498ms step_avg:54.82ms
step:1506/1900 train_time:82584ms step_avg:54.84ms
step:1507/1900 train_time:82671ms step_avg:54.86ms
step:1508/1900 train_time:82758ms step_avg:54.88ms
step:1509/1900 train_time:82845ms step_avg:54.90ms
step:1510/1900 train_time:82932ms step_avg:54.92ms
step:1511/1900 train_time:83020ms step_avg:54.94ms
step:1512/1900 train_time:83111ms step_avg:54.97ms
step:1513/1900 train_time:83202ms step_avg:54.99ms
step:1514/1900 train_time:83292ms step_avg:55.01ms
step:1515/1900 train_time:83381ms step_avg:55.04ms
step:1516/1900 train_time:83468ms step_avg:55.06ms
step:1517/1900 train_time:83556ms step_avg:55.08ms
step:1518/1900 train_time:83643ms step_avg:55.10ms
step:1519/1900 train_time:83731ms step_avg:55.12ms
step:1520/1900 train_time:83817ms step_avg:55.14ms
step:1521/1900 train_time:83905ms step_avg:55.16ms
step:1522/1900 train_time:83992ms step_avg:55.19ms
step:1523/1900 train_time:84081ms step_avg:55.21ms
step:1524/1900 train_time:84170ms step_avg:55.23ms
step:1525/1900 train_time:84259ms step_avg:55.25ms
step:1526/1900 train_time:84348ms step_avg:55.27ms
step:1527/1900 train_time:84437ms step_avg:55.30ms
step:1528/1900 train_time:84524ms step_avg:55.32ms
step:1529/1900 train_time:84612ms step_avg:55.34ms
step:1530/1900 train_time:84699ms step_avg:55.36ms
step:1531/1900 train_time:84787ms step_avg:55.38ms
step:1532/1900 train_time:84874ms step_avg:55.40ms
step:1533/1900 train_time:84962ms step_avg:55.42ms
step:1534/1900 train_time:85050ms step_avg:55.44ms
step:1535/1900 train_time:85138ms step_avg:55.46ms
step:1536/1900 train_time:85227ms step_avg:55.49ms
step:1537/1900 train_time:85316ms step_avg:55.51ms
step:1538/1900 train_time:85405ms step_avg:55.53ms
step:1539/1900 train_time:85493ms step_avg:55.55ms
step:1540/1900 train_time:85581ms step_avg:55.57ms
step:1541/1900 train_time:85668ms step_avg:55.59ms
step:1542/1900 train_time:85755ms step_avg:55.61ms
step:1543/1900 train_time:85842ms step_avg:55.63ms
step:1544/1900 train_time:85930ms step_avg:55.65ms
step:1545/1900 train_time:86018ms step_avg:55.68ms
step:1546/1900 train_time:86105ms step_avg:55.70ms
step:1547/1900 train_time:86196ms step_avg:55.72ms
step:1548/1900 train_time:86285ms step_avg:55.74ms
step:1549/1900 train_time:86374ms step_avg:55.76ms
step:1550/1900 train_time:86462ms step_avg:55.78ms
step:1551/1900 train_time:86552ms step_avg:55.80ms
step:1552/1900 train_time:86638ms step_avg:55.82ms
step:1553/1900 train_time:86726ms step_avg:55.84ms
step:1554/1900 train_time:86813ms step_avg:55.86ms
step:1555/1900 train_time:86901ms step_avg:55.88ms
step:1556/1900 train_time:86990ms step_avg:55.91ms
step:1557/1900 train_time:87078ms step_avg:55.93ms
step:1558/1900 train_time:87166ms step_avg:55.95ms
step:1559/1900 train_time:87255ms step_avg:55.97ms
step:1560/1900 train_time:87343ms step_avg:55.99ms
step:1561/1900 train_time:87432ms step_avg:56.01ms
step:1562/1900 train_time:87519ms step_avg:56.03ms
step:1563/1900 train_time:87607ms step_avg:56.05ms
step:1564/1900 train_time:87695ms step_avg:56.07ms
step:1565/1900 train_time:87783ms step_avg:56.09ms
step:1566/1900 train_time:87870ms step_avg:56.11ms
step:1567/1900 train_time:87958ms step_avg:56.13ms
step:1568/1900 train_time:88045ms step_avg:56.15ms
step:1569/1900 train_time:88134ms step_avg:56.17ms
step:1570/1900 train_time:88223ms step_avg:56.19ms
step:1571/1900 train_time:88312ms step_avg:56.21ms
step:1572/1900 train_time:88400ms step_avg:56.23ms
step:1573/1900 train_time:88489ms step_avg:56.25ms
step:1574/1900 train_time:88576ms step_avg:56.27ms
step:1575/1900 train_time:88664ms step_avg:56.29ms
step:1576/1900 train_time:88751ms step_avg:56.31ms
step:1577/1900 train_time:88840ms step_avg:56.33ms
step:1578/1900 train_time:88928ms step_avg:56.35ms
step:1579/1900 train_time:89016ms step_avg:56.38ms
step:1580/1900 train_time:89104ms step_avg:56.40ms
step:1581/1900 train_time:89194ms step_avg:56.42ms
step:1582/1900 train_time:89282ms step_avg:56.44ms
step:1583/1900 train_time:89371ms step_avg:56.46ms
step:1584/1900 train_time:89458ms step_avg:56.48ms
step:1585/1900 train_time:89547ms step_avg:56.50ms
step:1586/1900 train_time:89634ms step_avg:56.52ms
step:1587/1900 train_time:89722ms step_avg:56.54ms
step:1588/1900 train_time:89809ms step_avg:56.56ms
step:1589/1900 train_time:89897ms step_avg:56.57ms
step:1590/1900 train_time:89985ms step_avg:56.59ms
step:1591/1900 train_time:90075ms step_avg:56.62ms
step:1592/1900 train_time:90163ms step_avg:56.64ms
step:1593/1900 train_time:90253ms step_avg:56.66ms
step:1594/1900 train_time:90340ms step_avg:56.68ms
step:1595/1900 train_time:90429ms step_avg:56.70ms
step:1596/1900 train_time:90516ms step_avg:56.71ms
step:1597/1900 train_time:90604ms step_avg:56.73ms
step:1598/1900 train_time:90693ms step_avg:56.75ms
step:1599/1900 train_time:90781ms step_avg:56.77ms
step:1600/1900 train_time:90869ms step_avg:56.79ms
step:1601/1900 train_time:90957ms step_avg:56.81ms
step:1602/1900 train_time:91045ms step_avg:56.83ms
step:1603/1900 train_time:91134ms step_avg:56.85ms
step:1604/1900 train_time:91222ms step_avg:56.87ms
step:1605/1900 train_time:91312ms step_avg:56.89ms
step:1606/1900 train_time:91400ms step_avg:56.91ms
step:1607/1900 train_time:91489ms step_avg:56.93ms
step:1608/1900 train_time:91576ms step_avg:56.95ms
step:1609/1900 train_time:91664ms step_avg:56.97ms
step:1610/1900 train_time:91752ms step_avg:56.99ms
step:1611/1900 train_time:91840ms step_avg:57.01ms
step:1612/1900 train_time:91928ms step_avg:57.03ms
step:1613/1900 train_time:92016ms step_avg:57.05ms
step:1614/1900 train_time:92103ms step_avg:57.07ms
step:1615/1900 train_time:92193ms step_avg:57.09ms
step:1616/1900 train_time:92281ms step_avg:57.10ms
step:1617/1900 train_time:92370ms step_avg:57.12ms
step:1618/1900 train_time:92457ms step_avg:57.14ms
step:1619/1900 train_time:92546ms step_avg:57.16ms
step:1620/1900 train_time:92634ms step_avg:57.18ms
step:1621/1900 train_time:92723ms step_avg:57.20ms
step:1622/1900 train_time:92810ms step_avg:57.22ms
step:1623/1900 train_time:92899ms step_avg:57.24ms
step:1624/1900 train_time:92987ms step_avg:57.26ms
step:1625/1900 train_time:93075ms step_avg:57.28ms
step:1626/1900 train_time:93163ms step_avg:57.30ms
step:1627/1900 train_time:93252ms step_avg:57.32ms
step:1628/1900 train_time:93340ms step_avg:57.33ms
step:1629/1900 train_time:93431ms step_avg:57.35ms
step:1630/1900 train_time:93517ms step_avg:57.37ms
step:1631/1900 train_time:93606ms step_avg:57.39ms
step:1632/1900 train_time:93694ms step_avg:57.41ms
step:1633/1900 train_time:93783ms step_avg:57.43ms
step:1634/1900 train_time:93871ms step_avg:57.45ms
step:1635/1900 train_time:93958ms step_avg:57.47ms
step:1636/1900 train_time:94045ms step_avg:57.48ms
step:1637/1900 train_time:94135ms step_avg:57.50ms
step:1638/1900 train_time:94223ms step_avg:57.52ms
step:1639/1900 train_time:94313ms step_avg:57.54ms
step:1640/1900 train_time:94401ms step_avg:57.56ms
step:1641/1900 train_time:94489ms step_avg:57.58ms
step:1642/1900 train_time:94576ms step_avg:57.60ms
step:1643/1900 train_time:94665ms step_avg:57.62ms
step:1644/1900 train_time:94753ms step_avg:57.64ms
step:1645/1900 train_time:94841ms step_avg:57.65ms
step:1646/1900 train_time:94928ms step_avg:57.67ms
step:1647/1900 train_time:95016ms step_avg:57.69ms
step:1648/1900 train_time:95104ms step_avg:57.71ms
step:1649/1900 train_time:95193ms step_avg:57.73ms
step:1650/1900 train_time:95281ms step_avg:57.75ms
step:1651/1900 train_time:95371ms step_avg:57.77ms
step:1652/1900 train_time:95458ms step_avg:57.78ms
step:1653/1900 train_time:95546ms step_avg:57.80ms
step:1654/1900 train_time:95633ms step_avg:57.82ms
step:1655/1900 train_time:95722ms step_avg:57.84ms
step:1656/1900 train_time:95809ms step_avg:57.86ms
step:1657/1900 train_time:95898ms step_avg:57.87ms
step:1658/1900 train_time:95985ms step_avg:57.89ms
step:1659/1900 train_time:96074ms step_avg:57.91ms
step:1660/1900 train_time:96162ms step_avg:57.93ms
step:1661/1900 train_time:96252ms step_avg:57.95ms
step:1662/1900 train_time:96339ms step_avg:57.97ms
step:1663/1900 train_time:96429ms step_avg:57.98ms
step:1664/1900 train_time:96515ms step_avg:58.00ms
step:1665/1900 train_time:96604ms step_avg:58.02ms
step:1666/1900 train_time:96692ms step_avg:58.04ms
step:1667/1900 train_time:96780ms step_avg:58.06ms
step:1668/1900 train_time:96868ms step_avg:58.07ms
step:1669/1900 train_time:96956ms step_avg:58.09ms
step:1670/1900 train_time:97045ms step_avg:58.11ms
step:1671/1900 train_time:97135ms step_avg:58.13ms
step:1672/1900 train_time:97224ms step_avg:58.15ms
step:1673/1900 train_time:97313ms step_avg:58.17ms
step:1674/1900 train_time:97401ms step_avg:58.18ms
step:1675/1900 train_time:97491ms step_avg:58.20ms
step:1676/1900 train_time:97579ms step_avg:58.22ms
step:1677/1900 train_time:97667ms step_avg:58.24ms
step:1678/1900 train_time:97754ms step_avg:58.26ms
step:1679/1900 train_time:97844ms step_avg:58.27ms
step:1680/1900 train_time:97931ms step_avg:58.29ms
step:1681/1900 train_time:98020ms step_avg:58.31ms
step:1682/1900 train_time:98108ms step_avg:58.33ms
step:1683/1900 train_time:98197ms step_avg:58.35ms
step:1684/1900 train_time:98285ms step_avg:58.36ms
step:1685/1900 train_time:98375ms step_avg:58.38ms
step:1686/1900 train_time:98463ms step_avg:58.40ms
step:1687/1900 train_time:98552ms step_avg:58.42ms
step:1688/1900 train_time:98640ms step_avg:58.44ms
step:1689/1900 train_time:98728ms step_avg:58.45ms
step:1690/1900 train_time:98815ms step_avg:58.47ms
step:1691/1900 train_time:98904ms step_avg:58.49ms
step:1692/1900 train_time:98991ms step_avg:58.51ms
step:1693/1900 train_time:99080ms step_avg:58.52ms
step:1694/1900 train_time:99167ms step_avg:58.54ms
step:1695/1900 train_time:99257ms step_avg:58.56ms
step:1696/1900 train_time:99345ms step_avg:58.58ms
step:1697/1900 train_time:99434ms step_avg:58.59ms
step:1698/1900 train_time:99522ms step_avg:58.61ms
step:1699/1900 train_time:99611ms step_avg:58.63ms
step:1700/1900 train_time:99698ms step_avg:58.65ms
step:1701/1900 train_time:99787ms step_avg:58.66ms
step:1702/1900 train_time:99874ms step_avg:58.68ms
step:1703/1900 train_time:99963ms step_avg:58.70ms
step:1704/1900 train_time:100050ms step_avg:58.71ms
step:1705/1900 train_time:100138ms step_avg:58.73ms
step:1706/1900 train_time:100226ms step_avg:58.75ms
step:1707/1900 train_time:100315ms step_avg:58.77ms
step:1708/1900 train_time:100403ms step_avg:58.78ms
step:1709/1900 train_time:100492ms step_avg:58.80ms
step:1710/1900 train_time:100579ms step_avg:58.82ms
step:1711/1900 train_time:100668ms step_avg:58.84ms
step:1712/1900 train_time:100755ms step_avg:58.85ms
step:1713/1900 train_time:100844ms step_avg:58.87ms
step:1714/1900 train_time:100932ms step_avg:58.89ms
step:1715/1900 train_time:101020ms step_avg:58.90ms
step:1716/1900 train_time:101108ms step_avg:58.92ms
step:1717/1900 train_time:101197ms step_avg:58.94ms
step:1718/1900 train_time:101285ms step_avg:58.96ms
step:1719/1900 train_time:101374ms step_avg:58.97ms
step:1720/1900 train_time:101462ms step_avg:58.99ms
step:1721/1900 train_time:101551ms step_avg:59.01ms
step:1722/1900 train_time:101638ms step_avg:59.02ms
step:1723/1900 train_time:101727ms step_avg:59.04ms
step:1724/1900 train_time:101814ms step_avg:59.06ms
step:1725/1900 train_time:101902ms step_avg:59.07ms
step:1726/1900 train_time:101989ms step_avg:59.09ms
step:1727/1900 train_time:102078ms step_avg:59.11ms
step:1728/1900 train_time:102165ms step_avg:59.12ms
step:1729/1900 train_time:102255ms step_avg:59.14ms
step:1730/1900 train_time:102342ms step_avg:59.16ms
step:1731/1900 train_time:102432ms step_avg:59.17ms
step:1732/1900 train_time:102519ms step_avg:59.19ms
step:1733/1900 train_time:102608ms step_avg:59.21ms
step:1734/1900 train_time:102695ms step_avg:59.22ms
step:1735/1900 train_time:102784ms step_avg:59.24ms
step:1736/1900 train_time:102871ms step_avg:59.26ms
step:1737/1900 train_time:102960ms step_avg:59.27ms
step:1738/1900 train_time:103048ms step_avg:59.29ms
step:1739/1900 train_time:103136ms step_avg:59.31ms
step:1740/1900 train_time:103224ms step_avg:59.32ms
step:1741/1900 train_time:103313ms step_avg:59.34ms
step:1742/1900 train_time:103401ms step_avg:59.36ms
step:1743/1900 train_time:103490ms step_avg:59.37ms
step:1744/1900 train_time:103578ms step_avg:59.39ms
step:1745/1900 train_time:103666ms step_avg:59.41ms
step:1746/1900 train_time:103754ms step_avg:59.42ms
step:1747/1900 train_time:103842ms step_avg:59.44ms
step:1748/1900 train_time:103929ms step_avg:59.46ms
step:1749/1900 train_time:104018ms step_avg:59.47ms
step:1750/1900 train_time:104106ms step_avg:59.49ms
step:1750/1900 val_loss:3.3190 train_time:104197ms step_avg:59.54ms
step:1751/1900 train_time:104218ms step_avg:59.52ms
step:1752/1900 train_time:104287ms step_avg:59.52ms
step:1753/1900 train_time:104380ms step_avg:59.54ms
step:1754/1900 train_time:104470ms step_avg:59.56ms
step:1755/1900 train_time:104558ms step_avg:59.58ms
step:1756/1900 train_time:104645ms step_avg:59.59ms
step:1757/1900 train_time:104733ms step_avg:59.61ms
step:1758/1900 train_time:104819ms step_avg:59.62ms
step:1759/1900 train_time:104906ms step_avg:59.64ms
step:1760/1900 train_time:104993ms step_avg:59.66ms
step:1761/1900 train_time:105081ms step_avg:59.67ms
step:1762/1900 train_time:105169ms step_avg:59.69ms
step:1763/1900 train_time:105260ms step_avg:59.71ms
step:1764/1900 train_time:105350ms step_avg:59.72ms
step:1765/1900 train_time:105440ms step_avg:59.74ms
step:1766/1900 train_time:105529ms step_avg:59.76ms
step:1767/1900 train_time:105617ms step_avg:59.77ms
step:1768/1900 train_time:105703ms step_avg:59.79ms
step:1769/1900 train_time:105792ms step_avg:59.80ms
step:1770/1900 train_time:105878ms step_avg:59.82ms
step:1771/1900 train_time:105966ms step_avg:59.83ms
step:1772/1900 train_time:106052ms step_avg:59.85ms
step:1773/1900 train_time:106141ms step_avg:59.87ms
step:1774/1900 train_time:106231ms step_avg:59.88ms
step:1775/1900 train_time:106322ms step_avg:59.90ms
step:1776/1900 train_time:106411ms step_avg:59.92ms
step:1777/1900 train_time:106501ms step_avg:59.93ms
step:1778/1900 train_time:106589ms step_avg:59.95ms
step:1779/1900 train_time:106677ms step_avg:59.96ms
step:1780/1900 train_time:106763ms step_avg:59.98ms
step:1781/1900 train_time:106851ms step_avg:60.00ms
step:1782/1900 train_time:106938ms step_avg:60.01ms
step:1783/1900 train_time:107026ms step_avg:60.03ms
step:1784/1900 train_time:107113ms step_avg:60.04ms
step:1785/1900 train_time:107203ms step_avg:60.06ms
step:1786/1900 train_time:107291ms step_avg:60.07ms
step:1787/1900 train_time:107380ms step_avg:60.09ms
step:1788/1900 train_time:107470ms step_avg:60.11ms
step:1789/1900 train_time:107558ms step_avg:60.12ms
step:1790/1900 train_time:107646ms step_avg:60.14ms
step:1791/1900 train_time:107734ms step_avg:60.15ms
step:1792/1900 train_time:107821ms step_avg:60.17ms
step:1793/1900 train_time:107909ms step_avg:60.18ms
step:1794/1900 train_time:107996ms step_avg:60.20ms
step:1795/1900 train_time:108084ms step_avg:60.21ms
step:1796/1900 train_time:108173ms step_avg:60.23ms
step:1797/1900 train_time:108262ms step_avg:60.25ms
step:1798/1900 train_time:108350ms step_avg:60.26ms
step:1799/1900 train_time:108440ms step_avg:60.28ms
step:1800/1900 train_time:108528ms step_avg:60.29ms
step:1801/1900 train_time:108616ms step_avg:60.31ms
step:1802/1900 train_time:108704ms step_avg:60.32ms
step:1803/1900 train_time:108792ms step_avg:60.34ms
step:1804/1900 train_time:108879ms step_avg:60.35ms
step:1805/1900 train_time:108967ms step_avg:60.37ms
step:1806/1900 train_time:109055ms step_avg:60.38ms
step:1807/1900 train_time:109142ms step_avg:60.40ms
step:1808/1900 train_time:109231ms step_avg:60.42ms
step:1809/1900 train_time:109319ms step_avg:60.43ms
step:1810/1900 train_time:109409ms step_avg:60.45ms
step:1811/1900 train_time:109498ms step_avg:60.46ms
step:1812/1900 train_time:109585ms step_avg:60.48ms
step:1813/1900 train_time:109673ms step_avg:60.49ms
step:1814/1900 train_time:109761ms step_avg:60.51ms
step:1815/1900 train_time:109849ms step_avg:60.52ms
step:1816/1900 train_time:109935ms step_avg:60.54ms
step:1817/1900 train_time:110024ms step_avg:60.55ms
step:1818/1900 train_time:110111ms step_avg:60.57ms
step:1819/1900 train_time:110200ms step_avg:60.58ms
step:1820/1900 train_time:110288ms step_avg:60.60ms
step:1821/1900 train_time:110377ms step_avg:60.61ms
step:1822/1900 train_time:110465ms step_avg:60.63ms
step:1823/1900 train_time:110554ms step_avg:60.64ms
step:1824/1900 train_time:110642ms step_avg:60.66ms
step:1825/1900 train_time:110731ms step_avg:60.67ms
step:1826/1900 train_time:110818ms step_avg:60.69ms
step:1827/1900 train_time:110907ms step_avg:60.70ms
step:1828/1900 train_time:110994ms step_avg:60.72ms
step:1829/1900 train_time:111082ms step_avg:60.73ms
step:1830/1900 train_time:111170ms step_avg:60.75ms
step:1831/1900 train_time:111258ms step_avg:60.76ms
step:1832/1900 train_time:111345ms step_avg:60.78ms
step:1833/1900 train_time:111434ms step_avg:60.79ms
step:1834/1900 train_time:111523ms step_avg:60.81ms
step:1835/1900 train_time:111612ms step_avg:60.82ms
step:1836/1900 train_time:111699ms step_avg:60.84ms
step:1837/1900 train_time:111788ms step_avg:60.85ms
step:1838/1900 train_time:111874ms step_avg:60.87ms
step:1839/1900 train_time:111963ms step_avg:60.88ms
step:1840/1900 train_time:112050ms step_avg:60.90ms
step:1841/1900 train_time:112138ms step_avg:60.91ms
step:1842/1900 train_time:112226ms step_avg:60.93ms
step:1843/1900 train_time:112314ms step_avg:60.94ms
step:1844/1900 train_time:112402ms step_avg:60.96ms
step:1845/1900 train_time:112491ms step_avg:60.97ms
step:1846/1900 train_time:112579ms step_avg:60.99ms
step:1847/1900 train_time:112667ms step_avg:61.00ms
step:1848/1900 train_time:112754ms step_avg:61.01ms
step:1849/1900 train_time:112843ms step_avg:61.03ms
step:1850/1900 train_time:112931ms step_avg:61.04ms
step:1851/1900 train_time:113019ms step_avg:61.06ms
step:1852/1900 train_time:113107ms step_avg:61.07ms
step:1853/1900 train_time:113196ms step_avg:61.09ms
step:1854/1900 train_time:113283ms step_avg:61.10ms
step:1855/1900 train_time:113373ms step_avg:61.12ms
step:1856/1900 train_time:113461ms step_avg:61.13ms
step:1857/1900 train_time:113550ms step_avg:61.15ms
step:1858/1900 train_time:113638ms step_avg:61.16ms
step:1859/1900 train_time:113727ms step_avg:61.18ms
step:1860/1900 train_time:113813ms step_avg:61.19ms
step:1861/1900 train_time:113902ms step_avg:61.20ms
step:1862/1900 train_time:113989ms step_avg:61.22ms
step:1863/1900 train_time:114078ms step_avg:61.23ms
step:1864/1900 train_time:114165ms step_avg:61.25ms
step:1865/1900 train_time:114254ms step_avg:61.26ms
step:1866/1900 train_time:114342ms step_avg:61.28ms
step:1867/1900 train_time:114432ms step_avg:61.29ms
step:1868/1900 train_time:114520ms step_avg:61.31ms
step:1869/1900 train_time:114609ms step_avg:61.32ms
step:1870/1900 train_time:114696ms step_avg:61.33ms
step:1871/1900 train_time:114785ms step_avg:61.35ms
step:1872/1900 train_time:114873ms step_avg:61.36ms
step:1873/1900 train_time:114962ms step_avg:61.38ms
step:1874/1900 train_time:115049ms step_avg:61.39ms
step:1875/1900 train_time:115137ms step_avg:61.41ms
step:1876/1900 train_time:115225ms step_avg:61.42ms
step:1877/1900 train_time:115314ms step_avg:61.44ms
step:1878/1900 train_time:115402ms step_avg:61.45ms
step:1879/1900 train_time:115492ms step_avg:61.46ms
step:1880/1900 train_time:115580ms step_avg:61.48ms
step:1881/1900 train_time:115669ms step_avg:61.49ms
step:1882/1900 train_time:115757ms step_avg:61.51ms
step:1883/1900 train_time:115845ms step_avg:61.52ms
step:1884/1900 train_time:115933ms step_avg:61.54ms
step:1885/1900 train_time:116021ms step_avg:61.55ms
step:1886/1900 train_time:116109ms step_avg:61.56ms
step:1887/1900 train_time:116197ms step_avg:61.58ms
step:1888/1900 train_time:116285ms step_avg:61.59ms
step:1889/1900 train_time:116375ms step_avg:61.61ms
step:1890/1900 train_time:116462ms step_avg:61.62ms
step:1891/1900 train_time:116552ms step_avg:61.64ms
step:1892/1900 train_time:116640ms step_avg:61.65ms
step:1893/1900 train_time:116729ms step_avg:61.66ms
step:1894/1900 train_time:116816ms step_avg:61.68ms
step:1895/1900 train_time:116905ms step_avg:61.69ms
step:1896/1900 train_time:116993ms step_avg:61.71ms
step:1897/1900 train_time:117082ms step_avg:61.72ms
step:1898/1900 train_time:117170ms step_avg:61.73ms
step:1899/1900 train_time:117259ms step_avg:61.75ms
step:1900/1900 train_time:117346ms step_avg:61.76ms
step:1900/1900 val_loss:3.2784 train_time:117438ms step_avg:61.81ms
peak memory allocated: 29709 MiB reserved: 44018 MiB

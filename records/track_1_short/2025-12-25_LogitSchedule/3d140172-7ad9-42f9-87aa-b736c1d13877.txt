import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:05:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   45C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     42243      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42244      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42245      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42246      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42247      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42248      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42249      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42250      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     42244      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     42245      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     42246      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     42247      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     42248      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     42249      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     42250      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8320 train_time:0ms step_avg:0.19ms
step:1/1900 train_time:77ms step_avg:77.05ms
step:2/1900 train_time:98ms step_avg:49.22ms
step:3/1900 train_time:126ms step_avg:41.96ms
step:4/1900 train_time:160ms step_avg:39.92ms
step:5/1900 train_time:193ms step_avg:38.67ms
step:6/1900 train_time:263ms step_avg:43.77ms
step:7/1900 train_time:297ms step_avg:42.43ms
step:8/1900 train_time:331ms step_avg:41.37ms
step:9/1900 train_time:365ms step_avg:40.53ms
step:10/1900 train_time:399ms step_avg:39.87ms
step:11/1900 train_time:433ms step_avg:39.32ms
step:12/1900 train_time:466ms step_avg:38.87ms
step:13/1900 train_time:500ms step_avg:38.49ms
step:14/1900 train_time:534ms step_avg:38.17ms
step:15/1900 train_time:569ms step_avg:37.91ms
step:16/1900 train_time:603ms step_avg:37.66ms
step:17/1900 train_time:637ms step_avg:37.46ms
step:18/1900 train_time:671ms step_avg:37.27ms
step:19/1900 train_time:705ms step_avg:37.09ms
step:20/1900 train_time:739ms step_avg:36.93ms
step:21/1900 train_time:773ms step_avg:36.79ms
step:22/1900 train_time:807ms step_avg:36.67ms
step:23/1900 train_time:841ms step_avg:36.55ms
step:24/1900 train_time:875ms step_avg:36.44ms
step:25/1900 train_time:909ms step_avg:36.35ms
step:26/1900 train_time:943ms step_avg:36.26ms
step:27/1900 train_time:977ms step_avg:36.17ms
step:28/1900 train_time:1011ms step_avg:36.09ms
step:29/1900 train_time:1045ms step_avg:36.02ms
step:30/1900 train_time:1079ms step_avg:35.95ms
step:31/1900 train_time:1112ms step_avg:35.89ms
step:32/1900 train_time:1146ms step_avg:35.82ms
step:33/1900 train_time:1181ms step_avg:35.79ms
step:34/1900 train_time:1215ms step_avg:35.75ms
step:35/1900 train_time:1251ms step_avg:35.73ms
step:36/1900 train_time:1285ms step_avg:35.69ms
step:37/1900 train_time:1320ms step_avg:35.66ms
step:38/1900 train_time:1354ms step_avg:35.62ms
step:39/1900 train_time:1388ms step_avg:35.59ms
step:40/1900 train_time:1422ms step_avg:35.55ms
step:41/1900 train_time:1456ms step_avg:35.51ms
step:42/1900 train_time:1490ms step_avg:35.48ms
step:43/1900 train_time:1525ms step_avg:35.45ms
step:44/1900 train_time:1559ms step_avg:35.43ms
step:45/1900 train_time:1593ms step_avg:35.39ms
step:46/1900 train_time:1627ms step_avg:35.37ms
step:47/1900 train_time:1661ms step_avg:35.33ms
step:48/1900 train_time:1695ms step_avg:35.31ms
step:49/1900 train_time:1728ms step_avg:35.27ms
step:50/1900 train_time:1763ms step_avg:35.25ms
step:51/1900 train_time:1797ms step_avg:35.23ms
step:52/1900 train_time:1831ms step_avg:35.21ms
step:53/1900 train_time:1865ms step_avg:35.18ms
step:54/1900 train_time:1899ms step_avg:35.16ms
step:55/1900 train_time:1933ms step_avg:35.14ms
step:56/1900 train_time:1967ms step_avg:35.12ms
step:57/1900 train_time:2001ms step_avg:35.10ms
step:58/1900 train_time:2035ms step_avg:35.08ms
step:59/1900 train_time:2069ms step_avg:35.06ms
step:60/1900 train_time:2103ms step_avg:35.05ms
step:61/1900 train_time:2137ms step_avg:35.03ms
step:62/1900 train_time:2171ms step_avg:35.01ms
step:63/1900 train_time:2205ms step_avg:35.00ms
step:64/1900 train_time:2239ms step_avg:34.99ms
step:65/1900 train_time:2273ms step_avg:34.98ms
step:66/1900 train_time:2308ms step_avg:34.97ms
step:67/1900 train_time:2342ms step_avg:34.96ms
step:68/1900 train_time:2376ms step_avg:34.94ms
step:69/1900 train_time:2411ms step_avg:34.94ms
step:70/1900 train_time:2445ms step_avg:34.92ms
step:71/1900 train_time:2479ms step_avg:34.92ms
step:72/1900 train_time:2513ms step_avg:34.90ms
step:73/1900 train_time:2547ms step_avg:34.89ms
step:74/1900 train_time:2581ms step_avg:34.88ms
step:75/1900 train_time:2616ms step_avg:34.88ms
step:76/1900 train_time:2650ms step_avg:34.86ms
step:77/1900 train_time:2684ms step_avg:34.86ms
step:78/1900 train_time:2718ms step_avg:34.85ms
step:79/1900 train_time:2752ms step_avg:34.84ms
step:80/1900 train_time:2786ms step_avg:34.83ms
step:81/1900 train_time:2820ms step_avg:34.82ms
step:82/1900 train_time:2854ms step_avg:34.81ms
step:83/1900 train_time:2888ms step_avg:34.80ms
step:84/1900 train_time:2922ms step_avg:34.79ms
step:85/1900 train_time:2956ms step_avg:34.78ms
step:86/1900 train_time:2990ms step_avg:34.77ms
step:87/1900 train_time:3024ms step_avg:34.76ms
step:88/1900 train_time:3058ms step_avg:34.75ms
step:89/1900 train_time:3092ms step_avg:34.74ms
step:90/1900 train_time:3126ms step_avg:34.74ms
step:91/1900 train_time:3160ms step_avg:34.73ms
step:92/1900 train_time:3194ms step_avg:34.72ms
step:93/1900 train_time:3228ms step_avg:34.71ms
step:94/1900 train_time:3262ms step_avg:34.70ms
step:95/1900 train_time:3296ms step_avg:34.70ms
step:96/1900 train_time:3330ms step_avg:34.69ms
step:97/1900 train_time:3365ms step_avg:34.69ms
step:98/1900 train_time:3399ms step_avg:34.68ms
step:99/1900 train_time:3433ms step_avg:34.68ms
step:100/1900 train_time:3467ms step_avg:34.67ms
step:101/1900 train_time:3501ms step_avg:34.66ms
step:102/1900 train_time:3535ms step_avg:34.66ms
step:103/1900 train_time:3569ms step_avg:34.65ms
step:104/1900 train_time:3603ms step_avg:34.65ms
step:105/1900 train_time:3638ms step_avg:34.64ms
step:106/1900 train_time:3671ms step_avg:34.64ms
step:107/1900 train_time:3706ms step_avg:34.63ms
step:108/1900 train_time:3740ms step_avg:34.63ms
step:109/1900 train_time:3774ms step_avg:34.62ms
step:110/1900 train_time:3808ms step_avg:34.62ms
step:111/1900 train_time:3842ms step_avg:34.61ms
step:112/1900 train_time:3876ms step_avg:34.61ms
step:113/1900 train_time:3910ms step_avg:34.60ms
step:114/1900 train_time:3944ms step_avg:34.59ms
step:115/1900 train_time:3978ms step_avg:34.59ms
step:116/1900 train_time:4012ms step_avg:34.59ms
step:117/1900 train_time:4046ms step_avg:34.58ms
step:118/1900 train_time:4080ms step_avg:34.58ms
step:119/1900 train_time:4114ms step_avg:34.57ms
step:120/1900 train_time:4148ms step_avg:34.57ms
step:121/1900 train_time:4182ms step_avg:34.56ms
step:122/1900 train_time:4216ms step_avg:34.56ms
step:123/1900 train_time:4250ms step_avg:34.55ms
step:124/1900 train_time:4284ms step_avg:34.55ms
step:125/1900 train_time:4318ms step_avg:34.55ms
step:126/1900 train_time:4352ms step_avg:34.54ms
step:127/1900 train_time:4386ms step_avg:34.54ms
step:128/1900 train_time:4420ms step_avg:34.53ms
step:129/1900 train_time:4454ms step_avg:34.53ms
step:130/1900 train_time:4488ms step_avg:34.53ms
step:131/1900 train_time:4522ms step_avg:34.52ms
step:132/1900 train_time:4556ms step_avg:34.52ms
step:133/1900 train_time:4590ms step_avg:34.51ms
step:134/1900 train_time:4624ms step_avg:34.51ms
step:135/1900 train_time:4658ms step_avg:34.51ms
step:136/1900 train_time:4692ms step_avg:34.50ms
step:137/1900 train_time:4727ms step_avg:34.50ms
step:138/1900 train_time:4761ms step_avg:34.50ms
step:139/1900 train_time:4795ms step_avg:34.49ms
step:140/1900 train_time:4828ms step_avg:34.49ms
step:141/1900 train_time:4862ms step_avg:34.48ms
step:142/1900 train_time:4896ms step_avg:34.48ms
step:143/1900 train_time:4930ms step_avg:34.48ms
step:144/1900 train_time:4964ms step_avg:34.47ms
step:145/1900 train_time:4998ms step_avg:34.47ms
step:146/1900 train_time:5032ms step_avg:34.47ms
step:147/1900 train_time:5067ms step_avg:34.47ms
step:148/1900 train_time:5100ms step_avg:34.46ms
step:149/1900 train_time:5134ms step_avg:34.46ms
step:150/1900 train_time:5168ms step_avg:34.46ms
step:151/1900 train_time:5202ms step_avg:34.45ms
step:152/1900 train_time:5236ms step_avg:34.45ms
step:153/1900 train_time:5270ms step_avg:34.45ms
step:154/1900 train_time:5304ms step_avg:34.44ms
step:155/1900 train_time:5338ms step_avg:34.44ms
step:156/1900 train_time:5372ms step_avg:34.44ms
step:157/1900 train_time:5406ms step_avg:34.44ms
step:158/1900 train_time:5440ms step_avg:34.43ms
step:159/1900 train_time:5474ms step_avg:34.43ms
step:160/1900 train_time:5508ms step_avg:34.43ms
step:161/1900 train_time:5543ms step_avg:34.43ms
step:162/1900 train_time:5576ms step_avg:34.42ms
step:163/1900 train_time:5611ms step_avg:34.42ms
step:164/1900 train_time:5644ms step_avg:34.42ms
step:165/1900 train_time:5679ms step_avg:34.42ms
step:166/1900 train_time:5713ms step_avg:34.41ms
step:167/1900 train_time:5747ms step_avg:34.41ms
step:168/1900 train_time:5781ms step_avg:34.41ms
step:169/1900 train_time:5815ms step_avg:34.41ms
step:170/1900 train_time:5848ms step_avg:34.40ms
step:171/1900 train_time:5883ms step_avg:34.40ms
step:172/1900 train_time:5916ms step_avg:34.40ms
step:173/1900 train_time:5951ms step_avg:34.40ms
step:174/1900 train_time:5985ms step_avg:34.40ms
step:175/1900 train_time:6019ms step_avg:34.39ms
step:176/1900 train_time:6053ms step_avg:34.39ms
step:177/1900 train_time:6087ms step_avg:34.39ms
step:178/1900 train_time:6120ms step_avg:34.38ms
step:179/1900 train_time:6155ms step_avg:34.39ms
step:180/1900 train_time:6189ms step_avg:34.38ms
step:181/1900 train_time:6223ms step_avg:34.38ms
step:182/1900 train_time:6257ms step_avg:34.38ms
step:183/1900 train_time:6291ms step_avg:34.38ms
step:184/1900 train_time:6325ms step_avg:34.37ms
step:185/1900 train_time:6359ms step_avg:34.37ms
step:186/1900 train_time:6393ms step_avg:34.37ms
step:187/1900 train_time:6427ms step_avg:34.37ms
step:188/1900 train_time:6461ms step_avg:34.37ms
step:189/1900 train_time:6495ms step_avg:34.37ms
step:190/1900 train_time:6529ms step_avg:34.36ms
step:191/1900 train_time:6563ms step_avg:34.36ms
step:192/1900 train_time:6597ms step_avg:34.36ms
step:193/1900 train_time:6632ms step_avg:34.36ms
step:194/1900 train_time:6665ms step_avg:34.36ms
step:195/1900 train_time:6700ms step_avg:34.36ms
step:196/1900 train_time:6733ms step_avg:34.35ms
step:197/1900 train_time:6768ms step_avg:34.35ms
step:198/1900 train_time:6802ms step_avg:34.35ms
step:199/1900 train_time:6836ms step_avg:34.35ms
step:200/1900 train_time:6869ms step_avg:34.35ms
step:201/1900 train_time:6903ms step_avg:34.35ms
step:202/1900 train_time:6937ms step_avg:34.34ms
step:203/1900 train_time:6972ms step_avg:34.34ms
step:204/1900 train_time:7006ms step_avg:34.34ms
step:205/1900 train_time:7040ms step_avg:34.34ms
step:206/1900 train_time:7074ms step_avg:34.34ms
step:207/1900 train_time:7108ms step_avg:34.34ms
step:208/1900 train_time:7142ms step_avg:34.34ms
step:209/1900 train_time:7176ms step_avg:34.33ms
step:210/1900 train_time:7210ms step_avg:34.33ms
step:211/1900 train_time:7244ms step_avg:34.33ms
step:212/1900 train_time:7278ms step_avg:34.33ms
step:213/1900 train_time:7312ms step_avg:34.33ms
step:214/1900 train_time:7346ms step_avg:34.33ms
step:215/1900 train_time:7380ms step_avg:34.32ms
step:216/1900 train_time:7413ms step_avg:34.32ms
step:217/1900 train_time:7448ms step_avg:34.32ms
step:218/1900 train_time:7482ms step_avg:34.32ms
step:219/1900 train_time:7516ms step_avg:34.32ms
step:220/1900 train_time:7550ms step_avg:34.32ms
step:221/1900 train_time:7584ms step_avg:34.32ms
step:222/1900 train_time:7618ms step_avg:34.32ms
step:223/1900 train_time:7652ms step_avg:34.31ms
step:224/1900 train_time:7686ms step_avg:34.31ms
step:225/1900 train_time:7720ms step_avg:34.31ms
step:226/1900 train_time:7754ms step_avg:34.31ms
step:227/1900 train_time:7788ms step_avg:34.31ms
step:228/1900 train_time:7822ms step_avg:34.31ms
step:229/1900 train_time:7856ms step_avg:34.31ms
step:230/1900 train_time:7890ms step_avg:34.31ms
step:231/1900 train_time:7925ms step_avg:34.31ms
step:232/1900 train_time:7958ms step_avg:34.30ms
step:233/1900 train_time:7993ms step_avg:34.30ms
step:234/1900 train_time:8027ms step_avg:34.30ms
step:235/1900 train_time:8061ms step_avg:34.30ms
step:236/1900 train_time:8095ms step_avg:34.30ms
step:237/1900 train_time:8129ms step_avg:34.30ms
step:238/1900 train_time:8163ms step_avg:34.30ms
step:239/1900 train_time:8197ms step_avg:34.30ms
step:240/1900 train_time:8231ms step_avg:34.30ms
step:241/1900 train_time:8265ms step_avg:34.30ms
step:242/1900 train_time:8299ms step_avg:34.29ms
step:243/1900 train_time:8333ms step_avg:34.29ms
step:244/1900 train_time:8367ms step_avg:34.29ms
step:245/1900 train_time:8401ms step_avg:34.29ms
step:246/1900 train_time:8435ms step_avg:34.29ms
step:247/1900 train_time:8469ms step_avg:34.29ms
step:248/1900 train_time:8503ms step_avg:34.29ms
step:249/1900 train_time:8537ms step_avg:34.28ms
step:250/1900 train_time:8571ms step_avg:34.28ms
step:250/1900 val_loss:4.6066 train_time:8607ms step_avg:34.43ms
step:251/1900 train_time:8627ms step_avg:34.37ms
step:252/1900 train_time:8647ms step_avg:34.31ms
step:253/1900 train_time:8676ms step_avg:34.29ms
step:254/1900 train_time:8710ms step_avg:34.29ms
step:255/1900 train_time:8746ms step_avg:34.30ms
step:256/1900 train_time:8781ms step_avg:34.30ms
step:257/1900 train_time:8815ms step_avg:34.30ms
step:258/1900 train_time:8849ms step_avg:34.30ms
step:259/1900 train_time:8883ms step_avg:34.30ms
step:260/1900 train_time:8917ms step_avg:34.30ms
step:261/1900 train_time:8951ms step_avg:34.30ms
step:262/1900 train_time:8985ms step_avg:34.29ms
step:263/1900 train_time:9019ms step_avg:34.29ms
step:264/1900 train_time:9053ms step_avg:34.29ms
step:265/1900 train_time:9087ms step_avg:34.29ms
step:266/1900 train_time:9120ms step_avg:34.29ms
step:267/1900 train_time:9154ms step_avg:34.29ms
step:268/1900 train_time:9188ms step_avg:34.28ms
step:269/1900 train_time:9222ms step_avg:34.28ms
step:270/1900 train_time:9256ms step_avg:34.28ms
step:271/1900 train_time:9290ms step_avg:34.28ms
step:272/1900 train_time:9323ms step_avg:34.28ms
step:273/1900 train_time:9357ms step_avg:34.28ms
step:274/1900 train_time:9391ms step_avg:34.27ms
step:275/1900 train_time:9425ms step_avg:34.27ms
step:276/1900 train_time:9459ms step_avg:34.27ms
step:277/1900 train_time:9493ms step_avg:34.27ms
step:278/1900 train_time:9526ms step_avg:34.27ms
step:279/1900 train_time:9561ms step_avg:34.27ms
step:280/1900 train_time:9595ms step_avg:34.27ms
step:281/1900 train_time:9629ms step_avg:34.27ms
step:282/1900 train_time:9663ms step_avg:34.27ms
step:283/1900 train_time:9697ms step_avg:34.27ms
step:284/1900 train_time:9731ms step_avg:34.26ms
step:285/1900 train_time:9766ms step_avg:34.27ms
step:286/1900 train_time:9800ms step_avg:34.26ms
step:287/1900 train_time:9834ms step_avg:34.26ms
step:288/1900 train_time:9868ms step_avg:34.26ms
step:289/1900 train_time:9902ms step_avg:34.26ms
step:290/1900 train_time:9936ms step_avg:34.26ms
step:291/1900 train_time:9970ms step_avg:34.26ms
step:292/1900 train_time:10003ms step_avg:34.26ms
step:293/1900 train_time:10038ms step_avg:34.26ms
step:294/1900 train_time:10072ms step_avg:34.26ms
step:295/1900 train_time:10106ms step_avg:34.26ms
step:296/1900 train_time:10140ms step_avg:34.26ms
step:297/1900 train_time:10174ms step_avg:34.25ms
step:298/1900 train_time:10208ms step_avg:34.25ms
step:299/1900 train_time:10242ms step_avg:34.25ms
step:300/1900 train_time:10276ms step_avg:34.25ms
step:301/1900 train_time:10310ms step_avg:34.25ms
step:302/1900 train_time:10343ms step_avg:34.25ms
step:303/1900 train_time:10377ms step_avg:34.25ms
step:304/1900 train_time:10411ms step_avg:34.25ms
step:305/1900 train_time:10445ms step_avg:34.25ms
step:306/1900 train_time:10479ms step_avg:34.24ms
step:307/1900 train_time:10512ms step_avg:34.24ms
step:308/1900 train_time:10546ms step_avg:34.24ms
step:309/1900 train_time:10580ms step_avg:34.24ms
step:310/1900 train_time:10614ms step_avg:34.24ms
step:311/1900 train_time:10648ms step_avg:34.24ms
step:312/1900 train_time:10682ms step_avg:34.24ms
step:313/1900 train_time:10716ms step_avg:34.24ms
step:314/1900 train_time:10750ms step_avg:34.24ms
step:315/1900 train_time:10784ms step_avg:34.23ms
step:316/1900 train_time:10818ms step_avg:34.23ms
step:317/1900 train_time:10852ms step_avg:34.23ms
step:318/1900 train_time:10885ms step_avg:34.23ms
step:319/1900 train_time:10919ms step_avg:34.23ms
step:320/1900 train_time:10953ms step_avg:34.23ms
step:321/1900 train_time:10987ms step_avg:34.23ms
step:322/1900 train_time:11021ms step_avg:34.23ms
step:323/1900 train_time:11055ms step_avg:34.23ms
step:324/1900 train_time:11089ms step_avg:34.23ms
step:325/1900 train_time:11123ms step_avg:34.23ms
step:326/1900 train_time:11157ms step_avg:34.22ms
step:327/1900 train_time:11191ms step_avg:34.22ms
step:328/1900 train_time:11225ms step_avg:34.22ms
step:329/1900 train_time:11259ms step_avg:34.22ms
step:330/1900 train_time:11293ms step_avg:34.22ms
step:331/1900 train_time:11327ms step_avg:34.22ms
step:332/1900 train_time:11361ms step_avg:34.22ms
step:333/1900 train_time:11395ms step_avg:34.22ms
step:334/1900 train_time:11429ms step_avg:34.22ms
step:335/1900 train_time:11463ms step_avg:34.22ms
step:336/1900 train_time:11497ms step_avg:34.22ms
step:337/1900 train_time:11531ms step_avg:34.22ms
step:338/1900 train_time:11565ms step_avg:34.22ms
step:339/1900 train_time:11599ms step_avg:34.22ms
step:340/1900 train_time:11633ms step_avg:34.21ms
step:341/1900 train_time:11667ms step_avg:34.21ms
step:342/1900 train_time:11701ms step_avg:34.21ms
step:343/1900 train_time:11735ms step_avg:34.21ms
step:344/1900 train_time:11769ms step_avg:34.21ms
step:345/1900 train_time:11803ms step_avg:34.21ms
step:346/1900 train_time:11837ms step_avg:34.21ms
step:347/1900 train_time:11871ms step_avg:34.21ms
step:348/1900 train_time:11905ms step_avg:34.21ms
step:349/1900 train_time:11939ms step_avg:34.21ms
step:350/1900 train_time:11973ms step_avg:34.21ms
step:351/1900 train_time:12007ms step_avg:34.21ms
step:352/1900 train_time:12041ms step_avg:34.21ms
step:353/1900 train_time:12075ms step_avg:34.21ms
step:354/1900 train_time:12109ms step_avg:34.21ms
step:355/1900 train_time:12143ms step_avg:34.21ms
step:356/1900 train_time:12177ms step_avg:34.20ms
step:357/1900 train_time:12211ms step_avg:34.20ms
step:358/1900 train_time:12245ms step_avg:34.20ms
step:359/1900 train_time:12279ms step_avg:34.20ms
step:360/1900 train_time:12312ms step_avg:34.20ms
step:361/1900 train_time:12346ms step_avg:34.20ms
step:362/1900 train_time:12380ms step_avg:34.20ms
step:363/1900 train_time:12414ms step_avg:34.20ms
step:364/1900 train_time:12448ms step_avg:34.20ms
step:365/1900 train_time:12482ms step_avg:34.20ms
step:366/1900 train_time:12516ms step_avg:34.20ms
step:367/1900 train_time:12550ms step_avg:34.20ms
step:368/1900 train_time:12584ms step_avg:34.20ms
step:369/1900 train_time:12619ms step_avg:34.20ms
step:370/1900 train_time:12652ms step_avg:34.20ms
step:371/1900 train_time:12686ms step_avg:34.20ms
step:372/1900 train_time:12720ms step_avg:34.19ms
step:373/1900 train_time:12754ms step_avg:34.19ms
step:374/1900 train_time:12788ms step_avg:34.19ms
step:375/1900 train_time:12822ms step_avg:34.19ms
step:376/1900 train_time:12856ms step_avg:34.19ms
step:377/1900 train_time:12890ms step_avg:34.19ms
step:378/1900 train_time:12924ms step_avg:34.19ms
step:379/1900 train_time:12958ms step_avg:34.19ms
step:380/1900 train_time:12992ms step_avg:34.19ms
step:381/1900 train_time:13026ms step_avg:34.19ms
step:382/1900 train_time:13060ms step_avg:34.19ms
step:383/1900 train_time:13094ms step_avg:34.19ms
step:384/1900 train_time:13128ms step_avg:34.19ms
step:385/1900 train_time:13162ms step_avg:34.19ms
step:386/1900 train_time:13196ms step_avg:34.19ms
step:387/1900 train_time:13230ms step_avg:34.19ms
step:388/1900 train_time:13264ms step_avg:34.19ms
step:389/1900 train_time:13298ms step_avg:34.19ms
step:390/1900 train_time:13332ms step_avg:34.18ms
step:391/1900 train_time:13367ms step_avg:34.19ms
step:392/1900 train_time:13400ms step_avg:34.18ms
step:393/1900 train_time:13434ms step_avg:34.18ms
step:394/1900 train_time:13468ms step_avg:34.18ms
step:395/1900 train_time:13502ms step_avg:34.18ms
step:396/1900 train_time:13536ms step_avg:34.18ms
step:397/1900 train_time:13570ms step_avg:34.18ms
step:398/1900 train_time:13604ms step_avg:34.18ms
step:399/1900 train_time:13638ms step_avg:34.18ms
step:400/1900 train_time:13672ms step_avg:34.18ms
step:401/1900 train_time:13706ms step_avg:34.18ms
step:402/1900 train_time:13739ms step_avg:34.18ms
step:403/1900 train_time:13773ms step_avg:34.18ms
step:404/1900 train_time:13807ms step_avg:34.18ms
step:405/1900 train_time:13842ms step_avg:34.18ms
step:406/1900 train_time:13876ms step_avg:34.18ms
step:407/1900 train_time:13910ms step_avg:34.18ms
step:408/1900 train_time:13944ms step_avg:34.18ms
step:409/1900 train_time:13978ms step_avg:34.18ms
step:410/1900 train_time:14012ms step_avg:34.18ms
step:411/1900 train_time:14046ms step_avg:34.17ms
step:412/1900 train_time:14080ms step_avg:34.17ms
step:413/1900 train_time:14114ms step_avg:34.17ms
step:414/1900 train_time:14148ms step_avg:34.17ms
step:415/1900 train_time:14182ms step_avg:34.17ms
step:416/1900 train_time:14216ms step_avg:34.17ms
step:417/1900 train_time:14250ms step_avg:34.17ms
step:418/1900 train_time:14284ms step_avg:34.17ms
step:419/1900 train_time:14318ms step_avg:34.17ms
step:420/1900 train_time:14352ms step_avg:34.17ms
step:421/1900 train_time:14386ms step_avg:34.17ms
step:422/1900 train_time:14420ms step_avg:34.17ms
step:423/1900 train_time:14454ms step_avg:34.17ms
step:424/1900 train_time:14488ms step_avg:34.17ms
step:425/1900 train_time:14522ms step_avg:34.17ms
step:426/1900 train_time:14556ms step_avg:34.17ms
step:427/1900 train_time:14590ms step_avg:34.17ms
step:428/1900 train_time:14624ms step_avg:34.17ms
step:429/1900 train_time:14658ms step_avg:34.17ms
step:430/1900 train_time:14692ms step_avg:34.17ms
step:431/1900 train_time:14725ms step_avg:34.17ms
step:432/1900 train_time:14759ms step_avg:34.16ms
step:433/1900 train_time:14793ms step_avg:34.16ms
step:434/1900 train_time:14827ms step_avg:34.16ms
step:435/1900 train_time:14861ms step_avg:34.16ms
step:436/1900 train_time:14895ms step_avg:34.16ms
step:437/1900 train_time:14929ms step_avg:34.16ms
step:438/1900 train_time:14963ms step_avg:34.16ms
step:439/1900 train_time:14998ms step_avg:34.16ms
step:440/1900 train_time:15031ms step_avg:34.16ms
step:441/1900 train_time:15066ms step_avg:34.16ms
step:442/1900 train_time:15100ms step_avg:34.16ms
step:443/1900 train_time:15134ms step_avg:34.16ms
step:444/1900 train_time:15168ms step_avg:34.16ms
step:445/1900 train_time:15202ms step_avg:34.16ms
step:446/1900 train_time:15236ms step_avg:34.16ms
step:447/1900 train_time:15270ms step_avg:34.16ms
step:448/1900 train_time:15304ms step_avg:34.16ms
step:449/1900 train_time:15337ms step_avg:34.16ms
step:450/1900 train_time:15371ms step_avg:34.16ms
step:451/1900 train_time:15405ms step_avg:34.16ms
step:452/1900 train_time:15439ms step_avg:34.16ms
step:453/1900 train_time:15473ms step_avg:34.16ms
step:454/1900 train_time:15508ms step_avg:34.16ms
step:455/1900 train_time:15542ms step_avg:34.16ms
step:456/1900 train_time:15575ms step_avg:34.16ms
step:457/1900 train_time:15610ms step_avg:34.16ms
step:458/1900 train_time:15644ms step_avg:34.16ms
step:459/1900 train_time:15678ms step_avg:34.16ms
step:460/1900 train_time:15711ms step_avg:34.16ms
step:461/1900 train_time:15746ms step_avg:34.16ms
step:462/1900 train_time:15780ms step_avg:34.15ms
step:463/1900 train_time:15814ms step_avg:34.15ms
step:464/1900 train_time:15847ms step_avg:34.15ms
step:465/1900 train_time:15882ms step_avg:34.15ms
step:466/1900 train_time:15916ms step_avg:34.15ms
step:467/1900 train_time:15950ms step_avg:34.15ms
step:468/1900 train_time:15983ms step_avg:34.15ms
step:469/1900 train_time:16017ms step_avg:34.15ms
step:470/1900 train_time:16051ms step_avg:34.15ms
step:471/1900 train_time:16085ms step_avg:34.15ms
step:472/1900 train_time:16119ms step_avg:34.15ms
step:473/1900 train_time:16153ms step_avg:34.15ms
step:474/1900 train_time:16187ms step_avg:34.15ms
step:475/1900 train_time:16221ms step_avg:34.15ms
step:476/1900 train_time:16255ms step_avg:34.15ms
step:477/1900 train_time:16289ms step_avg:34.15ms
step:478/1900 train_time:16323ms step_avg:34.15ms
step:479/1900 train_time:16357ms step_avg:34.15ms
step:480/1900 train_time:16391ms step_avg:34.15ms
step:481/1900 train_time:16425ms step_avg:34.15ms
step:482/1900 train_time:16459ms step_avg:34.15ms
step:483/1900 train_time:16493ms step_avg:34.15ms
step:484/1900 train_time:16527ms step_avg:34.15ms
step:485/1900 train_time:16561ms step_avg:34.15ms
step:486/1900 train_time:16595ms step_avg:34.15ms
step:487/1900 train_time:16629ms step_avg:34.15ms
step:488/1900 train_time:16663ms step_avg:34.14ms
step:489/1900 train_time:16697ms step_avg:34.14ms
step:490/1900 train_time:16730ms step_avg:34.14ms
step:491/1900 train_time:16764ms step_avg:34.14ms
step:492/1900 train_time:16798ms step_avg:34.14ms
step:493/1900 train_time:16832ms step_avg:34.14ms
step:494/1900 train_time:16866ms step_avg:34.14ms
step:495/1900 train_time:16900ms step_avg:34.14ms
step:496/1900 train_time:16934ms step_avg:34.14ms
step:497/1900 train_time:16968ms step_avg:34.14ms
step:498/1900 train_time:17002ms step_avg:34.14ms
step:499/1900 train_time:17036ms step_avg:34.14ms
step:500/1900 train_time:17070ms step_avg:34.14ms
step:500/1900 val_loss:4.2766 train_time:17107ms step_avg:34.21ms
step:501/1900 train_time:17127ms step_avg:34.18ms
step:502/1900 train_time:17146ms step_avg:34.15ms
step:503/1900 train_time:17177ms step_avg:34.15ms
step:504/1900 train_time:17211ms step_avg:34.15ms
step:505/1900 train_time:17247ms step_avg:34.15ms
step:506/1900 train_time:17281ms step_avg:34.15ms
step:507/1900 train_time:17316ms step_avg:34.15ms
step:508/1900 train_time:17350ms step_avg:34.15ms
step:509/1900 train_time:17384ms step_avg:34.15ms
step:510/1900 train_time:17418ms step_avg:34.15ms
step:511/1900 train_time:17452ms step_avg:34.15ms
step:512/1900 train_time:17486ms step_avg:34.15ms
step:513/1900 train_time:17520ms step_avg:34.15ms
step:514/1900 train_time:17554ms step_avg:34.15ms
step:515/1900 train_time:17588ms step_avg:34.15ms
step:516/1900 train_time:17622ms step_avg:34.15ms
step:517/1900 train_time:17656ms step_avg:34.15ms
step:518/1900 train_time:17690ms step_avg:34.15ms
step:519/1900 train_time:17723ms step_avg:34.15ms
step:520/1900 train_time:17757ms step_avg:34.15ms
step:521/1900 train_time:17791ms step_avg:34.15ms
step:522/1900 train_time:17825ms step_avg:34.15ms
step:523/1900 train_time:17859ms step_avg:34.15ms
step:524/1900 train_time:17893ms step_avg:34.15ms
step:525/1900 train_time:17926ms step_avg:34.15ms
step:526/1900 train_time:17960ms step_avg:34.15ms
step:527/1900 train_time:17994ms step_avg:34.14ms
step:528/1900 train_time:18028ms step_avg:34.14ms
step:529/1900 train_time:18063ms step_avg:34.14ms
step:530/1900 train_time:18096ms step_avg:34.14ms
step:531/1900 train_time:18131ms step_avg:34.14ms
step:532/1900 train_time:18165ms step_avg:34.14ms
step:533/1900 train_time:18199ms step_avg:34.14ms
step:534/1900 train_time:18233ms step_avg:34.14ms
step:535/1900 train_time:18268ms step_avg:34.14ms
step:536/1900 train_time:18301ms step_avg:34.14ms
step:537/1900 train_time:18336ms step_avg:34.14ms
step:538/1900 train_time:18369ms step_avg:34.14ms
step:539/1900 train_time:18404ms step_avg:34.14ms
step:540/1900 train_time:18437ms step_avg:34.14ms
step:541/1900 train_time:18472ms step_avg:34.14ms
step:542/1900 train_time:18506ms step_avg:34.14ms
step:543/1900 train_time:18540ms step_avg:34.14ms
step:544/1900 train_time:18574ms step_avg:34.14ms
step:545/1900 train_time:18608ms step_avg:34.14ms
step:546/1900 train_time:18642ms step_avg:34.14ms
step:547/1900 train_time:18676ms step_avg:34.14ms
step:548/1900 train_time:18710ms step_avg:34.14ms
step:549/1900 train_time:18743ms step_avg:34.14ms
step:550/1900 train_time:18777ms step_avg:34.14ms
step:551/1900 train_time:18812ms step_avg:34.14ms
step:552/1900 train_time:18845ms step_avg:34.14ms
step:553/1900 train_time:18879ms step_avg:34.14ms
step:554/1900 train_time:18913ms step_avg:34.14ms
step:555/1900 train_time:18947ms step_avg:34.14ms
step:556/1900 train_time:18981ms step_avg:34.14ms
step:557/1900 train_time:19015ms step_avg:34.14ms
step:558/1900 train_time:19049ms step_avg:34.14ms
step:559/1900 train_time:19083ms step_avg:34.14ms
step:560/1900 train_time:19117ms step_avg:34.14ms
step:561/1900 train_time:19151ms step_avg:34.14ms
step:562/1900 train_time:19185ms step_avg:34.14ms
step:563/1900 train_time:19219ms step_avg:34.14ms
step:564/1900 train_time:19253ms step_avg:34.14ms
step:565/1900 train_time:19287ms step_avg:34.14ms
step:566/1900 train_time:19321ms step_avg:34.14ms
step:567/1900 train_time:19356ms step_avg:34.14ms
step:568/1900 train_time:19390ms step_avg:34.14ms
step:569/1900 train_time:19424ms step_avg:34.14ms
step:570/1900 train_time:19458ms step_avg:34.14ms
step:571/1900 train_time:19492ms step_avg:34.14ms
step:572/1900 train_time:19526ms step_avg:34.14ms
step:573/1900 train_time:19560ms step_avg:34.14ms
step:574/1900 train_time:19594ms step_avg:34.14ms
step:575/1900 train_time:19628ms step_avg:34.14ms
step:576/1900 train_time:19662ms step_avg:34.14ms
step:577/1900 train_time:19696ms step_avg:34.13ms
step:578/1900 train_time:19730ms step_avg:34.13ms
step:579/1900 train_time:19764ms step_avg:34.13ms
step:580/1900 train_time:19797ms step_avg:34.13ms
step:581/1900 train_time:19831ms step_avg:34.13ms
step:582/1900 train_time:19865ms step_avg:34.13ms
step:583/1900 train_time:19899ms step_avg:34.13ms
step:584/1900 train_time:19933ms step_avg:34.13ms
step:585/1900 train_time:19967ms step_avg:34.13ms
step:586/1900 train_time:20001ms step_avg:34.13ms
step:587/1900 train_time:20035ms step_avg:34.13ms
step:588/1900 train_time:20069ms step_avg:34.13ms
step:589/1900 train_time:20103ms step_avg:34.13ms
step:590/1900 train_time:20137ms step_avg:34.13ms
step:591/1900 train_time:20171ms step_avg:34.13ms
step:592/1900 train_time:20205ms step_avg:34.13ms
step:593/1900 train_time:20239ms step_avg:34.13ms
step:594/1900 train_time:20273ms step_avg:34.13ms
step:595/1900 train_time:20307ms step_avg:34.13ms
step:596/1900 train_time:20341ms step_avg:34.13ms
step:597/1900 train_time:20375ms step_avg:34.13ms
step:598/1900 train_time:20409ms step_avg:34.13ms
step:599/1900 train_time:20443ms step_avg:34.13ms
step:600/1900 train_time:20477ms step_avg:34.13ms
step:601/1900 train_time:20512ms step_avg:34.13ms
step:602/1900 train_time:20545ms step_avg:34.13ms
step:603/1900 train_time:20580ms step_avg:34.13ms
step:604/1900 train_time:20614ms step_avg:34.13ms
step:605/1900 train_time:20648ms step_avg:34.13ms
step:606/1900 train_time:20681ms step_avg:34.13ms
step:607/1900 train_time:20716ms step_avg:34.13ms
step:608/1900 train_time:20750ms step_avg:34.13ms
step:609/1900 train_time:20784ms step_avg:34.13ms
step:610/1900 train_time:20817ms step_avg:34.13ms
step:611/1900 train_time:20852ms step_avg:34.13ms
step:612/1900 train_time:20886ms step_avg:34.13ms
step:613/1900 train_time:20920ms step_avg:34.13ms
step:614/1900 train_time:20954ms step_avg:34.13ms
step:615/1900 train_time:20988ms step_avg:34.13ms
step:616/1900 train_time:21022ms step_avg:34.13ms
step:617/1900 train_time:21055ms step_avg:34.13ms
step:618/1900 train_time:21089ms step_avg:34.13ms
step:619/1900 train_time:21123ms step_avg:34.12ms
step:620/1900 train_time:21157ms step_avg:34.12ms
step:621/1900 train_time:21192ms step_avg:34.13ms
step:622/1900 train_time:21252ms step_avg:34.17ms
step:623/1900 train_time:21314ms step_avg:34.21ms
step:624/1900 train_time:21375ms step_avg:34.26ms
step:625/1900 train_time:21437ms step_avg:34.30ms
step:626/1900 train_time:21498ms step_avg:34.34ms
step:627/1900 train_time:21560ms step_avg:34.39ms
step:628/1900 train_time:21621ms step_avg:34.43ms
step:629/1900 train_time:21684ms step_avg:34.47ms
step:630/1900 train_time:21746ms step_avg:34.52ms
step:631/1900 train_time:21808ms step_avg:34.56ms
step:632/1900 train_time:21869ms step_avg:34.60ms
step:633/1900 train_time:21932ms step_avg:34.65ms
step:634/1900 train_time:21993ms step_avg:34.69ms
step:635/1900 train_time:22054ms step_avg:34.73ms
step:636/1900 train_time:22115ms step_avg:34.77ms
step:637/1900 train_time:22177ms step_avg:34.81ms
step:638/1900 train_time:22238ms step_avg:34.86ms
step:639/1900 train_time:22300ms step_avg:34.90ms
step:640/1900 train_time:22361ms step_avg:34.94ms
step:641/1900 train_time:22422ms step_avg:34.98ms
step:642/1900 train_time:22484ms step_avg:35.02ms
step:643/1900 train_time:22546ms step_avg:35.06ms
step:644/1900 train_time:22607ms step_avg:35.10ms
step:645/1900 train_time:22669ms step_avg:35.15ms
step:646/1900 train_time:22730ms step_avg:35.19ms
step:647/1900 train_time:22792ms step_avg:35.23ms
step:648/1900 train_time:22853ms step_avg:35.27ms
step:649/1900 train_time:22915ms step_avg:35.31ms
step:650/1900 train_time:22976ms step_avg:35.35ms
step:651/1900 train_time:23038ms step_avg:35.39ms
step:652/1900 train_time:23099ms step_avg:35.43ms
step:653/1900 train_time:23162ms step_avg:35.47ms
step:654/1900 train_time:23223ms step_avg:35.51ms
step:655/1900 train_time:23285ms step_avg:35.55ms
step:656/1900 train_time:23346ms step_avg:35.59ms
step:657/1900 train_time:23408ms step_avg:35.63ms
step:658/1900 train_time:23469ms step_avg:35.67ms
step:659/1900 train_time:23530ms step_avg:35.71ms
step:660/1900 train_time:23591ms step_avg:35.74ms
step:661/1900 train_time:23654ms step_avg:35.78ms
step:662/1900 train_time:23715ms step_avg:35.82ms
step:663/1900 train_time:23777ms step_avg:35.86ms
step:664/1900 train_time:23838ms step_avg:35.90ms
step:665/1900 train_time:23900ms step_avg:35.94ms
step:666/1900 train_time:23962ms step_avg:35.98ms
step:667/1900 train_time:24024ms step_avg:36.02ms
step:668/1900 train_time:24085ms step_avg:36.06ms
step:669/1900 train_time:24147ms step_avg:36.09ms
step:670/1900 train_time:24208ms step_avg:36.13ms
step:671/1900 train_time:24270ms step_avg:36.17ms
step:672/1900 train_time:24331ms step_avg:36.21ms
step:673/1900 train_time:24392ms step_avg:36.24ms
step:674/1900 train_time:24453ms step_avg:36.28ms
step:675/1900 train_time:24515ms step_avg:36.32ms
step:676/1900 train_time:24576ms step_avg:36.35ms
step:677/1900 train_time:24637ms step_avg:36.39ms
step:678/1900 train_time:24698ms step_avg:36.43ms
step:679/1900 train_time:24760ms step_avg:36.47ms
step:680/1900 train_time:24822ms step_avg:36.50ms
step:681/1900 train_time:24884ms step_avg:36.54ms
step:682/1900 train_time:24945ms step_avg:36.58ms
step:683/1900 train_time:25007ms step_avg:36.61ms
step:684/1900 train_time:25068ms step_avg:36.65ms
step:685/1900 train_time:25130ms step_avg:36.69ms
step:686/1900 train_time:25191ms step_avg:36.72ms
step:687/1900 train_time:25253ms step_avg:36.76ms
step:688/1900 train_time:25313ms step_avg:36.79ms
step:689/1900 train_time:25375ms step_avg:36.83ms
step:690/1900 train_time:25436ms step_avg:36.86ms
step:691/1900 train_time:25498ms step_avg:36.90ms
step:692/1900 train_time:25559ms step_avg:36.94ms
step:693/1900 train_time:25621ms step_avg:36.97ms
step:694/1900 train_time:25682ms step_avg:37.01ms
step:695/1900 train_time:25744ms step_avg:37.04ms
step:696/1900 train_time:25805ms step_avg:37.08ms
step:697/1900 train_time:25867ms step_avg:37.11ms
step:698/1900 train_time:25928ms step_avg:37.15ms
step:699/1900 train_time:25990ms step_avg:37.18ms
step:700/1900 train_time:26051ms step_avg:37.22ms
step:701/1900 train_time:26112ms step_avg:37.25ms
step:702/1900 train_time:26173ms step_avg:37.28ms
step:703/1900 train_time:26235ms step_avg:37.32ms
step:704/1900 train_time:26296ms step_avg:37.35ms
step:705/1900 train_time:26358ms step_avg:37.39ms
step:706/1900 train_time:26419ms step_avg:37.42ms
step:707/1900 train_time:26481ms step_avg:37.46ms
step:708/1900 train_time:26542ms step_avg:37.49ms
step:709/1900 train_time:26604ms step_avg:37.52ms
step:710/1900 train_time:26665ms step_avg:37.56ms
step:711/1900 train_time:26727ms step_avg:37.59ms
step:712/1900 train_time:26788ms step_avg:37.62ms
step:713/1900 train_time:26849ms step_avg:37.66ms
step:714/1900 train_time:26910ms step_avg:37.69ms
step:715/1900 train_time:26972ms step_avg:37.72ms
step:716/1900 train_time:27033ms step_avg:37.76ms
step:717/1900 train_time:27095ms step_avg:37.79ms
step:718/1900 train_time:27156ms step_avg:37.82ms
step:719/1900 train_time:27218ms step_avg:37.86ms
step:720/1900 train_time:27280ms step_avg:37.89ms
step:721/1900 train_time:27342ms step_avg:37.92ms
step:722/1900 train_time:27403ms step_avg:37.95ms
step:723/1900 train_time:27465ms step_avg:37.99ms
step:724/1900 train_time:27526ms step_avg:38.02ms
step:725/1900 train_time:27588ms step_avg:38.05ms
step:726/1900 train_time:27649ms step_avg:38.08ms
step:727/1900 train_time:27710ms step_avg:38.12ms
step:728/1900 train_time:27771ms step_avg:38.15ms
step:729/1900 train_time:27833ms step_avg:38.18ms
step:730/1900 train_time:27895ms step_avg:38.21ms
step:731/1900 train_time:27957ms step_avg:38.24ms
step:732/1900 train_time:28018ms step_avg:38.28ms
step:733/1900 train_time:28079ms step_avg:38.31ms
step:734/1900 train_time:28140ms step_avg:38.34ms
step:735/1900 train_time:28202ms step_avg:38.37ms
step:736/1900 train_time:28264ms step_avg:38.40ms
step:737/1900 train_time:28326ms step_avg:38.43ms
step:738/1900 train_time:28387ms step_avg:38.47ms
step:739/1900 train_time:28450ms step_avg:38.50ms
step:740/1900 train_time:28510ms step_avg:38.53ms
step:741/1900 train_time:28572ms step_avg:38.56ms
step:742/1900 train_time:28632ms step_avg:38.59ms
step:743/1900 train_time:28694ms step_avg:38.62ms
step:744/1900 train_time:28755ms step_avg:38.65ms
step:745/1900 train_time:28817ms step_avg:38.68ms
step:746/1900 train_time:28879ms step_avg:38.71ms
step:747/1900 train_time:28941ms step_avg:38.74ms
step:748/1900 train_time:29002ms step_avg:38.77ms
step:749/1900 train_time:29064ms step_avg:38.80ms
step:750/1900 train_time:29125ms step_avg:38.83ms
step:750/1900 val_loss:4.0188 train_time:29190ms step_avg:38.92ms
step:751/1900 train_time:29210ms step_avg:38.89ms
step:752/1900 train_time:29252ms step_avg:38.90ms
step:753/1900 train_time:29315ms step_avg:38.93ms
step:754/1900 train_time:29377ms step_avg:38.96ms
step:755/1900 train_time:29439ms step_avg:38.99ms
step:756/1900 train_time:29500ms step_avg:39.02ms
step:757/1900 train_time:29562ms step_avg:39.05ms
step:758/1900 train_time:29623ms step_avg:39.08ms
step:759/1900 train_time:29684ms step_avg:39.11ms
step:760/1900 train_time:29745ms step_avg:39.14ms
step:761/1900 train_time:29806ms step_avg:39.17ms
step:762/1900 train_time:29867ms step_avg:39.20ms
step:763/1900 train_time:29928ms step_avg:39.22ms
step:764/1900 train_time:29989ms step_avg:39.25ms
step:765/1900 train_time:30050ms step_avg:39.28ms
step:766/1900 train_time:30111ms step_avg:39.31ms
step:767/1900 train_time:30174ms step_avg:39.34ms
step:768/1900 train_time:30235ms step_avg:39.37ms
step:769/1900 train_time:30298ms step_avg:39.40ms
step:770/1900 train_time:30359ms step_avg:39.43ms
step:771/1900 train_time:30421ms step_avg:39.46ms
step:772/1900 train_time:30483ms step_avg:39.49ms
step:773/1900 train_time:30546ms step_avg:39.52ms
step:774/1900 train_time:30607ms step_avg:39.54ms
step:775/1900 train_time:30669ms step_avg:39.57ms
step:776/1900 train_time:30729ms step_avg:39.60ms
step:777/1900 train_time:30791ms step_avg:39.63ms
step:778/1900 train_time:30851ms step_avg:39.65ms
step:779/1900 train_time:30913ms step_avg:39.68ms
step:780/1900 train_time:30973ms step_avg:39.71ms
step:781/1900 train_time:31035ms step_avg:39.74ms
step:782/1900 train_time:31096ms step_avg:39.76ms
step:783/1900 train_time:31158ms step_avg:39.79ms
step:784/1900 train_time:31220ms step_avg:39.82ms
step:785/1900 train_time:31282ms step_avg:39.85ms
step:786/1900 train_time:31343ms step_avg:39.88ms
step:787/1900 train_time:31405ms step_avg:39.90ms
step:788/1900 train_time:31466ms step_avg:39.93ms
step:789/1900 train_time:31529ms step_avg:39.96ms
step:790/1900 train_time:31590ms step_avg:39.99ms
step:791/1900 train_time:31651ms step_avg:40.01ms
step:792/1900 train_time:31712ms step_avg:40.04ms
step:793/1900 train_time:31774ms step_avg:40.07ms
step:794/1900 train_time:31834ms step_avg:40.09ms
step:795/1900 train_time:31896ms step_avg:40.12ms
step:796/1900 train_time:31956ms step_avg:40.15ms
step:797/1900 train_time:32018ms step_avg:40.17ms
step:798/1900 train_time:32079ms step_avg:40.20ms
step:799/1900 train_time:32142ms step_avg:40.23ms
step:800/1900 train_time:32203ms step_avg:40.25ms
step:801/1900 train_time:32266ms step_avg:40.28ms
step:802/1900 train_time:32327ms step_avg:40.31ms
step:803/1900 train_time:32389ms step_avg:40.33ms
step:804/1900 train_time:32449ms step_avg:40.36ms
step:805/1900 train_time:32512ms step_avg:40.39ms
step:806/1900 train_time:32573ms step_avg:40.41ms
step:807/1900 train_time:32635ms step_avg:40.44ms
step:808/1900 train_time:32696ms step_avg:40.47ms
step:809/1900 train_time:32758ms step_avg:40.49ms
step:810/1900 train_time:32819ms step_avg:40.52ms
step:811/1900 train_time:32881ms step_avg:40.54ms
step:812/1900 train_time:32941ms step_avg:40.57ms
step:813/1900 train_time:33003ms step_avg:40.59ms
step:814/1900 train_time:33065ms step_avg:40.62ms
step:815/1900 train_time:33126ms step_avg:40.65ms
step:816/1900 train_time:33187ms step_avg:40.67ms
step:817/1900 train_time:33249ms step_avg:40.70ms
step:818/1900 train_time:33311ms step_avg:40.72ms
step:819/1900 train_time:33373ms step_avg:40.75ms
step:820/1900 train_time:33434ms step_avg:40.77ms
step:821/1900 train_time:33496ms step_avg:40.80ms
step:822/1900 train_time:33558ms step_avg:40.82ms
step:823/1900 train_time:33620ms step_avg:40.85ms
step:824/1900 train_time:33682ms step_avg:40.88ms
step:825/1900 train_time:33744ms step_avg:40.90ms
step:826/1900 train_time:33806ms step_avg:40.93ms
step:827/1900 train_time:33867ms step_avg:40.95ms
step:828/1900 train_time:33928ms step_avg:40.98ms
step:829/1900 train_time:33990ms step_avg:41.00ms
step:830/1900 train_time:34051ms step_avg:41.03ms
step:831/1900 train_time:34113ms step_avg:41.05ms
step:832/1900 train_time:34173ms step_avg:41.07ms
step:833/1900 train_time:34235ms step_avg:41.10ms
step:834/1900 train_time:34297ms step_avg:41.12ms
step:835/1900 train_time:34359ms step_avg:41.15ms
step:836/1900 train_time:34421ms step_avg:41.17ms
step:837/1900 train_time:34483ms step_avg:41.20ms
step:838/1900 train_time:34544ms step_avg:41.22ms
step:839/1900 train_time:34606ms step_avg:41.25ms
step:840/1900 train_time:34667ms step_avg:41.27ms
step:841/1900 train_time:34729ms step_avg:41.30ms
step:842/1900 train_time:34790ms step_avg:41.32ms
step:843/1900 train_time:34852ms step_avg:41.34ms
step:844/1900 train_time:34912ms step_avg:41.37ms
step:845/1900 train_time:34974ms step_avg:41.39ms
step:846/1900 train_time:35035ms step_avg:41.41ms
step:847/1900 train_time:35097ms step_avg:41.44ms
step:848/1900 train_time:35158ms step_avg:41.46ms
step:849/1900 train_time:35219ms step_avg:41.48ms
step:850/1900 train_time:35280ms step_avg:41.51ms
step:851/1900 train_time:35343ms step_avg:41.53ms
step:852/1900 train_time:35404ms step_avg:41.55ms
step:853/1900 train_time:35466ms step_avg:41.58ms
step:854/1900 train_time:35527ms step_avg:41.60ms
step:855/1900 train_time:35589ms step_avg:41.62ms
step:856/1900 train_time:35650ms step_avg:41.65ms
step:857/1900 train_time:35712ms step_avg:41.67ms
step:858/1900 train_time:35773ms step_avg:41.69ms
step:859/1900 train_time:35835ms step_avg:41.72ms
step:860/1900 train_time:35897ms step_avg:41.74ms
step:861/1900 train_time:35959ms step_avg:41.76ms
step:862/1900 train_time:36019ms step_avg:41.79ms
step:863/1900 train_time:36082ms step_avg:41.81ms
step:864/1900 train_time:36142ms step_avg:41.83ms
step:865/1900 train_time:36204ms step_avg:41.85ms
step:866/1900 train_time:36265ms step_avg:41.88ms
step:867/1900 train_time:36328ms step_avg:41.90ms
step:868/1900 train_time:36389ms step_avg:41.92ms
step:869/1900 train_time:36450ms step_avg:41.95ms
step:870/1900 train_time:36511ms step_avg:41.97ms
step:871/1900 train_time:36573ms step_avg:41.99ms
step:872/1900 train_time:36635ms step_avg:42.01ms
step:873/1900 train_time:36697ms step_avg:42.04ms
step:874/1900 train_time:36759ms step_avg:42.06ms
step:875/1900 train_time:36821ms step_avg:42.08ms
step:876/1900 train_time:36882ms step_avg:42.10ms
step:877/1900 train_time:36944ms step_avg:42.13ms
step:878/1900 train_time:37005ms step_avg:42.15ms
step:879/1900 train_time:37067ms step_avg:42.17ms
step:880/1900 train_time:37128ms step_avg:42.19ms
step:881/1900 train_time:37190ms step_avg:42.21ms
step:882/1900 train_time:37251ms step_avg:42.23ms
step:883/1900 train_time:37312ms step_avg:42.26ms
step:884/1900 train_time:37373ms step_avg:42.28ms
step:885/1900 train_time:37434ms step_avg:42.30ms
step:886/1900 train_time:37495ms step_avg:42.32ms
step:887/1900 train_time:37558ms step_avg:42.34ms
step:888/1900 train_time:37619ms step_avg:42.36ms
step:889/1900 train_time:37681ms step_avg:42.39ms
step:890/1900 train_time:37743ms step_avg:42.41ms
step:891/1900 train_time:37804ms step_avg:42.43ms
step:892/1900 train_time:37866ms step_avg:42.45ms
step:893/1900 train_time:37928ms step_avg:42.47ms
step:894/1900 train_time:37988ms step_avg:42.49ms
step:895/1900 train_time:38050ms step_avg:42.51ms
step:896/1900 train_time:38111ms step_avg:42.53ms
step:897/1900 train_time:38172ms step_avg:42.56ms
step:898/1900 train_time:38233ms step_avg:42.58ms
step:899/1900 train_time:38295ms step_avg:42.60ms
step:900/1900 train_time:38355ms step_avg:42.62ms
step:901/1900 train_time:38417ms step_avg:42.64ms
step:902/1900 train_time:38479ms step_avg:42.66ms
step:903/1900 train_time:38541ms step_avg:42.68ms
step:904/1900 train_time:38602ms step_avg:42.70ms
step:905/1900 train_time:38664ms step_avg:42.72ms
step:906/1900 train_time:38726ms step_avg:42.74ms
step:907/1900 train_time:38788ms step_avg:42.76ms
step:908/1900 train_time:38849ms step_avg:42.78ms
step:909/1900 train_time:38911ms step_avg:42.81ms
step:910/1900 train_time:38972ms step_avg:42.83ms
step:911/1900 train_time:39035ms step_avg:42.85ms
step:912/1900 train_time:39096ms step_avg:42.87ms
step:913/1900 train_time:39157ms step_avg:42.89ms
step:914/1900 train_time:39219ms step_avg:42.91ms
step:915/1900 train_time:39281ms step_avg:42.93ms
step:916/1900 train_time:39343ms step_avg:42.95ms
step:917/1900 train_time:39405ms step_avg:42.97ms
step:918/1900 train_time:39466ms step_avg:42.99ms
step:919/1900 train_time:39528ms step_avg:43.01ms
step:920/1900 train_time:39589ms step_avg:43.03ms
step:921/1900 train_time:39651ms step_avg:43.05ms
step:922/1900 train_time:39712ms step_avg:43.07ms
step:923/1900 train_time:39774ms step_avg:43.09ms
step:924/1900 train_time:39835ms step_avg:43.11ms
step:925/1900 train_time:39897ms step_avg:43.13ms
step:926/1900 train_time:39959ms step_avg:43.15ms
step:927/1900 train_time:40021ms step_avg:43.17ms
step:928/1900 train_time:40082ms step_avg:43.19ms
step:929/1900 train_time:40145ms step_avg:43.21ms
step:930/1900 train_time:40206ms step_avg:43.23ms
step:931/1900 train_time:40268ms step_avg:43.25ms
step:932/1900 train_time:40329ms step_avg:43.27ms
step:933/1900 train_time:40391ms step_avg:43.29ms
step:934/1900 train_time:40452ms step_avg:43.31ms
step:935/1900 train_time:40514ms step_avg:43.33ms
step:936/1900 train_time:40575ms step_avg:43.35ms
step:937/1900 train_time:40637ms step_avg:43.37ms
step:938/1900 train_time:40699ms step_avg:43.39ms
step:939/1900 train_time:40760ms step_avg:43.41ms
step:940/1900 train_time:40822ms step_avg:43.43ms
step:941/1900 train_time:40884ms step_avg:43.45ms
step:942/1900 train_time:40945ms step_avg:43.47ms
step:943/1900 train_time:41007ms step_avg:43.49ms
step:944/1900 train_time:41068ms step_avg:43.50ms
step:945/1900 train_time:41130ms step_avg:43.52ms
step:946/1900 train_time:41190ms step_avg:43.54ms
step:947/1900 train_time:41252ms step_avg:43.56ms
step:948/1900 train_time:41314ms step_avg:43.58ms
step:949/1900 train_time:41376ms step_avg:43.60ms
step:950/1900 train_time:41437ms step_avg:43.62ms
step:951/1900 train_time:41499ms step_avg:43.64ms
step:952/1900 train_time:41560ms step_avg:43.66ms
step:953/1900 train_time:41622ms step_avg:43.68ms
step:954/1900 train_time:41684ms step_avg:43.69ms
step:955/1900 train_time:41745ms step_avg:43.71ms
step:956/1900 train_time:41807ms step_avg:43.73ms
step:957/1900 train_time:41869ms step_avg:43.75ms
step:958/1900 train_time:41929ms step_avg:43.77ms
step:959/1900 train_time:41991ms step_avg:43.79ms
step:960/1900 train_time:42052ms step_avg:43.80ms
step:961/1900 train_time:42113ms step_avg:43.82ms
step:962/1900 train_time:42174ms step_avg:43.84ms
step:963/1900 train_time:42236ms step_avg:43.86ms
step:964/1900 train_time:42297ms step_avg:43.88ms
step:965/1900 train_time:42359ms step_avg:43.90ms
step:966/1900 train_time:42420ms step_avg:43.91ms
step:967/1900 train_time:42482ms step_avg:43.93ms
step:968/1900 train_time:42543ms step_avg:43.95ms
step:969/1900 train_time:42605ms step_avg:43.97ms
step:970/1900 train_time:42666ms step_avg:43.99ms
step:971/1900 train_time:42728ms step_avg:44.00ms
step:972/1900 train_time:42789ms step_avg:44.02ms
step:973/1900 train_time:42851ms step_avg:44.04ms
step:974/1900 train_time:42912ms step_avg:44.06ms
step:975/1900 train_time:42974ms step_avg:44.08ms
step:976/1900 train_time:43035ms step_avg:44.09ms
step:977/1900 train_time:43097ms step_avg:44.11ms
step:978/1900 train_time:43158ms step_avg:44.13ms
step:979/1900 train_time:43220ms step_avg:44.15ms
step:980/1900 train_time:43281ms step_avg:44.16ms
step:981/1900 train_time:43343ms step_avg:44.18ms
step:982/1900 train_time:43404ms step_avg:44.20ms
step:983/1900 train_time:43466ms step_avg:44.22ms
step:984/1900 train_time:43528ms step_avg:44.24ms
step:985/1900 train_time:43590ms step_avg:44.25ms
step:986/1900 train_time:43650ms step_avg:44.27ms
step:987/1900 train_time:43712ms step_avg:44.29ms
step:988/1900 train_time:43773ms step_avg:44.30ms
step:989/1900 train_time:43835ms step_avg:44.32ms
step:990/1900 train_time:43896ms step_avg:44.34ms
step:991/1900 train_time:43958ms step_avg:44.36ms
step:992/1900 train_time:44019ms step_avg:44.37ms
step:993/1900 train_time:44081ms step_avg:44.39ms
step:994/1900 train_time:44143ms step_avg:44.41ms
step:995/1900 train_time:44205ms step_avg:44.43ms
step:996/1900 train_time:44266ms step_avg:44.44ms
step:997/1900 train_time:44327ms step_avg:44.46ms
step:998/1900 train_time:44388ms step_avg:44.48ms
step:999/1900 train_time:44449ms step_avg:44.49ms
step:1000/1900 train_time:44510ms step_avg:44.51ms
step:1000/1900 val_loss:3.7807 train_time:44575ms step_avg:44.57ms
step:1001/1900 train_time:44596ms step_avg:44.55ms
step:1002/1900 train_time:44635ms step_avg:44.55ms
step:1003/1900 train_time:44700ms step_avg:44.57ms
step:1004/1900 train_time:44763ms step_avg:44.59ms
step:1005/1900 train_time:44825ms step_avg:44.60ms
step:1006/1900 train_time:44887ms step_avg:44.62ms
step:1007/1900 train_time:44948ms step_avg:44.64ms
step:1008/1900 train_time:45008ms step_avg:44.65ms
step:1009/1900 train_time:45069ms step_avg:44.67ms
step:1010/1900 train_time:45130ms step_avg:44.68ms
step:1011/1900 train_time:45191ms step_avg:44.70ms
step:1012/1900 train_time:45252ms step_avg:44.72ms
step:1013/1900 train_time:45313ms step_avg:44.73ms
step:1014/1900 train_time:45374ms step_avg:44.75ms
step:1015/1900 train_time:45435ms step_avg:44.76ms
step:1016/1900 train_time:45496ms step_avg:44.78ms
step:1017/1900 train_time:45559ms step_avg:44.80ms
step:1018/1900 train_time:45622ms step_avg:44.82ms
step:1019/1900 train_time:45686ms step_avg:44.83ms
step:1020/1900 train_time:45747ms step_avg:44.85ms
step:1021/1900 train_time:45810ms step_avg:44.87ms
step:1022/1900 train_time:45872ms step_avg:44.88ms
step:1023/1900 train_time:45934ms step_avg:44.90ms
step:1024/1900 train_time:45995ms step_avg:44.92ms
step:1025/1900 train_time:46056ms step_avg:44.93ms
step:1026/1900 train_time:46117ms step_avg:44.95ms
step:1027/1900 train_time:46179ms step_avg:44.97ms
step:1028/1900 train_time:46240ms step_avg:44.98ms
step:1029/1900 train_time:46302ms step_avg:45.00ms
step:1030/1900 train_time:46363ms step_avg:45.01ms
step:1031/1900 train_time:46424ms step_avg:45.03ms
step:1032/1900 train_time:46485ms step_avg:45.04ms
step:1033/1900 train_time:46547ms step_avg:45.06ms
step:1034/1900 train_time:46608ms step_avg:45.08ms
step:1035/1900 train_time:46671ms step_avg:45.09ms
step:1036/1900 train_time:46733ms step_avg:45.11ms
step:1037/1900 train_time:46795ms step_avg:45.13ms
step:1038/1900 train_time:46857ms step_avg:45.14ms
step:1039/1900 train_time:46919ms step_avg:45.16ms
step:1040/1900 train_time:46979ms step_avg:45.17ms
step:1041/1900 train_time:47041ms step_avg:45.19ms
step:1042/1900 train_time:47102ms step_avg:45.20ms
step:1043/1900 train_time:47163ms step_avg:45.22ms
step:1044/1900 train_time:47224ms step_avg:45.23ms
step:1045/1900 train_time:47285ms step_avg:45.25ms
step:1046/1900 train_time:47346ms step_avg:45.26ms
step:1047/1900 train_time:47407ms step_avg:45.28ms
step:1048/1900 train_time:47468ms step_avg:45.29ms
step:1049/1900 train_time:47530ms step_avg:45.31ms
step:1050/1900 train_time:47591ms step_avg:45.32ms
step:1051/1900 train_time:47653ms step_avg:45.34ms
step:1052/1900 train_time:47714ms step_avg:45.36ms
step:1053/1900 train_time:47777ms step_avg:45.37ms
step:1054/1900 train_time:47839ms step_avg:45.39ms
step:1055/1900 train_time:47901ms step_avg:45.40ms
step:1056/1900 train_time:47962ms step_avg:45.42ms
step:1057/1900 train_time:48023ms step_avg:45.43ms
step:1058/1900 train_time:48084ms step_avg:45.45ms
step:1059/1900 train_time:48145ms step_avg:45.46ms
step:1060/1900 train_time:48206ms step_avg:45.48ms
step:1061/1900 train_time:48268ms step_avg:45.49ms
step:1062/1900 train_time:48329ms step_avg:45.51ms
step:1063/1900 train_time:48391ms step_avg:45.52ms
step:1064/1900 train_time:48452ms step_avg:45.54ms
step:1065/1900 train_time:48514ms step_avg:45.55ms
step:1066/1900 train_time:48575ms step_avg:45.57ms
step:1067/1900 train_time:48637ms step_avg:45.58ms
step:1068/1900 train_time:48699ms step_avg:45.60ms
step:1069/1900 train_time:48762ms step_avg:45.61ms
step:1070/1900 train_time:48823ms step_avg:45.63ms
step:1071/1900 train_time:48885ms step_avg:45.64ms
step:1072/1900 train_time:48946ms step_avg:45.66ms
step:1073/1900 train_time:49008ms step_avg:45.67ms
step:1074/1900 train_time:49069ms step_avg:45.69ms
step:1075/1900 train_time:49130ms step_avg:45.70ms
step:1076/1900 train_time:49191ms step_avg:45.72ms
step:1077/1900 train_time:49254ms step_avg:45.73ms
step:1078/1900 train_time:49314ms step_avg:45.75ms
step:1079/1900 train_time:49376ms step_avg:45.76ms
step:1080/1900 train_time:49437ms step_avg:45.78ms
step:1081/1900 train_time:49499ms step_avg:45.79ms
step:1082/1900 train_time:49560ms step_avg:45.80ms
step:1083/1900 train_time:49622ms step_avg:45.82ms
step:1084/1900 train_time:49683ms step_avg:45.83ms
step:1085/1900 train_time:49745ms step_avg:45.85ms
step:1086/1900 train_time:49806ms step_avg:45.86ms
step:1087/1900 train_time:49868ms step_avg:45.88ms
step:1088/1900 train_time:49929ms step_avg:45.89ms
step:1089/1900 train_time:49991ms step_avg:45.91ms
step:1090/1900 train_time:50052ms step_avg:45.92ms
step:1091/1900 train_time:50114ms step_avg:45.93ms
step:1092/1900 train_time:50174ms step_avg:45.95ms
step:1093/1900 train_time:50237ms step_avg:45.96ms
step:1094/1900 train_time:50298ms step_avg:45.98ms
step:1095/1900 train_time:50359ms step_avg:45.99ms
step:1096/1900 train_time:50421ms step_avg:46.00ms
step:1097/1900 train_time:50482ms step_avg:46.02ms
step:1098/1900 train_time:50543ms step_avg:46.03ms
step:1099/1900 train_time:50605ms step_avg:46.05ms
step:1100/1900 train_time:50666ms step_avg:46.06ms
step:1101/1900 train_time:50728ms step_avg:46.07ms
step:1102/1900 train_time:50789ms step_avg:46.09ms
step:1103/1900 train_time:50852ms step_avg:46.10ms
step:1104/1900 train_time:50913ms step_avg:46.12ms
step:1105/1900 train_time:50975ms step_avg:46.13ms
step:1106/1900 train_time:51036ms step_avg:46.14ms
step:1107/1900 train_time:51099ms step_avg:46.16ms
step:1108/1900 train_time:51160ms step_avg:46.17ms
step:1109/1900 train_time:51222ms step_avg:46.19ms
step:1110/1900 train_time:51283ms step_avg:46.20ms
step:1111/1900 train_time:51345ms step_avg:46.21ms
step:1112/1900 train_time:51405ms step_avg:46.23ms
step:1113/1900 train_time:51467ms step_avg:46.24ms
step:1114/1900 train_time:51528ms step_avg:46.25ms
step:1115/1900 train_time:51590ms step_avg:46.27ms
step:1116/1900 train_time:51651ms step_avg:46.28ms
step:1117/1900 train_time:51713ms step_avg:46.30ms
step:1118/1900 train_time:51774ms step_avg:46.31ms
step:1119/1900 train_time:51836ms step_avg:46.32ms
step:1120/1900 train_time:51898ms step_avg:46.34ms
step:1121/1900 train_time:51960ms step_avg:46.35ms
step:1122/1900 train_time:52021ms step_avg:46.36ms
step:1123/1900 train_time:52083ms step_avg:46.38ms
step:1124/1900 train_time:52144ms step_avg:46.39ms
step:1125/1900 train_time:52206ms step_avg:46.41ms
step:1126/1900 train_time:52267ms step_avg:46.42ms
step:1127/1900 train_time:52328ms step_avg:46.43ms
step:1128/1900 train_time:52389ms step_avg:46.44ms
step:1129/1900 train_time:52451ms step_avg:46.46ms
step:1130/1900 train_time:52512ms step_avg:46.47ms
step:1131/1900 train_time:52574ms step_avg:46.48ms
step:1132/1900 train_time:52635ms step_avg:46.50ms
step:1133/1900 train_time:52697ms step_avg:46.51ms
step:1134/1900 train_time:52758ms step_avg:46.52ms
step:1135/1900 train_time:52820ms step_avg:46.54ms
step:1136/1900 train_time:52881ms step_avg:46.55ms
step:1137/1900 train_time:52943ms step_avg:46.56ms
step:1138/1900 train_time:53004ms step_avg:46.58ms
step:1139/1900 train_time:53066ms step_avg:46.59ms
step:1140/1900 train_time:53127ms step_avg:46.60ms
step:1141/1900 train_time:53188ms step_avg:46.62ms
step:1142/1900 train_time:53249ms step_avg:46.63ms
step:1143/1900 train_time:53311ms step_avg:46.64ms
step:1144/1900 train_time:53372ms step_avg:46.65ms
step:1145/1900 train_time:53434ms step_avg:46.67ms
step:1146/1900 train_time:53495ms step_avg:46.68ms
step:1147/1900 train_time:53558ms step_avg:46.69ms
step:1148/1900 train_time:53619ms step_avg:46.71ms
step:1149/1900 train_time:53681ms step_avg:46.72ms
step:1150/1900 train_time:53742ms step_avg:46.73ms
step:1151/1900 train_time:53804ms step_avg:46.75ms
step:1152/1900 train_time:53865ms step_avg:46.76ms
step:1153/1900 train_time:53927ms step_avg:46.77ms
step:1154/1900 train_time:53987ms step_avg:46.78ms
step:1155/1900 train_time:54049ms step_avg:46.80ms
step:1156/1900 train_time:54110ms step_avg:46.81ms
step:1157/1900 train_time:54172ms step_avg:46.82ms
step:1158/1900 train_time:54234ms step_avg:46.83ms
step:1159/1900 train_time:54296ms step_avg:46.85ms
step:1160/1900 train_time:54357ms step_avg:46.86ms
step:1161/1900 train_time:54419ms step_avg:46.87ms
step:1162/1900 train_time:54480ms step_avg:46.88ms
step:1163/1900 train_time:54542ms step_avg:46.90ms
step:1164/1900 train_time:54603ms step_avg:46.91ms
step:1165/1900 train_time:54665ms step_avg:46.92ms
step:1166/1900 train_time:54726ms step_avg:46.93ms
step:1167/1900 train_time:54788ms step_avg:46.95ms
step:1168/1900 train_time:54849ms step_avg:46.96ms
step:1169/1900 train_time:54911ms step_avg:46.97ms
step:1170/1900 train_time:54972ms step_avg:46.98ms
step:1171/1900 train_time:55034ms step_avg:47.00ms
step:1172/1900 train_time:55095ms step_avg:47.01ms
step:1173/1900 train_time:55158ms step_avg:47.02ms
step:1174/1900 train_time:55219ms step_avg:47.03ms
step:1175/1900 train_time:55281ms step_avg:47.05ms
step:1176/1900 train_time:55342ms step_avg:47.06ms
step:1177/1900 train_time:55404ms step_avg:47.07ms
step:1178/1900 train_time:55465ms step_avg:47.08ms
step:1179/1900 train_time:55526ms step_avg:47.10ms
step:1180/1900 train_time:55587ms step_avg:47.11ms
step:1181/1900 train_time:55649ms step_avg:47.12ms
step:1182/1900 train_time:55710ms step_avg:47.13ms
step:1183/1900 train_time:55772ms step_avg:47.14ms
step:1184/1900 train_time:55833ms step_avg:47.16ms
step:1185/1900 train_time:55895ms step_avg:47.17ms
step:1186/1900 train_time:55956ms step_avg:47.18ms
step:1187/1900 train_time:56018ms step_avg:47.19ms
step:1188/1900 train_time:56079ms step_avg:47.20ms
step:1189/1900 train_time:56141ms step_avg:47.22ms
step:1190/1900 train_time:56202ms step_avg:47.23ms
step:1191/1900 train_time:56264ms step_avg:47.24ms
step:1192/1900 train_time:56325ms step_avg:47.25ms
step:1193/1900 train_time:56386ms step_avg:47.26ms
step:1194/1900 train_time:56447ms step_avg:47.28ms
step:1195/1900 train_time:56508ms step_avg:47.29ms
step:1196/1900 train_time:56570ms step_avg:47.30ms
step:1197/1900 train_time:56632ms step_avg:47.31ms
step:1198/1900 train_time:56693ms step_avg:47.32ms
step:1199/1900 train_time:56755ms step_avg:47.34ms
step:1200/1900 train_time:56816ms step_avg:47.35ms
step:1201/1900 train_time:56878ms step_avg:47.36ms
step:1202/1900 train_time:56939ms step_avg:47.37ms
step:1203/1900 train_time:57001ms step_avg:47.38ms
step:1204/1900 train_time:57062ms step_avg:47.39ms
step:1205/1900 train_time:57124ms step_avg:47.41ms
step:1206/1900 train_time:57185ms step_avg:47.42ms
step:1207/1900 train_time:57248ms step_avg:47.43ms
step:1208/1900 train_time:57309ms step_avg:47.44ms
step:1209/1900 train_time:57371ms step_avg:47.45ms
step:1210/1900 train_time:57432ms step_avg:47.46ms
step:1211/1900 train_time:57494ms step_avg:47.48ms
step:1212/1900 train_time:57555ms step_avg:47.49ms
step:1213/1900 train_time:57617ms step_avg:47.50ms
step:1214/1900 train_time:57679ms step_avg:47.51ms
step:1215/1900 train_time:57741ms step_avg:47.52ms
step:1216/1900 train_time:57802ms step_avg:47.53ms
step:1217/1900 train_time:57863ms step_avg:47.55ms
step:1218/1900 train_time:57925ms step_avg:47.56ms
step:1219/1900 train_time:57987ms step_avg:47.57ms
step:1220/1900 train_time:58048ms step_avg:47.58ms
step:1221/1900 train_time:58110ms step_avg:47.59ms
step:1222/1900 train_time:58171ms step_avg:47.60ms
step:1223/1900 train_time:58233ms step_avg:47.61ms
step:1224/1900 train_time:58294ms step_avg:47.63ms
step:1225/1900 train_time:58357ms step_avg:47.64ms
step:1226/1900 train_time:58418ms step_avg:47.65ms
step:1227/1900 train_time:58480ms step_avg:47.66ms
step:1228/1900 train_time:58541ms step_avg:47.67ms
step:1229/1900 train_time:58603ms step_avg:47.68ms
step:1230/1900 train_time:58664ms step_avg:47.69ms
step:1231/1900 train_time:58726ms step_avg:47.71ms
step:1232/1900 train_time:58786ms step_avg:47.72ms
step:1233/1900 train_time:58848ms step_avg:47.73ms
step:1234/1900 train_time:58910ms step_avg:47.74ms
step:1235/1900 train_time:58971ms step_avg:47.75ms
step:1236/1900 train_time:59032ms step_avg:47.76ms
step:1237/1900 train_time:59094ms step_avg:47.77ms
step:1238/1900 train_time:59155ms step_avg:47.78ms
step:1239/1900 train_time:59217ms step_avg:47.79ms
step:1240/1900 train_time:59278ms step_avg:47.81ms
step:1241/1900 train_time:59341ms step_avg:47.82ms
step:1242/1900 train_time:59430ms step_avg:47.85ms
step:1243/1900 train_time:59518ms step_avg:47.88ms
step:1244/1900 train_time:59605ms step_avg:47.91ms
step:1245/1900 train_time:59695ms step_avg:47.95ms
step:1246/1900 train_time:59784ms step_avg:47.98ms
step:1247/1900 train_time:59872ms step_avg:48.01ms
step:1248/1900 train_time:59959ms step_avg:48.04ms
step:1249/1900 train_time:60047ms step_avg:48.08ms
step:1250/1900 train_time:60135ms step_avg:48.11ms
step:1250/1900 val_loss:3.5417 train_time:60226ms step_avg:48.18ms
step:1251/1900 train_time:60246ms step_avg:48.16ms
step:1252/1900 train_time:60314ms step_avg:48.17ms
step:1253/1900 train_time:60405ms step_avg:48.21ms
step:1254/1900 train_time:60493ms step_avg:48.24ms
step:1255/1900 train_time:60581ms step_avg:48.27ms
step:1256/1900 train_time:60667ms step_avg:48.30ms
step:1257/1900 train_time:60754ms step_avg:48.33ms
step:1258/1900 train_time:60842ms step_avg:48.36ms
step:1259/1900 train_time:60930ms step_avg:48.40ms
step:1260/1900 train_time:61018ms step_avg:48.43ms
step:1261/1900 train_time:61106ms step_avg:48.46ms
step:1262/1900 train_time:61196ms step_avg:48.49ms
step:1263/1900 train_time:61286ms step_avg:48.52ms
step:1264/1900 train_time:61376ms step_avg:48.56ms
step:1265/1900 train_time:61465ms step_avg:48.59ms
step:1266/1900 train_time:61552ms step_avg:48.62ms
step:1267/1900 train_time:61640ms step_avg:48.65ms
step:1268/1900 train_time:61727ms step_avg:48.68ms
step:1269/1900 train_time:61815ms step_avg:48.71ms
step:1270/1900 train_time:61902ms step_avg:48.74ms
step:1271/1900 train_time:61990ms step_avg:48.77ms
step:1272/1900 train_time:62078ms step_avg:48.80ms
step:1273/1900 train_time:62168ms step_avg:48.84ms
step:1274/1900 train_time:62258ms step_avg:48.87ms
step:1275/1900 train_time:62346ms step_avg:48.90ms
step:1276/1900 train_time:62434ms step_avg:48.93ms
step:1277/1900 train_time:62523ms step_avg:48.96ms
step:1278/1900 train_time:62611ms step_avg:48.99ms
step:1279/1900 train_time:62700ms step_avg:49.02ms
step:1280/1900 train_time:62787ms step_avg:49.05ms
step:1281/1900 train_time:62874ms step_avg:49.08ms
step:1282/1900 train_time:62962ms step_avg:49.11ms
step:1283/1900 train_time:63050ms step_avg:49.14ms
step:1284/1900 train_time:63139ms step_avg:49.17ms
step:1285/1900 train_time:63228ms step_avg:49.20ms
step:1286/1900 train_time:63316ms step_avg:49.24ms
step:1287/1900 train_time:63405ms step_avg:49.27ms
step:1288/1900 train_time:63493ms step_avg:49.30ms
step:1289/1900 train_time:63582ms step_avg:49.33ms
step:1290/1900 train_time:63670ms step_avg:49.36ms
step:1291/1900 train_time:63758ms step_avg:49.39ms
step:1292/1900 train_time:63845ms step_avg:49.42ms
step:1293/1900 train_time:63933ms step_avg:49.45ms
step:1294/1900 train_time:64021ms step_avg:49.47ms
step:1295/1900 train_time:64109ms step_avg:49.50ms
step:1296/1900 train_time:64197ms step_avg:49.53ms
step:1297/1900 train_time:64287ms step_avg:49.57ms
step:1298/1900 train_time:64374ms step_avg:49.59ms
step:1299/1900 train_time:64464ms step_avg:49.63ms
step:1300/1900 train_time:64551ms step_avg:49.65ms
step:1301/1900 train_time:64640ms step_avg:49.68ms
step:1302/1900 train_time:64727ms step_avg:49.71ms
step:1303/1900 train_time:64816ms step_avg:49.74ms
step:1304/1900 train_time:64903ms step_avg:49.77ms
step:1305/1900 train_time:64992ms step_avg:49.80ms
step:1306/1900 train_time:65080ms step_avg:49.83ms
step:1307/1900 train_time:65169ms step_avg:49.86ms
step:1308/1900 train_time:65258ms step_avg:49.89ms
step:1309/1900 train_time:65347ms step_avg:49.92ms
step:1310/1900 train_time:65435ms step_avg:49.95ms
step:1311/1900 train_time:65524ms step_avg:49.98ms
step:1312/1900 train_time:65612ms step_avg:50.01ms
step:1313/1900 train_time:65701ms step_avg:50.04ms
step:1314/1900 train_time:65789ms step_avg:50.07ms
step:1315/1900 train_time:65878ms step_avg:50.10ms
step:1316/1900 train_time:65964ms step_avg:50.12ms
step:1317/1900 train_time:66053ms step_avg:50.15ms
step:1318/1900 train_time:66141ms step_avg:50.18ms
step:1319/1900 train_time:66229ms step_avg:50.21ms
step:1320/1900 train_time:66317ms step_avg:50.24ms
step:1321/1900 train_time:66406ms step_avg:50.27ms
step:1322/1900 train_time:66494ms step_avg:50.30ms
step:1323/1900 train_time:66584ms step_avg:50.33ms
step:1324/1900 train_time:66671ms step_avg:50.36ms
step:1325/1900 train_time:66761ms step_avg:50.39ms
step:1326/1900 train_time:66848ms step_avg:50.41ms
step:1327/1900 train_time:66937ms step_avg:50.44ms
step:1328/1900 train_time:67024ms step_avg:50.47ms
step:1329/1900 train_time:67113ms step_avg:50.50ms
step:1330/1900 train_time:67200ms step_avg:50.53ms
step:1331/1900 train_time:67289ms step_avg:50.55ms
step:1332/1900 train_time:67377ms step_avg:50.58ms
step:1333/1900 train_time:67466ms step_avg:50.61ms
step:1334/1900 train_time:67555ms step_avg:50.64ms
step:1335/1900 train_time:67644ms step_avg:50.67ms
step:1336/1900 train_time:67731ms step_avg:50.70ms
step:1337/1900 train_time:67821ms step_avg:50.73ms
step:1338/1900 train_time:67908ms step_avg:50.75ms
step:1339/1900 train_time:67997ms step_avg:50.78ms
step:1340/1900 train_time:68085ms step_avg:50.81ms
step:1341/1900 train_time:68173ms step_avg:50.84ms
step:1342/1900 train_time:68261ms step_avg:50.86ms
step:1343/1900 train_time:68349ms step_avg:50.89ms
step:1344/1900 train_time:68438ms step_avg:50.92ms
step:1345/1900 train_time:68527ms step_avg:50.95ms
step:1346/1900 train_time:68615ms step_avg:50.98ms
step:1347/1900 train_time:68705ms step_avg:51.01ms
step:1348/1900 train_time:68793ms step_avg:51.03ms
step:1349/1900 train_time:68881ms step_avg:51.06ms
step:1350/1900 train_time:68969ms step_avg:51.09ms
step:1351/1900 train_time:69058ms step_avg:51.12ms
step:1352/1900 train_time:69145ms step_avg:51.14ms
step:1353/1900 train_time:69233ms step_avg:51.17ms
step:1354/1900 train_time:69321ms step_avg:51.20ms
step:1355/1900 train_time:69410ms step_avg:51.23ms
step:1356/1900 train_time:69498ms step_avg:51.25ms
step:1357/1900 train_time:69586ms step_avg:51.28ms
step:1358/1900 train_time:69674ms step_avg:51.31ms
step:1359/1900 train_time:69762ms step_avg:51.33ms
step:1360/1900 train_time:69850ms step_avg:51.36ms
step:1361/1900 train_time:69938ms step_avg:51.39ms
step:1362/1900 train_time:70026ms step_avg:51.41ms
step:1363/1900 train_time:70115ms step_avg:51.44ms
step:1364/1900 train_time:70202ms step_avg:51.47ms
step:1365/1900 train_time:70291ms step_avg:51.50ms
step:1366/1900 train_time:70380ms step_avg:51.52ms
step:1367/1900 train_time:70468ms step_avg:51.55ms
step:1368/1900 train_time:70556ms step_avg:51.58ms
step:1369/1900 train_time:70646ms step_avg:51.60ms
step:1370/1900 train_time:70733ms step_avg:51.63ms
step:1371/1900 train_time:70822ms step_avg:51.66ms
step:1372/1900 train_time:70911ms step_avg:51.68ms
step:1373/1900 train_time:71001ms step_avg:51.71ms
step:1374/1900 train_time:71087ms step_avg:51.74ms
step:1375/1900 train_time:71176ms step_avg:51.76ms
step:1376/1900 train_time:71264ms step_avg:51.79ms
step:1377/1900 train_time:71352ms step_avg:51.82ms
step:1378/1900 train_time:71440ms step_avg:51.84ms
step:1379/1900 train_time:71528ms step_avg:51.87ms
step:1380/1900 train_time:71616ms step_avg:51.90ms
step:1381/1900 train_time:71706ms step_avg:51.92ms
step:1382/1900 train_time:71794ms step_avg:51.95ms
step:1383/1900 train_time:71883ms step_avg:51.98ms
step:1384/1900 train_time:71971ms step_avg:52.00ms
step:1385/1900 train_time:72060ms step_avg:52.03ms
step:1386/1900 train_time:72148ms step_avg:52.05ms
step:1387/1900 train_time:72236ms step_avg:52.08ms
step:1388/1900 train_time:72323ms step_avg:52.11ms
step:1389/1900 train_time:72411ms step_avg:52.13ms
step:1390/1900 train_time:72498ms step_avg:52.16ms
step:1391/1900 train_time:72587ms step_avg:52.18ms
step:1392/1900 train_time:72676ms step_avg:52.21ms
step:1393/1900 train_time:72765ms step_avg:52.24ms
step:1394/1900 train_time:72853ms step_avg:52.26ms
step:1395/1900 train_time:72942ms step_avg:52.29ms
step:1396/1900 train_time:73030ms step_avg:52.31ms
step:1397/1900 train_time:73119ms step_avg:52.34ms
step:1398/1900 train_time:73205ms step_avg:52.36ms
step:1399/1900 train_time:73295ms step_avg:52.39ms
step:1400/1900 train_time:73382ms step_avg:52.42ms
step:1401/1900 train_time:73470ms step_avg:52.44ms
step:1402/1900 train_time:73558ms step_avg:52.47ms
step:1403/1900 train_time:73647ms step_avg:52.49ms
step:1404/1900 train_time:73734ms step_avg:52.52ms
step:1405/1900 train_time:73823ms step_avg:52.54ms
step:1406/1900 train_time:73910ms step_avg:52.57ms
step:1407/1900 train_time:74000ms step_avg:52.59ms
step:1408/1900 train_time:74088ms step_avg:52.62ms
step:1409/1900 train_time:74176ms step_avg:52.64ms
step:1410/1900 train_time:74264ms step_avg:52.67ms
step:1411/1900 train_time:74352ms step_avg:52.69ms
step:1412/1900 train_time:74440ms step_avg:52.72ms
step:1413/1900 train_time:74528ms step_avg:52.74ms
step:1414/1900 train_time:74615ms step_avg:52.77ms
step:1415/1900 train_time:74704ms step_avg:52.79ms
step:1416/1900 train_time:74792ms step_avg:52.82ms
step:1417/1900 train_time:74880ms step_avg:52.84ms
step:1418/1900 train_time:74968ms step_avg:52.87ms
step:1419/1900 train_time:75057ms step_avg:52.89ms
step:1420/1900 train_time:75144ms step_avg:52.92ms
step:1421/1900 train_time:75232ms step_avg:52.94ms
step:1422/1900 train_time:75320ms step_avg:52.97ms
step:1423/1900 train_time:75409ms step_avg:52.99ms
step:1424/1900 train_time:75497ms step_avg:53.02ms
step:1425/1900 train_time:75586ms step_avg:53.04ms
step:1426/1900 train_time:75674ms step_avg:53.07ms
step:1427/1900 train_time:75763ms step_avg:53.09ms
step:1428/1900 train_time:75851ms step_avg:53.12ms
step:1429/1900 train_time:75940ms step_avg:53.14ms
step:1430/1900 train_time:76027ms step_avg:53.17ms
step:1431/1900 train_time:76116ms step_avg:53.19ms
step:1432/1900 train_time:76204ms step_avg:53.21ms
step:1433/1900 train_time:76293ms step_avg:53.24ms
step:1434/1900 train_time:76380ms step_avg:53.26ms
step:1435/1900 train_time:76469ms step_avg:53.29ms
step:1436/1900 train_time:76557ms step_avg:53.31ms
step:1437/1900 train_time:76645ms step_avg:53.34ms
step:1438/1900 train_time:76732ms step_avg:53.36ms
step:1439/1900 train_time:76821ms step_avg:53.39ms
step:1440/1900 train_time:76909ms step_avg:53.41ms
step:1441/1900 train_time:76998ms step_avg:53.43ms
step:1442/1900 train_time:77086ms step_avg:53.46ms
step:1443/1900 train_time:77175ms step_avg:53.48ms
step:1444/1900 train_time:77262ms step_avg:53.51ms
step:1445/1900 train_time:77351ms step_avg:53.53ms
step:1446/1900 train_time:77439ms step_avg:53.55ms
step:1447/1900 train_time:77528ms step_avg:53.58ms
step:1448/1900 train_time:77616ms step_avg:53.60ms
step:1449/1900 train_time:77706ms step_avg:53.63ms
step:1450/1900 train_time:77793ms step_avg:53.65ms
step:1451/1900 train_time:77881ms step_avg:53.67ms
step:1452/1900 train_time:77969ms step_avg:53.70ms
step:1453/1900 train_time:78058ms step_avg:53.72ms
step:1454/1900 train_time:78145ms step_avg:53.75ms
step:1455/1900 train_time:78234ms step_avg:53.77ms
step:1456/1900 train_time:78322ms step_avg:53.79ms
step:1457/1900 train_time:78410ms step_avg:53.82ms
step:1458/1900 train_time:78498ms step_avg:53.84ms
step:1459/1900 train_time:78587ms step_avg:53.86ms
step:1460/1900 train_time:78675ms step_avg:53.89ms
step:1461/1900 train_time:78763ms step_avg:53.91ms
step:1462/1900 train_time:78851ms step_avg:53.93ms
step:1463/1900 train_time:78941ms step_avg:53.96ms
step:1464/1900 train_time:79029ms step_avg:53.98ms
step:1465/1900 train_time:79119ms step_avg:54.01ms
step:1466/1900 train_time:79206ms step_avg:54.03ms
step:1467/1900 train_time:79294ms step_avg:54.05ms
step:1468/1900 train_time:79382ms step_avg:54.07ms
step:1469/1900 train_time:79470ms step_avg:54.10ms
step:1470/1900 train_time:79558ms step_avg:54.12ms
step:1471/1900 train_time:79647ms step_avg:54.14ms
step:1472/1900 train_time:79735ms step_avg:54.17ms
step:1473/1900 train_time:79824ms step_avg:54.19ms
step:1474/1900 train_time:79911ms step_avg:54.21ms
step:1475/1900 train_time:80001ms step_avg:54.24ms
step:1476/1900 train_time:80090ms step_avg:54.26ms
step:1477/1900 train_time:80178ms step_avg:54.28ms
step:1478/1900 train_time:80266ms step_avg:54.31ms
step:1479/1900 train_time:80354ms step_avg:54.33ms
step:1480/1900 train_time:80441ms step_avg:54.35ms
step:1481/1900 train_time:80529ms step_avg:54.38ms
step:1482/1900 train_time:80617ms step_avg:54.40ms
step:1483/1900 train_time:80707ms step_avg:54.42ms
step:1484/1900 train_time:80794ms step_avg:54.44ms
step:1485/1900 train_time:80883ms step_avg:54.47ms
step:1486/1900 train_time:80970ms step_avg:54.49ms
step:1487/1900 train_time:81061ms step_avg:54.51ms
step:1488/1900 train_time:81149ms step_avg:54.54ms
step:1489/1900 train_time:81238ms step_avg:54.56ms
step:1490/1900 train_time:81325ms step_avg:54.58ms
step:1491/1900 train_time:81414ms step_avg:54.60ms
step:1492/1900 train_time:81501ms step_avg:54.63ms
step:1493/1900 train_time:81589ms step_avg:54.65ms
step:1494/1900 train_time:81677ms step_avg:54.67ms
step:1495/1900 train_time:81765ms step_avg:54.69ms
step:1496/1900 train_time:81854ms step_avg:54.71ms
step:1497/1900 train_time:81942ms step_avg:54.74ms
step:1498/1900 train_time:82030ms step_avg:54.76ms
step:1499/1900 train_time:82119ms step_avg:54.78ms
step:1500/1900 train_time:82207ms step_avg:54.80ms
step:1500/1900 val_loss:3.4115 train_time:82297ms step_avg:54.86ms
step:1501/1900 train_time:82317ms step_avg:54.84ms
step:1502/1900 train_time:82384ms step_avg:54.85ms
step:1503/1900 train_time:82476ms step_avg:54.87ms
step:1504/1900 train_time:82564ms step_avg:54.90ms
step:1505/1900 train_time:82653ms step_avg:54.92ms
step:1506/1900 train_time:82740ms step_avg:54.94ms
step:1507/1900 train_time:82827ms step_avg:54.96ms
step:1508/1900 train_time:82913ms step_avg:54.98ms
step:1509/1900 train_time:83000ms step_avg:55.00ms
step:1510/1900 train_time:83088ms step_avg:55.03ms
step:1511/1900 train_time:83176ms step_avg:55.05ms
step:1512/1900 train_time:83265ms step_avg:55.07ms
step:1513/1900 train_time:83357ms step_avg:55.09ms
step:1514/1900 train_time:83447ms step_avg:55.12ms
step:1515/1900 train_time:83536ms step_avg:55.14ms
step:1516/1900 train_time:83624ms step_avg:55.16ms
step:1517/1900 train_time:83714ms step_avg:55.18ms
step:1518/1900 train_time:83800ms step_avg:55.20ms
step:1519/1900 train_time:83889ms step_avg:55.23ms
step:1520/1900 train_time:83975ms step_avg:55.25ms
step:1521/1900 train_time:84062ms step_avg:55.27ms
step:1522/1900 train_time:84150ms step_avg:55.29ms
step:1523/1900 train_time:84240ms step_avg:55.31ms
step:1524/1900 train_time:84328ms step_avg:55.33ms
step:1525/1900 train_time:84418ms step_avg:55.36ms
step:1526/1900 train_time:84506ms step_avg:55.38ms
step:1527/1900 train_time:84595ms step_avg:55.40ms
step:1528/1900 train_time:84683ms step_avg:55.42ms
step:1529/1900 train_time:84771ms step_avg:55.44ms
step:1530/1900 train_time:84858ms step_avg:55.46ms
step:1531/1900 train_time:84947ms step_avg:55.48ms
step:1532/1900 train_time:85034ms step_avg:55.51ms
step:1533/1900 train_time:85122ms step_avg:55.53ms
step:1534/1900 train_time:85209ms step_avg:55.55ms
step:1535/1900 train_time:85298ms step_avg:55.57ms
step:1536/1900 train_time:85388ms step_avg:55.59ms
step:1537/1900 train_time:85477ms step_avg:55.61ms
step:1538/1900 train_time:85565ms step_avg:55.63ms
step:1539/1900 train_time:85654ms step_avg:55.66ms
step:1540/1900 train_time:85741ms step_avg:55.68ms
step:1541/1900 train_time:85830ms step_avg:55.70ms
step:1542/1900 train_time:85917ms step_avg:55.72ms
step:1543/1900 train_time:86005ms step_avg:55.74ms
step:1544/1900 train_time:86092ms step_avg:55.76ms
step:1545/1900 train_time:86181ms step_avg:55.78ms
step:1546/1900 train_time:86268ms step_avg:55.80ms
step:1547/1900 train_time:86358ms step_avg:55.82ms
step:1548/1900 train_time:86446ms step_avg:55.84ms
step:1549/1900 train_time:86535ms step_avg:55.87ms
step:1550/1900 train_time:86623ms step_avg:55.89ms
step:1551/1900 train_time:86712ms step_avg:55.91ms
step:1552/1900 train_time:86799ms step_avg:55.93ms
step:1553/1900 train_time:86888ms step_avg:55.95ms
step:1554/1900 train_time:86974ms step_avg:55.97ms
step:1555/1900 train_time:87063ms step_avg:55.99ms
step:1556/1900 train_time:87151ms step_avg:56.01ms
step:1557/1900 train_time:87239ms step_avg:56.03ms
step:1558/1900 train_time:87328ms step_avg:56.05ms
step:1559/1900 train_time:87417ms step_avg:56.07ms
step:1560/1900 train_time:87505ms step_avg:56.09ms
step:1561/1900 train_time:87593ms step_avg:56.11ms
step:1562/1900 train_time:87682ms step_avg:56.13ms
step:1563/1900 train_time:87771ms step_avg:56.16ms
step:1564/1900 train_time:87858ms step_avg:56.17ms
step:1565/1900 train_time:87946ms step_avg:56.20ms
step:1566/1900 train_time:88034ms step_avg:56.22ms
step:1567/1900 train_time:88123ms step_avg:56.24ms
step:1568/1900 train_time:88210ms step_avg:56.26ms
step:1569/1900 train_time:88298ms step_avg:56.28ms
step:1570/1900 train_time:88386ms step_avg:56.30ms
step:1571/1900 train_time:88475ms step_avg:56.32ms
step:1572/1900 train_time:88563ms step_avg:56.34ms
step:1573/1900 train_time:88652ms step_avg:56.36ms
step:1574/1900 train_time:88740ms step_avg:56.38ms
step:1575/1900 train_time:88829ms step_avg:56.40ms
step:1576/1900 train_time:88916ms step_avg:56.42ms
step:1577/1900 train_time:89005ms step_avg:56.44ms
step:1578/1900 train_time:89092ms step_avg:56.46ms
step:1579/1900 train_time:89181ms step_avg:56.48ms
step:1580/1900 train_time:89268ms step_avg:56.50ms
step:1581/1900 train_time:89357ms step_avg:56.52ms
step:1582/1900 train_time:89446ms step_avg:56.54ms
step:1583/1900 train_time:89534ms step_avg:56.56ms
step:1584/1900 train_time:89622ms step_avg:56.58ms
step:1585/1900 train_time:89711ms step_avg:56.60ms
step:1586/1900 train_time:89799ms step_avg:56.62ms
step:1587/1900 train_time:89888ms step_avg:56.64ms
step:1588/1900 train_time:89976ms step_avg:56.66ms
step:1589/1900 train_time:90065ms step_avg:56.68ms
step:1590/1900 train_time:90152ms step_avg:56.70ms
step:1591/1900 train_time:90240ms step_avg:56.72ms
step:1592/1900 train_time:90328ms step_avg:56.74ms
step:1593/1900 train_time:90417ms step_avg:56.76ms
step:1594/1900 train_time:90505ms step_avg:56.78ms
step:1595/1900 train_time:90594ms step_avg:56.80ms
step:1596/1900 train_time:90682ms step_avg:56.82ms
step:1597/1900 train_time:90771ms step_avg:56.84ms
step:1598/1900 train_time:90858ms step_avg:56.86ms
step:1599/1900 train_time:90947ms step_avg:56.88ms
step:1600/1900 train_time:91034ms step_avg:56.90ms
step:1601/1900 train_time:91122ms step_avg:56.92ms
step:1602/1900 train_time:91210ms step_avg:56.94ms
step:1603/1900 train_time:91298ms step_avg:56.95ms
step:1604/1900 train_time:91387ms step_avg:56.97ms
step:1605/1900 train_time:91476ms step_avg:56.99ms
step:1606/1900 train_time:91564ms step_avg:57.01ms
step:1607/1900 train_time:91653ms step_avg:57.03ms
step:1608/1900 train_time:91741ms step_avg:57.05ms
step:1609/1900 train_time:91830ms step_avg:57.07ms
step:1610/1900 train_time:91918ms step_avg:57.09ms
step:1611/1900 train_time:92007ms step_avg:57.11ms
step:1612/1900 train_time:92094ms step_avg:57.13ms
step:1613/1900 train_time:92183ms step_avg:57.15ms
step:1614/1900 train_time:92270ms step_avg:57.17ms
step:1615/1900 train_time:92359ms step_avg:57.19ms
step:1616/1900 train_time:92447ms step_avg:57.21ms
step:1617/1900 train_time:92536ms step_avg:57.23ms
step:1618/1900 train_time:92624ms step_avg:57.25ms
step:1619/1900 train_time:92714ms step_avg:57.27ms
step:1620/1900 train_time:92802ms step_avg:57.28ms
step:1621/1900 train_time:92892ms step_avg:57.31ms
step:1622/1900 train_time:92980ms step_avg:57.32ms
step:1623/1900 train_time:93069ms step_avg:57.34ms
step:1624/1900 train_time:93157ms step_avg:57.36ms
step:1625/1900 train_time:93246ms step_avg:57.38ms
step:1626/1900 train_time:93333ms step_avg:57.40ms
step:1627/1900 train_time:93422ms step_avg:57.42ms
step:1628/1900 train_time:93510ms step_avg:57.44ms
step:1629/1900 train_time:93599ms step_avg:57.46ms
step:1630/1900 train_time:93687ms step_avg:57.48ms
step:1631/1900 train_time:93775ms step_avg:57.50ms
step:1632/1900 train_time:93864ms step_avg:57.51ms
step:1633/1900 train_time:93954ms step_avg:57.53ms
step:1634/1900 train_time:94042ms step_avg:57.55ms
step:1635/1900 train_time:94131ms step_avg:57.57ms
step:1636/1900 train_time:94218ms step_avg:57.59ms
step:1637/1900 train_time:94308ms step_avg:57.61ms
step:1638/1900 train_time:94395ms step_avg:57.63ms
step:1639/1900 train_time:94483ms step_avg:57.65ms
step:1640/1900 train_time:94570ms step_avg:57.66ms
step:1641/1900 train_time:94659ms step_avg:57.68ms
step:1642/1900 train_time:94747ms step_avg:57.70ms
step:1643/1900 train_time:94835ms step_avg:57.72ms
step:1644/1900 train_time:94923ms step_avg:57.74ms
step:1645/1900 train_time:95012ms step_avg:57.76ms
step:1646/1900 train_time:95101ms step_avg:57.78ms
step:1647/1900 train_time:95190ms step_avg:57.80ms
step:1648/1900 train_time:95277ms step_avg:57.81ms
step:1649/1900 train_time:95366ms step_avg:57.83ms
step:1650/1900 train_time:95454ms step_avg:57.85ms
step:1651/1900 train_time:95542ms step_avg:57.87ms
step:1652/1900 train_time:95629ms step_avg:57.89ms
step:1653/1900 train_time:95718ms step_avg:57.91ms
step:1654/1900 train_time:95805ms step_avg:57.92ms
step:1655/1900 train_time:95893ms step_avg:57.94ms
step:1656/1900 train_time:95981ms step_avg:57.96ms
step:1657/1900 train_time:96070ms step_avg:57.98ms
step:1658/1900 train_time:96158ms step_avg:58.00ms
step:1659/1900 train_time:96247ms step_avg:58.02ms
step:1660/1900 train_time:96335ms step_avg:58.03ms
step:1661/1900 train_time:96424ms step_avg:58.05ms
step:1662/1900 train_time:96512ms step_avg:58.07ms
step:1663/1900 train_time:96600ms step_avg:58.09ms
step:1664/1900 train_time:96688ms step_avg:58.11ms
step:1665/1900 train_time:96776ms step_avg:58.12ms
step:1666/1900 train_time:96864ms step_avg:58.14ms
step:1667/1900 train_time:96953ms step_avg:58.16ms
step:1668/1900 train_time:97041ms step_avg:58.18ms
step:1669/1900 train_time:97131ms step_avg:58.20ms
step:1670/1900 train_time:97219ms step_avg:58.21ms
step:1671/1900 train_time:97308ms step_avg:58.23ms
step:1672/1900 train_time:97396ms step_avg:58.25ms
step:1673/1900 train_time:97484ms step_avg:58.27ms
step:1674/1900 train_time:97572ms step_avg:58.29ms
step:1675/1900 train_time:97660ms step_avg:58.30ms
step:1676/1900 train_time:97748ms step_avg:58.32ms
step:1677/1900 train_time:97836ms step_avg:58.34ms
step:1678/1900 train_time:97925ms step_avg:58.36ms
step:1679/1900 train_time:98014ms step_avg:58.38ms
step:1680/1900 train_time:98102ms step_avg:58.39ms
step:1681/1900 train_time:98192ms step_avg:58.41ms
step:1682/1900 train_time:98279ms step_avg:58.43ms
step:1683/1900 train_time:98368ms step_avg:58.45ms
step:1684/1900 train_time:98455ms step_avg:58.47ms
step:1685/1900 train_time:98544ms step_avg:58.48ms
step:1686/1900 train_time:98631ms step_avg:58.50ms
step:1687/1900 train_time:98720ms step_avg:58.52ms
step:1688/1900 train_time:98807ms step_avg:58.54ms
step:1689/1900 train_time:98896ms step_avg:58.55ms
step:1690/1900 train_time:98984ms step_avg:58.57ms
step:1691/1900 train_time:99072ms step_avg:58.59ms
step:1692/1900 train_time:99160ms step_avg:58.61ms
step:1693/1900 train_time:99248ms step_avg:58.62ms
step:1694/1900 train_time:99336ms step_avg:58.64ms
step:1695/1900 train_time:99424ms step_avg:58.66ms
step:1696/1900 train_time:99511ms step_avg:58.67ms
step:1697/1900 train_time:99600ms step_avg:58.69ms
step:1698/1900 train_time:99688ms step_avg:58.71ms
step:1699/1900 train_time:99776ms step_avg:58.73ms
step:1700/1900 train_time:99863ms step_avg:58.74ms
step:1701/1900 train_time:99952ms step_avg:58.76ms
step:1702/1900 train_time:100040ms step_avg:58.78ms
step:1703/1900 train_time:100129ms step_avg:58.80ms
step:1704/1900 train_time:100217ms step_avg:58.81ms
step:1705/1900 train_time:100306ms step_avg:58.83ms
step:1706/1900 train_time:100393ms step_avg:58.85ms
step:1707/1900 train_time:100482ms step_avg:58.86ms
step:1708/1900 train_time:100570ms step_avg:58.88ms
step:1709/1900 train_time:100658ms step_avg:58.90ms
step:1710/1900 train_time:100746ms step_avg:58.92ms
step:1711/1900 train_time:100834ms step_avg:58.93ms
step:1712/1900 train_time:100923ms step_avg:58.95ms
step:1713/1900 train_time:101011ms step_avg:58.97ms
step:1714/1900 train_time:101099ms step_avg:58.98ms
step:1715/1900 train_time:101189ms step_avg:59.00ms
step:1716/1900 train_time:101277ms step_avg:59.02ms
step:1717/1900 train_time:101365ms step_avg:59.04ms
step:1718/1900 train_time:101453ms step_avg:59.05ms
step:1719/1900 train_time:101542ms step_avg:59.07ms
step:1720/1900 train_time:101630ms step_avg:59.09ms
step:1721/1900 train_time:101718ms step_avg:59.10ms
step:1722/1900 train_time:101806ms step_avg:59.12ms
step:1723/1900 train_time:101894ms step_avg:59.14ms
step:1724/1900 train_time:101982ms step_avg:59.15ms
step:1725/1900 train_time:102071ms step_avg:59.17ms
step:1726/1900 train_time:102158ms step_avg:59.19ms
step:1727/1900 train_time:102248ms step_avg:59.21ms
step:1728/1900 train_time:102335ms step_avg:59.22ms
step:1729/1900 train_time:102423ms step_avg:59.24ms
step:1730/1900 train_time:102510ms step_avg:59.25ms
step:1731/1900 train_time:102599ms step_avg:59.27ms
step:1732/1900 train_time:102688ms step_avg:59.29ms
step:1733/1900 train_time:102776ms step_avg:59.31ms
step:1734/1900 train_time:102864ms step_avg:59.32ms
step:1735/1900 train_time:102953ms step_avg:59.34ms
step:1736/1900 train_time:103041ms step_avg:59.36ms
step:1737/1900 train_time:103131ms step_avg:59.37ms
step:1738/1900 train_time:103219ms step_avg:59.39ms
step:1739/1900 train_time:103307ms step_avg:59.41ms
step:1740/1900 train_time:103395ms step_avg:59.42ms
step:1741/1900 train_time:103483ms step_avg:59.44ms
step:1742/1900 train_time:103570ms step_avg:59.45ms
step:1743/1900 train_time:103660ms step_avg:59.47ms
step:1744/1900 train_time:103748ms step_avg:59.49ms
step:1745/1900 train_time:103836ms step_avg:59.51ms
step:1746/1900 train_time:103924ms step_avg:59.52ms
step:1747/1900 train_time:104013ms step_avg:59.54ms
step:1748/1900 train_time:104101ms step_avg:59.55ms
step:1749/1900 train_time:104189ms step_avg:59.57ms
step:1750/1900 train_time:104278ms step_avg:59.59ms
step:1750/1900 val_loss:3.3168 train_time:104368ms step_avg:59.64ms
step:1751/1900 train_time:104388ms step_avg:59.62ms
step:1752/1900 train_time:104459ms step_avg:59.62ms
step:1753/1900 train_time:104551ms step_avg:59.64ms
step:1754/1900 train_time:104639ms step_avg:59.66ms
step:1755/1900 train_time:104727ms step_avg:59.67ms
step:1756/1900 train_time:104813ms step_avg:59.69ms
step:1757/1900 train_time:104901ms step_avg:59.70ms
step:1758/1900 train_time:104987ms step_avg:59.72ms
step:1759/1900 train_time:105075ms step_avg:59.74ms
step:1760/1900 train_time:105162ms step_avg:59.75ms
step:1761/1900 train_time:105250ms step_avg:59.77ms
step:1762/1900 train_time:105339ms step_avg:59.78ms
step:1763/1900 train_time:105430ms step_avg:59.80ms
step:1764/1900 train_time:105520ms step_avg:59.82ms
step:1765/1900 train_time:105610ms step_avg:59.84ms
step:1766/1900 train_time:105698ms step_avg:59.85ms
step:1767/1900 train_time:105785ms step_avg:59.87ms
step:1768/1900 train_time:105872ms step_avg:59.88ms
step:1769/1900 train_time:105960ms step_avg:59.90ms
step:1770/1900 train_time:106046ms step_avg:59.91ms
step:1771/1900 train_time:106134ms step_avg:59.93ms
step:1772/1900 train_time:106221ms step_avg:59.94ms
step:1773/1900 train_time:106311ms step_avg:59.96ms
step:1774/1900 train_time:106400ms step_avg:59.98ms
step:1775/1900 train_time:106492ms step_avg:60.00ms
step:1776/1900 train_time:106581ms step_avg:60.01ms
step:1777/1900 train_time:106670ms step_avg:60.03ms
step:1778/1900 train_time:106758ms step_avg:60.04ms
step:1779/1900 train_time:106845ms step_avg:60.06ms
step:1780/1900 train_time:106932ms step_avg:60.07ms
step:1781/1900 train_time:107021ms step_avg:60.09ms
step:1782/1900 train_time:107108ms step_avg:60.11ms
step:1783/1900 train_time:107196ms step_avg:60.12ms
step:1784/1900 train_time:107283ms step_avg:60.14ms
step:1785/1900 train_time:107374ms step_avg:60.15ms
step:1786/1900 train_time:107462ms step_avg:60.17ms
step:1787/1900 train_time:107552ms step_avg:60.19ms
step:1788/1900 train_time:107640ms step_avg:60.20ms
step:1789/1900 train_time:107729ms step_avg:60.22ms
step:1790/1900 train_time:107817ms step_avg:60.23ms
step:1791/1900 train_time:107905ms step_avg:60.25ms
step:1792/1900 train_time:107992ms step_avg:60.26ms
step:1793/1900 train_time:108080ms step_avg:60.28ms
step:1794/1900 train_time:108167ms step_avg:60.29ms
step:1795/1900 train_time:108256ms step_avg:60.31ms
step:1796/1900 train_time:108344ms step_avg:60.33ms
step:1797/1900 train_time:108433ms step_avg:60.34ms
step:1798/1900 train_time:108522ms step_avg:60.36ms
step:1799/1900 train_time:108612ms step_avg:60.37ms
step:1800/1900 train_time:108700ms step_avg:60.39ms
step:1801/1900 train_time:108789ms step_avg:60.40ms
step:1802/1900 train_time:108876ms step_avg:60.42ms
step:1803/1900 train_time:108964ms step_avg:60.43ms
step:1804/1900 train_time:109051ms step_avg:60.45ms
step:1805/1900 train_time:109141ms step_avg:60.47ms
step:1806/1900 train_time:109228ms step_avg:60.48ms
step:1807/1900 train_time:109319ms step_avg:60.50ms
step:1808/1900 train_time:109406ms step_avg:60.51ms
step:1809/1900 train_time:109495ms step_avg:60.53ms
step:1810/1900 train_time:109583ms step_avg:60.54ms
step:1811/1900 train_time:109673ms step_avg:60.56ms
step:1812/1900 train_time:109760ms step_avg:60.57ms
step:1813/1900 train_time:109849ms step_avg:60.59ms
step:1814/1900 train_time:109936ms step_avg:60.60ms
step:1815/1900 train_time:110023ms step_avg:60.62ms
step:1816/1900 train_time:110111ms step_avg:60.63ms
step:1817/1900 train_time:110199ms step_avg:60.65ms
step:1818/1900 train_time:110286ms step_avg:60.66ms
step:1819/1900 train_time:110375ms step_avg:60.68ms
step:1820/1900 train_time:110463ms step_avg:60.69ms
step:1821/1900 train_time:110552ms step_avg:60.71ms
step:1822/1900 train_time:110640ms step_avg:60.72ms
step:1823/1900 train_time:110729ms step_avg:60.74ms
step:1824/1900 train_time:110817ms step_avg:60.75ms
step:1825/1900 train_time:110905ms step_avg:60.77ms
step:1826/1900 train_time:110991ms step_avg:60.78ms
step:1827/1900 train_time:111081ms step_avg:60.80ms
step:1828/1900 train_time:111168ms step_avg:60.81ms
step:1829/1900 train_time:111258ms step_avg:60.83ms
step:1830/1900 train_time:111346ms step_avg:60.84ms
step:1831/1900 train_time:111435ms step_avg:60.86ms
step:1832/1900 train_time:111523ms step_avg:60.87ms
step:1833/1900 train_time:111611ms step_avg:60.89ms
step:1834/1900 train_time:111700ms step_avg:60.90ms
step:1835/1900 train_time:111788ms step_avg:60.92ms
step:1836/1900 train_time:111876ms step_avg:60.93ms
step:1837/1900 train_time:111964ms step_avg:60.95ms
step:1838/1900 train_time:112051ms step_avg:60.96ms
step:1839/1900 train_time:112140ms step_avg:60.98ms
step:1840/1900 train_time:112227ms step_avg:60.99ms
step:1841/1900 train_time:112317ms step_avg:61.01ms
step:1842/1900 train_time:112404ms step_avg:61.02ms
step:1843/1900 train_time:112493ms step_avg:61.04ms
step:1844/1900 train_time:112581ms step_avg:61.05ms
step:1845/1900 train_time:112670ms step_avg:61.07ms
step:1846/1900 train_time:112758ms step_avg:61.08ms
step:1847/1900 train_time:112846ms step_avg:61.10ms
step:1848/1900 train_time:112934ms step_avg:61.11ms
step:1849/1900 train_time:113022ms step_avg:61.13ms
step:1850/1900 train_time:113110ms step_avg:61.14ms
step:1851/1900 train_time:113199ms step_avg:61.16ms
step:1852/1900 train_time:113286ms step_avg:61.17ms
step:1853/1900 train_time:113375ms step_avg:61.18ms
step:1854/1900 train_time:113462ms step_avg:61.20ms
step:1855/1900 train_time:113551ms step_avg:61.21ms
step:1856/1900 train_time:113639ms step_avg:61.23ms
step:1857/1900 train_time:113728ms step_avg:61.24ms
step:1858/1900 train_time:113816ms step_avg:61.26ms
step:1859/1900 train_time:113904ms step_avg:61.27ms
step:1860/1900 train_time:113991ms step_avg:61.29ms
step:1861/1900 train_time:114080ms step_avg:61.30ms
step:1862/1900 train_time:114168ms step_avg:61.31ms
step:1863/1900 train_time:114257ms step_avg:61.33ms
step:1864/1900 train_time:114345ms step_avg:61.34ms
step:1865/1900 train_time:114434ms step_avg:61.36ms
step:1866/1900 train_time:114522ms step_avg:61.37ms
step:1867/1900 train_time:114611ms step_avg:61.39ms
step:1868/1900 train_time:114699ms step_avg:61.40ms
step:1869/1900 train_time:114787ms step_avg:61.42ms
step:1870/1900 train_time:114876ms step_avg:61.43ms
step:1871/1900 train_time:114964ms step_avg:61.45ms
step:1872/1900 train_time:115053ms step_avg:61.46ms
step:1873/1900 train_time:115142ms step_avg:61.47ms
step:1874/1900 train_time:115229ms step_avg:61.49ms
step:1875/1900 train_time:115318ms step_avg:61.50ms
step:1876/1900 train_time:115406ms step_avg:61.52ms
step:1877/1900 train_time:115496ms step_avg:61.53ms
step:1878/1900 train_time:115584ms step_avg:61.55ms
step:1879/1900 train_time:115672ms step_avg:61.56ms
step:1880/1900 train_time:115760ms step_avg:61.57ms
step:1881/1900 train_time:115850ms step_avg:61.59ms
step:1882/1900 train_time:115937ms step_avg:61.60ms
step:1883/1900 train_time:116025ms step_avg:61.62ms
step:1884/1900 train_time:116113ms step_avg:61.63ms
step:1885/1900 train_time:116203ms step_avg:61.65ms
step:1886/1900 train_time:116290ms step_avg:61.66ms
step:1887/1900 train_time:116380ms step_avg:61.67ms
step:1888/1900 train_time:116468ms step_avg:61.69ms
step:1889/1900 train_time:116557ms step_avg:61.70ms
step:1890/1900 train_time:116646ms step_avg:61.72ms
step:1891/1900 train_time:116734ms step_avg:61.73ms
step:1892/1900 train_time:116823ms step_avg:61.75ms
step:1893/1900 train_time:116911ms step_avg:61.76ms
step:1894/1900 train_time:116999ms step_avg:61.77ms
step:1895/1900 train_time:117088ms step_avg:61.79ms
step:1896/1900 train_time:117176ms step_avg:61.80ms
step:1897/1900 train_time:117265ms step_avg:61.82ms
step:1898/1900 train_time:117353ms step_avg:61.83ms
step:1899/1900 train_time:117443ms step_avg:61.84ms
step:1900/1900 train_time:117531ms step_avg:61.86ms
step:1900/1900 val_loss:3.2760 train_time:117623ms step_avg:61.91ms
peak memory allocated: 29709 MiB reserved: 42878 MiB

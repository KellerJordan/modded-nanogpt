import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:17:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   43C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     48550      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48551      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48552      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48553      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48554      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48555      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48556      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48557      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     48551      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     48552      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     48553      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     48554      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     48555      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     48556      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     48557      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8306 train_time:0ms step_avg:0.16ms
step:1/1900 train_time:73ms step_avg:73.09ms
step:2/1900 train_time:96ms step_avg:48.12ms
step:3/1900 train_time:119ms step_avg:39.55ms
step:4/1900 train_time:152ms step_avg:38.11ms
step:5/1900 train_time:186ms step_avg:37.27ms
step:6/1900 train_time:268ms step_avg:44.62ms
step:7/1900 train_time:401ms step_avg:57.32ms
step:8/1900 train_time:435ms step_avg:54.40ms
step:9/1900 train_time:469ms step_avg:52.12ms
step:10/1900 train_time:503ms step_avg:50.29ms
step:11/1900 train_time:537ms step_avg:48.81ms
step:12/1900 train_time:571ms step_avg:47.56ms
step:13/1900 train_time:605ms step_avg:46.52ms
step:14/1900 train_time:639ms step_avg:45.61ms
step:15/1900 train_time:673ms step_avg:44.84ms
step:16/1900 train_time:707ms step_avg:44.16ms
step:17/1900 train_time:740ms step_avg:43.56ms
step:18/1900 train_time:774ms step_avg:43.02ms
step:19/1900 train_time:809ms step_avg:42.55ms
step:20/1900 train_time:842ms step_avg:42.12ms
step:21/1900 train_time:876ms step_avg:41.74ms
step:22/1900 train_time:910ms step_avg:41.38ms
step:23/1900 train_time:944ms step_avg:41.06ms
step:24/1900 train_time:978ms step_avg:40.76ms
step:25/1900 train_time:1012ms step_avg:40.49ms
step:26/1900 train_time:1046ms step_avg:40.23ms
step:27/1900 train_time:1080ms step_avg:40.00ms
step:28/1900 train_time:1114ms step_avg:39.79ms
step:29/1900 train_time:1148ms step_avg:39.59ms
step:30/1900 train_time:1182ms step_avg:39.40ms
step:31/1900 train_time:1216ms step_avg:39.22ms
step:32/1900 train_time:1250ms step_avg:39.06ms
step:33/1900 train_time:1284ms step_avg:38.91ms
step:34/1900 train_time:1318ms step_avg:38.78ms
step:35/1900 train_time:1353ms step_avg:38.67ms
step:36/1900 train_time:1387ms step_avg:38.54ms
step:37/1900 train_time:1422ms step_avg:38.43ms
step:38/1900 train_time:1456ms step_avg:38.31ms
step:39/1900 train_time:1490ms step_avg:38.22ms
step:40/1900 train_time:1524ms step_avg:38.11ms
step:41/1900 train_time:1559ms step_avg:38.02ms
step:42/1900 train_time:1593ms step_avg:37.93ms
step:43/1900 train_time:1627ms step_avg:37.84ms
step:44/1900 train_time:1661ms step_avg:37.75ms
step:45/1900 train_time:1695ms step_avg:37.67ms
step:46/1900 train_time:1729ms step_avg:37.59ms
step:47/1900 train_time:1763ms step_avg:37.52ms
step:48/1900 train_time:1797ms step_avg:37.44ms
step:49/1900 train_time:1831ms step_avg:37.37ms
step:50/1900 train_time:1865ms step_avg:37.30ms
step:51/1900 train_time:1899ms step_avg:37.24ms
step:52/1900 train_time:1933ms step_avg:37.17ms
step:53/1900 train_time:1967ms step_avg:37.11ms
step:54/1900 train_time:2001ms step_avg:37.05ms
step:55/1900 train_time:2035ms step_avg:37.00ms
step:56/1900 train_time:2069ms step_avg:36.94ms
step:57/1900 train_time:2103ms step_avg:36.89ms
step:58/1900 train_time:2137ms step_avg:36.84ms
step:59/1900 train_time:2171ms step_avg:36.79ms
step:60/1900 train_time:2205ms step_avg:36.74ms
step:61/1900 train_time:2239ms step_avg:36.70ms
step:62/1900 train_time:2273ms step_avg:36.65ms
step:63/1900 train_time:2307ms step_avg:36.62ms
step:64/1900 train_time:2341ms step_avg:36.57ms
step:65/1900 train_time:2375ms step_avg:36.54ms
step:66/1900 train_time:2409ms step_avg:36.50ms
step:67/1900 train_time:2443ms step_avg:36.47ms
step:68/1900 train_time:2477ms step_avg:36.43ms
step:69/1900 train_time:2511ms step_avg:36.40ms
step:70/1900 train_time:2545ms step_avg:36.36ms
step:71/1900 train_time:2579ms step_avg:36.33ms
step:72/1900 train_time:2613ms step_avg:36.30ms
step:73/1900 train_time:2647ms step_avg:36.26ms
step:74/1900 train_time:2681ms step_avg:36.23ms
step:75/1900 train_time:2716ms step_avg:36.21ms
step:76/1900 train_time:2749ms step_avg:36.18ms
step:77/1900 train_time:2784ms step_avg:36.15ms
step:78/1900 train_time:2818ms step_avg:36.13ms
step:79/1900 train_time:2852ms step_avg:36.10ms
step:80/1900 train_time:2886ms step_avg:36.07ms
step:81/1900 train_time:2920ms step_avg:36.05ms
step:82/1900 train_time:2954ms step_avg:36.02ms
step:83/1900 train_time:2988ms step_avg:36.00ms
step:84/1900 train_time:3022ms step_avg:35.98ms
step:85/1900 train_time:3056ms step_avg:35.95ms
step:86/1900 train_time:3090ms step_avg:35.93ms
step:87/1900 train_time:3124ms step_avg:35.91ms
step:88/1900 train_time:3158ms step_avg:35.89ms
step:89/1900 train_time:3192ms step_avg:35.87ms
step:90/1900 train_time:3226ms step_avg:35.85ms
step:91/1900 train_time:3260ms step_avg:35.82ms
step:92/1900 train_time:3294ms step_avg:35.80ms
step:93/1900 train_time:3328ms step_avg:35.79ms
step:94/1900 train_time:3362ms step_avg:35.77ms
step:95/1900 train_time:3396ms step_avg:35.75ms
step:96/1900 train_time:3430ms step_avg:35.73ms
step:97/1900 train_time:3464ms step_avg:35.71ms
step:98/1900 train_time:3498ms step_avg:35.69ms
step:99/1900 train_time:3532ms step_avg:35.68ms
step:100/1900 train_time:3566ms step_avg:35.66ms
step:101/1900 train_time:3600ms step_avg:35.64ms
step:102/1900 train_time:3634ms step_avg:35.62ms
step:103/1900 train_time:3668ms step_avg:35.61ms
step:104/1900 train_time:3702ms step_avg:35.59ms
step:105/1900 train_time:3736ms step_avg:35.58ms
step:106/1900 train_time:3770ms step_avg:35.56ms
step:107/1900 train_time:3804ms step_avg:35.55ms
step:108/1900 train_time:3838ms step_avg:35.54ms
step:109/1900 train_time:3872ms step_avg:35.52ms
step:110/1900 train_time:3906ms step_avg:35.51ms
step:111/1900 train_time:3940ms step_avg:35.50ms
step:112/1900 train_time:3974ms step_avg:35.48ms
step:113/1900 train_time:4008ms step_avg:35.47ms
step:114/1900 train_time:4042ms step_avg:35.46ms
step:115/1900 train_time:4077ms step_avg:35.45ms
step:116/1900 train_time:4111ms step_avg:35.44ms
step:117/1900 train_time:4145ms step_avg:35.43ms
step:118/1900 train_time:4179ms step_avg:35.41ms
step:119/1900 train_time:4212ms step_avg:35.40ms
step:120/1900 train_time:4246ms step_avg:35.39ms
step:121/1900 train_time:4280ms step_avg:35.37ms
step:122/1900 train_time:4314ms step_avg:35.36ms
step:123/1900 train_time:4348ms step_avg:35.35ms
step:124/1900 train_time:4382ms step_avg:35.34ms
step:125/1900 train_time:4416ms step_avg:35.33ms
step:126/1900 train_time:4450ms step_avg:35.32ms
step:127/1900 train_time:4484ms step_avg:35.31ms
step:128/1900 train_time:4518ms step_avg:35.30ms
step:129/1900 train_time:4552ms step_avg:35.29ms
step:130/1900 train_time:4586ms step_avg:35.27ms
step:131/1900 train_time:4620ms step_avg:35.26ms
step:132/1900 train_time:4653ms step_avg:35.25ms
step:133/1900 train_time:4688ms step_avg:35.25ms
step:134/1900 train_time:4722ms step_avg:35.24ms
step:135/1900 train_time:4756ms step_avg:35.23ms
step:136/1900 train_time:4790ms step_avg:35.22ms
step:137/1900 train_time:4824ms step_avg:35.21ms
step:138/1900 train_time:4858ms step_avg:35.20ms
step:139/1900 train_time:4892ms step_avg:35.19ms
step:140/1900 train_time:4926ms step_avg:35.18ms
step:141/1900 train_time:4960ms step_avg:35.18ms
step:142/1900 train_time:4994ms step_avg:35.17ms
step:143/1900 train_time:5028ms step_avg:35.16ms
step:144/1900 train_time:5061ms step_avg:35.15ms
step:145/1900 train_time:5096ms step_avg:35.14ms
step:146/1900 train_time:5130ms step_avg:35.13ms
step:147/1900 train_time:5164ms step_avg:35.13ms
step:148/1900 train_time:5198ms step_avg:35.12ms
step:149/1900 train_time:5232ms step_avg:35.11ms
step:150/1900 train_time:5266ms step_avg:35.11ms
step:151/1900 train_time:5299ms step_avg:35.10ms
step:152/1900 train_time:5333ms step_avg:35.09ms
step:153/1900 train_time:5367ms step_avg:35.08ms
step:154/1900 train_time:5401ms step_avg:35.07ms
step:155/1900 train_time:5435ms step_avg:35.06ms
step:156/1900 train_time:5469ms step_avg:35.06ms
step:157/1900 train_time:5503ms step_avg:35.05ms
step:158/1900 train_time:5537ms step_avg:35.04ms
step:159/1900 train_time:5570ms step_avg:35.03ms
step:160/1900 train_time:5604ms step_avg:35.03ms
step:161/1900 train_time:5638ms step_avg:35.02ms
step:162/1900 train_time:5672ms step_avg:35.01ms
step:163/1900 train_time:5706ms step_avg:35.01ms
step:164/1900 train_time:5740ms step_avg:35.00ms
step:165/1900 train_time:5774ms step_avg:34.99ms
step:166/1900 train_time:5807ms step_avg:34.98ms
step:167/1900 train_time:5841ms step_avg:34.98ms
step:168/1900 train_time:5875ms step_avg:34.97ms
step:169/1900 train_time:5909ms step_avg:34.97ms
step:170/1900 train_time:5943ms step_avg:34.96ms
step:171/1900 train_time:5977ms step_avg:34.95ms
step:172/1900 train_time:6011ms step_avg:34.95ms
step:173/1900 train_time:6045ms step_avg:34.94ms
step:174/1900 train_time:6079ms step_avg:34.94ms
step:175/1900 train_time:6113ms step_avg:34.93ms
step:176/1900 train_time:6147ms step_avg:34.92ms
step:177/1900 train_time:6181ms step_avg:34.92ms
step:178/1900 train_time:6214ms step_avg:34.91ms
step:179/1900 train_time:6249ms step_avg:34.91ms
step:180/1900 train_time:6283ms step_avg:34.90ms
step:181/1900 train_time:6317ms step_avg:34.90ms
step:182/1900 train_time:6351ms step_avg:34.89ms
step:183/1900 train_time:6385ms step_avg:34.89ms
step:184/1900 train_time:6419ms step_avg:34.88ms
step:185/1900 train_time:6453ms step_avg:34.88ms
step:186/1900 train_time:6486ms step_avg:34.87ms
step:187/1900 train_time:6521ms step_avg:34.87ms
step:188/1900 train_time:6555ms step_avg:34.87ms
step:189/1900 train_time:6589ms step_avg:34.86ms
step:190/1900 train_time:6623ms step_avg:34.86ms
step:191/1900 train_time:6657ms step_avg:34.85ms
step:192/1900 train_time:6691ms step_avg:34.85ms
step:193/1900 train_time:6725ms step_avg:34.84ms
step:194/1900 train_time:6759ms step_avg:34.84ms
step:195/1900 train_time:6792ms step_avg:34.83ms
step:196/1900 train_time:6826ms step_avg:34.83ms
step:197/1900 train_time:6860ms step_avg:34.82ms
step:198/1900 train_time:6894ms step_avg:34.82ms
step:199/1900 train_time:6928ms step_avg:34.81ms
step:200/1900 train_time:6962ms step_avg:34.81ms
step:201/1900 train_time:6995ms step_avg:34.80ms
step:202/1900 train_time:7029ms step_avg:34.80ms
step:203/1900 train_time:7063ms step_avg:34.79ms
step:204/1900 train_time:7097ms step_avg:34.79ms
step:205/1900 train_time:7131ms step_avg:34.79ms
step:206/1900 train_time:7165ms step_avg:34.78ms
step:207/1900 train_time:7199ms step_avg:34.78ms
step:208/1900 train_time:7233ms step_avg:34.77ms
step:209/1900 train_time:7267ms step_avg:34.77ms
step:210/1900 train_time:7301ms step_avg:34.77ms
step:211/1900 train_time:7335ms step_avg:34.76ms
step:212/1900 train_time:7368ms step_avg:34.76ms
step:213/1900 train_time:7402ms step_avg:34.75ms
step:214/1900 train_time:7436ms step_avg:34.75ms
step:215/1900 train_time:7470ms step_avg:34.75ms
step:216/1900 train_time:7504ms step_avg:34.74ms
step:217/1900 train_time:7538ms step_avg:34.74ms
step:218/1900 train_time:7572ms step_avg:34.73ms
step:219/1900 train_time:7606ms step_avg:34.73ms
step:220/1900 train_time:7640ms step_avg:34.73ms
step:221/1900 train_time:7674ms step_avg:34.72ms
step:222/1900 train_time:7708ms step_avg:34.72ms
step:223/1900 train_time:7742ms step_avg:34.72ms
step:224/1900 train_time:7776ms step_avg:34.71ms
step:225/1900 train_time:7810ms step_avg:34.71ms
step:226/1900 train_time:7844ms step_avg:34.71ms
step:227/1900 train_time:7878ms step_avg:34.70ms
step:228/1900 train_time:7912ms step_avg:34.70ms
step:229/1900 train_time:7946ms step_avg:34.70ms
step:230/1900 train_time:7979ms step_avg:34.69ms
step:231/1900 train_time:8013ms step_avg:34.69ms
step:232/1900 train_time:8047ms step_avg:34.69ms
step:233/1900 train_time:8081ms step_avg:34.68ms
step:234/1900 train_time:8115ms step_avg:34.68ms
step:235/1900 train_time:8149ms step_avg:34.68ms
step:236/1900 train_time:8183ms step_avg:34.67ms
step:237/1900 train_time:8217ms step_avg:34.67ms
step:238/1900 train_time:8251ms step_avg:34.67ms
step:239/1900 train_time:8285ms step_avg:34.66ms
step:240/1900 train_time:8319ms step_avg:34.66ms
step:241/1900 train_time:8353ms step_avg:34.66ms
step:242/1900 train_time:8387ms step_avg:34.66ms
step:243/1900 train_time:8420ms step_avg:34.65ms
step:244/1900 train_time:8454ms step_avg:34.65ms
step:245/1900 train_time:8488ms step_avg:34.65ms
step:246/1900 train_time:8522ms step_avg:34.64ms
step:247/1900 train_time:8556ms step_avg:34.64ms
step:248/1900 train_time:8590ms step_avg:34.64ms
step:249/1900 train_time:8624ms step_avg:34.64ms
step:250/1900 train_time:8658ms step_avg:34.63ms
step:250/1900 val_loss:4.6033 train_time:8695ms step_avg:34.78ms
step:251/1900 train_time:8714ms step_avg:34.72ms
step:252/1900 train_time:8734ms step_avg:34.66ms
step:253/1900 train_time:8763ms step_avg:34.64ms
step:254/1900 train_time:8797ms step_avg:34.63ms
step:255/1900 train_time:8832ms step_avg:34.63ms
step:256/1900 train_time:8866ms step_avg:34.63ms
step:257/1900 train_time:8900ms step_avg:34.63ms
step:258/1900 train_time:8934ms step_avg:34.63ms
step:259/1900 train_time:8968ms step_avg:34.62ms
step:260/1900 train_time:9002ms step_avg:34.62ms
step:261/1900 train_time:9036ms step_avg:34.62ms
step:262/1900 train_time:9070ms step_avg:34.62ms
step:263/1900 train_time:9104ms step_avg:34.61ms
step:264/1900 train_time:9137ms step_avg:34.61ms
step:265/1900 train_time:9171ms step_avg:34.61ms
step:266/1900 train_time:9205ms step_avg:34.61ms
step:267/1900 train_time:9239ms step_avg:34.60ms
step:268/1900 train_time:9273ms step_avg:34.60ms
step:269/1900 train_time:9307ms step_avg:34.60ms
step:270/1900 train_time:9341ms step_avg:34.60ms
step:271/1900 train_time:9375ms step_avg:34.59ms
step:272/1900 train_time:9408ms step_avg:34.59ms
step:273/1900 train_time:9442ms step_avg:34.59ms
step:274/1900 train_time:9476ms step_avg:34.58ms
step:275/1900 train_time:9510ms step_avg:34.58ms
step:276/1900 train_time:9544ms step_avg:34.58ms
step:277/1900 train_time:9578ms step_avg:34.58ms
step:278/1900 train_time:9611ms step_avg:34.57ms
step:279/1900 train_time:9645ms step_avg:34.57ms
step:280/1900 train_time:9679ms step_avg:34.57ms
step:281/1900 train_time:9713ms step_avg:34.57ms
step:282/1900 train_time:9747ms step_avg:34.56ms
step:283/1900 train_time:9781ms step_avg:34.56ms
step:284/1900 train_time:9815ms step_avg:34.56ms
step:285/1900 train_time:9849ms step_avg:34.56ms
step:286/1900 train_time:9883ms step_avg:34.56ms
step:287/1900 train_time:9918ms step_avg:34.56ms
step:288/1900 train_time:9951ms step_avg:34.55ms
step:289/1900 train_time:9985ms step_avg:34.55ms
step:290/1900 train_time:10019ms step_avg:34.55ms
step:291/1900 train_time:10053ms step_avg:34.55ms
step:292/1900 train_time:10087ms step_avg:34.54ms
step:293/1900 train_time:10121ms step_avg:34.54ms
step:294/1900 train_time:10155ms step_avg:34.54ms
step:295/1900 train_time:10189ms step_avg:34.54ms
step:296/1900 train_time:10223ms step_avg:34.54ms
step:297/1900 train_time:10257ms step_avg:34.54ms
step:298/1900 train_time:10291ms step_avg:34.53ms
step:299/1900 train_time:10325ms step_avg:34.53ms
step:300/1900 train_time:10358ms step_avg:34.53ms
step:301/1900 train_time:10393ms step_avg:34.53ms
step:302/1900 train_time:10426ms step_avg:34.52ms
step:303/1900 train_time:10460ms step_avg:34.52ms
step:304/1900 train_time:10494ms step_avg:34.52ms
step:305/1900 train_time:10528ms step_avg:34.52ms
step:306/1900 train_time:10562ms step_avg:34.52ms
step:307/1900 train_time:10596ms step_avg:34.52ms
step:308/1900 train_time:10630ms step_avg:34.51ms
step:309/1900 train_time:10664ms step_avg:34.51ms
step:310/1900 train_time:10698ms step_avg:34.51ms
step:311/1900 train_time:10732ms step_avg:34.51ms
step:312/1900 train_time:10765ms step_avg:34.50ms
step:313/1900 train_time:10799ms step_avg:34.50ms
step:314/1900 train_time:10833ms step_avg:34.50ms
step:315/1900 train_time:10867ms step_avg:34.50ms
step:316/1900 train_time:10901ms step_avg:34.50ms
step:317/1900 train_time:10935ms step_avg:34.50ms
step:318/1900 train_time:10969ms step_avg:34.49ms
step:319/1900 train_time:11003ms step_avg:34.49ms
step:320/1900 train_time:11037ms step_avg:34.49ms
step:321/1900 train_time:11071ms step_avg:34.49ms
step:322/1900 train_time:11105ms step_avg:34.49ms
step:323/1900 train_time:11139ms step_avg:34.49ms
step:324/1900 train_time:11173ms step_avg:34.48ms
step:325/1900 train_time:11207ms step_avg:34.48ms
step:326/1900 train_time:11241ms step_avg:34.48ms
step:327/1900 train_time:11275ms step_avg:34.48ms
step:328/1900 train_time:11309ms step_avg:34.48ms
step:329/1900 train_time:11343ms step_avg:34.48ms
step:330/1900 train_time:11377ms step_avg:34.47ms
step:331/1900 train_time:11411ms step_avg:34.47ms
step:332/1900 train_time:11445ms step_avg:34.47ms
step:333/1900 train_time:11478ms step_avg:34.47ms
step:334/1900 train_time:11512ms step_avg:34.47ms
step:335/1900 train_time:11546ms step_avg:34.47ms
step:336/1900 train_time:11580ms step_avg:34.46ms
step:337/1900 train_time:11614ms step_avg:34.46ms
step:338/1900 train_time:11648ms step_avg:34.46ms
step:339/1900 train_time:11682ms step_avg:34.46ms
step:340/1900 train_time:11716ms step_avg:34.46ms
step:341/1900 train_time:11750ms step_avg:34.46ms
step:342/1900 train_time:11784ms step_avg:34.46ms
step:343/1900 train_time:11818ms step_avg:34.46ms
step:344/1900 train_time:11852ms step_avg:34.45ms
step:345/1900 train_time:11886ms step_avg:34.45ms
step:346/1900 train_time:11920ms step_avg:34.45ms
step:347/1900 train_time:11954ms step_avg:34.45ms
step:348/1900 train_time:11988ms step_avg:34.45ms
step:349/1900 train_time:12022ms step_avg:34.45ms
step:350/1900 train_time:12056ms step_avg:34.44ms
step:351/1900 train_time:12090ms step_avg:34.44ms
step:352/1900 train_time:12123ms step_avg:34.44ms
step:353/1900 train_time:12157ms step_avg:34.44ms
step:354/1900 train_time:12191ms step_avg:34.44ms
step:355/1900 train_time:12225ms step_avg:34.44ms
step:356/1900 train_time:12259ms step_avg:34.44ms
step:357/1900 train_time:12293ms step_avg:34.44ms
step:358/1900 train_time:12327ms step_avg:34.43ms
step:359/1900 train_time:12361ms step_avg:34.43ms
step:360/1900 train_time:12395ms step_avg:34.43ms
step:361/1900 train_time:12429ms step_avg:34.43ms
step:362/1900 train_time:12463ms step_avg:34.43ms
step:363/1900 train_time:12497ms step_avg:34.43ms
step:364/1900 train_time:12531ms step_avg:34.43ms
step:365/1900 train_time:12565ms step_avg:34.42ms
step:366/1900 train_time:12599ms step_avg:34.42ms
step:367/1900 train_time:12633ms step_avg:34.42ms
step:368/1900 train_time:12666ms step_avg:34.42ms
step:369/1900 train_time:12700ms step_avg:34.42ms
step:370/1900 train_time:12734ms step_avg:34.42ms
step:371/1900 train_time:12768ms step_avg:34.42ms
step:372/1900 train_time:12802ms step_avg:34.41ms
step:373/1900 train_time:12836ms step_avg:34.41ms
step:374/1900 train_time:12870ms step_avg:34.41ms
step:375/1900 train_time:12903ms step_avg:34.41ms
step:376/1900 train_time:12937ms step_avg:34.41ms
step:377/1900 train_time:12971ms step_avg:34.41ms
step:378/1900 train_time:13005ms step_avg:34.40ms
step:379/1900 train_time:13039ms step_avg:34.40ms
step:380/1900 train_time:13072ms step_avg:34.40ms
step:381/1900 train_time:13106ms step_avg:34.40ms
step:382/1900 train_time:13140ms step_avg:34.40ms
step:383/1900 train_time:13174ms step_avg:34.40ms
step:384/1900 train_time:13208ms step_avg:34.40ms
step:385/1900 train_time:13242ms step_avg:34.39ms
step:386/1900 train_time:13275ms step_avg:34.39ms
step:387/1900 train_time:13309ms step_avg:34.39ms
step:388/1900 train_time:13343ms step_avg:34.39ms
step:389/1900 train_time:13377ms step_avg:34.39ms
step:390/1900 train_time:13411ms step_avg:34.39ms
step:391/1900 train_time:13445ms step_avg:34.39ms
step:392/1900 train_time:13479ms step_avg:34.38ms
step:393/1900 train_time:13513ms step_avg:34.38ms
step:394/1900 train_time:13547ms step_avg:34.38ms
step:395/1900 train_time:13581ms step_avg:34.38ms
step:396/1900 train_time:13615ms step_avg:34.38ms
step:397/1900 train_time:13649ms step_avg:34.38ms
step:398/1900 train_time:13683ms step_avg:34.38ms
step:399/1900 train_time:13717ms step_avg:34.38ms
step:400/1900 train_time:13750ms step_avg:34.38ms
step:401/1900 train_time:13785ms step_avg:34.38ms
step:402/1900 train_time:13818ms step_avg:34.37ms
step:403/1900 train_time:13852ms step_avg:34.37ms
step:404/1900 train_time:13886ms step_avg:34.37ms
step:405/1900 train_time:13920ms step_avg:34.37ms
step:406/1900 train_time:13954ms step_avg:34.37ms
step:407/1900 train_time:13988ms step_avg:34.37ms
step:408/1900 train_time:14022ms step_avg:34.37ms
step:409/1900 train_time:14056ms step_avg:34.37ms
step:410/1900 train_time:14090ms step_avg:34.37ms
step:411/1900 train_time:14124ms step_avg:34.36ms
step:412/1900 train_time:14158ms step_avg:34.36ms
step:413/1900 train_time:14192ms step_avg:34.36ms
step:414/1900 train_time:14225ms step_avg:34.36ms
step:415/1900 train_time:14259ms step_avg:34.36ms
step:416/1900 train_time:14293ms step_avg:34.36ms
step:417/1900 train_time:14327ms step_avg:34.36ms
step:418/1900 train_time:14361ms step_avg:34.36ms
step:419/1900 train_time:14395ms step_avg:34.36ms
step:420/1900 train_time:14429ms step_avg:34.35ms
step:421/1900 train_time:14463ms step_avg:34.35ms
step:422/1900 train_time:14497ms step_avg:34.35ms
step:423/1900 train_time:14531ms step_avg:34.35ms
step:424/1900 train_time:14564ms step_avg:34.35ms
step:425/1900 train_time:14598ms step_avg:34.35ms
step:426/1900 train_time:14632ms step_avg:34.35ms
step:427/1900 train_time:14666ms step_avg:34.35ms
step:428/1900 train_time:14700ms step_avg:34.35ms
step:429/1900 train_time:14734ms step_avg:34.34ms
step:430/1900 train_time:14768ms step_avg:34.34ms
step:431/1900 train_time:14802ms step_avg:34.34ms
step:432/1900 train_time:14835ms step_avg:34.34ms
step:433/1900 train_time:14869ms step_avg:34.34ms
step:434/1900 train_time:14903ms step_avg:34.34ms
step:435/1900 train_time:14937ms step_avg:34.34ms
step:436/1900 train_time:14971ms step_avg:34.34ms
step:437/1900 train_time:15005ms step_avg:34.34ms
step:438/1900 train_time:15039ms step_avg:34.34ms
step:439/1900 train_time:15073ms step_avg:34.34ms
step:440/1900 train_time:15107ms step_avg:34.33ms
step:441/1900 train_time:15141ms step_avg:34.33ms
step:442/1900 train_time:15175ms step_avg:34.33ms
step:443/1900 train_time:15209ms step_avg:34.33ms
step:444/1900 train_time:15242ms step_avg:34.33ms
step:445/1900 train_time:15277ms step_avg:34.33ms
step:446/1900 train_time:15310ms step_avg:34.33ms
step:447/1900 train_time:15344ms step_avg:34.33ms
step:448/1900 train_time:15378ms step_avg:34.33ms
step:449/1900 train_time:15412ms step_avg:34.33ms
step:450/1900 train_time:15446ms step_avg:34.32ms
step:451/1900 train_time:15480ms step_avg:34.32ms
step:452/1900 train_time:15514ms step_avg:34.32ms
step:453/1900 train_time:15548ms step_avg:34.32ms
step:454/1900 train_time:15582ms step_avg:34.32ms
step:455/1900 train_time:15616ms step_avg:34.32ms
step:456/1900 train_time:15650ms step_avg:34.32ms
step:457/1900 train_time:15684ms step_avg:34.32ms
step:458/1900 train_time:15718ms step_avg:34.32ms
step:459/1900 train_time:15752ms step_avg:34.32ms
step:460/1900 train_time:15786ms step_avg:34.32ms
step:461/1900 train_time:15820ms step_avg:34.32ms
step:462/1900 train_time:15853ms step_avg:34.31ms
step:463/1900 train_time:15887ms step_avg:34.31ms
step:464/1900 train_time:15921ms step_avg:34.31ms
step:465/1900 train_time:15955ms step_avg:34.31ms
step:466/1900 train_time:15989ms step_avg:34.31ms
step:467/1900 train_time:16023ms step_avg:34.31ms
step:468/1900 train_time:16057ms step_avg:34.31ms
step:469/1900 train_time:16091ms step_avg:34.31ms
step:470/1900 train_time:16124ms step_avg:34.31ms
step:471/1900 train_time:16158ms step_avg:34.31ms
step:472/1900 train_time:16192ms step_avg:34.31ms
step:473/1900 train_time:16226ms step_avg:34.30ms
step:474/1900 train_time:16260ms step_avg:34.30ms
step:475/1900 train_time:16293ms step_avg:34.30ms
step:476/1900 train_time:16327ms step_avg:34.30ms
step:477/1900 train_time:16361ms step_avg:34.30ms
step:478/1900 train_time:16395ms step_avg:34.30ms
step:479/1900 train_time:16429ms step_avg:34.30ms
step:480/1900 train_time:16463ms step_avg:34.30ms
step:481/1900 train_time:16497ms step_avg:34.30ms
step:482/1900 train_time:16530ms step_avg:34.30ms
step:483/1900 train_time:16564ms step_avg:34.29ms
step:484/1900 train_time:16598ms step_avg:34.29ms
step:485/1900 train_time:16632ms step_avg:34.29ms
step:486/1900 train_time:16666ms step_avg:34.29ms
step:487/1900 train_time:16700ms step_avg:34.29ms
step:488/1900 train_time:16734ms step_avg:34.29ms
step:489/1900 train_time:16768ms step_avg:34.29ms
step:490/1900 train_time:16802ms step_avg:34.29ms
step:491/1900 train_time:16836ms step_avg:34.29ms
step:492/1900 train_time:16870ms step_avg:34.29ms
step:493/1900 train_time:16904ms step_avg:34.29ms
step:494/1900 train_time:16937ms step_avg:34.29ms
step:495/1900 train_time:16972ms step_avg:34.29ms
step:496/1900 train_time:17005ms step_avg:34.29ms
step:497/1900 train_time:17039ms step_avg:34.28ms
step:498/1900 train_time:17073ms step_avg:34.28ms
step:499/1900 train_time:17107ms step_avg:34.28ms
step:500/1900 train_time:17141ms step_avg:34.28ms
step:500/1900 val_loss:4.2746 train_time:17178ms step_avg:34.36ms
step:501/1900 train_time:17197ms step_avg:34.33ms
step:502/1900 train_time:17216ms step_avg:34.30ms
step:503/1900 train_time:17248ms step_avg:34.29ms
step:504/1900 train_time:17282ms step_avg:34.29ms
step:505/1900 train_time:17316ms step_avg:34.29ms
step:506/1900 train_time:17351ms step_avg:34.29ms
step:507/1900 train_time:17386ms step_avg:34.29ms
step:508/1900 train_time:17419ms step_avg:34.29ms
step:509/1900 train_time:17454ms step_avg:34.29ms
step:510/1900 train_time:17487ms step_avg:34.29ms
step:511/1900 train_time:17521ms step_avg:34.29ms
step:512/1900 train_time:17555ms step_avg:34.29ms
step:513/1900 train_time:17589ms step_avg:34.29ms
step:514/1900 train_time:17623ms step_avg:34.29ms
step:515/1900 train_time:17657ms step_avg:34.28ms
step:516/1900 train_time:17690ms step_avg:34.28ms
step:517/1900 train_time:17724ms step_avg:34.28ms
step:518/1900 train_time:17758ms step_avg:34.28ms
step:519/1900 train_time:17792ms step_avg:34.28ms
step:520/1900 train_time:17826ms step_avg:34.28ms
step:521/1900 train_time:17860ms step_avg:34.28ms
step:522/1900 train_time:17894ms step_avg:34.28ms
step:523/1900 train_time:17928ms step_avg:34.28ms
step:524/1900 train_time:17961ms step_avg:34.28ms
step:525/1900 train_time:17995ms step_avg:34.28ms
step:526/1900 train_time:18029ms step_avg:34.28ms
step:527/1900 train_time:18063ms step_avg:34.28ms
step:528/1900 train_time:18097ms step_avg:34.27ms
step:529/1900 train_time:18131ms step_avg:34.27ms
step:530/1900 train_time:18164ms step_avg:34.27ms
step:531/1900 train_time:18199ms step_avg:34.27ms
step:532/1900 train_time:18233ms step_avg:34.27ms
step:533/1900 train_time:18267ms step_avg:34.27ms
step:534/1900 train_time:18301ms step_avg:34.27ms
step:535/1900 train_time:18335ms step_avg:34.27ms
step:536/1900 train_time:18369ms step_avg:34.27ms
step:537/1900 train_time:18403ms step_avg:34.27ms
step:538/1900 train_time:18437ms step_avg:34.27ms
step:539/1900 train_time:18471ms step_avg:34.27ms
step:540/1900 train_time:18505ms step_avg:34.27ms
step:541/1900 train_time:18539ms step_avg:34.27ms
step:542/1900 train_time:18573ms step_avg:34.27ms
step:543/1900 train_time:18607ms step_avg:34.27ms
step:544/1900 train_time:18641ms step_avg:34.27ms
step:545/1900 train_time:18674ms step_avg:34.26ms
step:546/1900 train_time:18708ms step_avg:34.26ms
step:547/1900 train_time:18742ms step_avg:34.26ms
step:548/1900 train_time:18776ms step_avg:34.26ms
step:549/1900 train_time:18810ms step_avg:34.26ms
step:550/1900 train_time:18844ms step_avg:34.26ms
step:551/1900 train_time:18878ms step_avg:34.26ms
step:552/1900 train_time:18911ms step_avg:34.26ms
step:553/1900 train_time:18945ms step_avg:34.26ms
step:554/1900 train_time:18979ms step_avg:34.26ms
step:555/1900 train_time:19013ms step_avg:34.26ms
step:556/1900 train_time:19047ms step_avg:34.26ms
step:557/1900 train_time:19081ms step_avg:34.26ms
step:558/1900 train_time:19115ms step_avg:34.26ms
step:559/1900 train_time:19149ms step_avg:34.26ms
step:560/1900 train_time:19183ms step_avg:34.25ms
step:561/1900 train_time:19217ms step_avg:34.25ms
step:562/1900 train_time:19251ms step_avg:34.25ms
step:563/1900 train_time:19285ms step_avg:34.25ms
step:564/1900 train_time:19319ms step_avg:34.25ms
step:565/1900 train_time:19353ms step_avg:34.25ms
step:566/1900 train_time:19386ms step_avg:34.25ms
step:567/1900 train_time:19421ms step_avg:34.25ms
step:568/1900 train_time:19455ms step_avg:34.25ms
step:569/1900 train_time:19489ms step_avg:34.25ms
step:570/1900 train_time:19523ms step_avg:34.25ms
step:571/1900 train_time:19557ms step_avg:34.25ms
step:572/1900 train_time:19590ms step_avg:34.25ms
step:573/1900 train_time:19624ms step_avg:34.25ms
step:574/1900 train_time:19658ms step_avg:34.25ms
step:575/1900 train_time:19692ms step_avg:34.25ms
step:576/1900 train_time:19726ms step_avg:34.25ms
step:577/1900 train_time:19760ms step_avg:34.25ms
step:578/1900 train_time:19794ms step_avg:34.25ms
step:579/1900 train_time:19828ms step_avg:34.24ms
step:580/1900 train_time:19862ms step_avg:34.24ms
step:581/1900 train_time:19895ms step_avg:34.24ms
step:582/1900 train_time:19929ms step_avg:34.24ms
step:583/1900 train_time:19963ms step_avg:34.24ms
step:584/1900 train_time:19997ms step_avg:34.24ms
step:585/1900 train_time:20031ms step_avg:34.24ms
step:586/1900 train_time:20065ms step_avg:34.24ms
step:587/1900 train_time:20099ms step_avg:34.24ms
step:588/1900 train_time:20133ms step_avg:34.24ms
step:589/1900 train_time:20167ms step_avg:34.24ms
step:590/1900 train_time:20200ms step_avg:34.24ms
step:591/1900 train_time:20234ms step_avg:34.24ms
step:592/1900 train_time:20268ms step_avg:34.24ms
step:593/1900 train_time:20302ms step_avg:34.24ms
step:594/1900 train_time:20336ms step_avg:34.24ms
step:595/1900 train_time:20370ms step_avg:34.24ms
step:596/1900 train_time:20404ms step_avg:34.24ms
step:597/1900 train_time:20438ms step_avg:34.24ms
step:598/1900 train_time:20472ms step_avg:34.23ms
step:599/1900 train_time:20506ms step_avg:34.23ms
step:600/1900 train_time:20540ms step_avg:34.23ms
step:601/1900 train_time:20574ms step_avg:34.23ms
step:602/1900 train_time:20608ms step_avg:34.23ms
step:603/1900 train_time:20642ms step_avg:34.23ms
step:604/1900 train_time:20676ms step_avg:34.23ms
step:605/1900 train_time:20710ms step_avg:34.23ms
step:606/1900 train_time:20743ms step_avg:34.23ms
step:607/1900 train_time:20777ms step_avg:34.23ms
step:608/1900 train_time:20811ms step_avg:34.23ms
step:609/1900 train_time:20845ms step_avg:34.23ms
step:610/1900 train_time:20879ms step_avg:34.23ms
step:611/1900 train_time:20913ms step_avg:34.23ms
step:612/1900 train_time:20946ms step_avg:34.23ms
step:613/1900 train_time:20980ms step_avg:34.23ms
step:614/1900 train_time:21015ms step_avg:34.23ms
step:615/1900 train_time:21048ms step_avg:34.22ms
step:616/1900 train_time:21082ms step_avg:34.22ms
step:617/1900 train_time:21116ms step_avg:34.22ms
step:618/1900 train_time:21150ms step_avg:34.22ms
step:619/1900 train_time:21184ms step_avg:34.22ms
step:620/1900 train_time:21218ms step_avg:34.22ms
step:621/1900 train_time:21253ms step_avg:34.22ms
step:622/1900 train_time:21313ms step_avg:34.26ms
step:623/1900 train_time:21374ms step_avg:34.31ms
step:624/1900 train_time:21435ms step_avg:34.35ms
step:625/1900 train_time:21497ms step_avg:34.40ms
step:626/1900 train_time:21558ms step_avg:34.44ms
step:627/1900 train_time:21620ms step_avg:34.48ms
step:628/1900 train_time:21681ms step_avg:34.52ms
step:629/1900 train_time:21742ms step_avg:34.57ms
step:630/1900 train_time:21804ms step_avg:34.61ms
step:631/1900 train_time:21865ms step_avg:34.65ms
step:632/1900 train_time:21926ms step_avg:34.69ms
step:633/1900 train_time:21988ms step_avg:34.74ms
step:634/1900 train_time:22049ms step_avg:34.78ms
step:635/1900 train_time:22111ms step_avg:34.82ms
step:636/1900 train_time:22172ms step_avg:34.86ms
step:637/1900 train_time:22234ms step_avg:34.90ms
step:638/1900 train_time:22294ms step_avg:34.94ms
step:639/1900 train_time:22356ms step_avg:34.99ms
step:640/1900 train_time:22417ms step_avg:35.03ms
step:641/1900 train_time:22479ms step_avg:35.07ms
step:642/1900 train_time:22540ms step_avg:35.11ms
step:643/1900 train_time:22601ms step_avg:35.15ms
step:644/1900 train_time:22662ms step_avg:35.19ms
step:645/1900 train_time:22724ms step_avg:35.23ms
step:646/1900 train_time:22785ms step_avg:35.27ms
step:647/1900 train_time:22847ms step_avg:35.31ms
step:648/1900 train_time:22908ms step_avg:35.35ms
step:649/1900 train_time:22970ms step_avg:35.39ms
step:650/1900 train_time:23031ms step_avg:35.43ms
step:651/1900 train_time:23093ms step_avg:35.47ms
step:652/1900 train_time:23153ms step_avg:35.51ms
step:653/1900 train_time:23215ms step_avg:35.55ms
step:654/1900 train_time:23276ms step_avg:35.59ms
step:655/1900 train_time:23337ms step_avg:35.63ms
step:656/1900 train_time:23398ms step_avg:35.67ms
step:657/1900 train_time:23460ms step_avg:35.71ms
step:658/1900 train_time:23521ms step_avg:35.75ms
step:659/1900 train_time:23582ms step_avg:35.78ms
step:660/1900 train_time:23643ms step_avg:35.82ms
step:661/1900 train_time:23705ms step_avg:35.86ms
step:662/1900 train_time:23765ms step_avg:35.90ms
step:663/1900 train_time:23827ms step_avg:35.94ms
step:664/1900 train_time:23889ms step_avg:35.98ms
step:665/1900 train_time:23951ms step_avg:36.02ms
step:666/1900 train_time:24011ms step_avg:36.05ms
step:667/1900 train_time:24073ms step_avg:36.09ms
step:668/1900 train_time:24133ms step_avg:36.13ms
step:669/1900 train_time:24196ms step_avg:36.17ms
step:670/1900 train_time:24257ms step_avg:36.20ms
step:671/1900 train_time:24319ms step_avg:36.24ms
step:672/1900 train_time:24379ms step_avg:36.28ms
step:673/1900 train_time:24441ms step_avg:36.32ms
step:674/1900 train_time:24502ms step_avg:36.35ms
step:675/1900 train_time:24564ms step_avg:36.39ms
step:676/1900 train_time:24624ms step_avg:36.43ms
step:677/1900 train_time:24686ms step_avg:36.46ms
step:678/1900 train_time:24747ms step_avg:36.50ms
step:679/1900 train_time:24809ms step_avg:36.54ms
step:680/1900 train_time:24870ms step_avg:36.57ms
step:681/1900 train_time:24932ms step_avg:36.61ms
step:682/1900 train_time:24993ms step_avg:36.65ms
step:683/1900 train_time:25055ms step_avg:36.68ms
step:684/1900 train_time:25115ms step_avg:36.72ms
step:685/1900 train_time:25177ms step_avg:36.75ms
step:686/1900 train_time:25238ms step_avg:36.79ms
step:687/1900 train_time:25300ms step_avg:36.83ms
step:688/1900 train_time:25361ms step_avg:36.86ms
step:689/1900 train_time:25422ms step_avg:36.90ms
step:690/1900 train_time:25483ms step_avg:36.93ms
step:691/1900 train_time:25545ms step_avg:36.97ms
step:692/1900 train_time:25606ms step_avg:37.00ms
step:693/1900 train_time:25667ms step_avg:37.04ms
step:694/1900 train_time:25727ms step_avg:37.07ms
step:695/1900 train_time:25789ms step_avg:37.11ms
step:696/1900 train_time:25850ms step_avg:37.14ms
step:697/1900 train_time:25912ms step_avg:37.18ms
step:698/1900 train_time:25973ms step_avg:37.21ms
step:699/1900 train_time:26035ms step_avg:37.25ms
step:700/1900 train_time:26096ms step_avg:37.28ms
step:701/1900 train_time:26158ms step_avg:37.32ms
step:702/1900 train_time:26218ms step_avg:37.35ms
step:703/1900 train_time:26280ms step_avg:37.38ms
step:704/1900 train_time:26341ms step_avg:37.42ms
step:705/1900 train_time:26402ms step_avg:37.45ms
step:706/1900 train_time:26463ms step_avg:37.48ms
step:707/1900 train_time:26525ms step_avg:37.52ms
step:708/1900 train_time:26585ms step_avg:37.55ms
step:709/1900 train_time:26647ms step_avg:37.58ms
step:710/1900 train_time:26708ms step_avg:37.62ms
step:711/1900 train_time:26769ms step_avg:37.65ms
step:712/1900 train_time:26830ms step_avg:37.68ms
step:713/1900 train_time:26892ms step_avg:37.72ms
step:714/1900 train_time:26953ms step_avg:37.75ms
step:715/1900 train_time:27014ms step_avg:37.78ms
step:716/1900 train_time:27076ms step_avg:37.82ms
step:717/1900 train_time:27138ms step_avg:37.85ms
step:718/1900 train_time:27199ms step_avg:37.88ms
step:719/1900 train_time:27261ms step_avg:37.91ms
step:720/1900 train_time:27322ms step_avg:37.95ms
step:721/1900 train_time:27383ms step_avg:37.98ms
step:722/1900 train_time:27444ms step_avg:38.01ms
step:723/1900 train_time:27506ms step_avg:38.04ms
step:724/1900 train_time:27567ms step_avg:38.08ms
step:725/1900 train_time:27628ms step_avg:38.11ms
step:726/1900 train_time:27689ms step_avg:38.14ms
step:727/1900 train_time:27751ms step_avg:38.17ms
step:728/1900 train_time:27812ms step_avg:38.20ms
step:729/1900 train_time:27873ms step_avg:38.23ms
step:730/1900 train_time:27934ms step_avg:38.27ms
step:731/1900 train_time:27996ms step_avg:38.30ms
step:732/1900 train_time:28058ms step_avg:38.33ms
step:733/1900 train_time:28119ms step_avg:38.36ms
step:734/1900 train_time:28181ms step_avg:38.39ms
step:735/1900 train_time:28242ms step_avg:38.42ms
step:736/1900 train_time:28303ms step_avg:38.45ms
step:737/1900 train_time:28365ms step_avg:38.49ms
step:738/1900 train_time:28425ms step_avg:38.52ms
step:739/1900 train_time:28487ms step_avg:38.55ms
step:740/1900 train_time:28548ms step_avg:38.58ms
step:741/1900 train_time:28609ms step_avg:38.61ms
step:742/1900 train_time:28670ms step_avg:38.64ms
step:743/1900 train_time:28732ms step_avg:38.67ms
step:744/1900 train_time:28792ms step_avg:38.70ms
step:745/1900 train_time:28854ms step_avg:38.73ms
step:746/1900 train_time:28915ms step_avg:38.76ms
step:747/1900 train_time:28977ms step_avg:38.79ms
step:748/1900 train_time:29038ms step_avg:38.82ms
step:749/1900 train_time:29100ms step_avg:38.85ms
step:750/1900 train_time:29160ms step_avg:38.88ms
step:750/1900 val_loss:4.0084 train_time:29224ms step_avg:38.97ms
step:751/1900 train_time:29244ms step_avg:38.94ms
step:752/1900 train_time:29285ms step_avg:38.94ms
step:753/1900 train_time:29348ms step_avg:38.97ms
step:754/1900 train_time:29410ms step_avg:39.01ms
step:755/1900 train_time:29473ms step_avg:39.04ms
step:756/1900 train_time:29534ms step_avg:39.07ms
step:757/1900 train_time:29595ms step_avg:39.10ms
step:758/1900 train_time:29656ms step_avg:39.12ms
step:759/1900 train_time:29716ms step_avg:39.15ms
step:760/1900 train_time:29777ms step_avg:39.18ms
step:761/1900 train_time:29838ms step_avg:39.21ms
step:762/1900 train_time:29899ms step_avg:39.24ms
step:763/1900 train_time:29960ms step_avg:39.27ms
step:764/1900 train_time:30021ms step_avg:39.29ms
step:765/1900 train_time:30082ms step_avg:39.32ms
step:766/1900 train_time:30143ms step_avg:39.35ms
step:767/1900 train_time:30206ms step_avg:39.38ms
step:768/1900 train_time:30268ms step_avg:39.41ms
step:769/1900 train_time:30331ms step_avg:39.44ms
step:770/1900 train_time:30393ms step_avg:39.47ms
step:771/1900 train_time:30455ms step_avg:39.50ms
step:772/1900 train_time:30516ms step_avg:39.53ms
step:773/1900 train_time:30577ms step_avg:39.56ms
step:774/1900 train_time:30638ms step_avg:39.58ms
step:775/1900 train_time:30699ms step_avg:39.61ms
step:776/1900 train_time:30760ms step_avg:39.64ms
step:777/1900 train_time:30822ms step_avg:39.67ms
step:778/1900 train_time:30882ms step_avg:39.69ms
step:779/1900 train_time:30943ms step_avg:39.72ms
step:780/1900 train_time:31004ms step_avg:39.75ms
step:781/1900 train_time:31066ms step_avg:39.78ms
step:782/1900 train_time:31127ms step_avg:39.80ms
step:783/1900 train_time:31189ms step_avg:39.83ms
step:784/1900 train_time:31251ms step_avg:39.86ms
step:785/1900 train_time:31313ms step_avg:39.89ms
step:786/1900 train_time:31374ms step_avg:39.92ms
step:787/1900 train_time:31437ms step_avg:39.94ms
step:788/1900 train_time:31498ms step_avg:39.97ms
step:789/1900 train_time:31560ms step_avg:40.00ms
step:790/1900 train_time:31621ms step_avg:40.03ms
step:791/1900 train_time:31683ms step_avg:40.05ms
step:792/1900 train_time:31743ms step_avg:40.08ms
step:793/1900 train_time:31805ms step_avg:40.11ms
step:794/1900 train_time:31865ms step_avg:40.13ms
step:795/1900 train_time:31927ms step_avg:40.16ms
step:796/1900 train_time:31987ms step_avg:40.18ms
step:797/1900 train_time:32049ms step_avg:40.21ms
step:798/1900 train_time:32110ms step_avg:40.24ms
step:799/1900 train_time:32171ms step_avg:40.26ms
step:800/1900 train_time:32233ms step_avg:40.29ms
step:801/1900 train_time:32294ms step_avg:40.32ms
step:802/1900 train_time:32356ms step_avg:40.34ms
step:803/1900 train_time:32418ms step_avg:40.37ms
step:804/1900 train_time:32479ms step_avg:40.40ms
step:805/1900 train_time:32542ms step_avg:40.42ms
step:806/1900 train_time:32603ms step_avg:40.45ms
step:807/1900 train_time:32665ms step_avg:40.48ms
step:808/1900 train_time:32726ms step_avg:40.50ms
step:809/1900 train_time:32787ms step_avg:40.53ms
step:810/1900 train_time:32848ms step_avg:40.55ms
step:811/1900 train_time:32910ms step_avg:40.58ms
step:812/1900 train_time:32970ms step_avg:40.60ms
step:813/1900 train_time:33032ms step_avg:40.63ms
step:814/1900 train_time:33093ms step_avg:40.65ms
step:815/1900 train_time:33155ms step_avg:40.68ms
step:816/1900 train_time:33216ms step_avg:40.71ms
step:817/1900 train_time:33278ms step_avg:40.73ms
step:818/1900 train_time:33339ms step_avg:40.76ms
step:819/1900 train_time:33401ms step_avg:40.78ms
step:820/1900 train_time:33463ms step_avg:40.81ms
step:821/1900 train_time:33525ms step_avg:40.83ms
step:822/1900 train_time:33586ms step_avg:40.86ms
step:823/1900 train_time:33648ms step_avg:40.88ms
step:824/1900 train_time:33709ms step_avg:40.91ms
step:825/1900 train_time:33771ms step_avg:40.93ms
step:826/1900 train_time:33831ms step_avg:40.96ms
step:827/1900 train_time:33893ms step_avg:40.98ms
step:828/1900 train_time:33954ms step_avg:41.01ms
step:829/1900 train_time:34016ms step_avg:41.03ms
step:830/1900 train_time:34077ms step_avg:41.06ms
step:831/1900 train_time:34138ms step_avg:41.08ms
step:832/1900 train_time:34199ms step_avg:41.10ms
step:833/1900 train_time:34261ms step_avg:41.13ms
step:834/1900 train_time:34322ms step_avg:41.15ms
step:835/1900 train_time:34384ms step_avg:41.18ms
step:836/1900 train_time:34446ms step_avg:41.20ms
step:837/1900 train_time:34508ms step_avg:41.23ms
step:838/1900 train_time:34569ms step_avg:41.25ms
step:839/1900 train_time:34631ms step_avg:41.28ms
step:840/1900 train_time:34693ms step_avg:41.30ms
step:841/1900 train_time:34754ms step_avg:41.33ms
step:842/1900 train_time:34815ms step_avg:41.35ms
step:843/1900 train_time:34877ms step_avg:41.37ms
step:844/1900 train_time:34938ms step_avg:41.40ms
step:845/1900 train_time:35000ms step_avg:41.42ms
step:846/1900 train_time:35061ms step_avg:41.44ms
step:847/1900 train_time:35122ms step_avg:41.47ms
step:848/1900 train_time:35183ms step_avg:41.49ms
step:849/1900 train_time:35245ms step_avg:41.51ms
step:850/1900 train_time:35307ms step_avg:41.54ms
step:851/1900 train_time:35369ms step_avg:41.56ms
step:852/1900 train_time:35429ms step_avg:41.58ms
step:853/1900 train_time:35492ms step_avg:41.61ms
step:854/1900 train_time:35553ms step_avg:41.63ms
step:855/1900 train_time:35614ms step_avg:41.65ms
step:856/1900 train_time:35675ms step_avg:41.68ms
step:857/1900 train_time:35737ms step_avg:41.70ms
step:858/1900 train_time:35798ms step_avg:41.72ms
step:859/1900 train_time:35860ms step_avg:41.75ms
step:860/1900 train_time:35920ms step_avg:41.77ms
step:861/1900 train_time:35982ms step_avg:41.79ms
step:862/1900 train_time:36044ms step_avg:41.81ms
step:863/1900 train_time:36105ms step_avg:41.84ms
step:864/1900 train_time:36166ms step_avg:41.86ms
step:865/1900 train_time:36228ms step_avg:41.88ms
step:866/1900 train_time:36289ms step_avg:41.90ms
step:867/1900 train_time:36351ms step_avg:41.93ms
step:868/1900 train_time:36412ms step_avg:41.95ms
step:869/1900 train_time:36474ms step_avg:41.97ms
step:870/1900 train_time:36535ms step_avg:41.99ms
step:871/1900 train_time:36597ms step_avg:42.02ms
step:872/1900 train_time:36658ms step_avg:42.04ms
step:873/1900 train_time:36719ms step_avg:42.06ms
step:874/1900 train_time:36781ms step_avg:42.08ms
step:875/1900 train_time:36843ms step_avg:42.11ms
step:876/1900 train_time:36904ms step_avg:42.13ms
step:877/1900 train_time:36966ms step_avg:42.15ms
step:878/1900 train_time:37027ms step_avg:42.17ms
step:879/1900 train_time:37089ms step_avg:42.20ms
step:880/1900 train_time:37150ms step_avg:42.22ms
step:881/1900 train_time:37212ms step_avg:42.24ms
step:882/1900 train_time:37272ms step_avg:42.26ms
step:883/1900 train_time:37334ms step_avg:42.28ms
step:884/1900 train_time:37395ms step_avg:42.30ms
step:885/1900 train_time:37457ms step_avg:42.32ms
step:886/1900 train_time:37518ms step_avg:42.35ms
step:887/1900 train_time:37580ms step_avg:42.37ms
step:888/1900 train_time:37641ms step_avg:42.39ms
step:889/1900 train_time:37703ms step_avg:42.41ms
step:890/1900 train_time:37764ms step_avg:42.43ms
step:891/1900 train_time:37826ms step_avg:42.45ms
step:892/1900 train_time:37888ms step_avg:42.48ms
step:893/1900 train_time:37950ms step_avg:42.50ms
step:894/1900 train_time:38011ms step_avg:42.52ms
step:895/1900 train_time:38072ms step_avg:42.54ms
step:896/1900 train_time:38133ms step_avg:42.56ms
step:897/1900 train_time:38195ms step_avg:42.58ms
step:898/1900 train_time:38256ms step_avg:42.60ms
step:899/1900 train_time:38317ms step_avg:42.62ms
step:900/1900 train_time:38378ms step_avg:42.64ms
step:901/1900 train_time:38440ms step_avg:42.66ms
step:902/1900 train_time:38501ms step_avg:42.68ms
step:903/1900 train_time:38562ms step_avg:42.70ms
step:904/1900 train_time:38624ms step_avg:42.73ms
step:905/1900 train_time:38686ms step_avg:42.75ms
step:906/1900 train_time:38747ms step_avg:42.77ms
step:907/1900 train_time:38809ms step_avg:42.79ms
step:908/1900 train_time:38869ms step_avg:42.81ms
step:909/1900 train_time:38931ms step_avg:42.83ms
step:910/1900 train_time:38992ms step_avg:42.85ms
step:911/1900 train_time:39054ms step_avg:42.87ms
step:912/1900 train_time:39115ms step_avg:42.89ms
step:913/1900 train_time:39176ms step_avg:42.91ms
step:914/1900 train_time:39237ms step_avg:42.93ms
step:915/1900 train_time:39299ms step_avg:42.95ms
step:916/1900 train_time:39360ms step_avg:42.97ms
step:917/1900 train_time:39422ms step_avg:42.99ms
step:918/1900 train_time:39483ms step_avg:43.01ms
step:919/1900 train_time:39545ms step_avg:43.03ms
step:920/1900 train_time:39605ms step_avg:43.05ms
step:921/1900 train_time:39667ms step_avg:43.07ms
step:922/1900 train_time:39728ms step_avg:43.09ms
step:923/1900 train_time:39790ms step_avg:43.11ms
step:924/1900 train_time:39851ms step_avg:43.13ms
step:925/1900 train_time:39913ms step_avg:43.15ms
step:926/1900 train_time:39974ms step_avg:43.17ms
step:927/1900 train_time:40036ms step_avg:43.19ms
step:928/1900 train_time:40097ms step_avg:43.21ms
step:929/1900 train_time:40159ms step_avg:43.23ms
step:930/1900 train_time:40220ms step_avg:43.25ms
step:931/1900 train_time:40282ms step_avg:43.27ms
step:932/1900 train_time:40343ms step_avg:43.29ms
step:933/1900 train_time:40404ms step_avg:43.31ms
step:934/1900 train_time:40465ms step_avg:43.32ms
step:935/1900 train_time:40527ms step_avg:43.34ms
step:936/1900 train_time:40588ms step_avg:43.36ms
step:937/1900 train_time:40650ms step_avg:43.38ms
step:938/1900 train_time:40711ms step_avg:43.40ms
step:939/1900 train_time:40773ms step_avg:43.42ms
step:940/1900 train_time:40834ms step_avg:43.44ms
step:941/1900 train_time:40896ms step_avg:43.46ms
step:942/1900 train_time:40957ms step_avg:43.48ms
step:943/1900 train_time:41019ms step_avg:43.50ms
step:944/1900 train_time:41080ms step_avg:43.52ms
step:945/1900 train_time:41141ms step_avg:43.54ms
step:946/1900 train_time:41202ms step_avg:43.55ms
step:947/1900 train_time:41265ms step_avg:43.57ms
step:948/1900 train_time:41326ms step_avg:43.59ms
step:949/1900 train_time:41387ms step_avg:43.61ms
step:950/1900 train_time:41448ms step_avg:43.63ms
step:951/1900 train_time:41510ms step_avg:43.65ms
step:952/1900 train_time:41571ms step_avg:43.67ms
step:953/1900 train_time:41633ms step_avg:43.69ms
step:954/1900 train_time:41693ms step_avg:43.70ms
step:955/1900 train_time:41755ms step_avg:43.72ms
step:956/1900 train_time:41816ms step_avg:43.74ms
step:957/1900 train_time:41878ms step_avg:43.76ms
step:958/1900 train_time:41939ms step_avg:43.78ms
step:959/1900 train_time:42000ms step_avg:43.80ms
step:960/1900 train_time:42061ms step_avg:43.81ms
step:961/1900 train_time:42123ms step_avg:43.83ms
step:962/1900 train_time:42185ms step_avg:43.85ms
step:963/1900 train_time:42247ms step_avg:43.87ms
step:964/1900 train_time:42308ms step_avg:43.89ms
step:965/1900 train_time:42370ms step_avg:43.91ms
step:966/1900 train_time:42431ms step_avg:43.92ms
step:967/1900 train_time:42492ms step_avg:43.94ms
step:968/1900 train_time:42553ms step_avg:43.96ms
step:969/1900 train_time:42614ms step_avg:43.98ms
step:970/1900 train_time:42676ms step_avg:44.00ms
step:971/1900 train_time:42737ms step_avg:44.01ms
step:972/1900 train_time:42798ms step_avg:44.03ms
step:973/1900 train_time:42859ms step_avg:44.05ms
step:974/1900 train_time:42920ms step_avg:44.07ms
step:975/1900 train_time:42982ms step_avg:44.08ms
step:976/1900 train_time:43043ms step_avg:44.10ms
step:977/1900 train_time:43105ms step_avg:44.12ms
step:978/1900 train_time:43166ms step_avg:44.14ms
step:979/1900 train_time:43229ms step_avg:44.16ms
step:980/1900 train_time:43290ms step_avg:44.17ms
step:981/1900 train_time:43351ms step_avg:44.19ms
step:982/1900 train_time:43412ms step_avg:44.21ms
step:983/1900 train_time:43474ms step_avg:44.23ms
step:984/1900 train_time:43535ms step_avg:44.24ms
step:985/1900 train_time:43597ms step_avg:44.26ms
step:986/1900 train_time:43658ms step_avg:44.28ms
step:987/1900 train_time:43719ms step_avg:44.29ms
step:988/1900 train_time:43780ms step_avg:44.31ms
step:989/1900 train_time:43841ms step_avg:44.33ms
step:990/1900 train_time:43902ms step_avg:44.35ms
step:991/1900 train_time:43964ms step_avg:44.36ms
step:992/1900 train_time:44025ms step_avg:44.38ms
step:993/1900 train_time:44087ms step_avg:44.40ms
step:994/1900 train_time:44149ms step_avg:44.42ms
step:995/1900 train_time:44211ms step_avg:44.43ms
step:996/1900 train_time:44271ms step_avg:44.45ms
step:997/1900 train_time:44333ms step_avg:44.47ms
step:998/1900 train_time:44393ms step_avg:44.48ms
step:999/1900 train_time:44455ms step_avg:44.50ms
step:1000/1900 train_time:44516ms step_avg:44.52ms
step:1000/1900 val_loss:3.7843 train_time:44580ms step_avg:44.58ms
step:1001/1900 train_time:44600ms step_avg:44.56ms
step:1002/1900 train_time:44641ms step_avg:44.55ms
step:1003/1900 train_time:44704ms step_avg:44.57ms
step:1004/1900 train_time:44767ms step_avg:44.59ms
step:1005/1900 train_time:44828ms step_avg:44.61ms
step:1006/1900 train_time:44889ms step_avg:44.62ms
step:1007/1900 train_time:44950ms step_avg:44.64ms
step:1008/1900 train_time:45010ms step_avg:44.65ms
step:1009/1900 train_time:45072ms step_avg:44.67ms
step:1010/1900 train_time:45132ms step_avg:44.69ms
step:1011/1900 train_time:45194ms step_avg:44.70ms
step:1012/1900 train_time:45254ms step_avg:44.72ms
step:1013/1900 train_time:45316ms step_avg:44.73ms
step:1014/1900 train_time:45377ms step_avg:44.75ms
step:1015/1900 train_time:45438ms step_avg:44.77ms
step:1016/1900 train_time:45500ms step_avg:44.78ms
step:1017/1900 train_time:45563ms step_avg:44.80ms
step:1018/1900 train_time:45624ms step_avg:44.82ms
step:1019/1900 train_time:45687ms step_avg:44.83ms
step:1020/1900 train_time:45748ms step_avg:44.85ms
step:1021/1900 train_time:45810ms step_avg:44.87ms
step:1022/1900 train_time:45872ms step_avg:44.88ms
step:1023/1900 train_time:45933ms step_avg:44.90ms
step:1024/1900 train_time:45994ms step_avg:44.92ms
step:1025/1900 train_time:46056ms step_avg:44.93ms
step:1026/1900 train_time:46116ms step_avg:44.95ms
step:1027/1900 train_time:46177ms step_avg:44.96ms
step:1028/1900 train_time:46237ms step_avg:44.98ms
step:1029/1900 train_time:46299ms step_avg:44.99ms
step:1030/1900 train_time:46359ms step_avg:45.01ms
step:1031/1900 train_time:46421ms step_avg:45.03ms
step:1032/1900 train_time:46482ms step_avg:45.04ms
step:1033/1900 train_time:46544ms step_avg:45.06ms
step:1034/1900 train_time:46606ms step_avg:45.07ms
step:1035/1900 train_time:46668ms step_avg:45.09ms
step:1036/1900 train_time:46729ms step_avg:45.11ms
step:1037/1900 train_time:46792ms step_avg:45.12ms
step:1038/1900 train_time:46853ms step_avg:45.14ms
step:1039/1900 train_time:46915ms step_avg:45.15ms
step:1040/1900 train_time:46976ms step_avg:45.17ms
step:1041/1900 train_time:47038ms step_avg:45.19ms
step:1042/1900 train_time:47099ms step_avg:45.20ms
step:1043/1900 train_time:47161ms step_avg:45.22ms
step:1044/1900 train_time:47221ms step_avg:45.23ms
step:1045/1900 train_time:47282ms step_avg:45.25ms
step:1046/1900 train_time:47343ms step_avg:45.26ms
step:1047/1900 train_time:47405ms step_avg:45.28ms
step:1048/1900 train_time:47465ms step_avg:45.29ms
step:1049/1900 train_time:47527ms step_avg:45.31ms
step:1050/1900 train_time:47588ms step_avg:45.32ms
step:1051/1900 train_time:47651ms step_avg:45.34ms
step:1052/1900 train_time:47712ms step_avg:45.35ms
step:1053/1900 train_time:47774ms step_avg:45.37ms
step:1054/1900 train_time:47835ms step_avg:45.38ms
step:1055/1900 train_time:47897ms step_avg:45.40ms
step:1056/1900 train_time:47958ms step_avg:45.41ms
step:1057/1900 train_time:48020ms step_avg:45.43ms
step:1058/1900 train_time:48081ms step_avg:45.44ms
step:1059/1900 train_time:48143ms step_avg:45.46ms
step:1060/1900 train_time:48203ms step_avg:45.47ms
step:1061/1900 train_time:48265ms step_avg:45.49ms
step:1062/1900 train_time:48325ms step_avg:45.50ms
step:1063/1900 train_time:48387ms step_avg:45.52ms
step:1064/1900 train_time:48448ms step_avg:45.53ms
step:1065/1900 train_time:48510ms step_avg:45.55ms
step:1066/1900 train_time:48571ms step_avg:45.56ms
step:1067/1900 train_time:48633ms step_avg:45.58ms
step:1068/1900 train_time:48694ms step_avg:45.59ms
step:1069/1900 train_time:48756ms step_avg:45.61ms
step:1070/1900 train_time:48817ms step_avg:45.62ms
step:1071/1900 train_time:48879ms step_avg:45.64ms
step:1072/1900 train_time:48940ms step_avg:45.65ms
step:1073/1900 train_time:49001ms step_avg:45.67ms
step:1074/1900 train_time:49062ms step_avg:45.68ms
step:1075/1900 train_time:49124ms step_avg:45.70ms
step:1076/1900 train_time:49185ms step_avg:45.71ms
step:1077/1900 train_time:49247ms step_avg:45.73ms
step:1078/1900 train_time:49308ms step_avg:45.74ms
step:1079/1900 train_time:49369ms step_avg:45.75ms
step:1080/1900 train_time:49430ms step_avg:45.77ms
step:1081/1900 train_time:49492ms step_avg:45.78ms
step:1082/1900 train_time:49553ms step_avg:45.80ms
step:1083/1900 train_time:49614ms step_avg:45.81ms
step:1084/1900 train_time:49675ms step_avg:45.83ms
step:1085/1900 train_time:49737ms step_avg:45.84ms
step:1086/1900 train_time:49798ms step_avg:45.85ms
step:1087/1900 train_time:49861ms step_avg:45.87ms
step:1088/1900 train_time:49921ms step_avg:45.88ms
step:1089/1900 train_time:49983ms step_avg:45.90ms
step:1090/1900 train_time:50044ms step_avg:45.91ms
step:1091/1900 train_time:50105ms step_avg:45.93ms
step:1092/1900 train_time:50166ms step_avg:45.94ms
step:1093/1900 train_time:50227ms step_avg:45.95ms
step:1094/1900 train_time:50288ms step_avg:45.97ms
step:1095/1900 train_time:50349ms step_avg:45.98ms
step:1096/1900 train_time:50410ms step_avg:45.99ms
step:1097/1900 train_time:50472ms step_avg:46.01ms
step:1098/1900 train_time:50533ms step_avg:46.02ms
step:1099/1900 train_time:50595ms step_avg:46.04ms
step:1100/1900 train_time:50656ms step_avg:46.05ms
step:1101/1900 train_time:50718ms step_avg:46.07ms
step:1102/1900 train_time:50779ms step_avg:46.08ms
step:1103/1900 train_time:50841ms step_avg:46.09ms
step:1104/1900 train_time:50902ms step_avg:46.11ms
step:1105/1900 train_time:50964ms step_avg:46.12ms
step:1106/1900 train_time:51025ms step_avg:46.13ms
step:1107/1900 train_time:51086ms step_avg:46.15ms
step:1108/1900 train_time:51147ms step_avg:46.16ms
step:1109/1900 train_time:51209ms step_avg:46.18ms
step:1110/1900 train_time:51269ms step_avg:46.19ms
step:1111/1900 train_time:51331ms step_avg:46.20ms
step:1112/1900 train_time:51393ms step_avg:46.22ms
step:1113/1900 train_time:51455ms step_avg:46.23ms
step:1114/1900 train_time:51516ms step_avg:46.24ms
step:1115/1900 train_time:51578ms step_avg:46.26ms
step:1116/1900 train_time:51638ms step_avg:46.27ms
step:1117/1900 train_time:51701ms step_avg:46.29ms
step:1118/1900 train_time:51762ms step_avg:46.30ms
step:1119/1900 train_time:51823ms step_avg:46.31ms
step:1120/1900 train_time:51884ms step_avg:46.33ms
step:1121/1900 train_time:51946ms step_avg:46.34ms
step:1122/1900 train_time:52008ms step_avg:46.35ms
step:1123/1900 train_time:52069ms step_avg:46.37ms
step:1124/1900 train_time:52130ms step_avg:46.38ms
step:1125/1900 train_time:52192ms step_avg:46.39ms
step:1126/1900 train_time:52253ms step_avg:46.41ms
step:1127/1900 train_time:52314ms step_avg:46.42ms
step:1128/1900 train_time:52375ms step_avg:46.43ms
step:1129/1900 train_time:52437ms step_avg:46.45ms
step:1130/1900 train_time:52498ms step_avg:46.46ms
step:1131/1900 train_time:52560ms step_avg:46.47ms
step:1132/1900 train_time:52621ms step_avg:46.49ms
step:1133/1900 train_time:52683ms step_avg:46.50ms
step:1134/1900 train_time:52744ms step_avg:46.51ms
step:1135/1900 train_time:52806ms step_avg:46.52ms
step:1136/1900 train_time:52867ms step_avg:46.54ms
step:1137/1900 train_time:52928ms step_avg:46.55ms
step:1138/1900 train_time:52989ms step_avg:46.56ms
step:1139/1900 train_time:53050ms step_avg:46.58ms
step:1140/1900 train_time:53111ms step_avg:46.59ms
step:1141/1900 train_time:53173ms step_avg:46.60ms
step:1142/1900 train_time:53234ms step_avg:46.61ms
step:1143/1900 train_time:53295ms step_avg:46.63ms
step:1144/1900 train_time:53356ms step_avg:46.64ms
step:1145/1900 train_time:53418ms step_avg:46.65ms
step:1146/1900 train_time:53479ms step_avg:46.67ms
step:1147/1900 train_time:53541ms step_avg:46.68ms
step:1148/1900 train_time:53602ms step_avg:46.69ms
step:1149/1900 train_time:53664ms step_avg:46.71ms
step:1150/1900 train_time:53725ms step_avg:46.72ms
step:1151/1900 train_time:53787ms step_avg:46.73ms
step:1152/1900 train_time:53848ms step_avg:46.74ms
step:1153/1900 train_time:53909ms step_avg:46.76ms
step:1154/1900 train_time:53970ms step_avg:46.77ms
step:1155/1900 train_time:54032ms step_avg:46.78ms
step:1156/1900 train_time:54093ms step_avg:46.79ms
step:1157/1900 train_time:54155ms step_avg:46.81ms
step:1158/1900 train_time:54216ms step_avg:46.82ms
step:1159/1900 train_time:54277ms step_avg:46.83ms
step:1160/1900 train_time:54338ms step_avg:46.84ms
step:1161/1900 train_time:54400ms step_avg:46.86ms
step:1162/1900 train_time:54461ms step_avg:46.87ms
step:1163/1900 train_time:54523ms step_avg:46.88ms
step:1164/1900 train_time:54584ms step_avg:46.89ms
step:1165/1900 train_time:54646ms step_avg:46.91ms
step:1166/1900 train_time:54707ms step_avg:46.92ms
step:1167/1900 train_time:54769ms step_avg:46.93ms
step:1168/1900 train_time:54830ms step_avg:46.94ms
step:1169/1900 train_time:54891ms step_avg:46.96ms
step:1170/1900 train_time:54952ms step_avg:46.97ms
step:1171/1900 train_time:55014ms step_avg:46.98ms
step:1172/1900 train_time:55075ms step_avg:46.99ms
step:1173/1900 train_time:55137ms step_avg:47.00ms
step:1174/1900 train_time:55197ms step_avg:47.02ms
step:1175/1900 train_time:55260ms step_avg:47.03ms
step:1176/1900 train_time:55320ms step_avg:47.04ms
step:1177/1900 train_time:55382ms step_avg:47.05ms
step:1178/1900 train_time:55443ms step_avg:47.07ms
step:1179/1900 train_time:55505ms step_avg:47.08ms
step:1180/1900 train_time:55565ms step_avg:47.09ms
step:1181/1900 train_time:55627ms step_avg:47.10ms
step:1182/1900 train_time:55688ms step_avg:47.11ms
step:1183/1900 train_time:55750ms step_avg:47.13ms
step:1184/1900 train_time:55810ms step_avg:47.14ms
step:1185/1900 train_time:55872ms step_avg:47.15ms
step:1186/1900 train_time:55933ms step_avg:47.16ms
step:1187/1900 train_time:55995ms step_avg:47.17ms
step:1188/1900 train_time:56056ms step_avg:47.19ms
step:1189/1900 train_time:56118ms step_avg:47.20ms
step:1190/1900 train_time:56179ms step_avg:47.21ms
step:1191/1900 train_time:56241ms step_avg:47.22ms
step:1192/1900 train_time:56302ms step_avg:47.23ms
step:1193/1900 train_time:56363ms step_avg:47.24ms
step:1194/1900 train_time:56424ms step_avg:47.26ms
step:1195/1900 train_time:56485ms step_avg:47.27ms
step:1196/1900 train_time:56546ms step_avg:47.28ms
step:1197/1900 train_time:56608ms step_avg:47.29ms
step:1198/1900 train_time:56669ms step_avg:47.30ms
step:1199/1900 train_time:56730ms step_avg:47.31ms
step:1200/1900 train_time:56791ms step_avg:47.33ms
step:1201/1900 train_time:56853ms step_avg:47.34ms
step:1202/1900 train_time:56914ms step_avg:47.35ms
step:1203/1900 train_time:56975ms step_avg:47.36ms
step:1204/1900 train_time:57036ms step_avg:47.37ms
step:1205/1900 train_time:57098ms step_avg:47.38ms
step:1206/1900 train_time:57159ms step_avg:47.40ms
step:1207/1900 train_time:57221ms step_avg:47.41ms
step:1208/1900 train_time:57282ms step_avg:47.42ms
step:1209/1900 train_time:57343ms step_avg:47.43ms
step:1210/1900 train_time:57404ms step_avg:47.44ms
step:1211/1900 train_time:57466ms step_avg:47.45ms
step:1212/1900 train_time:57527ms step_avg:47.46ms
step:1213/1900 train_time:57588ms step_avg:47.48ms
step:1214/1900 train_time:57649ms step_avg:47.49ms
step:1215/1900 train_time:57711ms step_avg:47.50ms
step:1216/1900 train_time:57772ms step_avg:47.51ms
step:1217/1900 train_time:57834ms step_avg:47.52ms
step:1218/1900 train_time:57895ms step_avg:47.53ms
step:1219/1900 train_time:57957ms step_avg:47.54ms
step:1220/1900 train_time:58018ms step_avg:47.56ms
step:1221/1900 train_time:58079ms step_avg:47.57ms
step:1222/1900 train_time:58139ms step_avg:47.58ms
step:1223/1900 train_time:58201ms step_avg:47.59ms
step:1224/1900 train_time:58262ms step_avg:47.60ms
step:1225/1900 train_time:58323ms step_avg:47.61ms
step:1226/1900 train_time:58385ms step_avg:47.62ms
step:1227/1900 train_time:58447ms step_avg:47.63ms
step:1228/1900 train_time:58508ms step_avg:47.64ms
step:1229/1900 train_time:58570ms step_avg:47.66ms
step:1230/1900 train_time:58630ms step_avg:47.67ms
step:1231/1900 train_time:58692ms step_avg:47.68ms
step:1232/1900 train_time:58753ms step_avg:47.69ms
step:1233/1900 train_time:58815ms step_avg:47.70ms
step:1234/1900 train_time:58876ms step_avg:47.71ms
step:1235/1900 train_time:58937ms step_avg:47.72ms
step:1236/1900 train_time:58998ms step_avg:47.73ms
step:1237/1900 train_time:59060ms step_avg:47.74ms
step:1238/1900 train_time:59120ms step_avg:47.75ms
step:1239/1900 train_time:59183ms step_avg:47.77ms
step:1240/1900 train_time:59243ms step_avg:47.78ms
step:1241/1900 train_time:59306ms step_avg:47.79ms
step:1242/1900 train_time:59394ms step_avg:47.82ms
step:1243/1900 train_time:59483ms step_avg:47.85ms
step:1244/1900 train_time:59571ms step_avg:47.89ms
step:1245/1900 train_time:59660ms step_avg:47.92ms
step:1246/1900 train_time:59748ms step_avg:47.95ms
step:1247/1900 train_time:59837ms step_avg:47.99ms
step:1248/1900 train_time:59924ms step_avg:48.02ms
step:1249/1900 train_time:60012ms step_avg:48.05ms
step:1250/1900 train_time:60100ms step_avg:48.08ms
step:1250/1900 val_loss:3.5416 train_time:60191ms step_avg:48.15ms
step:1251/1900 train_time:60211ms step_avg:48.13ms
step:1252/1900 train_time:60280ms step_avg:48.15ms
step:1253/1900 train_time:60371ms step_avg:48.18ms
step:1254/1900 train_time:60458ms step_avg:48.21ms
step:1255/1900 train_time:60546ms step_avg:48.24ms
step:1256/1900 train_time:60633ms step_avg:48.27ms
step:1257/1900 train_time:60720ms step_avg:48.31ms
step:1258/1900 train_time:60808ms step_avg:48.34ms
step:1259/1900 train_time:60896ms step_avg:48.37ms
step:1260/1900 train_time:60983ms step_avg:48.40ms
step:1261/1900 train_time:61071ms step_avg:48.43ms
step:1262/1900 train_time:61161ms step_avg:48.46ms
step:1263/1900 train_time:61251ms step_avg:48.50ms
step:1264/1900 train_time:61339ms step_avg:48.53ms
step:1265/1900 train_time:61427ms step_avg:48.56ms
step:1266/1900 train_time:61514ms step_avg:48.59ms
step:1267/1900 train_time:61602ms step_avg:48.62ms
step:1268/1900 train_time:61689ms step_avg:48.65ms
step:1269/1900 train_time:61777ms step_avg:48.68ms
step:1270/1900 train_time:61865ms step_avg:48.71ms
step:1271/1900 train_time:61954ms step_avg:48.74ms
step:1272/1900 train_time:62041ms step_avg:48.77ms
step:1273/1900 train_time:62131ms step_avg:48.81ms
step:1274/1900 train_time:62219ms step_avg:48.84ms
step:1275/1900 train_time:62308ms step_avg:48.87ms
step:1276/1900 train_time:62396ms step_avg:48.90ms
step:1277/1900 train_time:62484ms step_avg:48.93ms
step:1278/1900 train_time:62570ms step_avg:48.96ms
step:1279/1900 train_time:62659ms step_avg:48.99ms
step:1280/1900 train_time:62746ms step_avg:49.02ms
step:1281/1900 train_time:62834ms step_avg:49.05ms
step:1282/1900 train_time:62921ms step_avg:49.08ms
step:1283/1900 train_time:63010ms step_avg:49.11ms
step:1284/1900 train_time:63097ms step_avg:49.14ms
step:1285/1900 train_time:63186ms step_avg:49.17ms
step:1286/1900 train_time:63274ms step_avg:49.20ms
step:1287/1900 train_time:63363ms step_avg:49.23ms
step:1288/1900 train_time:63450ms step_avg:49.26ms
step:1289/1900 train_time:63539ms step_avg:49.29ms
step:1290/1900 train_time:63626ms step_avg:49.32ms
step:1291/1900 train_time:63715ms step_avg:49.35ms
step:1292/1900 train_time:63803ms step_avg:49.38ms
step:1293/1900 train_time:63890ms step_avg:49.41ms
step:1294/1900 train_time:63978ms step_avg:49.44ms
step:1295/1900 train_time:64066ms step_avg:49.47ms
step:1296/1900 train_time:64154ms step_avg:49.50ms
step:1297/1900 train_time:64243ms step_avg:49.53ms
step:1298/1900 train_time:64331ms step_avg:49.56ms
step:1299/1900 train_time:64420ms step_avg:49.59ms
step:1300/1900 train_time:64509ms step_avg:49.62ms
step:1301/1900 train_time:64596ms step_avg:49.65ms
step:1302/1900 train_time:64683ms step_avg:49.68ms
step:1303/1900 train_time:64773ms step_avg:49.71ms
step:1304/1900 train_time:64860ms step_avg:49.74ms
step:1305/1900 train_time:64948ms step_avg:49.77ms
step:1306/1900 train_time:65036ms step_avg:49.80ms
step:1307/1900 train_time:65124ms step_avg:49.83ms
step:1308/1900 train_time:65213ms step_avg:49.86ms
step:1309/1900 train_time:65301ms step_avg:49.89ms
step:1310/1900 train_time:65388ms step_avg:49.91ms
step:1311/1900 train_time:65477ms step_avg:49.94ms
step:1312/1900 train_time:65565ms step_avg:49.97ms
step:1313/1900 train_time:65653ms step_avg:50.00ms
step:1314/1900 train_time:65741ms step_avg:50.03ms
step:1315/1900 train_time:65829ms step_avg:50.06ms
step:1316/1900 train_time:65916ms step_avg:50.09ms
step:1317/1900 train_time:66005ms step_avg:50.12ms
step:1318/1900 train_time:66093ms step_avg:50.15ms
step:1319/1900 train_time:66182ms step_avg:50.18ms
step:1320/1900 train_time:66269ms step_avg:50.20ms
step:1321/1900 train_time:66357ms step_avg:50.23ms
step:1322/1900 train_time:66445ms step_avg:50.26ms
step:1323/1900 train_time:66533ms step_avg:50.29ms
step:1324/1900 train_time:66621ms step_avg:50.32ms
step:1325/1900 train_time:66710ms step_avg:50.35ms
step:1326/1900 train_time:66797ms step_avg:50.37ms
step:1327/1900 train_time:66885ms step_avg:50.40ms
step:1328/1900 train_time:66973ms step_avg:50.43ms
step:1329/1900 train_time:67061ms step_avg:50.46ms
step:1330/1900 train_time:67150ms step_avg:50.49ms
step:1331/1900 train_time:67238ms step_avg:50.52ms
step:1332/1900 train_time:67326ms step_avg:50.54ms
step:1333/1900 train_time:67414ms step_avg:50.57ms
step:1334/1900 train_time:67501ms step_avg:50.60ms
step:1335/1900 train_time:67589ms step_avg:50.63ms
step:1336/1900 train_time:67678ms step_avg:50.66ms
step:1337/1900 train_time:67766ms step_avg:50.69ms
step:1338/1900 train_time:67853ms step_avg:50.71ms
step:1339/1900 train_time:67942ms step_avg:50.74ms
step:1340/1900 train_time:68029ms step_avg:50.77ms
step:1341/1900 train_time:68118ms step_avg:50.80ms
step:1342/1900 train_time:68206ms step_avg:50.82ms
step:1343/1900 train_time:68294ms step_avg:50.85ms
step:1344/1900 train_time:68381ms step_avg:50.88ms
step:1345/1900 train_time:68470ms step_avg:50.91ms
step:1346/1900 train_time:68557ms step_avg:50.93ms
step:1347/1900 train_time:68646ms step_avg:50.96ms
step:1348/1900 train_time:68733ms step_avg:50.99ms
step:1349/1900 train_time:68822ms step_avg:51.02ms
step:1350/1900 train_time:68910ms step_avg:51.04ms
step:1351/1900 train_time:68999ms step_avg:51.07ms
step:1352/1900 train_time:69087ms step_avg:51.10ms
step:1353/1900 train_time:69176ms step_avg:51.13ms
step:1354/1900 train_time:69264ms step_avg:51.15ms
step:1355/1900 train_time:69351ms step_avg:51.18ms
step:1356/1900 train_time:69439ms step_avg:51.21ms
step:1357/1900 train_time:69527ms step_avg:51.24ms
step:1358/1900 train_time:69614ms step_avg:51.26ms
step:1359/1900 train_time:69702ms step_avg:51.29ms
step:1360/1900 train_time:69790ms step_avg:51.32ms
step:1361/1900 train_time:69879ms step_avg:51.34ms
step:1362/1900 train_time:69966ms step_avg:51.37ms
step:1363/1900 train_time:70055ms step_avg:51.40ms
step:1364/1900 train_time:70143ms step_avg:51.42ms
step:1365/1900 train_time:70232ms step_avg:51.45ms
step:1366/1900 train_time:70319ms step_avg:51.48ms
step:1367/1900 train_time:70408ms step_avg:51.51ms
step:1368/1900 train_time:70495ms step_avg:51.53ms
step:1369/1900 train_time:70584ms step_avg:51.56ms
step:1370/1900 train_time:70671ms step_avg:51.58ms
step:1371/1900 train_time:70760ms step_avg:51.61ms
step:1372/1900 train_time:70847ms step_avg:51.64ms
step:1373/1900 train_time:70936ms step_avg:51.66ms
step:1374/1900 train_time:71023ms step_avg:51.69ms
step:1375/1900 train_time:71112ms step_avg:51.72ms
step:1376/1900 train_time:71199ms step_avg:51.74ms
step:1377/1900 train_time:71287ms step_avg:51.77ms
step:1378/1900 train_time:71375ms step_avg:51.80ms
step:1379/1900 train_time:71462ms step_avg:51.82ms
step:1380/1900 train_time:71550ms step_avg:51.85ms
step:1381/1900 train_time:71640ms step_avg:51.88ms
step:1382/1900 train_time:71728ms step_avg:51.90ms
step:1383/1900 train_time:71815ms step_avg:51.93ms
step:1384/1900 train_time:71904ms step_avg:51.95ms
step:1385/1900 train_time:71992ms step_avg:51.98ms
step:1386/1900 train_time:72079ms step_avg:52.01ms
step:1387/1900 train_time:72168ms step_avg:52.03ms
step:1388/1900 train_time:72255ms step_avg:52.06ms
step:1389/1900 train_time:72345ms step_avg:52.08ms
step:1390/1900 train_time:72432ms step_avg:52.11ms
step:1391/1900 train_time:72519ms step_avg:52.13ms
step:1392/1900 train_time:72607ms step_avg:52.16ms
step:1393/1900 train_time:72696ms step_avg:52.19ms
step:1394/1900 train_time:72784ms step_avg:52.21ms
step:1395/1900 train_time:72872ms step_avg:52.24ms
step:1396/1900 train_time:72959ms step_avg:52.26ms
step:1397/1900 train_time:73048ms step_avg:52.29ms
step:1398/1900 train_time:73135ms step_avg:52.31ms
step:1399/1900 train_time:73224ms step_avg:52.34ms
step:1400/1900 train_time:73313ms step_avg:52.37ms
step:1401/1900 train_time:73401ms step_avg:52.39ms
step:1402/1900 train_time:73489ms step_avg:52.42ms
step:1403/1900 train_time:73577ms step_avg:52.44ms
step:1404/1900 train_time:73665ms step_avg:52.47ms
step:1405/1900 train_time:73753ms step_avg:52.49ms
step:1406/1900 train_time:73840ms step_avg:52.52ms
step:1407/1900 train_time:73929ms step_avg:52.54ms
step:1408/1900 train_time:74016ms step_avg:52.57ms
step:1409/1900 train_time:74105ms step_avg:52.59ms
step:1410/1900 train_time:74192ms step_avg:52.62ms
step:1411/1900 train_time:74281ms step_avg:52.64ms
step:1412/1900 train_time:74368ms step_avg:52.67ms
step:1413/1900 train_time:74457ms step_avg:52.69ms
step:1414/1900 train_time:74544ms step_avg:52.72ms
step:1415/1900 train_time:74633ms step_avg:52.74ms
step:1416/1900 train_time:74720ms step_avg:52.77ms
step:1417/1900 train_time:74809ms step_avg:52.79ms
step:1418/1900 train_time:74897ms step_avg:52.82ms
step:1419/1900 train_time:74986ms step_avg:52.84ms
step:1420/1900 train_time:75073ms step_avg:52.87ms
step:1421/1900 train_time:75162ms step_avg:52.89ms
step:1422/1900 train_time:75250ms step_avg:52.92ms
step:1423/1900 train_time:75339ms step_avg:52.94ms
step:1424/1900 train_time:75427ms step_avg:52.97ms
step:1425/1900 train_time:75516ms step_avg:52.99ms
step:1426/1900 train_time:75603ms step_avg:53.02ms
step:1427/1900 train_time:75692ms step_avg:53.04ms
step:1428/1900 train_time:75779ms step_avg:53.07ms
step:1429/1900 train_time:75867ms step_avg:53.09ms
step:1430/1900 train_time:75954ms step_avg:53.11ms
step:1431/1900 train_time:76042ms step_avg:53.14ms
step:1432/1900 train_time:76130ms step_avg:53.16ms
step:1433/1900 train_time:76219ms step_avg:53.19ms
step:1434/1900 train_time:76306ms step_avg:53.21ms
step:1435/1900 train_time:76395ms step_avg:53.24ms
step:1436/1900 train_time:76484ms step_avg:53.26ms
step:1437/1900 train_time:76572ms step_avg:53.29ms
step:1438/1900 train_time:76660ms step_avg:53.31ms
step:1439/1900 train_time:76748ms step_avg:53.33ms
step:1440/1900 train_time:76835ms step_avg:53.36ms
step:1441/1900 train_time:76923ms step_avg:53.38ms
step:1442/1900 train_time:77010ms step_avg:53.41ms
step:1443/1900 train_time:77099ms step_avg:53.43ms
step:1444/1900 train_time:77186ms step_avg:53.45ms
step:1445/1900 train_time:77276ms step_avg:53.48ms
step:1446/1900 train_time:77364ms step_avg:53.50ms
step:1447/1900 train_time:77452ms step_avg:53.53ms
step:1448/1900 train_time:77541ms step_avg:53.55ms
step:1449/1900 train_time:77629ms step_avg:53.57ms
step:1450/1900 train_time:77716ms step_avg:53.60ms
step:1451/1900 train_time:77805ms step_avg:53.62ms
step:1452/1900 train_time:77892ms step_avg:53.64ms
step:1453/1900 train_time:77981ms step_avg:53.67ms
step:1454/1900 train_time:78068ms step_avg:53.69ms
step:1455/1900 train_time:78156ms step_avg:53.72ms
step:1456/1900 train_time:78244ms step_avg:53.74ms
step:1457/1900 train_time:78332ms step_avg:53.76ms
step:1458/1900 train_time:78419ms step_avg:53.79ms
step:1459/1900 train_time:78509ms step_avg:53.81ms
step:1460/1900 train_time:78596ms step_avg:53.83ms
step:1461/1900 train_time:78684ms step_avg:53.86ms
step:1462/1900 train_time:78772ms step_avg:53.88ms
step:1463/1900 train_time:78860ms step_avg:53.90ms
step:1464/1900 train_time:78948ms step_avg:53.93ms
step:1465/1900 train_time:79036ms step_avg:53.95ms
step:1466/1900 train_time:79124ms step_avg:53.97ms
step:1467/1900 train_time:79213ms step_avg:54.00ms
step:1468/1900 train_time:79301ms step_avg:54.02ms
step:1469/1900 train_time:79389ms step_avg:54.04ms
step:1470/1900 train_time:79477ms step_avg:54.07ms
step:1471/1900 train_time:79565ms step_avg:54.09ms
step:1472/1900 train_time:79652ms step_avg:54.11ms
step:1473/1900 train_time:79741ms step_avg:54.14ms
step:1474/1900 train_time:79828ms step_avg:54.16ms
step:1475/1900 train_time:79916ms step_avg:54.18ms
step:1476/1900 train_time:80004ms step_avg:54.20ms
step:1477/1900 train_time:80093ms step_avg:54.23ms
step:1478/1900 train_time:80181ms step_avg:54.25ms
step:1479/1900 train_time:80268ms step_avg:54.27ms
step:1480/1900 train_time:80356ms step_avg:54.29ms
step:1481/1900 train_time:80444ms step_avg:54.32ms
step:1482/1900 train_time:80531ms step_avg:54.34ms
step:1483/1900 train_time:80619ms step_avg:54.36ms
step:1484/1900 train_time:80707ms step_avg:54.38ms
step:1485/1900 train_time:80795ms step_avg:54.41ms
step:1486/1900 train_time:80882ms step_avg:54.43ms
step:1487/1900 train_time:80970ms step_avg:54.45ms
step:1488/1900 train_time:81058ms step_avg:54.47ms
step:1489/1900 train_time:81147ms step_avg:54.50ms
step:1490/1900 train_time:81234ms step_avg:54.52ms
step:1491/1900 train_time:81322ms step_avg:54.54ms
step:1492/1900 train_time:81410ms step_avg:54.56ms
step:1493/1900 train_time:81498ms step_avg:54.59ms
step:1494/1900 train_time:81586ms step_avg:54.61ms
step:1495/1900 train_time:81674ms step_avg:54.63ms
step:1496/1900 train_time:81762ms step_avg:54.65ms
step:1497/1900 train_time:81850ms step_avg:54.68ms
step:1498/1900 train_time:81938ms step_avg:54.70ms
step:1499/1900 train_time:82027ms step_avg:54.72ms
step:1500/1900 train_time:82114ms step_avg:54.74ms
step:1500/1900 val_loss:3.4124 train_time:82204ms step_avg:54.80ms
step:1501/1900 train_time:82224ms step_avg:54.78ms
step:1502/1900 train_time:82292ms step_avg:54.79ms
step:1503/1900 train_time:82385ms step_avg:54.81ms
step:1504/1900 train_time:82472ms step_avg:54.84ms
step:1505/1900 train_time:82560ms step_avg:54.86ms
step:1506/1900 train_time:82646ms step_avg:54.88ms
step:1507/1900 train_time:82733ms step_avg:54.90ms
step:1508/1900 train_time:82821ms step_avg:54.92ms
step:1509/1900 train_time:82909ms step_avg:54.94ms
step:1510/1900 train_time:82996ms step_avg:54.96ms
step:1511/1900 train_time:83084ms step_avg:54.99ms
step:1512/1900 train_time:83173ms step_avg:55.01ms
step:1513/1900 train_time:83263ms step_avg:55.03ms
step:1514/1900 train_time:83354ms step_avg:55.06ms
step:1515/1900 train_time:83442ms step_avg:55.08ms
step:1516/1900 train_time:83529ms step_avg:55.10ms
step:1517/1900 train_time:83617ms step_avg:55.12ms
step:1518/1900 train_time:83704ms step_avg:55.14ms
step:1519/1900 train_time:83791ms step_avg:55.16ms
step:1520/1900 train_time:83878ms step_avg:55.18ms
step:1521/1900 train_time:83966ms step_avg:55.20ms
step:1522/1900 train_time:84053ms step_avg:55.23ms
step:1523/1900 train_time:84142ms step_avg:55.25ms
step:1524/1900 train_time:84231ms step_avg:55.27ms
step:1525/1900 train_time:84322ms step_avg:55.29ms
step:1526/1900 train_time:84411ms step_avg:55.32ms
step:1527/1900 train_time:84500ms step_avg:55.34ms
step:1528/1900 train_time:84587ms step_avg:55.36ms
step:1529/1900 train_time:84675ms step_avg:55.38ms
step:1530/1900 train_time:84763ms step_avg:55.40ms
step:1531/1900 train_time:84851ms step_avg:55.42ms
step:1532/1900 train_time:84937ms step_avg:55.44ms
step:1533/1900 train_time:85025ms step_avg:55.46ms
step:1534/1900 train_time:85112ms step_avg:55.48ms
step:1535/1900 train_time:85201ms step_avg:55.51ms
step:1536/1900 train_time:85289ms step_avg:55.53ms
step:1537/1900 train_time:85380ms step_avg:55.55ms
step:1538/1900 train_time:85468ms step_avg:55.57ms
step:1539/1900 train_time:85556ms step_avg:55.59ms
step:1540/1900 train_time:85643ms step_avg:55.61ms
step:1541/1900 train_time:85731ms step_avg:55.63ms
step:1542/1900 train_time:85818ms step_avg:55.65ms
step:1543/1900 train_time:85906ms step_avg:55.67ms
step:1544/1900 train_time:85992ms step_avg:55.69ms
step:1545/1900 train_time:86081ms step_avg:55.72ms
step:1546/1900 train_time:86169ms step_avg:55.74ms
step:1547/1900 train_time:86258ms step_avg:55.76ms
step:1548/1900 train_time:86346ms step_avg:55.78ms
step:1549/1900 train_time:86434ms step_avg:55.80ms
step:1550/1900 train_time:86522ms step_avg:55.82ms
step:1551/1900 train_time:86611ms step_avg:55.84ms
step:1552/1900 train_time:86698ms step_avg:55.86ms
step:1553/1900 train_time:86786ms step_avg:55.88ms
step:1554/1900 train_time:86874ms step_avg:55.90ms
step:1555/1900 train_time:86962ms step_avg:55.92ms
step:1556/1900 train_time:87049ms step_avg:55.94ms
step:1557/1900 train_time:87138ms step_avg:55.97ms
step:1558/1900 train_time:87225ms step_avg:55.99ms
step:1559/1900 train_time:87314ms step_avg:56.01ms
step:1560/1900 train_time:87402ms step_avg:56.03ms
step:1561/1900 train_time:87491ms step_avg:56.05ms
step:1562/1900 train_time:87580ms step_avg:56.07ms
step:1563/1900 train_time:87668ms step_avg:56.09ms
step:1564/1900 train_time:87756ms step_avg:56.11ms
step:1565/1900 train_time:87844ms step_avg:56.13ms
step:1566/1900 train_time:87931ms step_avg:56.15ms
step:1567/1900 train_time:88020ms step_avg:56.17ms
step:1568/1900 train_time:88108ms step_avg:56.19ms
step:1569/1900 train_time:88196ms step_avg:56.21ms
step:1570/1900 train_time:88284ms step_avg:56.23ms
step:1571/1900 train_time:88372ms step_avg:56.25ms
step:1572/1900 train_time:88460ms step_avg:56.27ms
step:1573/1900 train_time:88549ms step_avg:56.29ms
step:1574/1900 train_time:88637ms step_avg:56.31ms
step:1575/1900 train_time:88724ms step_avg:56.33ms
step:1576/1900 train_time:88812ms step_avg:56.35ms
step:1577/1900 train_time:88901ms step_avg:56.37ms
step:1578/1900 train_time:88988ms step_avg:56.39ms
step:1579/1900 train_time:89077ms step_avg:56.41ms
step:1580/1900 train_time:89164ms step_avg:56.43ms
step:1581/1900 train_time:89254ms step_avg:56.45ms
step:1582/1900 train_time:89341ms step_avg:56.47ms
step:1583/1900 train_time:89430ms step_avg:56.49ms
step:1584/1900 train_time:89518ms step_avg:56.51ms
step:1585/1900 train_time:89607ms step_avg:56.53ms
step:1586/1900 train_time:89695ms step_avg:56.55ms
step:1587/1900 train_time:89783ms step_avg:56.57ms
step:1588/1900 train_time:89871ms step_avg:56.59ms
step:1589/1900 train_time:89959ms step_avg:56.61ms
step:1590/1900 train_time:90047ms step_avg:56.63ms
step:1591/1900 train_time:90136ms step_avg:56.65ms
step:1592/1900 train_time:90224ms step_avg:56.67ms
step:1593/1900 train_time:90313ms step_avg:56.69ms
step:1594/1900 train_time:90401ms step_avg:56.71ms
step:1595/1900 train_time:90489ms step_avg:56.73ms
step:1596/1900 train_time:90578ms step_avg:56.75ms
step:1597/1900 train_time:90666ms step_avg:56.77ms
step:1598/1900 train_time:90753ms step_avg:56.79ms
step:1599/1900 train_time:90841ms step_avg:56.81ms
step:1600/1900 train_time:90928ms step_avg:56.83ms
step:1601/1900 train_time:91016ms step_avg:56.85ms
step:1602/1900 train_time:91104ms step_avg:56.87ms
step:1603/1900 train_time:91192ms step_avg:56.89ms
step:1604/1900 train_time:91280ms step_avg:56.91ms
step:1605/1900 train_time:91369ms step_avg:56.93ms
step:1606/1900 train_time:91456ms step_avg:56.95ms
step:1607/1900 train_time:91545ms step_avg:56.97ms
step:1608/1900 train_time:91632ms step_avg:56.99ms
step:1609/1900 train_time:91721ms step_avg:57.01ms
step:1610/1900 train_time:91809ms step_avg:57.02ms
step:1611/1900 train_time:91897ms step_avg:57.04ms
step:1612/1900 train_time:91985ms step_avg:57.06ms
step:1613/1900 train_time:92074ms step_avg:57.08ms
step:1614/1900 train_time:92161ms step_avg:57.10ms
step:1615/1900 train_time:92249ms step_avg:57.12ms
step:1616/1900 train_time:92337ms step_avg:57.14ms
step:1617/1900 train_time:92425ms step_avg:57.16ms
step:1618/1900 train_time:92512ms step_avg:57.18ms
step:1619/1900 train_time:92600ms step_avg:57.20ms
step:1620/1900 train_time:92687ms step_avg:57.21ms
step:1621/1900 train_time:92776ms step_avg:57.23ms
step:1622/1900 train_time:92864ms step_avg:57.25ms
step:1623/1900 train_time:92953ms step_avg:57.27ms
step:1624/1900 train_time:93040ms step_avg:57.29ms
step:1625/1900 train_time:93129ms step_avg:57.31ms
step:1626/1900 train_time:93216ms step_avg:57.33ms
step:1627/1900 train_time:93305ms step_avg:57.35ms
step:1628/1900 train_time:93392ms step_avg:57.37ms
step:1629/1900 train_time:93482ms step_avg:57.39ms
step:1630/1900 train_time:93569ms step_avg:57.40ms
step:1631/1900 train_time:93657ms step_avg:57.42ms
step:1632/1900 train_time:93745ms step_avg:57.44ms
step:1633/1900 train_time:93833ms step_avg:57.46ms
step:1634/1900 train_time:93920ms step_avg:57.48ms
step:1635/1900 train_time:94008ms step_avg:57.50ms
step:1636/1900 train_time:94096ms step_avg:57.52ms
step:1637/1900 train_time:94184ms step_avg:57.53ms
step:1638/1900 train_time:94272ms step_avg:57.55ms
step:1639/1900 train_time:94360ms step_avg:57.57ms
step:1640/1900 train_time:94448ms step_avg:57.59ms
step:1641/1900 train_time:94537ms step_avg:57.61ms
step:1642/1900 train_time:94624ms step_avg:57.63ms
step:1643/1900 train_time:94713ms step_avg:57.65ms
step:1644/1900 train_time:94801ms step_avg:57.66ms
step:1645/1900 train_time:94889ms step_avg:57.68ms
step:1646/1900 train_time:94977ms step_avg:57.70ms
step:1647/1900 train_time:95065ms step_avg:57.72ms
step:1648/1900 train_time:95152ms step_avg:57.74ms
step:1649/1900 train_time:95240ms step_avg:57.76ms
step:1650/1900 train_time:95328ms step_avg:57.77ms
step:1651/1900 train_time:95416ms step_avg:57.79ms
step:1652/1900 train_time:95504ms step_avg:57.81ms
step:1653/1900 train_time:95593ms step_avg:57.83ms
step:1654/1900 train_time:95680ms step_avg:57.85ms
step:1655/1900 train_time:95768ms step_avg:57.87ms
step:1656/1900 train_time:95856ms step_avg:57.88ms
step:1657/1900 train_time:95944ms step_avg:57.90ms
step:1658/1900 train_time:96033ms step_avg:57.92ms
step:1659/1900 train_time:96121ms step_avg:57.94ms
step:1660/1900 train_time:96209ms step_avg:57.96ms
step:1661/1900 train_time:96298ms step_avg:57.98ms
step:1662/1900 train_time:96385ms step_avg:57.99ms
step:1663/1900 train_time:96475ms step_avg:58.01ms
step:1664/1900 train_time:96561ms step_avg:58.03ms
step:1665/1900 train_time:96650ms step_avg:58.05ms
step:1666/1900 train_time:96738ms step_avg:58.07ms
step:1667/1900 train_time:96826ms step_avg:58.08ms
step:1668/1900 train_time:96913ms step_avg:58.10ms
step:1669/1900 train_time:97002ms step_avg:58.12ms
step:1670/1900 train_time:97090ms step_avg:58.14ms
step:1671/1900 train_time:97180ms step_avg:58.16ms
step:1672/1900 train_time:97267ms step_avg:58.17ms
step:1673/1900 train_time:97355ms step_avg:58.19ms
step:1674/1900 train_time:97442ms step_avg:58.21ms
step:1675/1900 train_time:97531ms step_avg:58.23ms
step:1676/1900 train_time:97618ms step_avg:58.24ms
step:1677/1900 train_time:97707ms step_avg:58.26ms
step:1678/1900 train_time:97794ms step_avg:58.28ms
step:1679/1900 train_time:97883ms step_avg:58.30ms
step:1680/1900 train_time:97971ms step_avg:58.32ms
step:1681/1900 train_time:98060ms step_avg:58.33ms
step:1682/1900 train_time:98147ms step_avg:58.35ms
step:1683/1900 train_time:98236ms step_avg:58.37ms
step:1684/1900 train_time:98323ms step_avg:58.39ms
step:1685/1900 train_time:98413ms step_avg:58.41ms
step:1686/1900 train_time:98500ms step_avg:58.42ms
step:1687/1900 train_time:98588ms step_avg:58.44ms
step:1688/1900 train_time:98675ms step_avg:58.46ms
step:1689/1900 train_time:98763ms step_avg:58.47ms
step:1690/1900 train_time:98851ms step_avg:58.49ms
step:1691/1900 train_time:98940ms step_avg:58.51ms
step:1692/1900 train_time:99027ms step_avg:58.53ms
step:1693/1900 train_time:99115ms step_avg:58.54ms
step:1694/1900 train_time:99203ms step_avg:58.56ms
step:1695/1900 train_time:99291ms step_avg:58.58ms
step:1696/1900 train_time:99379ms step_avg:58.60ms
step:1697/1900 train_time:99467ms step_avg:58.61ms
step:1698/1900 train_time:99554ms step_avg:58.63ms
step:1699/1900 train_time:99642ms step_avg:58.65ms
step:1700/1900 train_time:99730ms step_avg:58.66ms
step:1701/1900 train_time:99818ms step_avg:58.68ms
step:1702/1900 train_time:99907ms step_avg:58.70ms
step:1703/1900 train_time:99995ms step_avg:58.72ms
step:1704/1900 train_time:100082ms step_avg:58.73ms
step:1705/1900 train_time:100170ms step_avg:58.75ms
step:1706/1900 train_time:100258ms step_avg:58.77ms
step:1707/1900 train_time:100347ms step_avg:58.79ms
step:1708/1900 train_time:100434ms step_avg:58.80ms
step:1709/1900 train_time:100522ms step_avg:58.82ms
step:1710/1900 train_time:100610ms step_avg:58.84ms
step:1711/1900 train_time:100699ms step_avg:58.85ms
step:1712/1900 train_time:100786ms step_avg:58.87ms
step:1713/1900 train_time:100874ms step_avg:58.89ms
step:1714/1900 train_time:100962ms step_avg:58.90ms
step:1715/1900 train_time:101052ms step_avg:58.92ms
step:1716/1900 train_time:101139ms step_avg:58.94ms
step:1717/1900 train_time:101227ms step_avg:58.96ms
step:1718/1900 train_time:101315ms step_avg:58.97ms
step:1719/1900 train_time:101404ms step_avg:58.99ms
step:1720/1900 train_time:101492ms step_avg:59.01ms
step:1721/1900 train_time:101581ms step_avg:59.02ms
step:1722/1900 train_time:101668ms step_avg:59.04ms
step:1723/1900 train_time:101757ms step_avg:59.06ms
step:1724/1900 train_time:101844ms step_avg:59.07ms
step:1725/1900 train_time:101933ms step_avg:59.09ms
step:1726/1900 train_time:102020ms step_avg:59.11ms
step:1727/1900 train_time:102109ms step_avg:59.13ms
step:1728/1900 train_time:102197ms step_avg:59.14ms
step:1729/1900 train_time:102285ms step_avg:59.16ms
step:1730/1900 train_time:102372ms step_avg:59.17ms
step:1731/1900 train_time:102461ms step_avg:59.19ms
step:1732/1900 train_time:102548ms step_avg:59.21ms
step:1733/1900 train_time:102637ms step_avg:59.22ms
step:1734/1900 train_time:102724ms step_avg:59.24ms
step:1735/1900 train_time:102812ms step_avg:59.26ms
step:1736/1900 train_time:102900ms step_avg:59.27ms
step:1737/1900 train_time:102988ms step_avg:59.29ms
step:1738/1900 train_time:103076ms step_avg:59.31ms
step:1739/1900 train_time:103164ms step_avg:59.32ms
step:1740/1900 train_time:103251ms step_avg:59.34ms
step:1741/1900 train_time:103340ms step_avg:59.36ms
step:1742/1900 train_time:103428ms step_avg:59.37ms
step:1743/1900 train_time:103517ms step_avg:59.39ms
step:1744/1900 train_time:103605ms step_avg:59.41ms
step:1745/1900 train_time:103693ms step_avg:59.42ms
step:1746/1900 train_time:103780ms step_avg:59.44ms
step:1747/1900 train_time:103868ms step_avg:59.45ms
step:1748/1900 train_time:103955ms step_avg:59.47ms
step:1749/1900 train_time:104043ms step_avg:59.49ms
step:1750/1900 train_time:104131ms step_avg:59.50ms
step:1750/1900 val_loss:3.3174 train_time:104222ms step_avg:59.56ms
step:1751/1900 train_time:104242ms step_avg:59.53ms
step:1752/1900 train_time:104310ms step_avg:59.54ms
step:1753/1900 train_time:104403ms step_avg:59.56ms
step:1754/1900 train_time:104491ms step_avg:59.57ms
step:1755/1900 train_time:104579ms step_avg:59.59ms
step:1756/1900 train_time:104665ms step_avg:59.60ms
step:1757/1900 train_time:104754ms step_avg:59.62ms
step:1758/1900 train_time:104840ms step_avg:59.64ms
step:1759/1900 train_time:104928ms step_avg:59.65ms
step:1760/1900 train_time:105016ms step_avg:59.67ms
step:1761/1900 train_time:105103ms step_avg:59.68ms
step:1762/1900 train_time:105193ms step_avg:59.70ms
step:1763/1900 train_time:105284ms step_avg:59.72ms
step:1764/1900 train_time:105373ms step_avg:59.74ms
step:1765/1900 train_time:105462ms step_avg:59.75ms
step:1766/1900 train_time:105549ms step_avg:59.77ms
step:1767/1900 train_time:105637ms step_avg:59.78ms
step:1768/1900 train_time:105723ms step_avg:59.80ms
step:1769/1900 train_time:105811ms step_avg:59.81ms
step:1770/1900 train_time:105898ms step_avg:59.83ms
step:1771/1900 train_time:105986ms step_avg:59.85ms
step:1772/1900 train_time:106073ms step_avg:59.86ms
step:1773/1900 train_time:106162ms step_avg:59.88ms
step:1774/1900 train_time:106251ms step_avg:59.89ms
step:1775/1900 train_time:106341ms step_avg:59.91ms
step:1776/1900 train_time:106429ms step_avg:59.93ms
step:1777/1900 train_time:106518ms step_avg:59.94ms
step:1778/1900 train_time:106605ms step_avg:59.96ms
step:1779/1900 train_time:106693ms step_avg:59.97ms
step:1780/1900 train_time:106779ms step_avg:59.99ms
step:1781/1900 train_time:106868ms step_avg:60.00ms
step:1782/1900 train_time:106955ms step_avg:60.02ms
step:1783/1900 train_time:107042ms step_avg:60.03ms
step:1784/1900 train_time:107130ms step_avg:60.05ms
step:1785/1900 train_time:107219ms step_avg:60.07ms
step:1786/1900 train_time:107307ms step_avg:60.08ms
step:1787/1900 train_time:107396ms step_avg:60.10ms
step:1788/1900 train_time:107485ms step_avg:60.11ms
step:1789/1900 train_time:107573ms step_avg:60.13ms
step:1790/1900 train_time:107661ms step_avg:60.15ms
step:1791/1900 train_time:107749ms step_avg:60.16ms
step:1792/1900 train_time:107837ms step_avg:60.18ms
step:1793/1900 train_time:107925ms step_avg:60.19ms
step:1794/1900 train_time:108012ms step_avg:60.21ms
step:1795/1900 train_time:108100ms step_avg:60.22ms
step:1796/1900 train_time:108188ms step_avg:60.24ms
step:1797/1900 train_time:108277ms step_avg:60.25ms
step:1798/1900 train_time:108364ms step_avg:60.27ms
step:1799/1900 train_time:108453ms step_avg:60.29ms
step:1800/1900 train_time:108541ms step_avg:60.30ms
step:1801/1900 train_time:108629ms step_avg:60.32ms
step:1802/1900 train_time:108716ms step_avg:60.33ms
step:1803/1900 train_time:108804ms step_avg:60.35ms
step:1804/1900 train_time:108891ms step_avg:60.36ms
step:1805/1900 train_time:108979ms step_avg:60.38ms
step:1806/1900 train_time:109066ms step_avg:60.39ms
step:1807/1900 train_time:109155ms step_avg:60.41ms
step:1808/1900 train_time:109243ms step_avg:60.42ms
step:1809/1900 train_time:109332ms step_avg:60.44ms
step:1810/1900 train_time:109420ms step_avg:60.45ms
step:1811/1900 train_time:109509ms step_avg:60.47ms
step:1812/1900 train_time:109596ms step_avg:60.48ms
step:1813/1900 train_time:109684ms step_avg:60.50ms
step:1814/1900 train_time:109771ms step_avg:60.51ms
step:1815/1900 train_time:109860ms step_avg:60.53ms
step:1816/1900 train_time:109948ms step_avg:60.54ms
step:1817/1900 train_time:110037ms step_avg:60.56ms
step:1818/1900 train_time:110124ms step_avg:60.57ms
step:1819/1900 train_time:110213ms step_avg:60.59ms
step:1820/1900 train_time:110301ms step_avg:60.60ms
step:1821/1900 train_time:110391ms step_avg:60.62ms
step:1822/1900 train_time:110478ms step_avg:60.64ms
step:1823/1900 train_time:110566ms step_avg:60.65ms
step:1824/1900 train_time:110654ms step_avg:60.67ms
step:1825/1900 train_time:110741ms step_avg:60.68ms
step:1826/1900 train_time:110829ms step_avg:60.69ms
step:1827/1900 train_time:110917ms step_avg:60.71ms
step:1828/1900 train_time:111004ms step_avg:60.72ms
step:1829/1900 train_time:111092ms step_avg:60.74ms
step:1830/1900 train_time:111180ms step_avg:60.75ms
step:1831/1900 train_time:111268ms step_avg:60.77ms
step:1832/1900 train_time:111355ms step_avg:60.78ms
step:1833/1900 train_time:111443ms step_avg:60.80ms
step:1834/1900 train_time:111531ms step_avg:60.81ms
step:1835/1900 train_time:111621ms step_avg:60.83ms
step:1836/1900 train_time:111709ms step_avg:60.84ms
step:1837/1900 train_time:111797ms step_avg:60.86ms
step:1838/1900 train_time:111884ms step_avg:60.87ms
step:1839/1900 train_time:111972ms step_avg:60.89ms
step:1840/1900 train_time:112059ms step_avg:60.90ms
step:1841/1900 train_time:112147ms step_avg:60.92ms
step:1842/1900 train_time:112234ms step_avg:60.93ms
step:1843/1900 train_time:112324ms step_avg:60.95ms
step:1844/1900 train_time:112412ms step_avg:60.96ms
step:1845/1900 train_time:112500ms step_avg:60.98ms
step:1846/1900 train_time:112588ms step_avg:60.99ms
step:1847/1900 train_time:112677ms step_avg:61.01ms
step:1848/1900 train_time:112764ms step_avg:61.02ms
step:1849/1900 train_time:112853ms step_avg:61.03ms
step:1850/1900 train_time:112940ms step_avg:61.05ms
step:1851/1900 train_time:113028ms step_avg:61.06ms
step:1852/1900 train_time:113115ms step_avg:61.08ms
step:1853/1900 train_time:113204ms step_avg:61.09ms
step:1854/1900 train_time:113291ms step_avg:61.11ms
step:1855/1900 train_time:113381ms step_avg:61.12ms
step:1856/1900 train_time:113468ms step_avg:61.14ms
step:1857/1900 train_time:113557ms step_avg:61.15ms
step:1858/1900 train_time:113644ms step_avg:61.16ms
step:1859/1900 train_time:113733ms step_avg:61.18ms
step:1860/1900 train_time:113822ms step_avg:61.19ms
step:1861/1900 train_time:113911ms step_avg:61.21ms
step:1862/1900 train_time:113998ms step_avg:61.22ms
step:1863/1900 train_time:114086ms step_avg:61.24ms
step:1864/1900 train_time:114173ms step_avg:61.25ms
step:1865/1900 train_time:114262ms step_avg:61.27ms
step:1866/1900 train_time:114350ms step_avg:61.28ms
step:1867/1900 train_time:114440ms step_avg:61.30ms
step:1868/1900 train_time:114527ms step_avg:61.31ms
step:1869/1900 train_time:114616ms step_avg:61.32ms
step:1870/1900 train_time:114704ms step_avg:61.34ms
step:1871/1900 train_time:114793ms step_avg:61.35ms
step:1872/1900 train_time:114882ms step_avg:61.37ms
step:1873/1900 train_time:114972ms step_avg:61.38ms
step:1874/1900 train_time:115059ms step_avg:61.40ms
step:1875/1900 train_time:115147ms step_avg:61.41ms
step:1876/1900 train_time:115235ms step_avg:61.43ms
step:1877/1900 train_time:115324ms step_avg:61.44ms
step:1878/1900 train_time:115412ms step_avg:61.45ms
step:1879/1900 train_time:115502ms step_avg:61.47ms
step:1880/1900 train_time:115590ms step_avg:61.48ms
step:1881/1900 train_time:115679ms step_avg:61.50ms
step:1882/1900 train_time:115767ms step_avg:61.51ms
step:1883/1900 train_time:115857ms step_avg:61.53ms
step:1884/1900 train_time:115945ms step_avg:61.54ms
step:1885/1900 train_time:116034ms step_avg:61.56ms
step:1886/1900 train_time:116121ms step_avg:61.57ms
step:1887/1900 train_time:116210ms step_avg:61.58ms
step:1888/1900 train_time:116297ms step_avg:61.60ms
step:1889/1900 train_time:116386ms step_avg:61.61ms
step:1890/1900 train_time:116474ms step_avg:61.63ms
step:1891/1900 train_time:116563ms step_avg:61.64ms
step:1892/1900 train_time:116650ms step_avg:61.65ms
step:1893/1900 train_time:116740ms step_avg:61.67ms
step:1894/1900 train_time:116828ms step_avg:61.68ms
step:1895/1900 train_time:116917ms step_avg:61.70ms
step:1896/1900 train_time:117005ms step_avg:61.71ms
step:1897/1900 train_time:117094ms step_avg:61.73ms
step:1898/1900 train_time:117181ms step_avg:61.74ms
step:1899/1900 train_time:117270ms step_avg:61.75ms
step:1900/1900 train_time:117358ms step_avg:61.77ms
step:1900/1900 val_loss:3.2767 train_time:117448ms step_avg:61.81ms
peak memory allocated: 29709 MiB reserved: 43778 MiB

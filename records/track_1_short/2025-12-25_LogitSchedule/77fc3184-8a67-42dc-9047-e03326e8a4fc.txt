import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:01:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     40136      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40137      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40138      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40139      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40140      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40141      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40142      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40143      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     40137      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     40138      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     40139      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     40140      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     40141      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     40142      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     40143      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8324 train_time:0ms step_avg:0.20ms
step:1/1900 train_time:80ms step_avg:80.06ms
step:2/1900 train_time:106ms step_avg:53.08ms
step:3/1900 train_time:127ms step_avg:42.48ms
step:4/1900 train_time:158ms step_avg:39.49ms
step:5/1900 train_time:192ms step_avg:38.34ms
step:6/1900 train_time:274ms step_avg:45.65ms
step:7/1900 train_time:395ms step_avg:56.43ms
step:8/1900 train_time:429ms step_avg:53.62ms
step:9/1900 train_time:463ms step_avg:51.41ms
step:10/1900 train_time:497ms step_avg:49.65ms
step:11/1900 train_time:530ms step_avg:48.23ms
step:12/1900 train_time:564ms step_avg:47.03ms
step:13/1900 train_time:598ms step_avg:46.04ms
step:14/1900 train_time:632ms step_avg:45.17ms
step:15/1900 train_time:666ms step_avg:44.42ms
step:16/1900 train_time:700ms step_avg:43.77ms
step:17/1900 train_time:734ms step_avg:43.19ms
step:18/1900 train_time:768ms step_avg:42.68ms
step:19/1900 train_time:802ms step_avg:42.21ms
step:20/1900 train_time:836ms step_avg:41.81ms
step:21/1900 train_time:870ms step_avg:41.43ms
step:22/1900 train_time:904ms step_avg:41.09ms
step:23/1900 train_time:938ms step_avg:40.79ms
step:24/1900 train_time:972ms step_avg:40.50ms
step:25/1900 train_time:1006ms step_avg:40.25ms
step:26/1900 train_time:1040ms step_avg:40.01ms
step:27/1900 train_time:1074ms step_avg:39.79ms
step:28/1900 train_time:1108ms step_avg:39.59ms
step:29/1900 train_time:1142ms step_avg:39.39ms
step:30/1900 train_time:1176ms step_avg:39.21ms
step:31/1900 train_time:1210ms step_avg:39.04ms
step:32/1900 train_time:1244ms step_avg:38.89ms
step:33/1900 train_time:1278ms step_avg:38.74ms
step:34/1900 train_time:1313ms step_avg:38.60ms
step:35/1900 train_time:1347ms step_avg:38.49ms
step:36/1900 train_time:1382ms step_avg:38.38ms
step:37/1900 train_time:1416ms step_avg:38.27ms
step:38/1900 train_time:1450ms step_avg:38.16ms
step:39/1900 train_time:1484ms step_avg:38.05ms
step:40/1900 train_time:1518ms step_avg:37.96ms
step:41/1900 train_time:1552ms step_avg:37.86ms
step:42/1900 train_time:1586ms step_avg:37.77ms
step:43/1900 train_time:1621ms step_avg:37.69ms
step:44/1900 train_time:1654ms step_avg:37.60ms
step:45/1900 train_time:1689ms step_avg:37.53ms
step:46/1900 train_time:1723ms step_avg:37.45ms
step:47/1900 train_time:1757ms step_avg:37.38ms
step:48/1900 train_time:1791ms step_avg:37.31ms
step:49/1900 train_time:1825ms step_avg:37.24ms
step:50/1900 train_time:1859ms step_avg:37.18ms
step:51/1900 train_time:1893ms step_avg:37.12ms
step:52/1900 train_time:1927ms step_avg:37.06ms
step:53/1900 train_time:1961ms step_avg:37.00ms
step:54/1900 train_time:1995ms step_avg:36.95ms
step:55/1900 train_time:2029ms step_avg:36.90ms
step:56/1900 train_time:2063ms step_avg:36.85ms
step:57/1900 train_time:2097ms step_avg:36.80ms
step:58/1900 train_time:2131ms step_avg:36.75ms
step:59/1900 train_time:2165ms step_avg:36.70ms
step:60/1900 train_time:2199ms step_avg:36.65ms
step:61/1900 train_time:2233ms step_avg:36.61ms
step:62/1900 train_time:2267ms step_avg:36.57ms
step:63/1900 train_time:2301ms step_avg:36.53ms
step:64/1900 train_time:2335ms step_avg:36.49ms
step:65/1900 train_time:2369ms step_avg:36.45ms
step:66/1900 train_time:2404ms step_avg:36.42ms
step:67/1900 train_time:2438ms step_avg:36.39ms
step:68/1900 train_time:2472ms step_avg:36.36ms
step:69/1900 train_time:2507ms step_avg:36.33ms
step:70/1900 train_time:2541ms step_avg:36.30ms
step:71/1900 train_time:2575ms step_avg:36.26ms
step:72/1900 train_time:2609ms step_avg:36.24ms
step:73/1900 train_time:2643ms step_avg:36.20ms
step:74/1900 train_time:2677ms step_avg:36.18ms
step:75/1900 train_time:2712ms step_avg:36.15ms
step:76/1900 train_time:2746ms step_avg:36.13ms
step:77/1900 train_time:2780ms step_avg:36.10ms
step:78/1900 train_time:2813ms step_avg:36.07ms
step:79/1900 train_time:2848ms step_avg:36.05ms
step:80/1900 train_time:2882ms step_avg:36.02ms
step:81/1900 train_time:2916ms step_avg:36.00ms
step:82/1900 train_time:2950ms step_avg:35.98ms
step:83/1900 train_time:2984ms step_avg:35.95ms
step:84/1900 train_time:3018ms step_avg:35.93ms
step:85/1900 train_time:3052ms step_avg:35.90ms
step:86/1900 train_time:3086ms step_avg:35.88ms
step:87/1900 train_time:3120ms step_avg:35.86ms
step:88/1900 train_time:3154ms step_avg:35.84ms
step:89/1900 train_time:3188ms step_avg:35.82ms
step:90/1900 train_time:3222ms step_avg:35.80ms
step:91/1900 train_time:3256ms step_avg:35.78ms
step:92/1900 train_time:3290ms step_avg:35.76ms
step:93/1900 train_time:3324ms step_avg:35.74ms
step:94/1900 train_time:3358ms step_avg:35.72ms
step:95/1900 train_time:3392ms step_avg:35.70ms
step:96/1900 train_time:3426ms step_avg:35.68ms
step:97/1900 train_time:3460ms step_avg:35.67ms
step:98/1900 train_time:3494ms step_avg:35.65ms
step:99/1900 train_time:3528ms step_avg:35.63ms
step:100/1900 train_time:3562ms step_avg:35.62ms
step:101/1900 train_time:3596ms step_avg:35.60ms
step:102/1900 train_time:3630ms step_avg:35.59ms
step:103/1900 train_time:3664ms step_avg:35.57ms
step:104/1900 train_time:3698ms step_avg:35.56ms
step:105/1900 train_time:3732ms step_avg:35.55ms
step:106/1900 train_time:3766ms step_avg:35.53ms
step:107/1900 train_time:3801ms step_avg:35.52ms
step:108/1900 train_time:3835ms step_avg:35.50ms
step:109/1900 train_time:3869ms step_avg:35.49ms
step:110/1900 train_time:3903ms step_avg:35.48ms
step:111/1900 train_time:3937ms step_avg:35.47ms
step:112/1900 train_time:3971ms step_avg:35.45ms
step:113/1900 train_time:4005ms step_avg:35.44ms
step:114/1900 train_time:4039ms step_avg:35.43ms
step:115/1900 train_time:4073ms step_avg:35.42ms
step:116/1900 train_time:4107ms step_avg:35.41ms
step:117/1900 train_time:4141ms step_avg:35.39ms
step:118/1900 train_time:4175ms step_avg:35.38ms
step:119/1900 train_time:4209ms step_avg:35.37ms
step:120/1900 train_time:4243ms step_avg:35.36ms
step:121/1900 train_time:4278ms step_avg:35.35ms
step:122/1900 train_time:4312ms step_avg:35.34ms
step:123/1900 train_time:4345ms step_avg:35.33ms
step:124/1900 train_time:4379ms step_avg:35.32ms
step:125/1900 train_time:4414ms step_avg:35.31ms
step:126/1900 train_time:4448ms step_avg:35.30ms
step:127/1900 train_time:4482ms step_avg:35.29ms
step:128/1900 train_time:4516ms step_avg:35.28ms
step:129/1900 train_time:4550ms step_avg:35.27ms
step:130/1900 train_time:4584ms step_avg:35.26ms
step:131/1900 train_time:4618ms step_avg:35.25ms
step:132/1900 train_time:4652ms step_avg:35.24ms
step:133/1900 train_time:4686ms step_avg:35.24ms
step:134/1900 train_time:4720ms step_avg:35.23ms
step:135/1900 train_time:4755ms step_avg:35.22ms
step:136/1900 train_time:4789ms step_avg:35.21ms
step:137/1900 train_time:4823ms step_avg:35.20ms
step:138/1900 train_time:4857ms step_avg:35.19ms
step:139/1900 train_time:4891ms step_avg:35.19ms
step:140/1900 train_time:4925ms step_avg:35.18ms
step:141/1900 train_time:4959ms step_avg:35.17ms
step:142/1900 train_time:4993ms step_avg:35.16ms
step:143/1900 train_time:5027ms step_avg:35.15ms
step:144/1900 train_time:5061ms step_avg:35.15ms
step:145/1900 train_time:5095ms step_avg:35.14ms
step:146/1900 train_time:5129ms step_avg:35.13ms
step:147/1900 train_time:5164ms step_avg:35.13ms
step:148/1900 train_time:5198ms step_avg:35.12ms
step:149/1900 train_time:5232ms step_avg:35.11ms
step:150/1900 train_time:5266ms step_avg:35.10ms
step:151/1900 train_time:5300ms step_avg:35.10ms
step:152/1900 train_time:5334ms step_avg:35.09ms
step:153/1900 train_time:5368ms step_avg:35.08ms
step:154/1900 train_time:5402ms step_avg:35.08ms
step:155/1900 train_time:5436ms step_avg:35.07ms
step:156/1900 train_time:5469ms step_avg:35.06ms
step:157/1900 train_time:5504ms step_avg:35.05ms
step:158/1900 train_time:5537ms step_avg:35.05ms
step:159/1900 train_time:5572ms step_avg:35.04ms
step:160/1900 train_time:5606ms step_avg:35.04ms
step:161/1900 train_time:5640ms step_avg:35.03ms
step:162/1900 train_time:5674ms step_avg:35.02ms
step:163/1900 train_time:5708ms step_avg:35.02ms
step:164/1900 train_time:5742ms step_avg:35.01ms
step:165/1900 train_time:5776ms step_avg:35.01ms
step:166/1900 train_time:5810ms step_avg:35.00ms
step:167/1900 train_time:5844ms step_avg:35.00ms
step:168/1900 train_time:5878ms step_avg:34.99ms
step:169/1900 train_time:5913ms step_avg:34.99ms
step:170/1900 train_time:5947ms step_avg:34.98ms
step:171/1900 train_time:5981ms step_avg:34.98ms
step:172/1900 train_time:6015ms step_avg:34.97ms
step:173/1900 train_time:6049ms step_avg:34.97ms
step:174/1900 train_time:6083ms step_avg:34.96ms
step:175/1900 train_time:6117ms step_avg:34.95ms
step:176/1900 train_time:6151ms step_avg:34.95ms
step:177/1900 train_time:6185ms step_avg:34.94ms
step:178/1900 train_time:6219ms step_avg:34.94ms
step:179/1900 train_time:6254ms step_avg:34.94ms
step:180/1900 train_time:6287ms step_avg:34.93ms
step:181/1900 train_time:6322ms step_avg:34.93ms
step:182/1900 train_time:6356ms step_avg:34.92ms
step:183/1900 train_time:6389ms step_avg:34.92ms
step:184/1900 train_time:6423ms step_avg:34.91ms
step:185/1900 train_time:6457ms step_avg:34.90ms
step:186/1900 train_time:6491ms step_avg:34.90ms
step:187/1900 train_time:6525ms step_avg:34.89ms
step:188/1900 train_time:6559ms step_avg:34.89ms
step:189/1900 train_time:6593ms step_avg:34.88ms
step:190/1900 train_time:6627ms step_avg:34.88ms
step:191/1900 train_time:6661ms step_avg:34.87ms
step:192/1900 train_time:6695ms step_avg:34.87ms
step:193/1900 train_time:6729ms step_avg:34.86ms
step:194/1900 train_time:6763ms step_avg:34.86ms
step:195/1900 train_time:6797ms step_avg:34.86ms
step:196/1900 train_time:6831ms step_avg:34.85ms
step:197/1900 train_time:6865ms step_avg:34.85ms
step:198/1900 train_time:6899ms step_avg:34.84ms
step:199/1900 train_time:6933ms step_avg:34.84ms
step:200/1900 train_time:6966ms step_avg:34.83ms
step:201/1900 train_time:7001ms step_avg:34.83ms
step:202/1900 train_time:7034ms step_avg:34.82ms
step:203/1900 train_time:7068ms step_avg:34.82ms
step:204/1900 train_time:7102ms step_avg:34.82ms
step:205/1900 train_time:7136ms step_avg:34.81ms
step:206/1900 train_time:7170ms step_avg:34.81ms
step:207/1900 train_time:7204ms step_avg:34.80ms
step:208/1900 train_time:7238ms step_avg:34.80ms
step:209/1900 train_time:7273ms step_avg:34.80ms
step:210/1900 train_time:7307ms step_avg:34.79ms
step:211/1900 train_time:7341ms step_avg:34.79ms
step:212/1900 train_time:7375ms step_avg:34.79ms
step:213/1900 train_time:7409ms step_avg:34.78ms
step:214/1900 train_time:7443ms step_avg:34.78ms
step:215/1900 train_time:7477ms step_avg:34.78ms
step:216/1900 train_time:7511ms step_avg:34.77ms
step:217/1900 train_time:7545ms step_avg:34.77ms
step:218/1900 train_time:7579ms step_avg:34.77ms
step:219/1900 train_time:7613ms step_avg:34.76ms
step:220/1900 train_time:7647ms step_avg:34.76ms
step:221/1900 train_time:7681ms step_avg:34.75ms
step:222/1900 train_time:7715ms step_avg:34.75ms
step:223/1900 train_time:7749ms step_avg:34.75ms
step:224/1900 train_time:7782ms step_avg:34.74ms
step:225/1900 train_time:7817ms step_avg:34.74ms
step:226/1900 train_time:7850ms step_avg:34.74ms
step:227/1900 train_time:7884ms step_avg:34.73ms
step:228/1900 train_time:7918ms step_avg:34.73ms
step:229/1900 train_time:7952ms step_avg:34.73ms
step:230/1900 train_time:7986ms step_avg:34.72ms
step:231/1900 train_time:8020ms step_avg:34.72ms
step:232/1900 train_time:8054ms step_avg:34.72ms
step:233/1900 train_time:8088ms step_avg:34.71ms
step:234/1900 train_time:8122ms step_avg:34.71ms
step:235/1900 train_time:8156ms step_avg:34.71ms
step:236/1900 train_time:8190ms step_avg:34.70ms
step:237/1900 train_time:8224ms step_avg:34.70ms
step:238/1900 train_time:8258ms step_avg:34.70ms
step:239/1900 train_time:8292ms step_avg:34.69ms
step:240/1900 train_time:8326ms step_avg:34.69ms
step:241/1900 train_time:8360ms step_avg:34.69ms
step:242/1900 train_time:8394ms step_avg:34.69ms
step:243/1900 train_time:8428ms step_avg:34.68ms
step:244/1900 train_time:8462ms step_avg:34.68ms
step:245/1900 train_time:8496ms step_avg:34.68ms
step:246/1900 train_time:8530ms step_avg:34.67ms
step:247/1900 train_time:8564ms step_avg:34.67ms
step:248/1900 train_time:8598ms step_avg:34.67ms
step:249/1900 train_time:8633ms step_avg:34.67ms
step:250/1900 train_time:8666ms step_avg:34.67ms
step:250/1900 val_loss:4.6191 train_time:8703ms step_avg:34.81ms
step:251/1900 train_time:8723ms step_avg:34.75ms
step:252/1900 train_time:8744ms step_avg:34.70ms
step:253/1900 train_time:8771ms step_avg:34.67ms
step:254/1900 train_time:8806ms step_avg:34.67ms
step:255/1900 train_time:8842ms step_avg:34.67ms
step:256/1900 train_time:8876ms step_avg:34.67ms
step:257/1900 train_time:8911ms step_avg:34.67ms
step:258/1900 train_time:8945ms step_avg:34.67ms
step:259/1900 train_time:8979ms step_avg:34.67ms
step:260/1900 train_time:9013ms step_avg:34.67ms
step:261/1900 train_time:9047ms step_avg:34.66ms
step:262/1900 train_time:9081ms step_avg:34.66ms
step:263/1900 train_time:9115ms step_avg:34.66ms
step:264/1900 train_time:9149ms step_avg:34.65ms
step:265/1900 train_time:9183ms step_avg:34.65ms
step:266/1900 train_time:9216ms step_avg:34.65ms
step:267/1900 train_time:9250ms step_avg:34.65ms
step:268/1900 train_time:9284ms step_avg:34.64ms
step:269/1900 train_time:9318ms step_avg:34.64ms
step:270/1900 train_time:9352ms step_avg:34.64ms
step:271/1900 train_time:9386ms step_avg:34.63ms
step:272/1900 train_time:9419ms step_avg:34.63ms
step:273/1900 train_time:9453ms step_avg:34.63ms
step:274/1900 train_time:9487ms step_avg:34.62ms
step:275/1900 train_time:9521ms step_avg:34.62ms
step:276/1900 train_time:9555ms step_avg:34.62ms
step:277/1900 train_time:9589ms step_avg:34.62ms
step:278/1900 train_time:9622ms step_avg:34.61ms
step:279/1900 train_time:9656ms step_avg:34.61ms
step:280/1900 train_time:9690ms step_avg:34.61ms
step:281/1900 train_time:9724ms step_avg:34.60ms
step:282/1900 train_time:9758ms step_avg:34.60ms
step:283/1900 train_time:9792ms step_avg:34.60ms
step:284/1900 train_time:9826ms step_avg:34.60ms
step:285/1900 train_time:9861ms step_avg:34.60ms
step:286/1900 train_time:9895ms step_avg:34.60ms
step:287/1900 train_time:9929ms step_avg:34.60ms
step:288/1900 train_time:9963ms step_avg:34.59ms
step:289/1900 train_time:9997ms step_avg:34.59ms
step:290/1900 train_time:10031ms step_avg:34.59ms
step:291/1900 train_time:10065ms step_avg:34.59ms
step:292/1900 train_time:10099ms step_avg:34.59ms
step:293/1900 train_time:10133ms step_avg:34.58ms
step:294/1900 train_time:10167ms step_avg:34.58ms
step:295/1900 train_time:10201ms step_avg:34.58ms
step:296/1900 train_time:10235ms step_avg:34.58ms
step:297/1900 train_time:10269ms step_avg:34.58ms
step:298/1900 train_time:10303ms step_avg:34.57ms
step:299/1900 train_time:10337ms step_avg:34.57ms
step:300/1900 train_time:10371ms step_avg:34.57ms
step:301/1900 train_time:10404ms step_avg:34.57ms
step:302/1900 train_time:10438ms step_avg:34.56ms
step:303/1900 train_time:10472ms step_avg:34.56ms
step:304/1900 train_time:10506ms step_avg:34.56ms
step:305/1900 train_time:10540ms step_avg:34.56ms
step:306/1900 train_time:10574ms step_avg:34.56ms
step:307/1900 train_time:10608ms step_avg:34.55ms
step:308/1900 train_time:10642ms step_avg:34.55ms
step:309/1900 train_time:10676ms step_avg:34.55ms
step:310/1900 train_time:10710ms step_avg:34.55ms
step:311/1900 train_time:10744ms step_avg:34.55ms
step:312/1900 train_time:10777ms step_avg:34.54ms
step:313/1900 train_time:10812ms step_avg:34.54ms
step:314/1900 train_time:10845ms step_avg:34.54ms
step:315/1900 train_time:10880ms step_avg:34.54ms
step:316/1900 train_time:10914ms step_avg:34.54ms
step:317/1900 train_time:10948ms step_avg:34.54ms
step:318/1900 train_time:10982ms step_avg:34.53ms
step:319/1900 train_time:11016ms step_avg:34.53ms
step:320/1900 train_time:11050ms step_avg:34.53ms
step:321/1900 train_time:11084ms step_avg:34.53ms
step:322/1900 train_time:11117ms step_avg:34.53ms
step:323/1900 train_time:11152ms step_avg:34.53ms
step:324/1900 train_time:11186ms step_avg:34.52ms
step:325/1900 train_time:11219ms step_avg:34.52ms
step:326/1900 train_time:11253ms step_avg:34.52ms
step:327/1900 train_time:11287ms step_avg:34.52ms
step:328/1900 train_time:11321ms step_avg:34.52ms
step:329/1900 train_time:11355ms step_avg:34.51ms
step:330/1900 train_time:11389ms step_avg:34.51ms
step:331/1900 train_time:11423ms step_avg:34.51ms
step:332/1900 train_time:11457ms step_avg:34.51ms
step:333/1900 train_time:11491ms step_avg:34.51ms
step:334/1900 train_time:11525ms step_avg:34.50ms
step:335/1900 train_time:11559ms step_avg:34.50ms
step:336/1900 train_time:11592ms step_avg:34.50ms
step:337/1900 train_time:11627ms step_avg:34.50ms
step:338/1900 train_time:11660ms step_avg:34.50ms
step:339/1900 train_time:11695ms step_avg:34.50ms
step:340/1900 train_time:11728ms step_avg:34.50ms
step:341/1900 train_time:11763ms step_avg:34.49ms
step:342/1900 train_time:11796ms step_avg:34.49ms
step:343/1900 train_time:11830ms step_avg:34.49ms
step:344/1900 train_time:11864ms step_avg:34.49ms
step:345/1900 train_time:11898ms step_avg:34.49ms
step:346/1900 train_time:11932ms step_avg:34.48ms
step:347/1900 train_time:11966ms step_avg:34.48ms
step:348/1900 train_time:12000ms step_avg:34.48ms
step:349/1900 train_time:12034ms step_avg:34.48ms
step:350/1900 train_time:12068ms step_avg:34.48ms
step:351/1900 train_time:12102ms step_avg:34.48ms
step:352/1900 train_time:12136ms step_avg:34.48ms
step:353/1900 train_time:12170ms step_avg:34.48ms
step:354/1900 train_time:12204ms step_avg:34.47ms
step:355/1900 train_time:12238ms step_avg:34.47ms
step:356/1900 train_time:12272ms step_avg:34.47ms
step:357/1900 train_time:12306ms step_avg:34.47ms
step:358/1900 train_time:12340ms step_avg:34.47ms
step:359/1900 train_time:12374ms step_avg:34.47ms
step:360/1900 train_time:12407ms step_avg:34.47ms
step:361/1900 train_time:12442ms step_avg:34.46ms
step:362/1900 train_time:12476ms step_avg:34.46ms
step:363/1900 train_time:12510ms step_avg:34.46ms
step:364/1900 train_time:12543ms step_avg:34.46ms
step:365/1900 train_time:12577ms step_avg:34.46ms
step:366/1900 train_time:12611ms step_avg:34.46ms
step:367/1900 train_time:12645ms step_avg:34.46ms
step:368/1900 train_time:12679ms step_avg:34.45ms
step:369/1900 train_time:12713ms step_avg:34.45ms
step:370/1900 train_time:12747ms step_avg:34.45ms
step:371/1900 train_time:12781ms step_avg:34.45ms
step:372/1900 train_time:12815ms step_avg:34.45ms
step:373/1900 train_time:12849ms step_avg:34.45ms
step:374/1900 train_time:12883ms step_avg:34.45ms
step:375/1900 train_time:12917ms step_avg:34.44ms
step:376/1900 train_time:12951ms step_avg:34.44ms
step:377/1900 train_time:12985ms step_avg:34.44ms
step:378/1900 train_time:13019ms step_avg:34.44ms
step:379/1900 train_time:13053ms step_avg:34.44ms
step:380/1900 train_time:13087ms step_avg:34.44ms
step:381/1900 train_time:13121ms step_avg:34.44ms
step:382/1900 train_time:13155ms step_avg:34.44ms
step:383/1900 train_time:13189ms step_avg:34.44ms
step:384/1900 train_time:13223ms step_avg:34.43ms
step:385/1900 train_time:13257ms step_avg:34.43ms
step:386/1900 train_time:13291ms step_avg:34.43ms
step:387/1900 train_time:13325ms step_avg:34.43ms
step:388/1900 train_time:13359ms step_avg:34.43ms
step:389/1900 train_time:13392ms step_avg:34.43ms
step:390/1900 train_time:13426ms step_avg:34.43ms
step:391/1900 train_time:13460ms step_avg:34.43ms
step:392/1900 train_time:13494ms step_avg:34.42ms
step:393/1900 train_time:13528ms step_avg:34.42ms
step:394/1900 train_time:13562ms step_avg:34.42ms
step:395/1900 train_time:13596ms step_avg:34.42ms
step:396/1900 train_time:13630ms step_avg:34.42ms
step:397/1900 train_time:13664ms step_avg:34.42ms
step:398/1900 train_time:13698ms step_avg:34.42ms
step:399/1900 train_time:13732ms step_avg:34.42ms
step:400/1900 train_time:13766ms step_avg:34.42ms
step:401/1900 train_time:13800ms step_avg:34.42ms
step:402/1900 train_time:13834ms step_avg:34.41ms
step:403/1900 train_time:13868ms step_avg:34.41ms
step:404/1900 train_time:13902ms step_avg:34.41ms
step:405/1900 train_time:13936ms step_avg:34.41ms
step:406/1900 train_time:13970ms step_avg:34.41ms
step:407/1900 train_time:14004ms step_avg:34.41ms
step:408/1900 train_time:14038ms step_avg:34.41ms
step:409/1900 train_time:14072ms step_avg:34.41ms
step:410/1900 train_time:14106ms step_avg:34.41ms
step:411/1900 train_time:14141ms step_avg:34.41ms
step:412/1900 train_time:14174ms step_avg:34.40ms
step:413/1900 train_time:14209ms step_avg:34.40ms
step:414/1900 train_time:14243ms step_avg:34.40ms
step:415/1900 train_time:14277ms step_avg:34.40ms
step:416/1900 train_time:14310ms step_avg:34.40ms
step:417/1900 train_time:14344ms step_avg:34.40ms
step:418/1900 train_time:14378ms step_avg:34.40ms
step:419/1900 train_time:14412ms step_avg:34.40ms
step:420/1900 train_time:14446ms step_avg:34.39ms
step:421/1900 train_time:14480ms step_avg:34.39ms
step:422/1900 train_time:14514ms step_avg:34.39ms
step:423/1900 train_time:14548ms step_avg:34.39ms
step:424/1900 train_time:14582ms step_avg:34.39ms
step:425/1900 train_time:14616ms step_avg:34.39ms
step:426/1900 train_time:14650ms step_avg:34.39ms
step:427/1900 train_time:14684ms step_avg:34.39ms
step:428/1900 train_time:14718ms step_avg:34.39ms
step:429/1900 train_time:14752ms step_avg:34.39ms
step:430/1900 train_time:14786ms step_avg:34.39ms
step:431/1900 train_time:14820ms step_avg:34.39ms
step:432/1900 train_time:14854ms step_avg:34.38ms
step:433/1900 train_time:14888ms step_avg:34.38ms
step:434/1900 train_time:14922ms step_avg:34.38ms
step:435/1900 train_time:14956ms step_avg:34.38ms
step:436/1900 train_time:14990ms step_avg:34.38ms
step:437/1900 train_time:15024ms step_avg:34.38ms
step:438/1900 train_time:15058ms step_avg:34.38ms
step:439/1900 train_time:15092ms step_avg:34.38ms
step:440/1900 train_time:15126ms step_avg:34.38ms
step:441/1900 train_time:15160ms step_avg:34.38ms
step:442/1900 train_time:15194ms step_avg:34.38ms
step:443/1900 train_time:15228ms step_avg:34.38ms
step:444/1900 train_time:15262ms step_avg:34.37ms
step:445/1900 train_time:15296ms step_avg:34.37ms
step:446/1900 train_time:15330ms step_avg:34.37ms
step:447/1900 train_time:15365ms step_avg:34.37ms
step:448/1900 train_time:15399ms step_avg:34.37ms
step:449/1900 train_time:15433ms step_avg:34.37ms
step:450/1900 train_time:15466ms step_avg:34.37ms
step:451/1900 train_time:15501ms step_avg:34.37ms
step:452/1900 train_time:15534ms step_avg:34.37ms
step:453/1900 train_time:15568ms step_avg:34.37ms
step:454/1900 train_time:15602ms step_avg:34.37ms
step:455/1900 train_time:15636ms step_avg:34.37ms
step:456/1900 train_time:15670ms step_avg:34.36ms
step:457/1900 train_time:15704ms step_avg:34.36ms
step:458/1900 train_time:15738ms step_avg:34.36ms
step:459/1900 train_time:15772ms step_avg:34.36ms
step:460/1900 train_time:15806ms step_avg:34.36ms
step:461/1900 train_time:15840ms step_avg:34.36ms
step:462/1900 train_time:15874ms step_avg:34.36ms
step:463/1900 train_time:15908ms step_avg:34.36ms
step:464/1900 train_time:15942ms step_avg:34.36ms
step:465/1900 train_time:15976ms step_avg:34.36ms
step:466/1900 train_time:16010ms step_avg:34.36ms
step:467/1900 train_time:16044ms step_avg:34.36ms
step:468/1900 train_time:16078ms step_avg:34.35ms
step:469/1900 train_time:16112ms step_avg:34.35ms
step:470/1900 train_time:16146ms step_avg:34.35ms
step:471/1900 train_time:16180ms step_avg:34.35ms
step:472/1900 train_time:16214ms step_avg:34.35ms
step:473/1900 train_time:16248ms step_avg:34.35ms
step:474/1900 train_time:16282ms step_avg:34.35ms
step:475/1900 train_time:16315ms step_avg:34.35ms
step:476/1900 train_time:16349ms step_avg:34.35ms
step:477/1900 train_time:16383ms step_avg:34.35ms
step:478/1900 train_time:16417ms step_avg:34.35ms
step:479/1900 train_time:16452ms step_avg:34.35ms
step:480/1900 train_time:16485ms step_avg:34.34ms
step:481/1900 train_time:16520ms step_avg:34.34ms
step:482/1900 train_time:16553ms step_avg:34.34ms
step:483/1900 train_time:16588ms step_avg:34.34ms
step:484/1900 train_time:16622ms step_avg:34.34ms
step:485/1900 train_time:16656ms step_avg:34.34ms
step:486/1900 train_time:16689ms step_avg:34.34ms
step:487/1900 train_time:16724ms step_avg:34.34ms
step:488/1900 train_time:16758ms step_avg:34.34ms
step:489/1900 train_time:16792ms step_avg:34.34ms
step:490/1900 train_time:16825ms step_avg:34.34ms
step:491/1900 train_time:16859ms step_avg:34.34ms
step:492/1900 train_time:16893ms step_avg:34.34ms
step:493/1900 train_time:16927ms step_avg:34.34ms
step:494/1900 train_time:16961ms step_avg:34.33ms
step:495/1900 train_time:16995ms step_avg:34.33ms
step:496/1900 train_time:17029ms step_avg:34.33ms
step:497/1900 train_time:17063ms step_avg:34.33ms
step:498/1900 train_time:17097ms step_avg:34.33ms
step:499/1900 train_time:17131ms step_avg:34.33ms
step:500/1900 train_time:17165ms step_avg:34.33ms
step:500/1900 val_loss:4.2792 train_time:17202ms step_avg:34.40ms
step:501/1900 train_time:17223ms step_avg:34.38ms
step:502/1900 train_time:17243ms step_avg:34.35ms
step:503/1900 train_time:17272ms step_avg:34.34ms
step:504/1900 train_time:17306ms step_avg:34.34ms
step:505/1900 train_time:17341ms step_avg:34.34ms
step:506/1900 train_time:17375ms step_avg:34.34ms
step:507/1900 train_time:17410ms step_avg:34.34ms
step:508/1900 train_time:17444ms step_avg:34.34ms
step:509/1900 train_time:17479ms step_avg:34.34ms
step:510/1900 train_time:17512ms step_avg:34.34ms
step:511/1900 train_time:17547ms step_avg:34.34ms
step:512/1900 train_time:17581ms step_avg:34.34ms
step:513/1900 train_time:17615ms step_avg:34.34ms
step:514/1900 train_time:17649ms step_avg:34.34ms
step:515/1900 train_time:17683ms step_avg:34.34ms
step:516/1900 train_time:17716ms step_avg:34.33ms
step:517/1900 train_time:17750ms step_avg:34.33ms
step:518/1900 train_time:17784ms step_avg:34.33ms
step:519/1900 train_time:17818ms step_avg:34.33ms
step:520/1900 train_time:17851ms step_avg:34.33ms
step:521/1900 train_time:17885ms step_avg:34.33ms
step:522/1900 train_time:17919ms step_avg:34.33ms
step:523/1900 train_time:17953ms step_avg:34.33ms
step:524/1900 train_time:17987ms step_avg:34.33ms
step:525/1900 train_time:18020ms step_avg:34.32ms
step:526/1900 train_time:18054ms step_avg:34.32ms
step:527/1900 train_time:18088ms step_avg:34.32ms
step:528/1900 train_time:18122ms step_avg:34.32ms
step:529/1900 train_time:18155ms step_avg:34.32ms
step:530/1900 train_time:18189ms step_avg:34.32ms
step:531/1900 train_time:18224ms step_avg:34.32ms
step:532/1900 train_time:18257ms step_avg:34.32ms
step:533/1900 train_time:18292ms step_avg:34.32ms
step:534/1900 train_time:18326ms step_avg:34.32ms
step:535/1900 train_time:18361ms step_avg:34.32ms
step:536/1900 train_time:18395ms step_avg:34.32ms
step:537/1900 train_time:18429ms step_avg:34.32ms
step:538/1900 train_time:18463ms step_avg:34.32ms
step:539/1900 train_time:18497ms step_avg:34.32ms
step:540/1900 train_time:18531ms step_avg:34.32ms
step:541/1900 train_time:18566ms step_avg:34.32ms
step:542/1900 train_time:18600ms step_avg:34.32ms
step:543/1900 train_time:18634ms step_avg:34.32ms
step:544/1900 train_time:18668ms step_avg:34.32ms
step:545/1900 train_time:18702ms step_avg:34.32ms
step:546/1900 train_time:18736ms step_avg:34.32ms
step:547/1900 train_time:18770ms step_avg:34.31ms
step:548/1900 train_time:18804ms step_avg:34.31ms
step:549/1900 train_time:18838ms step_avg:34.31ms
step:550/1900 train_time:18872ms step_avg:34.31ms
step:551/1900 train_time:18905ms step_avg:34.31ms
step:552/1900 train_time:18939ms step_avg:34.31ms
step:553/1900 train_time:18973ms step_avg:34.31ms
step:554/1900 train_time:19007ms step_avg:34.31ms
step:555/1900 train_time:19041ms step_avg:34.31ms
step:556/1900 train_time:19075ms step_avg:34.31ms
step:557/1900 train_time:19109ms step_avg:34.31ms
step:558/1900 train_time:19142ms step_avg:34.31ms
step:559/1900 train_time:19176ms step_avg:34.30ms
step:560/1900 train_time:19210ms step_avg:34.30ms
step:561/1900 train_time:19244ms step_avg:34.30ms
step:562/1900 train_time:19278ms step_avg:34.30ms
step:563/1900 train_time:19311ms step_avg:34.30ms
step:564/1900 train_time:19345ms step_avg:34.30ms
step:565/1900 train_time:19379ms step_avg:34.30ms
step:566/1900 train_time:19413ms step_avg:34.30ms
step:567/1900 train_time:19448ms step_avg:34.30ms
step:568/1900 train_time:19482ms step_avg:34.30ms
step:569/1900 train_time:19516ms step_avg:34.30ms
step:570/1900 train_time:19550ms step_avg:34.30ms
step:571/1900 train_time:19585ms step_avg:34.30ms
step:572/1900 train_time:19618ms step_avg:34.30ms
step:573/1900 train_time:19653ms step_avg:34.30ms
step:574/1900 train_time:19686ms step_avg:34.30ms
step:575/1900 train_time:19721ms step_avg:34.30ms
step:576/1900 train_time:19755ms step_avg:34.30ms
step:577/1900 train_time:19789ms step_avg:34.30ms
step:578/1900 train_time:19822ms step_avg:34.29ms
step:579/1900 train_time:19856ms step_avg:34.29ms
step:580/1900 train_time:19890ms step_avg:34.29ms
step:581/1900 train_time:19924ms step_avg:34.29ms
step:582/1900 train_time:19958ms step_avg:34.29ms
step:583/1900 train_time:19992ms step_avg:34.29ms
step:584/1900 train_time:20026ms step_avg:34.29ms
step:585/1900 train_time:20060ms step_avg:34.29ms
step:586/1900 train_time:20094ms step_avg:34.29ms
step:587/1900 train_time:20128ms step_avg:34.29ms
step:588/1900 train_time:20162ms step_avg:34.29ms
step:589/1900 train_time:20196ms step_avg:34.29ms
step:590/1900 train_time:20230ms step_avg:34.29ms
step:591/1900 train_time:20264ms step_avg:34.29ms
step:592/1900 train_time:20298ms step_avg:34.29ms
step:593/1900 train_time:20332ms step_avg:34.29ms
step:594/1900 train_time:20365ms step_avg:34.29ms
step:595/1900 train_time:20400ms step_avg:34.28ms
step:596/1900 train_time:20433ms step_avg:34.28ms
step:597/1900 train_time:20468ms step_avg:34.28ms
step:598/1900 train_time:20502ms step_avg:34.28ms
step:599/1900 train_time:20536ms step_avg:34.28ms
step:600/1900 train_time:20570ms step_avg:34.28ms
step:601/1900 train_time:20604ms step_avg:34.28ms
step:602/1900 train_time:20638ms step_avg:34.28ms
step:603/1900 train_time:20672ms step_avg:34.28ms
step:604/1900 train_time:20706ms step_avg:34.28ms
step:605/1900 train_time:20740ms step_avg:34.28ms
step:606/1900 train_time:20774ms step_avg:34.28ms
step:607/1900 train_time:20808ms step_avg:34.28ms
step:608/1900 train_time:20842ms step_avg:34.28ms
step:609/1900 train_time:20876ms step_avg:34.28ms
step:610/1900 train_time:20910ms step_avg:34.28ms
step:611/1900 train_time:20944ms step_avg:34.28ms
step:612/1900 train_time:20977ms step_avg:34.28ms
step:613/1900 train_time:21011ms step_avg:34.28ms
step:614/1900 train_time:21045ms step_avg:34.27ms
step:615/1900 train_time:21079ms step_avg:34.27ms
step:616/1900 train_time:21113ms step_avg:34.27ms
step:617/1900 train_time:21147ms step_avg:34.27ms
step:618/1900 train_time:21181ms step_avg:34.27ms
step:619/1900 train_time:21215ms step_avg:34.27ms
step:620/1900 train_time:21249ms step_avg:34.27ms
step:621/1900 train_time:21285ms step_avg:34.27ms
step:622/1900 train_time:21344ms step_avg:34.32ms
step:623/1900 train_time:21406ms step_avg:34.36ms
step:624/1900 train_time:21466ms step_avg:34.40ms
step:625/1900 train_time:21528ms step_avg:34.44ms
step:626/1900 train_time:21589ms step_avg:34.49ms
step:627/1900 train_time:21651ms step_avg:34.53ms
step:628/1900 train_time:21712ms step_avg:34.57ms
step:629/1900 train_time:21774ms step_avg:34.62ms
step:630/1900 train_time:21836ms step_avg:34.66ms
step:631/1900 train_time:21898ms step_avg:34.70ms
step:632/1900 train_time:21960ms step_avg:34.75ms
step:633/1900 train_time:22022ms step_avg:34.79ms
step:634/1900 train_time:22082ms step_avg:34.83ms
step:635/1900 train_time:22144ms step_avg:34.87ms
step:636/1900 train_time:22205ms step_avg:34.91ms
step:637/1900 train_time:22266ms step_avg:34.95ms
step:638/1900 train_time:22327ms step_avg:35.00ms
step:639/1900 train_time:22389ms step_avg:35.04ms
step:640/1900 train_time:22450ms step_avg:35.08ms
step:641/1900 train_time:22512ms step_avg:35.12ms
step:642/1900 train_time:22573ms step_avg:35.16ms
step:643/1900 train_time:22635ms step_avg:35.20ms
step:644/1900 train_time:22696ms step_avg:35.24ms
step:645/1900 train_time:22757ms step_avg:35.28ms
step:646/1900 train_time:22818ms step_avg:35.32ms
step:647/1900 train_time:22880ms step_avg:35.36ms
step:648/1900 train_time:22941ms step_avg:35.40ms
step:649/1900 train_time:23003ms step_avg:35.44ms
step:650/1900 train_time:23064ms step_avg:35.48ms
step:651/1900 train_time:23126ms step_avg:35.52ms
step:652/1900 train_time:23187ms step_avg:35.56ms
step:653/1900 train_time:23248ms step_avg:35.60ms
step:654/1900 train_time:23309ms step_avg:35.64ms
step:655/1900 train_time:23370ms step_avg:35.68ms
step:656/1900 train_time:23431ms step_avg:35.72ms
step:657/1900 train_time:23493ms step_avg:35.76ms
step:658/1900 train_time:23554ms step_avg:35.80ms
step:659/1900 train_time:23616ms step_avg:35.84ms
step:660/1900 train_time:23677ms step_avg:35.87ms
step:661/1900 train_time:23739ms step_avg:35.91ms
step:662/1900 train_time:23799ms step_avg:35.95ms
step:663/1900 train_time:23861ms step_avg:35.99ms
step:664/1900 train_time:23922ms step_avg:36.03ms
step:665/1900 train_time:23984ms step_avg:36.07ms
step:666/1900 train_time:24045ms step_avg:36.10ms
step:667/1900 train_time:24107ms step_avg:36.14ms
step:668/1900 train_time:24167ms step_avg:36.18ms
step:669/1900 train_time:24229ms step_avg:36.22ms
step:670/1900 train_time:24290ms step_avg:36.25ms
step:671/1900 train_time:24352ms step_avg:36.29ms
step:672/1900 train_time:24413ms step_avg:36.33ms
step:673/1900 train_time:24474ms step_avg:36.37ms
step:674/1900 train_time:24535ms step_avg:36.40ms
step:675/1900 train_time:24597ms step_avg:36.44ms
step:676/1900 train_time:24658ms step_avg:36.48ms
step:677/1900 train_time:24720ms step_avg:36.51ms
step:678/1900 train_time:24780ms step_avg:36.55ms
step:679/1900 train_time:24842ms step_avg:36.59ms
step:680/1900 train_time:24903ms step_avg:36.62ms
step:681/1900 train_time:24965ms step_avg:36.66ms
step:682/1900 train_time:25026ms step_avg:36.70ms
step:683/1900 train_time:25088ms step_avg:36.73ms
step:684/1900 train_time:25149ms step_avg:36.77ms
step:685/1900 train_time:25210ms step_avg:36.80ms
step:686/1900 train_time:25271ms step_avg:36.84ms
step:687/1900 train_time:25332ms step_avg:36.87ms
step:688/1900 train_time:25393ms step_avg:36.91ms
step:689/1900 train_time:25455ms step_avg:36.95ms
step:690/1900 train_time:25517ms step_avg:36.98ms
step:691/1900 train_time:25579ms step_avg:37.02ms
step:692/1900 train_time:25639ms step_avg:37.05ms
step:693/1900 train_time:25701ms step_avg:37.09ms
step:694/1900 train_time:25762ms step_avg:37.12ms
step:695/1900 train_time:25824ms step_avg:37.16ms
step:696/1900 train_time:25884ms step_avg:37.19ms
step:697/1900 train_time:25946ms step_avg:37.22ms
step:698/1900 train_time:26007ms step_avg:37.26ms
step:699/1900 train_time:26069ms step_avg:37.29ms
step:700/1900 train_time:26130ms step_avg:37.33ms
step:701/1900 train_time:26192ms step_avg:37.36ms
step:702/1900 train_time:26253ms step_avg:37.40ms
step:703/1900 train_time:26315ms step_avg:37.43ms
step:704/1900 train_time:26376ms step_avg:37.47ms
step:705/1900 train_time:26438ms step_avg:37.50ms
step:706/1900 train_time:26499ms step_avg:37.53ms
step:707/1900 train_time:26561ms step_avg:37.57ms
step:708/1900 train_time:26622ms step_avg:37.60ms
step:709/1900 train_time:26685ms step_avg:37.64ms
step:710/1900 train_time:26745ms step_avg:37.67ms
step:711/1900 train_time:26808ms step_avg:37.70ms
step:712/1900 train_time:26869ms step_avg:37.74ms
step:713/1900 train_time:26932ms step_avg:37.77ms
step:714/1900 train_time:26993ms step_avg:37.81ms
step:715/1900 train_time:27055ms step_avg:37.84ms
step:716/1900 train_time:27116ms step_avg:37.87ms
step:717/1900 train_time:27177ms step_avg:37.90ms
step:718/1900 train_time:27238ms step_avg:37.94ms
step:719/1900 train_time:27300ms step_avg:37.97ms
step:720/1900 train_time:27361ms step_avg:38.00ms
step:721/1900 train_time:27422ms step_avg:38.03ms
step:722/1900 train_time:27483ms step_avg:38.07ms
step:723/1900 train_time:27545ms step_avg:38.10ms
step:724/1900 train_time:27606ms step_avg:38.13ms
step:725/1900 train_time:27668ms step_avg:38.16ms
step:726/1900 train_time:27730ms step_avg:38.20ms
step:727/1900 train_time:27792ms step_avg:38.23ms
step:728/1900 train_time:27853ms step_avg:38.26ms
step:729/1900 train_time:27915ms step_avg:38.29ms
step:730/1900 train_time:27976ms step_avg:38.32ms
step:731/1900 train_time:28038ms step_avg:38.36ms
step:732/1900 train_time:28098ms step_avg:38.39ms
step:733/1900 train_time:28160ms step_avg:38.42ms
step:734/1900 train_time:28221ms step_avg:38.45ms
step:735/1900 train_time:28283ms step_avg:38.48ms
step:736/1900 train_time:28344ms step_avg:38.51ms
step:737/1900 train_time:28406ms step_avg:38.54ms
step:738/1900 train_time:28466ms step_avg:38.57ms
step:739/1900 train_time:28528ms step_avg:38.60ms
step:740/1900 train_time:28590ms step_avg:38.63ms
step:741/1900 train_time:28652ms step_avg:38.67ms
step:742/1900 train_time:28714ms step_avg:38.70ms
step:743/1900 train_time:28775ms step_avg:38.73ms
step:744/1900 train_time:28836ms step_avg:38.76ms
step:745/1900 train_time:28898ms step_avg:38.79ms
step:746/1900 train_time:28959ms step_avg:38.82ms
step:747/1900 train_time:29020ms step_avg:38.85ms
step:748/1900 train_time:29081ms step_avg:38.88ms
step:749/1900 train_time:29142ms step_avg:38.91ms
step:750/1900 train_time:29203ms step_avg:38.94ms
step:750/1900 val_loss:4.0121 train_time:29267ms step_avg:39.02ms
step:751/1900 train_time:29288ms step_avg:39.00ms
step:752/1900 train_time:29329ms step_avg:39.00ms
step:753/1900 train_time:29393ms step_avg:39.04ms
step:754/1900 train_time:29456ms step_avg:39.07ms
step:755/1900 train_time:29519ms step_avg:39.10ms
step:756/1900 train_time:29580ms step_avg:39.13ms
step:757/1900 train_time:29641ms step_avg:39.16ms
step:758/1900 train_time:29702ms step_avg:39.18ms
step:759/1900 train_time:29763ms step_avg:39.21ms
step:760/1900 train_time:29824ms step_avg:39.24ms
step:761/1900 train_time:29885ms step_avg:39.27ms
step:762/1900 train_time:29946ms step_avg:39.30ms
step:763/1900 train_time:30007ms step_avg:39.33ms
step:764/1900 train_time:30067ms step_avg:39.35ms
step:765/1900 train_time:30128ms step_avg:39.38ms
step:766/1900 train_time:30189ms step_avg:39.41ms
step:767/1900 train_time:30252ms step_avg:39.44ms
step:768/1900 train_time:30313ms step_avg:39.47ms
step:769/1900 train_time:30376ms step_avg:39.50ms
step:770/1900 train_time:30438ms step_avg:39.53ms
step:771/1900 train_time:30501ms step_avg:39.56ms
step:772/1900 train_time:30562ms step_avg:39.59ms
step:773/1900 train_time:30623ms step_avg:39.62ms
step:774/1900 train_time:30684ms step_avg:39.64ms
step:775/1900 train_time:30746ms step_avg:39.67ms
step:776/1900 train_time:30807ms step_avg:39.70ms
step:777/1900 train_time:30868ms step_avg:39.73ms
step:778/1900 train_time:30929ms step_avg:39.75ms
step:779/1900 train_time:30990ms step_avg:39.78ms
step:780/1900 train_time:31051ms step_avg:39.81ms
step:781/1900 train_time:31113ms step_avg:39.84ms
step:782/1900 train_time:31174ms step_avg:39.86ms
step:783/1900 train_time:31236ms step_avg:39.89ms
step:784/1900 train_time:31298ms step_avg:39.92ms
step:785/1900 train_time:31360ms step_avg:39.95ms
step:786/1900 train_time:31422ms step_avg:39.98ms
step:787/1900 train_time:31484ms step_avg:40.00ms
step:788/1900 train_time:31545ms step_avg:40.03ms
step:789/1900 train_time:31607ms step_avg:40.06ms
step:790/1900 train_time:31668ms step_avg:40.09ms
step:791/1900 train_time:31729ms step_avg:40.11ms
step:792/1900 train_time:31790ms step_avg:40.14ms
step:793/1900 train_time:31852ms step_avg:40.17ms
step:794/1900 train_time:31913ms step_avg:40.19ms
step:795/1900 train_time:31974ms step_avg:40.22ms
step:796/1900 train_time:32035ms step_avg:40.25ms
step:797/1900 train_time:32097ms step_avg:40.27ms
step:798/1900 train_time:32159ms step_avg:40.30ms
step:799/1900 train_time:32221ms step_avg:40.33ms
step:800/1900 train_time:32282ms step_avg:40.35ms
step:801/1900 train_time:32345ms step_avg:40.38ms
step:802/1900 train_time:32406ms step_avg:40.41ms
step:803/1900 train_time:32467ms step_avg:40.43ms
step:804/1900 train_time:32528ms step_avg:40.46ms
step:805/1900 train_time:32590ms step_avg:40.48ms
step:806/1900 train_time:32651ms step_avg:40.51ms
step:807/1900 train_time:32713ms step_avg:40.54ms
step:808/1900 train_time:32774ms step_avg:40.56ms
step:809/1900 train_time:32835ms step_avg:40.59ms
step:810/1900 train_time:32896ms step_avg:40.61ms
step:811/1900 train_time:32958ms step_avg:40.64ms
step:812/1900 train_time:33019ms step_avg:40.66ms
step:813/1900 train_time:33080ms step_avg:40.69ms
step:814/1900 train_time:33142ms step_avg:40.71ms
step:815/1900 train_time:33203ms step_avg:40.74ms
step:816/1900 train_time:33264ms step_avg:40.76ms
step:817/1900 train_time:33326ms step_avg:40.79ms
step:818/1900 train_time:33388ms step_avg:40.82ms
step:819/1900 train_time:33450ms step_avg:40.84ms
step:820/1900 train_time:33511ms step_avg:40.87ms
step:821/1900 train_time:33573ms step_avg:40.89ms
step:822/1900 train_time:33634ms step_avg:40.92ms
step:823/1900 train_time:33696ms step_avg:40.94ms
step:824/1900 train_time:33758ms step_avg:40.97ms
step:825/1900 train_time:33819ms step_avg:40.99ms
step:826/1900 train_time:33880ms step_avg:41.02ms
step:827/1900 train_time:33942ms step_avg:41.04ms
step:828/1900 train_time:34002ms step_avg:41.07ms
step:829/1900 train_time:34064ms step_avg:41.09ms
step:830/1900 train_time:34124ms step_avg:41.11ms
step:831/1900 train_time:34187ms step_avg:41.14ms
step:832/1900 train_time:34248ms step_avg:41.16ms
step:833/1900 train_time:34310ms step_avg:41.19ms
step:834/1900 train_time:34371ms step_avg:41.21ms
step:835/1900 train_time:34433ms step_avg:41.24ms
step:836/1900 train_time:34494ms step_avg:41.26ms
step:837/1900 train_time:34556ms step_avg:41.29ms
step:838/1900 train_time:34617ms step_avg:41.31ms
step:839/1900 train_time:34680ms step_avg:41.33ms
step:840/1900 train_time:34741ms step_avg:41.36ms
step:841/1900 train_time:34802ms step_avg:41.38ms
step:842/1900 train_time:34863ms step_avg:41.41ms
step:843/1900 train_time:34924ms step_avg:41.43ms
step:844/1900 train_time:34985ms step_avg:41.45ms
step:845/1900 train_time:35047ms step_avg:41.48ms
step:846/1900 train_time:35109ms step_avg:41.50ms
step:847/1900 train_time:35170ms step_avg:41.52ms
step:848/1900 train_time:35231ms step_avg:41.55ms
step:849/1900 train_time:35293ms step_avg:41.57ms
step:850/1900 train_time:35354ms step_avg:41.59ms
step:851/1900 train_time:35416ms step_avg:41.62ms
step:852/1900 train_time:35477ms step_avg:41.64ms
step:853/1900 train_time:35540ms step_avg:41.66ms
step:854/1900 train_time:35601ms step_avg:41.69ms
step:855/1900 train_time:35663ms step_avg:41.71ms
step:856/1900 train_time:35724ms step_avg:41.73ms
step:857/1900 train_time:35786ms step_avg:41.76ms
step:858/1900 train_time:35847ms step_avg:41.78ms
step:859/1900 train_time:35908ms step_avg:41.80ms
step:860/1900 train_time:35969ms step_avg:41.82ms
step:861/1900 train_time:36031ms step_avg:41.85ms
step:862/1900 train_time:36092ms step_avg:41.87ms
step:863/1900 train_time:36153ms step_avg:41.89ms
step:864/1900 train_time:36214ms step_avg:41.91ms
step:865/1900 train_time:36277ms step_avg:41.94ms
step:866/1900 train_time:36338ms step_avg:41.96ms
step:867/1900 train_time:36399ms step_avg:41.98ms
step:868/1900 train_time:36461ms step_avg:42.01ms
step:869/1900 train_time:36523ms step_avg:42.03ms
step:870/1900 train_time:36584ms step_avg:42.05ms
step:871/1900 train_time:36646ms step_avg:42.07ms
step:872/1900 train_time:36706ms step_avg:42.09ms
step:873/1900 train_time:36768ms step_avg:42.12ms
step:874/1900 train_time:36829ms step_avg:42.14ms
step:875/1900 train_time:36891ms step_avg:42.16ms
step:876/1900 train_time:36952ms step_avg:42.18ms
step:877/1900 train_time:37014ms step_avg:42.21ms
step:878/1900 train_time:37075ms step_avg:42.23ms
step:879/1900 train_time:37137ms step_avg:42.25ms
step:880/1900 train_time:37198ms step_avg:42.27ms
step:881/1900 train_time:37260ms step_avg:42.29ms
step:882/1900 train_time:37321ms step_avg:42.31ms
step:883/1900 train_time:37383ms step_avg:42.34ms
step:884/1900 train_time:37444ms step_avg:42.36ms
step:885/1900 train_time:37506ms step_avg:42.38ms
step:886/1900 train_time:37568ms step_avg:42.40ms
step:887/1900 train_time:37629ms step_avg:42.42ms
step:888/1900 train_time:37690ms step_avg:42.44ms
step:889/1900 train_time:37753ms step_avg:42.47ms
step:890/1900 train_time:37815ms step_avg:42.49ms
step:891/1900 train_time:37877ms step_avg:42.51ms
step:892/1900 train_time:37938ms step_avg:42.53ms
step:893/1900 train_time:38001ms step_avg:42.55ms
step:894/1900 train_time:38062ms step_avg:42.57ms
step:895/1900 train_time:38124ms step_avg:42.60ms
step:896/1900 train_time:38184ms step_avg:42.62ms
step:897/1900 train_time:38246ms step_avg:42.64ms
step:898/1900 train_time:38307ms step_avg:42.66ms
step:899/1900 train_time:38369ms step_avg:42.68ms
step:900/1900 train_time:38429ms step_avg:42.70ms
step:901/1900 train_time:38491ms step_avg:42.72ms
step:902/1900 train_time:38553ms step_avg:42.74ms
step:903/1900 train_time:38614ms step_avg:42.76ms
step:904/1900 train_time:38675ms step_avg:42.78ms
step:905/1900 train_time:38738ms step_avg:42.80ms
step:906/1900 train_time:38799ms step_avg:42.82ms
step:907/1900 train_time:38861ms step_avg:42.85ms
step:908/1900 train_time:38922ms step_avg:42.87ms
step:909/1900 train_time:38984ms step_avg:42.89ms
step:910/1900 train_time:39044ms step_avg:42.91ms
step:911/1900 train_time:39106ms step_avg:42.93ms
step:912/1900 train_time:39167ms step_avg:42.95ms
step:913/1900 train_time:39229ms step_avg:42.97ms
step:914/1900 train_time:39289ms step_avg:42.99ms
step:915/1900 train_time:39352ms step_avg:43.01ms
step:916/1900 train_time:39413ms step_avg:43.03ms
step:917/1900 train_time:39475ms step_avg:43.05ms
step:918/1900 train_time:39535ms step_avg:43.07ms
step:919/1900 train_time:39598ms step_avg:43.09ms
step:920/1900 train_time:39659ms step_avg:43.11ms
step:921/1900 train_time:39721ms step_avg:43.13ms
step:922/1900 train_time:39782ms step_avg:43.15ms
step:923/1900 train_time:39843ms step_avg:43.17ms
step:924/1900 train_time:39904ms step_avg:43.19ms
step:925/1900 train_time:39965ms step_avg:43.21ms
step:926/1900 train_time:40026ms step_avg:43.22ms
step:927/1900 train_time:40088ms step_avg:43.25ms
step:928/1900 train_time:40149ms step_avg:43.26ms
step:929/1900 train_time:40211ms step_avg:43.28ms
step:930/1900 train_time:40272ms step_avg:43.30ms
step:931/1900 train_time:40334ms step_avg:43.32ms
step:932/1900 train_time:40395ms step_avg:43.34ms
step:933/1900 train_time:40457ms step_avg:43.36ms
step:934/1900 train_time:40518ms step_avg:43.38ms
step:935/1900 train_time:40580ms step_avg:43.40ms
step:936/1900 train_time:40642ms step_avg:43.42ms
step:937/1900 train_time:40704ms step_avg:43.44ms
step:938/1900 train_time:40764ms step_avg:43.46ms
step:939/1900 train_time:40826ms step_avg:43.48ms
step:940/1900 train_time:40887ms step_avg:43.50ms
step:941/1900 train_time:40948ms step_avg:43.52ms
step:942/1900 train_time:41009ms step_avg:43.53ms
step:943/1900 train_time:41071ms step_avg:43.55ms
step:944/1900 train_time:41133ms step_avg:43.57ms
step:945/1900 train_time:41195ms step_avg:43.59ms
step:946/1900 train_time:41256ms step_avg:43.61ms
step:947/1900 train_time:41317ms step_avg:43.63ms
step:948/1900 train_time:41378ms step_avg:43.65ms
step:949/1900 train_time:41440ms step_avg:43.67ms
step:950/1900 train_time:41501ms step_avg:43.69ms
step:951/1900 train_time:41563ms step_avg:43.70ms
step:952/1900 train_time:41624ms step_avg:43.72ms
step:953/1900 train_time:41686ms step_avg:43.74ms
step:954/1900 train_time:41748ms step_avg:43.76ms
step:955/1900 train_time:41809ms step_avg:43.78ms
step:956/1900 train_time:41870ms step_avg:43.80ms
step:957/1900 train_time:41932ms step_avg:43.82ms
step:958/1900 train_time:41993ms step_avg:43.83ms
step:959/1900 train_time:42055ms step_avg:43.85ms
step:960/1900 train_time:42117ms step_avg:43.87ms
step:961/1900 train_time:42179ms step_avg:43.89ms
step:962/1900 train_time:42240ms step_avg:43.91ms
step:963/1900 train_time:42301ms step_avg:43.93ms
step:964/1900 train_time:42362ms step_avg:43.94ms
step:965/1900 train_time:42424ms step_avg:43.96ms
step:966/1900 train_time:42485ms step_avg:43.98ms
step:967/1900 train_time:42546ms step_avg:44.00ms
step:968/1900 train_time:42608ms step_avg:44.02ms
step:969/1900 train_time:42670ms step_avg:44.03ms
step:970/1900 train_time:42731ms step_avg:44.05ms
step:971/1900 train_time:42793ms step_avg:44.07ms
step:972/1900 train_time:42855ms step_avg:44.09ms
step:973/1900 train_time:42917ms step_avg:44.11ms
step:974/1900 train_time:42978ms step_avg:44.13ms
step:975/1900 train_time:43040ms step_avg:44.14ms
step:976/1900 train_time:43102ms step_avg:44.16ms
step:977/1900 train_time:43164ms step_avg:44.18ms
step:978/1900 train_time:43224ms step_avg:44.20ms
step:979/1900 train_time:43286ms step_avg:44.21ms
step:980/1900 train_time:43347ms step_avg:44.23ms
step:981/1900 train_time:43409ms step_avg:44.25ms
step:982/1900 train_time:43470ms step_avg:44.27ms
step:983/1900 train_time:43532ms step_avg:44.29ms
step:984/1900 train_time:43594ms step_avg:44.30ms
step:985/1900 train_time:43655ms step_avg:44.32ms
step:986/1900 train_time:43717ms step_avg:44.34ms
step:987/1900 train_time:43779ms step_avg:44.36ms
step:988/1900 train_time:43840ms step_avg:44.37ms
step:989/1900 train_time:43902ms step_avg:44.39ms
step:990/1900 train_time:43963ms step_avg:44.41ms
step:991/1900 train_time:44024ms step_avg:44.42ms
step:992/1900 train_time:44085ms step_avg:44.44ms
step:993/1900 train_time:44147ms step_avg:44.46ms
step:994/1900 train_time:44208ms step_avg:44.47ms
step:995/1900 train_time:44270ms step_avg:44.49ms
step:996/1900 train_time:44331ms step_avg:44.51ms
step:997/1900 train_time:44393ms step_avg:44.53ms
step:998/1900 train_time:44454ms step_avg:44.54ms
step:999/1900 train_time:44516ms step_avg:44.56ms
step:1000/1900 train_time:44577ms step_avg:44.58ms
step:1000/1900 val_loss:3.7886 train_time:44641ms step_avg:44.64ms
step:1001/1900 train_time:44662ms step_avg:44.62ms
step:1002/1900 train_time:44703ms step_avg:44.61ms
step:1003/1900 train_time:44767ms step_avg:44.63ms
step:1004/1900 train_time:44830ms step_avg:44.65ms
step:1005/1900 train_time:44893ms step_avg:44.67ms
step:1006/1900 train_time:44954ms step_avg:44.69ms
step:1007/1900 train_time:45015ms step_avg:44.70ms
step:1008/1900 train_time:45076ms step_avg:44.72ms
step:1009/1900 train_time:45137ms step_avg:44.73ms
step:1010/1900 train_time:45198ms step_avg:44.75ms
step:1011/1900 train_time:45260ms step_avg:44.77ms
step:1012/1900 train_time:45321ms step_avg:44.78ms
step:1013/1900 train_time:45382ms step_avg:44.80ms
step:1014/1900 train_time:45442ms step_avg:44.81ms
step:1015/1900 train_time:45503ms step_avg:44.83ms
step:1016/1900 train_time:45565ms step_avg:44.85ms
step:1017/1900 train_time:45628ms step_avg:44.86ms
step:1018/1900 train_time:45689ms step_avg:44.88ms
step:1019/1900 train_time:45753ms step_avg:44.90ms
step:1020/1900 train_time:45815ms step_avg:44.92ms
step:1021/1900 train_time:45877ms step_avg:44.93ms
step:1022/1900 train_time:45939ms step_avg:44.95ms
step:1023/1900 train_time:46001ms step_avg:44.97ms
step:1024/1900 train_time:46062ms step_avg:44.98ms
step:1025/1900 train_time:46124ms step_avg:45.00ms
step:1026/1900 train_time:46184ms step_avg:45.01ms
step:1027/1900 train_time:46246ms step_avg:45.03ms
step:1028/1900 train_time:46307ms step_avg:45.05ms
step:1029/1900 train_time:46368ms step_avg:45.06ms
step:1030/1900 train_time:46429ms step_avg:45.08ms
step:1031/1900 train_time:46491ms step_avg:45.09ms
step:1032/1900 train_time:46552ms step_avg:45.11ms
step:1033/1900 train_time:46614ms step_avg:45.12ms
step:1034/1900 train_time:46676ms step_avg:45.14ms
step:1035/1900 train_time:46738ms step_avg:45.16ms
step:1036/1900 train_time:46800ms step_avg:45.17ms
step:1037/1900 train_time:46862ms step_avg:45.19ms
step:1038/1900 train_time:46924ms step_avg:45.21ms
step:1039/1900 train_time:46986ms step_avg:45.22ms
step:1040/1900 train_time:47047ms step_avg:45.24ms
step:1041/1900 train_time:47109ms step_avg:45.25ms
step:1042/1900 train_time:47169ms step_avg:45.27ms
step:1043/1900 train_time:47231ms step_avg:45.28ms
step:1044/1900 train_time:47292ms step_avg:45.30ms
step:1045/1900 train_time:47353ms step_avg:45.31ms
step:1046/1900 train_time:47413ms step_avg:45.33ms
step:1047/1900 train_time:47475ms step_avg:45.34ms
step:1048/1900 train_time:47535ms step_avg:45.36ms
step:1049/1900 train_time:47598ms step_avg:45.37ms
step:1050/1900 train_time:47659ms step_avg:45.39ms
step:1051/1900 train_time:47721ms step_avg:45.41ms
step:1052/1900 train_time:47783ms step_avg:45.42ms
step:1053/1900 train_time:47845ms step_avg:45.44ms
step:1054/1900 train_time:47906ms step_avg:45.45ms
step:1055/1900 train_time:47968ms step_avg:45.47ms
step:1056/1900 train_time:48029ms step_avg:45.48ms
step:1057/1900 train_time:48091ms step_avg:45.50ms
step:1058/1900 train_time:48151ms step_avg:45.51ms
step:1059/1900 train_time:48213ms step_avg:45.53ms
step:1060/1900 train_time:48274ms step_avg:45.54ms
step:1061/1900 train_time:48335ms step_avg:45.56ms
step:1062/1900 train_time:48395ms step_avg:45.57ms
step:1063/1900 train_time:48457ms step_avg:45.59ms
step:1064/1900 train_time:48518ms step_avg:45.60ms
step:1065/1900 train_time:48580ms step_avg:45.61ms
step:1066/1900 train_time:48641ms step_avg:45.63ms
step:1067/1900 train_time:48703ms step_avg:45.64ms
step:1068/1900 train_time:48764ms step_avg:45.66ms
step:1069/1900 train_time:48827ms step_avg:45.68ms
step:1070/1900 train_time:48888ms step_avg:45.69ms
step:1071/1900 train_time:48950ms step_avg:45.71ms
step:1072/1900 train_time:49011ms step_avg:45.72ms
step:1073/1900 train_time:49074ms step_avg:45.74ms
step:1074/1900 train_time:49135ms step_avg:45.75ms
step:1075/1900 train_time:49197ms step_avg:45.76ms
step:1076/1900 train_time:49258ms step_avg:45.78ms
step:1077/1900 train_time:49320ms step_avg:45.79ms
step:1078/1900 train_time:49381ms step_avg:45.81ms
step:1079/1900 train_time:49442ms step_avg:45.82ms
step:1080/1900 train_time:49503ms step_avg:45.84ms
step:1081/1900 train_time:49565ms step_avg:45.85ms
step:1082/1900 train_time:49626ms step_avg:45.86ms
step:1083/1900 train_time:49687ms step_avg:45.88ms
step:1084/1900 train_time:49748ms step_avg:45.89ms
step:1085/1900 train_time:49810ms step_avg:45.91ms
step:1086/1900 train_time:49872ms step_avg:45.92ms
step:1087/1900 train_time:49934ms step_avg:45.94ms
step:1088/1900 train_time:49995ms step_avg:45.95ms
step:1089/1900 train_time:50057ms step_avg:45.97ms
step:1090/1900 train_time:50119ms step_avg:45.98ms
step:1091/1900 train_time:50181ms step_avg:46.00ms
step:1092/1900 train_time:50242ms step_avg:46.01ms
step:1093/1900 train_time:50304ms step_avg:46.02ms
step:1094/1900 train_time:50365ms step_avg:46.04ms
step:1095/1900 train_time:50427ms step_avg:46.05ms
step:1096/1900 train_time:50487ms step_avg:46.07ms
step:1097/1900 train_time:50549ms step_avg:46.08ms
step:1098/1900 train_time:50610ms step_avg:46.09ms
step:1099/1900 train_time:50672ms step_avg:46.11ms
step:1100/1900 train_time:50732ms step_avg:46.12ms
step:1101/1900 train_time:50794ms step_avg:46.13ms
step:1102/1900 train_time:50855ms step_avg:46.15ms
step:1103/1900 train_time:50917ms step_avg:46.16ms
step:1104/1900 train_time:50978ms step_avg:46.18ms
step:1105/1900 train_time:51041ms step_avg:46.19ms
step:1106/1900 train_time:51102ms step_avg:46.20ms
step:1107/1900 train_time:51164ms step_avg:46.22ms
step:1108/1900 train_time:51225ms step_avg:46.23ms
step:1109/1900 train_time:51287ms step_avg:46.25ms
step:1110/1900 train_time:51348ms step_avg:46.26ms
step:1111/1900 train_time:51410ms step_avg:46.27ms
step:1112/1900 train_time:51471ms step_avg:46.29ms
step:1113/1900 train_time:51533ms step_avg:46.30ms
step:1114/1900 train_time:51593ms step_avg:46.31ms
step:1115/1900 train_time:51656ms step_avg:46.33ms
step:1116/1900 train_time:51716ms step_avg:46.34ms
step:1117/1900 train_time:51778ms step_avg:46.35ms
step:1118/1900 train_time:51839ms step_avg:46.37ms
step:1119/1900 train_time:51901ms step_avg:46.38ms
step:1120/1900 train_time:51963ms step_avg:46.40ms
step:1121/1900 train_time:52025ms step_avg:46.41ms
step:1122/1900 train_time:52086ms step_avg:46.42ms
step:1123/1900 train_time:52147ms step_avg:46.44ms
step:1124/1900 train_time:52208ms step_avg:46.45ms
step:1125/1900 train_time:52270ms step_avg:46.46ms
step:1126/1900 train_time:52331ms step_avg:46.47ms
step:1127/1900 train_time:52392ms step_avg:46.49ms
step:1128/1900 train_time:52454ms step_avg:46.50ms
step:1129/1900 train_time:52515ms step_avg:46.52ms
step:1130/1900 train_time:52576ms step_avg:46.53ms
step:1131/1900 train_time:52638ms step_avg:46.54ms
step:1132/1900 train_time:52699ms step_avg:46.55ms
step:1133/1900 train_time:52761ms step_avg:46.57ms
step:1134/1900 train_time:52822ms step_avg:46.58ms
step:1135/1900 train_time:52884ms step_avg:46.59ms
step:1136/1900 train_time:52945ms step_avg:46.61ms
step:1137/1900 train_time:53007ms step_avg:46.62ms
step:1138/1900 train_time:53068ms step_avg:46.63ms
step:1139/1900 train_time:53129ms step_avg:46.65ms
step:1140/1900 train_time:53190ms step_avg:46.66ms
step:1141/1900 train_time:53252ms step_avg:46.67ms
step:1142/1900 train_time:53312ms step_avg:46.68ms
step:1143/1900 train_time:53375ms step_avg:46.70ms
step:1144/1900 train_time:53436ms step_avg:46.71ms
step:1145/1900 train_time:53498ms step_avg:46.72ms
step:1146/1900 train_time:53559ms step_avg:46.74ms
step:1147/1900 train_time:53621ms step_avg:46.75ms
step:1148/1900 train_time:53681ms step_avg:46.76ms
step:1149/1900 train_time:53744ms step_avg:46.77ms
step:1150/1900 train_time:53805ms step_avg:46.79ms
step:1151/1900 train_time:53866ms step_avg:46.80ms
step:1152/1900 train_time:53928ms step_avg:46.81ms
step:1153/1900 train_time:53989ms step_avg:46.83ms
step:1154/1900 train_time:54050ms step_avg:46.84ms
step:1155/1900 train_time:54112ms step_avg:46.85ms
step:1156/1900 train_time:54173ms step_avg:46.86ms
step:1157/1900 train_time:54235ms step_avg:46.88ms
step:1158/1900 train_time:54295ms step_avg:46.89ms
step:1159/1900 train_time:54357ms step_avg:46.90ms
step:1160/1900 train_time:54418ms step_avg:46.91ms
step:1161/1900 train_time:54480ms step_avg:46.93ms
step:1162/1900 train_time:54542ms step_avg:46.94ms
step:1163/1900 train_time:54604ms step_avg:46.95ms
step:1164/1900 train_time:54665ms step_avg:46.96ms
step:1165/1900 train_time:54726ms step_avg:46.98ms
step:1166/1900 train_time:54788ms step_avg:46.99ms
step:1167/1900 train_time:54849ms step_avg:47.00ms
step:1168/1900 train_time:54911ms step_avg:47.01ms
step:1169/1900 train_time:54973ms step_avg:47.03ms
step:1170/1900 train_time:55034ms step_avg:47.04ms
step:1171/1900 train_time:55096ms step_avg:47.05ms
step:1172/1900 train_time:55157ms step_avg:47.06ms
step:1173/1900 train_time:55219ms step_avg:47.08ms
step:1174/1900 train_time:55281ms step_avg:47.09ms
step:1175/1900 train_time:55342ms step_avg:47.10ms
step:1176/1900 train_time:55404ms step_avg:47.11ms
step:1177/1900 train_time:55466ms step_avg:47.12ms
step:1178/1900 train_time:55527ms step_avg:47.14ms
step:1179/1900 train_time:55588ms step_avg:47.15ms
step:1180/1900 train_time:55649ms step_avg:47.16ms
step:1181/1900 train_time:55711ms step_avg:47.17ms
step:1182/1900 train_time:55772ms step_avg:47.18ms
step:1183/1900 train_time:55834ms step_avg:47.20ms
step:1184/1900 train_time:55895ms step_avg:47.21ms
step:1185/1900 train_time:55957ms step_avg:47.22ms
step:1186/1900 train_time:56018ms step_avg:47.23ms
step:1187/1900 train_time:56080ms step_avg:47.25ms
step:1188/1900 train_time:56141ms step_avg:47.26ms
step:1189/1900 train_time:56203ms step_avg:47.27ms
step:1190/1900 train_time:56264ms step_avg:47.28ms
step:1191/1900 train_time:56325ms step_avg:47.29ms
step:1192/1900 train_time:56387ms step_avg:47.30ms
step:1193/1900 train_time:56448ms step_avg:47.32ms
step:1194/1900 train_time:56509ms step_avg:47.33ms
step:1195/1900 train_time:56571ms step_avg:47.34ms
step:1196/1900 train_time:56631ms step_avg:47.35ms
step:1197/1900 train_time:56693ms step_avg:47.36ms
step:1198/1900 train_time:56754ms step_avg:47.37ms
step:1199/1900 train_time:56815ms step_avg:47.39ms
step:1200/1900 train_time:56876ms step_avg:47.40ms
step:1201/1900 train_time:56938ms step_avg:47.41ms
step:1202/1900 train_time:57000ms step_avg:47.42ms
step:1203/1900 train_time:57062ms step_avg:47.43ms
step:1204/1900 train_time:57123ms step_avg:47.44ms
step:1205/1900 train_time:57185ms step_avg:47.46ms
step:1206/1900 train_time:57247ms step_avg:47.47ms
step:1207/1900 train_time:57308ms step_avg:47.48ms
step:1208/1900 train_time:57369ms step_avg:47.49ms
step:1209/1900 train_time:57431ms step_avg:47.50ms
step:1210/1900 train_time:57491ms step_avg:47.51ms
step:1211/1900 train_time:57553ms step_avg:47.53ms
step:1212/1900 train_time:57614ms step_avg:47.54ms
step:1213/1900 train_time:57676ms step_avg:47.55ms
step:1214/1900 train_time:57737ms step_avg:47.56ms
step:1215/1900 train_time:57798ms step_avg:47.57ms
step:1216/1900 train_time:57859ms step_avg:47.58ms
step:1217/1900 train_time:57922ms step_avg:47.59ms
step:1218/1900 train_time:57983ms step_avg:47.60ms
step:1219/1900 train_time:58045ms step_avg:47.62ms
step:1220/1900 train_time:58105ms step_avg:47.63ms
step:1221/1900 train_time:58167ms step_avg:47.64ms
step:1222/1900 train_time:58228ms step_avg:47.65ms
step:1223/1900 train_time:58290ms step_avg:47.66ms
step:1224/1900 train_time:58351ms step_avg:47.67ms
step:1225/1900 train_time:58413ms step_avg:47.68ms
step:1226/1900 train_time:58474ms step_avg:47.70ms
step:1227/1900 train_time:58535ms step_avg:47.71ms
step:1228/1900 train_time:58596ms step_avg:47.72ms
step:1229/1900 train_time:58658ms step_avg:47.73ms
step:1230/1900 train_time:58719ms step_avg:47.74ms
step:1231/1900 train_time:58781ms step_avg:47.75ms
step:1232/1900 train_time:58842ms step_avg:47.76ms
step:1233/1900 train_time:58904ms step_avg:47.77ms
step:1234/1900 train_time:58965ms step_avg:47.78ms
step:1235/1900 train_time:59026ms step_avg:47.79ms
step:1236/1900 train_time:59087ms step_avg:47.81ms
step:1237/1900 train_time:59149ms step_avg:47.82ms
step:1238/1900 train_time:59210ms step_avg:47.83ms
step:1239/1900 train_time:59272ms step_avg:47.84ms
step:1240/1900 train_time:59333ms step_avg:47.85ms
step:1241/1900 train_time:59395ms step_avg:47.86ms
step:1242/1900 train_time:59483ms step_avg:47.89ms
step:1243/1900 train_time:59572ms step_avg:47.93ms
step:1244/1900 train_time:59659ms step_avg:47.96ms
step:1245/1900 train_time:59748ms step_avg:47.99ms
step:1246/1900 train_time:59836ms step_avg:48.02ms
step:1247/1900 train_time:59924ms step_avg:48.05ms
step:1248/1900 train_time:60013ms step_avg:48.09ms
step:1249/1900 train_time:60101ms step_avg:48.12ms
step:1250/1900 train_time:60189ms step_avg:48.15ms
step:1250/1900 val_loss:3.5481 train_time:60281ms step_avg:48.22ms
step:1251/1900 train_time:60301ms step_avg:48.20ms
step:1252/1900 train_time:60370ms step_avg:48.22ms
step:1253/1900 train_time:60461ms step_avg:48.25ms
step:1254/1900 train_time:60549ms step_avg:48.28ms
step:1255/1900 train_time:60638ms step_avg:48.32ms
step:1256/1900 train_time:60724ms step_avg:48.35ms
step:1257/1900 train_time:60812ms step_avg:48.38ms
step:1258/1900 train_time:60899ms step_avg:48.41ms
step:1259/1900 train_time:60986ms step_avg:48.44ms
step:1260/1900 train_time:61074ms step_avg:48.47ms
step:1261/1900 train_time:61161ms step_avg:48.50ms
step:1262/1900 train_time:61250ms step_avg:48.53ms
step:1263/1900 train_time:61339ms step_avg:48.57ms
step:1264/1900 train_time:61429ms step_avg:48.60ms
step:1265/1900 train_time:61518ms step_avg:48.63ms
step:1266/1900 train_time:61605ms step_avg:48.66ms
step:1267/1900 train_time:61693ms step_avg:48.69ms
step:1268/1900 train_time:61780ms step_avg:48.72ms
step:1269/1900 train_time:61869ms step_avg:48.75ms
step:1270/1900 train_time:61955ms step_avg:48.78ms
step:1271/1900 train_time:62044ms step_avg:48.81ms
step:1272/1900 train_time:62131ms step_avg:48.85ms
step:1273/1900 train_time:62219ms step_avg:48.88ms
step:1274/1900 train_time:62307ms step_avg:48.91ms
step:1275/1900 train_time:62397ms step_avg:48.94ms
step:1276/1900 train_time:62485ms step_avg:48.97ms
step:1277/1900 train_time:62575ms step_avg:49.00ms
step:1278/1900 train_time:62662ms step_avg:49.03ms
step:1279/1900 train_time:62751ms step_avg:49.06ms
step:1280/1900 train_time:62838ms step_avg:49.09ms
step:1281/1900 train_time:62925ms step_avg:49.12ms
step:1282/1900 train_time:63012ms step_avg:49.15ms
step:1283/1900 train_time:63100ms step_avg:49.18ms
step:1284/1900 train_time:63188ms step_avg:49.21ms
step:1285/1900 train_time:63277ms step_avg:49.24ms
step:1286/1900 train_time:63365ms step_avg:49.27ms
step:1287/1900 train_time:63454ms step_avg:49.30ms
step:1288/1900 train_time:63543ms step_avg:49.33ms
step:1289/1900 train_time:63632ms step_avg:49.37ms
step:1290/1900 train_time:63719ms step_avg:49.39ms
step:1291/1900 train_time:63807ms step_avg:49.42ms
step:1292/1900 train_time:63894ms step_avg:49.45ms
step:1293/1900 train_time:63982ms step_avg:49.48ms
step:1294/1900 train_time:64069ms step_avg:49.51ms
step:1295/1900 train_time:64157ms step_avg:49.54ms
step:1296/1900 train_time:64245ms step_avg:49.57ms
step:1297/1900 train_time:64334ms step_avg:49.60ms
step:1298/1900 train_time:64422ms step_avg:49.63ms
step:1299/1900 train_time:64511ms step_avg:49.66ms
step:1300/1900 train_time:64598ms step_avg:49.69ms
step:1301/1900 train_time:64688ms step_avg:49.72ms
step:1302/1900 train_time:64775ms step_avg:49.75ms
step:1303/1900 train_time:64863ms step_avg:49.78ms
step:1304/1900 train_time:64951ms step_avg:49.81ms
step:1305/1900 train_time:65039ms step_avg:49.84ms
step:1306/1900 train_time:65127ms step_avg:49.87ms
step:1307/1900 train_time:65216ms step_avg:49.90ms
step:1308/1900 train_time:65304ms step_avg:49.93ms
step:1309/1900 train_time:65394ms step_avg:49.96ms
step:1310/1900 train_time:65482ms step_avg:49.99ms
step:1311/1900 train_time:65572ms step_avg:50.02ms
step:1312/1900 train_time:65659ms step_avg:50.05ms
step:1313/1900 train_time:65748ms step_avg:50.07ms
step:1314/1900 train_time:65836ms step_avg:50.10ms
step:1315/1900 train_time:65924ms step_avg:50.13ms
step:1316/1900 train_time:66011ms step_avg:50.16ms
step:1317/1900 train_time:66099ms step_avg:50.19ms
step:1318/1900 train_time:66187ms step_avg:50.22ms
step:1319/1900 train_time:66276ms step_avg:50.25ms
step:1320/1900 train_time:66364ms step_avg:50.28ms
step:1321/1900 train_time:66453ms step_avg:50.30ms
step:1322/1900 train_time:66541ms step_avg:50.33ms
step:1323/1900 train_time:66629ms step_avg:50.36ms
step:1324/1900 train_time:66717ms step_avg:50.39ms
step:1325/1900 train_time:66806ms step_avg:50.42ms
step:1326/1900 train_time:66893ms step_avg:50.45ms
step:1327/1900 train_time:66981ms step_avg:50.48ms
step:1328/1900 train_time:67068ms step_avg:50.50ms
step:1329/1900 train_time:67157ms step_avg:50.53ms
step:1330/1900 train_time:67245ms step_avg:50.56ms
step:1331/1900 train_time:67334ms step_avg:50.59ms
step:1332/1900 train_time:67421ms step_avg:50.62ms
step:1333/1900 train_time:67510ms step_avg:50.65ms
step:1334/1900 train_time:67598ms step_avg:50.67ms
step:1335/1900 train_time:67687ms step_avg:50.70ms
step:1336/1900 train_time:67775ms step_avg:50.73ms
step:1337/1900 train_time:67863ms step_avg:50.76ms
step:1338/1900 train_time:67951ms step_avg:50.79ms
step:1339/1900 train_time:68039ms step_avg:50.81ms
step:1340/1900 train_time:68126ms step_avg:50.84ms
step:1341/1900 train_time:68215ms step_avg:50.87ms
step:1342/1900 train_time:68303ms step_avg:50.90ms
step:1343/1900 train_time:68391ms step_avg:50.92ms
step:1344/1900 train_time:68479ms step_avg:50.95ms
step:1345/1900 train_time:68568ms step_avg:50.98ms
step:1346/1900 train_time:68656ms step_avg:51.01ms
step:1347/1900 train_time:68744ms step_avg:51.03ms
step:1348/1900 train_time:68832ms step_avg:51.06ms
step:1349/1900 train_time:68920ms step_avg:51.09ms
step:1350/1900 train_time:69008ms step_avg:51.12ms
step:1351/1900 train_time:69097ms step_avg:51.14ms
step:1352/1900 train_time:69185ms step_avg:51.17ms
step:1353/1900 train_time:69274ms step_avg:51.20ms
step:1354/1900 train_time:69362ms step_avg:51.23ms
step:1355/1900 train_time:69450ms step_avg:51.25ms
step:1356/1900 train_time:69538ms step_avg:51.28ms
step:1357/1900 train_time:69627ms step_avg:51.31ms
step:1358/1900 train_time:69714ms step_avg:51.34ms
step:1359/1900 train_time:69803ms step_avg:51.36ms
step:1360/1900 train_time:69889ms step_avg:51.39ms
step:1361/1900 train_time:69978ms step_avg:51.42ms
step:1362/1900 train_time:70066ms step_avg:51.44ms
step:1363/1900 train_time:70155ms step_avg:51.47ms
step:1364/1900 train_time:70244ms step_avg:51.50ms
step:1365/1900 train_time:70333ms step_avg:51.53ms
step:1366/1900 train_time:70421ms step_avg:51.55ms
step:1367/1900 train_time:70510ms step_avg:51.58ms
step:1368/1900 train_time:70597ms step_avg:51.61ms
step:1369/1900 train_time:70685ms step_avg:51.63ms
step:1370/1900 train_time:70773ms step_avg:51.66ms
step:1371/1900 train_time:70861ms step_avg:51.69ms
step:1372/1900 train_time:70948ms step_avg:51.71ms
step:1373/1900 train_time:71037ms step_avg:51.74ms
step:1374/1900 train_time:71125ms step_avg:51.76ms
step:1375/1900 train_time:71214ms step_avg:51.79ms
step:1376/1900 train_time:71302ms step_avg:51.82ms
step:1377/1900 train_time:71390ms step_avg:51.84ms
step:1378/1900 train_time:71478ms step_avg:51.87ms
step:1379/1900 train_time:71567ms step_avg:51.90ms
step:1380/1900 train_time:71655ms step_avg:51.92ms
step:1381/1900 train_time:71743ms step_avg:51.95ms
step:1382/1900 train_time:71832ms step_avg:51.98ms
step:1383/1900 train_time:71920ms step_avg:52.00ms
step:1384/1900 train_time:72007ms step_avg:52.03ms
step:1385/1900 train_time:72095ms step_avg:52.05ms
step:1386/1900 train_time:72183ms step_avg:52.08ms
step:1387/1900 train_time:72273ms step_avg:52.11ms
step:1388/1900 train_time:72361ms step_avg:52.13ms
step:1389/1900 train_time:72449ms step_avg:52.16ms
step:1390/1900 train_time:72537ms step_avg:52.18ms
step:1391/1900 train_time:72625ms step_avg:52.21ms
step:1392/1900 train_time:72713ms step_avg:52.24ms
step:1393/1900 train_time:72801ms step_avg:52.26ms
step:1394/1900 train_time:72889ms step_avg:52.29ms
step:1395/1900 train_time:72977ms step_avg:52.31ms
step:1396/1900 train_time:73065ms step_avg:52.34ms
step:1397/1900 train_time:73153ms step_avg:52.36ms
step:1398/1900 train_time:73241ms step_avg:52.39ms
step:1399/1900 train_time:73330ms step_avg:52.42ms
step:1400/1900 train_time:73417ms step_avg:52.44ms
step:1401/1900 train_time:73505ms step_avg:52.47ms
step:1402/1900 train_time:73594ms step_avg:52.49ms
step:1403/1900 train_time:73681ms step_avg:52.52ms
step:1404/1900 train_time:73770ms step_avg:52.54ms
step:1405/1900 train_time:73858ms step_avg:52.57ms
step:1406/1900 train_time:73945ms step_avg:52.59ms
step:1407/1900 train_time:74034ms step_avg:52.62ms
step:1408/1900 train_time:74122ms step_avg:52.64ms
step:1409/1900 train_time:74210ms step_avg:52.67ms
step:1410/1900 train_time:74297ms step_avg:52.69ms
step:1411/1900 train_time:74386ms step_avg:52.72ms
step:1412/1900 train_time:74474ms step_avg:52.74ms
step:1413/1900 train_time:74563ms step_avg:52.77ms
step:1414/1900 train_time:74650ms step_avg:52.79ms
step:1415/1900 train_time:74738ms step_avg:52.82ms
step:1416/1900 train_time:74827ms step_avg:52.84ms
step:1417/1900 train_time:74916ms step_avg:52.87ms
step:1418/1900 train_time:75004ms step_avg:52.89ms
step:1419/1900 train_time:75093ms step_avg:52.92ms
step:1420/1900 train_time:75181ms step_avg:52.94ms
step:1421/1900 train_time:75270ms step_avg:52.97ms
step:1422/1900 train_time:75357ms step_avg:52.99ms
step:1423/1900 train_time:75445ms step_avg:53.02ms
step:1424/1900 train_time:75533ms step_avg:53.04ms
step:1425/1900 train_time:75622ms step_avg:53.07ms
step:1426/1900 train_time:75710ms step_avg:53.09ms
step:1427/1900 train_time:75798ms step_avg:53.12ms
step:1428/1900 train_time:75885ms step_avg:53.14ms
step:1429/1900 train_time:75974ms step_avg:53.17ms
step:1430/1900 train_time:76061ms step_avg:53.19ms
step:1431/1900 train_time:76151ms step_avg:53.21ms
step:1432/1900 train_time:76238ms step_avg:53.24ms
step:1433/1900 train_time:76326ms step_avg:53.26ms
step:1434/1900 train_time:76414ms step_avg:53.29ms
step:1435/1900 train_time:76503ms step_avg:53.31ms
step:1436/1900 train_time:76591ms step_avg:53.34ms
step:1437/1900 train_time:76679ms step_avg:53.36ms
step:1438/1900 train_time:76767ms step_avg:53.38ms
step:1439/1900 train_time:76856ms step_avg:53.41ms
step:1440/1900 train_time:76944ms step_avg:53.43ms
step:1441/1900 train_time:77033ms step_avg:53.46ms
step:1442/1900 train_time:77121ms step_avg:53.48ms
step:1443/1900 train_time:77211ms step_avg:53.51ms
step:1444/1900 train_time:77298ms step_avg:53.53ms
step:1445/1900 train_time:77387ms step_avg:53.56ms
step:1446/1900 train_time:77475ms step_avg:53.58ms
step:1447/1900 train_time:77564ms step_avg:53.60ms
step:1448/1900 train_time:77652ms step_avg:53.63ms
step:1449/1900 train_time:77740ms step_avg:53.65ms
step:1450/1900 train_time:77827ms step_avg:53.67ms
step:1451/1900 train_time:77916ms step_avg:53.70ms
step:1452/1900 train_time:78004ms step_avg:53.72ms
step:1453/1900 train_time:78094ms step_avg:53.75ms
step:1454/1900 train_time:78181ms step_avg:53.77ms
step:1455/1900 train_time:78270ms step_avg:53.79ms
step:1456/1900 train_time:78357ms step_avg:53.82ms
step:1457/1900 train_time:78446ms step_avg:53.84ms
step:1458/1900 train_time:78533ms step_avg:53.86ms
step:1459/1900 train_time:78622ms step_avg:53.89ms
step:1460/1900 train_time:78709ms step_avg:53.91ms
step:1461/1900 train_time:78797ms step_avg:53.93ms
step:1462/1900 train_time:78885ms step_avg:53.96ms
step:1463/1900 train_time:78974ms step_avg:53.98ms
step:1464/1900 train_time:79062ms step_avg:54.00ms
step:1465/1900 train_time:79152ms step_avg:54.03ms
step:1466/1900 train_time:79239ms step_avg:54.05ms
step:1467/1900 train_time:79326ms step_avg:54.07ms
step:1468/1900 train_time:79414ms step_avg:54.10ms
step:1469/1900 train_time:79503ms step_avg:54.12ms
step:1470/1900 train_time:79591ms step_avg:54.14ms
step:1471/1900 train_time:79679ms step_avg:54.17ms
step:1472/1900 train_time:79767ms step_avg:54.19ms
step:1473/1900 train_time:79856ms step_avg:54.21ms
step:1474/1900 train_time:79943ms step_avg:54.24ms
step:1475/1900 train_time:80032ms step_avg:54.26ms
step:1476/1900 train_time:80120ms step_avg:54.28ms
step:1477/1900 train_time:80209ms step_avg:54.31ms
step:1478/1900 train_time:80296ms step_avg:54.33ms
step:1479/1900 train_time:80384ms step_avg:54.35ms
step:1480/1900 train_time:80473ms step_avg:54.37ms
step:1481/1900 train_time:80561ms step_avg:54.40ms
step:1482/1900 train_time:80649ms step_avg:54.42ms
step:1483/1900 train_time:80738ms step_avg:54.44ms
step:1484/1900 train_time:80825ms step_avg:54.46ms
step:1485/1900 train_time:80914ms step_avg:54.49ms
step:1486/1900 train_time:81001ms step_avg:54.51ms
step:1487/1900 train_time:81090ms step_avg:54.53ms
step:1488/1900 train_time:81177ms step_avg:54.55ms
step:1489/1900 train_time:81266ms step_avg:54.58ms
step:1490/1900 train_time:81354ms step_avg:54.60ms
step:1491/1900 train_time:81443ms step_avg:54.62ms
step:1492/1900 train_time:81531ms step_avg:54.65ms
step:1493/1900 train_time:81618ms step_avg:54.67ms
step:1494/1900 train_time:81705ms step_avg:54.69ms
step:1495/1900 train_time:81794ms step_avg:54.71ms
step:1496/1900 train_time:81881ms step_avg:54.73ms
step:1497/1900 train_time:81970ms step_avg:54.76ms
step:1498/1900 train_time:82057ms step_avg:54.78ms
step:1499/1900 train_time:82145ms step_avg:54.80ms
step:1500/1900 train_time:82233ms step_avg:54.82ms
step:1500/1900 val_loss:3.4138 train_time:82323ms step_avg:54.88ms
step:1501/1900 train_time:82349ms step_avg:54.86ms
step:1502/1900 train_time:82415ms step_avg:54.87ms
step:1503/1900 train_time:82507ms step_avg:54.89ms
step:1504/1900 train_time:82594ms step_avg:54.92ms
step:1505/1900 train_time:82682ms step_avg:54.94ms
step:1506/1900 train_time:82769ms step_avg:54.96ms
step:1507/1900 train_time:82856ms step_avg:54.98ms
step:1508/1900 train_time:82943ms step_avg:55.00ms
step:1509/1900 train_time:83030ms step_avg:55.02ms
step:1510/1900 train_time:83117ms step_avg:55.04ms
step:1511/1900 train_time:83204ms step_avg:55.07ms
step:1512/1900 train_time:83292ms step_avg:55.09ms
step:1513/1900 train_time:83384ms step_avg:55.11ms
step:1514/1900 train_time:83474ms step_avg:55.13ms
step:1515/1900 train_time:83563ms step_avg:55.16ms
step:1516/1900 train_time:83651ms step_avg:55.18ms
step:1517/1900 train_time:83739ms step_avg:55.20ms
step:1518/1900 train_time:83826ms step_avg:55.22ms
step:1519/1900 train_time:83913ms step_avg:55.24ms
step:1520/1900 train_time:84000ms step_avg:55.26ms
step:1521/1900 train_time:84087ms step_avg:55.28ms
step:1522/1900 train_time:84174ms step_avg:55.31ms
step:1523/1900 train_time:84263ms step_avg:55.33ms
step:1524/1900 train_time:84352ms step_avg:55.35ms
step:1525/1900 train_time:84442ms step_avg:55.37ms
step:1526/1900 train_time:84531ms step_avg:55.39ms
step:1527/1900 train_time:84620ms step_avg:55.42ms
step:1528/1900 train_time:84708ms step_avg:55.44ms
step:1529/1900 train_time:84796ms step_avg:55.46ms
step:1530/1900 train_time:84884ms step_avg:55.48ms
step:1531/1900 train_time:84972ms step_avg:55.50ms
step:1532/1900 train_time:85059ms step_avg:55.52ms
step:1533/1900 train_time:85149ms step_avg:55.54ms
step:1534/1900 train_time:85235ms step_avg:55.56ms
step:1535/1900 train_time:85324ms step_avg:55.59ms
step:1536/1900 train_time:85412ms step_avg:55.61ms
step:1537/1900 train_time:85502ms step_avg:55.63ms
step:1538/1900 train_time:85591ms step_avg:55.65ms
step:1539/1900 train_time:85680ms step_avg:55.67ms
step:1540/1900 train_time:85767ms step_avg:55.69ms
step:1541/1900 train_time:85854ms step_avg:55.71ms
step:1542/1900 train_time:85942ms step_avg:55.73ms
step:1543/1900 train_time:86029ms step_avg:55.75ms
step:1544/1900 train_time:86116ms step_avg:55.77ms
step:1545/1900 train_time:86205ms step_avg:55.80ms
step:1546/1900 train_time:86292ms step_avg:55.82ms
step:1547/1900 train_time:86381ms step_avg:55.84ms
step:1548/1900 train_time:86469ms step_avg:55.86ms
step:1549/1900 train_time:86557ms step_avg:55.88ms
step:1550/1900 train_time:86645ms step_avg:55.90ms
step:1551/1900 train_time:86734ms step_avg:55.92ms
step:1552/1900 train_time:86821ms step_avg:55.94ms
step:1553/1900 train_time:86910ms step_avg:55.96ms
step:1554/1900 train_time:86996ms step_avg:55.98ms
step:1555/1900 train_time:87084ms step_avg:56.00ms
step:1556/1900 train_time:87172ms step_avg:56.02ms
step:1557/1900 train_time:87260ms step_avg:56.04ms
step:1558/1900 train_time:87349ms step_avg:56.06ms
step:1559/1900 train_time:87438ms step_avg:56.09ms
step:1560/1900 train_time:87527ms step_avg:56.11ms
step:1561/1900 train_time:87615ms step_avg:56.13ms
step:1562/1900 train_time:87703ms step_avg:56.15ms
step:1563/1900 train_time:87792ms step_avg:56.17ms
step:1564/1900 train_time:87880ms step_avg:56.19ms
step:1565/1900 train_time:87968ms step_avg:56.21ms
step:1566/1900 train_time:88055ms step_avg:56.23ms
step:1567/1900 train_time:88144ms step_avg:56.25ms
step:1568/1900 train_time:88231ms step_avg:56.27ms
step:1569/1900 train_time:88320ms step_avg:56.29ms
step:1570/1900 train_time:88408ms step_avg:56.31ms
step:1571/1900 train_time:88497ms step_avg:56.33ms
step:1572/1900 train_time:88585ms step_avg:56.35ms
step:1573/1900 train_time:88673ms step_avg:56.37ms
step:1574/1900 train_time:88761ms step_avg:56.39ms
step:1575/1900 train_time:88850ms step_avg:56.41ms
step:1576/1900 train_time:88938ms step_avg:56.43ms
step:1577/1900 train_time:89026ms step_avg:56.45ms
step:1578/1900 train_time:89113ms step_avg:56.47ms
step:1579/1900 train_time:89202ms step_avg:56.49ms
step:1580/1900 train_time:89289ms step_avg:56.51ms
step:1581/1900 train_time:89378ms step_avg:56.53ms
step:1582/1900 train_time:89465ms step_avg:56.55ms
step:1583/1900 train_time:89554ms step_avg:56.57ms
step:1584/1900 train_time:89642ms step_avg:56.59ms
step:1585/1900 train_time:89731ms step_avg:56.61ms
step:1586/1900 train_time:89819ms step_avg:56.63ms
step:1587/1900 train_time:89908ms step_avg:56.65ms
step:1588/1900 train_time:89995ms step_avg:56.67ms
step:1589/1900 train_time:90084ms step_avg:56.69ms
step:1590/1900 train_time:90172ms step_avg:56.71ms
step:1591/1900 train_time:90261ms step_avg:56.73ms
step:1592/1900 train_time:90349ms step_avg:56.75ms
step:1593/1900 train_time:90437ms step_avg:56.77ms
step:1594/1900 train_time:90525ms step_avg:56.79ms
step:1595/1900 train_time:90614ms step_avg:56.81ms
step:1596/1900 train_time:90702ms step_avg:56.83ms
step:1597/1900 train_time:90790ms step_avg:56.85ms
step:1598/1900 train_time:90878ms step_avg:56.87ms
step:1599/1900 train_time:90967ms step_avg:56.89ms
step:1600/1900 train_time:91054ms step_avg:56.91ms
step:1601/1900 train_time:91143ms step_avg:56.93ms
step:1602/1900 train_time:91230ms step_avg:56.95ms
step:1603/1900 train_time:91318ms step_avg:56.97ms
step:1604/1900 train_time:91407ms step_avg:56.99ms
step:1605/1900 train_time:91496ms step_avg:57.01ms
step:1606/1900 train_time:91583ms step_avg:57.03ms
step:1607/1900 train_time:91672ms step_avg:57.05ms
step:1608/1900 train_time:91760ms step_avg:57.06ms
step:1609/1900 train_time:91849ms step_avg:57.08ms
step:1610/1900 train_time:91936ms step_avg:57.10ms
step:1611/1900 train_time:92026ms step_avg:57.12ms
step:1612/1900 train_time:92113ms step_avg:57.14ms
step:1613/1900 train_time:92201ms step_avg:57.16ms
step:1614/1900 train_time:92289ms step_avg:57.18ms
step:1615/1900 train_time:92377ms step_avg:57.20ms
step:1616/1900 train_time:92466ms step_avg:57.22ms
step:1617/1900 train_time:92554ms step_avg:57.24ms
step:1618/1900 train_time:92641ms step_avg:57.26ms
step:1619/1900 train_time:92730ms step_avg:57.28ms
step:1620/1900 train_time:92818ms step_avg:57.29ms
step:1621/1900 train_time:92906ms step_avg:57.31ms
step:1622/1900 train_time:92993ms step_avg:57.33ms
step:1623/1900 train_time:93082ms step_avg:57.35ms
step:1624/1900 train_time:93170ms step_avg:57.37ms
step:1625/1900 train_time:93259ms step_avg:57.39ms
step:1626/1900 train_time:93345ms step_avg:57.41ms
step:1627/1900 train_time:93434ms step_avg:57.43ms
step:1628/1900 train_time:93522ms step_avg:57.45ms
step:1629/1900 train_time:93612ms step_avg:57.47ms
step:1630/1900 train_time:93700ms step_avg:57.48ms
step:1631/1900 train_time:93789ms step_avg:57.50ms
step:1632/1900 train_time:93877ms step_avg:57.52ms
step:1633/1900 train_time:93965ms step_avg:57.54ms
step:1634/1900 train_time:94052ms step_avg:57.56ms
step:1635/1900 train_time:94141ms step_avg:57.58ms
step:1636/1900 train_time:94228ms step_avg:57.60ms
step:1637/1900 train_time:94317ms step_avg:57.62ms
step:1638/1900 train_time:94404ms step_avg:57.63ms
step:1639/1900 train_time:94493ms step_avg:57.65ms
step:1640/1900 train_time:94580ms step_avg:57.67ms
step:1641/1900 train_time:94670ms step_avg:57.69ms
step:1642/1900 train_time:94758ms step_avg:57.71ms
step:1643/1900 train_time:94847ms step_avg:57.73ms
step:1644/1900 train_time:94935ms step_avg:57.75ms
step:1645/1900 train_time:95023ms step_avg:57.76ms
step:1646/1900 train_time:95111ms step_avg:57.78ms
step:1647/1900 train_time:95200ms step_avg:57.80ms
step:1648/1900 train_time:95288ms step_avg:57.82ms
step:1649/1900 train_time:95376ms step_avg:57.84ms
step:1650/1900 train_time:95464ms step_avg:57.86ms
step:1651/1900 train_time:95552ms step_avg:57.88ms
step:1652/1900 train_time:95640ms step_avg:57.89ms
step:1653/1900 train_time:95729ms step_avg:57.91ms
step:1654/1900 train_time:95817ms step_avg:57.93ms
step:1655/1900 train_time:95905ms step_avg:57.95ms
step:1656/1900 train_time:95993ms step_avg:57.97ms
step:1657/1900 train_time:96082ms step_avg:57.99ms
step:1658/1900 train_time:96170ms step_avg:58.00ms
step:1659/1900 train_time:96260ms step_avg:58.02ms
step:1660/1900 train_time:96348ms step_avg:58.04ms
step:1661/1900 train_time:96436ms step_avg:58.06ms
step:1662/1900 train_time:96524ms step_avg:58.08ms
step:1663/1900 train_time:96613ms step_avg:58.10ms
step:1664/1900 train_time:96701ms step_avg:58.11ms
step:1665/1900 train_time:96790ms step_avg:58.13ms
step:1666/1900 train_time:96878ms step_avg:58.15ms
step:1667/1900 train_time:96967ms step_avg:58.17ms
step:1668/1900 train_time:97054ms step_avg:58.19ms
step:1669/1900 train_time:97144ms step_avg:58.20ms
step:1670/1900 train_time:97231ms step_avg:58.22ms
step:1671/1900 train_time:97320ms step_avg:58.24ms
step:1672/1900 train_time:97408ms step_avg:58.26ms
step:1673/1900 train_time:97496ms step_avg:58.28ms
step:1674/1900 train_time:97584ms step_avg:58.29ms
step:1675/1900 train_time:97673ms step_avg:58.31ms
step:1676/1900 train_time:97761ms step_avg:58.33ms
step:1677/1900 train_time:97851ms step_avg:58.35ms
step:1678/1900 train_time:97939ms step_avg:58.37ms
step:1679/1900 train_time:98028ms step_avg:58.38ms
step:1680/1900 train_time:98116ms step_avg:58.40ms
step:1681/1900 train_time:98204ms step_avg:58.42ms
step:1682/1900 train_time:98292ms step_avg:58.44ms
step:1683/1900 train_time:98381ms step_avg:58.46ms
step:1684/1900 train_time:98469ms step_avg:58.47ms
step:1685/1900 train_time:98557ms step_avg:58.49ms
step:1686/1900 train_time:98645ms step_avg:58.51ms
step:1687/1900 train_time:98732ms step_avg:58.53ms
step:1688/1900 train_time:98820ms step_avg:58.54ms
step:1689/1900 train_time:98909ms step_avg:58.56ms
step:1690/1900 train_time:98997ms step_avg:58.58ms
step:1691/1900 train_time:99086ms step_avg:58.60ms
step:1692/1900 train_time:99174ms step_avg:58.61ms
step:1693/1900 train_time:99262ms step_avg:58.63ms
step:1694/1900 train_time:99349ms step_avg:58.65ms
step:1695/1900 train_time:99438ms step_avg:58.67ms
step:1696/1900 train_time:99525ms step_avg:58.68ms
step:1697/1900 train_time:99614ms step_avg:58.70ms
step:1698/1900 train_time:99701ms step_avg:58.72ms
step:1699/1900 train_time:99791ms step_avg:58.73ms
step:1700/1900 train_time:99879ms step_avg:58.75ms
step:1701/1900 train_time:99968ms step_avg:58.77ms
step:1702/1900 train_time:100056ms step_avg:58.79ms
step:1703/1900 train_time:100145ms step_avg:58.81ms
step:1704/1900 train_time:100233ms step_avg:58.82ms
step:1705/1900 train_time:100321ms step_avg:58.84ms
step:1706/1900 train_time:100409ms step_avg:58.86ms
step:1707/1900 train_time:100497ms step_avg:58.87ms
step:1708/1900 train_time:100585ms step_avg:58.89ms
step:1709/1900 train_time:100674ms step_avg:58.91ms
step:1710/1900 train_time:100761ms step_avg:58.92ms
step:1711/1900 train_time:100850ms step_avg:58.94ms
step:1712/1900 train_time:100939ms step_avg:58.96ms
step:1713/1900 train_time:101027ms step_avg:58.98ms
step:1714/1900 train_time:101115ms step_avg:58.99ms
step:1715/1900 train_time:101204ms step_avg:59.01ms
step:1716/1900 train_time:101291ms step_avg:59.03ms
step:1717/1900 train_time:101380ms step_avg:59.04ms
step:1718/1900 train_time:101467ms step_avg:59.06ms
step:1719/1900 train_time:101556ms step_avg:59.08ms
step:1720/1900 train_time:101644ms step_avg:59.10ms
step:1721/1900 train_time:101732ms step_avg:59.11ms
step:1722/1900 train_time:101819ms step_avg:59.13ms
step:1723/1900 train_time:101909ms step_avg:59.15ms
step:1724/1900 train_time:101996ms step_avg:59.16ms
step:1725/1900 train_time:102085ms step_avg:59.18ms
step:1726/1900 train_time:102172ms step_avg:59.20ms
step:1727/1900 train_time:102261ms step_avg:59.21ms
step:1728/1900 train_time:102348ms step_avg:59.23ms
step:1729/1900 train_time:102436ms step_avg:59.25ms
step:1730/1900 train_time:102524ms step_avg:59.26ms
step:1731/1900 train_time:102613ms step_avg:59.28ms
step:1732/1900 train_time:102700ms step_avg:59.30ms
step:1733/1900 train_time:102789ms step_avg:59.31ms
step:1734/1900 train_time:102877ms step_avg:59.33ms
step:1735/1900 train_time:102966ms step_avg:59.35ms
step:1736/1900 train_time:103053ms step_avg:59.36ms
step:1737/1900 train_time:103141ms step_avg:59.38ms
step:1738/1900 train_time:103229ms step_avg:59.40ms
step:1739/1900 train_time:103318ms step_avg:59.41ms
step:1740/1900 train_time:103406ms step_avg:59.43ms
step:1741/1900 train_time:103495ms step_avg:59.45ms
step:1742/1900 train_time:103583ms step_avg:59.46ms
step:1743/1900 train_time:103672ms step_avg:59.48ms
step:1744/1900 train_time:103759ms step_avg:59.50ms
step:1745/1900 train_time:103849ms step_avg:59.51ms
step:1746/1900 train_time:103937ms step_avg:59.53ms
step:1747/1900 train_time:104026ms step_avg:59.55ms
step:1748/1900 train_time:104113ms step_avg:59.56ms
step:1749/1900 train_time:104202ms step_avg:59.58ms
step:1750/1900 train_time:104289ms step_avg:59.59ms
step:1750/1900 val_loss:3.3186 train_time:104380ms step_avg:59.65ms
step:1751/1900 train_time:104402ms step_avg:59.62ms
step:1752/1900 train_time:104469ms step_avg:59.63ms
step:1753/1900 train_time:104564ms step_avg:59.65ms
step:1754/1900 train_time:104654ms step_avg:59.67ms
step:1755/1900 train_time:104743ms step_avg:59.68ms
step:1756/1900 train_time:104829ms step_avg:59.70ms
step:1757/1900 train_time:104917ms step_avg:59.71ms
step:1758/1900 train_time:105004ms step_avg:59.73ms
step:1759/1900 train_time:105090ms step_avg:59.74ms
step:1760/1900 train_time:105176ms step_avg:59.76ms
step:1761/1900 train_time:105264ms step_avg:59.78ms
step:1762/1900 train_time:105353ms step_avg:59.79ms
step:1763/1900 train_time:105445ms step_avg:59.81ms
step:1764/1900 train_time:105537ms step_avg:59.83ms
step:1765/1900 train_time:105627ms step_avg:59.85ms
step:1766/1900 train_time:105715ms step_avg:59.86ms
step:1767/1900 train_time:105804ms step_avg:59.88ms
step:1768/1900 train_time:105890ms step_avg:59.89ms
step:1769/1900 train_time:105978ms step_avg:59.91ms
step:1770/1900 train_time:106064ms step_avg:59.92ms
step:1771/1900 train_time:106152ms step_avg:59.94ms
step:1772/1900 train_time:106239ms step_avg:59.95ms
step:1773/1900 train_time:106328ms step_avg:59.97ms
step:1774/1900 train_time:106417ms step_avg:59.99ms
step:1775/1900 train_time:106508ms step_avg:60.00ms
step:1776/1900 train_time:106596ms step_avg:60.02ms
step:1777/1900 train_time:106686ms step_avg:60.04ms
step:1778/1900 train_time:106773ms step_avg:60.05ms
step:1779/1900 train_time:106861ms step_avg:60.07ms
step:1780/1900 train_time:106947ms step_avg:60.08ms
step:1781/1900 train_time:107035ms step_avg:60.10ms
step:1782/1900 train_time:107121ms step_avg:60.11ms
step:1783/1900 train_time:107210ms step_avg:60.13ms
step:1784/1900 train_time:107297ms step_avg:60.14ms
step:1785/1900 train_time:107386ms step_avg:60.16ms
step:1786/1900 train_time:107476ms step_avg:60.18ms
step:1787/1900 train_time:107566ms step_avg:60.19ms
step:1788/1900 train_time:107654ms step_avg:60.21ms
step:1789/1900 train_time:107744ms step_avg:60.23ms
step:1790/1900 train_time:107832ms step_avg:60.24ms
step:1791/1900 train_time:107919ms step_avg:60.26ms
step:1792/1900 train_time:108006ms step_avg:60.27ms
step:1793/1900 train_time:108093ms step_avg:60.29ms
step:1794/1900 train_time:108181ms step_avg:60.30ms
step:1795/1900 train_time:108270ms step_avg:60.32ms
step:1796/1900 train_time:108358ms step_avg:60.33ms
step:1797/1900 train_time:108447ms step_avg:60.35ms
step:1798/1900 train_time:108536ms step_avg:60.36ms
step:1799/1900 train_time:108626ms step_avg:60.38ms
step:1800/1900 train_time:108714ms step_avg:60.40ms
step:1801/1900 train_time:108803ms step_avg:60.41ms
step:1802/1900 train_time:108890ms step_avg:60.43ms
step:1803/1900 train_time:108978ms step_avg:60.44ms
step:1804/1900 train_time:109065ms step_avg:60.46ms
step:1805/1900 train_time:109153ms step_avg:60.47ms
step:1806/1900 train_time:109240ms step_avg:60.49ms
step:1807/1900 train_time:109329ms step_avg:60.50ms
step:1808/1900 train_time:109417ms step_avg:60.52ms
step:1809/1900 train_time:109507ms step_avg:60.53ms
step:1810/1900 train_time:109595ms step_avg:60.55ms
step:1811/1900 train_time:109684ms step_avg:60.57ms
step:1812/1900 train_time:109771ms step_avg:60.58ms
step:1813/1900 train_time:109860ms step_avg:60.60ms
step:1814/1900 train_time:109947ms step_avg:60.61ms
step:1815/1900 train_time:110036ms step_avg:60.63ms
step:1816/1900 train_time:110123ms step_avg:60.64ms
step:1817/1900 train_time:110211ms step_avg:60.66ms
step:1818/1900 train_time:110299ms step_avg:60.67ms
step:1819/1900 train_time:110387ms step_avg:60.69ms
step:1820/1900 train_time:110476ms step_avg:60.70ms
step:1821/1900 train_time:110565ms step_avg:60.72ms
step:1822/1900 train_time:110653ms step_avg:60.73ms
step:1823/1900 train_time:110742ms step_avg:60.75ms
step:1824/1900 train_time:110829ms step_avg:60.76ms
step:1825/1900 train_time:110918ms step_avg:60.78ms
step:1826/1900 train_time:111006ms step_avg:60.79ms
step:1827/1900 train_time:111094ms step_avg:60.81ms
step:1828/1900 train_time:111181ms step_avg:60.82ms
step:1829/1900 train_time:111270ms step_avg:60.84ms
step:1830/1900 train_time:111358ms step_avg:60.85ms
step:1831/1900 train_time:111447ms step_avg:60.87ms
step:1832/1900 train_time:111534ms step_avg:60.88ms
step:1833/1900 train_time:111623ms step_avg:60.90ms
step:1834/1900 train_time:111711ms step_avg:60.91ms
step:1835/1900 train_time:111800ms step_avg:60.93ms
step:1836/1900 train_time:111887ms step_avg:60.94ms
step:1837/1900 train_time:111975ms step_avg:60.96ms
step:1838/1900 train_time:112063ms step_avg:60.97ms
step:1839/1900 train_time:112150ms step_avg:60.98ms
step:1840/1900 train_time:112237ms step_avg:61.00ms
step:1841/1900 train_time:112326ms step_avg:61.01ms
step:1842/1900 train_time:112414ms step_avg:61.03ms
step:1843/1900 train_time:112503ms step_avg:61.04ms
step:1844/1900 train_time:112591ms step_avg:61.06ms
step:1845/1900 train_time:112680ms step_avg:61.07ms
step:1846/1900 train_time:112767ms step_avg:61.09ms
step:1847/1900 train_time:112856ms step_avg:61.10ms
step:1848/1900 train_time:112943ms step_avg:61.12ms
step:1849/1900 train_time:113032ms step_avg:61.13ms
step:1850/1900 train_time:113120ms step_avg:61.15ms
step:1851/1900 train_time:113208ms step_avg:61.16ms
step:1852/1900 train_time:113296ms step_avg:61.17ms
step:1853/1900 train_time:113385ms step_avg:61.19ms
step:1854/1900 train_time:113473ms step_avg:61.20ms
step:1855/1900 train_time:113562ms step_avg:61.22ms
step:1856/1900 train_time:113650ms step_avg:61.23ms
step:1857/1900 train_time:113739ms step_avg:61.25ms
step:1858/1900 train_time:113826ms step_avg:61.26ms
step:1859/1900 train_time:113915ms step_avg:61.28ms
step:1860/1900 train_time:114003ms step_avg:61.29ms
step:1861/1900 train_time:114091ms step_avg:61.31ms
step:1862/1900 train_time:114178ms step_avg:61.32ms
step:1863/1900 train_time:114267ms step_avg:61.34ms
step:1864/1900 train_time:114355ms step_avg:61.35ms
step:1865/1900 train_time:114445ms step_avg:61.36ms
step:1866/1900 train_time:114533ms step_avg:61.38ms
step:1867/1900 train_time:114623ms step_avg:61.39ms
step:1868/1900 train_time:114712ms step_avg:61.41ms
step:1869/1900 train_time:114802ms step_avg:61.42ms
step:1870/1900 train_time:114889ms step_avg:61.44ms
step:1871/1900 train_time:114979ms step_avg:61.45ms
step:1872/1900 train_time:115066ms step_avg:61.47ms
step:1873/1900 train_time:115155ms step_avg:61.48ms
step:1874/1900 train_time:115242ms step_avg:61.50ms
step:1875/1900 train_time:115330ms step_avg:61.51ms
step:1876/1900 train_time:115419ms step_avg:61.52ms
step:1877/1900 train_time:115508ms step_avg:61.54ms
step:1878/1900 train_time:115597ms step_avg:61.55ms
step:1879/1900 train_time:115686ms step_avg:61.57ms
step:1880/1900 train_time:115774ms step_avg:61.58ms
step:1881/1900 train_time:115864ms step_avg:61.60ms
step:1882/1900 train_time:115952ms step_avg:61.61ms
step:1883/1900 train_time:116041ms step_avg:61.63ms
step:1884/1900 train_time:116129ms step_avg:61.64ms
step:1885/1900 train_time:116218ms step_avg:61.65ms
step:1886/1900 train_time:116306ms step_avg:61.67ms
step:1887/1900 train_time:116395ms step_avg:61.68ms
step:1888/1900 train_time:116483ms step_avg:61.70ms
step:1889/1900 train_time:116572ms step_avg:61.71ms
step:1890/1900 train_time:116660ms step_avg:61.72ms
step:1891/1900 train_time:116749ms step_avg:61.74ms
step:1892/1900 train_time:116837ms step_avg:61.75ms
step:1893/1900 train_time:116927ms step_avg:61.77ms
step:1894/1900 train_time:117015ms step_avg:61.78ms
step:1895/1900 train_time:117105ms step_avg:61.80ms
step:1896/1900 train_time:117193ms step_avg:61.81ms
step:1897/1900 train_time:117283ms step_avg:61.83ms
step:1898/1900 train_time:117370ms step_avg:61.84ms
step:1899/1900 train_time:117459ms step_avg:61.85ms
step:1900/1900 train_time:117546ms step_avg:61.87ms
step:1900/1900 val_loss:3.2780 train_time:117637ms step_avg:61.91ms
peak memory allocated: 29162 MiB reserved: 43938 MiB

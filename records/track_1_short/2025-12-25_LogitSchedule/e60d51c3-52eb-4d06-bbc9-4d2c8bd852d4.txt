import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:24:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   43C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     52725      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52726      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52727      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52728      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52729      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52730      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52731      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     52732      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     52726      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     52727      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     52728      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     52729      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     52730      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     52731      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     52732      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8307 train_time:0ms step_avg:0.21ms
step:1/1900 train_time:79ms step_avg:78.57ms
step:2/1900 train_time:101ms step_avg:50.72ms
step:3/1900 train_time:122ms step_avg:40.55ms
step:4/1900 train_time:155ms step_avg:38.71ms
step:5/1900 train_time:189ms step_avg:37.74ms
step:6/1900 train_time:267ms step_avg:44.45ms
step:7/1900 train_time:415ms step_avg:59.32ms
step:8/1900 train_time:449ms step_avg:56.12ms
step:9/1900 train_time:483ms step_avg:53.66ms
step:10/1900 train_time:517ms step_avg:51.66ms
step:11/1900 train_time:551ms step_avg:50.05ms
step:12/1900 train_time:584ms step_avg:48.70ms
step:13/1900 train_time:618ms step_avg:47.57ms
step:14/1900 train_time:652ms step_avg:46.60ms
step:15/1900 train_time:686ms step_avg:45.76ms
step:16/1900 train_time:720ms step_avg:45.01ms
step:17/1900 train_time:754ms step_avg:44.38ms
step:18/1900 train_time:788ms step_avg:43.79ms
step:19/1900 train_time:822ms step_avg:43.28ms
step:20/1900 train_time:856ms step_avg:42.81ms
step:21/1900 train_time:891ms step_avg:42.42ms
step:22/1900 train_time:925ms step_avg:42.03ms
step:23/1900 train_time:959ms step_avg:41.68ms
step:24/1900 train_time:993ms step_avg:41.36ms
step:25/1900 train_time:1027ms step_avg:41.06ms
step:26/1900 train_time:1060ms step_avg:40.79ms
step:27/1900 train_time:1095ms step_avg:40.54ms
step:28/1900 train_time:1129ms step_avg:40.31ms
step:29/1900 train_time:1163ms step_avg:40.09ms
step:30/1900 train_time:1196ms step_avg:39.88ms
step:31/1900 train_time:1231ms step_avg:39.70ms
step:32/1900 train_time:1265ms step_avg:39.52ms
step:33/1900 train_time:1299ms step_avg:39.35ms
step:34/1900 train_time:1333ms step_avg:39.21ms
step:35/1900 train_time:1368ms step_avg:39.08ms
step:36/1900 train_time:1402ms step_avg:38.94ms
step:37/1900 train_time:1436ms step_avg:38.82ms
step:38/1900 train_time:1470ms step_avg:38.69ms
step:39/1900 train_time:1505ms step_avg:38.60ms
step:40/1900 train_time:1539ms step_avg:38.48ms
step:41/1900 train_time:1574ms step_avg:38.38ms
step:42/1900 train_time:1607ms step_avg:38.27ms
step:43/1900 train_time:1642ms step_avg:38.18ms
step:44/1900 train_time:1676ms step_avg:38.08ms
step:45/1900 train_time:1710ms step_avg:38.00ms
step:46/1900 train_time:1744ms step_avg:37.91ms
step:47/1900 train_time:1778ms step_avg:37.83ms
step:48/1900 train_time:1812ms step_avg:37.75ms
step:49/1900 train_time:1846ms step_avg:37.67ms
step:50/1900 train_time:1880ms step_avg:37.60ms
step:51/1900 train_time:1914ms step_avg:37.53ms
step:52/1900 train_time:1948ms step_avg:37.46ms
step:53/1900 train_time:1982ms step_avg:37.39ms
step:54/1900 train_time:2016ms step_avg:37.33ms
step:55/1900 train_time:2050ms step_avg:37.27ms
step:56/1900 train_time:2084ms step_avg:37.21ms
step:57/1900 train_time:2118ms step_avg:37.15ms
step:58/1900 train_time:2152ms step_avg:37.10ms
step:59/1900 train_time:2186ms step_avg:37.05ms
step:60/1900 train_time:2219ms step_avg:36.99ms
step:61/1900 train_time:2254ms step_avg:36.95ms
step:62/1900 train_time:2288ms step_avg:36.90ms
step:63/1900 train_time:2322ms step_avg:36.86ms
step:64/1900 train_time:2356ms step_avg:36.81ms
step:65/1900 train_time:2391ms step_avg:36.78ms
step:66/1900 train_time:2424ms step_avg:36.73ms
step:67/1900 train_time:2459ms step_avg:36.70ms
step:68/1900 train_time:2493ms step_avg:36.65ms
step:69/1900 train_time:2527ms step_avg:36.62ms
step:70/1900 train_time:2561ms step_avg:36.58ms
step:71/1900 train_time:2595ms step_avg:36.55ms
step:72/1900 train_time:2629ms step_avg:36.52ms
step:73/1900 train_time:2663ms step_avg:36.48ms
step:74/1900 train_time:2697ms step_avg:36.45ms
step:75/1900 train_time:2731ms step_avg:36.41ms
step:76/1900 train_time:2765ms step_avg:36.38ms
step:77/1900 train_time:2799ms step_avg:36.35ms
step:78/1900 train_time:2833ms step_avg:36.32ms
step:79/1900 train_time:2867ms step_avg:36.29ms
step:80/1900 train_time:2901ms step_avg:36.26ms
step:81/1900 train_time:2936ms step_avg:36.24ms
step:82/1900 train_time:2969ms step_avg:36.21ms
step:83/1900 train_time:3004ms step_avg:36.19ms
step:84/1900 train_time:3038ms step_avg:36.16ms
step:85/1900 train_time:3072ms step_avg:36.14ms
step:86/1900 train_time:3106ms step_avg:36.11ms
step:87/1900 train_time:3140ms step_avg:36.09ms
step:88/1900 train_time:3174ms step_avg:36.06ms
step:89/1900 train_time:3207ms step_avg:36.04ms
step:90/1900 train_time:3241ms step_avg:36.01ms
step:91/1900 train_time:3275ms step_avg:35.99ms
step:92/1900 train_time:3309ms step_avg:35.97ms
step:93/1900 train_time:3343ms step_avg:35.95ms
step:94/1900 train_time:3377ms step_avg:35.93ms
step:95/1900 train_time:3411ms step_avg:35.91ms
step:96/1900 train_time:3445ms step_avg:35.89ms
step:97/1900 train_time:3480ms step_avg:35.87ms
step:98/1900 train_time:3514ms step_avg:35.85ms
step:99/1900 train_time:3548ms step_avg:35.84ms
step:100/1900 train_time:3582ms step_avg:35.82ms
step:101/1900 train_time:3616ms step_avg:35.80ms
step:102/1900 train_time:3650ms step_avg:35.78ms
step:103/1900 train_time:3684ms step_avg:35.77ms
step:104/1900 train_time:3718ms step_avg:35.75ms
step:105/1900 train_time:3752ms step_avg:35.73ms
step:106/1900 train_time:3786ms step_avg:35.71ms
step:107/1900 train_time:3820ms step_avg:35.70ms
step:108/1900 train_time:3854ms step_avg:35.68ms
step:109/1900 train_time:3888ms step_avg:35.67ms
step:110/1900 train_time:3921ms step_avg:35.65ms
step:111/1900 train_time:3956ms step_avg:35.64ms
step:112/1900 train_time:3989ms step_avg:35.62ms
step:113/1900 train_time:4024ms step_avg:35.61ms
step:114/1900 train_time:4057ms step_avg:35.59ms
step:115/1900 train_time:4092ms step_avg:35.58ms
step:116/1900 train_time:4126ms step_avg:35.57ms
step:117/1900 train_time:4160ms step_avg:35.56ms
step:118/1900 train_time:4194ms step_avg:35.54ms
step:119/1900 train_time:4228ms step_avg:35.53ms
step:120/1900 train_time:4262ms step_avg:35.51ms
step:121/1900 train_time:4296ms step_avg:35.50ms
step:122/1900 train_time:4330ms step_avg:35.49ms
step:123/1900 train_time:4364ms step_avg:35.48ms
step:124/1900 train_time:4398ms step_avg:35.46ms
step:125/1900 train_time:4432ms step_avg:35.46ms
step:126/1900 train_time:4466ms step_avg:35.44ms
step:127/1900 train_time:4500ms step_avg:35.44ms
step:128/1900 train_time:4534ms step_avg:35.42ms
step:129/1900 train_time:4568ms step_avg:35.41ms
step:130/1900 train_time:4602ms step_avg:35.40ms
step:131/1900 train_time:4636ms step_avg:35.39ms
step:132/1900 train_time:4670ms step_avg:35.38ms
step:133/1900 train_time:4704ms step_avg:35.37ms
step:134/1900 train_time:4738ms step_avg:35.36ms
step:135/1900 train_time:4772ms step_avg:35.35ms
step:136/1900 train_time:4806ms step_avg:35.34ms
step:137/1900 train_time:4840ms step_avg:35.33ms
step:138/1900 train_time:4874ms step_avg:35.32ms
step:139/1900 train_time:4908ms step_avg:35.31ms
step:140/1900 train_time:4942ms step_avg:35.30ms
step:141/1900 train_time:4976ms step_avg:35.29ms
step:142/1900 train_time:5010ms step_avg:35.28ms
step:143/1900 train_time:5044ms step_avg:35.27ms
step:144/1900 train_time:5078ms step_avg:35.26ms
step:145/1900 train_time:5112ms step_avg:35.25ms
step:146/1900 train_time:5146ms step_avg:35.24ms
step:147/1900 train_time:5180ms step_avg:35.24ms
step:148/1900 train_time:5214ms step_avg:35.23ms
step:149/1900 train_time:5248ms step_avg:35.22ms
step:150/1900 train_time:5282ms step_avg:35.21ms
step:151/1900 train_time:5316ms step_avg:35.20ms
step:152/1900 train_time:5349ms step_avg:35.19ms
step:153/1900 train_time:5384ms step_avg:35.19ms
step:154/1900 train_time:5417ms step_avg:35.18ms
step:155/1900 train_time:5451ms step_avg:35.17ms
step:156/1900 train_time:5485ms step_avg:35.16ms
step:157/1900 train_time:5519ms step_avg:35.15ms
step:158/1900 train_time:5553ms step_avg:35.15ms
step:159/1900 train_time:5587ms step_avg:35.14ms
step:160/1900 train_time:5621ms step_avg:35.13ms
step:161/1900 train_time:5655ms step_avg:35.12ms
step:162/1900 train_time:5689ms step_avg:35.12ms
step:163/1900 train_time:5723ms step_avg:35.11ms
step:164/1900 train_time:5757ms step_avg:35.10ms
step:165/1900 train_time:5791ms step_avg:35.10ms
step:166/1900 train_time:5825ms step_avg:35.09ms
step:167/1900 train_time:5859ms step_avg:35.09ms
step:168/1900 train_time:5893ms step_avg:35.08ms
step:169/1900 train_time:5927ms step_avg:35.07ms
step:170/1900 train_time:5961ms step_avg:35.07ms
step:171/1900 train_time:5995ms step_avg:35.06ms
step:172/1900 train_time:6029ms step_avg:35.05ms
step:173/1900 train_time:6063ms step_avg:35.05ms
step:174/1900 train_time:6097ms step_avg:35.04ms
step:175/1900 train_time:6131ms step_avg:35.03ms
step:176/1900 train_time:6165ms step_avg:35.03ms
step:177/1900 train_time:6199ms step_avg:35.02ms
step:178/1900 train_time:6233ms step_avg:35.02ms
step:179/1900 train_time:6267ms step_avg:35.01ms
step:180/1900 train_time:6301ms step_avg:35.00ms
step:181/1900 train_time:6335ms step_avg:35.00ms
step:182/1900 train_time:6369ms step_avg:34.99ms
step:183/1900 train_time:6403ms step_avg:34.99ms
step:184/1900 train_time:6437ms step_avg:34.98ms
step:185/1900 train_time:6471ms step_avg:34.98ms
step:186/1900 train_time:6504ms step_avg:34.97ms
step:187/1900 train_time:6538ms step_avg:34.96ms
step:188/1900 train_time:6572ms step_avg:34.96ms
step:189/1900 train_time:6606ms step_avg:34.95ms
step:190/1900 train_time:6640ms step_avg:34.95ms
step:191/1900 train_time:6674ms step_avg:34.94ms
step:192/1900 train_time:6708ms step_avg:34.94ms
step:193/1900 train_time:6742ms step_avg:34.93ms
step:194/1900 train_time:6776ms step_avg:34.93ms
step:195/1900 train_time:6811ms step_avg:34.93ms
step:196/1900 train_time:6844ms step_avg:34.92ms
step:197/1900 train_time:6879ms step_avg:34.92ms
step:198/1900 train_time:6912ms step_avg:34.91ms
step:199/1900 train_time:6947ms step_avg:34.91ms
step:200/1900 train_time:6980ms step_avg:34.90ms
step:201/1900 train_time:7014ms step_avg:34.90ms
step:202/1900 train_time:7048ms step_avg:34.89ms
step:203/1900 train_time:7082ms step_avg:34.89ms
step:204/1900 train_time:7116ms step_avg:34.88ms
step:205/1900 train_time:7150ms step_avg:34.88ms
step:206/1900 train_time:7184ms step_avg:34.87ms
step:207/1900 train_time:7218ms step_avg:34.87ms
step:208/1900 train_time:7252ms step_avg:34.86ms
step:209/1900 train_time:7286ms step_avg:34.86ms
step:210/1900 train_time:7319ms step_avg:34.85ms
step:211/1900 train_time:7353ms step_avg:34.85ms
step:212/1900 train_time:7387ms step_avg:34.84ms
step:213/1900 train_time:7421ms step_avg:34.84ms
step:214/1900 train_time:7455ms step_avg:34.84ms
step:215/1900 train_time:7489ms step_avg:34.83ms
step:216/1900 train_time:7523ms step_avg:34.83ms
step:217/1900 train_time:7557ms step_avg:34.82ms
step:218/1900 train_time:7590ms step_avg:34.82ms
step:219/1900 train_time:7625ms step_avg:34.82ms
step:220/1900 train_time:7658ms step_avg:34.81ms
step:221/1900 train_time:7692ms step_avg:34.81ms
step:222/1900 train_time:7726ms step_avg:34.80ms
step:223/1900 train_time:7761ms step_avg:34.80ms
step:224/1900 train_time:7794ms step_avg:34.80ms
step:225/1900 train_time:7829ms step_avg:34.79ms
step:226/1900 train_time:7862ms step_avg:34.79ms
step:227/1900 train_time:7897ms step_avg:34.79ms
step:228/1900 train_time:7930ms step_avg:34.78ms
step:229/1900 train_time:7965ms step_avg:34.78ms
step:230/1900 train_time:7999ms step_avg:34.78ms
step:231/1900 train_time:8033ms step_avg:34.78ms
step:232/1900 train_time:8067ms step_avg:34.77ms
step:233/1900 train_time:8101ms step_avg:34.77ms
step:234/1900 train_time:8135ms step_avg:34.76ms
step:235/1900 train_time:8169ms step_avg:34.76ms
step:236/1900 train_time:8203ms step_avg:34.76ms
step:237/1900 train_time:8237ms step_avg:34.75ms
step:238/1900 train_time:8270ms step_avg:34.75ms
step:239/1900 train_time:8304ms step_avg:34.75ms
step:240/1900 train_time:8338ms step_avg:34.74ms
step:241/1900 train_time:8372ms step_avg:34.74ms
step:242/1900 train_time:8406ms step_avg:34.74ms
step:243/1900 train_time:8441ms step_avg:34.74ms
step:244/1900 train_time:8474ms step_avg:34.73ms
step:245/1900 train_time:8509ms step_avg:34.73ms
step:246/1900 train_time:8542ms step_avg:34.72ms
step:247/1900 train_time:8576ms step_avg:34.72ms
step:248/1900 train_time:8610ms step_avg:34.72ms
step:249/1900 train_time:8644ms step_avg:34.71ms
step:250/1900 train_time:8678ms step_avg:34.71ms
step:250/1900 val_loss:4.6180 train_time:8715ms step_avg:34.86ms
step:251/1900 train_time:8736ms step_avg:34.80ms
step:252/1900 train_time:8756ms step_avg:34.75ms
step:253/1900 train_time:8783ms step_avg:34.71ms
step:254/1900 train_time:8817ms step_avg:34.71ms
step:255/1900 train_time:8851ms step_avg:34.71ms
step:256/1900 train_time:8886ms step_avg:34.71ms
step:257/1900 train_time:8920ms step_avg:34.71ms
step:258/1900 train_time:8954ms step_avg:34.70ms
step:259/1900 train_time:8988ms step_avg:34.70ms
step:260/1900 train_time:9021ms step_avg:34.70ms
step:261/1900 train_time:9056ms step_avg:34.70ms
step:262/1900 train_time:9089ms step_avg:34.69ms
step:263/1900 train_time:9123ms step_avg:34.69ms
step:264/1900 train_time:9157ms step_avg:34.69ms
step:265/1900 train_time:9191ms step_avg:34.68ms
step:266/1900 train_time:9225ms step_avg:34.68ms
step:267/1900 train_time:9259ms step_avg:34.68ms
step:268/1900 train_time:9293ms step_avg:34.67ms
step:269/1900 train_time:9327ms step_avg:34.67ms
step:270/1900 train_time:9360ms step_avg:34.67ms
step:271/1900 train_time:9394ms step_avg:34.67ms
step:272/1900 train_time:9428ms step_avg:34.66ms
step:273/1900 train_time:9462ms step_avg:34.66ms
step:274/1900 train_time:9496ms step_avg:34.66ms
step:275/1900 train_time:9530ms step_avg:34.65ms
step:276/1900 train_time:9563ms step_avg:34.65ms
step:277/1900 train_time:9597ms step_avg:34.65ms
step:278/1900 train_time:9631ms step_avg:34.64ms
step:279/1900 train_time:9665ms step_avg:34.64ms
step:280/1900 train_time:9699ms step_avg:34.64ms
step:281/1900 train_time:9733ms step_avg:34.64ms
step:282/1900 train_time:9767ms step_avg:34.64ms
step:283/1900 train_time:9802ms step_avg:34.63ms
step:284/1900 train_time:9835ms step_avg:34.63ms
step:285/1900 train_time:9870ms step_avg:34.63ms
step:286/1900 train_time:9903ms step_avg:34.63ms
step:287/1900 train_time:9938ms step_avg:34.63ms
step:288/1900 train_time:9972ms step_avg:34.62ms
step:289/1900 train_time:10006ms step_avg:34.62ms
step:290/1900 train_time:10040ms step_avg:34.62ms
step:291/1900 train_time:10074ms step_avg:34.62ms
step:292/1900 train_time:10108ms step_avg:34.62ms
step:293/1900 train_time:10142ms step_avg:34.61ms
step:294/1900 train_time:10176ms step_avg:34.61ms
step:295/1900 train_time:10210ms step_avg:34.61ms
step:296/1900 train_time:10243ms step_avg:34.61ms
step:297/1900 train_time:10278ms step_avg:34.61ms
step:298/1900 train_time:10312ms step_avg:34.60ms
step:299/1900 train_time:10346ms step_avg:34.60ms
step:300/1900 train_time:10379ms step_avg:34.60ms
step:301/1900 train_time:10413ms step_avg:34.60ms
step:302/1900 train_time:10447ms step_avg:34.59ms
step:303/1900 train_time:10481ms step_avg:34.59ms
step:304/1900 train_time:10515ms step_avg:34.59ms
step:305/1900 train_time:10549ms step_avg:34.59ms
step:306/1900 train_time:10582ms step_avg:34.58ms
step:307/1900 train_time:10616ms step_avg:34.58ms
step:308/1900 train_time:10650ms step_avg:34.58ms
step:309/1900 train_time:10684ms step_avg:34.58ms
step:310/1900 train_time:10718ms step_avg:34.57ms
step:311/1900 train_time:10752ms step_avg:34.57ms
step:312/1900 train_time:10786ms step_avg:34.57ms
step:313/1900 train_time:10820ms step_avg:34.57ms
step:314/1900 train_time:10853ms step_avg:34.56ms
step:315/1900 train_time:10888ms step_avg:34.56ms
step:316/1900 train_time:10921ms step_avg:34.56ms
step:317/1900 train_time:10955ms step_avg:34.56ms
step:318/1900 train_time:10989ms step_avg:34.56ms
step:319/1900 train_time:11023ms step_avg:34.56ms
step:320/1900 train_time:11057ms step_avg:34.55ms
step:321/1900 train_time:11091ms step_avg:34.55ms
step:322/1900 train_time:11125ms step_avg:34.55ms
step:323/1900 train_time:11159ms step_avg:34.55ms
step:324/1900 train_time:11193ms step_avg:34.55ms
step:325/1900 train_time:11227ms step_avg:34.54ms
step:326/1900 train_time:11261ms step_avg:34.54ms
step:327/1900 train_time:11295ms step_avg:34.54ms
step:328/1900 train_time:11328ms step_avg:34.54ms
step:329/1900 train_time:11362ms step_avg:34.54ms
step:330/1900 train_time:11396ms step_avg:34.53ms
step:331/1900 train_time:11430ms step_avg:34.53ms
step:332/1900 train_time:11464ms step_avg:34.53ms
step:333/1900 train_time:11498ms step_avg:34.53ms
step:334/1900 train_time:11532ms step_avg:34.53ms
step:335/1900 train_time:11565ms step_avg:34.52ms
step:336/1900 train_time:11599ms step_avg:34.52ms
step:337/1900 train_time:11633ms step_avg:34.52ms
step:338/1900 train_time:11667ms step_avg:34.52ms
step:339/1900 train_time:11701ms step_avg:34.52ms
step:340/1900 train_time:11734ms step_avg:34.51ms
step:341/1900 train_time:11768ms step_avg:34.51ms
step:342/1900 train_time:11802ms step_avg:34.51ms
step:343/1900 train_time:11837ms step_avg:34.51ms
step:344/1900 train_time:11871ms step_avg:34.51ms
step:345/1900 train_time:11904ms step_avg:34.51ms
step:346/1900 train_time:11938ms step_avg:34.50ms
step:347/1900 train_time:11972ms step_avg:34.50ms
step:348/1900 train_time:12006ms step_avg:34.50ms
step:349/1900 train_time:12040ms step_avg:34.50ms
step:350/1900 train_time:12074ms step_avg:34.50ms
step:351/1900 train_time:12108ms step_avg:34.50ms
step:352/1900 train_time:12141ms step_avg:34.49ms
step:353/1900 train_time:12176ms step_avg:34.49ms
step:354/1900 train_time:12209ms step_avg:34.49ms
step:355/1900 train_time:12243ms step_avg:34.49ms
step:356/1900 train_time:12277ms step_avg:34.49ms
step:357/1900 train_time:12311ms step_avg:34.49ms
step:358/1900 train_time:12345ms step_avg:34.48ms
step:359/1900 train_time:12379ms step_avg:34.48ms
step:360/1900 train_time:12413ms step_avg:34.48ms
step:361/1900 train_time:12447ms step_avg:34.48ms
step:362/1900 train_time:12481ms step_avg:34.48ms
step:363/1900 train_time:12515ms step_avg:34.48ms
step:364/1900 train_time:12549ms step_avg:34.47ms
step:365/1900 train_time:12583ms step_avg:34.47ms
step:366/1900 train_time:12616ms step_avg:34.47ms
step:367/1900 train_time:12650ms step_avg:34.47ms
step:368/1900 train_time:12684ms step_avg:34.47ms
step:369/1900 train_time:12718ms step_avg:34.47ms
step:370/1900 train_time:12752ms step_avg:34.46ms
step:371/1900 train_time:12786ms step_avg:34.46ms
step:372/1900 train_time:12820ms step_avg:34.46ms
step:373/1900 train_time:12854ms step_avg:34.46ms
step:374/1900 train_time:12887ms step_avg:34.46ms
step:375/1900 train_time:12921ms step_avg:34.46ms
step:376/1900 train_time:12955ms step_avg:34.45ms
step:377/1900 train_time:12989ms step_avg:34.45ms
step:378/1900 train_time:13022ms step_avg:34.45ms
step:379/1900 train_time:13057ms step_avg:34.45ms
step:380/1900 train_time:13090ms step_avg:34.45ms
step:381/1900 train_time:13124ms step_avg:34.45ms
step:382/1900 train_time:13158ms step_avg:34.45ms
step:383/1900 train_time:13193ms step_avg:34.45ms
step:384/1900 train_time:13226ms step_avg:34.44ms
step:385/1900 train_time:13261ms step_avg:34.44ms
step:386/1900 train_time:13294ms step_avg:34.44ms
step:387/1900 train_time:13329ms step_avg:34.44ms
step:388/1900 train_time:13362ms step_avg:34.44ms
step:389/1900 train_time:13397ms step_avg:34.44ms
step:390/1900 train_time:13430ms step_avg:34.44ms
step:391/1900 train_time:13465ms step_avg:34.44ms
step:392/1900 train_time:13498ms step_avg:34.43ms
step:393/1900 train_time:13532ms step_avg:34.43ms
step:394/1900 train_time:13566ms step_avg:34.43ms
step:395/1900 train_time:13600ms step_avg:34.43ms
step:396/1900 train_time:13634ms step_avg:34.43ms
step:397/1900 train_time:13668ms step_avg:34.43ms
step:398/1900 train_time:13702ms step_avg:34.43ms
step:399/1900 train_time:13736ms step_avg:34.42ms
step:400/1900 train_time:13769ms step_avg:34.42ms
step:401/1900 train_time:13803ms step_avg:34.42ms
step:402/1900 train_time:13837ms step_avg:34.42ms
step:403/1900 train_time:13871ms step_avg:34.42ms
step:404/1900 train_time:13905ms step_avg:34.42ms
step:405/1900 train_time:13939ms step_avg:34.42ms
step:406/1900 train_time:13972ms step_avg:34.41ms
step:407/1900 train_time:14007ms step_avg:34.41ms
step:408/1900 train_time:14040ms step_avg:34.41ms
step:409/1900 train_time:14075ms step_avg:34.41ms
step:410/1900 train_time:14108ms step_avg:34.41ms
step:411/1900 train_time:14142ms step_avg:34.41ms
step:412/1900 train_time:14176ms step_avg:34.41ms
step:413/1900 train_time:14210ms step_avg:34.41ms
step:414/1900 train_time:14244ms step_avg:34.41ms
step:415/1900 train_time:14278ms step_avg:34.40ms
step:416/1900 train_time:14312ms step_avg:34.40ms
step:417/1900 train_time:14346ms step_avg:34.40ms
step:418/1900 train_time:14379ms step_avg:34.40ms
step:419/1900 train_time:14413ms step_avg:34.40ms
step:420/1900 train_time:14447ms step_avg:34.40ms
step:421/1900 train_time:14481ms step_avg:34.40ms
step:422/1900 train_time:14515ms step_avg:34.39ms
step:423/1900 train_time:14549ms step_avg:34.39ms
step:424/1900 train_time:14582ms step_avg:34.39ms
step:425/1900 train_time:14616ms step_avg:34.39ms
step:426/1900 train_time:14650ms step_avg:34.39ms
step:427/1900 train_time:14684ms step_avg:34.39ms
step:428/1900 train_time:14718ms step_avg:34.39ms
step:429/1900 train_time:14752ms step_avg:34.39ms
step:430/1900 train_time:14786ms step_avg:34.39ms
step:431/1900 train_time:14820ms step_avg:34.38ms
step:432/1900 train_time:14854ms step_avg:34.38ms
step:433/1900 train_time:14888ms step_avg:34.38ms
step:434/1900 train_time:14921ms step_avg:34.38ms
step:435/1900 train_time:14955ms step_avg:34.38ms
step:436/1900 train_time:14989ms step_avg:34.38ms
step:437/1900 train_time:15023ms step_avg:34.38ms
step:438/1900 train_time:15057ms step_avg:34.38ms
step:439/1900 train_time:15091ms step_avg:34.38ms
step:440/1900 train_time:15125ms step_avg:34.37ms
step:441/1900 train_time:15159ms step_avg:34.37ms
step:442/1900 train_time:15193ms step_avg:34.37ms
step:443/1900 train_time:15227ms step_avg:34.37ms
step:444/1900 train_time:15261ms step_avg:34.37ms
step:445/1900 train_time:15295ms step_avg:34.37ms
step:446/1900 train_time:15329ms step_avg:34.37ms
step:447/1900 train_time:15363ms step_avg:34.37ms
step:448/1900 train_time:15397ms step_avg:34.37ms
step:449/1900 train_time:15431ms step_avg:34.37ms
step:450/1900 train_time:15464ms step_avg:34.37ms
step:451/1900 train_time:15498ms step_avg:34.36ms
step:452/1900 train_time:15532ms step_avg:34.36ms
step:453/1900 train_time:15566ms step_avg:34.36ms
step:454/1900 train_time:15600ms step_avg:34.36ms
step:455/1900 train_time:15634ms step_avg:34.36ms
step:456/1900 train_time:15667ms step_avg:34.36ms
step:457/1900 train_time:15701ms step_avg:34.36ms
step:458/1900 train_time:15735ms step_avg:34.36ms
step:459/1900 train_time:15769ms step_avg:34.35ms
step:460/1900 train_time:15803ms step_avg:34.35ms
step:461/1900 train_time:15837ms step_avg:34.35ms
step:462/1900 train_time:15871ms step_avg:34.35ms
step:463/1900 train_time:15905ms step_avg:34.35ms
step:464/1900 train_time:15939ms step_avg:34.35ms
step:465/1900 train_time:15973ms step_avg:34.35ms
step:466/1900 train_time:16006ms step_avg:34.35ms
step:467/1900 train_time:16041ms step_avg:34.35ms
step:468/1900 train_time:16074ms step_avg:34.35ms
step:469/1900 train_time:16108ms step_avg:34.35ms
step:470/1900 train_time:16142ms step_avg:34.34ms
step:471/1900 train_time:16177ms step_avg:34.35ms
step:472/1900 train_time:16211ms step_avg:34.34ms
step:473/1900 train_time:16245ms step_avg:34.34ms
step:474/1900 train_time:16279ms step_avg:34.34ms
step:475/1900 train_time:16312ms step_avg:34.34ms
step:476/1900 train_time:16346ms step_avg:34.34ms
step:477/1900 train_time:16380ms step_avg:34.34ms
step:478/1900 train_time:16414ms step_avg:34.34ms
step:479/1900 train_time:16448ms step_avg:34.34ms
step:480/1900 train_time:16482ms step_avg:34.34ms
step:481/1900 train_time:16516ms step_avg:34.34ms
step:482/1900 train_time:16550ms step_avg:34.34ms
step:483/1900 train_time:16584ms step_avg:34.34ms
step:484/1900 train_time:16618ms step_avg:34.33ms
step:485/1900 train_time:16652ms step_avg:34.33ms
step:486/1900 train_time:16685ms step_avg:34.33ms
step:487/1900 train_time:16720ms step_avg:34.33ms
step:488/1900 train_time:16753ms step_avg:34.33ms
step:489/1900 train_time:16788ms step_avg:34.33ms
step:490/1900 train_time:16821ms step_avg:34.33ms
step:491/1900 train_time:16856ms step_avg:34.33ms
step:492/1900 train_time:16889ms step_avg:34.33ms
step:493/1900 train_time:16923ms step_avg:34.33ms
step:494/1900 train_time:16957ms step_avg:34.33ms
step:495/1900 train_time:16991ms step_avg:34.33ms
step:496/1900 train_time:17025ms step_avg:34.32ms
step:497/1900 train_time:17059ms step_avg:34.32ms
step:498/1900 train_time:17093ms step_avg:34.32ms
step:499/1900 train_time:17127ms step_avg:34.32ms
step:500/1900 train_time:17160ms step_avg:34.32ms
step:500/1900 val_loss:4.2763 train_time:17198ms step_avg:34.40ms
step:501/1900 train_time:17219ms step_avg:34.37ms
step:502/1900 train_time:17240ms step_avg:34.34ms
step:503/1900 train_time:17266ms step_avg:34.33ms
step:504/1900 train_time:17301ms step_avg:34.33ms
step:505/1900 train_time:17336ms step_avg:34.33ms
step:506/1900 train_time:17370ms step_avg:34.33ms
step:507/1900 train_time:17404ms step_avg:34.33ms
step:508/1900 train_time:17438ms step_avg:34.33ms
step:509/1900 train_time:17472ms step_avg:34.33ms
step:510/1900 train_time:17506ms step_avg:34.33ms
step:511/1900 train_time:17540ms step_avg:34.32ms
step:512/1900 train_time:17574ms step_avg:34.32ms
step:513/1900 train_time:17607ms step_avg:34.32ms
step:514/1900 train_time:17641ms step_avg:34.32ms
step:515/1900 train_time:17675ms step_avg:34.32ms
step:516/1900 train_time:17708ms step_avg:34.32ms
step:517/1900 train_time:17742ms step_avg:34.32ms
step:518/1900 train_time:17776ms step_avg:34.32ms
step:519/1900 train_time:17810ms step_avg:34.32ms
step:520/1900 train_time:17844ms step_avg:34.31ms
step:521/1900 train_time:17877ms step_avg:34.31ms
step:522/1900 train_time:17911ms step_avg:34.31ms
step:523/1900 train_time:17945ms step_avg:34.31ms
step:524/1900 train_time:17979ms step_avg:34.31ms
step:525/1900 train_time:18012ms step_avg:34.31ms
step:526/1900 train_time:18046ms step_avg:34.31ms
step:527/1900 train_time:18080ms step_avg:34.31ms
step:528/1900 train_time:18114ms step_avg:34.31ms
step:529/1900 train_time:18148ms step_avg:34.31ms
step:530/1900 train_time:18181ms step_avg:34.30ms
step:531/1900 train_time:18215ms step_avg:34.30ms
step:532/1900 train_time:18249ms step_avg:34.30ms
step:533/1900 train_time:18283ms step_avg:34.30ms
step:534/1900 train_time:18317ms step_avg:34.30ms
step:535/1900 train_time:18351ms step_avg:34.30ms
step:536/1900 train_time:18385ms step_avg:34.30ms
step:537/1900 train_time:18419ms step_avg:34.30ms
step:538/1900 train_time:18453ms step_avg:34.30ms
step:539/1900 train_time:18486ms step_avg:34.30ms
step:540/1900 train_time:18520ms step_avg:34.30ms
step:541/1900 train_time:18554ms step_avg:34.30ms
step:542/1900 train_time:18588ms step_avg:34.30ms
step:543/1900 train_time:18622ms step_avg:34.29ms
step:544/1900 train_time:18656ms step_avg:34.29ms
step:545/1900 train_time:18690ms step_avg:34.29ms
step:546/1900 train_time:18724ms step_avg:34.29ms
step:547/1900 train_time:18758ms step_avg:34.29ms
step:548/1900 train_time:18791ms step_avg:34.29ms
step:549/1900 train_time:18825ms step_avg:34.29ms
step:550/1900 train_time:18859ms step_avg:34.29ms
step:551/1900 train_time:18893ms step_avg:34.29ms
step:552/1900 train_time:18927ms step_avg:34.29ms
step:553/1900 train_time:18961ms step_avg:34.29ms
step:554/1900 train_time:18994ms step_avg:34.29ms
step:555/1900 train_time:19028ms step_avg:34.28ms
step:556/1900 train_time:19062ms step_avg:34.28ms
step:557/1900 train_time:19096ms step_avg:34.28ms
step:558/1900 train_time:19130ms step_avg:34.28ms
step:559/1900 train_time:19164ms step_avg:34.28ms
step:560/1900 train_time:19198ms step_avg:34.28ms
step:561/1900 train_time:19232ms step_avg:34.28ms
step:562/1900 train_time:19265ms step_avg:34.28ms
step:563/1900 train_time:19299ms step_avg:34.28ms
step:564/1900 train_time:19333ms step_avg:34.28ms
step:565/1900 train_time:19367ms step_avg:34.28ms
step:566/1900 train_time:19401ms step_avg:34.28ms
step:567/1900 train_time:19435ms step_avg:34.28ms
step:568/1900 train_time:19469ms step_avg:34.28ms
step:569/1900 train_time:19503ms step_avg:34.28ms
step:570/1900 train_time:19537ms step_avg:34.27ms
step:571/1900 train_time:19571ms step_avg:34.27ms
step:572/1900 train_time:19605ms step_avg:34.27ms
step:573/1900 train_time:19638ms step_avg:34.27ms
step:574/1900 train_time:19672ms step_avg:34.27ms
step:575/1900 train_time:19706ms step_avg:34.27ms
step:576/1900 train_time:19740ms step_avg:34.27ms
step:577/1900 train_time:19774ms step_avg:34.27ms
step:578/1900 train_time:19807ms step_avg:34.27ms
step:579/1900 train_time:19841ms step_avg:34.27ms
step:580/1900 train_time:19875ms step_avg:34.27ms
step:581/1900 train_time:19909ms step_avg:34.27ms
step:582/1900 train_time:19943ms step_avg:34.27ms
step:583/1900 train_time:19977ms step_avg:34.27ms
step:584/1900 train_time:20010ms step_avg:34.26ms
step:585/1900 train_time:20044ms step_avg:34.26ms
step:586/1900 train_time:20078ms step_avg:34.26ms
step:587/1900 train_time:20112ms step_avg:34.26ms
step:588/1900 train_time:20146ms step_avg:34.26ms
step:589/1900 train_time:20180ms step_avg:34.26ms
step:590/1900 train_time:20213ms step_avg:34.26ms
step:591/1900 train_time:20247ms step_avg:34.26ms
step:592/1900 train_time:20281ms step_avg:34.26ms
step:593/1900 train_time:20315ms step_avg:34.26ms
step:594/1900 train_time:20349ms step_avg:34.26ms
step:595/1900 train_time:20383ms step_avg:34.26ms
step:596/1900 train_time:20417ms step_avg:34.26ms
step:597/1900 train_time:20451ms step_avg:34.26ms
step:598/1900 train_time:20485ms step_avg:34.26ms
step:599/1900 train_time:20518ms step_avg:34.25ms
step:600/1900 train_time:20552ms step_avg:34.25ms
step:601/1900 train_time:20586ms step_avg:34.25ms
step:602/1900 train_time:20620ms step_avg:34.25ms
step:603/1900 train_time:20654ms step_avg:34.25ms
step:604/1900 train_time:20688ms step_avg:34.25ms
step:605/1900 train_time:20722ms step_avg:34.25ms
step:606/1900 train_time:20756ms step_avg:34.25ms
step:607/1900 train_time:20790ms step_avg:34.25ms
step:608/1900 train_time:20823ms step_avg:34.25ms
step:609/1900 train_time:20858ms step_avg:34.25ms
step:610/1900 train_time:20891ms step_avg:34.25ms
step:611/1900 train_time:20925ms step_avg:34.25ms
step:612/1900 train_time:20959ms step_avg:34.25ms
step:613/1900 train_time:20993ms step_avg:34.25ms
step:614/1900 train_time:21027ms step_avg:34.25ms
step:615/1900 train_time:21061ms step_avg:34.24ms
step:616/1900 train_time:21094ms step_avg:34.24ms
step:617/1900 train_time:21128ms step_avg:34.24ms
step:618/1900 train_time:21162ms step_avg:34.24ms
step:619/1900 train_time:21196ms step_avg:34.24ms
step:620/1900 train_time:21230ms step_avg:34.24ms
step:621/1900 train_time:21264ms step_avg:34.24ms
step:622/1900 train_time:21324ms step_avg:34.28ms
step:623/1900 train_time:21386ms step_avg:34.33ms
step:624/1900 train_time:21447ms step_avg:34.37ms
step:625/1900 train_time:21509ms step_avg:34.41ms
step:626/1900 train_time:21570ms step_avg:34.46ms
step:627/1900 train_time:21632ms step_avg:34.50ms
step:628/1900 train_time:21692ms step_avg:34.54ms
step:629/1900 train_time:21754ms step_avg:34.59ms
step:630/1900 train_time:21815ms step_avg:34.63ms
step:631/1900 train_time:21877ms step_avg:34.67ms
step:632/1900 train_time:21939ms step_avg:34.71ms
step:633/1900 train_time:22001ms step_avg:34.76ms
step:634/1900 train_time:22061ms step_avg:34.80ms
step:635/1900 train_time:22124ms step_avg:34.84ms
step:636/1900 train_time:22185ms step_avg:34.88ms
step:637/1900 train_time:22246ms step_avg:34.92ms
step:638/1900 train_time:22306ms step_avg:34.96ms
step:639/1900 train_time:22368ms step_avg:35.01ms
step:640/1900 train_time:22429ms step_avg:35.05ms
step:641/1900 train_time:22490ms step_avg:35.09ms
step:642/1900 train_time:22551ms step_avg:35.13ms
step:643/1900 train_time:22613ms step_avg:35.17ms
step:644/1900 train_time:22674ms step_avg:35.21ms
step:645/1900 train_time:22736ms step_avg:35.25ms
step:646/1900 train_time:22797ms step_avg:35.29ms
step:647/1900 train_time:22858ms step_avg:35.33ms
step:648/1900 train_time:22919ms step_avg:35.37ms
step:649/1900 train_time:22981ms step_avg:35.41ms
step:650/1900 train_time:23041ms step_avg:35.45ms
step:651/1900 train_time:23103ms step_avg:35.49ms
step:652/1900 train_time:23164ms step_avg:35.53ms
step:653/1900 train_time:23225ms step_avg:35.57ms
step:654/1900 train_time:23286ms step_avg:35.61ms
step:655/1900 train_time:23348ms step_avg:35.65ms
step:656/1900 train_time:23409ms step_avg:35.68ms
step:657/1900 train_time:23470ms step_avg:35.72ms
step:658/1900 train_time:23531ms step_avg:35.76ms
step:659/1900 train_time:23593ms step_avg:35.80ms
step:660/1900 train_time:23654ms step_avg:35.84ms
step:661/1900 train_time:23715ms step_avg:35.88ms
step:662/1900 train_time:23776ms step_avg:35.92ms
step:663/1900 train_time:23839ms step_avg:35.96ms
step:664/1900 train_time:23900ms step_avg:35.99ms
step:665/1900 train_time:23961ms step_avg:36.03ms
step:666/1900 train_time:24022ms step_avg:36.07ms
step:667/1900 train_time:24083ms step_avg:36.11ms
step:668/1900 train_time:24144ms step_avg:36.14ms
step:669/1900 train_time:24206ms step_avg:36.18ms
step:670/1900 train_time:24267ms step_avg:36.22ms
step:671/1900 train_time:24329ms step_avg:36.26ms
step:672/1900 train_time:24390ms step_avg:36.29ms
step:673/1900 train_time:24452ms step_avg:36.33ms
step:674/1900 train_time:24512ms step_avg:36.37ms
step:675/1900 train_time:24574ms step_avg:36.41ms
step:676/1900 train_time:24635ms step_avg:36.44ms
step:677/1900 train_time:24696ms step_avg:36.48ms
step:678/1900 train_time:24757ms step_avg:36.51ms
step:679/1900 train_time:24819ms step_avg:36.55ms
step:680/1900 train_time:24880ms step_avg:36.59ms
step:681/1900 train_time:24941ms step_avg:36.62ms
step:682/1900 train_time:25002ms step_avg:36.66ms
step:683/1900 train_time:25064ms step_avg:36.70ms
step:684/1900 train_time:25125ms step_avg:36.73ms
step:685/1900 train_time:25186ms step_avg:36.77ms
step:686/1900 train_time:25247ms step_avg:36.80ms
step:687/1900 train_time:25310ms step_avg:36.84ms
step:688/1900 train_time:25370ms step_avg:36.88ms
step:689/1900 train_time:25432ms step_avg:36.91ms
step:690/1900 train_time:25493ms step_avg:36.95ms
step:691/1900 train_time:25554ms step_avg:36.98ms
step:692/1900 train_time:25615ms step_avg:37.02ms
step:693/1900 train_time:25677ms step_avg:37.05ms
step:694/1900 train_time:25738ms step_avg:37.09ms
step:695/1900 train_time:25800ms step_avg:37.12ms
step:696/1900 train_time:25861ms step_avg:37.16ms
step:697/1900 train_time:25923ms step_avg:37.19ms
step:698/1900 train_time:25984ms step_avg:37.23ms
step:699/1900 train_time:26045ms step_avg:37.26ms
step:700/1900 train_time:26106ms step_avg:37.29ms
step:701/1900 train_time:26167ms step_avg:37.33ms
step:702/1900 train_time:26228ms step_avg:37.36ms
step:703/1900 train_time:26290ms step_avg:37.40ms
step:704/1900 train_time:26351ms step_avg:37.43ms
step:705/1900 train_time:26412ms step_avg:37.46ms
step:706/1900 train_time:26473ms step_avg:37.50ms
step:707/1900 train_time:26534ms step_avg:37.53ms
step:708/1900 train_time:26595ms step_avg:37.56ms
step:709/1900 train_time:26657ms step_avg:37.60ms
step:710/1900 train_time:26718ms step_avg:37.63ms
step:711/1900 train_time:26780ms step_avg:37.67ms
step:712/1900 train_time:26841ms step_avg:37.70ms
step:713/1900 train_time:26903ms step_avg:37.73ms
step:714/1900 train_time:26963ms step_avg:37.76ms
step:715/1900 train_time:27025ms step_avg:37.80ms
step:716/1900 train_time:27086ms step_avg:37.83ms
step:717/1900 train_time:27148ms step_avg:37.86ms
step:718/1900 train_time:27209ms step_avg:37.89ms
step:719/1900 train_time:27271ms step_avg:37.93ms
step:720/1900 train_time:27331ms step_avg:37.96ms
step:721/1900 train_time:27393ms step_avg:37.99ms
step:722/1900 train_time:27454ms step_avg:38.02ms
step:723/1900 train_time:27515ms step_avg:38.06ms
step:724/1900 train_time:27576ms step_avg:38.09ms
step:725/1900 train_time:27638ms step_avg:38.12ms
step:726/1900 train_time:27699ms step_avg:38.15ms
step:727/1900 train_time:27761ms step_avg:38.19ms
step:728/1900 train_time:27822ms step_avg:38.22ms
step:729/1900 train_time:27884ms step_avg:38.25ms
step:730/1900 train_time:27944ms step_avg:38.28ms
step:731/1900 train_time:28006ms step_avg:38.31ms
step:732/1900 train_time:28067ms step_avg:38.34ms
step:733/1900 train_time:28128ms step_avg:38.37ms
step:734/1900 train_time:28189ms step_avg:38.40ms
step:735/1900 train_time:28251ms step_avg:38.44ms
step:736/1900 train_time:28312ms step_avg:38.47ms
step:737/1900 train_time:28373ms step_avg:38.50ms
step:738/1900 train_time:28434ms step_avg:38.53ms
step:739/1900 train_time:28496ms step_avg:38.56ms
step:740/1900 train_time:28557ms step_avg:38.59ms
step:741/1900 train_time:28618ms step_avg:38.62ms
step:742/1900 train_time:28679ms step_avg:38.65ms
step:743/1900 train_time:28741ms step_avg:38.68ms
step:744/1900 train_time:28802ms step_avg:38.71ms
step:745/1900 train_time:28863ms step_avg:38.74ms
step:746/1900 train_time:28924ms step_avg:38.77ms
step:747/1900 train_time:28986ms step_avg:38.80ms
step:748/1900 train_time:29047ms step_avg:38.83ms
step:749/1900 train_time:29109ms step_avg:38.86ms
step:750/1900 train_time:29170ms step_avg:38.89ms
step:750/1900 val_loss:4.0261 train_time:29234ms step_avg:38.98ms
step:751/1900 train_time:29256ms step_avg:38.96ms
step:752/1900 train_time:29295ms step_avg:38.96ms
step:753/1900 train_time:29358ms step_avg:38.99ms
step:754/1900 train_time:29421ms step_avg:39.02ms
step:755/1900 train_time:29483ms step_avg:39.05ms
step:756/1900 train_time:29544ms step_avg:39.08ms
step:757/1900 train_time:29606ms step_avg:39.11ms
step:758/1900 train_time:29666ms step_avg:39.14ms
step:759/1900 train_time:29728ms step_avg:39.17ms
step:760/1900 train_time:29788ms step_avg:39.20ms
step:761/1900 train_time:29849ms step_avg:39.22ms
step:762/1900 train_time:29910ms step_avg:39.25ms
step:763/1900 train_time:29970ms step_avg:39.28ms
step:764/1900 train_time:30031ms step_avg:39.31ms
step:765/1900 train_time:30093ms step_avg:39.34ms
step:766/1900 train_time:30154ms step_avg:39.37ms
step:767/1900 train_time:30218ms step_avg:39.40ms
step:768/1900 train_time:30279ms step_avg:39.43ms
step:769/1900 train_time:30341ms step_avg:39.46ms
step:770/1900 train_time:30403ms step_avg:39.48ms
step:771/1900 train_time:30466ms step_avg:39.51ms
step:772/1900 train_time:30527ms step_avg:39.54ms
step:773/1900 train_time:30589ms step_avg:39.57ms
step:774/1900 train_time:30649ms step_avg:39.60ms
step:775/1900 train_time:30711ms step_avg:39.63ms
step:776/1900 train_time:30772ms step_avg:39.66ms
step:777/1900 train_time:30833ms step_avg:39.68ms
step:778/1900 train_time:30894ms step_avg:39.71ms
step:779/1900 train_time:30956ms step_avg:39.74ms
step:780/1900 train_time:31016ms step_avg:39.76ms
step:781/1900 train_time:31078ms step_avg:39.79ms
step:782/1900 train_time:31139ms step_avg:39.82ms
step:783/1900 train_time:31201ms step_avg:39.85ms
step:784/1900 train_time:31261ms step_avg:39.87ms
step:785/1900 train_time:31324ms step_avg:39.90ms
step:786/1900 train_time:31385ms step_avg:39.93ms
step:787/1900 train_time:31448ms step_avg:39.96ms
step:788/1900 train_time:31509ms step_avg:39.99ms
step:789/1900 train_time:31571ms step_avg:40.01ms
step:790/1900 train_time:31632ms step_avg:40.04ms
step:791/1900 train_time:31694ms step_avg:40.07ms
step:792/1900 train_time:31754ms step_avg:40.09ms
step:793/1900 train_time:31816ms step_avg:40.12ms
step:794/1900 train_time:31876ms step_avg:40.15ms
step:795/1900 train_time:31938ms step_avg:40.17ms
step:796/1900 train_time:31998ms step_avg:40.20ms
step:797/1900 train_time:32060ms step_avg:40.23ms
step:798/1900 train_time:32121ms step_avg:40.25ms
step:799/1900 train_time:32183ms step_avg:40.28ms
step:800/1900 train_time:32244ms step_avg:40.30ms
step:801/1900 train_time:32306ms step_avg:40.33ms
step:802/1900 train_time:32367ms step_avg:40.36ms
step:803/1900 train_time:32429ms step_avg:40.38ms
step:804/1900 train_time:32489ms step_avg:40.41ms
step:805/1900 train_time:32551ms step_avg:40.44ms
step:806/1900 train_time:32613ms step_avg:40.46ms
step:807/1900 train_time:32675ms step_avg:40.49ms
step:808/1900 train_time:32735ms step_avg:40.51ms
step:809/1900 train_time:32797ms step_avg:40.54ms
step:810/1900 train_time:32857ms step_avg:40.56ms
step:811/1900 train_time:32918ms step_avg:40.59ms
step:812/1900 train_time:32979ms step_avg:40.61ms
step:813/1900 train_time:33041ms step_avg:40.64ms
step:814/1900 train_time:33101ms step_avg:40.66ms
step:815/1900 train_time:33163ms step_avg:40.69ms
step:816/1900 train_time:33224ms step_avg:40.72ms
step:817/1900 train_time:33286ms step_avg:40.74ms
step:818/1900 train_time:33347ms step_avg:40.77ms
step:819/1900 train_time:33409ms step_avg:40.79ms
step:820/1900 train_time:33469ms step_avg:40.82ms
step:821/1900 train_time:33532ms step_avg:40.84ms
step:822/1900 train_time:33593ms step_avg:40.87ms
step:823/1900 train_time:33655ms step_avg:40.89ms
step:824/1900 train_time:33716ms step_avg:40.92ms
step:825/1900 train_time:33778ms step_avg:40.94ms
step:826/1900 train_time:33838ms step_avg:40.97ms
step:827/1900 train_time:33900ms step_avg:40.99ms
step:828/1900 train_time:33961ms step_avg:41.02ms
step:829/1900 train_time:34022ms step_avg:41.04ms
step:830/1900 train_time:34083ms step_avg:41.06ms
step:831/1900 train_time:34145ms step_avg:41.09ms
step:832/1900 train_time:34206ms step_avg:41.11ms
step:833/1900 train_time:34267ms step_avg:41.14ms
step:834/1900 train_time:34329ms step_avg:41.16ms
step:835/1900 train_time:34390ms step_avg:41.19ms
step:836/1900 train_time:34451ms step_avg:41.21ms
step:837/1900 train_time:34513ms step_avg:41.23ms
step:838/1900 train_time:34574ms step_avg:41.26ms
step:839/1900 train_time:34637ms step_avg:41.28ms
step:840/1900 train_time:34697ms step_avg:41.31ms
step:841/1900 train_time:34759ms step_avg:41.33ms
step:842/1900 train_time:34819ms step_avg:41.35ms
step:843/1900 train_time:34881ms step_avg:41.38ms
step:844/1900 train_time:34942ms step_avg:41.40ms
step:845/1900 train_time:35004ms step_avg:41.42ms
step:846/1900 train_time:35064ms step_avg:41.45ms
step:847/1900 train_time:35126ms step_avg:41.47ms
step:848/1900 train_time:35187ms step_avg:41.49ms
step:849/1900 train_time:35249ms step_avg:41.52ms
step:850/1900 train_time:35310ms step_avg:41.54ms
step:851/1900 train_time:35371ms step_avg:41.56ms
step:852/1900 train_time:35433ms step_avg:41.59ms
step:853/1900 train_time:35495ms step_avg:41.61ms
step:854/1900 train_time:35555ms step_avg:41.63ms
step:855/1900 train_time:35616ms step_avg:41.66ms
step:856/1900 train_time:35678ms step_avg:41.68ms
step:857/1900 train_time:35739ms step_avg:41.70ms
step:858/1900 train_time:35800ms step_avg:41.72ms
step:859/1900 train_time:35862ms step_avg:41.75ms
step:860/1900 train_time:35923ms step_avg:41.77ms
step:861/1900 train_time:35985ms step_avg:41.79ms
step:862/1900 train_time:36045ms step_avg:41.82ms
step:863/1900 train_time:36107ms step_avg:41.84ms
step:864/1900 train_time:36168ms step_avg:41.86ms
step:865/1900 train_time:36230ms step_avg:41.88ms
step:866/1900 train_time:36290ms step_avg:41.91ms
step:867/1900 train_time:36352ms step_avg:41.93ms
step:868/1900 train_time:36413ms step_avg:41.95ms
step:869/1900 train_time:36475ms step_avg:41.97ms
step:870/1900 train_time:36537ms step_avg:42.00ms
step:871/1900 train_time:36598ms step_avg:42.02ms
step:872/1900 train_time:36659ms step_avg:42.04ms
step:873/1900 train_time:36721ms step_avg:42.06ms
step:874/1900 train_time:36782ms step_avg:42.08ms
step:875/1900 train_time:36844ms step_avg:42.11ms
step:876/1900 train_time:36905ms step_avg:42.13ms
step:877/1900 train_time:36966ms step_avg:42.15ms
step:878/1900 train_time:37027ms step_avg:42.17ms
step:879/1900 train_time:37089ms step_avg:42.19ms
step:880/1900 train_time:37149ms step_avg:42.22ms
step:881/1900 train_time:37211ms step_avg:42.24ms
step:882/1900 train_time:37272ms step_avg:42.26ms
step:883/1900 train_time:37334ms step_avg:42.28ms
step:884/1900 train_time:37395ms step_avg:42.30ms
step:885/1900 train_time:37456ms step_avg:42.32ms
step:886/1900 train_time:37517ms step_avg:42.34ms
step:887/1900 train_time:37579ms step_avg:42.37ms
step:888/1900 train_time:37640ms step_avg:42.39ms
step:889/1900 train_time:37702ms step_avg:42.41ms
step:890/1900 train_time:37763ms step_avg:42.43ms
step:891/1900 train_time:37825ms step_avg:42.45ms
step:892/1900 train_time:37886ms step_avg:42.47ms
step:893/1900 train_time:37948ms step_avg:42.49ms
step:894/1900 train_time:38008ms step_avg:42.52ms
step:895/1900 train_time:38070ms step_avg:42.54ms
step:896/1900 train_time:38131ms step_avg:42.56ms
step:897/1900 train_time:38193ms step_avg:42.58ms
step:898/1900 train_time:38255ms step_avg:42.60ms
step:899/1900 train_time:38317ms step_avg:42.62ms
step:900/1900 train_time:38378ms step_avg:42.64ms
step:901/1900 train_time:38440ms step_avg:42.66ms
step:902/1900 train_time:38500ms step_avg:42.68ms
step:903/1900 train_time:38562ms step_avg:42.70ms
step:904/1900 train_time:38623ms step_avg:42.72ms
step:905/1900 train_time:38685ms step_avg:42.75ms
step:906/1900 train_time:38746ms step_avg:42.77ms
step:907/1900 train_time:38807ms step_avg:42.79ms
step:908/1900 train_time:38869ms step_avg:42.81ms
step:909/1900 train_time:38930ms step_avg:42.83ms
step:910/1900 train_time:38992ms step_avg:42.85ms
step:911/1900 train_time:39053ms step_avg:42.87ms
step:912/1900 train_time:39114ms step_avg:42.89ms
step:913/1900 train_time:39175ms step_avg:42.91ms
step:914/1900 train_time:39236ms step_avg:42.93ms
step:915/1900 train_time:39298ms step_avg:42.95ms
step:916/1900 train_time:39359ms step_avg:42.97ms
step:917/1900 train_time:39421ms step_avg:42.99ms
step:918/1900 train_time:39482ms step_avg:43.01ms
step:919/1900 train_time:39544ms step_avg:43.03ms
step:920/1900 train_time:39605ms step_avg:43.05ms
step:921/1900 train_time:39667ms step_avg:43.07ms
step:922/1900 train_time:39727ms step_avg:43.09ms
step:923/1900 train_time:39789ms step_avg:43.11ms
step:924/1900 train_time:39850ms step_avg:43.13ms
step:925/1900 train_time:39912ms step_avg:43.15ms
step:926/1900 train_time:39973ms step_avg:43.17ms
step:927/1900 train_time:40035ms step_avg:43.19ms
step:928/1900 train_time:40096ms step_avg:43.21ms
step:929/1900 train_time:40157ms step_avg:43.23ms
step:930/1900 train_time:40218ms step_avg:43.25ms
step:931/1900 train_time:40280ms step_avg:43.27ms
step:932/1900 train_time:40341ms step_avg:43.28ms
step:933/1900 train_time:40402ms step_avg:43.30ms
step:934/1900 train_time:40464ms step_avg:43.32ms
step:935/1900 train_time:40526ms step_avg:43.34ms
step:936/1900 train_time:40587ms step_avg:43.36ms
step:937/1900 train_time:40649ms step_avg:43.38ms
step:938/1900 train_time:40710ms step_avg:43.40ms
step:939/1900 train_time:40772ms step_avg:43.42ms
step:940/1900 train_time:40833ms step_avg:43.44ms
step:941/1900 train_time:40895ms step_avg:43.46ms
step:942/1900 train_time:40956ms step_avg:43.48ms
step:943/1900 train_time:41018ms step_avg:43.50ms
step:944/1900 train_time:41079ms step_avg:43.52ms
step:945/1900 train_time:41140ms step_avg:43.53ms
step:946/1900 train_time:41201ms step_avg:43.55ms
step:947/1900 train_time:41262ms step_avg:43.57ms
step:948/1900 train_time:41324ms step_avg:43.59ms
step:949/1900 train_time:41385ms step_avg:43.61ms
step:950/1900 train_time:41446ms step_avg:43.63ms
step:951/1900 train_time:41509ms step_avg:43.65ms
step:952/1900 train_time:41569ms step_avg:43.67ms
step:953/1900 train_time:41631ms step_avg:43.68ms
step:954/1900 train_time:41691ms step_avg:43.70ms
step:955/1900 train_time:41754ms step_avg:43.72ms
step:956/1900 train_time:41815ms step_avg:43.74ms
step:957/1900 train_time:41876ms step_avg:43.76ms
step:958/1900 train_time:41937ms step_avg:43.78ms
step:959/1900 train_time:41999ms step_avg:43.79ms
step:960/1900 train_time:42060ms step_avg:43.81ms
step:961/1900 train_time:42122ms step_avg:43.83ms
step:962/1900 train_time:42183ms step_avg:43.85ms
step:963/1900 train_time:42245ms step_avg:43.87ms
step:964/1900 train_time:42306ms step_avg:43.89ms
step:965/1900 train_time:42368ms step_avg:43.90ms
step:966/1900 train_time:42429ms step_avg:43.92ms
step:967/1900 train_time:42491ms step_avg:43.94ms
step:968/1900 train_time:42551ms step_avg:43.96ms
step:969/1900 train_time:42612ms step_avg:43.98ms
step:970/1900 train_time:42674ms step_avg:43.99ms
step:971/1900 train_time:42736ms step_avg:44.01ms
step:972/1900 train_time:42797ms step_avg:44.03ms
step:973/1900 train_time:42859ms step_avg:44.05ms
step:974/1900 train_time:42919ms step_avg:44.06ms
step:975/1900 train_time:42981ms step_avg:44.08ms
step:976/1900 train_time:43042ms step_avg:44.10ms
step:977/1900 train_time:43104ms step_avg:44.12ms
step:978/1900 train_time:43165ms step_avg:44.14ms
step:979/1900 train_time:43227ms step_avg:44.15ms
step:980/1900 train_time:43287ms step_avg:44.17ms
step:981/1900 train_time:43349ms step_avg:44.19ms
step:982/1900 train_time:43410ms step_avg:44.21ms
step:983/1900 train_time:43472ms step_avg:44.22ms
step:984/1900 train_time:43533ms step_avg:44.24ms
step:985/1900 train_time:43595ms step_avg:44.26ms
step:986/1900 train_time:43655ms step_avg:44.28ms
step:987/1900 train_time:43717ms step_avg:44.29ms
step:988/1900 train_time:43777ms step_avg:44.31ms
step:989/1900 train_time:43840ms step_avg:44.33ms
step:990/1900 train_time:43901ms step_avg:44.34ms
step:991/1900 train_time:43962ms step_avg:44.36ms
step:992/1900 train_time:44023ms step_avg:44.38ms
step:993/1900 train_time:44085ms step_avg:44.40ms
step:994/1900 train_time:44146ms step_avg:44.41ms
step:995/1900 train_time:44208ms step_avg:44.43ms
step:996/1900 train_time:44269ms step_avg:44.45ms
step:997/1900 train_time:44330ms step_avg:44.46ms
step:998/1900 train_time:44391ms step_avg:44.48ms
step:999/1900 train_time:44453ms step_avg:44.50ms
step:1000/1900 train_time:44515ms step_avg:44.51ms
step:1000/1900 val_loss:3.7762 train_time:44579ms step_avg:44.58ms
step:1001/1900 train_time:44600ms step_avg:44.55ms
step:1002/1900 train_time:44641ms step_avg:44.55ms
step:1003/1900 train_time:44706ms step_avg:44.57ms
step:1004/1900 train_time:44769ms step_avg:44.59ms
step:1005/1900 train_time:44831ms step_avg:44.61ms
step:1006/1900 train_time:44891ms step_avg:44.62ms
step:1007/1900 train_time:44953ms step_avg:44.64ms
step:1008/1900 train_time:45015ms step_avg:44.66ms
step:1009/1900 train_time:45076ms step_avg:44.67ms
step:1010/1900 train_time:45137ms step_avg:44.69ms
step:1011/1900 train_time:45199ms step_avg:44.71ms
step:1012/1900 train_time:45259ms step_avg:44.72ms
step:1013/1900 train_time:45320ms step_avg:44.74ms
step:1014/1900 train_time:45380ms step_avg:44.75ms
step:1015/1900 train_time:45441ms step_avg:44.77ms
step:1016/1900 train_time:45503ms step_avg:44.79ms
step:1017/1900 train_time:45566ms step_avg:44.80ms
step:1018/1900 train_time:45628ms step_avg:44.82ms
step:1019/1900 train_time:45690ms step_avg:44.84ms
step:1020/1900 train_time:45752ms step_avg:44.85ms
step:1021/1900 train_time:45814ms step_avg:44.87ms
step:1022/1900 train_time:45875ms step_avg:44.89ms
step:1023/1900 train_time:45938ms step_avg:44.90ms
step:1024/1900 train_time:45998ms step_avg:44.92ms
step:1025/1900 train_time:46060ms step_avg:44.94ms
step:1026/1900 train_time:46121ms step_avg:44.95ms
step:1027/1900 train_time:46183ms step_avg:44.97ms
step:1028/1900 train_time:46243ms step_avg:44.98ms
step:1029/1900 train_time:46304ms step_avg:45.00ms
step:1030/1900 train_time:46365ms step_avg:45.01ms
step:1031/1900 train_time:46426ms step_avg:45.03ms
step:1032/1900 train_time:46487ms step_avg:45.05ms
step:1033/1900 train_time:46549ms step_avg:45.06ms
step:1034/1900 train_time:46610ms step_avg:45.08ms
step:1035/1900 train_time:46672ms step_avg:45.09ms
step:1036/1900 train_time:46733ms step_avg:45.11ms
step:1037/1900 train_time:46795ms step_avg:45.13ms
step:1038/1900 train_time:46856ms step_avg:45.14ms
step:1039/1900 train_time:46918ms step_avg:45.16ms
step:1040/1900 train_time:46979ms step_avg:45.17ms
step:1041/1900 train_time:47041ms step_avg:45.19ms
step:1042/1900 train_time:47102ms step_avg:45.20ms
step:1043/1900 train_time:47163ms step_avg:45.22ms
step:1044/1900 train_time:47224ms step_avg:45.23ms
step:1045/1900 train_time:47285ms step_avg:45.25ms
step:1046/1900 train_time:47345ms step_avg:45.26ms
step:1047/1900 train_time:47407ms step_avg:45.28ms
step:1048/1900 train_time:47467ms step_avg:45.29ms
step:1049/1900 train_time:47529ms step_avg:45.31ms
step:1050/1900 train_time:47589ms step_avg:45.32ms
step:1051/1900 train_time:47651ms step_avg:45.34ms
step:1052/1900 train_time:47712ms step_avg:45.35ms
step:1053/1900 train_time:47774ms step_avg:45.37ms
step:1054/1900 train_time:47836ms step_avg:45.38ms
step:1055/1900 train_time:47897ms step_avg:45.40ms
step:1056/1900 train_time:47958ms step_avg:45.41ms
step:1057/1900 train_time:48020ms step_avg:45.43ms
step:1058/1900 train_time:48082ms step_avg:45.45ms
step:1059/1900 train_time:48144ms step_avg:45.46ms
step:1060/1900 train_time:48204ms step_avg:45.48ms
step:1061/1900 train_time:48266ms step_avg:45.49ms
step:1062/1900 train_time:48326ms step_avg:45.50ms
step:1063/1900 train_time:48387ms step_avg:45.52ms
step:1064/1900 train_time:48448ms step_avg:45.53ms
step:1065/1900 train_time:48510ms step_avg:45.55ms
step:1066/1900 train_time:48571ms step_avg:45.56ms
step:1067/1900 train_time:48633ms step_avg:45.58ms
step:1068/1900 train_time:48694ms step_avg:45.59ms
step:1069/1900 train_time:48756ms step_avg:45.61ms
step:1070/1900 train_time:48816ms step_avg:45.62ms
step:1071/1900 train_time:48878ms step_avg:45.64ms
step:1072/1900 train_time:48939ms step_avg:45.65ms
step:1073/1900 train_time:49001ms step_avg:45.67ms
step:1074/1900 train_time:49062ms step_avg:45.68ms
step:1075/1900 train_time:49124ms step_avg:45.70ms
step:1076/1900 train_time:49184ms step_avg:45.71ms
step:1077/1900 train_time:49246ms step_avg:45.73ms
step:1078/1900 train_time:49307ms step_avg:45.74ms
step:1079/1900 train_time:49368ms step_avg:45.75ms
step:1080/1900 train_time:49429ms step_avg:45.77ms
step:1081/1900 train_time:49491ms step_avg:45.78ms
step:1082/1900 train_time:49552ms step_avg:45.80ms
step:1083/1900 train_time:49613ms step_avg:45.81ms
step:1084/1900 train_time:49674ms step_avg:45.82ms
step:1085/1900 train_time:49736ms step_avg:45.84ms
step:1086/1900 train_time:49797ms step_avg:45.85ms
step:1087/1900 train_time:49859ms step_avg:45.87ms
step:1088/1900 train_time:49920ms step_avg:45.88ms
step:1089/1900 train_time:49981ms step_avg:45.90ms
step:1090/1900 train_time:50042ms step_avg:45.91ms
step:1091/1900 train_time:50104ms step_avg:45.93ms
step:1092/1900 train_time:50165ms step_avg:45.94ms
step:1093/1900 train_time:50226ms step_avg:45.95ms
step:1094/1900 train_time:50287ms step_avg:45.97ms
step:1095/1900 train_time:50349ms step_avg:45.98ms
step:1096/1900 train_time:50409ms step_avg:45.99ms
step:1097/1900 train_time:50471ms step_avg:46.01ms
step:1098/1900 train_time:50532ms step_avg:46.02ms
step:1099/1900 train_time:50593ms step_avg:46.04ms
step:1100/1900 train_time:50654ms step_avg:46.05ms
step:1101/1900 train_time:50715ms step_avg:46.06ms
step:1102/1900 train_time:50776ms step_avg:46.08ms
step:1103/1900 train_time:50838ms step_avg:46.09ms
step:1104/1900 train_time:50899ms step_avg:46.10ms
step:1105/1900 train_time:50961ms step_avg:46.12ms
step:1106/1900 train_time:51022ms step_avg:46.13ms
step:1107/1900 train_time:51084ms step_avg:46.15ms
step:1108/1900 train_time:51145ms step_avg:46.16ms
step:1109/1900 train_time:51207ms step_avg:46.17ms
step:1110/1900 train_time:51268ms step_avg:46.19ms
step:1111/1900 train_time:51330ms step_avg:46.20ms
step:1112/1900 train_time:51391ms step_avg:46.21ms
step:1113/1900 train_time:51452ms step_avg:46.23ms
step:1114/1900 train_time:51513ms step_avg:46.24ms
step:1115/1900 train_time:51574ms step_avg:46.26ms
step:1116/1900 train_time:51635ms step_avg:46.27ms
step:1117/1900 train_time:51697ms step_avg:46.28ms
step:1118/1900 train_time:51759ms step_avg:46.30ms
step:1119/1900 train_time:51820ms step_avg:46.31ms
step:1120/1900 train_time:51881ms step_avg:46.32ms
step:1121/1900 train_time:51944ms step_avg:46.34ms
step:1122/1900 train_time:52005ms step_avg:46.35ms
step:1123/1900 train_time:52067ms step_avg:46.36ms
step:1124/1900 train_time:52127ms step_avg:46.38ms
step:1125/1900 train_time:52189ms step_avg:46.39ms
step:1126/1900 train_time:52250ms step_avg:46.40ms
step:1127/1900 train_time:52312ms step_avg:46.42ms
step:1128/1900 train_time:52373ms step_avg:46.43ms
step:1129/1900 train_time:52435ms step_avg:46.44ms
step:1130/1900 train_time:52496ms step_avg:46.46ms
step:1131/1900 train_time:52557ms step_avg:46.47ms
step:1132/1900 train_time:52618ms step_avg:46.48ms
step:1133/1900 train_time:52680ms step_avg:46.50ms
step:1134/1900 train_time:52741ms step_avg:46.51ms
step:1135/1900 train_time:52803ms step_avg:46.52ms
step:1136/1900 train_time:52864ms step_avg:46.53ms
step:1137/1900 train_time:52926ms step_avg:46.55ms
step:1138/1900 train_time:52986ms step_avg:46.56ms
step:1139/1900 train_time:53048ms step_avg:46.57ms
step:1140/1900 train_time:53109ms step_avg:46.59ms
step:1141/1900 train_time:53171ms step_avg:46.60ms
step:1142/1900 train_time:53232ms step_avg:46.61ms
step:1143/1900 train_time:53294ms step_avg:46.63ms
step:1144/1900 train_time:53355ms step_avg:46.64ms
step:1145/1900 train_time:53416ms step_avg:46.65ms
step:1146/1900 train_time:53477ms step_avg:46.66ms
step:1147/1900 train_time:53538ms step_avg:46.68ms
step:1148/1900 train_time:53599ms step_avg:46.69ms
step:1149/1900 train_time:53661ms step_avg:46.70ms
step:1150/1900 train_time:53722ms step_avg:46.71ms
step:1151/1900 train_time:53784ms step_avg:46.73ms
step:1152/1900 train_time:53845ms step_avg:46.74ms
step:1153/1900 train_time:53907ms step_avg:46.75ms
step:1154/1900 train_time:53968ms step_avg:46.77ms
step:1155/1900 train_time:54029ms step_avg:46.78ms
step:1156/1900 train_time:54090ms step_avg:46.79ms
step:1157/1900 train_time:54152ms step_avg:46.80ms
step:1158/1900 train_time:54213ms step_avg:46.82ms
step:1159/1900 train_time:54275ms step_avg:46.83ms
step:1160/1900 train_time:54336ms step_avg:46.84ms
step:1161/1900 train_time:54398ms step_avg:46.85ms
step:1162/1900 train_time:54459ms step_avg:46.87ms
step:1163/1900 train_time:54521ms step_avg:46.88ms
step:1164/1900 train_time:54582ms step_avg:46.89ms
step:1165/1900 train_time:54644ms step_avg:46.90ms
step:1166/1900 train_time:54704ms step_avg:46.92ms
step:1167/1900 train_time:54766ms step_avg:46.93ms
step:1168/1900 train_time:54827ms step_avg:46.94ms
step:1169/1900 train_time:54889ms step_avg:46.95ms
step:1170/1900 train_time:54950ms step_avg:46.97ms
step:1171/1900 train_time:55011ms step_avg:46.98ms
step:1172/1900 train_time:55072ms step_avg:46.99ms
step:1173/1900 train_time:55134ms step_avg:47.00ms
step:1174/1900 train_time:55195ms step_avg:47.01ms
step:1175/1900 train_time:55257ms step_avg:47.03ms
step:1176/1900 train_time:55318ms step_avg:47.04ms
step:1177/1900 train_time:55380ms step_avg:47.05ms
step:1178/1900 train_time:55440ms step_avg:47.06ms
step:1179/1900 train_time:55502ms step_avg:47.08ms
step:1180/1900 train_time:55563ms step_avg:47.09ms
step:1181/1900 train_time:55625ms step_avg:47.10ms
step:1182/1900 train_time:55686ms step_avg:47.11ms
step:1183/1900 train_time:55748ms step_avg:47.12ms
step:1184/1900 train_time:55808ms step_avg:47.14ms
step:1185/1900 train_time:55870ms step_avg:47.15ms
step:1186/1900 train_time:55931ms step_avg:47.16ms
step:1187/1900 train_time:55992ms step_avg:47.17ms
step:1188/1900 train_time:56053ms step_avg:47.18ms
step:1189/1900 train_time:56115ms step_avg:47.19ms
step:1190/1900 train_time:56176ms step_avg:47.21ms
step:1191/1900 train_time:56237ms step_avg:47.22ms
step:1192/1900 train_time:56298ms step_avg:47.23ms
step:1193/1900 train_time:56359ms step_avg:47.24ms
step:1194/1900 train_time:56420ms step_avg:47.25ms
step:1195/1900 train_time:56482ms step_avg:47.27ms
step:1196/1900 train_time:56543ms step_avg:47.28ms
step:1197/1900 train_time:56605ms step_avg:47.29ms
step:1198/1900 train_time:56665ms step_avg:47.30ms
step:1199/1900 train_time:56727ms step_avg:47.31ms
step:1200/1900 train_time:56788ms step_avg:47.32ms
step:1201/1900 train_time:56850ms step_avg:47.34ms
step:1202/1900 train_time:56910ms step_avg:47.35ms
step:1203/1900 train_time:56972ms step_avg:47.36ms
step:1204/1900 train_time:57033ms step_avg:47.37ms
step:1205/1900 train_time:57095ms step_avg:47.38ms
step:1206/1900 train_time:57156ms step_avg:47.39ms
step:1207/1900 train_time:57218ms step_avg:47.40ms
step:1208/1900 train_time:57279ms step_avg:47.42ms
step:1209/1900 train_time:57341ms step_avg:47.43ms
step:1210/1900 train_time:57402ms step_avg:47.44ms
step:1211/1900 train_time:57463ms step_avg:47.45ms
step:1212/1900 train_time:57524ms step_avg:47.46ms
step:1213/1900 train_time:57586ms step_avg:47.47ms
step:1214/1900 train_time:57647ms step_avg:47.49ms
step:1215/1900 train_time:57709ms step_avg:47.50ms
step:1216/1900 train_time:57770ms step_avg:47.51ms
step:1217/1900 train_time:57832ms step_avg:47.52ms
step:1218/1900 train_time:57893ms step_avg:47.53ms
step:1219/1900 train_time:57955ms step_avg:47.54ms
step:1220/1900 train_time:58015ms step_avg:47.55ms
step:1221/1900 train_time:58077ms step_avg:47.57ms
step:1222/1900 train_time:58138ms step_avg:47.58ms
step:1223/1900 train_time:58200ms step_avg:47.59ms
step:1224/1900 train_time:58260ms step_avg:47.60ms
step:1225/1900 train_time:58322ms step_avg:47.61ms
step:1226/1900 train_time:58383ms step_avg:47.62ms
step:1227/1900 train_time:58445ms step_avg:47.63ms
step:1228/1900 train_time:58505ms step_avg:47.64ms
step:1229/1900 train_time:58567ms step_avg:47.65ms
step:1230/1900 train_time:58628ms step_avg:47.67ms
step:1231/1900 train_time:58690ms step_avg:47.68ms
step:1232/1900 train_time:58751ms step_avg:47.69ms
step:1233/1900 train_time:58813ms step_avg:47.70ms
step:1234/1900 train_time:58874ms step_avg:47.71ms
step:1235/1900 train_time:58936ms step_avg:47.72ms
step:1236/1900 train_time:58996ms step_avg:47.73ms
step:1237/1900 train_time:59057ms step_avg:47.74ms
step:1238/1900 train_time:59118ms step_avg:47.75ms
step:1239/1900 train_time:59180ms step_avg:47.76ms
step:1240/1900 train_time:59242ms step_avg:47.78ms
step:1241/1900 train_time:59304ms step_avg:47.79ms
step:1242/1900 train_time:59391ms step_avg:47.82ms
step:1243/1900 train_time:59480ms step_avg:47.85ms
step:1244/1900 train_time:59568ms step_avg:47.88ms
step:1245/1900 train_time:59655ms step_avg:47.92ms
step:1246/1900 train_time:59743ms step_avg:47.95ms
step:1247/1900 train_time:59832ms step_avg:47.98ms
step:1248/1900 train_time:59919ms step_avg:48.01ms
step:1249/1900 train_time:60007ms step_avg:48.04ms
step:1250/1900 train_time:60094ms step_avg:48.08ms
step:1250/1900 val_loss:3.5456 train_time:60185ms step_avg:48.15ms
step:1251/1900 train_time:60206ms step_avg:48.13ms
step:1252/1900 train_time:60273ms step_avg:48.14ms
step:1253/1900 train_time:60364ms step_avg:48.18ms
step:1254/1900 train_time:60450ms step_avg:48.21ms
step:1255/1900 train_time:60538ms step_avg:48.24ms
step:1256/1900 train_time:60624ms step_avg:48.27ms
step:1257/1900 train_time:60713ms step_avg:48.30ms
step:1258/1900 train_time:60799ms step_avg:48.33ms
step:1259/1900 train_time:60885ms step_avg:48.36ms
step:1260/1900 train_time:60973ms step_avg:48.39ms
step:1261/1900 train_time:61061ms step_avg:48.42ms
step:1262/1900 train_time:61151ms step_avg:48.46ms
step:1263/1900 train_time:61241ms step_avg:48.49ms
step:1264/1900 train_time:61330ms step_avg:48.52ms
step:1265/1900 train_time:61419ms step_avg:48.55ms
step:1266/1900 train_time:61506ms step_avg:48.58ms
step:1267/1900 train_time:61593ms step_avg:48.61ms
step:1268/1900 train_time:61680ms step_avg:48.64ms
step:1269/1900 train_time:61768ms step_avg:48.67ms
step:1270/1900 train_time:61854ms step_avg:48.70ms
step:1271/1900 train_time:61942ms step_avg:48.73ms
step:1272/1900 train_time:62029ms step_avg:48.77ms
step:1273/1900 train_time:62119ms step_avg:48.80ms
step:1274/1900 train_time:62207ms step_avg:48.83ms
step:1275/1900 train_time:62296ms step_avg:48.86ms
step:1276/1900 train_time:62385ms step_avg:48.89ms
step:1277/1900 train_time:62474ms step_avg:48.92ms
step:1278/1900 train_time:62560ms step_avg:48.95ms
step:1279/1900 train_time:62648ms step_avg:48.98ms
step:1280/1900 train_time:62736ms step_avg:49.01ms
step:1281/1900 train_time:62824ms step_avg:49.04ms
step:1282/1900 train_time:62911ms step_avg:49.07ms
step:1283/1900 train_time:62999ms step_avg:49.10ms
step:1284/1900 train_time:63087ms step_avg:49.13ms
step:1285/1900 train_time:63176ms step_avg:49.16ms
step:1286/1900 train_time:63264ms step_avg:49.19ms
step:1287/1900 train_time:63354ms step_avg:49.23ms
step:1288/1900 train_time:63442ms step_avg:49.26ms
step:1289/1900 train_time:63530ms step_avg:49.29ms
step:1290/1900 train_time:63617ms step_avg:49.32ms
step:1291/1900 train_time:63706ms step_avg:49.35ms
step:1292/1900 train_time:63793ms step_avg:49.38ms
step:1293/1900 train_time:63880ms step_avg:49.40ms
step:1294/1900 train_time:63967ms step_avg:49.43ms
step:1295/1900 train_time:64055ms step_avg:49.46ms
step:1296/1900 train_time:64143ms step_avg:49.49ms
step:1297/1900 train_time:64231ms step_avg:49.52ms
step:1298/1900 train_time:64319ms step_avg:49.55ms
step:1299/1900 train_time:64408ms step_avg:49.58ms
step:1300/1900 train_time:64495ms step_avg:49.61ms
step:1301/1900 train_time:64583ms step_avg:49.64ms
step:1302/1900 train_time:64670ms step_avg:49.67ms
step:1303/1900 train_time:64759ms step_avg:49.70ms
step:1304/1900 train_time:64846ms step_avg:49.73ms
step:1305/1900 train_time:64934ms step_avg:49.76ms
step:1306/1900 train_time:65021ms step_avg:49.79ms
step:1307/1900 train_time:65109ms step_avg:49.82ms
step:1308/1900 train_time:65196ms step_avg:49.84ms
step:1309/1900 train_time:65285ms step_avg:49.87ms
step:1310/1900 train_time:65373ms step_avg:49.90ms
step:1311/1900 train_time:65463ms step_avg:49.93ms
step:1312/1900 train_time:65549ms step_avg:49.96ms
step:1313/1900 train_time:65638ms step_avg:49.99ms
step:1314/1900 train_time:65725ms step_avg:50.02ms
step:1315/1900 train_time:65814ms step_avg:50.05ms
step:1316/1900 train_time:65901ms step_avg:50.08ms
step:1317/1900 train_time:65989ms step_avg:50.11ms
step:1318/1900 train_time:66076ms step_avg:50.13ms
step:1319/1900 train_time:66164ms step_avg:50.16ms
step:1320/1900 train_time:66253ms step_avg:50.19ms
step:1321/1900 train_time:66341ms step_avg:50.22ms
step:1322/1900 train_time:66429ms step_avg:50.25ms
step:1323/1900 train_time:66517ms step_avg:50.28ms
step:1324/1900 train_time:66605ms step_avg:50.31ms
step:1325/1900 train_time:66694ms step_avg:50.33ms
step:1326/1900 train_time:66782ms step_avg:50.36ms
step:1327/1900 train_time:66870ms step_avg:50.39ms
step:1328/1900 train_time:66957ms step_avg:50.42ms
step:1329/1900 train_time:67046ms step_avg:50.45ms
step:1330/1900 train_time:67134ms step_avg:50.48ms
step:1331/1900 train_time:67222ms step_avg:50.51ms
step:1332/1900 train_time:67309ms step_avg:50.53ms
step:1333/1900 train_time:67398ms step_avg:50.56ms
step:1334/1900 train_time:67486ms step_avg:50.59ms
step:1335/1900 train_time:67575ms step_avg:50.62ms
step:1336/1900 train_time:67662ms step_avg:50.65ms
step:1337/1900 train_time:67751ms step_avg:50.67ms
step:1338/1900 train_time:67838ms step_avg:50.70ms
step:1339/1900 train_time:67926ms step_avg:50.73ms
step:1340/1900 train_time:68014ms step_avg:50.76ms
step:1341/1900 train_time:68103ms step_avg:50.78ms
step:1342/1900 train_time:68190ms step_avg:50.81ms
step:1343/1900 train_time:68278ms step_avg:50.84ms
step:1344/1900 train_time:68365ms step_avg:50.87ms
step:1345/1900 train_time:68455ms step_avg:50.90ms
step:1346/1900 train_time:68543ms step_avg:50.92ms
step:1347/1900 train_time:68631ms step_avg:50.95ms
step:1348/1900 train_time:68719ms step_avg:50.98ms
step:1349/1900 train_time:68807ms step_avg:51.01ms
step:1350/1900 train_time:68895ms step_avg:51.03ms
step:1351/1900 train_time:68983ms step_avg:51.06ms
step:1352/1900 train_time:69070ms step_avg:51.09ms
step:1353/1900 train_time:69159ms step_avg:51.12ms
step:1354/1900 train_time:69246ms step_avg:51.14ms
step:1355/1900 train_time:69335ms step_avg:51.17ms
step:1356/1900 train_time:69422ms step_avg:51.20ms
step:1357/1900 train_time:69510ms step_avg:51.22ms
step:1358/1900 train_time:69598ms step_avg:51.25ms
step:1359/1900 train_time:69687ms step_avg:51.28ms
step:1360/1900 train_time:69774ms step_avg:51.30ms
step:1361/1900 train_time:69863ms step_avg:51.33ms
step:1362/1900 train_time:69950ms step_avg:51.36ms
step:1363/1900 train_time:70038ms step_avg:51.39ms
step:1364/1900 train_time:70126ms step_avg:51.41ms
step:1365/1900 train_time:70216ms step_avg:51.44ms
step:1366/1900 train_time:70304ms step_avg:51.47ms
step:1367/1900 train_time:70393ms step_avg:51.49ms
step:1368/1900 train_time:70481ms step_avg:51.52ms
step:1369/1900 train_time:70568ms step_avg:51.55ms
step:1370/1900 train_time:70655ms step_avg:51.57ms
step:1371/1900 train_time:70744ms step_avg:51.60ms
step:1372/1900 train_time:70832ms step_avg:51.63ms
step:1373/1900 train_time:70920ms step_avg:51.65ms
step:1374/1900 train_time:71007ms step_avg:51.68ms
step:1375/1900 train_time:71095ms step_avg:51.71ms
step:1376/1900 train_time:71184ms step_avg:51.73ms
step:1377/1900 train_time:71272ms step_avg:51.76ms
step:1378/1900 train_time:71359ms step_avg:51.78ms
step:1379/1900 train_time:71448ms step_avg:51.81ms
step:1380/1900 train_time:71535ms step_avg:51.84ms
step:1381/1900 train_time:71623ms step_avg:51.86ms
step:1382/1900 train_time:71710ms step_avg:51.89ms
step:1383/1900 train_time:71799ms step_avg:51.92ms
step:1384/1900 train_time:71886ms step_avg:51.94ms
step:1385/1900 train_time:71974ms step_avg:51.97ms
step:1386/1900 train_time:72061ms step_avg:51.99ms
step:1387/1900 train_time:72150ms step_avg:52.02ms
step:1388/1900 train_time:72237ms step_avg:52.04ms
step:1389/1900 train_time:72326ms step_avg:52.07ms
step:1390/1900 train_time:72414ms step_avg:52.10ms
step:1391/1900 train_time:72501ms step_avg:52.12ms
step:1392/1900 train_time:72590ms step_avg:52.15ms
step:1393/1900 train_time:72678ms step_avg:52.17ms
step:1394/1900 train_time:72766ms step_avg:52.20ms
step:1395/1900 train_time:72853ms step_avg:52.22ms
step:1396/1900 train_time:72941ms step_avg:52.25ms
step:1397/1900 train_time:73030ms step_avg:52.28ms
step:1398/1900 train_time:73118ms step_avg:52.30ms
step:1399/1900 train_time:73206ms step_avg:52.33ms
step:1400/1900 train_time:73295ms step_avg:52.35ms
step:1401/1900 train_time:73385ms step_avg:52.38ms
step:1402/1900 train_time:73473ms step_avg:52.41ms
step:1403/1900 train_time:73562ms step_avg:52.43ms
step:1404/1900 train_time:73648ms step_avg:52.46ms
step:1405/1900 train_time:73737ms step_avg:52.48ms
step:1406/1900 train_time:73824ms step_avg:52.51ms
step:1407/1900 train_time:73913ms step_avg:52.53ms
step:1408/1900 train_time:74000ms step_avg:52.56ms
step:1409/1900 train_time:74088ms step_avg:52.58ms
step:1410/1900 train_time:74175ms step_avg:52.61ms
step:1411/1900 train_time:74263ms step_avg:52.63ms
step:1412/1900 train_time:74350ms step_avg:52.66ms
step:1413/1900 train_time:74439ms step_avg:52.68ms
step:1414/1900 train_time:74526ms step_avg:52.71ms
step:1415/1900 train_time:74615ms step_avg:52.73ms
step:1416/1900 train_time:74703ms step_avg:52.76ms
step:1417/1900 train_time:74792ms step_avg:52.78ms
step:1418/1900 train_time:74879ms step_avg:52.81ms
step:1419/1900 train_time:74968ms step_avg:52.83ms
step:1420/1900 train_time:75056ms step_avg:52.86ms
step:1421/1900 train_time:75145ms step_avg:52.88ms
step:1422/1900 train_time:75232ms step_avg:52.91ms
step:1423/1900 train_time:75321ms step_avg:52.93ms
step:1424/1900 train_time:75408ms step_avg:52.95ms
step:1425/1900 train_time:75496ms step_avg:52.98ms
step:1426/1900 train_time:75585ms step_avg:53.00ms
step:1427/1900 train_time:75673ms step_avg:53.03ms
step:1428/1900 train_time:75760ms step_avg:53.05ms
step:1429/1900 train_time:75848ms step_avg:53.08ms
step:1430/1900 train_time:75936ms step_avg:53.10ms
step:1431/1900 train_time:76024ms step_avg:53.13ms
step:1432/1900 train_time:76112ms step_avg:53.15ms
step:1433/1900 train_time:76201ms step_avg:53.18ms
step:1434/1900 train_time:76287ms step_avg:53.20ms
step:1435/1900 train_time:76376ms step_avg:53.22ms
step:1436/1900 train_time:76464ms step_avg:53.25ms
step:1437/1900 train_time:76552ms step_avg:53.27ms
step:1438/1900 train_time:76639ms step_avg:53.30ms
step:1439/1900 train_time:76728ms step_avg:53.32ms
step:1440/1900 train_time:76815ms step_avg:53.34ms
step:1441/1900 train_time:76904ms step_avg:53.37ms
step:1442/1900 train_time:76991ms step_avg:53.39ms
step:1443/1900 train_time:77079ms step_avg:53.42ms
step:1444/1900 train_time:77166ms step_avg:53.44ms
step:1445/1900 train_time:77256ms step_avg:53.46ms
step:1446/1900 train_time:77343ms step_avg:53.49ms
step:1447/1900 train_time:77432ms step_avg:53.51ms
step:1448/1900 train_time:77519ms step_avg:53.53ms
step:1449/1900 train_time:77608ms step_avg:53.56ms
step:1450/1900 train_time:77694ms step_avg:53.58ms
step:1451/1900 train_time:77782ms step_avg:53.61ms
step:1452/1900 train_time:77869ms step_avg:53.63ms
step:1453/1900 train_time:77957ms step_avg:53.65ms
step:1454/1900 train_time:78045ms step_avg:53.68ms
step:1455/1900 train_time:78134ms step_avg:53.70ms
step:1456/1900 train_time:78222ms step_avg:53.72ms
step:1457/1900 train_time:78310ms step_avg:53.75ms
step:1458/1900 train_time:78396ms step_avg:53.77ms
step:1459/1900 train_time:78486ms step_avg:53.79ms
step:1460/1900 train_time:78574ms step_avg:53.82ms
step:1461/1900 train_time:78662ms step_avg:53.84ms
step:1462/1900 train_time:78749ms step_avg:53.86ms
step:1463/1900 train_time:78838ms step_avg:53.89ms
step:1464/1900 train_time:78927ms step_avg:53.91ms
step:1465/1900 train_time:79016ms step_avg:53.94ms
step:1466/1900 train_time:79103ms step_avg:53.96ms
step:1467/1900 train_time:79191ms step_avg:53.98ms
step:1468/1900 train_time:79279ms step_avg:54.00ms
step:1469/1900 train_time:79366ms step_avg:54.03ms
step:1470/1900 train_time:79454ms step_avg:54.05ms
step:1471/1900 train_time:79543ms step_avg:54.07ms
step:1472/1900 train_time:79630ms step_avg:54.10ms
step:1473/1900 train_time:79719ms step_avg:54.12ms
step:1474/1900 train_time:79806ms step_avg:54.14ms
step:1475/1900 train_time:79894ms step_avg:54.17ms
step:1476/1900 train_time:79982ms step_avg:54.19ms
step:1477/1900 train_time:80070ms step_avg:54.21ms
step:1478/1900 train_time:80158ms step_avg:54.23ms
step:1479/1900 train_time:80247ms step_avg:54.26ms
step:1480/1900 train_time:80334ms step_avg:54.28ms
step:1481/1900 train_time:80422ms step_avg:54.30ms
step:1482/1900 train_time:80510ms step_avg:54.33ms
step:1483/1900 train_time:80598ms step_avg:54.35ms
step:1484/1900 train_time:80686ms step_avg:54.37ms
step:1485/1900 train_time:80773ms step_avg:54.39ms
step:1486/1900 train_time:80861ms step_avg:54.42ms
step:1487/1900 train_time:80949ms step_avg:54.44ms
step:1488/1900 train_time:81037ms step_avg:54.46ms
step:1489/1900 train_time:81125ms step_avg:54.48ms
step:1490/1900 train_time:81213ms step_avg:54.51ms
step:1491/1900 train_time:81302ms step_avg:54.53ms
step:1492/1900 train_time:81391ms step_avg:54.55ms
step:1493/1900 train_time:81479ms step_avg:54.57ms
step:1494/1900 train_time:81565ms step_avg:54.60ms
step:1495/1900 train_time:81655ms step_avg:54.62ms
step:1496/1900 train_time:81742ms step_avg:54.64ms
step:1497/1900 train_time:81831ms step_avg:54.66ms
step:1498/1900 train_time:81918ms step_avg:54.68ms
step:1499/1900 train_time:82007ms step_avg:54.71ms
step:1500/1900 train_time:82094ms step_avg:54.73ms
step:1500/1900 val_loss:3.4105 train_time:82185ms step_avg:54.79ms
step:1501/1900 train_time:82206ms step_avg:54.77ms
step:1502/1900 train_time:82272ms step_avg:54.77ms
step:1503/1900 train_time:82363ms step_avg:54.80ms
step:1504/1900 train_time:82453ms step_avg:54.82ms
step:1505/1900 train_time:82542ms step_avg:54.85ms
step:1506/1900 train_time:82628ms step_avg:54.87ms
step:1507/1900 train_time:82716ms step_avg:54.89ms
step:1508/1900 train_time:82803ms step_avg:54.91ms
step:1509/1900 train_time:82891ms step_avg:54.93ms
step:1510/1900 train_time:82978ms step_avg:54.95ms
step:1511/1900 train_time:83066ms step_avg:54.97ms
step:1512/1900 train_time:83155ms step_avg:55.00ms
step:1513/1900 train_time:83245ms step_avg:55.02ms
step:1514/1900 train_time:83334ms step_avg:55.04ms
step:1515/1900 train_time:83423ms step_avg:55.06ms
step:1516/1900 train_time:83510ms step_avg:55.09ms
step:1517/1900 train_time:83598ms step_avg:55.11ms
step:1518/1900 train_time:83685ms step_avg:55.13ms
step:1519/1900 train_time:83773ms step_avg:55.15ms
step:1520/1900 train_time:83859ms step_avg:55.17ms
step:1521/1900 train_time:83948ms step_avg:55.19ms
step:1522/1900 train_time:84034ms step_avg:55.21ms
step:1523/1900 train_time:84123ms step_avg:55.23ms
step:1524/1900 train_time:84211ms step_avg:55.26ms
step:1525/1900 train_time:84300ms step_avg:55.28ms
step:1526/1900 train_time:84388ms step_avg:55.30ms
step:1527/1900 train_time:84477ms step_avg:55.32ms
step:1528/1900 train_time:84564ms step_avg:55.34ms
step:1529/1900 train_time:84652ms step_avg:55.36ms
step:1530/1900 train_time:84739ms step_avg:55.38ms
step:1531/1900 train_time:84827ms step_avg:55.41ms
step:1532/1900 train_time:84914ms step_avg:55.43ms
step:1533/1900 train_time:85004ms step_avg:55.45ms
step:1534/1900 train_time:85091ms step_avg:55.47ms
step:1535/1900 train_time:85181ms step_avg:55.49ms
step:1536/1900 train_time:85268ms step_avg:55.51ms
step:1537/1900 train_time:85357ms step_avg:55.54ms
step:1538/1900 train_time:85445ms step_avg:55.56ms
step:1539/1900 train_time:85533ms step_avg:55.58ms
step:1540/1900 train_time:85620ms step_avg:55.60ms
step:1541/1900 train_time:85709ms step_avg:55.62ms
step:1542/1900 train_time:85795ms step_avg:55.64ms
step:1543/1900 train_time:85883ms step_avg:55.66ms
step:1544/1900 train_time:85971ms step_avg:55.68ms
step:1545/1900 train_time:86059ms step_avg:55.70ms
step:1546/1900 train_time:86146ms step_avg:55.72ms
step:1547/1900 train_time:86235ms step_avg:55.74ms
step:1548/1900 train_time:86324ms step_avg:55.76ms
step:1549/1900 train_time:86413ms step_avg:55.79ms
step:1550/1900 train_time:86501ms step_avg:55.81ms
step:1551/1900 train_time:86589ms step_avg:55.83ms
step:1552/1900 train_time:86676ms step_avg:55.85ms
step:1553/1900 train_time:86764ms step_avg:55.87ms
step:1554/1900 train_time:86851ms step_avg:55.89ms
step:1555/1900 train_time:86940ms step_avg:55.91ms
step:1556/1900 train_time:87027ms step_avg:55.93ms
step:1557/1900 train_time:87116ms step_avg:55.95ms
step:1558/1900 train_time:87203ms step_avg:55.97ms
step:1559/1900 train_time:87293ms step_avg:55.99ms
step:1560/1900 train_time:87382ms step_avg:56.01ms
step:1561/1900 train_time:87470ms step_avg:56.03ms
step:1562/1900 train_time:87557ms step_avg:56.05ms
step:1563/1900 train_time:87645ms step_avg:56.08ms
step:1564/1900 train_time:87732ms step_avg:56.09ms
step:1565/1900 train_time:87821ms step_avg:56.12ms
step:1566/1900 train_time:87907ms step_avg:56.13ms
step:1567/1900 train_time:87995ms step_avg:56.16ms
step:1568/1900 train_time:88083ms step_avg:56.18ms
step:1569/1900 train_time:88172ms step_avg:56.20ms
step:1570/1900 train_time:88260ms step_avg:56.22ms
step:1571/1900 train_time:88348ms step_avg:56.24ms
step:1572/1900 train_time:88435ms step_avg:56.26ms
step:1573/1900 train_time:88524ms step_avg:56.28ms
step:1574/1900 train_time:88611ms step_avg:56.30ms
step:1575/1900 train_time:88699ms step_avg:56.32ms
step:1576/1900 train_time:88786ms step_avg:56.34ms
step:1577/1900 train_time:88875ms step_avg:56.36ms
step:1578/1900 train_time:88963ms step_avg:56.38ms
step:1579/1900 train_time:89052ms step_avg:56.40ms
step:1580/1900 train_time:89139ms step_avg:56.42ms
step:1581/1900 train_time:89228ms step_avg:56.44ms
step:1582/1900 train_time:89315ms step_avg:56.46ms
step:1583/1900 train_time:89404ms step_avg:56.48ms
step:1584/1900 train_time:89492ms step_avg:56.50ms
step:1585/1900 train_time:89580ms step_avg:56.52ms
step:1586/1900 train_time:89668ms step_avg:56.54ms
step:1587/1900 train_time:89756ms step_avg:56.56ms
step:1588/1900 train_time:89844ms step_avg:56.58ms
step:1589/1900 train_time:89932ms step_avg:56.60ms
step:1590/1900 train_time:90020ms step_avg:56.62ms
step:1591/1900 train_time:90108ms step_avg:56.64ms
step:1592/1900 train_time:90196ms step_avg:56.66ms
step:1593/1900 train_time:90285ms step_avg:56.68ms
step:1594/1900 train_time:90373ms step_avg:56.70ms
step:1595/1900 train_time:90462ms step_avg:56.72ms
step:1596/1900 train_time:90550ms step_avg:56.74ms
step:1597/1900 train_time:90639ms step_avg:56.76ms
step:1598/1900 train_time:90726ms step_avg:56.77ms
step:1599/1900 train_time:90815ms step_avg:56.79ms
step:1600/1900 train_time:90901ms step_avg:56.81ms
step:1601/1900 train_time:90990ms step_avg:56.83ms
step:1602/1900 train_time:91078ms step_avg:56.85ms
step:1603/1900 train_time:91166ms step_avg:56.87ms
step:1604/1900 train_time:91254ms step_avg:56.89ms
step:1605/1900 train_time:91344ms step_avg:56.91ms
step:1606/1900 train_time:91431ms step_avg:56.93ms
step:1607/1900 train_time:91520ms step_avg:56.95ms
step:1608/1900 train_time:91608ms step_avg:56.97ms
step:1609/1900 train_time:91697ms step_avg:56.99ms
step:1610/1900 train_time:91784ms step_avg:57.01ms
step:1611/1900 train_time:91873ms step_avg:57.03ms
step:1612/1900 train_time:91961ms step_avg:57.05ms
step:1613/1900 train_time:92050ms step_avg:57.07ms
step:1614/1900 train_time:92137ms step_avg:57.09ms
step:1615/1900 train_time:92226ms step_avg:57.11ms
step:1616/1900 train_time:92314ms step_avg:57.13ms
step:1617/1900 train_time:92403ms step_avg:57.14ms
step:1618/1900 train_time:92491ms step_avg:57.16ms
step:1619/1900 train_time:92580ms step_avg:57.18ms
step:1620/1900 train_time:92666ms step_avg:57.20ms
step:1621/1900 train_time:92755ms step_avg:57.22ms
step:1622/1900 train_time:92842ms step_avg:57.24ms
step:1623/1900 train_time:92931ms step_avg:57.26ms
step:1624/1900 train_time:93018ms step_avg:57.28ms
step:1625/1900 train_time:93106ms step_avg:57.30ms
step:1626/1900 train_time:93194ms step_avg:57.31ms
step:1627/1900 train_time:93282ms step_avg:57.33ms
step:1628/1900 train_time:93370ms step_avg:57.35ms
step:1629/1900 train_time:93458ms step_avg:57.37ms
step:1630/1900 train_time:93545ms step_avg:57.39ms
step:1631/1900 train_time:93634ms step_avg:57.41ms
step:1632/1900 train_time:93723ms step_avg:57.43ms
step:1633/1900 train_time:93811ms step_avg:57.45ms
step:1634/1900 train_time:93897ms step_avg:57.46ms
step:1635/1900 train_time:93987ms step_avg:57.48ms
step:1636/1900 train_time:94074ms step_avg:57.50ms
step:1637/1900 train_time:94164ms step_avg:57.52ms
step:1638/1900 train_time:94251ms step_avg:57.54ms
step:1639/1900 train_time:94340ms step_avg:57.56ms
step:1640/1900 train_time:94427ms step_avg:57.58ms
step:1641/1900 train_time:94515ms step_avg:57.60ms
step:1642/1900 train_time:94603ms step_avg:57.61ms
step:1643/1900 train_time:94692ms step_avg:57.63ms
step:1644/1900 train_time:94780ms step_avg:57.65ms
step:1645/1900 train_time:94868ms step_avg:57.67ms
step:1646/1900 train_time:94955ms step_avg:57.69ms
step:1647/1900 train_time:95044ms step_avg:57.71ms
step:1648/1900 train_time:95131ms step_avg:57.73ms
step:1649/1900 train_time:95220ms step_avg:57.74ms
step:1650/1900 train_time:95308ms step_avg:57.76ms
step:1651/1900 train_time:95397ms step_avg:57.78ms
step:1652/1900 train_time:95484ms step_avg:57.80ms
step:1653/1900 train_time:95573ms step_avg:57.82ms
step:1654/1900 train_time:95660ms step_avg:57.84ms
step:1655/1900 train_time:95749ms step_avg:57.85ms
step:1656/1900 train_time:95836ms step_avg:57.87ms
step:1657/1900 train_time:95925ms step_avg:57.89ms
step:1658/1900 train_time:96013ms step_avg:57.91ms
step:1659/1900 train_time:96103ms step_avg:57.93ms
step:1660/1900 train_time:96190ms step_avg:57.95ms
step:1661/1900 train_time:96278ms step_avg:57.96ms
step:1662/1900 train_time:96365ms step_avg:57.98ms
step:1663/1900 train_time:96454ms step_avg:58.00ms
step:1664/1900 train_time:96541ms step_avg:58.02ms
step:1665/1900 train_time:96629ms step_avg:58.04ms
step:1666/1900 train_time:96717ms step_avg:58.05ms
step:1667/1900 train_time:96805ms step_avg:58.07ms
step:1668/1900 train_time:96893ms step_avg:58.09ms
step:1669/1900 train_time:96981ms step_avg:58.11ms
step:1670/1900 train_time:97069ms step_avg:58.13ms
step:1671/1900 train_time:97158ms step_avg:58.14ms
step:1672/1900 train_time:97246ms step_avg:58.16ms
step:1673/1900 train_time:97334ms step_avg:58.18ms
step:1674/1900 train_time:97421ms step_avg:58.20ms
step:1675/1900 train_time:97510ms step_avg:58.21ms
step:1676/1900 train_time:97597ms step_avg:58.23ms
step:1677/1900 train_time:97686ms step_avg:58.25ms
step:1678/1900 train_time:97773ms step_avg:58.27ms
step:1679/1900 train_time:97863ms step_avg:58.29ms
step:1680/1900 train_time:97950ms step_avg:58.30ms
step:1681/1900 train_time:98038ms step_avg:58.32ms
step:1682/1900 train_time:98125ms step_avg:58.34ms
step:1683/1900 train_time:98214ms step_avg:58.36ms
step:1684/1900 train_time:98302ms step_avg:58.37ms
step:1685/1900 train_time:98391ms step_avg:58.39ms
step:1686/1900 train_time:98478ms step_avg:58.41ms
step:1687/1900 train_time:98566ms step_avg:58.43ms
step:1688/1900 train_time:98653ms step_avg:58.44ms
step:1689/1900 train_time:98742ms step_avg:58.46ms
step:1690/1900 train_time:98829ms step_avg:58.48ms
step:1691/1900 train_time:98917ms step_avg:58.50ms
step:1692/1900 train_time:99005ms step_avg:58.51ms
step:1693/1900 train_time:99093ms step_avg:58.53ms
step:1694/1900 train_time:99181ms step_avg:58.55ms
step:1695/1900 train_time:99271ms step_avg:58.57ms
step:1696/1900 train_time:99359ms step_avg:58.58ms
step:1697/1900 train_time:99447ms step_avg:58.60ms
step:1698/1900 train_time:99534ms step_avg:58.62ms
step:1699/1900 train_time:99624ms step_avg:58.64ms
step:1700/1900 train_time:99710ms step_avg:58.65ms
step:1701/1900 train_time:99798ms step_avg:58.67ms
step:1702/1900 train_time:99885ms step_avg:58.69ms
step:1703/1900 train_time:99974ms step_avg:58.70ms
step:1704/1900 train_time:100061ms step_avg:58.72ms
step:1705/1900 train_time:100149ms step_avg:58.74ms
step:1706/1900 train_time:100237ms step_avg:58.76ms
step:1707/1900 train_time:100326ms step_avg:58.77ms
step:1708/1900 train_time:100414ms step_avg:58.79ms
step:1709/1900 train_time:100503ms step_avg:58.81ms
step:1710/1900 train_time:100591ms step_avg:58.83ms
step:1711/1900 train_time:100678ms step_avg:58.84ms
step:1712/1900 train_time:100765ms step_avg:58.86ms
step:1713/1900 train_time:100854ms step_avg:58.88ms
step:1714/1900 train_time:100942ms step_avg:58.89ms
step:1715/1900 train_time:101030ms step_avg:58.91ms
step:1716/1900 train_time:101118ms step_avg:58.93ms
step:1717/1900 train_time:101206ms step_avg:58.94ms
step:1718/1900 train_time:101293ms step_avg:58.96ms
step:1719/1900 train_time:101384ms step_avg:58.98ms
step:1720/1900 train_time:101471ms step_avg:58.99ms
step:1721/1900 train_time:101560ms step_avg:59.01ms
step:1722/1900 train_time:101648ms step_avg:59.03ms
step:1723/1900 train_time:101736ms step_avg:59.05ms
step:1724/1900 train_time:101824ms step_avg:59.06ms
step:1725/1900 train_time:101913ms step_avg:59.08ms
step:1726/1900 train_time:102000ms step_avg:59.10ms
step:1727/1900 train_time:102089ms step_avg:59.11ms
step:1728/1900 train_time:102176ms step_avg:59.13ms
step:1729/1900 train_time:102265ms step_avg:59.15ms
step:1730/1900 train_time:102353ms step_avg:59.16ms
step:1731/1900 train_time:102443ms step_avg:59.18ms
step:1732/1900 train_time:102531ms step_avg:59.20ms
step:1733/1900 train_time:102619ms step_avg:59.21ms
step:1734/1900 train_time:102707ms step_avg:59.23ms
step:1735/1900 train_time:102795ms step_avg:59.25ms
step:1736/1900 train_time:102883ms step_avg:59.26ms
step:1737/1900 train_time:102972ms step_avg:59.28ms
step:1738/1900 train_time:103059ms step_avg:59.30ms
step:1739/1900 train_time:103147ms step_avg:59.31ms
step:1740/1900 train_time:103235ms step_avg:59.33ms
step:1741/1900 train_time:103323ms step_avg:59.35ms
step:1742/1900 train_time:103411ms step_avg:59.36ms
step:1743/1900 train_time:103501ms step_avg:59.38ms
step:1744/1900 train_time:103588ms step_avg:59.40ms
step:1745/1900 train_time:103676ms step_avg:59.41ms
step:1746/1900 train_time:103764ms step_avg:59.43ms
step:1747/1900 train_time:103852ms step_avg:59.45ms
step:1748/1900 train_time:103940ms step_avg:59.46ms
step:1749/1900 train_time:104028ms step_avg:59.48ms
step:1750/1900 train_time:104116ms step_avg:59.49ms
step:1750/1900 val_loss:3.3153 train_time:104207ms step_avg:59.55ms
step:1751/1900 train_time:104228ms step_avg:59.52ms
step:1752/1900 train_time:104297ms step_avg:59.53ms
step:1753/1900 train_time:104389ms step_avg:59.55ms
step:1754/1900 train_time:104478ms step_avg:59.57ms
step:1755/1900 train_time:104565ms step_avg:59.58ms
step:1756/1900 train_time:104653ms step_avg:59.60ms
step:1757/1900 train_time:104743ms step_avg:59.61ms
step:1758/1900 train_time:104830ms step_avg:59.63ms
step:1759/1900 train_time:104917ms step_avg:59.65ms
step:1760/1900 train_time:105003ms step_avg:59.66ms
step:1761/1900 train_time:105091ms step_avg:59.68ms
step:1762/1900 train_time:105178ms step_avg:59.69ms
step:1763/1900 train_time:105269ms step_avg:59.71ms
step:1764/1900 train_time:105359ms step_avg:59.73ms
step:1765/1900 train_time:105448ms step_avg:59.74ms
step:1766/1900 train_time:105535ms step_avg:59.76ms
step:1767/1900 train_time:105623ms step_avg:59.78ms
step:1768/1900 train_time:105710ms step_avg:59.79ms
step:1769/1900 train_time:105798ms step_avg:59.81ms
step:1770/1900 train_time:105885ms step_avg:59.82ms
step:1771/1900 train_time:105972ms step_avg:59.84ms
step:1772/1900 train_time:106060ms step_avg:59.85ms
step:1773/1900 train_time:106148ms step_avg:59.87ms
step:1774/1900 train_time:106235ms step_avg:59.88ms
step:1775/1900 train_time:106326ms step_avg:59.90ms
step:1776/1900 train_time:106414ms step_avg:59.92ms
step:1777/1900 train_time:106502ms step_avg:59.93ms
step:1778/1900 train_time:106590ms step_avg:59.95ms
step:1779/1900 train_time:106677ms step_avg:59.96ms
step:1780/1900 train_time:106764ms step_avg:59.98ms
step:1781/1900 train_time:106852ms step_avg:60.00ms
step:1782/1900 train_time:106939ms step_avg:60.01ms
step:1783/1900 train_time:107027ms step_avg:60.03ms
step:1784/1900 train_time:107115ms step_avg:60.04ms
step:1785/1900 train_time:107204ms step_avg:60.06ms
step:1786/1900 train_time:107292ms step_avg:60.07ms
step:1787/1900 train_time:107381ms step_avg:60.09ms
step:1788/1900 train_time:107468ms step_avg:60.11ms
step:1789/1900 train_time:107557ms step_avg:60.12ms
step:1790/1900 train_time:107643ms step_avg:60.14ms
step:1791/1900 train_time:107733ms step_avg:60.15ms
step:1792/1900 train_time:107820ms step_avg:60.17ms
step:1793/1900 train_time:107908ms step_avg:60.18ms
step:1794/1900 train_time:107995ms step_avg:60.20ms
step:1795/1900 train_time:108084ms step_avg:60.21ms
step:1796/1900 train_time:108172ms step_avg:60.23ms
step:1797/1900 train_time:108261ms step_avg:60.25ms
step:1798/1900 train_time:108350ms step_avg:60.26ms
step:1799/1900 train_time:108439ms step_avg:60.28ms
step:1800/1900 train_time:108527ms step_avg:60.29ms
step:1801/1900 train_time:108616ms step_avg:60.31ms
step:1802/1900 train_time:108703ms step_avg:60.32ms
step:1803/1900 train_time:108792ms step_avg:60.34ms
step:1804/1900 train_time:108879ms step_avg:60.35ms
step:1805/1900 train_time:108967ms step_avg:60.37ms
step:1806/1900 train_time:109054ms step_avg:60.38ms
step:1807/1900 train_time:109143ms step_avg:60.40ms
step:1808/1900 train_time:109230ms step_avg:60.42ms
step:1809/1900 train_time:109319ms step_avg:60.43ms
step:1810/1900 train_time:109407ms step_avg:60.45ms
step:1811/1900 train_time:109496ms step_avg:60.46ms
step:1812/1900 train_time:109584ms step_avg:60.48ms
step:1813/1900 train_time:109672ms step_avg:60.49ms
step:1814/1900 train_time:109759ms step_avg:60.51ms
step:1815/1900 train_time:109847ms step_avg:60.52ms
step:1816/1900 train_time:109933ms step_avg:60.54ms
step:1817/1900 train_time:110022ms step_avg:60.55ms
step:1818/1900 train_time:110110ms step_avg:60.57ms
step:1819/1900 train_time:110198ms step_avg:60.58ms
step:1820/1900 train_time:110285ms step_avg:60.60ms
step:1821/1900 train_time:110374ms step_avg:60.61ms
step:1822/1900 train_time:110462ms step_avg:60.63ms
step:1823/1900 train_time:110551ms step_avg:60.64ms
step:1824/1900 train_time:110639ms step_avg:60.66ms
step:1825/1900 train_time:110728ms step_avg:60.67ms
step:1826/1900 train_time:110814ms step_avg:60.69ms
step:1827/1900 train_time:110902ms step_avg:60.70ms
step:1828/1900 train_time:110990ms step_avg:60.72ms
step:1829/1900 train_time:111079ms step_avg:60.73ms
step:1830/1900 train_time:111165ms step_avg:60.75ms
step:1831/1900 train_time:111253ms step_avg:60.76ms
step:1832/1900 train_time:111341ms step_avg:60.78ms
step:1833/1900 train_time:111429ms step_avg:60.79ms
step:1834/1900 train_time:111517ms step_avg:60.81ms
step:1835/1900 train_time:111606ms step_avg:60.82ms
step:1836/1900 train_time:111693ms step_avg:60.83ms
step:1837/1900 train_time:111781ms step_avg:60.85ms
step:1838/1900 train_time:111868ms step_avg:60.86ms
step:1839/1900 train_time:111957ms step_avg:60.88ms
step:1840/1900 train_time:112044ms step_avg:60.89ms
step:1841/1900 train_time:112133ms step_avg:60.91ms
step:1842/1900 train_time:112221ms step_avg:60.92ms
step:1843/1900 train_time:112309ms step_avg:60.94ms
step:1844/1900 train_time:112397ms step_avg:60.95ms
step:1845/1900 train_time:112485ms step_avg:60.97ms
step:1846/1900 train_time:112573ms step_avg:60.98ms
step:1847/1900 train_time:112661ms step_avg:61.00ms
step:1848/1900 train_time:112749ms step_avg:61.01ms
step:1849/1900 train_time:112837ms step_avg:61.03ms
step:1850/1900 train_time:112924ms step_avg:61.04ms
step:1851/1900 train_time:113012ms step_avg:61.05ms
step:1852/1900 train_time:113099ms step_avg:61.07ms
step:1853/1900 train_time:113188ms step_avg:61.08ms
step:1854/1900 train_time:113275ms step_avg:61.10ms
step:1855/1900 train_time:113364ms step_avg:61.11ms
step:1856/1900 train_time:113451ms step_avg:61.13ms
step:1857/1900 train_time:113539ms step_avg:61.14ms
step:1858/1900 train_time:113628ms step_avg:61.16ms
step:1859/1900 train_time:113716ms step_avg:61.17ms
step:1860/1900 train_time:113803ms step_avg:61.18ms
step:1861/1900 train_time:113892ms step_avg:61.20ms
step:1862/1900 train_time:113980ms step_avg:61.21ms
step:1863/1900 train_time:114068ms step_avg:61.23ms
step:1864/1900 train_time:114155ms step_avg:61.24ms
step:1865/1900 train_time:114244ms step_avg:61.26ms
step:1866/1900 train_time:114332ms step_avg:61.27ms
step:1867/1900 train_time:114421ms step_avg:61.29ms
step:1868/1900 train_time:114509ms step_avg:61.30ms
step:1869/1900 train_time:114597ms step_avg:61.31ms
step:1870/1900 train_time:114684ms step_avg:61.33ms
step:1871/1900 train_time:114773ms step_avg:61.34ms
step:1872/1900 train_time:114861ms step_avg:61.36ms
step:1873/1900 train_time:114949ms step_avg:61.37ms
step:1874/1900 train_time:115037ms step_avg:61.39ms
step:1875/1900 train_time:115126ms step_avg:61.40ms
step:1876/1900 train_time:115214ms step_avg:61.41ms
step:1877/1900 train_time:115303ms step_avg:61.43ms
step:1878/1900 train_time:115391ms step_avg:61.44ms
step:1879/1900 train_time:115480ms step_avg:61.46ms
step:1880/1900 train_time:115568ms step_avg:61.47ms
step:1881/1900 train_time:115656ms step_avg:61.49ms
step:1882/1900 train_time:115744ms step_avg:61.50ms
step:1883/1900 train_time:115833ms step_avg:61.52ms
step:1884/1900 train_time:115922ms step_avg:61.53ms
step:1885/1900 train_time:116011ms step_avg:61.54ms
step:1886/1900 train_time:116099ms step_avg:61.56ms
step:1887/1900 train_time:116187ms step_avg:61.57ms
step:1888/1900 train_time:116275ms step_avg:61.59ms
step:1889/1900 train_time:116363ms step_avg:61.60ms
step:1890/1900 train_time:116451ms step_avg:61.61ms
step:1891/1900 train_time:116539ms step_avg:61.63ms
step:1892/1900 train_time:116627ms step_avg:61.64ms
step:1893/1900 train_time:116716ms step_avg:61.66ms
step:1894/1900 train_time:116803ms step_avg:61.67ms
step:1895/1900 train_time:116892ms step_avg:61.68ms
step:1896/1900 train_time:116980ms step_avg:61.70ms
step:1897/1900 train_time:117068ms step_avg:61.71ms
step:1898/1900 train_time:117156ms step_avg:61.73ms
step:1899/1900 train_time:117245ms step_avg:61.74ms
step:1900/1900 train_time:117333ms step_avg:61.75ms
step:1900/1900 val_loss:3.2744 train_time:117423ms step_avg:61.80ms
peak memory allocated: 29114 MiB reserved: 42218 MiB

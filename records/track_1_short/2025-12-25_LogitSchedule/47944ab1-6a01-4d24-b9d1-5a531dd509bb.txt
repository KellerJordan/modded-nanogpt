import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:09:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   45C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     44348      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44349      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44350      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44351      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44352      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44353      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44354      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     44355      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     44349      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     44350      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     44351      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     44352      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     44353      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     44354      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     44355      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8322 train_time:0ms step_avg:0.14ms
step:1/1900 train_time:79ms step_avg:79.24ms
step:2/1900 train_time:103ms step_avg:51.34ms
step:3/1900 train_time:125ms step_avg:41.70ms
step:4/1900 train_time:159ms step_avg:39.72ms
step:5/1900 train_time:193ms step_avg:38.52ms
step:6/1900 train_time:268ms step_avg:44.74ms
step:7/1900 train_time:406ms step_avg:58.03ms
step:8/1900 train_time:440ms step_avg:55.02ms
step:9/1900 train_time:474ms step_avg:52.64ms
step:10/1900 train_time:508ms step_avg:50.75ms
step:11/1900 train_time:542ms step_avg:49.23ms
step:12/1900 train_time:576ms step_avg:47.96ms
step:13/1900 train_time:610ms step_avg:46.89ms
step:14/1900 train_time:643ms step_avg:45.96ms
step:15/1900 train_time:677ms step_avg:45.16ms
step:16/1900 train_time:711ms step_avg:44.46ms
step:17/1900 train_time:745ms step_avg:43.84ms
step:18/1900 train_time:779ms step_avg:43.28ms
step:19/1900 train_time:813ms step_avg:42.81ms
step:20/1900 train_time:848ms step_avg:42.38ms
step:21/1900 train_time:881ms step_avg:41.97ms
step:22/1900 train_time:915ms step_avg:41.60ms
step:23/1900 train_time:949ms step_avg:41.27ms
step:24/1900 train_time:983ms step_avg:40.96ms
step:25/1900 train_time:1017ms step_avg:40.70ms
step:26/1900 train_time:1051ms step_avg:40.44ms
step:27/1900 train_time:1085ms step_avg:40.20ms
step:28/1900 train_time:1119ms step_avg:39.98ms
step:29/1900 train_time:1153ms step_avg:39.77ms
step:30/1900 train_time:1187ms step_avg:39.58ms
step:31/1900 train_time:1221ms step_avg:39.39ms
step:32/1900 train_time:1255ms step_avg:39.22ms
step:33/1900 train_time:1289ms step_avg:39.06ms
step:34/1900 train_time:1323ms step_avg:38.91ms
step:35/1900 train_time:1358ms step_avg:38.79ms
step:36/1900 train_time:1392ms step_avg:38.66ms
step:37/1900 train_time:1427ms step_avg:38.56ms
step:38/1900 train_time:1460ms step_avg:38.43ms
step:39/1900 train_time:1495ms step_avg:38.34ms
step:40/1900 train_time:1529ms step_avg:38.23ms
step:41/1900 train_time:1563ms step_avg:38.13ms
step:42/1900 train_time:1597ms step_avg:38.03ms
step:43/1900 train_time:1631ms step_avg:37.94ms
step:44/1900 train_time:1665ms step_avg:37.85ms
step:45/1900 train_time:1700ms step_avg:37.77ms
step:46/1900 train_time:1734ms step_avg:37.69ms
step:47/1900 train_time:1768ms step_avg:37.61ms
step:48/1900 train_time:1802ms step_avg:37.53ms
step:49/1900 train_time:1836ms step_avg:37.46ms
step:50/1900 train_time:1870ms step_avg:37.39ms
step:51/1900 train_time:1904ms step_avg:37.33ms
step:52/1900 train_time:1938ms step_avg:37.27ms
step:53/1900 train_time:1972ms step_avg:37.21ms
step:54/1900 train_time:2006ms step_avg:37.14ms
step:55/1900 train_time:2040ms step_avg:37.09ms
step:56/1900 train_time:2074ms step_avg:37.03ms
step:57/1900 train_time:2108ms step_avg:36.98ms
step:58/1900 train_time:2142ms step_avg:36.92ms
step:59/1900 train_time:2176ms step_avg:36.87ms
step:60/1900 train_time:2209ms step_avg:36.82ms
step:61/1900 train_time:2244ms step_avg:36.78ms
step:62/1900 train_time:2277ms step_avg:36.73ms
step:63/1900 train_time:2312ms step_avg:36.69ms
step:64/1900 train_time:2346ms step_avg:36.65ms
step:65/1900 train_time:2380ms step_avg:36.62ms
step:66/1900 train_time:2414ms step_avg:36.58ms
step:67/1900 train_time:2449ms step_avg:36.54ms
step:68/1900 train_time:2482ms step_avg:36.51ms
step:69/1900 train_time:2517ms step_avg:36.47ms
step:70/1900 train_time:2550ms step_avg:36.43ms
step:71/1900 train_time:2585ms step_avg:36.40ms
step:72/1900 train_time:2619ms step_avg:36.37ms
step:73/1900 train_time:2653ms step_avg:36.34ms
step:74/1900 train_time:2687ms step_avg:36.31ms
step:75/1900 train_time:2721ms step_avg:36.28ms
step:76/1900 train_time:2755ms step_avg:36.24ms
step:77/1900 train_time:2789ms step_avg:36.22ms
step:78/1900 train_time:2823ms step_avg:36.19ms
step:79/1900 train_time:2857ms step_avg:36.17ms
step:80/1900 train_time:2891ms step_avg:36.14ms
step:81/1900 train_time:2926ms step_avg:36.12ms
step:82/1900 train_time:2960ms step_avg:36.09ms
step:83/1900 train_time:2994ms step_avg:36.07ms
step:84/1900 train_time:3028ms step_avg:36.05ms
step:85/1900 train_time:3062ms step_avg:36.02ms
step:86/1900 train_time:3096ms step_avg:35.99ms
step:87/1900 train_time:3130ms step_avg:35.97ms
step:88/1900 train_time:3164ms step_avg:35.95ms
step:89/1900 train_time:3198ms step_avg:35.93ms
step:90/1900 train_time:3231ms step_avg:35.91ms
step:91/1900 train_time:3266ms step_avg:35.89ms
step:92/1900 train_time:3300ms step_avg:35.86ms
step:93/1900 train_time:3334ms step_avg:35.85ms
step:94/1900 train_time:3368ms step_avg:35.83ms
step:95/1900 train_time:3402ms step_avg:35.81ms
step:96/1900 train_time:3436ms step_avg:35.79ms
step:97/1900 train_time:3470ms step_avg:35.77ms
step:98/1900 train_time:3504ms step_avg:35.75ms
step:99/1900 train_time:3538ms step_avg:35.74ms
step:100/1900 train_time:3572ms step_avg:35.72ms
step:101/1900 train_time:3606ms step_avg:35.71ms
step:102/1900 train_time:3640ms step_avg:35.69ms
step:103/1900 train_time:3675ms step_avg:35.68ms
step:104/1900 train_time:3709ms step_avg:35.66ms
step:105/1900 train_time:3743ms step_avg:35.64ms
step:106/1900 train_time:3777ms step_avg:35.63ms
step:107/1900 train_time:3811ms step_avg:35.61ms
step:108/1900 train_time:3845ms step_avg:35.60ms
step:109/1900 train_time:3879ms step_avg:35.59ms
step:110/1900 train_time:3913ms step_avg:35.58ms
step:111/1900 train_time:3948ms step_avg:35.57ms
step:112/1900 train_time:3982ms step_avg:35.55ms
step:113/1900 train_time:4016ms step_avg:35.54ms
step:114/1900 train_time:4050ms step_avg:35.53ms
step:115/1900 train_time:4084ms step_avg:35.52ms
step:116/1900 train_time:4118ms step_avg:35.50ms
step:117/1900 train_time:4153ms step_avg:35.49ms
step:118/1900 train_time:4186ms step_avg:35.48ms
step:119/1900 train_time:4220ms step_avg:35.47ms
step:120/1900 train_time:4254ms step_avg:35.45ms
step:121/1900 train_time:4288ms step_avg:35.44ms
step:122/1900 train_time:4322ms step_avg:35.43ms
step:123/1900 train_time:4357ms step_avg:35.42ms
step:124/1900 train_time:4391ms step_avg:35.41ms
step:125/1900 train_time:4425ms step_avg:35.40ms
step:126/1900 train_time:4459ms step_avg:35.39ms
step:127/1900 train_time:4493ms step_avg:35.38ms
step:128/1900 train_time:4527ms step_avg:35.37ms
step:129/1900 train_time:4561ms step_avg:35.36ms
step:130/1900 train_time:4595ms step_avg:35.35ms
step:131/1900 train_time:4629ms step_avg:35.34ms
step:132/1900 train_time:4663ms step_avg:35.33ms
step:133/1900 train_time:4697ms step_avg:35.32ms
step:134/1900 train_time:4731ms step_avg:35.31ms
step:135/1900 train_time:4765ms step_avg:35.30ms
step:136/1900 train_time:4799ms step_avg:35.29ms
step:137/1900 train_time:4833ms step_avg:35.28ms
step:138/1900 train_time:4867ms step_avg:35.27ms
step:139/1900 train_time:4901ms step_avg:35.26ms
step:140/1900 train_time:4935ms step_avg:35.25ms
step:141/1900 train_time:4969ms step_avg:35.24ms
step:142/1900 train_time:5003ms step_avg:35.23ms
step:143/1900 train_time:5037ms step_avg:35.23ms
step:144/1900 train_time:5071ms step_avg:35.22ms
step:145/1900 train_time:5105ms step_avg:35.21ms
step:146/1900 train_time:5139ms step_avg:35.20ms
step:147/1900 train_time:5174ms step_avg:35.19ms
step:148/1900 train_time:5208ms step_avg:35.19ms
step:149/1900 train_time:5242ms step_avg:35.18ms
step:150/1900 train_time:5276ms step_avg:35.17ms
step:151/1900 train_time:5309ms step_avg:35.16ms
step:152/1900 train_time:5343ms step_avg:35.15ms
step:153/1900 train_time:5378ms step_avg:35.15ms
step:154/1900 train_time:5411ms step_avg:35.14ms
step:155/1900 train_time:5446ms step_avg:35.13ms
step:156/1900 train_time:5480ms step_avg:35.13ms
step:157/1900 train_time:5514ms step_avg:35.12ms
step:158/1900 train_time:5548ms step_avg:35.11ms
step:159/1900 train_time:5582ms step_avg:35.11ms
step:160/1900 train_time:5616ms step_avg:35.10ms
step:161/1900 train_time:5650ms step_avg:35.09ms
step:162/1900 train_time:5684ms step_avg:35.09ms
step:163/1900 train_time:5718ms step_avg:35.08ms
step:164/1900 train_time:5752ms step_avg:35.07ms
step:165/1900 train_time:5786ms step_avg:35.07ms
step:166/1900 train_time:5820ms step_avg:35.06ms
step:167/1900 train_time:5854ms step_avg:35.05ms
step:168/1900 train_time:5888ms step_avg:35.05ms
step:169/1900 train_time:5922ms step_avg:35.04ms
step:170/1900 train_time:5956ms step_avg:35.03ms
step:171/1900 train_time:5990ms step_avg:35.03ms
step:172/1900 train_time:6024ms step_avg:35.02ms
step:173/1900 train_time:6058ms step_avg:35.02ms
step:174/1900 train_time:6092ms step_avg:35.01ms
step:175/1900 train_time:6126ms step_avg:35.00ms
step:176/1900 train_time:6160ms step_avg:35.00ms
step:177/1900 train_time:6194ms step_avg:34.99ms
step:178/1900 train_time:6227ms step_avg:34.98ms
step:179/1900 train_time:6261ms step_avg:34.98ms
step:180/1900 train_time:6295ms step_avg:34.97ms
step:181/1900 train_time:6329ms step_avg:34.97ms
step:182/1900 train_time:6363ms step_avg:34.96ms
step:183/1900 train_time:6397ms step_avg:34.96ms
step:184/1900 train_time:6431ms step_avg:34.95ms
step:185/1900 train_time:6465ms step_avg:34.95ms
step:186/1900 train_time:6499ms step_avg:34.94ms
step:187/1900 train_time:6533ms step_avg:34.93ms
step:188/1900 train_time:6567ms step_avg:34.93ms
step:189/1900 train_time:6600ms step_avg:34.92ms
step:190/1900 train_time:6634ms step_avg:34.92ms
step:191/1900 train_time:6668ms step_avg:34.91ms
step:192/1900 train_time:6702ms step_avg:34.91ms
step:193/1900 train_time:6736ms step_avg:34.90ms
step:194/1900 train_time:6770ms step_avg:34.90ms
step:195/1900 train_time:6804ms step_avg:34.89ms
step:196/1900 train_time:6838ms step_avg:34.89ms
step:197/1900 train_time:6872ms step_avg:34.88ms
step:198/1900 train_time:6906ms step_avg:34.88ms
step:199/1900 train_time:6940ms step_avg:34.87ms
step:200/1900 train_time:6973ms step_avg:34.87ms
step:201/1900 train_time:7007ms step_avg:34.86ms
step:202/1900 train_time:7041ms step_avg:34.86ms
step:203/1900 train_time:7075ms step_avg:34.85ms
step:204/1900 train_time:7109ms step_avg:34.85ms
step:205/1900 train_time:7143ms step_avg:34.84ms
step:206/1900 train_time:7177ms step_avg:34.84ms
step:207/1900 train_time:7211ms step_avg:34.83ms
step:208/1900 train_time:7244ms step_avg:34.83ms
step:209/1900 train_time:7279ms step_avg:34.83ms
step:210/1900 train_time:7312ms step_avg:34.82ms
step:211/1900 train_time:7346ms step_avg:34.82ms
step:212/1900 train_time:7380ms step_avg:34.81ms
step:213/1900 train_time:7414ms step_avg:34.81ms
step:214/1900 train_time:7448ms step_avg:34.80ms
step:215/1900 train_time:7482ms step_avg:34.80ms
step:216/1900 train_time:7516ms step_avg:34.80ms
step:217/1900 train_time:7550ms step_avg:34.79ms
step:218/1900 train_time:7584ms step_avg:34.79ms
step:219/1900 train_time:7618ms step_avg:34.79ms
step:220/1900 train_time:7652ms step_avg:34.78ms
step:221/1900 train_time:7686ms step_avg:34.78ms
step:222/1900 train_time:7720ms step_avg:34.77ms
step:223/1900 train_time:7754ms step_avg:34.77ms
step:224/1900 train_time:7787ms step_avg:34.76ms
step:225/1900 train_time:7822ms step_avg:34.76ms
step:226/1900 train_time:7855ms step_avg:34.76ms
step:227/1900 train_time:7890ms step_avg:34.76ms
step:228/1900 train_time:7923ms step_avg:34.75ms
step:229/1900 train_time:7958ms step_avg:34.75ms
step:230/1900 train_time:7991ms step_avg:34.74ms
step:231/1900 train_time:8026ms step_avg:34.74ms
step:232/1900 train_time:8060ms step_avg:34.74ms
step:233/1900 train_time:8094ms step_avg:34.74ms
step:234/1900 train_time:8127ms step_avg:34.73ms
step:235/1900 train_time:8162ms step_avg:34.73ms
step:236/1900 train_time:8196ms step_avg:34.73ms
step:237/1900 train_time:8230ms step_avg:34.73ms
step:238/1900 train_time:8264ms step_avg:34.72ms
step:239/1900 train_time:8298ms step_avg:34.72ms
step:240/1900 train_time:8332ms step_avg:34.72ms
step:241/1900 train_time:8366ms step_avg:34.71ms
step:242/1900 train_time:8400ms step_avg:34.71ms
step:243/1900 train_time:8434ms step_avg:34.71ms
step:244/1900 train_time:8468ms step_avg:34.70ms
step:245/1900 train_time:8502ms step_avg:34.70ms
step:246/1900 train_time:8535ms step_avg:34.70ms
step:247/1900 train_time:8570ms step_avg:34.70ms
step:248/1900 train_time:8604ms step_avg:34.69ms
step:249/1900 train_time:8638ms step_avg:34.69ms
step:250/1900 train_time:8671ms step_avg:34.69ms
step:250/1900 val_loss:4.6024 train_time:8709ms step_avg:34.83ms
step:251/1900 train_time:8728ms step_avg:34.77ms
step:252/1900 train_time:8748ms step_avg:34.71ms
step:253/1900 train_time:8778ms step_avg:34.70ms
step:254/1900 train_time:8812ms step_avg:34.69ms
step:255/1900 train_time:8847ms step_avg:34.69ms
step:256/1900 train_time:8881ms step_avg:34.69ms
step:257/1900 train_time:8915ms step_avg:34.69ms
step:258/1900 train_time:8949ms step_avg:34.69ms
step:259/1900 train_time:8983ms step_avg:34.68ms
step:260/1900 train_time:9017ms step_avg:34.68ms
step:261/1900 train_time:9050ms step_avg:34.68ms
step:262/1900 train_time:9084ms step_avg:34.67ms
step:263/1900 train_time:9118ms step_avg:34.67ms
step:264/1900 train_time:9152ms step_avg:34.67ms
step:265/1900 train_time:9186ms step_avg:34.66ms
step:266/1900 train_time:9220ms step_avg:34.66ms
step:267/1900 train_time:9254ms step_avg:34.66ms
step:268/1900 train_time:9287ms step_avg:34.65ms
step:269/1900 train_time:9321ms step_avg:34.65ms
step:270/1900 train_time:9355ms step_avg:34.65ms
step:271/1900 train_time:9389ms step_avg:34.65ms
step:272/1900 train_time:9423ms step_avg:34.64ms
step:273/1900 train_time:9457ms step_avg:34.64ms
step:274/1900 train_time:9490ms step_avg:34.64ms
step:275/1900 train_time:9524ms step_avg:34.63ms
step:276/1900 train_time:9558ms step_avg:34.63ms
step:277/1900 train_time:9592ms step_avg:34.63ms
step:278/1900 train_time:9625ms step_avg:34.62ms
step:279/1900 train_time:9660ms step_avg:34.62ms
step:280/1900 train_time:9694ms step_avg:34.62ms
step:281/1900 train_time:9728ms step_avg:34.62ms
step:282/1900 train_time:9762ms step_avg:34.62ms
step:283/1900 train_time:9796ms step_avg:34.61ms
step:284/1900 train_time:9830ms step_avg:34.61ms
step:285/1900 train_time:9864ms step_avg:34.61ms
step:286/1900 train_time:9898ms step_avg:34.61ms
step:287/1900 train_time:9932ms step_avg:34.61ms
step:288/1900 train_time:9966ms step_avg:34.60ms
step:289/1900 train_time:10000ms step_avg:34.60ms
step:290/1900 train_time:10034ms step_avg:34.60ms
step:291/1900 train_time:10068ms step_avg:34.60ms
step:292/1900 train_time:10101ms step_avg:34.59ms
step:293/1900 train_time:10135ms step_avg:34.59ms
step:294/1900 train_time:10169ms step_avg:34.59ms
step:295/1900 train_time:10203ms step_avg:34.59ms
step:296/1900 train_time:10237ms step_avg:34.58ms
step:297/1900 train_time:10271ms step_avg:34.58ms
step:298/1900 train_time:10305ms step_avg:34.58ms
step:299/1900 train_time:10339ms step_avg:34.58ms
step:300/1900 train_time:10373ms step_avg:34.58ms
step:301/1900 train_time:10407ms step_avg:34.58ms
step:302/1900 train_time:10441ms step_avg:34.57ms
step:303/1900 train_time:10475ms step_avg:34.57ms
step:304/1900 train_time:10509ms step_avg:34.57ms
step:305/1900 train_time:10542ms step_avg:34.57ms
step:306/1900 train_time:10576ms step_avg:34.56ms
step:307/1900 train_time:10610ms step_avg:34.56ms
step:308/1900 train_time:10644ms step_avg:34.56ms
step:309/1900 train_time:10678ms step_avg:34.56ms
step:310/1900 train_time:10712ms step_avg:34.55ms
step:311/1900 train_time:10746ms step_avg:34.55ms
step:312/1900 train_time:10780ms step_avg:34.55ms
step:313/1900 train_time:10814ms step_avg:34.55ms
step:314/1900 train_time:10848ms step_avg:34.55ms
step:315/1900 train_time:10882ms step_avg:34.55ms
step:316/1900 train_time:10916ms step_avg:34.54ms
step:317/1900 train_time:10950ms step_avg:34.54ms
step:318/1900 train_time:10984ms step_avg:34.54ms
step:319/1900 train_time:11018ms step_avg:34.54ms
step:320/1900 train_time:11052ms step_avg:34.54ms
step:321/1900 train_time:11086ms step_avg:34.53ms
step:322/1900 train_time:11119ms step_avg:34.53ms
step:323/1900 train_time:11153ms step_avg:34.53ms
step:324/1900 train_time:11187ms step_avg:34.53ms
step:325/1900 train_time:11221ms step_avg:34.53ms
step:326/1900 train_time:11255ms step_avg:34.52ms
step:327/1900 train_time:11289ms step_avg:34.52ms
step:328/1900 train_time:11323ms step_avg:34.52ms
step:329/1900 train_time:11357ms step_avg:34.52ms
step:330/1900 train_time:11390ms step_avg:34.52ms
step:331/1900 train_time:11424ms step_avg:34.51ms
step:332/1900 train_time:11458ms step_avg:34.51ms
step:333/1900 train_time:11492ms step_avg:34.51ms
step:334/1900 train_time:11526ms step_avg:34.51ms
step:335/1900 train_time:11560ms step_avg:34.51ms
step:336/1900 train_time:11594ms step_avg:34.50ms
step:337/1900 train_time:11627ms step_avg:34.50ms
step:338/1900 train_time:11661ms step_avg:34.50ms
step:339/1900 train_time:11695ms step_avg:34.50ms
step:340/1900 train_time:11729ms step_avg:34.50ms
step:341/1900 train_time:11763ms step_avg:34.49ms
step:342/1900 train_time:11797ms step_avg:34.49ms
step:343/1900 train_time:11831ms step_avg:34.49ms
step:344/1900 train_time:11864ms step_avg:34.49ms
step:345/1900 train_time:11899ms step_avg:34.49ms
step:346/1900 train_time:11933ms step_avg:34.49ms
step:347/1900 train_time:11967ms step_avg:34.49ms
step:348/1900 train_time:12000ms step_avg:34.48ms
step:349/1900 train_time:12034ms step_avg:34.48ms
step:350/1900 train_time:12068ms step_avg:34.48ms
step:351/1900 train_time:12102ms step_avg:34.48ms
step:352/1900 train_time:12136ms step_avg:34.48ms
step:353/1900 train_time:12170ms step_avg:34.48ms
step:354/1900 train_time:12203ms step_avg:34.47ms
step:355/1900 train_time:12238ms step_avg:34.47ms
step:356/1900 train_time:12271ms step_avg:34.47ms
step:357/1900 train_time:12306ms step_avg:34.47ms
step:358/1900 train_time:12339ms step_avg:34.47ms
step:359/1900 train_time:12374ms step_avg:34.47ms
step:360/1900 train_time:12407ms step_avg:34.47ms
step:361/1900 train_time:12442ms step_avg:34.47ms
step:362/1900 train_time:12476ms step_avg:34.46ms
step:363/1900 train_time:12510ms step_avg:34.46ms
step:364/1900 train_time:12543ms step_avg:34.46ms
step:365/1900 train_time:12577ms step_avg:34.46ms
step:366/1900 train_time:12611ms step_avg:34.46ms
step:367/1900 train_time:12645ms step_avg:34.45ms
step:368/1900 train_time:12679ms step_avg:34.45ms
step:369/1900 train_time:12713ms step_avg:34.45ms
step:370/1900 train_time:12747ms step_avg:34.45ms
step:371/1900 train_time:12780ms step_avg:34.45ms
step:372/1900 train_time:12814ms step_avg:34.45ms
step:373/1900 train_time:12848ms step_avg:34.45ms
step:374/1900 train_time:12882ms step_avg:34.44ms
step:375/1900 train_time:12916ms step_avg:34.44ms
step:376/1900 train_time:12950ms step_avg:34.44ms
step:377/1900 train_time:12984ms step_avg:34.44ms
step:378/1900 train_time:13018ms step_avg:34.44ms
step:379/1900 train_time:13052ms step_avg:34.44ms
step:380/1900 train_time:13086ms step_avg:34.44ms
step:381/1900 train_time:13120ms step_avg:34.44ms
step:382/1900 train_time:13154ms step_avg:34.43ms
step:383/1900 train_time:13188ms step_avg:34.43ms
step:384/1900 train_time:13222ms step_avg:34.43ms
step:385/1900 train_time:13255ms step_avg:34.43ms
step:386/1900 train_time:13289ms step_avg:34.43ms
step:387/1900 train_time:13323ms step_avg:34.43ms
step:388/1900 train_time:13357ms step_avg:34.43ms
step:389/1900 train_time:13391ms step_avg:34.42ms
step:390/1900 train_time:13425ms step_avg:34.42ms
step:391/1900 train_time:13459ms step_avg:34.42ms
step:392/1900 train_time:13492ms step_avg:34.42ms
step:393/1900 train_time:13526ms step_avg:34.42ms
step:394/1900 train_time:13560ms step_avg:34.42ms
step:395/1900 train_time:13594ms step_avg:34.41ms
step:396/1900 train_time:13628ms step_avg:34.41ms
step:397/1900 train_time:13661ms step_avg:34.41ms
step:398/1900 train_time:13695ms step_avg:34.41ms
step:399/1900 train_time:13729ms step_avg:34.41ms
step:400/1900 train_time:13763ms step_avg:34.41ms
step:401/1900 train_time:13797ms step_avg:34.41ms
step:402/1900 train_time:13831ms step_avg:34.41ms
step:403/1900 train_time:13865ms step_avg:34.40ms
step:404/1900 train_time:13899ms step_avg:34.40ms
step:405/1900 train_time:13932ms step_avg:34.40ms
step:406/1900 train_time:13966ms step_avg:34.40ms
step:407/1900 train_time:14000ms step_avg:34.40ms
step:408/1900 train_time:14034ms step_avg:34.40ms
step:409/1900 train_time:14068ms step_avg:34.40ms
step:410/1900 train_time:14102ms step_avg:34.39ms
step:411/1900 train_time:14136ms step_avg:34.39ms
step:412/1900 train_time:14170ms step_avg:34.39ms
step:413/1900 train_time:14204ms step_avg:34.39ms
step:414/1900 train_time:14238ms step_avg:34.39ms
step:415/1900 train_time:14272ms step_avg:34.39ms
step:416/1900 train_time:14306ms step_avg:34.39ms
step:417/1900 train_time:14340ms step_avg:34.39ms
step:418/1900 train_time:14374ms step_avg:34.39ms
step:419/1900 train_time:14408ms step_avg:34.39ms
step:420/1900 train_time:14442ms step_avg:34.38ms
step:421/1900 train_time:14475ms step_avg:34.38ms
step:422/1900 train_time:14509ms step_avg:34.38ms
step:423/1900 train_time:14543ms step_avg:34.38ms
step:424/1900 train_time:14577ms step_avg:34.38ms
step:425/1900 train_time:14611ms step_avg:34.38ms
step:426/1900 train_time:14644ms step_avg:34.38ms
step:427/1900 train_time:14679ms step_avg:34.38ms
step:428/1900 train_time:14713ms step_avg:34.38ms
step:429/1900 train_time:14746ms step_avg:34.37ms
step:430/1900 train_time:14780ms step_avg:34.37ms
step:431/1900 train_time:14814ms step_avg:34.37ms
step:432/1900 train_time:14848ms step_avg:34.37ms
step:433/1900 train_time:14882ms step_avg:34.37ms
step:434/1900 train_time:14916ms step_avg:34.37ms
step:435/1900 train_time:14949ms step_avg:34.37ms
step:436/1900 train_time:14983ms step_avg:34.36ms
step:437/1900 train_time:15017ms step_avg:34.36ms
step:438/1900 train_time:15051ms step_avg:34.36ms
step:439/1900 train_time:15086ms step_avg:34.36ms
step:440/1900 train_time:15119ms step_avg:34.36ms
step:441/1900 train_time:15154ms step_avg:34.36ms
step:442/1900 train_time:15187ms step_avg:34.36ms
step:443/1900 train_time:15221ms step_avg:34.36ms
step:444/1900 train_time:15255ms step_avg:34.36ms
step:445/1900 train_time:15289ms step_avg:34.36ms
step:446/1900 train_time:15323ms step_avg:34.36ms
step:447/1900 train_time:15357ms step_avg:34.36ms
step:448/1900 train_time:15391ms step_avg:34.35ms
step:449/1900 train_time:15425ms step_avg:34.35ms
step:450/1900 train_time:15458ms step_avg:34.35ms
step:451/1900 train_time:15492ms step_avg:34.35ms
step:452/1900 train_time:15526ms step_avg:34.35ms
step:453/1900 train_time:15560ms step_avg:34.35ms
step:454/1900 train_time:15594ms step_avg:34.35ms
step:455/1900 train_time:15628ms step_avg:34.35ms
step:456/1900 train_time:15662ms step_avg:34.35ms
step:457/1900 train_time:15696ms step_avg:34.34ms
step:458/1900 train_time:15729ms step_avg:34.34ms
step:459/1900 train_time:15763ms step_avg:34.34ms
step:460/1900 train_time:15797ms step_avg:34.34ms
step:461/1900 train_time:15831ms step_avg:34.34ms
step:462/1900 train_time:15865ms step_avg:34.34ms
step:463/1900 train_time:15899ms step_avg:34.34ms
step:464/1900 train_time:15933ms step_avg:34.34ms
step:465/1900 train_time:15967ms step_avg:34.34ms
step:466/1900 train_time:16001ms step_avg:34.34ms
step:467/1900 train_time:16035ms step_avg:34.34ms
step:468/1900 train_time:16069ms step_avg:34.33ms
step:469/1900 train_time:16102ms step_avg:34.33ms
step:470/1900 train_time:16137ms step_avg:34.33ms
step:471/1900 train_time:16170ms step_avg:34.33ms
step:472/1900 train_time:16204ms step_avg:34.33ms
step:473/1900 train_time:16239ms step_avg:34.33ms
step:474/1900 train_time:16273ms step_avg:34.33ms
step:475/1900 train_time:16307ms step_avg:34.33ms
step:476/1900 train_time:16340ms step_avg:34.33ms
step:477/1900 train_time:16375ms step_avg:34.33ms
step:478/1900 train_time:16408ms step_avg:34.33ms
step:479/1900 train_time:16443ms step_avg:34.33ms
step:480/1900 train_time:16476ms step_avg:34.33ms
step:481/1900 train_time:16510ms step_avg:34.32ms
step:482/1900 train_time:16544ms step_avg:34.32ms
step:483/1900 train_time:16578ms step_avg:34.32ms
step:484/1900 train_time:16612ms step_avg:34.32ms
step:485/1900 train_time:16646ms step_avg:34.32ms
step:486/1900 train_time:16680ms step_avg:34.32ms
step:487/1900 train_time:16714ms step_avg:34.32ms
step:488/1900 train_time:16748ms step_avg:34.32ms
step:489/1900 train_time:16782ms step_avg:34.32ms
step:490/1900 train_time:16816ms step_avg:34.32ms
step:491/1900 train_time:16849ms step_avg:34.32ms
step:492/1900 train_time:16883ms step_avg:34.32ms
step:493/1900 train_time:16917ms step_avg:34.31ms
step:494/1900 train_time:16951ms step_avg:34.31ms
step:495/1900 train_time:16985ms step_avg:34.31ms
step:496/1900 train_time:17019ms step_avg:34.31ms
step:497/1900 train_time:17053ms step_avg:34.31ms
step:498/1900 train_time:17087ms step_avg:34.31ms
step:499/1900 train_time:17121ms step_avg:34.31ms
step:500/1900 train_time:17155ms step_avg:34.31ms
step:500/1900 val_loss:4.2898 train_time:17191ms step_avg:34.38ms
step:501/1900 train_time:17211ms step_avg:34.35ms
step:502/1900 train_time:17231ms step_avg:34.32ms
step:503/1900 train_time:17260ms step_avg:34.31ms
step:504/1900 train_time:17294ms step_avg:34.31ms
step:505/1900 train_time:17329ms step_avg:34.31ms
step:506/1900 train_time:17363ms step_avg:34.31ms
step:507/1900 train_time:17398ms step_avg:34.32ms
step:508/1900 train_time:17432ms step_avg:34.31ms
step:509/1900 train_time:17466ms step_avg:34.31ms
step:510/1900 train_time:17500ms step_avg:34.31ms
step:511/1900 train_time:17534ms step_avg:34.31ms
step:512/1900 train_time:17568ms step_avg:34.31ms
step:513/1900 train_time:17601ms step_avg:34.31ms
step:514/1900 train_time:17635ms step_avg:34.31ms
step:515/1900 train_time:17669ms step_avg:34.31ms
step:516/1900 train_time:17703ms step_avg:34.31ms
step:517/1900 train_time:17737ms step_avg:34.31ms
step:518/1900 train_time:17770ms step_avg:34.31ms
step:519/1900 train_time:17804ms step_avg:34.30ms
step:520/1900 train_time:17838ms step_avg:34.30ms
step:521/1900 train_time:17872ms step_avg:34.30ms
step:522/1900 train_time:17906ms step_avg:34.30ms
step:523/1900 train_time:17939ms step_avg:34.30ms
step:524/1900 train_time:17973ms step_avg:34.30ms
step:525/1900 train_time:18007ms step_avg:34.30ms
step:526/1900 train_time:18041ms step_avg:34.30ms
step:527/1900 train_time:18074ms step_avg:34.30ms
step:528/1900 train_time:18108ms step_avg:34.30ms
step:529/1900 train_time:18142ms step_avg:34.30ms
step:530/1900 train_time:18176ms step_avg:34.29ms
step:531/1900 train_time:18210ms step_avg:34.29ms
step:532/1900 train_time:18244ms step_avg:34.29ms
step:533/1900 train_time:18279ms step_avg:34.29ms
step:534/1900 train_time:18312ms step_avg:34.29ms
step:535/1900 train_time:18347ms step_avg:34.29ms
step:536/1900 train_time:18381ms step_avg:34.29ms
step:537/1900 train_time:18415ms step_avg:34.29ms
step:538/1900 train_time:18449ms step_avg:34.29ms
step:539/1900 train_time:18483ms step_avg:34.29ms
step:540/1900 train_time:18517ms step_avg:34.29ms
step:541/1900 train_time:18551ms step_avg:34.29ms
step:542/1900 train_time:18584ms step_avg:34.29ms
step:543/1900 train_time:18618ms step_avg:34.29ms
step:544/1900 train_time:18652ms step_avg:34.29ms
step:545/1900 train_time:18686ms step_avg:34.29ms
step:546/1900 train_time:18720ms step_avg:34.29ms
step:547/1900 train_time:18754ms step_avg:34.28ms
step:548/1900 train_time:18788ms step_avg:34.28ms
step:549/1900 train_time:18822ms step_avg:34.28ms
step:550/1900 train_time:18856ms step_avg:34.28ms
step:551/1900 train_time:18889ms step_avg:34.28ms
step:552/1900 train_time:18923ms step_avg:34.28ms
step:553/1900 train_time:18957ms step_avg:34.28ms
step:554/1900 train_time:18991ms step_avg:34.28ms
step:555/1900 train_time:19025ms step_avg:34.28ms
step:556/1900 train_time:19058ms step_avg:34.28ms
step:557/1900 train_time:19092ms step_avg:34.28ms
step:558/1900 train_time:19126ms step_avg:34.28ms
step:559/1900 train_time:19161ms step_avg:34.28ms
step:560/1900 train_time:19194ms step_avg:34.28ms
step:561/1900 train_time:19229ms step_avg:34.28ms
step:562/1900 train_time:19263ms step_avg:34.28ms
step:563/1900 train_time:19297ms step_avg:34.28ms
step:564/1900 train_time:19331ms step_avg:34.27ms
step:565/1900 train_time:19365ms step_avg:34.28ms
step:566/1900 train_time:19399ms step_avg:34.27ms
step:567/1900 train_time:19434ms step_avg:34.27ms
step:568/1900 train_time:19467ms step_avg:34.27ms
step:569/1900 train_time:19502ms step_avg:34.27ms
step:570/1900 train_time:19535ms step_avg:34.27ms
step:571/1900 train_time:19570ms step_avg:34.27ms
step:572/1900 train_time:19604ms step_avg:34.27ms
step:573/1900 train_time:19638ms step_avg:34.27ms
step:574/1900 train_time:19672ms step_avg:34.27ms
step:575/1900 train_time:19706ms step_avg:34.27ms
step:576/1900 train_time:19740ms step_avg:34.27ms
step:577/1900 train_time:19774ms step_avg:34.27ms
step:578/1900 train_time:19808ms step_avg:34.27ms
step:579/1900 train_time:19843ms step_avg:34.27ms
step:580/1900 train_time:19876ms step_avg:34.27ms
step:581/1900 train_time:19911ms step_avg:34.27ms
step:582/1900 train_time:19945ms step_avg:34.27ms
step:583/1900 train_time:19978ms step_avg:34.27ms
step:584/1900 train_time:20012ms step_avg:34.27ms
step:585/1900 train_time:20046ms step_avg:34.27ms
step:586/1900 train_time:20080ms step_avg:34.27ms
step:587/1900 train_time:20114ms step_avg:34.27ms
step:588/1900 train_time:20147ms step_avg:34.26ms
step:589/1900 train_time:20182ms step_avg:34.26ms
step:590/1900 train_time:20215ms step_avg:34.26ms
step:591/1900 train_time:20249ms step_avg:34.26ms
step:592/1900 train_time:20283ms step_avg:34.26ms
step:593/1900 train_time:20317ms step_avg:34.26ms
step:594/1900 train_time:20350ms step_avg:34.26ms
step:595/1900 train_time:20385ms step_avg:34.26ms
step:596/1900 train_time:20419ms step_avg:34.26ms
step:597/1900 train_time:20453ms step_avg:34.26ms
step:598/1900 train_time:20486ms step_avg:34.26ms
step:599/1900 train_time:20521ms step_avg:34.26ms
step:600/1900 train_time:20555ms step_avg:34.26ms
step:601/1900 train_time:20589ms step_avg:34.26ms
step:602/1900 train_time:20623ms step_avg:34.26ms
step:603/1900 train_time:20657ms step_avg:34.26ms
step:604/1900 train_time:20691ms step_avg:34.26ms
step:605/1900 train_time:20725ms step_avg:34.26ms
step:606/1900 train_time:20758ms step_avg:34.25ms
step:607/1900 train_time:20793ms step_avg:34.25ms
step:608/1900 train_time:20826ms step_avg:34.25ms
step:609/1900 train_time:20861ms step_avg:34.25ms
step:610/1900 train_time:20894ms step_avg:34.25ms
step:611/1900 train_time:20929ms step_avg:34.25ms
step:612/1900 train_time:20962ms step_avg:34.25ms
step:613/1900 train_time:20996ms step_avg:34.25ms
step:614/1900 train_time:21030ms step_avg:34.25ms
step:615/1900 train_time:21064ms step_avg:34.25ms
step:616/1900 train_time:21098ms step_avg:34.25ms
step:617/1900 train_time:21132ms step_avg:34.25ms
step:618/1900 train_time:21166ms step_avg:34.25ms
step:619/1900 train_time:21200ms step_avg:34.25ms
step:620/1900 train_time:21234ms step_avg:34.25ms
step:621/1900 train_time:21268ms step_avg:34.25ms
step:622/1900 train_time:21328ms step_avg:34.29ms
step:623/1900 train_time:21390ms step_avg:34.33ms
step:624/1900 train_time:21452ms step_avg:34.38ms
step:625/1900 train_time:21514ms step_avg:34.42ms
step:626/1900 train_time:21575ms step_avg:34.47ms
step:627/1900 train_time:21637ms step_avg:34.51ms
step:628/1900 train_time:21698ms step_avg:34.55ms
step:629/1900 train_time:21760ms step_avg:34.59ms
step:630/1900 train_time:21821ms step_avg:34.64ms
step:631/1900 train_time:21883ms step_avg:34.68ms
step:632/1900 train_time:21944ms step_avg:34.72ms
step:633/1900 train_time:22006ms step_avg:34.76ms
step:634/1900 train_time:22067ms step_avg:34.81ms
step:635/1900 train_time:22129ms step_avg:34.85ms
step:636/1900 train_time:22190ms step_avg:34.89ms
step:637/1900 train_time:22252ms step_avg:34.93ms
step:638/1900 train_time:22312ms step_avg:34.97ms
step:639/1900 train_time:22374ms step_avg:35.01ms
step:640/1900 train_time:22435ms step_avg:35.05ms
step:641/1900 train_time:22497ms step_avg:35.10ms
step:642/1900 train_time:22558ms step_avg:35.14ms
step:643/1900 train_time:22620ms step_avg:35.18ms
step:644/1900 train_time:22680ms step_avg:35.22ms
step:645/1900 train_time:22742ms step_avg:35.26ms
step:646/1900 train_time:22803ms step_avg:35.30ms
step:647/1900 train_time:22864ms step_avg:35.34ms
step:648/1900 train_time:22925ms step_avg:35.38ms
step:649/1900 train_time:22987ms step_avg:35.42ms
step:650/1900 train_time:23049ms step_avg:35.46ms
step:651/1900 train_time:23111ms step_avg:35.50ms
step:652/1900 train_time:23171ms step_avg:35.54ms
step:653/1900 train_time:23233ms step_avg:35.58ms
step:654/1900 train_time:23294ms step_avg:35.62ms
step:655/1900 train_time:23356ms step_avg:35.66ms
step:656/1900 train_time:23417ms step_avg:35.70ms
step:657/1900 train_time:23478ms step_avg:35.74ms
step:658/1900 train_time:23539ms step_avg:35.77ms
step:659/1900 train_time:23601ms step_avg:35.81ms
step:660/1900 train_time:23662ms step_avg:35.85ms
step:661/1900 train_time:23724ms step_avg:35.89ms
step:662/1900 train_time:23785ms step_avg:35.93ms
step:663/1900 train_time:23847ms step_avg:35.97ms
step:664/1900 train_time:23908ms step_avg:36.01ms
step:665/1900 train_time:23970ms step_avg:36.05ms
step:666/1900 train_time:24032ms step_avg:36.08ms
step:667/1900 train_time:24094ms step_avg:36.12ms
step:668/1900 train_time:24155ms step_avg:36.16ms
step:669/1900 train_time:24217ms step_avg:36.20ms
step:670/1900 train_time:24278ms step_avg:36.24ms
step:671/1900 train_time:24339ms step_avg:36.27ms
step:672/1900 train_time:24400ms step_avg:36.31ms
step:673/1900 train_time:24462ms step_avg:36.35ms
step:674/1900 train_time:24523ms step_avg:36.38ms
step:675/1900 train_time:24584ms step_avg:36.42ms
step:676/1900 train_time:24645ms step_avg:36.46ms
step:677/1900 train_time:24708ms step_avg:36.50ms
step:678/1900 train_time:24769ms step_avg:36.53ms
step:679/1900 train_time:24831ms step_avg:36.57ms
step:680/1900 train_time:24892ms step_avg:36.61ms
step:681/1900 train_time:24954ms step_avg:36.64ms
step:682/1900 train_time:25016ms step_avg:36.68ms
step:683/1900 train_time:25077ms step_avg:36.72ms
step:684/1900 train_time:25138ms step_avg:36.75ms
step:685/1900 train_time:25200ms step_avg:36.79ms
step:686/1900 train_time:25260ms step_avg:36.82ms
step:687/1900 train_time:25322ms step_avg:36.86ms
step:688/1900 train_time:25383ms step_avg:36.89ms
step:689/1900 train_time:25446ms step_avg:36.93ms
step:690/1900 train_time:25506ms step_avg:36.97ms
step:691/1900 train_time:25568ms step_avg:37.00ms
step:692/1900 train_time:25630ms step_avg:37.04ms
step:693/1900 train_time:25692ms step_avg:37.07ms
step:694/1900 train_time:25753ms step_avg:37.11ms
step:695/1900 train_time:25814ms step_avg:37.14ms
step:696/1900 train_time:25876ms step_avg:37.18ms
step:697/1900 train_time:25938ms step_avg:37.21ms
step:698/1900 train_time:25998ms step_avg:37.25ms
step:699/1900 train_time:26060ms step_avg:37.28ms
step:700/1900 train_time:26121ms step_avg:37.32ms
step:701/1900 train_time:26182ms step_avg:37.35ms
step:702/1900 train_time:26243ms step_avg:37.38ms
step:703/1900 train_time:26305ms step_avg:37.42ms
step:704/1900 train_time:26366ms step_avg:37.45ms
step:705/1900 train_time:26428ms step_avg:37.49ms
step:706/1900 train_time:26489ms step_avg:37.52ms
step:707/1900 train_time:26551ms step_avg:37.55ms
step:708/1900 train_time:26612ms step_avg:37.59ms
step:709/1900 train_time:26674ms step_avg:37.62ms
step:710/1900 train_time:26735ms step_avg:37.66ms
step:711/1900 train_time:26797ms step_avg:37.69ms
step:712/1900 train_time:26858ms step_avg:37.72ms
step:713/1900 train_time:26920ms step_avg:37.76ms
step:714/1900 train_time:26981ms step_avg:37.79ms
step:715/1900 train_time:27043ms step_avg:37.82ms
step:716/1900 train_time:27103ms step_avg:37.85ms
step:717/1900 train_time:27165ms step_avg:37.89ms
step:718/1900 train_time:27226ms step_avg:37.92ms
step:719/1900 train_time:27288ms step_avg:37.95ms
step:720/1900 train_time:27349ms step_avg:37.98ms
step:721/1900 train_time:27410ms step_avg:38.02ms
step:722/1900 train_time:27471ms step_avg:38.05ms
step:723/1900 train_time:27534ms step_avg:38.08ms
step:724/1900 train_time:27594ms step_avg:38.11ms
step:725/1900 train_time:27656ms step_avg:38.15ms
step:726/1900 train_time:27717ms step_avg:38.18ms
step:727/1900 train_time:27779ms step_avg:38.21ms
step:728/1900 train_time:27840ms step_avg:38.24ms
step:729/1900 train_time:27902ms step_avg:38.27ms
step:730/1900 train_time:27962ms step_avg:38.30ms
step:731/1900 train_time:28024ms step_avg:38.34ms
step:732/1900 train_time:28085ms step_avg:38.37ms
step:733/1900 train_time:28147ms step_avg:38.40ms
step:734/1900 train_time:28207ms step_avg:38.43ms
step:735/1900 train_time:28269ms step_avg:38.46ms
step:736/1900 train_time:28330ms step_avg:38.49ms
step:737/1900 train_time:28392ms step_avg:38.52ms
step:738/1900 train_time:28453ms step_avg:38.55ms
step:739/1900 train_time:28515ms step_avg:38.59ms
step:740/1900 train_time:28576ms step_avg:38.62ms
step:741/1900 train_time:28638ms step_avg:38.65ms
step:742/1900 train_time:28699ms step_avg:38.68ms
step:743/1900 train_time:28761ms step_avg:38.71ms
step:744/1900 train_time:28821ms step_avg:38.74ms
step:745/1900 train_time:28883ms step_avg:38.77ms
step:746/1900 train_time:28944ms step_avg:38.80ms
step:747/1900 train_time:29006ms step_avg:38.83ms
step:748/1900 train_time:29067ms step_avg:38.86ms
step:749/1900 train_time:29129ms step_avg:38.89ms
step:750/1900 train_time:29190ms step_avg:38.92ms
step:750/1900 val_loss:4.0212 train_time:29254ms step_avg:39.01ms
step:751/1900 train_time:29275ms step_avg:38.98ms
step:752/1900 train_time:29316ms step_avg:38.98ms
step:753/1900 train_time:29379ms step_avg:39.02ms
step:754/1900 train_time:29440ms step_avg:39.05ms
step:755/1900 train_time:29502ms step_avg:39.08ms
step:756/1900 train_time:29563ms step_avg:39.10ms
step:757/1900 train_time:29625ms step_avg:39.13ms
step:758/1900 train_time:29685ms step_avg:39.16ms
step:759/1900 train_time:29746ms step_avg:39.19ms
step:760/1900 train_time:29807ms step_avg:39.22ms
step:761/1900 train_time:29868ms step_avg:39.25ms
step:762/1900 train_time:29928ms step_avg:39.28ms
step:763/1900 train_time:29989ms step_avg:39.30ms
step:764/1900 train_time:30050ms step_avg:39.33ms
step:765/1900 train_time:30111ms step_avg:39.36ms
step:766/1900 train_time:30172ms step_avg:39.39ms
step:767/1900 train_time:30235ms step_avg:39.42ms
step:768/1900 train_time:30297ms step_avg:39.45ms
step:769/1900 train_time:30360ms step_avg:39.48ms
step:770/1900 train_time:30421ms step_avg:39.51ms
step:771/1900 train_time:30483ms step_avg:39.54ms
step:772/1900 train_time:30544ms step_avg:39.56ms
step:773/1900 train_time:30606ms step_avg:39.59ms
step:774/1900 train_time:30667ms step_avg:39.62ms
step:775/1900 train_time:30729ms step_avg:39.65ms
step:776/1900 train_time:30789ms step_avg:39.68ms
step:777/1900 train_time:30850ms step_avg:39.70ms
step:778/1900 train_time:30910ms step_avg:39.73ms
step:779/1900 train_time:30972ms step_avg:39.76ms
step:780/1900 train_time:31032ms step_avg:39.79ms
step:781/1900 train_time:31094ms step_avg:39.81ms
step:782/1900 train_time:31155ms step_avg:39.84ms
step:783/1900 train_time:31217ms step_avg:39.87ms
step:784/1900 train_time:31278ms step_avg:39.90ms
step:785/1900 train_time:31341ms step_avg:39.92ms
step:786/1900 train_time:31402ms step_avg:39.95ms
step:787/1900 train_time:31464ms step_avg:39.98ms
step:788/1900 train_time:31526ms step_avg:40.01ms
step:789/1900 train_time:31587ms step_avg:40.03ms
step:790/1900 train_time:31648ms step_avg:40.06ms
step:791/1900 train_time:31709ms step_avg:40.09ms
step:792/1900 train_time:31770ms step_avg:40.11ms
step:793/1900 train_time:31832ms step_avg:40.14ms
step:794/1900 train_time:31892ms step_avg:40.17ms
step:795/1900 train_time:31953ms step_avg:40.19ms
step:796/1900 train_time:32014ms step_avg:40.22ms
step:797/1900 train_time:32075ms step_avg:40.25ms
step:798/1900 train_time:32136ms step_avg:40.27ms
step:799/1900 train_time:32198ms step_avg:40.30ms
step:800/1900 train_time:32259ms step_avg:40.32ms
step:801/1900 train_time:32321ms step_avg:40.35ms
step:802/1900 train_time:32382ms step_avg:40.38ms
step:803/1900 train_time:32444ms step_avg:40.40ms
step:804/1900 train_time:32506ms step_avg:40.43ms
step:805/1900 train_time:32567ms step_avg:40.46ms
step:806/1900 train_time:32628ms step_avg:40.48ms
step:807/1900 train_time:32690ms step_avg:40.51ms
step:808/1900 train_time:32750ms step_avg:40.53ms
step:809/1900 train_time:32812ms step_avg:40.56ms
step:810/1900 train_time:32872ms step_avg:40.58ms
step:811/1900 train_time:32933ms step_avg:40.61ms
step:812/1900 train_time:32994ms step_avg:40.63ms
step:813/1900 train_time:33055ms step_avg:40.66ms
step:814/1900 train_time:33116ms step_avg:40.68ms
step:815/1900 train_time:33178ms step_avg:40.71ms
step:816/1900 train_time:33239ms step_avg:40.73ms
step:817/1900 train_time:33301ms step_avg:40.76ms
step:818/1900 train_time:33362ms step_avg:40.78ms
step:819/1900 train_time:33425ms step_avg:40.81ms
step:820/1900 train_time:33486ms step_avg:40.84ms
step:821/1900 train_time:33548ms step_avg:40.86ms
step:822/1900 train_time:33609ms step_avg:40.89ms
step:823/1900 train_time:33670ms step_avg:40.91ms
step:824/1900 train_time:33732ms step_avg:40.94ms
step:825/1900 train_time:33794ms step_avg:40.96ms
step:826/1900 train_time:33855ms step_avg:40.99ms
step:827/1900 train_time:33917ms step_avg:41.01ms
step:828/1900 train_time:33979ms step_avg:41.04ms
step:829/1900 train_time:34040ms step_avg:41.06ms
step:830/1900 train_time:34101ms step_avg:41.09ms
step:831/1900 train_time:34163ms step_avg:41.11ms
step:832/1900 train_time:34224ms step_avg:41.13ms
step:833/1900 train_time:34286ms step_avg:41.16ms
step:834/1900 train_time:34348ms step_avg:41.18ms
step:835/1900 train_time:34410ms step_avg:41.21ms
step:836/1900 train_time:34471ms step_avg:41.23ms
step:837/1900 train_time:34533ms step_avg:41.26ms
step:838/1900 train_time:34594ms step_avg:41.28ms
step:839/1900 train_time:34656ms step_avg:41.31ms
step:840/1900 train_time:34717ms step_avg:41.33ms
step:841/1900 train_time:34779ms step_avg:41.35ms
step:842/1900 train_time:34840ms step_avg:41.38ms
step:843/1900 train_time:34903ms step_avg:41.40ms
step:844/1900 train_time:34964ms step_avg:41.43ms
step:845/1900 train_time:35026ms step_avg:41.45ms
step:846/1900 train_time:35087ms step_avg:41.47ms
step:847/1900 train_time:35148ms step_avg:41.50ms
step:848/1900 train_time:35209ms step_avg:41.52ms
step:849/1900 train_time:35271ms step_avg:41.54ms
step:850/1900 train_time:35332ms step_avg:41.57ms
step:851/1900 train_time:35394ms step_avg:41.59ms
step:852/1900 train_time:35455ms step_avg:41.61ms
step:853/1900 train_time:35517ms step_avg:41.64ms
step:854/1900 train_time:35578ms step_avg:41.66ms
step:855/1900 train_time:35640ms step_avg:41.68ms
step:856/1900 train_time:35701ms step_avg:41.71ms
step:857/1900 train_time:35763ms step_avg:41.73ms
step:858/1900 train_time:35825ms step_avg:41.75ms
step:859/1900 train_time:35887ms step_avg:41.78ms
step:860/1900 train_time:35948ms step_avg:41.80ms
step:861/1900 train_time:36009ms step_avg:41.82ms
step:862/1900 train_time:36070ms step_avg:41.84ms
step:863/1900 train_time:36132ms step_avg:41.87ms
step:864/1900 train_time:36192ms step_avg:41.89ms
step:865/1900 train_time:36254ms step_avg:41.91ms
step:866/1900 train_time:36315ms step_avg:41.93ms
step:867/1900 train_time:36377ms step_avg:41.96ms
step:868/1900 train_time:36438ms step_avg:41.98ms
step:869/1900 train_time:36500ms step_avg:42.00ms
step:870/1900 train_time:36561ms step_avg:42.02ms
step:871/1900 train_time:36623ms step_avg:42.05ms
step:872/1900 train_time:36684ms step_avg:42.07ms
step:873/1900 train_time:36746ms step_avg:42.09ms
step:874/1900 train_time:36807ms step_avg:42.11ms
step:875/1900 train_time:36869ms step_avg:42.14ms
step:876/1900 train_time:36930ms step_avg:42.16ms
step:877/1900 train_time:36992ms step_avg:42.18ms
step:878/1900 train_time:37052ms step_avg:42.20ms
step:879/1900 train_time:37114ms step_avg:42.22ms
step:880/1900 train_time:37175ms step_avg:42.24ms
step:881/1900 train_time:37237ms step_avg:42.27ms
step:882/1900 train_time:37298ms step_avg:42.29ms
step:883/1900 train_time:37359ms step_avg:42.31ms
step:884/1900 train_time:37420ms step_avg:42.33ms
step:885/1900 train_time:37482ms step_avg:42.35ms
step:886/1900 train_time:37543ms step_avg:42.37ms
step:887/1900 train_time:37606ms step_avg:42.40ms
step:888/1900 train_time:37667ms step_avg:42.42ms
step:889/1900 train_time:37729ms step_avg:42.44ms
step:890/1900 train_time:37790ms step_avg:42.46ms
step:891/1900 train_time:37852ms step_avg:42.48ms
step:892/1900 train_time:37913ms step_avg:42.50ms
step:893/1900 train_time:37975ms step_avg:42.53ms
step:894/1900 train_time:38036ms step_avg:42.55ms
step:895/1900 train_time:38098ms step_avg:42.57ms
step:896/1900 train_time:38159ms step_avg:42.59ms
step:897/1900 train_time:38221ms step_avg:42.61ms
step:898/1900 train_time:38282ms step_avg:42.63ms
step:899/1900 train_time:38344ms step_avg:42.65ms
step:900/1900 train_time:38405ms step_avg:42.67ms
step:901/1900 train_time:38467ms step_avg:42.69ms
step:902/1900 train_time:38528ms step_avg:42.71ms
step:903/1900 train_time:38589ms step_avg:42.73ms
step:904/1900 train_time:38650ms step_avg:42.75ms
step:905/1900 train_time:38712ms step_avg:42.78ms
step:906/1900 train_time:38773ms step_avg:42.80ms
step:907/1900 train_time:38835ms step_avg:42.82ms
step:908/1900 train_time:38896ms step_avg:42.84ms
step:909/1900 train_time:38958ms step_avg:42.86ms
step:910/1900 train_time:39018ms step_avg:42.88ms
step:911/1900 train_time:39080ms step_avg:42.90ms
step:912/1900 train_time:39142ms step_avg:42.92ms
step:913/1900 train_time:39203ms step_avg:42.94ms
step:914/1900 train_time:39264ms step_avg:42.96ms
step:915/1900 train_time:39326ms step_avg:42.98ms
step:916/1900 train_time:39388ms step_avg:43.00ms
step:917/1900 train_time:39450ms step_avg:43.02ms
step:918/1900 train_time:39511ms step_avg:43.04ms
step:919/1900 train_time:39572ms step_avg:43.06ms
step:920/1900 train_time:39633ms step_avg:43.08ms
step:921/1900 train_time:39695ms step_avg:43.10ms
step:922/1900 train_time:39756ms step_avg:43.12ms
step:923/1900 train_time:39818ms step_avg:43.14ms
step:924/1900 train_time:39880ms step_avg:43.16ms
step:925/1900 train_time:39942ms step_avg:43.18ms
step:926/1900 train_time:40002ms step_avg:43.20ms
step:927/1900 train_time:40064ms step_avg:43.22ms
step:928/1900 train_time:40125ms step_avg:43.24ms
step:929/1900 train_time:40187ms step_avg:43.26ms
step:930/1900 train_time:40248ms step_avg:43.28ms
step:931/1900 train_time:40310ms step_avg:43.30ms
step:932/1900 train_time:40371ms step_avg:43.32ms
step:933/1900 train_time:40433ms step_avg:43.34ms
step:934/1900 train_time:40493ms step_avg:43.35ms
step:935/1900 train_time:40556ms step_avg:43.37ms
step:936/1900 train_time:40616ms step_avg:43.39ms
step:937/1900 train_time:40678ms step_avg:43.41ms
step:938/1900 train_time:40739ms step_avg:43.43ms
step:939/1900 train_time:40801ms step_avg:43.45ms
step:940/1900 train_time:40862ms step_avg:43.47ms
step:941/1900 train_time:40924ms step_avg:43.49ms
step:942/1900 train_time:40985ms step_avg:43.51ms
step:943/1900 train_time:41047ms step_avg:43.53ms
step:944/1900 train_time:41108ms step_avg:43.55ms
step:945/1900 train_time:41170ms step_avg:43.57ms
step:946/1900 train_time:41231ms step_avg:43.58ms
step:947/1900 train_time:41293ms step_avg:43.60ms
step:948/1900 train_time:41354ms step_avg:43.62ms
step:949/1900 train_time:41415ms step_avg:43.64ms
step:950/1900 train_time:41476ms step_avg:43.66ms
step:951/1900 train_time:41538ms step_avg:43.68ms
step:952/1900 train_time:41599ms step_avg:43.70ms
step:953/1900 train_time:41661ms step_avg:43.72ms
step:954/1900 train_time:41722ms step_avg:43.73ms
step:955/1900 train_time:41784ms step_avg:43.75ms
step:956/1900 train_time:41845ms step_avg:43.77ms
step:957/1900 train_time:41907ms step_avg:43.79ms
step:958/1900 train_time:41968ms step_avg:43.81ms
step:959/1900 train_time:42029ms step_avg:43.83ms
step:960/1900 train_time:42090ms step_avg:43.84ms
step:961/1900 train_time:42152ms step_avg:43.86ms
step:962/1900 train_time:42213ms step_avg:43.88ms
step:963/1900 train_time:42275ms step_avg:43.90ms
step:964/1900 train_time:42336ms step_avg:43.92ms
step:965/1900 train_time:42397ms step_avg:43.94ms
step:966/1900 train_time:42459ms step_avg:43.95ms
step:967/1900 train_time:42521ms step_avg:43.97ms
step:968/1900 train_time:42582ms step_avg:43.99ms
step:969/1900 train_time:42644ms step_avg:44.01ms
step:970/1900 train_time:42706ms step_avg:44.03ms
step:971/1900 train_time:42768ms step_avg:44.05ms
step:972/1900 train_time:42828ms step_avg:44.06ms
step:973/1900 train_time:42890ms step_avg:44.08ms
step:974/1900 train_time:42951ms step_avg:44.10ms
step:975/1900 train_time:43013ms step_avg:44.12ms
step:976/1900 train_time:43074ms step_avg:44.13ms
step:977/1900 train_time:43137ms step_avg:44.15ms
step:978/1900 train_time:43198ms step_avg:44.17ms
step:979/1900 train_time:43261ms step_avg:44.19ms
step:980/1900 train_time:43321ms step_avg:44.21ms
step:981/1900 train_time:43384ms step_avg:44.22ms
step:982/1900 train_time:43445ms step_avg:44.24ms
step:983/1900 train_time:43507ms step_avg:44.26ms
step:984/1900 train_time:43568ms step_avg:44.28ms
step:985/1900 train_time:43630ms step_avg:44.29ms
step:986/1900 train_time:43691ms step_avg:44.31ms
step:987/1900 train_time:43752ms step_avg:44.33ms
step:988/1900 train_time:43813ms step_avg:44.34ms
step:989/1900 train_time:43875ms step_avg:44.36ms
step:990/1900 train_time:43936ms step_avg:44.38ms
step:991/1900 train_time:43998ms step_avg:44.40ms
step:992/1900 train_time:44059ms step_avg:44.41ms
step:993/1900 train_time:44122ms step_avg:44.43ms
step:994/1900 train_time:44184ms step_avg:44.45ms
step:995/1900 train_time:44246ms step_avg:44.47ms
step:996/1900 train_time:44307ms step_avg:44.48ms
step:997/1900 train_time:44369ms step_avg:44.50ms
step:998/1900 train_time:44430ms step_avg:44.52ms
step:999/1900 train_time:44491ms step_avg:44.54ms
step:1000/1900 train_time:44552ms step_avg:44.55ms
step:1000/1900 val_loss:3.7874 train_time:44617ms step_avg:44.62ms
step:1001/1900 train_time:44636ms step_avg:44.59ms
step:1002/1900 train_time:44677ms step_avg:44.59ms
step:1003/1900 train_time:44740ms step_avg:44.61ms
step:1004/1900 train_time:44803ms step_avg:44.62ms
step:1005/1900 train_time:44865ms step_avg:44.64ms
step:1006/1900 train_time:44926ms step_avg:44.66ms
step:1007/1900 train_time:44987ms step_avg:44.67ms
step:1008/1900 train_time:45048ms step_avg:44.69ms
step:1009/1900 train_time:45110ms step_avg:44.71ms
step:1010/1900 train_time:45170ms step_avg:44.72ms
step:1011/1900 train_time:45232ms step_avg:44.74ms
step:1012/1900 train_time:45293ms step_avg:44.76ms
step:1013/1900 train_time:45355ms step_avg:44.77ms
step:1014/1900 train_time:45416ms step_avg:44.79ms
step:1015/1900 train_time:45478ms step_avg:44.81ms
step:1016/1900 train_time:45539ms step_avg:44.82ms
step:1017/1900 train_time:45602ms step_avg:44.84ms
step:1018/1900 train_time:45663ms step_avg:44.86ms
step:1019/1900 train_time:45726ms step_avg:44.87ms
step:1020/1900 train_time:45787ms step_avg:44.89ms
step:1021/1900 train_time:45849ms step_avg:44.91ms
step:1022/1900 train_time:45911ms step_avg:44.92ms
step:1023/1900 train_time:45973ms step_avg:44.94ms
step:1024/1900 train_time:46034ms step_avg:44.95ms
step:1025/1900 train_time:46095ms step_avg:44.97ms
step:1026/1900 train_time:46156ms step_avg:44.99ms
step:1027/1900 train_time:46218ms step_avg:45.00ms
step:1028/1900 train_time:46278ms step_avg:45.02ms
step:1029/1900 train_time:46340ms step_avg:45.03ms
step:1030/1900 train_time:46400ms step_avg:45.05ms
step:1031/1900 train_time:46462ms step_avg:45.06ms
step:1032/1900 train_time:46523ms step_avg:45.08ms
step:1033/1900 train_time:46585ms step_avg:45.10ms
step:1034/1900 train_time:46646ms step_avg:45.11ms
step:1035/1900 train_time:46709ms step_avg:45.13ms
step:1036/1900 train_time:46771ms step_avg:45.15ms
step:1037/1900 train_time:46833ms step_avg:45.16ms
step:1038/1900 train_time:46895ms step_avg:45.18ms
step:1039/1900 train_time:46956ms step_avg:45.19ms
step:1040/1900 train_time:47017ms step_avg:45.21ms
step:1041/1900 train_time:47079ms step_avg:45.22ms
step:1042/1900 train_time:47139ms step_avg:45.24ms
step:1043/1900 train_time:47201ms step_avg:45.25ms
step:1044/1900 train_time:47261ms step_avg:45.27ms
step:1045/1900 train_time:47323ms step_avg:45.29ms
step:1046/1900 train_time:47384ms step_avg:45.30ms
step:1047/1900 train_time:47446ms step_avg:45.32ms
step:1048/1900 train_time:47507ms step_avg:45.33ms
step:1049/1900 train_time:47569ms step_avg:45.35ms
step:1050/1900 train_time:47630ms step_avg:45.36ms
step:1051/1900 train_time:47692ms step_avg:45.38ms
step:1052/1900 train_time:47754ms step_avg:45.39ms
step:1053/1900 train_time:47816ms step_avg:45.41ms
step:1054/1900 train_time:47877ms step_avg:45.42ms
step:1055/1900 train_time:47939ms step_avg:45.44ms
step:1056/1900 train_time:48000ms step_avg:45.45ms
step:1057/1900 train_time:48061ms step_avg:45.47ms
step:1058/1900 train_time:48122ms step_avg:45.48ms
step:1059/1900 train_time:48183ms step_avg:45.50ms
step:1060/1900 train_time:48244ms step_avg:45.51ms
step:1061/1900 train_time:48306ms step_avg:45.53ms
step:1062/1900 train_time:48366ms step_avg:45.54ms
step:1063/1900 train_time:48428ms step_avg:45.56ms
step:1064/1900 train_time:48489ms step_avg:45.57ms
step:1065/1900 train_time:48551ms step_avg:45.59ms
step:1066/1900 train_time:48612ms step_avg:45.60ms
step:1067/1900 train_time:48674ms step_avg:45.62ms
step:1068/1900 train_time:48735ms step_avg:45.63ms
step:1069/1900 train_time:48797ms step_avg:45.65ms
step:1070/1900 train_time:48858ms step_avg:45.66ms
step:1071/1900 train_time:48920ms step_avg:45.68ms
step:1072/1900 train_time:48982ms step_avg:45.69ms
step:1073/1900 train_time:49043ms step_avg:45.71ms
step:1074/1900 train_time:49104ms step_avg:45.72ms
step:1075/1900 train_time:49165ms step_avg:45.74ms
step:1076/1900 train_time:49226ms step_avg:45.75ms
step:1077/1900 train_time:49288ms step_avg:45.76ms
step:1078/1900 train_time:49348ms step_avg:45.78ms
step:1079/1900 train_time:49411ms step_avg:45.79ms
step:1080/1900 train_time:49472ms step_avg:45.81ms
step:1081/1900 train_time:49534ms step_avg:45.82ms
step:1082/1900 train_time:49595ms step_avg:45.84ms
step:1083/1900 train_time:49656ms step_avg:45.85ms
step:1084/1900 train_time:49717ms step_avg:45.86ms
step:1085/1900 train_time:49780ms step_avg:45.88ms
step:1086/1900 train_time:49841ms step_avg:45.89ms
step:1087/1900 train_time:49903ms step_avg:45.91ms
step:1088/1900 train_time:49964ms step_avg:45.92ms
step:1089/1900 train_time:50025ms step_avg:45.94ms
step:1090/1900 train_time:50087ms step_avg:45.95ms
step:1091/1900 train_time:50149ms step_avg:45.97ms
step:1092/1900 train_time:50209ms step_avg:45.98ms
step:1093/1900 train_time:50271ms step_avg:45.99ms
step:1094/1900 train_time:50332ms step_avg:46.01ms
step:1095/1900 train_time:50394ms step_avg:46.02ms
step:1096/1900 train_time:50455ms step_avg:46.04ms
step:1097/1900 train_time:50516ms step_avg:46.05ms
step:1098/1900 train_time:50577ms step_avg:46.06ms
step:1099/1900 train_time:50639ms step_avg:46.08ms
step:1100/1900 train_time:50700ms step_avg:46.09ms
step:1101/1900 train_time:50761ms step_avg:46.10ms
step:1102/1900 train_time:50822ms step_avg:46.12ms
step:1103/1900 train_time:50884ms step_avg:46.13ms
step:1104/1900 train_time:50945ms step_avg:46.15ms
step:1105/1900 train_time:51006ms step_avg:46.16ms
step:1106/1900 train_time:51067ms step_avg:46.17ms
step:1107/1900 train_time:51129ms step_avg:46.19ms
step:1108/1900 train_time:51190ms step_avg:46.20ms
step:1109/1900 train_time:51252ms step_avg:46.21ms
step:1110/1900 train_time:51313ms step_avg:46.23ms
step:1111/1900 train_time:51376ms step_avg:46.24ms
step:1112/1900 train_time:51437ms step_avg:46.26ms
step:1113/1900 train_time:51500ms step_avg:46.27ms
step:1114/1900 train_time:51561ms step_avg:46.28ms
step:1115/1900 train_time:51622ms step_avg:46.30ms
step:1116/1900 train_time:51683ms step_avg:46.31ms
step:1117/1900 train_time:51745ms step_avg:46.33ms
step:1118/1900 train_time:51806ms step_avg:46.34ms
step:1119/1900 train_time:51868ms step_avg:46.35ms
step:1120/1900 train_time:51929ms step_avg:46.37ms
step:1121/1900 train_time:51991ms step_avg:46.38ms
step:1122/1900 train_time:52052ms step_avg:46.39ms
step:1123/1900 train_time:52113ms step_avg:46.41ms
step:1124/1900 train_time:52174ms step_avg:46.42ms
step:1125/1900 train_time:52236ms step_avg:46.43ms
step:1126/1900 train_time:52297ms step_avg:46.44ms
step:1127/1900 train_time:52359ms step_avg:46.46ms
step:1128/1900 train_time:52420ms step_avg:46.47ms
step:1129/1900 train_time:52481ms step_avg:46.48ms
step:1130/1900 train_time:52543ms step_avg:46.50ms
step:1131/1900 train_time:52604ms step_avg:46.51ms
step:1132/1900 train_time:52665ms step_avg:46.52ms
step:1133/1900 train_time:52727ms step_avg:46.54ms
step:1134/1900 train_time:52788ms step_avg:46.55ms
step:1135/1900 train_time:52850ms step_avg:46.56ms
step:1136/1900 train_time:52911ms step_avg:46.58ms
step:1137/1900 train_time:52972ms step_avg:46.59ms
step:1138/1900 train_time:53033ms step_avg:46.60ms
step:1139/1900 train_time:53094ms step_avg:46.61ms
step:1140/1900 train_time:53155ms step_avg:46.63ms
step:1141/1900 train_time:53217ms step_avg:46.64ms
step:1142/1900 train_time:53278ms step_avg:46.65ms
step:1143/1900 train_time:53340ms step_avg:46.67ms
step:1144/1900 train_time:53401ms step_avg:46.68ms
step:1145/1900 train_time:53463ms step_avg:46.69ms
step:1146/1900 train_time:53524ms step_avg:46.70ms
step:1147/1900 train_time:53586ms step_avg:46.72ms
step:1148/1900 train_time:53646ms step_avg:46.73ms
step:1149/1900 train_time:53708ms step_avg:46.74ms
step:1150/1900 train_time:53769ms step_avg:46.76ms
step:1151/1900 train_time:53831ms step_avg:46.77ms
step:1152/1900 train_time:53892ms step_avg:46.78ms
step:1153/1900 train_time:53954ms step_avg:46.79ms
step:1154/1900 train_time:54015ms step_avg:46.81ms
step:1155/1900 train_time:54077ms step_avg:46.82ms
step:1156/1900 train_time:54138ms step_avg:46.83ms
step:1157/1900 train_time:54200ms step_avg:46.84ms
step:1158/1900 train_time:54260ms step_avg:46.86ms
step:1159/1900 train_time:54322ms step_avg:46.87ms
step:1160/1900 train_time:54383ms step_avg:46.88ms
step:1161/1900 train_time:54445ms step_avg:46.89ms
step:1162/1900 train_time:54505ms step_avg:46.91ms
step:1163/1900 train_time:54567ms step_avg:46.92ms
step:1164/1900 train_time:54628ms step_avg:46.93ms
step:1165/1900 train_time:54690ms step_avg:46.94ms
step:1166/1900 train_time:54752ms step_avg:46.96ms
step:1167/1900 train_time:54814ms step_avg:46.97ms
step:1168/1900 train_time:54875ms step_avg:46.98ms
step:1169/1900 train_time:54936ms step_avg:46.99ms
step:1170/1900 train_time:54997ms step_avg:47.01ms
step:1171/1900 train_time:55060ms step_avg:47.02ms
step:1172/1900 train_time:55120ms step_avg:47.03ms
step:1173/1900 train_time:55182ms step_avg:47.04ms
step:1174/1900 train_time:55243ms step_avg:47.06ms
step:1175/1900 train_time:55305ms step_avg:47.07ms
step:1176/1900 train_time:55366ms step_avg:47.08ms
step:1177/1900 train_time:55428ms step_avg:47.09ms
step:1178/1900 train_time:55489ms step_avg:47.10ms
step:1179/1900 train_time:55551ms step_avg:47.12ms
step:1180/1900 train_time:55612ms step_avg:47.13ms
step:1181/1900 train_time:55674ms step_avg:47.14ms
step:1182/1900 train_time:55736ms step_avg:47.15ms
step:1183/1900 train_time:55798ms step_avg:47.17ms
step:1184/1900 train_time:55859ms step_avg:47.18ms
step:1185/1900 train_time:55920ms step_avg:47.19ms
step:1186/1900 train_time:55981ms step_avg:47.20ms
step:1187/1900 train_time:56043ms step_avg:47.21ms
step:1188/1900 train_time:56104ms step_avg:47.23ms
step:1189/1900 train_time:56165ms step_avg:47.24ms
step:1190/1900 train_time:56226ms step_avg:47.25ms
step:1191/1900 train_time:56288ms step_avg:47.26ms
step:1192/1900 train_time:56349ms step_avg:47.27ms
step:1193/1900 train_time:56411ms step_avg:47.29ms
step:1194/1900 train_time:56472ms step_avg:47.30ms
step:1195/1900 train_time:56535ms step_avg:47.31ms
step:1196/1900 train_time:56596ms step_avg:47.32ms
step:1197/1900 train_time:56658ms step_avg:47.33ms
step:1198/1900 train_time:56719ms step_avg:47.35ms
step:1199/1900 train_time:56781ms step_avg:47.36ms
step:1200/1900 train_time:56841ms step_avg:47.37ms
step:1201/1900 train_time:56903ms step_avg:47.38ms
step:1202/1900 train_time:56964ms step_avg:47.39ms
step:1203/1900 train_time:57027ms step_avg:47.40ms
step:1204/1900 train_time:57087ms step_avg:47.41ms
step:1205/1900 train_time:57149ms step_avg:47.43ms
step:1206/1900 train_time:57210ms step_avg:47.44ms
step:1207/1900 train_time:57272ms step_avg:47.45ms
step:1208/1900 train_time:57333ms step_avg:47.46ms
step:1209/1900 train_time:57395ms step_avg:47.47ms
step:1210/1900 train_time:57457ms step_avg:47.48ms
step:1211/1900 train_time:57519ms step_avg:47.50ms
step:1212/1900 train_time:57580ms step_avg:47.51ms
step:1213/1900 train_time:57641ms step_avg:47.52ms
step:1214/1900 train_time:57702ms step_avg:47.53ms
step:1215/1900 train_time:57763ms step_avg:47.54ms
step:1216/1900 train_time:57824ms step_avg:47.55ms
step:1217/1900 train_time:57886ms step_avg:47.56ms
step:1218/1900 train_time:57947ms step_avg:47.58ms
step:1219/1900 train_time:58009ms step_avg:47.59ms
step:1220/1900 train_time:58070ms step_avg:47.60ms
step:1221/1900 train_time:58132ms step_avg:47.61ms
step:1222/1900 train_time:58193ms step_avg:47.62ms
step:1223/1900 train_time:58255ms step_avg:47.63ms
step:1224/1900 train_time:58316ms step_avg:47.64ms
step:1225/1900 train_time:58378ms step_avg:47.66ms
step:1226/1900 train_time:58439ms step_avg:47.67ms
step:1227/1900 train_time:58500ms step_avg:47.68ms
step:1228/1900 train_time:58561ms step_avg:47.69ms
step:1229/1900 train_time:58623ms step_avg:47.70ms
step:1230/1900 train_time:58684ms step_avg:47.71ms
step:1231/1900 train_time:58745ms step_avg:47.72ms
step:1232/1900 train_time:58806ms step_avg:47.73ms
step:1233/1900 train_time:58868ms step_avg:47.74ms
step:1234/1900 train_time:58929ms step_avg:47.75ms
step:1235/1900 train_time:58990ms step_avg:47.77ms
step:1236/1900 train_time:59051ms step_avg:47.78ms
step:1237/1900 train_time:59112ms step_avg:47.79ms
step:1238/1900 train_time:59173ms step_avg:47.80ms
step:1239/1900 train_time:59235ms step_avg:47.81ms
step:1240/1900 train_time:59296ms step_avg:47.82ms
step:1241/1900 train_time:59358ms step_avg:47.83ms
step:1242/1900 train_time:59446ms step_avg:47.86ms
step:1243/1900 train_time:59536ms step_avg:47.90ms
step:1244/1900 train_time:59624ms step_avg:47.93ms
step:1245/1900 train_time:59712ms step_avg:47.96ms
step:1246/1900 train_time:59800ms step_avg:47.99ms
step:1247/1900 train_time:59888ms step_avg:48.03ms
step:1248/1900 train_time:59975ms step_avg:48.06ms
step:1249/1900 train_time:60063ms step_avg:48.09ms
step:1250/1900 train_time:60151ms step_avg:48.12ms
step:1250/1900 val_loss:3.5444 train_time:60242ms step_avg:48.19ms
step:1251/1900 train_time:60263ms step_avg:48.17ms
step:1252/1900 train_time:60329ms step_avg:48.19ms
step:1253/1900 train_time:60419ms step_avg:48.22ms
step:1254/1900 train_time:60507ms step_avg:48.25ms
step:1255/1900 train_time:60595ms step_avg:48.28ms
step:1256/1900 train_time:60682ms step_avg:48.31ms
step:1257/1900 train_time:60770ms step_avg:48.35ms
step:1258/1900 train_time:60857ms step_avg:48.38ms
step:1259/1900 train_time:60945ms step_avg:48.41ms
step:1260/1900 train_time:61033ms step_avg:48.44ms
step:1261/1900 train_time:61121ms step_avg:48.47ms
step:1262/1900 train_time:61209ms step_avg:48.50ms
step:1263/1900 train_time:61299ms step_avg:48.53ms
step:1264/1900 train_time:61387ms step_avg:48.57ms
step:1265/1900 train_time:61476ms step_avg:48.60ms
step:1266/1900 train_time:61563ms step_avg:48.63ms
step:1267/1900 train_time:61652ms step_avg:48.66ms
step:1268/1900 train_time:61739ms step_avg:48.69ms
step:1269/1900 train_time:61827ms step_avg:48.72ms
step:1270/1900 train_time:61914ms step_avg:48.75ms
step:1271/1900 train_time:62003ms step_avg:48.78ms
step:1272/1900 train_time:62091ms step_avg:48.81ms
step:1273/1900 train_time:62180ms step_avg:48.85ms
step:1274/1900 train_time:62267ms step_avg:48.88ms
step:1275/1900 train_time:62357ms step_avg:48.91ms
step:1276/1900 train_time:62444ms step_avg:48.94ms
step:1277/1900 train_time:62533ms step_avg:48.97ms
step:1278/1900 train_time:62620ms step_avg:49.00ms
step:1279/1900 train_time:62709ms step_avg:49.03ms
step:1280/1900 train_time:62797ms step_avg:49.06ms
step:1281/1900 train_time:62885ms step_avg:49.09ms
step:1282/1900 train_time:62972ms step_avg:49.12ms
step:1283/1900 train_time:63061ms step_avg:49.15ms
step:1284/1900 train_time:63148ms step_avg:49.18ms
step:1285/1900 train_time:63237ms step_avg:49.21ms
step:1286/1900 train_time:63325ms step_avg:49.24ms
step:1287/1900 train_time:63414ms step_avg:49.27ms
step:1288/1900 train_time:63501ms step_avg:49.30ms
step:1289/1900 train_time:63589ms step_avg:49.33ms
step:1290/1900 train_time:63678ms step_avg:49.36ms
step:1291/1900 train_time:63766ms step_avg:49.39ms
step:1292/1900 train_time:63853ms step_avg:49.42ms
step:1293/1900 train_time:63941ms step_avg:49.45ms
step:1294/1900 train_time:64028ms step_avg:49.48ms
step:1295/1900 train_time:64116ms step_avg:49.51ms
step:1296/1900 train_time:64204ms step_avg:49.54ms
step:1297/1900 train_time:64294ms step_avg:49.57ms
step:1298/1900 train_time:64382ms step_avg:49.60ms
step:1299/1900 train_time:64471ms step_avg:49.63ms
step:1300/1900 train_time:64558ms step_avg:49.66ms
step:1301/1900 train_time:64647ms step_avg:49.69ms
step:1302/1900 train_time:64734ms step_avg:49.72ms
step:1303/1900 train_time:64823ms step_avg:49.75ms
step:1304/1900 train_time:64910ms step_avg:49.78ms
step:1305/1900 train_time:64999ms step_avg:49.81ms
step:1306/1900 train_time:65086ms step_avg:49.84ms
step:1307/1900 train_time:65175ms step_avg:49.87ms
step:1308/1900 train_time:65264ms step_avg:49.90ms
step:1309/1900 train_time:65352ms step_avg:49.93ms
step:1310/1900 train_time:65440ms step_avg:49.95ms
step:1311/1900 train_time:65529ms step_avg:49.98ms
step:1312/1900 train_time:65616ms step_avg:50.01ms
step:1313/1900 train_time:65705ms step_avg:50.04ms
step:1314/1900 train_time:65792ms step_avg:50.07ms
step:1315/1900 train_time:65881ms step_avg:50.10ms
step:1316/1900 train_time:65968ms step_avg:50.13ms
step:1317/1900 train_time:66057ms step_avg:50.16ms
step:1318/1900 train_time:66144ms step_avg:50.19ms
step:1319/1900 train_time:66233ms step_avg:50.21ms
step:1320/1900 train_time:66320ms step_avg:50.24ms
step:1321/1900 train_time:66409ms step_avg:50.27ms
step:1322/1900 train_time:66496ms step_avg:50.30ms
step:1323/1900 train_time:66586ms step_avg:50.33ms
step:1324/1900 train_time:66673ms step_avg:50.36ms
step:1325/1900 train_time:66762ms step_avg:50.39ms
step:1326/1900 train_time:66849ms step_avg:50.41ms
step:1327/1900 train_time:66938ms step_avg:50.44ms
step:1328/1900 train_time:67026ms step_avg:50.47ms
step:1329/1900 train_time:67114ms step_avg:50.50ms
step:1330/1900 train_time:67202ms step_avg:50.53ms
step:1331/1900 train_time:67290ms step_avg:50.56ms
step:1332/1900 train_time:67378ms step_avg:50.58ms
step:1333/1900 train_time:67466ms step_avg:50.61ms
step:1334/1900 train_time:67554ms step_avg:50.64ms
step:1335/1900 train_time:67642ms step_avg:50.67ms
step:1336/1900 train_time:67730ms step_avg:50.70ms
step:1337/1900 train_time:67819ms step_avg:50.72ms
step:1338/1900 train_time:67907ms step_avg:50.75ms
step:1339/1900 train_time:67996ms step_avg:50.78ms
step:1340/1900 train_time:68083ms step_avg:50.81ms
step:1341/1900 train_time:68173ms step_avg:50.84ms
step:1342/1900 train_time:68260ms step_avg:50.86ms
step:1343/1900 train_time:68349ms step_avg:50.89ms
step:1344/1900 train_time:68436ms step_avg:50.92ms
step:1345/1900 train_time:68526ms step_avg:50.95ms
step:1346/1900 train_time:68613ms step_avg:50.98ms
step:1347/1900 train_time:68702ms step_avg:51.00ms
step:1348/1900 train_time:68789ms step_avg:51.03ms
step:1349/1900 train_time:68877ms step_avg:51.06ms
step:1350/1900 train_time:68965ms step_avg:51.09ms
step:1351/1900 train_time:69053ms step_avg:51.11ms
step:1352/1900 train_time:69141ms step_avg:51.14ms
step:1353/1900 train_time:69229ms step_avg:51.17ms
step:1354/1900 train_time:69317ms step_avg:51.19ms
step:1355/1900 train_time:69405ms step_avg:51.22ms
step:1356/1900 train_time:69492ms step_avg:51.25ms
step:1357/1900 train_time:69581ms step_avg:51.28ms
step:1358/1900 train_time:69669ms step_avg:51.30ms
step:1359/1900 train_time:69757ms step_avg:51.33ms
step:1360/1900 train_time:69845ms step_avg:51.36ms
step:1361/1900 train_time:69934ms step_avg:51.38ms
step:1362/1900 train_time:70022ms step_avg:51.41ms
step:1363/1900 train_time:70110ms step_avg:51.44ms
step:1364/1900 train_time:70198ms step_avg:51.46ms
step:1365/1900 train_time:70286ms step_avg:51.49ms
step:1366/1900 train_time:70374ms step_avg:51.52ms
step:1367/1900 train_time:70462ms step_avg:51.54ms
step:1368/1900 train_time:70550ms step_avg:51.57ms
step:1369/1900 train_time:70638ms step_avg:51.60ms
step:1370/1900 train_time:70726ms step_avg:51.62ms
step:1371/1900 train_time:70815ms step_avg:51.65ms
step:1372/1900 train_time:70903ms step_avg:51.68ms
step:1373/1900 train_time:70992ms step_avg:51.71ms
step:1374/1900 train_time:71080ms step_avg:51.73ms
step:1375/1900 train_time:71169ms step_avg:51.76ms
step:1376/1900 train_time:71256ms step_avg:51.78ms
step:1377/1900 train_time:71344ms step_avg:51.81ms
step:1378/1900 train_time:71432ms step_avg:51.84ms
step:1379/1900 train_time:71520ms step_avg:51.86ms
step:1380/1900 train_time:71608ms step_avg:51.89ms
step:1381/1900 train_time:71697ms step_avg:51.92ms
step:1382/1900 train_time:71785ms step_avg:51.94ms
step:1383/1900 train_time:71875ms step_avg:51.97ms
step:1384/1900 train_time:71963ms step_avg:52.00ms
step:1385/1900 train_time:72051ms step_avg:52.02ms
step:1386/1900 train_time:72138ms step_avg:52.05ms
step:1387/1900 train_time:72227ms step_avg:52.07ms
step:1388/1900 train_time:72315ms step_avg:52.10ms
step:1389/1900 train_time:72404ms step_avg:52.13ms
step:1390/1900 train_time:72491ms step_avg:52.15ms
step:1391/1900 train_time:72579ms step_avg:52.18ms
step:1392/1900 train_time:72666ms step_avg:52.20ms
step:1393/1900 train_time:72754ms step_avg:52.23ms
step:1394/1900 train_time:72842ms step_avg:52.25ms
step:1395/1900 train_time:72930ms step_avg:52.28ms
step:1396/1900 train_time:73018ms step_avg:52.30ms
step:1397/1900 train_time:73107ms step_avg:52.33ms
step:1398/1900 train_time:73194ms step_avg:52.36ms
step:1399/1900 train_time:73282ms step_avg:52.38ms
step:1400/1900 train_time:73370ms step_avg:52.41ms
step:1401/1900 train_time:73459ms step_avg:52.43ms
step:1402/1900 train_time:73547ms step_avg:52.46ms
step:1403/1900 train_time:73634ms step_avg:52.48ms
step:1404/1900 train_time:73722ms step_avg:52.51ms
step:1405/1900 train_time:73810ms step_avg:52.53ms
step:1406/1900 train_time:73898ms step_avg:52.56ms
step:1407/1900 train_time:73986ms step_avg:52.58ms
step:1408/1900 train_time:74074ms step_avg:52.61ms
step:1409/1900 train_time:74163ms step_avg:52.63ms
step:1410/1900 train_time:74251ms step_avg:52.66ms
step:1411/1900 train_time:74339ms step_avg:52.69ms
step:1412/1900 train_time:74426ms step_avg:52.71ms
step:1413/1900 train_time:74515ms step_avg:52.74ms
step:1414/1900 train_time:74603ms step_avg:52.76ms
step:1415/1900 train_time:74691ms step_avg:52.79ms
step:1416/1900 train_time:74779ms step_avg:52.81ms
step:1417/1900 train_time:74867ms step_avg:52.84ms
step:1418/1900 train_time:74955ms step_avg:52.86ms
step:1419/1900 train_time:75044ms step_avg:52.89ms
step:1420/1900 train_time:75131ms step_avg:52.91ms
step:1421/1900 train_time:75219ms step_avg:52.93ms
step:1422/1900 train_time:75307ms step_avg:52.96ms
step:1423/1900 train_time:75394ms step_avg:52.98ms
step:1424/1900 train_time:75483ms step_avg:53.01ms
step:1425/1900 train_time:75571ms step_avg:53.03ms
step:1426/1900 train_time:75659ms step_avg:53.06ms
step:1427/1900 train_time:75748ms step_avg:53.08ms
step:1428/1900 train_time:75835ms step_avg:53.11ms
step:1429/1900 train_time:75923ms step_avg:53.13ms
step:1430/1900 train_time:76011ms step_avg:53.15ms
step:1431/1900 train_time:76099ms step_avg:53.18ms
step:1432/1900 train_time:76186ms step_avg:53.20ms
step:1433/1900 train_time:76275ms step_avg:53.23ms
step:1434/1900 train_time:76362ms step_avg:53.25ms
step:1435/1900 train_time:76451ms step_avg:53.28ms
step:1436/1900 train_time:76539ms step_avg:53.30ms
step:1437/1900 train_time:76628ms step_avg:53.32ms
step:1438/1900 train_time:76715ms step_avg:53.35ms
step:1439/1900 train_time:76803ms step_avg:53.37ms
step:1440/1900 train_time:76891ms step_avg:53.40ms
step:1441/1900 train_time:76980ms step_avg:53.42ms
step:1442/1900 train_time:77067ms step_avg:53.44ms
step:1443/1900 train_time:77155ms step_avg:53.47ms
step:1444/1900 train_time:77243ms step_avg:53.49ms
step:1445/1900 train_time:77333ms step_avg:53.52ms
step:1446/1900 train_time:77420ms step_avg:53.54ms
step:1447/1900 train_time:77509ms step_avg:53.57ms
step:1448/1900 train_time:77596ms step_avg:53.59ms
step:1449/1900 train_time:77685ms step_avg:53.61ms
step:1450/1900 train_time:77772ms step_avg:53.64ms
step:1451/1900 train_time:77861ms step_avg:53.66ms
step:1452/1900 train_time:77949ms step_avg:53.68ms
step:1453/1900 train_time:78037ms step_avg:53.71ms
step:1454/1900 train_time:78124ms step_avg:53.73ms
step:1455/1900 train_time:78214ms step_avg:53.76ms
step:1456/1900 train_time:78301ms step_avg:53.78ms
step:1457/1900 train_time:78389ms step_avg:53.80ms
step:1458/1900 train_time:78478ms step_avg:53.83ms
step:1459/1900 train_time:78567ms step_avg:53.85ms
step:1460/1900 train_time:78654ms step_avg:53.87ms
step:1461/1900 train_time:78742ms step_avg:53.90ms
step:1462/1900 train_time:78830ms step_avg:53.92ms
step:1463/1900 train_time:78918ms step_avg:53.94ms
step:1464/1900 train_time:79006ms step_avg:53.97ms
step:1465/1900 train_time:79095ms step_avg:53.99ms
step:1466/1900 train_time:79181ms step_avg:54.01ms
step:1467/1900 train_time:79271ms step_avg:54.04ms
step:1468/1900 train_time:79357ms step_avg:54.06ms
step:1469/1900 train_time:79446ms step_avg:54.08ms
step:1470/1900 train_time:79534ms step_avg:54.10ms
step:1471/1900 train_time:79623ms step_avg:54.13ms
step:1472/1900 train_time:79711ms step_avg:54.15ms
step:1473/1900 train_time:79800ms step_avg:54.18ms
step:1474/1900 train_time:79888ms step_avg:54.20ms
step:1475/1900 train_time:79977ms step_avg:54.22ms
step:1476/1900 train_time:80065ms step_avg:54.24ms
step:1477/1900 train_time:80153ms step_avg:54.27ms
step:1478/1900 train_time:80241ms step_avg:54.29ms
step:1479/1900 train_time:80329ms step_avg:54.31ms
step:1480/1900 train_time:80416ms step_avg:54.34ms
step:1481/1900 train_time:80505ms step_avg:54.36ms
step:1482/1900 train_time:80594ms step_avg:54.38ms
step:1483/1900 train_time:80682ms step_avg:54.40ms
step:1484/1900 train_time:80769ms step_avg:54.43ms
step:1485/1900 train_time:80857ms step_avg:54.45ms
step:1486/1900 train_time:80945ms step_avg:54.47ms
step:1487/1900 train_time:81034ms step_avg:54.49ms
step:1488/1900 train_time:81121ms step_avg:54.52ms
step:1489/1900 train_time:81210ms step_avg:54.54ms
step:1490/1900 train_time:81298ms step_avg:54.56ms
step:1491/1900 train_time:81387ms step_avg:54.59ms
step:1492/1900 train_time:81474ms step_avg:54.61ms
step:1493/1900 train_time:81562ms step_avg:54.63ms
step:1494/1900 train_time:81649ms step_avg:54.65ms
step:1495/1900 train_time:81737ms step_avg:54.67ms
step:1496/1900 train_time:81825ms step_avg:54.70ms
step:1497/1900 train_time:81914ms step_avg:54.72ms
step:1498/1900 train_time:82001ms step_avg:54.74ms
step:1499/1900 train_time:82089ms step_avg:54.76ms
step:1500/1900 train_time:82177ms step_avg:54.78ms
step:1500/1900 val_loss:3.4144 train_time:82268ms step_avg:54.85ms
step:1501/1900 train_time:82288ms step_avg:54.82ms
step:1502/1900 train_time:82356ms step_avg:54.83ms
step:1503/1900 train_time:82447ms step_avg:54.85ms
step:1504/1900 train_time:82534ms step_avg:54.88ms
step:1505/1900 train_time:82623ms step_avg:54.90ms
step:1506/1900 train_time:82709ms step_avg:54.92ms
step:1507/1900 train_time:82796ms step_avg:54.94ms
step:1508/1900 train_time:82883ms step_avg:54.96ms
step:1509/1900 train_time:82971ms step_avg:54.98ms
step:1510/1900 train_time:83058ms step_avg:55.00ms
step:1511/1900 train_time:83146ms step_avg:55.03ms
step:1512/1900 train_time:83236ms step_avg:55.05ms
step:1513/1900 train_time:83326ms step_avg:55.07ms
step:1514/1900 train_time:83415ms step_avg:55.10ms
step:1515/1900 train_time:83504ms step_avg:55.12ms
step:1516/1900 train_time:83591ms step_avg:55.14ms
step:1517/1900 train_time:83680ms step_avg:55.16ms
step:1518/1900 train_time:83767ms step_avg:55.18ms
step:1519/1900 train_time:83854ms step_avg:55.20ms
step:1520/1900 train_time:83941ms step_avg:55.22ms
step:1521/1900 train_time:84029ms step_avg:55.25ms
step:1522/1900 train_time:84116ms step_avg:55.27ms
step:1523/1900 train_time:84205ms step_avg:55.29ms
step:1524/1900 train_time:84295ms step_avg:55.31ms
step:1525/1900 train_time:84383ms step_avg:55.33ms
step:1526/1900 train_time:84471ms step_avg:55.35ms
step:1527/1900 train_time:84561ms step_avg:55.38ms
step:1528/1900 train_time:84648ms step_avg:55.40ms
step:1529/1900 train_time:84736ms step_avg:55.42ms
step:1530/1900 train_time:84823ms step_avg:55.44ms
step:1531/1900 train_time:84911ms step_avg:55.46ms
step:1532/1900 train_time:84997ms step_avg:55.48ms
step:1533/1900 train_time:85087ms step_avg:55.50ms
step:1534/1900 train_time:85174ms step_avg:55.52ms
step:1535/1900 train_time:85263ms step_avg:55.55ms
step:1536/1900 train_time:85352ms step_avg:55.57ms
step:1537/1900 train_time:85441ms step_avg:55.59ms
step:1538/1900 train_time:85529ms step_avg:55.61ms
step:1539/1900 train_time:85618ms step_avg:55.63ms
step:1540/1900 train_time:85705ms step_avg:55.65ms
step:1541/1900 train_time:85793ms step_avg:55.67ms
step:1542/1900 train_time:85881ms step_avg:55.69ms
step:1543/1900 train_time:85969ms step_avg:55.72ms
step:1544/1900 train_time:86056ms step_avg:55.74ms
step:1545/1900 train_time:86145ms step_avg:55.76ms
step:1546/1900 train_time:86233ms step_avg:55.78ms
step:1547/1900 train_time:86323ms step_avg:55.80ms
step:1548/1900 train_time:86411ms step_avg:55.82ms
step:1549/1900 train_time:86499ms step_avg:55.84ms
step:1550/1900 train_time:86587ms step_avg:55.86ms
step:1551/1900 train_time:86675ms step_avg:55.88ms
step:1552/1900 train_time:86762ms step_avg:55.90ms
step:1553/1900 train_time:86851ms step_avg:55.92ms
step:1554/1900 train_time:86938ms step_avg:55.94ms
step:1555/1900 train_time:87026ms step_avg:55.97ms
step:1556/1900 train_time:87113ms step_avg:55.99ms
step:1557/1900 train_time:87202ms step_avg:56.01ms
step:1558/1900 train_time:87289ms step_avg:56.03ms
step:1559/1900 train_time:87378ms step_avg:56.05ms
step:1560/1900 train_time:87465ms step_avg:56.07ms
step:1561/1900 train_time:87554ms step_avg:56.09ms
step:1562/1900 train_time:87641ms step_avg:56.11ms
step:1563/1900 train_time:87729ms step_avg:56.13ms
step:1564/1900 train_time:87817ms step_avg:56.15ms
step:1565/1900 train_time:87906ms step_avg:56.17ms
step:1566/1900 train_time:87992ms step_avg:56.19ms
step:1567/1900 train_time:88080ms step_avg:56.21ms
step:1568/1900 train_time:88168ms step_avg:56.23ms
step:1569/1900 train_time:88256ms step_avg:56.25ms
step:1570/1900 train_time:88344ms step_avg:56.27ms
step:1571/1900 train_time:88434ms step_avg:56.29ms
step:1572/1900 train_time:88522ms step_avg:56.31ms
step:1573/1900 train_time:88610ms step_avg:56.33ms
step:1574/1900 train_time:88697ms step_avg:56.35ms
step:1575/1900 train_time:88786ms step_avg:56.37ms
step:1576/1900 train_time:88873ms step_avg:56.39ms
step:1577/1900 train_time:88961ms step_avg:56.41ms
step:1578/1900 train_time:89049ms step_avg:56.43ms
step:1579/1900 train_time:89138ms step_avg:56.45ms
step:1580/1900 train_time:89225ms step_avg:56.47ms
step:1581/1900 train_time:89316ms step_avg:56.49ms
step:1582/1900 train_time:89404ms step_avg:56.51ms
step:1583/1900 train_time:89494ms step_avg:56.53ms
step:1584/1900 train_time:89582ms step_avg:56.55ms
step:1585/1900 train_time:89670ms step_avg:56.57ms
step:1586/1900 train_time:89758ms step_avg:56.59ms
step:1587/1900 train_time:89847ms step_avg:56.61ms
step:1588/1900 train_time:89934ms step_avg:56.63ms
step:1589/1900 train_time:90023ms step_avg:56.65ms
step:1590/1900 train_time:90110ms step_avg:56.67ms
step:1591/1900 train_time:90198ms step_avg:56.69ms
step:1592/1900 train_time:90286ms step_avg:56.71ms
step:1593/1900 train_time:90375ms step_avg:56.73ms
step:1594/1900 train_time:90463ms step_avg:56.75ms
step:1595/1900 train_time:90554ms step_avg:56.77ms
step:1596/1900 train_time:90641ms step_avg:56.79ms
step:1597/1900 train_time:90730ms step_avg:56.81ms
step:1598/1900 train_time:90817ms step_avg:56.83ms
step:1599/1900 train_time:90905ms step_avg:56.85ms
step:1600/1900 train_time:90992ms step_avg:56.87ms
step:1601/1900 train_time:91081ms step_avg:56.89ms
step:1602/1900 train_time:91168ms step_avg:56.91ms
step:1603/1900 train_time:91256ms step_avg:56.93ms
step:1604/1900 train_time:91345ms step_avg:56.95ms
step:1605/1900 train_time:91434ms step_avg:56.97ms
step:1606/1900 train_time:91522ms step_avg:56.99ms
step:1607/1900 train_time:91611ms step_avg:57.01ms
step:1608/1900 train_time:91698ms step_avg:57.03ms
step:1609/1900 train_time:91787ms step_avg:57.05ms
step:1610/1900 train_time:91874ms step_avg:57.06ms
step:1611/1900 train_time:91961ms step_avg:57.08ms
step:1612/1900 train_time:92050ms step_avg:57.10ms
step:1613/1900 train_time:92138ms step_avg:57.12ms
step:1614/1900 train_time:92226ms step_avg:57.14ms
step:1615/1900 train_time:92314ms step_avg:57.16ms
step:1616/1900 train_time:92402ms step_avg:57.18ms
step:1617/1900 train_time:92491ms step_avg:57.20ms
step:1618/1900 train_time:92579ms step_avg:57.22ms
step:1619/1900 train_time:92667ms step_avg:57.24ms
step:1620/1900 train_time:92754ms step_avg:57.26ms
step:1621/1900 train_time:92843ms step_avg:57.27ms
step:1622/1900 train_time:92931ms step_avg:57.29ms
step:1623/1900 train_time:93020ms step_avg:57.31ms
step:1624/1900 train_time:93107ms step_avg:57.33ms
step:1625/1900 train_time:93195ms step_avg:57.35ms
step:1626/1900 train_time:93283ms step_avg:57.37ms
step:1627/1900 train_time:93371ms step_avg:57.39ms
step:1628/1900 train_time:93459ms step_avg:57.41ms
step:1629/1900 train_time:93549ms step_avg:57.43ms
step:1630/1900 train_time:93636ms step_avg:57.45ms
step:1631/1900 train_time:93724ms step_avg:57.46ms
step:1632/1900 train_time:93813ms step_avg:57.48ms
step:1633/1900 train_time:93902ms step_avg:57.50ms
step:1634/1900 train_time:93989ms step_avg:57.52ms
step:1635/1900 train_time:94077ms step_avg:57.54ms
step:1636/1900 train_time:94164ms step_avg:57.56ms
step:1637/1900 train_time:94254ms step_avg:57.58ms
step:1638/1900 train_time:94342ms step_avg:57.60ms
step:1639/1900 train_time:94432ms step_avg:57.62ms
step:1640/1900 train_time:94520ms step_avg:57.63ms
step:1641/1900 train_time:94609ms step_avg:57.65ms
step:1642/1900 train_time:94697ms step_avg:57.67ms
step:1643/1900 train_time:94785ms step_avg:57.69ms
step:1644/1900 train_time:94872ms step_avg:57.71ms
step:1645/1900 train_time:94960ms step_avg:57.73ms
step:1646/1900 train_time:95049ms step_avg:57.75ms
step:1647/1900 train_time:95138ms step_avg:57.76ms
step:1648/1900 train_time:95226ms step_avg:57.78ms
step:1649/1900 train_time:95314ms step_avg:57.80ms
step:1650/1900 train_time:95403ms step_avg:57.82ms
step:1651/1900 train_time:95492ms step_avg:57.84ms
step:1652/1900 train_time:95580ms step_avg:57.86ms
step:1653/1900 train_time:95668ms step_avg:57.88ms
step:1654/1900 train_time:95756ms step_avg:57.89ms
step:1655/1900 train_time:95844ms step_avg:57.91ms
step:1656/1900 train_time:95932ms step_avg:57.93ms
step:1657/1900 train_time:96021ms step_avg:57.95ms
step:1658/1900 train_time:96108ms step_avg:57.97ms
step:1659/1900 train_time:96196ms step_avg:57.98ms
step:1660/1900 train_time:96284ms step_avg:58.00ms
step:1661/1900 train_time:96373ms step_avg:58.02ms
step:1662/1900 train_time:96461ms step_avg:58.04ms
step:1663/1900 train_time:96551ms step_avg:58.06ms
step:1664/1900 train_time:96639ms step_avg:58.08ms
step:1665/1900 train_time:96727ms step_avg:58.09ms
step:1666/1900 train_time:96815ms step_avg:58.11ms
step:1667/1900 train_time:96903ms step_avg:58.13ms
step:1668/1900 train_time:96991ms step_avg:58.15ms
step:1669/1900 train_time:97079ms step_avg:58.17ms
step:1670/1900 train_time:97166ms step_avg:58.18ms
step:1671/1900 train_time:97256ms step_avg:58.20ms
step:1672/1900 train_time:97344ms step_avg:58.22ms
step:1673/1900 train_time:97433ms step_avg:58.24ms
step:1674/1900 train_time:97521ms step_avg:58.26ms
step:1675/1900 train_time:97610ms step_avg:58.27ms
step:1676/1900 train_time:97697ms step_avg:58.29ms
step:1677/1900 train_time:97786ms step_avg:58.31ms
step:1678/1900 train_time:97874ms step_avg:58.33ms
step:1679/1900 train_time:97962ms step_avg:58.35ms
step:1680/1900 train_time:98050ms step_avg:58.36ms
step:1681/1900 train_time:98138ms step_avg:58.38ms
step:1682/1900 train_time:98226ms step_avg:58.40ms
step:1683/1900 train_time:98314ms step_avg:58.42ms
step:1684/1900 train_time:98402ms step_avg:58.43ms
step:1685/1900 train_time:98490ms step_avg:58.45ms
step:1686/1900 train_time:98578ms step_avg:58.47ms
step:1687/1900 train_time:98667ms step_avg:58.49ms
step:1688/1900 train_time:98754ms step_avg:58.50ms
step:1689/1900 train_time:98842ms step_avg:58.52ms
step:1690/1900 train_time:98930ms step_avg:58.54ms
step:1691/1900 train_time:99018ms step_avg:58.56ms
step:1692/1900 train_time:99106ms step_avg:58.57ms
step:1693/1900 train_time:99194ms step_avg:58.59ms
step:1694/1900 train_time:99282ms step_avg:58.61ms
step:1695/1900 train_time:99371ms step_avg:58.63ms
step:1696/1900 train_time:99459ms step_avg:58.64ms
step:1697/1900 train_time:99548ms step_avg:58.66ms
step:1698/1900 train_time:99636ms step_avg:58.68ms
step:1699/1900 train_time:99725ms step_avg:58.70ms
step:1700/1900 train_time:99814ms step_avg:58.71ms
step:1701/1900 train_time:99902ms step_avg:58.73ms
step:1702/1900 train_time:99990ms step_avg:58.75ms
step:1703/1900 train_time:100077ms step_avg:58.77ms
step:1704/1900 train_time:100165ms step_avg:58.78ms
step:1705/1900 train_time:100254ms step_avg:58.80ms
step:1706/1900 train_time:100342ms step_avg:58.82ms
step:1707/1900 train_time:100430ms step_avg:58.83ms
step:1708/1900 train_time:100518ms step_avg:58.85ms
step:1709/1900 train_time:100606ms step_avg:58.87ms
step:1710/1900 train_time:100693ms step_avg:58.88ms
step:1711/1900 train_time:100781ms step_avg:58.90ms
step:1712/1900 train_time:100869ms step_avg:58.92ms
step:1713/1900 train_time:100958ms step_avg:58.94ms
step:1714/1900 train_time:101045ms step_avg:58.95ms
step:1715/1900 train_time:101133ms step_avg:58.97ms
step:1716/1900 train_time:101221ms step_avg:58.99ms
step:1717/1900 train_time:101310ms step_avg:59.00ms
step:1718/1900 train_time:101398ms step_avg:59.02ms
step:1719/1900 train_time:101487ms step_avg:59.04ms
step:1720/1900 train_time:101574ms step_avg:59.05ms
step:1721/1900 train_time:101662ms step_avg:59.07ms
step:1722/1900 train_time:101750ms step_avg:59.09ms
step:1723/1900 train_time:101839ms step_avg:59.11ms
step:1724/1900 train_time:101926ms step_avg:59.12ms
step:1725/1900 train_time:102014ms step_avg:59.14ms
step:1726/1900 train_time:102102ms step_avg:59.16ms
step:1727/1900 train_time:102191ms step_avg:59.17ms
step:1728/1900 train_time:102278ms step_avg:59.19ms
step:1729/1900 train_time:102366ms step_avg:59.21ms
step:1730/1900 train_time:102454ms step_avg:59.22ms
step:1731/1900 train_time:102543ms step_avg:59.24ms
step:1732/1900 train_time:102630ms step_avg:59.26ms
step:1733/1900 train_time:102719ms step_avg:59.27ms
step:1734/1900 train_time:102806ms step_avg:59.29ms
step:1735/1900 train_time:102894ms step_avg:59.31ms
step:1736/1900 train_time:102982ms step_avg:59.32ms
step:1737/1900 train_time:103071ms step_avg:59.34ms
step:1738/1900 train_time:103159ms step_avg:59.36ms
step:1739/1900 train_time:103248ms step_avg:59.37ms
step:1740/1900 train_time:103335ms step_avg:59.39ms
step:1741/1900 train_time:103425ms step_avg:59.41ms
step:1742/1900 train_time:103513ms step_avg:59.42ms
step:1743/1900 train_time:103602ms step_avg:59.44ms
step:1744/1900 train_time:103689ms step_avg:59.45ms
step:1745/1900 train_time:103777ms step_avg:59.47ms
step:1746/1900 train_time:103865ms step_avg:59.49ms
step:1747/1900 train_time:103953ms step_avg:59.50ms
step:1748/1900 train_time:104040ms step_avg:59.52ms
step:1749/1900 train_time:104130ms step_avg:59.54ms
step:1750/1900 train_time:104217ms step_avg:59.55ms
step:1750/1900 val_loss:3.3185 train_time:104309ms step_avg:59.60ms
step:1751/1900 train_time:104329ms step_avg:59.58ms
step:1752/1900 train_time:104396ms step_avg:59.59ms
step:1753/1900 train_time:104486ms step_avg:59.60ms
step:1754/1900 train_time:104573ms step_avg:59.62ms
step:1755/1900 train_time:104661ms step_avg:59.64ms
step:1756/1900 train_time:104748ms step_avg:59.65ms
step:1757/1900 train_time:104836ms step_avg:59.67ms
step:1758/1900 train_time:104923ms step_avg:59.68ms
step:1759/1900 train_time:105012ms step_avg:59.70ms
step:1760/1900 train_time:105100ms step_avg:59.72ms
step:1761/1900 train_time:105188ms step_avg:59.73ms
step:1762/1900 train_time:105277ms step_avg:59.75ms
step:1763/1900 train_time:105368ms step_avg:59.77ms
step:1764/1900 train_time:105456ms step_avg:59.78ms
step:1765/1900 train_time:105547ms step_avg:59.80ms
step:1766/1900 train_time:105633ms step_avg:59.81ms
step:1767/1900 train_time:105721ms step_avg:59.83ms
step:1768/1900 train_time:105808ms step_avg:59.85ms
step:1769/1900 train_time:105896ms step_avg:59.86ms
step:1770/1900 train_time:105983ms step_avg:59.88ms
step:1771/1900 train_time:106072ms step_avg:59.89ms
step:1772/1900 train_time:106159ms step_avg:59.91ms
step:1773/1900 train_time:106248ms step_avg:59.93ms
step:1774/1900 train_time:106335ms step_avg:59.94ms
step:1775/1900 train_time:106424ms step_avg:59.96ms
step:1776/1900 train_time:106512ms step_avg:59.97ms
step:1777/1900 train_time:106601ms step_avg:59.99ms
step:1778/1900 train_time:106688ms step_avg:60.00ms
step:1779/1900 train_time:106776ms step_avg:60.02ms
step:1780/1900 train_time:106863ms step_avg:60.04ms
step:1781/1900 train_time:106951ms step_avg:60.05ms
step:1782/1900 train_time:107039ms step_avg:60.07ms
step:1783/1900 train_time:107128ms step_avg:60.08ms
step:1784/1900 train_time:107216ms step_avg:60.10ms
step:1785/1900 train_time:107305ms step_avg:60.11ms
step:1786/1900 train_time:107393ms step_avg:60.13ms
step:1787/1900 train_time:107482ms step_avg:60.15ms
step:1788/1900 train_time:107569ms step_avg:60.16ms
step:1789/1900 train_time:107658ms step_avg:60.18ms
step:1790/1900 train_time:107745ms step_avg:60.19ms
step:1791/1900 train_time:107833ms step_avg:60.21ms
step:1792/1900 train_time:107921ms step_avg:60.22ms
step:1793/1900 train_time:108010ms step_avg:60.24ms
step:1794/1900 train_time:108097ms step_avg:60.25ms
step:1795/1900 train_time:108187ms step_avg:60.27ms
step:1796/1900 train_time:108274ms step_avg:60.29ms
step:1797/1900 train_time:108364ms step_avg:60.30ms
step:1798/1900 train_time:108451ms step_avg:60.32ms
step:1799/1900 train_time:108539ms step_avg:60.33ms
step:1800/1900 train_time:108627ms step_avg:60.35ms
step:1801/1900 train_time:108715ms step_avg:60.36ms
step:1802/1900 train_time:108802ms step_avg:60.38ms
step:1803/1900 train_time:108892ms step_avg:60.39ms
step:1804/1900 train_time:108979ms step_avg:60.41ms
step:1805/1900 train_time:109068ms step_avg:60.43ms
step:1806/1900 train_time:109156ms step_avg:60.44ms
step:1807/1900 train_time:109245ms step_avg:60.46ms
step:1808/1900 train_time:109333ms step_avg:60.47ms
step:1809/1900 train_time:109422ms step_avg:60.49ms
step:1810/1900 train_time:109510ms step_avg:60.50ms
step:1811/1900 train_time:109598ms step_avg:60.52ms
step:1812/1900 train_time:109685ms step_avg:60.53ms
step:1813/1900 train_time:109773ms step_avg:60.55ms
step:1814/1900 train_time:109860ms step_avg:60.56ms
step:1815/1900 train_time:109949ms step_avg:60.58ms
step:1816/1900 train_time:110037ms step_avg:60.59ms
step:1817/1900 train_time:110126ms step_avg:60.61ms
step:1818/1900 train_time:110213ms step_avg:60.62ms
step:1819/1900 train_time:110301ms step_avg:60.64ms
step:1820/1900 train_time:110389ms step_avg:60.65ms
step:1821/1900 train_time:110478ms step_avg:60.67ms
step:1822/1900 train_time:110566ms step_avg:60.68ms
step:1823/1900 train_time:110654ms step_avg:60.70ms
step:1824/1900 train_time:110741ms step_avg:60.71ms
step:1825/1900 train_time:110829ms step_avg:60.73ms
step:1826/1900 train_time:110916ms step_avg:60.74ms
step:1827/1900 train_time:111006ms step_avg:60.76ms
step:1828/1900 train_time:111093ms step_avg:60.77ms
step:1829/1900 train_time:111182ms step_avg:60.79ms
step:1830/1900 train_time:111269ms step_avg:60.80ms
step:1831/1900 train_time:111358ms step_avg:60.82ms
step:1832/1900 train_time:111446ms step_avg:60.83ms
step:1833/1900 train_time:111534ms step_avg:60.85ms
step:1834/1900 train_time:111622ms step_avg:60.86ms
step:1835/1900 train_time:111710ms step_avg:60.88ms
step:1836/1900 train_time:111798ms step_avg:60.89ms
step:1837/1900 train_time:111886ms step_avg:60.91ms
step:1838/1900 train_time:111974ms step_avg:60.92ms
step:1839/1900 train_time:112063ms step_avg:60.94ms
step:1840/1900 train_time:112151ms step_avg:60.95ms
step:1841/1900 train_time:112239ms step_avg:60.97ms
step:1842/1900 train_time:112326ms step_avg:60.98ms
step:1843/1900 train_time:112415ms step_avg:61.00ms
step:1844/1900 train_time:112503ms step_avg:61.01ms
step:1845/1900 train_time:112593ms step_avg:61.03ms
step:1846/1900 train_time:112680ms step_avg:61.04ms
step:1847/1900 train_time:112768ms step_avg:61.05ms
step:1848/1900 train_time:112856ms step_avg:61.07ms
step:1849/1900 train_time:112944ms step_avg:61.08ms
step:1850/1900 train_time:113032ms step_avg:61.10ms
step:1851/1900 train_time:113120ms step_avg:61.11ms
step:1852/1900 train_time:113207ms step_avg:61.13ms
step:1853/1900 train_time:113296ms step_avg:61.14ms
step:1854/1900 train_time:113384ms step_avg:61.16ms
step:1855/1900 train_time:113473ms step_avg:61.17ms
step:1856/1900 train_time:113560ms step_avg:61.19ms
step:1857/1900 train_time:113649ms step_avg:61.20ms
step:1858/1900 train_time:113736ms step_avg:61.21ms
step:1859/1900 train_time:113825ms step_avg:61.23ms
step:1860/1900 train_time:113912ms step_avg:61.24ms
step:1861/1900 train_time:114001ms step_avg:61.26ms
step:1862/1900 train_time:114089ms step_avg:61.27ms
step:1863/1900 train_time:114177ms step_avg:61.29ms
step:1864/1900 train_time:114264ms step_avg:61.30ms
step:1865/1900 train_time:114353ms step_avg:61.32ms
step:1866/1900 train_time:114441ms step_avg:61.33ms
step:1867/1900 train_time:114530ms step_avg:61.34ms
step:1868/1900 train_time:114619ms step_avg:61.36ms
step:1869/1900 train_time:114707ms step_avg:61.37ms
step:1870/1900 train_time:114794ms step_avg:61.39ms
step:1871/1900 train_time:114883ms step_avg:61.40ms
step:1872/1900 train_time:114972ms step_avg:61.42ms
step:1873/1900 train_time:115062ms step_avg:61.43ms
step:1874/1900 train_time:115151ms step_avg:61.45ms
step:1875/1900 train_time:115239ms step_avg:61.46ms
step:1876/1900 train_time:115327ms step_avg:61.48ms
step:1877/1900 train_time:115416ms step_avg:61.49ms
step:1878/1900 train_time:115504ms step_avg:61.50ms
step:1879/1900 train_time:115593ms step_avg:61.52ms
step:1880/1900 train_time:115682ms step_avg:61.53ms
step:1881/1900 train_time:115771ms step_avg:61.55ms
step:1882/1900 train_time:115858ms step_avg:61.56ms
step:1883/1900 train_time:115946ms step_avg:61.58ms
step:1884/1900 train_time:116034ms step_avg:61.59ms
step:1885/1900 train_time:116123ms step_avg:61.60ms
step:1886/1900 train_time:116210ms step_avg:61.62ms
step:1887/1900 train_time:116299ms step_avg:61.63ms
step:1888/1900 train_time:116387ms step_avg:61.65ms
step:1889/1900 train_time:116476ms step_avg:61.66ms
step:1890/1900 train_time:116564ms step_avg:61.67ms
step:1891/1900 train_time:116654ms step_avg:61.69ms
step:1892/1900 train_time:116742ms step_avg:61.70ms
step:1893/1900 train_time:116832ms step_avg:61.72ms
step:1894/1900 train_time:116920ms step_avg:61.73ms
step:1895/1900 train_time:117009ms step_avg:61.75ms
step:1896/1900 train_time:117096ms step_avg:61.76ms
step:1897/1900 train_time:117185ms step_avg:61.77ms
step:1898/1900 train_time:117273ms step_avg:61.79ms
step:1899/1900 train_time:117362ms step_avg:61.80ms
step:1900/1900 train_time:117450ms step_avg:61.82ms
step:1900/1900 val_loss:3.2783 train_time:117540ms step_avg:61.86ms
peak memory allocated: 29709 MiB reserved: 43878 MiB

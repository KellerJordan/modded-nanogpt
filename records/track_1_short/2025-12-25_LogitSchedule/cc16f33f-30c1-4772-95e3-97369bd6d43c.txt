import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:13:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     46457      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46458      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46459      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46460      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46461      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46462      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46463      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     46464      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     46458      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     46459      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     46460      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     46461      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     46462      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     46463      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     46464      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8307 train_time:0ms step_avg:0.18ms
step:1/1900 train_time:69ms step_avg:68.90ms
step:2/1900 train_time:93ms step_avg:46.71ms
step:3/1900 train_time:118ms step_avg:39.42ms
step:4/1900 train_time:152ms step_avg:38.02ms
step:5/1900 train_time:186ms step_avg:37.19ms
step:6/1900 train_time:255ms step_avg:42.55ms
step:7/1900 train_time:405ms step_avg:57.92ms
step:8/1900 train_time:439ms step_avg:54.92ms
step:9/1900 train_time:473ms step_avg:52.57ms
step:10/1900 train_time:507ms step_avg:50.69ms
step:11/1900 train_time:541ms step_avg:49.18ms
step:12/1900 train_time:575ms step_avg:47.90ms
step:13/1900 train_time:609ms step_avg:46.84ms
step:14/1900 train_time:643ms step_avg:45.91ms
step:15/1900 train_time:677ms step_avg:45.13ms
step:16/1900 train_time:711ms step_avg:44.43ms
step:17/1900 train_time:745ms step_avg:43.83ms
step:18/1900 train_time:779ms step_avg:43.28ms
step:19/1900 train_time:813ms step_avg:42.79ms
step:20/1900 train_time:847ms step_avg:42.34ms
step:21/1900 train_time:881ms step_avg:41.97ms
step:22/1900 train_time:915ms step_avg:41.60ms
step:23/1900 train_time:949ms step_avg:41.27ms
step:24/1900 train_time:983ms step_avg:40.97ms
step:25/1900 train_time:1017ms step_avg:40.70ms
step:26/1900 train_time:1051ms step_avg:40.43ms
step:27/1900 train_time:1086ms step_avg:40.21ms
step:28/1900 train_time:1120ms step_avg:39.99ms
step:29/1900 train_time:1154ms step_avg:39.79ms
step:30/1900 train_time:1188ms step_avg:39.59ms
step:31/1900 train_time:1222ms step_avg:39.42ms
step:32/1900 train_time:1256ms step_avg:39.25ms
step:33/1900 train_time:1290ms step_avg:39.10ms
step:34/1900 train_time:1325ms step_avg:38.96ms
step:35/1900 train_time:1360ms step_avg:38.85ms
step:36/1900 train_time:1394ms step_avg:38.71ms
step:37/1900 train_time:1428ms step_avg:38.61ms
step:38/1900 train_time:1462ms step_avg:38.48ms
step:39/1900 train_time:1497ms step_avg:38.38ms
step:40/1900 train_time:1531ms step_avg:38.27ms
step:41/1900 train_time:1565ms step_avg:38.17ms
step:42/1900 train_time:1599ms step_avg:38.07ms
step:43/1900 train_time:1633ms step_avg:37.98ms
step:44/1900 train_time:1667ms step_avg:37.89ms
step:45/1900 train_time:1701ms step_avg:37.81ms
step:46/1900 train_time:1735ms step_avg:37.73ms
step:47/1900 train_time:1770ms step_avg:37.65ms
step:48/1900 train_time:1804ms step_avg:37.58ms
step:49/1900 train_time:1838ms step_avg:37.51ms
step:50/1900 train_time:1872ms step_avg:37.44ms
step:51/1900 train_time:1906ms step_avg:37.38ms
step:52/1900 train_time:1940ms step_avg:37.31ms
step:53/1900 train_time:1974ms step_avg:37.24ms
step:54/1900 train_time:2008ms step_avg:37.18ms
step:55/1900 train_time:2042ms step_avg:37.12ms
step:56/1900 train_time:2076ms step_avg:37.07ms
step:57/1900 train_time:2110ms step_avg:37.02ms
step:58/1900 train_time:2144ms step_avg:36.97ms
step:59/1900 train_time:2178ms step_avg:36.92ms
step:60/1900 train_time:2212ms step_avg:36.86ms
step:61/1900 train_time:2246ms step_avg:36.82ms
step:62/1900 train_time:2280ms step_avg:36.77ms
step:63/1900 train_time:2314ms step_avg:36.73ms
step:64/1900 train_time:2348ms step_avg:36.69ms
step:65/1900 train_time:2382ms step_avg:36.65ms
step:66/1900 train_time:2416ms step_avg:36.61ms
step:67/1900 train_time:2451ms step_avg:36.58ms
step:68/1900 train_time:2485ms step_avg:36.54ms
step:69/1900 train_time:2519ms step_avg:36.51ms
step:70/1900 train_time:2553ms step_avg:36.47ms
step:71/1900 train_time:2587ms step_avg:36.44ms
step:72/1900 train_time:2621ms step_avg:36.41ms
step:73/1900 train_time:2655ms step_avg:36.37ms
step:74/1900 train_time:2689ms step_avg:36.34ms
step:75/1900 train_time:2724ms step_avg:36.31ms
step:76/1900 train_time:2757ms step_avg:36.28ms
step:77/1900 train_time:2792ms step_avg:36.26ms
step:78/1900 train_time:2826ms step_avg:36.23ms
step:79/1900 train_time:2860ms step_avg:36.21ms
step:80/1900 train_time:2894ms step_avg:36.18ms
step:81/1900 train_time:2928ms step_avg:36.15ms
step:82/1900 train_time:2962ms step_avg:36.12ms
step:83/1900 train_time:2996ms step_avg:36.10ms
step:84/1900 train_time:3030ms step_avg:36.07ms
step:85/1900 train_time:3064ms step_avg:36.05ms
step:86/1900 train_time:3098ms step_avg:36.02ms
step:87/1900 train_time:3132ms step_avg:36.00ms
step:88/1900 train_time:3166ms step_avg:35.98ms
step:89/1900 train_time:3200ms step_avg:35.96ms
step:90/1900 train_time:3234ms step_avg:35.93ms
step:91/1900 train_time:3268ms step_avg:35.91ms
step:92/1900 train_time:3302ms step_avg:35.89ms
step:93/1900 train_time:3336ms step_avg:35.87ms
step:94/1900 train_time:3370ms step_avg:35.85ms
step:95/1900 train_time:3404ms step_avg:35.83ms
step:96/1900 train_time:3438ms step_avg:35.81ms
step:97/1900 train_time:3472ms step_avg:35.79ms
step:98/1900 train_time:3506ms step_avg:35.77ms
step:99/1900 train_time:3540ms step_avg:35.76ms
step:100/1900 train_time:3574ms step_avg:35.74ms
step:101/1900 train_time:3608ms step_avg:35.72ms
step:102/1900 train_time:3642ms step_avg:35.70ms
step:103/1900 train_time:3676ms step_avg:35.69ms
step:104/1900 train_time:3710ms step_avg:35.67ms
step:105/1900 train_time:3744ms step_avg:35.66ms
step:106/1900 train_time:3778ms step_avg:35.64ms
step:107/1900 train_time:3812ms step_avg:35.63ms
step:108/1900 train_time:3846ms step_avg:35.61ms
step:109/1900 train_time:3880ms step_avg:35.60ms
step:110/1900 train_time:3914ms step_avg:35.58ms
step:111/1900 train_time:3948ms step_avg:35.57ms
step:112/1900 train_time:3982ms step_avg:35.55ms
step:113/1900 train_time:4016ms step_avg:35.54ms
step:114/1900 train_time:4050ms step_avg:35.53ms
step:115/1900 train_time:4084ms step_avg:35.52ms
step:116/1900 train_time:4118ms step_avg:35.50ms
step:117/1900 train_time:4153ms step_avg:35.49ms
step:118/1900 train_time:4186ms step_avg:35.48ms
step:119/1900 train_time:4220ms step_avg:35.46ms
step:120/1900 train_time:4254ms step_avg:35.45ms
step:121/1900 train_time:4288ms step_avg:35.44ms
step:122/1900 train_time:4322ms step_avg:35.43ms
step:123/1900 train_time:4356ms step_avg:35.42ms
step:124/1900 train_time:4390ms step_avg:35.40ms
step:125/1900 train_time:4425ms step_avg:35.40ms
step:126/1900 train_time:4459ms step_avg:35.39ms
step:127/1900 train_time:4493ms step_avg:35.38ms
step:128/1900 train_time:4527ms step_avg:35.37ms
step:129/1900 train_time:4561ms step_avg:35.35ms
step:130/1900 train_time:4595ms step_avg:35.34ms
step:131/1900 train_time:4629ms step_avg:35.33ms
step:132/1900 train_time:4663ms step_avg:35.32ms
step:133/1900 train_time:4697ms step_avg:35.32ms
step:134/1900 train_time:4731ms step_avg:35.30ms
step:135/1900 train_time:4765ms step_avg:35.30ms
step:136/1900 train_time:4799ms step_avg:35.29ms
step:137/1900 train_time:4833ms step_avg:35.28ms
step:138/1900 train_time:4867ms step_avg:35.27ms
step:139/1900 train_time:4901ms step_avg:35.26ms
step:140/1900 train_time:4935ms step_avg:35.25ms
step:141/1900 train_time:4970ms step_avg:35.24ms
step:142/1900 train_time:5003ms step_avg:35.24ms
step:143/1900 train_time:5037ms step_avg:35.23ms
step:144/1900 train_time:5071ms step_avg:35.22ms
step:145/1900 train_time:5105ms step_avg:35.21ms
step:146/1900 train_time:5139ms step_avg:35.20ms
step:147/1900 train_time:5173ms step_avg:35.19ms
step:148/1900 train_time:5207ms step_avg:35.18ms
step:149/1900 train_time:5241ms step_avg:35.17ms
step:150/1900 train_time:5275ms step_avg:35.17ms
step:151/1900 train_time:5309ms step_avg:35.16ms
step:152/1900 train_time:5343ms step_avg:35.15ms
step:153/1900 train_time:5377ms step_avg:35.14ms
step:154/1900 train_time:5411ms step_avg:35.14ms
step:155/1900 train_time:5445ms step_avg:35.13ms
step:156/1900 train_time:5479ms step_avg:35.12ms
step:157/1900 train_time:5513ms step_avg:35.12ms
step:158/1900 train_time:5547ms step_avg:35.11ms
step:159/1900 train_time:5581ms step_avg:35.10ms
step:160/1900 train_time:5615ms step_avg:35.09ms
step:161/1900 train_time:5649ms step_avg:35.09ms
step:162/1900 train_time:5683ms step_avg:35.08ms
step:163/1900 train_time:5718ms step_avg:35.08ms
step:164/1900 train_time:5751ms step_avg:35.07ms
step:165/1900 train_time:5785ms step_avg:35.06ms
step:166/1900 train_time:5819ms step_avg:35.06ms
step:167/1900 train_time:5853ms step_avg:35.05ms
step:168/1900 train_time:5887ms step_avg:35.04ms
step:169/1900 train_time:5922ms step_avg:35.04ms
step:170/1900 train_time:5956ms step_avg:35.03ms
step:171/1900 train_time:5989ms step_avg:35.03ms
step:172/1900 train_time:6023ms step_avg:35.02ms
step:173/1900 train_time:6057ms step_avg:35.01ms
step:174/1900 train_time:6091ms step_avg:35.01ms
step:175/1900 train_time:6126ms step_avg:35.00ms
step:176/1900 train_time:6159ms step_avg:35.00ms
step:177/1900 train_time:6193ms step_avg:34.99ms
step:178/1900 train_time:6227ms step_avg:34.98ms
step:179/1900 train_time:6261ms step_avg:34.98ms
step:180/1900 train_time:6295ms step_avg:34.97ms
step:181/1900 train_time:6329ms step_avg:34.97ms
step:182/1900 train_time:6363ms step_avg:34.96ms
step:183/1900 train_time:6397ms step_avg:34.96ms
step:184/1900 train_time:6431ms step_avg:34.95ms
step:185/1900 train_time:6465ms step_avg:34.95ms
step:186/1900 train_time:6499ms step_avg:34.94ms
step:187/1900 train_time:6533ms step_avg:34.93ms
step:188/1900 train_time:6567ms step_avg:34.93ms
step:189/1900 train_time:6601ms step_avg:34.92ms
step:190/1900 train_time:6634ms step_avg:34.92ms
step:191/1900 train_time:6668ms step_avg:34.91ms
step:192/1900 train_time:6702ms step_avg:34.91ms
step:193/1900 train_time:6737ms step_avg:34.91ms
step:194/1900 train_time:6771ms step_avg:34.90ms
step:195/1900 train_time:6805ms step_avg:34.90ms
step:196/1900 train_time:6839ms step_avg:34.89ms
step:197/1900 train_time:6873ms step_avg:34.89ms
step:198/1900 train_time:6907ms step_avg:34.88ms
step:199/1900 train_time:6941ms step_avg:34.88ms
step:200/1900 train_time:6975ms step_avg:34.87ms
step:201/1900 train_time:7009ms step_avg:34.87ms
step:202/1900 train_time:7043ms step_avg:34.86ms
step:203/1900 train_time:7077ms step_avg:34.86ms
step:204/1900 train_time:7111ms step_avg:34.86ms
step:205/1900 train_time:7145ms step_avg:34.85ms
step:206/1900 train_time:7179ms step_avg:34.85ms
step:207/1900 train_time:7213ms step_avg:34.84ms
step:208/1900 train_time:7246ms step_avg:34.84ms
step:209/1900 train_time:7281ms step_avg:34.84ms
step:210/1900 train_time:7314ms step_avg:34.83ms
step:211/1900 train_time:7348ms step_avg:34.83ms
step:212/1900 train_time:7382ms step_avg:34.82ms
step:213/1900 train_time:7416ms step_avg:34.82ms
step:214/1900 train_time:7450ms step_avg:34.81ms
step:215/1900 train_time:7485ms step_avg:34.81ms
step:216/1900 train_time:7518ms step_avg:34.81ms
step:217/1900 train_time:7553ms step_avg:34.81ms
step:218/1900 train_time:7587ms step_avg:34.80ms
step:219/1900 train_time:7621ms step_avg:34.80ms
step:220/1900 train_time:7654ms step_avg:34.79ms
step:221/1900 train_time:7688ms step_avg:34.79ms
step:222/1900 train_time:7722ms step_avg:34.79ms
step:223/1900 train_time:7757ms step_avg:34.78ms
step:224/1900 train_time:7790ms step_avg:34.78ms
step:225/1900 train_time:7825ms step_avg:34.78ms
step:226/1900 train_time:7859ms step_avg:34.77ms
step:227/1900 train_time:7893ms step_avg:34.77ms
step:228/1900 train_time:7926ms step_avg:34.76ms
step:229/1900 train_time:7960ms step_avg:34.76ms
step:230/1900 train_time:7994ms step_avg:34.76ms
step:231/1900 train_time:8028ms step_avg:34.76ms
step:232/1900 train_time:8062ms step_avg:34.75ms
step:233/1900 train_time:8096ms step_avg:34.75ms
step:234/1900 train_time:8130ms step_avg:34.74ms
step:235/1900 train_time:8164ms step_avg:34.74ms
step:236/1900 train_time:8198ms step_avg:34.74ms
step:237/1900 train_time:8232ms step_avg:34.73ms
step:238/1900 train_time:8266ms step_avg:34.73ms
step:239/1900 train_time:8300ms step_avg:34.73ms
step:240/1900 train_time:8333ms step_avg:34.72ms
step:241/1900 train_time:8367ms step_avg:34.72ms
step:242/1900 train_time:8401ms step_avg:34.72ms
step:243/1900 train_time:8435ms step_avg:34.71ms
step:244/1900 train_time:8469ms step_avg:34.71ms
step:245/1900 train_time:8503ms step_avg:34.71ms
step:246/1900 train_time:8537ms step_avg:34.70ms
step:247/1900 train_time:8571ms step_avg:34.70ms
step:248/1900 train_time:8605ms step_avg:34.70ms
step:249/1900 train_time:8639ms step_avg:34.70ms
step:250/1900 train_time:8673ms step_avg:34.69ms
step:250/1900 val_loss:4.5915 train_time:8710ms step_avg:34.84ms
step:251/1900 train_time:8729ms step_avg:34.78ms
step:252/1900 train_time:8748ms step_avg:34.71ms
step:253/1900 train_time:8778ms step_avg:34.70ms
step:254/1900 train_time:8813ms step_avg:34.70ms
step:255/1900 train_time:8847ms step_avg:34.70ms
step:256/1900 train_time:8881ms step_avg:34.69ms
step:257/1900 train_time:8916ms step_avg:34.69ms
step:258/1900 train_time:8950ms step_avg:34.69ms
step:259/1900 train_time:8984ms step_avg:34.69ms
step:260/1900 train_time:9018ms step_avg:34.68ms
step:261/1900 train_time:9052ms step_avg:34.68ms
step:262/1900 train_time:9086ms step_avg:34.68ms
step:263/1900 train_time:9120ms step_avg:34.68ms
step:264/1900 train_time:9153ms step_avg:34.67ms
step:265/1900 train_time:9187ms step_avg:34.67ms
step:266/1900 train_time:9221ms step_avg:34.67ms
step:267/1900 train_time:9255ms step_avg:34.66ms
step:268/1900 train_time:9289ms step_avg:34.66ms
step:269/1900 train_time:9322ms step_avg:34.66ms
step:270/1900 train_time:9356ms step_avg:34.65ms
step:271/1900 train_time:9390ms step_avg:34.65ms
step:272/1900 train_time:9424ms step_avg:34.65ms
step:273/1900 train_time:9458ms step_avg:34.64ms
step:274/1900 train_time:9491ms step_avg:34.64ms
step:275/1900 train_time:9526ms step_avg:34.64ms
step:276/1900 train_time:9559ms step_avg:34.64ms
step:277/1900 train_time:9593ms step_avg:34.63ms
step:278/1900 train_time:9627ms step_avg:34.63ms
step:279/1900 train_time:9661ms step_avg:34.63ms
step:280/1900 train_time:9695ms step_avg:34.62ms
step:281/1900 train_time:9730ms step_avg:34.62ms
step:282/1900 train_time:9764ms step_avg:34.62ms
step:283/1900 train_time:9798ms step_avg:34.62ms
step:284/1900 train_time:9832ms step_avg:34.62ms
step:285/1900 train_time:9867ms step_avg:34.62ms
step:286/1900 train_time:9901ms step_avg:34.62ms
step:287/1900 train_time:9935ms step_avg:34.62ms
step:288/1900 train_time:9969ms step_avg:34.61ms
step:289/1900 train_time:10003ms step_avg:34.61ms
step:290/1900 train_time:10036ms step_avg:34.61ms
step:291/1900 train_time:10071ms step_avg:34.61ms
step:292/1900 train_time:10104ms step_avg:34.60ms
step:293/1900 train_time:10138ms step_avg:34.60ms
step:294/1900 train_time:10172ms step_avg:34.60ms
step:295/1900 train_time:10206ms step_avg:34.60ms
step:296/1900 train_time:10240ms step_avg:34.59ms
step:297/1900 train_time:10274ms step_avg:34.59ms
step:298/1900 train_time:10308ms step_avg:34.59ms
step:299/1900 train_time:10342ms step_avg:34.59ms
step:300/1900 train_time:10376ms step_avg:34.59ms
step:301/1900 train_time:10410ms step_avg:34.58ms
step:302/1900 train_time:10443ms step_avg:34.58ms
step:303/1900 train_time:10477ms step_avg:34.58ms
step:304/1900 train_time:10511ms step_avg:34.58ms
step:305/1900 train_time:10545ms step_avg:34.57ms
step:306/1900 train_time:10579ms step_avg:34.57ms
step:307/1900 train_time:10613ms step_avg:34.57ms
step:308/1900 train_time:10646ms step_avg:34.57ms
step:309/1900 train_time:10680ms step_avg:34.56ms
step:310/1900 train_time:10714ms step_avg:34.56ms
step:311/1900 train_time:10748ms step_avg:34.56ms
step:312/1900 train_time:10782ms step_avg:34.56ms
step:313/1900 train_time:10816ms step_avg:34.56ms
step:314/1900 train_time:10850ms step_avg:34.55ms
step:315/1900 train_time:10885ms step_avg:34.55ms
step:316/1900 train_time:10918ms step_avg:34.55ms
step:317/1900 train_time:10952ms step_avg:34.55ms
step:318/1900 train_time:10986ms step_avg:34.55ms
step:319/1900 train_time:11021ms step_avg:34.55ms
step:320/1900 train_time:11055ms step_avg:34.55ms
step:321/1900 train_time:11089ms step_avg:34.54ms
step:322/1900 train_time:11122ms step_avg:34.54ms
step:323/1900 train_time:11157ms step_avg:34.54ms
step:324/1900 train_time:11190ms step_avg:34.54ms
step:325/1900 train_time:11225ms step_avg:34.54ms
step:326/1900 train_time:11259ms step_avg:34.54ms
step:327/1900 train_time:11293ms step_avg:34.53ms
step:328/1900 train_time:11327ms step_avg:34.53ms
step:329/1900 train_time:11361ms step_avg:34.53ms
step:330/1900 train_time:11395ms step_avg:34.53ms
step:331/1900 train_time:11429ms step_avg:34.53ms
step:332/1900 train_time:11462ms step_avg:34.53ms
step:333/1900 train_time:11496ms step_avg:34.52ms
step:334/1900 train_time:11530ms step_avg:34.52ms
step:335/1900 train_time:11564ms step_avg:34.52ms
step:336/1900 train_time:11598ms step_avg:34.52ms
step:337/1900 train_time:11632ms step_avg:34.52ms
step:338/1900 train_time:11666ms step_avg:34.51ms
step:339/1900 train_time:11700ms step_avg:34.51ms
step:340/1900 train_time:11734ms step_avg:34.51ms
step:341/1900 train_time:11768ms step_avg:34.51ms
step:342/1900 train_time:11802ms step_avg:34.51ms
step:343/1900 train_time:11836ms step_avg:34.51ms
step:344/1900 train_time:11869ms step_avg:34.50ms
step:345/1900 train_time:11903ms step_avg:34.50ms
step:346/1900 train_time:11937ms step_avg:34.50ms
step:347/1900 train_time:11972ms step_avg:34.50ms
step:348/1900 train_time:12005ms step_avg:34.50ms
step:349/1900 train_time:12040ms step_avg:34.50ms
step:350/1900 train_time:12074ms step_avg:34.50ms
step:351/1900 train_time:12108ms step_avg:34.50ms
step:352/1900 train_time:12141ms step_avg:34.49ms
step:353/1900 train_time:12176ms step_avg:34.49ms
step:354/1900 train_time:12209ms step_avg:34.49ms
step:355/1900 train_time:12243ms step_avg:34.49ms
step:356/1900 train_time:12277ms step_avg:34.49ms
step:357/1900 train_time:12311ms step_avg:34.48ms
step:358/1900 train_time:12345ms step_avg:34.48ms
step:359/1900 train_time:12379ms step_avg:34.48ms
step:360/1900 train_time:12413ms step_avg:34.48ms
step:361/1900 train_time:12447ms step_avg:34.48ms
step:362/1900 train_time:12481ms step_avg:34.48ms
step:363/1900 train_time:12515ms step_avg:34.48ms
step:364/1900 train_time:12548ms step_avg:34.47ms
step:365/1900 train_time:12583ms step_avg:34.47ms
step:366/1900 train_time:12616ms step_avg:34.47ms
step:367/1900 train_time:12650ms step_avg:34.47ms
step:368/1900 train_time:12684ms step_avg:34.47ms
step:369/1900 train_time:12718ms step_avg:34.47ms
step:370/1900 train_time:12752ms step_avg:34.46ms
step:371/1900 train_time:12786ms step_avg:34.46ms
step:372/1900 train_time:12820ms step_avg:34.46ms
step:373/1900 train_time:12853ms step_avg:34.46ms
step:374/1900 train_time:12887ms step_avg:34.46ms
step:375/1900 train_time:12921ms step_avg:34.46ms
step:376/1900 train_time:12955ms step_avg:34.45ms
step:377/1900 train_time:12989ms step_avg:34.45ms
step:378/1900 train_time:13023ms step_avg:34.45ms
step:379/1900 train_time:13057ms step_avg:34.45ms
step:380/1900 train_time:13091ms step_avg:34.45ms
step:381/1900 train_time:13125ms step_avg:34.45ms
step:382/1900 train_time:13159ms step_avg:34.45ms
step:383/1900 train_time:13193ms step_avg:34.45ms
step:384/1900 train_time:13227ms step_avg:34.44ms
step:385/1900 train_time:13261ms step_avg:34.44ms
step:386/1900 train_time:13295ms step_avg:34.44ms
step:387/1900 train_time:13329ms step_avg:34.44ms
step:388/1900 train_time:13363ms step_avg:34.44ms
step:389/1900 train_time:13397ms step_avg:34.44ms
step:390/1900 train_time:13431ms step_avg:34.44ms
step:391/1900 train_time:13465ms step_avg:34.44ms
step:392/1900 train_time:13498ms step_avg:34.43ms
step:393/1900 train_time:13533ms step_avg:34.43ms
step:394/1900 train_time:13566ms step_avg:34.43ms
step:395/1900 train_time:13600ms step_avg:34.43ms
step:396/1900 train_time:13634ms step_avg:34.43ms
step:397/1900 train_time:13668ms step_avg:34.43ms
step:398/1900 train_time:13702ms step_avg:34.43ms
step:399/1900 train_time:13736ms step_avg:34.43ms
step:400/1900 train_time:13770ms step_avg:34.42ms
step:401/1900 train_time:13804ms step_avg:34.42ms
step:402/1900 train_time:13838ms step_avg:34.42ms
step:403/1900 train_time:13872ms step_avg:34.42ms
step:404/1900 train_time:13906ms step_avg:34.42ms
step:405/1900 train_time:13940ms step_avg:34.42ms
step:406/1900 train_time:13974ms step_avg:34.42ms
step:407/1900 train_time:14008ms step_avg:34.42ms
step:408/1900 train_time:14041ms step_avg:34.42ms
step:409/1900 train_time:14076ms step_avg:34.41ms
step:410/1900 train_time:14109ms step_avg:34.41ms
step:411/1900 train_time:14143ms step_avg:34.41ms
step:412/1900 train_time:14177ms step_avg:34.41ms
step:413/1900 train_time:14212ms step_avg:34.41ms
step:414/1900 train_time:14246ms step_avg:34.41ms
step:415/1900 train_time:14280ms step_avg:34.41ms
step:416/1900 train_time:14314ms step_avg:34.41ms
step:417/1900 train_time:14348ms step_avg:34.41ms
step:418/1900 train_time:14382ms step_avg:34.41ms
step:419/1900 train_time:14416ms step_avg:34.41ms
step:420/1900 train_time:14450ms step_avg:34.40ms
step:421/1900 train_time:14484ms step_avg:34.40ms
step:422/1900 train_time:14517ms step_avg:34.40ms
step:423/1900 train_time:14551ms step_avg:34.40ms
step:424/1900 train_time:14585ms step_avg:34.40ms
step:425/1900 train_time:14619ms step_avg:34.40ms
step:426/1900 train_time:14653ms step_avg:34.40ms
step:427/1900 train_time:14687ms step_avg:34.40ms
step:428/1900 train_time:14720ms step_avg:34.39ms
step:429/1900 train_time:14754ms step_avg:34.39ms
step:430/1900 train_time:14788ms step_avg:34.39ms
step:431/1900 train_time:14822ms step_avg:34.39ms
step:432/1900 train_time:14856ms step_avg:34.39ms
step:433/1900 train_time:14890ms step_avg:34.39ms
step:434/1900 train_time:14924ms step_avg:34.39ms
step:435/1900 train_time:14958ms step_avg:34.39ms
step:436/1900 train_time:14992ms step_avg:34.39ms
step:437/1900 train_time:15026ms step_avg:34.38ms
step:438/1900 train_time:15060ms step_avg:34.38ms
step:439/1900 train_time:15094ms step_avg:34.38ms
step:440/1900 train_time:15128ms step_avg:34.38ms
step:441/1900 train_time:15162ms step_avg:34.38ms
step:442/1900 train_time:15196ms step_avg:34.38ms
step:443/1900 train_time:15230ms step_avg:34.38ms
step:444/1900 train_time:15263ms step_avg:34.38ms
step:445/1900 train_time:15297ms step_avg:34.38ms
step:446/1900 train_time:15331ms step_avg:34.38ms
step:447/1900 train_time:15365ms step_avg:34.37ms
step:448/1900 train_time:15399ms step_avg:34.37ms
step:449/1900 train_time:15433ms step_avg:34.37ms
step:450/1900 train_time:15467ms step_avg:34.37ms
step:451/1900 train_time:15501ms step_avg:34.37ms
step:452/1900 train_time:15535ms step_avg:34.37ms
step:453/1900 train_time:15569ms step_avg:34.37ms
step:454/1900 train_time:15603ms step_avg:34.37ms
step:455/1900 train_time:15637ms step_avg:34.37ms
step:456/1900 train_time:15671ms step_avg:34.37ms
step:457/1900 train_time:15705ms step_avg:34.37ms
step:458/1900 train_time:15739ms step_avg:34.36ms
step:459/1900 train_time:15773ms step_avg:34.36ms
step:460/1900 train_time:15807ms step_avg:34.36ms
step:461/1900 train_time:15841ms step_avg:34.36ms
step:462/1900 train_time:15875ms step_avg:34.36ms
step:463/1900 train_time:15909ms step_avg:34.36ms
step:464/1900 train_time:15942ms step_avg:34.36ms
step:465/1900 train_time:15976ms step_avg:34.36ms
step:466/1900 train_time:16010ms step_avg:34.36ms
step:467/1900 train_time:16044ms step_avg:34.36ms
step:468/1900 train_time:16078ms step_avg:34.36ms
step:469/1900 train_time:16112ms step_avg:34.35ms
step:470/1900 train_time:16146ms step_avg:34.35ms
step:471/1900 train_time:16180ms step_avg:34.35ms
step:472/1900 train_time:16214ms step_avg:34.35ms
step:473/1900 train_time:16248ms step_avg:34.35ms
step:474/1900 train_time:16282ms step_avg:34.35ms
step:475/1900 train_time:16316ms step_avg:34.35ms
step:476/1900 train_time:16349ms step_avg:34.35ms
step:477/1900 train_time:16383ms step_avg:34.35ms
step:478/1900 train_time:16417ms step_avg:34.35ms
step:479/1900 train_time:16451ms step_avg:34.34ms
step:480/1900 train_time:16485ms step_avg:34.34ms
step:481/1900 train_time:16519ms step_avg:34.34ms
step:482/1900 train_time:16553ms step_avg:34.34ms
step:483/1900 train_time:16587ms step_avg:34.34ms
step:484/1900 train_time:16621ms step_avg:34.34ms
step:485/1900 train_time:16655ms step_avg:34.34ms
step:486/1900 train_time:16689ms step_avg:34.34ms
step:487/1900 train_time:16723ms step_avg:34.34ms
step:488/1900 train_time:16757ms step_avg:34.34ms
step:489/1900 train_time:16791ms step_avg:34.34ms
step:490/1900 train_time:16824ms step_avg:34.34ms
step:491/1900 train_time:16858ms step_avg:34.33ms
step:492/1900 train_time:16892ms step_avg:34.33ms
step:493/1900 train_time:16926ms step_avg:34.33ms
step:494/1900 train_time:16960ms step_avg:34.33ms
step:495/1900 train_time:16994ms step_avg:34.33ms
step:496/1900 train_time:17028ms step_avg:34.33ms
step:497/1900 train_time:17061ms step_avg:34.33ms
step:498/1900 train_time:17095ms step_avg:34.33ms
step:499/1900 train_time:17129ms step_avg:34.33ms
step:500/1900 train_time:17163ms step_avg:34.33ms
step:500/1900 val_loss:4.2791 train_time:17200ms step_avg:34.40ms
step:501/1900 train_time:17220ms step_avg:34.37ms
step:502/1900 train_time:17239ms step_avg:34.34ms
step:503/1900 train_time:17268ms step_avg:34.33ms
step:504/1900 train_time:17302ms step_avg:34.33ms
step:505/1900 train_time:17337ms step_avg:34.33ms
step:506/1900 train_time:17371ms step_avg:34.33ms
step:507/1900 train_time:17406ms step_avg:34.33ms
step:508/1900 train_time:17440ms step_avg:34.33ms
step:509/1900 train_time:17474ms step_avg:34.33ms
step:510/1900 train_time:17508ms step_avg:34.33ms
step:511/1900 train_time:17542ms step_avg:34.33ms
step:512/1900 train_time:17576ms step_avg:34.33ms
step:513/1900 train_time:17610ms step_avg:34.33ms
step:514/1900 train_time:17644ms step_avg:34.33ms
step:515/1900 train_time:17677ms step_avg:34.32ms
step:516/1900 train_time:17711ms step_avg:34.32ms
step:517/1900 train_time:17745ms step_avg:34.32ms
step:518/1900 train_time:17779ms step_avg:34.32ms
step:519/1900 train_time:17813ms step_avg:34.32ms
step:520/1900 train_time:17846ms step_avg:34.32ms
step:521/1900 train_time:17880ms step_avg:34.32ms
step:522/1900 train_time:17914ms step_avg:34.32ms
step:523/1900 train_time:17948ms step_avg:34.32ms
step:524/1900 train_time:17981ms step_avg:34.32ms
step:525/1900 train_time:18015ms step_avg:34.31ms
step:526/1900 train_time:18049ms step_avg:34.31ms
step:527/1900 train_time:18083ms step_avg:34.31ms
step:528/1900 train_time:18117ms step_avg:34.31ms
step:529/1900 train_time:18151ms step_avg:34.31ms
step:530/1900 train_time:18185ms step_avg:34.31ms
step:531/1900 train_time:18219ms step_avg:34.31ms
step:532/1900 train_time:18253ms step_avg:34.31ms
step:533/1900 train_time:18287ms step_avg:34.31ms
step:534/1900 train_time:18321ms step_avg:34.31ms
step:535/1900 train_time:18355ms step_avg:34.31ms
step:536/1900 train_time:18389ms step_avg:34.31ms
step:537/1900 train_time:18423ms step_avg:34.31ms
step:538/1900 train_time:18457ms step_avg:34.31ms
step:539/1900 train_time:18491ms step_avg:34.31ms
step:540/1900 train_time:18525ms step_avg:34.31ms
step:541/1900 train_time:18559ms step_avg:34.31ms
step:542/1900 train_time:18593ms step_avg:34.30ms
step:543/1900 train_time:18627ms step_avg:34.30ms
step:544/1900 train_time:18661ms step_avg:34.30ms
step:545/1900 train_time:18695ms step_avg:34.30ms
step:546/1900 train_time:18728ms step_avg:34.30ms
step:547/1900 train_time:18763ms step_avg:34.30ms
step:548/1900 train_time:18796ms step_avg:34.30ms
step:549/1900 train_time:18830ms step_avg:34.30ms
step:550/1900 train_time:18864ms step_avg:34.30ms
step:551/1900 train_time:18898ms step_avg:34.30ms
step:552/1900 train_time:18932ms step_avg:34.30ms
step:553/1900 train_time:18966ms step_avg:34.30ms
step:554/1900 train_time:18999ms step_avg:34.29ms
step:555/1900 train_time:19033ms step_avg:34.29ms
step:556/1900 train_time:19067ms step_avg:34.29ms
step:557/1900 train_time:19101ms step_avg:34.29ms
step:558/1900 train_time:19135ms step_avg:34.29ms
step:559/1900 train_time:19169ms step_avg:34.29ms
step:560/1900 train_time:19202ms step_avg:34.29ms
step:561/1900 train_time:19237ms step_avg:34.29ms
step:562/1900 train_time:19271ms step_avg:34.29ms
step:563/1900 train_time:19305ms step_avg:34.29ms
step:564/1900 train_time:19338ms step_avg:34.29ms
step:565/1900 train_time:19373ms step_avg:34.29ms
step:566/1900 train_time:19406ms step_avg:34.29ms
step:567/1900 train_time:19441ms step_avg:34.29ms
step:568/1900 train_time:19474ms step_avg:34.29ms
step:569/1900 train_time:19509ms step_avg:34.29ms
step:570/1900 train_time:19542ms step_avg:34.28ms
step:571/1900 train_time:19576ms step_avg:34.28ms
step:572/1900 train_time:19610ms step_avg:34.28ms
step:573/1900 train_time:19644ms step_avg:34.28ms
step:574/1900 train_time:19678ms step_avg:34.28ms
step:575/1900 train_time:19712ms step_avg:34.28ms
step:576/1900 train_time:19746ms step_avg:34.28ms
step:577/1900 train_time:19780ms step_avg:34.28ms
step:578/1900 train_time:19814ms step_avg:34.28ms
step:579/1900 train_time:19848ms step_avg:34.28ms
step:580/1900 train_time:19881ms step_avg:34.28ms
step:581/1900 train_time:19915ms step_avg:34.28ms
step:582/1900 train_time:19949ms step_avg:34.28ms
step:583/1900 train_time:19983ms step_avg:34.28ms
step:584/1900 train_time:20017ms step_avg:34.28ms
step:585/1900 train_time:20051ms step_avg:34.28ms
step:586/1900 train_time:20085ms step_avg:34.27ms
step:587/1900 train_time:20119ms step_avg:34.27ms
step:588/1900 train_time:20153ms step_avg:34.27ms
step:589/1900 train_time:20187ms step_avg:34.27ms
step:590/1900 train_time:20220ms step_avg:34.27ms
step:591/1900 train_time:20254ms step_avg:34.27ms
step:592/1900 train_time:20288ms step_avg:34.27ms
step:593/1900 train_time:20322ms step_avg:34.27ms
step:594/1900 train_time:20356ms step_avg:34.27ms
step:595/1900 train_time:20390ms step_avg:34.27ms
step:596/1900 train_time:20424ms step_avg:34.27ms
step:597/1900 train_time:20458ms step_avg:34.27ms
step:598/1900 train_time:20492ms step_avg:34.27ms
step:599/1900 train_time:20526ms step_avg:34.27ms
step:600/1900 train_time:20560ms step_avg:34.27ms
step:601/1900 train_time:20594ms step_avg:34.27ms
step:602/1900 train_time:20627ms step_avg:34.26ms
step:603/1900 train_time:20661ms step_avg:34.26ms
step:604/1900 train_time:20695ms step_avg:34.26ms
step:605/1900 train_time:20729ms step_avg:34.26ms
step:606/1900 train_time:20763ms step_avg:34.26ms
step:607/1900 train_time:20797ms step_avg:34.26ms
step:608/1900 train_time:20831ms step_avg:34.26ms
step:609/1900 train_time:20865ms step_avg:34.26ms
step:610/1900 train_time:20899ms step_avg:34.26ms
step:611/1900 train_time:20933ms step_avg:34.26ms
step:612/1900 train_time:20967ms step_avg:34.26ms
step:613/1900 train_time:21001ms step_avg:34.26ms
step:614/1900 train_time:21035ms step_avg:34.26ms
step:615/1900 train_time:21068ms step_avg:34.26ms
step:616/1900 train_time:21102ms step_avg:34.26ms
step:617/1900 train_time:21136ms step_avg:34.26ms
step:618/1900 train_time:21170ms step_avg:34.26ms
step:619/1900 train_time:21204ms step_avg:34.26ms
step:620/1900 train_time:21238ms step_avg:34.25ms
step:621/1900 train_time:21273ms step_avg:34.26ms
step:622/1900 train_time:21333ms step_avg:34.30ms
step:623/1900 train_time:21394ms step_avg:34.34ms
step:624/1900 train_time:21455ms step_avg:34.38ms
step:625/1900 train_time:21517ms step_avg:34.43ms
step:626/1900 train_time:21578ms step_avg:34.47ms
step:627/1900 train_time:21640ms step_avg:34.51ms
step:628/1900 train_time:21701ms step_avg:34.56ms
step:629/1900 train_time:21763ms step_avg:34.60ms
step:630/1900 train_time:21824ms step_avg:34.64ms
step:631/1900 train_time:21886ms step_avg:34.68ms
step:632/1900 train_time:21947ms step_avg:34.73ms
step:633/1900 train_time:22009ms step_avg:34.77ms
step:634/1900 train_time:22071ms step_avg:34.81ms
step:635/1900 train_time:22132ms step_avg:34.85ms
step:636/1900 train_time:22192ms step_avg:34.89ms
step:637/1900 train_time:22254ms step_avg:34.94ms
step:638/1900 train_time:22315ms step_avg:34.98ms
step:639/1900 train_time:22376ms step_avg:35.02ms
step:640/1900 train_time:22437ms step_avg:35.06ms
step:641/1900 train_time:22499ms step_avg:35.10ms
step:642/1900 train_time:22560ms step_avg:35.14ms
step:643/1900 train_time:22622ms step_avg:35.18ms
step:644/1900 train_time:22683ms step_avg:35.22ms
step:645/1900 train_time:22745ms step_avg:35.26ms
step:646/1900 train_time:22806ms step_avg:35.30ms
step:647/1900 train_time:22868ms step_avg:35.34ms
step:648/1900 train_time:22929ms step_avg:35.38ms
step:649/1900 train_time:22991ms step_avg:35.43ms
step:650/1900 train_time:23052ms step_avg:35.46ms
step:651/1900 train_time:23113ms step_avg:35.50ms
step:652/1900 train_time:23174ms step_avg:35.54ms
step:653/1900 train_time:23235ms step_avg:35.58ms
step:654/1900 train_time:23296ms step_avg:35.62ms
step:655/1900 train_time:23358ms step_avg:35.66ms
step:656/1900 train_time:23418ms step_avg:35.70ms
step:657/1900 train_time:23481ms step_avg:35.74ms
step:658/1900 train_time:23542ms step_avg:35.78ms
step:659/1900 train_time:23604ms step_avg:35.82ms
step:660/1900 train_time:23665ms step_avg:35.86ms
step:661/1900 train_time:23727ms step_avg:35.90ms
step:662/1900 train_time:23788ms step_avg:35.93ms
step:663/1900 train_time:23850ms step_avg:35.97ms
step:664/1900 train_time:23911ms step_avg:36.01ms
step:665/1900 train_time:23973ms step_avg:36.05ms
step:666/1900 train_time:24034ms step_avg:36.09ms
step:667/1900 train_time:24095ms step_avg:36.12ms
step:668/1900 train_time:24156ms step_avg:36.16ms
step:669/1900 train_time:24217ms step_avg:36.20ms
step:670/1900 train_time:24278ms step_avg:36.24ms
step:671/1900 train_time:24340ms step_avg:36.27ms
step:672/1900 train_time:24401ms step_avg:36.31ms
step:673/1900 train_time:24463ms step_avg:36.35ms
step:674/1900 train_time:24523ms step_avg:36.38ms
step:675/1900 train_time:24586ms step_avg:36.42ms
step:676/1900 train_time:24646ms step_avg:36.46ms
step:677/1900 train_time:24708ms step_avg:36.50ms
step:678/1900 train_time:24769ms step_avg:36.53ms
step:679/1900 train_time:24831ms step_avg:36.57ms
step:680/1900 train_time:24892ms step_avg:36.61ms
step:681/1900 train_time:24953ms step_avg:36.64ms
step:682/1900 train_time:25014ms step_avg:36.68ms
step:683/1900 train_time:25075ms step_avg:36.71ms
step:684/1900 train_time:25136ms step_avg:36.75ms
step:685/1900 train_time:25197ms step_avg:36.78ms
step:686/1900 train_time:25258ms step_avg:36.82ms
step:687/1900 train_time:25320ms step_avg:36.86ms
step:688/1900 train_time:25381ms step_avg:36.89ms
step:689/1900 train_time:25443ms step_avg:36.93ms
step:690/1900 train_time:25504ms step_avg:36.96ms
step:691/1900 train_time:25566ms step_avg:37.00ms
step:692/1900 train_time:25627ms step_avg:37.03ms
step:693/1900 train_time:25689ms step_avg:37.07ms
step:694/1900 train_time:25749ms step_avg:37.10ms
step:695/1900 train_time:25812ms step_avg:37.14ms
step:696/1900 train_time:25873ms step_avg:37.17ms
step:697/1900 train_time:25935ms step_avg:37.21ms
step:698/1900 train_time:25995ms step_avg:37.24ms
step:699/1900 train_time:26057ms step_avg:37.28ms
step:700/1900 train_time:26118ms step_avg:37.31ms
step:701/1900 train_time:26179ms step_avg:37.35ms
step:702/1900 train_time:26240ms step_avg:37.38ms
step:703/1900 train_time:26301ms step_avg:37.41ms
step:704/1900 train_time:26362ms step_avg:37.45ms
step:705/1900 train_time:26423ms step_avg:37.48ms
step:706/1900 train_time:26484ms step_avg:37.51ms
step:707/1900 train_time:26546ms step_avg:37.55ms
step:708/1900 train_time:26607ms step_avg:37.58ms
step:709/1900 train_time:26668ms step_avg:37.61ms
step:710/1900 train_time:26729ms step_avg:37.65ms
step:711/1900 train_time:26791ms step_avg:37.68ms
step:712/1900 train_time:26852ms step_avg:37.71ms
step:713/1900 train_time:26914ms step_avg:37.75ms
step:714/1900 train_time:26975ms step_avg:37.78ms
step:715/1900 train_time:27037ms step_avg:37.81ms
step:716/1900 train_time:27098ms step_avg:37.85ms
step:717/1900 train_time:27160ms step_avg:37.88ms
step:718/1900 train_time:27221ms step_avg:37.91ms
step:719/1900 train_time:27283ms step_avg:37.95ms
step:720/1900 train_time:27344ms step_avg:37.98ms
step:721/1900 train_time:27406ms step_avg:38.01ms
step:722/1900 train_time:27466ms step_avg:38.04ms
step:723/1900 train_time:27528ms step_avg:38.07ms
step:724/1900 train_time:27589ms step_avg:38.11ms
step:725/1900 train_time:27651ms step_avg:38.14ms
step:726/1900 train_time:27712ms step_avg:38.17ms
step:727/1900 train_time:27774ms step_avg:38.20ms
step:728/1900 train_time:27835ms step_avg:38.23ms
step:729/1900 train_time:27897ms step_avg:38.27ms
step:730/1900 train_time:27957ms step_avg:38.30ms
step:731/1900 train_time:28019ms step_avg:38.33ms
step:732/1900 train_time:28080ms step_avg:38.36ms
step:733/1900 train_time:28142ms step_avg:38.39ms
step:734/1900 train_time:28203ms step_avg:38.42ms
step:735/1900 train_time:28264ms step_avg:38.45ms
step:736/1900 train_time:28325ms step_avg:38.49ms
step:737/1900 train_time:28387ms step_avg:38.52ms
step:738/1900 train_time:28448ms step_avg:38.55ms
step:739/1900 train_time:28510ms step_avg:38.58ms
step:740/1900 train_time:28571ms step_avg:38.61ms
step:741/1900 train_time:28632ms step_avg:38.64ms
step:742/1900 train_time:28693ms step_avg:38.67ms
step:743/1900 train_time:28755ms step_avg:38.70ms
step:744/1900 train_time:28816ms step_avg:38.73ms
step:745/1900 train_time:28878ms step_avg:38.76ms
step:746/1900 train_time:28938ms step_avg:38.79ms
step:747/1900 train_time:29000ms step_avg:38.82ms
step:748/1900 train_time:29061ms step_avg:38.85ms
step:749/1900 train_time:29123ms step_avg:38.88ms
step:750/1900 train_time:29184ms step_avg:38.91ms
step:750/1900 val_loss:4.0230 train_time:29248ms step_avg:39.00ms
step:751/1900 train_time:29268ms step_avg:38.97ms
step:752/1900 train_time:29309ms step_avg:38.98ms
step:753/1900 train_time:29372ms step_avg:39.01ms
step:754/1900 train_time:29435ms step_avg:39.04ms
step:755/1900 train_time:29497ms step_avg:39.07ms
step:756/1900 train_time:29558ms step_avg:39.10ms
step:757/1900 train_time:29619ms step_avg:39.13ms
step:758/1900 train_time:29680ms step_avg:39.16ms
step:759/1900 train_time:29741ms step_avg:39.18ms
step:760/1900 train_time:29801ms step_avg:39.21ms
step:761/1900 train_time:29863ms step_avg:39.24ms
step:762/1900 train_time:29924ms step_avg:39.27ms
step:763/1900 train_time:29985ms step_avg:39.30ms
step:764/1900 train_time:30046ms step_avg:39.33ms
step:765/1900 train_time:30108ms step_avg:39.36ms
step:766/1900 train_time:30169ms step_avg:39.39ms
step:767/1900 train_time:30232ms step_avg:39.42ms
step:768/1900 train_time:30294ms step_avg:39.45ms
step:769/1900 train_time:30357ms step_avg:39.48ms
step:770/1900 train_time:30418ms step_avg:39.50ms
step:771/1900 train_time:30481ms step_avg:39.53ms
step:772/1900 train_time:30541ms step_avg:39.56ms
step:773/1900 train_time:30603ms step_avg:39.59ms
step:774/1900 train_time:30664ms step_avg:39.62ms
step:775/1900 train_time:30725ms step_avg:39.65ms
step:776/1900 train_time:30786ms step_avg:39.67ms
step:777/1900 train_time:30848ms step_avg:39.70ms
step:778/1900 train_time:30909ms step_avg:39.73ms
step:779/1900 train_time:30970ms step_avg:39.76ms
step:780/1900 train_time:31031ms step_avg:39.78ms
step:781/1900 train_time:31092ms step_avg:39.81ms
step:782/1900 train_time:31153ms step_avg:39.84ms
step:783/1900 train_time:31215ms step_avg:39.87ms
step:784/1900 train_time:31277ms step_avg:39.89ms
step:785/1900 train_time:31339ms step_avg:39.92ms
step:786/1900 train_time:31400ms step_avg:39.95ms
step:787/1900 train_time:31463ms step_avg:39.98ms
step:788/1900 train_time:31524ms step_avg:40.01ms
step:789/1900 train_time:31586ms step_avg:40.03ms
step:790/1900 train_time:31647ms step_avg:40.06ms
step:791/1900 train_time:31709ms step_avg:40.09ms
step:792/1900 train_time:31769ms step_avg:40.11ms
step:793/1900 train_time:31831ms step_avg:40.14ms
step:794/1900 train_time:31892ms step_avg:40.17ms
step:795/1900 train_time:31954ms step_avg:40.19ms
step:796/1900 train_time:32015ms step_avg:40.22ms
step:797/1900 train_time:32076ms step_avg:40.25ms
step:798/1900 train_time:32137ms step_avg:40.27ms
step:799/1900 train_time:32199ms step_avg:40.30ms
step:800/1900 train_time:32260ms step_avg:40.32ms
step:801/1900 train_time:32322ms step_avg:40.35ms
step:802/1900 train_time:32384ms step_avg:40.38ms
step:803/1900 train_time:32447ms step_avg:40.41ms
step:804/1900 train_time:32507ms step_avg:40.43ms
step:805/1900 train_time:32570ms step_avg:40.46ms
step:806/1900 train_time:32631ms step_avg:40.49ms
step:807/1900 train_time:32693ms step_avg:40.51ms
step:808/1900 train_time:32754ms step_avg:40.54ms
step:809/1900 train_time:32815ms step_avg:40.56ms
step:810/1900 train_time:32876ms step_avg:40.59ms
step:811/1900 train_time:32938ms step_avg:40.61ms
step:812/1900 train_time:32999ms step_avg:40.64ms
step:813/1900 train_time:33061ms step_avg:40.67ms
step:814/1900 train_time:33122ms step_avg:40.69ms
step:815/1900 train_time:33184ms step_avg:40.72ms
step:816/1900 train_time:33245ms step_avg:40.74ms
step:817/1900 train_time:33306ms step_avg:40.77ms
step:818/1900 train_time:33368ms step_avg:40.79ms
step:819/1900 train_time:33430ms step_avg:40.82ms
step:820/1900 train_time:33491ms step_avg:40.84ms
step:821/1900 train_time:33553ms step_avg:40.87ms
step:822/1900 train_time:33615ms step_avg:40.89ms
step:823/1900 train_time:33676ms step_avg:40.92ms
step:824/1900 train_time:33737ms step_avg:40.94ms
step:825/1900 train_time:33799ms step_avg:40.97ms
step:826/1900 train_time:33860ms step_avg:40.99ms
step:827/1900 train_time:33921ms step_avg:41.02ms
step:828/1900 train_time:33982ms step_avg:41.04ms
step:829/1900 train_time:34044ms step_avg:41.07ms
step:830/1900 train_time:34104ms step_avg:41.09ms
step:831/1900 train_time:34166ms step_avg:41.11ms
step:832/1900 train_time:34227ms step_avg:41.14ms
step:833/1900 train_time:34289ms step_avg:41.16ms
step:834/1900 train_time:34351ms step_avg:41.19ms
step:835/1900 train_time:34413ms step_avg:41.21ms
step:836/1900 train_time:34474ms step_avg:41.24ms
step:837/1900 train_time:34536ms step_avg:41.26ms
step:838/1900 train_time:34597ms step_avg:41.29ms
step:839/1900 train_time:34659ms step_avg:41.31ms
step:840/1900 train_time:34720ms step_avg:41.33ms
step:841/1900 train_time:34782ms step_avg:41.36ms
step:842/1900 train_time:34843ms step_avg:41.38ms
step:843/1900 train_time:34905ms step_avg:41.41ms
step:844/1900 train_time:34966ms step_avg:41.43ms
step:845/1900 train_time:35028ms step_avg:41.45ms
step:846/1900 train_time:35089ms step_avg:41.48ms
step:847/1900 train_time:35150ms step_avg:41.50ms
step:848/1900 train_time:35211ms step_avg:41.52ms
step:849/1900 train_time:35274ms step_avg:41.55ms
step:850/1900 train_time:35334ms step_avg:41.57ms
step:851/1900 train_time:35396ms step_avg:41.59ms
step:852/1900 train_time:35457ms step_avg:41.62ms
step:853/1900 train_time:35520ms step_avg:41.64ms
step:854/1900 train_time:35581ms step_avg:41.66ms
step:855/1900 train_time:35643ms step_avg:41.69ms
step:856/1900 train_time:35704ms step_avg:41.71ms
step:857/1900 train_time:35766ms step_avg:41.73ms
step:858/1900 train_time:35827ms step_avg:41.76ms
step:859/1900 train_time:35889ms step_avg:41.78ms
step:860/1900 train_time:35950ms step_avg:41.80ms
step:861/1900 train_time:36012ms step_avg:41.83ms
step:862/1900 train_time:36073ms step_avg:41.85ms
step:863/1900 train_time:36135ms step_avg:41.87ms
step:864/1900 train_time:36196ms step_avg:41.89ms
step:865/1900 train_time:36257ms step_avg:41.92ms
step:866/1900 train_time:36318ms step_avg:41.94ms
step:867/1900 train_time:36380ms step_avg:41.96ms
step:868/1900 train_time:36440ms step_avg:41.98ms
step:869/1900 train_time:36502ms step_avg:42.01ms
step:870/1900 train_time:36564ms step_avg:42.03ms
step:871/1900 train_time:36626ms step_avg:42.05ms
step:872/1900 train_time:36687ms step_avg:42.07ms
step:873/1900 train_time:36749ms step_avg:42.09ms
step:874/1900 train_time:36809ms step_avg:42.12ms
step:875/1900 train_time:36871ms step_avg:42.14ms
step:876/1900 train_time:36933ms step_avg:42.16ms
step:877/1900 train_time:36995ms step_avg:42.18ms
step:878/1900 train_time:37055ms step_avg:42.20ms
step:879/1900 train_time:37117ms step_avg:42.23ms
step:880/1900 train_time:37178ms step_avg:42.25ms
step:881/1900 train_time:37240ms step_avg:42.27ms
step:882/1900 train_time:37301ms step_avg:42.29ms
step:883/1900 train_time:37362ms step_avg:42.31ms
step:884/1900 train_time:37424ms step_avg:42.33ms
step:885/1900 train_time:37486ms step_avg:42.36ms
step:886/1900 train_time:37547ms step_avg:42.38ms
step:887/1900 train_time:37609ms step_avg:42.40ms
step:888/1900 train_time:37670ms step_avg:42.42ms
step:889/1900 train_time:37732ms step_avg:42.44ms
step:890/1900 train_time:37793ms step_avg:42.46ms
step:891/1900 train_time:37855ms step_avg:42.49ms
step:892/1900 train_time:37916ms step_avg:42.51ms
step:893/1900 train_time:37978ms step_avg:42.53ms
step:894/1900 train_time:38039ms step_avg:42.55ms
step:895/1900 train_time:38101ms step_avg:42.57ms
step:896/1900 train_time:38162ms step_avg:42.59ms
step:897/1900 train_time:38224ms step_avg:42.61ms
step:898/1900 train_time:38285ms step_avg:42.63ms
step:899/1900 train_time:38347ms step_avg:42.66ms
step:900/1900 train_time:38408ms step_avg:42.68ms
step:901/1900 train_time:38469ms step_avg:42.70ms
step:902/1900 train_time:38530ms step_avg:42.72ms
step:903/1900 train_time:38592ms step_avg:42.74ms
step:904/1900 train_time:38653ms step_avg:42.76ms
step:905/1900 train_time:38715ms step_avg:42.78ms
step:906/1900 train_time:38775ms step_avg:42.80ms
step:907/1900 train_time:38837ms step_avg:42.82ms
step:908/1900 train_time:38898ms step_avg:42.84ms
step:909/1900 train_time:38960ms step_avg:42.86ms
step:910/1900 train_time:39021ms step_avg:42.88ms
step:911/1900 train_time:39083ms step_avg:42.90ms
step:912/1900 train_time:39144ms step_avg:42.92ms
step:913/1900 train_time:39205ms step_avg:42.94ms
step:914/1900 train_time:39266ms step_avg:42.96ms
step:915/1900 train_time:39328ms step_avg:42.98ms
step:916/1900 train_time:39389ms step_avg:43.00ms
step:917/1900 train_time:39451ms step_avg:43.02ms
step:918/1900 train_time:39512ms step_avg:43.04ms
step:919/1900 train_time:39574ms step_avg:43.06ms
step:920/1900 train_time:39635ms step_avg:43.08ms
step:921/1900 train_time:39696ms step_avg:43.10ms
step:922/1900 train_time:39757ms step_avg:43.12ms
step:923/1900 train_time:39819ms step_avg:43.14ms
step:924/1900 train_time:39880ms step_avg:43.16ms
step:925/1900 train_time:39941ms step_avg:43.18ms
step:926/1900 train_time:40002ms step_avg:43.20ms
step:927/1900 train_time:40064ms step_avg:43.22ms
step:928/1900 train_time:40125ms step_avg:43.24ms
step:929/1900 train_time:40187ms step_avg:43.26ms
step:930/1900 train_time:40248ms step_avg:43.28ms
step:931/1900 train_time:40310ms step_avg:43.30ms
step:932/1900 train_time:40371ms step_avg:43.32ms
step:933/1900 train_time:40433ms step_avg:43.34ms
step:934/1900 train_time:40494ms step_avg:43.36ms
step:935/1900 train_time:40556ms step_avg:43.38ms
step:936/1900 train_time:40617ms step_avg:43.39ms
step:937/1900 train_time:40679ms step_avg:43.41ms
step:938/1900 train_time:40740ms step_avg:43.43ms
step:939/1900 train_time:40802ms step_avg:43.45ms
step:940/1900 train_time:40863ms step_avg:43.47ms
step:941/1900 train_time:40925ms step_avg:43.49ms
step:942/1900 train_time:40986ms step_avg:43.51ms
step:943/1900 train_time:41048ms step_avg:43.53ms
step:944/1900 train_time:41109ms step_avg:43.55ms
step:945/1900 train_time:41171ms step_avg:43.57ms
step:946/1900 train_time:41232ms step_avg:43.59ms
step:947/1900 train_time:41294ms step_avg:43.61ms
step:948/1900 train_time:41355ms step_avg:43.62ms
step:949/1900 train_time:41417ms step_avg:43.64ms
step:950/1900 train_time:41479ms step_avg:43.66ms
step:951/1900 train_time:41541ms step_avg:43.68ms
step:952/1900 train_time:41602ms step_avg:43.70ms
step:953/1900 train_time:41664ms step_avg:43.72ms
step:954/1900 train_time:41725ms step_avg:43.74ms
step:955/1900 train_time:41786ms step_avg:43.76ms
step:956/1900 train_time:41848ms step_avg:43.77ms
step:957/1900 train_time:41910ms step_avg:43.79ms
step:958/1900 train_time:41970ms step_avg:43.81ms
step:959/1900 train_time:42032ms step_avg:43.83ms
step:960/1900 train_time:42093ms step_avg:43.85ms
step:961/1900 train_time:42155ms step_avg:43.87ms
step:962/1900 train_time:42216ms step_avg:43.88ms
step:963/1900 train_time:42278ms step_avg:43.90ms
step:964/1900 train_time:42339ms step_avg:43.92ms
step:965/1900 train_time:42401ms step_avg:43.94ms
step:966/1900 train_time:42462ms step_avg:43.96ms
step:967/1900 train_time:42524ms step_avg:43.98ms
step:968/1900 train_time:42586ms step_avg:43.99ms
step:969/1900 train_time:42647ms step_avg:44.01ms
step:970/1900 train_time:42708ms step_avg:44.03ms
step:971/1900 train_time:42771ms step_avg:44.05ms
step:972/1900 train_time:42832ms step_avg:44.07ms
step:973/1900 train_time:42894ms step_avg:44.08ms
step:974/1900 train_time:42955ms step_avg:44.10ms
step:975/1900 train_time:43016ms step_avg:44.12ms
step:976/1900 train_time:43077ms step_avg:44.14ms
step:977/1900 train_time:43139ms step_avg:44.15ms
step:978/1900 train_time:43200ms step_avg:44.17ms
step:979/1900 train_time:43262ms step_avg:44.19ms
step:980/1900 train_time:43323ms step_avg:44.21ms
step:981/1900 train_time:43384ms step_avg:44.22ms
step:982/1900 train_time:43446ms step_avg:44.24ms
step:983/1900 train_time:43508ms step_avg:44.26ms
step:984/1900 train_time:43569ms step_avg:44.28ms
step:985/1900 train_time:43631ms step_avg:44.30ms
step:986/1900 train_time:43692ms step_avg:44.31ms
step:987/1900 train_time:43753ms step_avg:44.33ms
step:988/1900 train_time:43814ms step_avg:44.35ms
step:989/1900 train_time:43876ms step_avg:44.36ms
step:990/1900 train_time:43937ms step_avg:44.38ms
step:991/1900 train_time:43999ms step_avg:44.40ms
step:992/1900 train_time:44060ms step_avg:44.42ms
step:993/1900 train_time:44122ms step_avg:44.43ms
step:994/1900 train_time:44183ms step_avg:44.45ms
step:995/1900 train_time:44245ms step_avg:44.47ms
step:996/1900 train_time:44306ms step_avg:44.48ms
step:997/1900 train_time:44368ms step_avg:44.50ms
step:998/1900 train_time:44430ms step_avg:44.52ms
step:999/1900 train_time:44492ms step_avg:44.54ms
step:1000/1900 train_time:44553ms step_avg:44.55ms
step:1000/1900 val_loss:3.7818 train_time:44617ms step_avg:44.62ms
step:1001/1900 train_time:44637ms step_avg:44.59ms
step:1002/1900 train_time:44679ms step_avg:44.59ms
step:1003/1900 train_time:44742ms step_avg:44.61ms
step:1004/1900 train_time:44804ms step_avg:44.63ms
step:1005/1900 train_time:44867ms step_avg:44.64ms
step:1006/1900 train_time:44928ms step_avg:44.66ms
step:1007/1900 train_time:44989ms step_avg:44.68ms
step:1008/1900 train_time:45049ms step_avg:44.69ms
step:1009/1900 train_time:45111ms step_avg:44.71ms
step:1010/1900 train_time:45171ms step_avg:44.72ms
step:1011/1900 train_time:45232ms step_avg:44.74ms
step:1012/1900 train_time:45293ms step_avg:44.76ms
step:1013/1900 train_time:45354ms step_avg:44.77ms
step:1014/1900 train_time:45416ms step_avg:44.79ms
step:1015/1900 train_time:45478ms step_avg:44.81ms
step:1016/1900 train_time:45539ms step_avg:44.82ms
step:1017/1900 train_time:45601ms step_avg:44.84ms
step:1018/1900 train_time:45664ms step_avg:44.86ms
step:1019/1900 train_time:45727ms step_avg:44.87ms
step:1020/1900 train_time:45789ms step_avg:44.89ms
step:1021/1900 train_time:45852ms step_avg:44.91ms
step:1022/1900 train_time:45913ms step_avg:44.93ms
step:1023/1900 train_time:45976ms step_avg:44.94ms
step:1024/1900 train_time:46036ms step_avg:44.96ms
step:1025/1900 train_time:46097ms step_avg:44.97ms
step:1026/1900 train_time:46158ms step_avg:44.99ms
step:1027/1900 train_time:46220ms step_avg:45.00ms
step:1028/1900 train_time:46280ms step_avg:45.02ms
step:1029/1900 train_time:46343ms step_avg:45.04ms
step:1030/1900 train_time:46404ms step_avg:45.05ms
step:1031/1900 train_time:46466ms step_avg:45.07ms
step:1032/1900 train_time:46527ms step_avg:45.08ms
step:1033/1900 train_time:46590ms step_avg:45.10ms
step:1034/1900 train_time:46651ms step_avg:45.12ms
step:1035/1900 train_time:46714ms step_avg:45.13ms
step:1036/1900 train_time:46775ms step_avg:45.15ms
step:1037/1900 train_time:46838ms step_avg:45.17ms
step:1038/1900 train_time:46900ms step_avg:45.18ms
step:1039/1900 train_time:46961ms step_avg:45.20ms
step:1040/1900 train_time:47022ms step_avg:45.21ms
step:1041/1900 train_time:47084ms step_avg:45.23ms
step:1042/1900 train_time:47145ms step_avg:45.24ms
step:1043/1900 train_time:47206ms step_avg:45.26ms
step:1044/1900 train_time:47267ms step_avg:45.27ms
step:1045/1900 train_time:47329ms step_avg:45.29ms
step:1046/1900 train_time:47389ms step_avg:45.31ms
step:1047/1900 train_time:47451ms step_avg:45.32ms
step:1048/1900 train_time:47511ms step_avg:45.34ms
step:1049/1900 train_time:47574ms step_avg:45.35ms
step:1050/1900 train_time:47636ms step_avg:45.37ms
step:1051/1900 train_time:47697ms step_avg:45.38ms
step:1052/1900 train_time:47759ms step_avg:45.40ms
step:1053/1900 train_time:47821ms step_avg:45.41ms
step:1054/1900 train_time:47882ms step_avg:45.43ms
step:1055/1900 train_time:47944ms step_avg:45.44ms
step:1056/1900 train_time:48005ms step_avg:45.46ms
step:1057/1900 train_time:48066ms step_avg:45.47ms
step:1058/1900 train_time:48127ms step_avg:45.49ms
step:1059/1900 train_time:48189ms step_avg:45.50ms
step:1060/1900 train_time:48250ms step_avg:45.52ms
step:1061/1900 train_time:48312ms step_avg:45.53ms
step:1062/1900 train_time:48372ms step_avg:45.55ms
step:1063/1900 train_time:48434ms step_avg:45.56ms
step:1064/1900 train_time:48495ms step_avg:45.58ms
step:1065/1900 train_time:48557ms step_avg:45.59ms
step:1066/1900 train_time:48618ms step_avg:45.61ms
step:1067/1900 train_time:48680ms step_avg:45.62ms
step:1068/1900 train_time:48741ms step_avg:45.64ms
step:1069/1900 train_time:48803ms step_avg:45.65ms
step:1070/1900 train_time:48864ms step_avg:45.67ms
step:1071/1900 train_time:48926ms step_avg:45.68ms
step:1072/1900 train_time:48987ms step_avg:45.70ms
step:1073/1900 train_time:49049ms step_avg:45.71ms
step:1074/1900 train_time:49110ms step_avg:45.73ms
step:1075/1900 train_time:49172ms step_avg:45.74ms
step:1076/1900 train_time:49233ms step_avg:45.76ms
step:1077/1900 train_time:49295ms step_avg:45.77ms
step:1078/1900 train_time:49355ms step_avg:45.78ms
step:1079/1900 train_time:49417ms step_avg:45.80ms
step:1080/1900 train_time:49478ms step_avg:45.81ms
step:1081/1900 train_time:49539ms step_avg:45.83ms
step:1082/1900 train_time:49600ms step_avg:45.84ms
step:1083/1900 train_time:49663ms step_avg:45.86ms
step:1084/1900 train_time:49724ms step_avg:45.87ms
step:1085/1900 train_time:49786ms step_avg:45.89ms
step:1086/1900 train_time:49847ms step_avg:45.90ms
step:1087/1900 train_time:49909ms step_avg:45.91ms
step:1088/1900 train_time:49970ms step_avg:45.93ms
step:1089/1900 train_time:50031ms step_avg:45.94ms
step:1090/1900 train_time:50092ms step_avg:45.96ms
step:1091/1900 train_time:50154ms step_avg:45.97ms
step:1092/1900 train_time:50215ms step_avg:45.98ms
step:1093/1900 train_time:50276ms step_avg:46.00ms
step:1094/1900 train_time:50337ms step_avg:46.01ms
step:1095/1900 train_time:50399ms step_avg:46.03ms
step:1096/1900 train_time:50459ms step_avg:46.04ms
step:1097/1900 train_time:50521ms step_avg:46.05ms
step:1098/1900 train_time:50582ms step_avg:46.07ms
step:1099/1900 train_time:50644ms step_avg:46.08ms
step:1100/1900 train_time:50705ms step_avg:46.10ms
step:1101/1900 train_time:50767ms step_avg:46.11ms
step:1102/1900 train_time:50829ms step_avg:46.12ms
step:1103/1900 train_time:50892ms step_avg:46.14ms
step:1104/1900 train_time:50953ms step_avg:46.15ms
step:1105/1900 train_time:51015ms step_avg:46.17ms
step:1106/1900 train_time:51076ms step_avg:46.18ms
step:1107/1900 train_time:51138ms step_avg:46.19ms
step:1108/1900 train_time:51199ms step_avg:46.21ms
step:1109/1900 train_time:51261ms step_avg:46.22ms
step:1110/1900 train_time:51321ms step_avg:46.24ms
step:1111/1900 train_time:51383ms step_avg:46.25ms
step:1112/1900 train_time:51444ms step_avg:46.26ms
step:1113/1900 train_time:51506ms step_avg:46.28ms
step:1114/1900 train_time:51566ms step_avg:46.29ms
step:1115/1900 train_time:51628ms step_avg:46.30ms
step:1116/1900 train_time:51689ms step_avg:46.32ms
step:1117/1900 train_time:51752ms step_avg:46.33ms
step:1118/1900 train_time:51813ms step_avg:46.34ms
step:1119/1900 train_time:51875ms step_avg:46.36ms
step:1120/1900 train_time:51936ms step_avg:46.37ms
step:1121/1900 train_time:51999ms step_avg:46.39ms
step:1122/1900 train_time:52060ms step_avg:46.40ms
step:1123/1900 train_time:52121ms step_avg:46.41ms
step:1124/1900 train_time:52182ms step_avg:46.43ms
step:1125/1900 train_time:52244ms step_avg:46.44ms
step:1126/1900 train_time:52305ms step_avg:46.45ms
step:1127/1900 train_time:52367ms step_avg:46.47ms
step:1128/1900 train_time:52428ms step_avg:46.48ms
step:1129/1900 train_time:52490ms step_avg:46.49ms
step:1130/1900 train_time:52551ms step_avg:46.50ms
step:1131/1900 train_time:52612ms step_avg:46.52ms
step:1132/1900 train_time:52674ms step_avg:46.53ms
step:1133/1900 train_time:52736ms step_avg:46.55ms
step:1134/1900 train_time:52797ms step_avg:46.56ms
step:1135/1900 train_time:52858ms step_avg:46.57ms
step:1136/1900 train_time:52919ms step_avg:46.58ms
step:1137/1900 train_time:52981ms step_avg:46.60ms
step:1138/1900 train_time:53042ms step_avg:46.61ms
step:1139/1900 train_time:53104ms step_avg:46.62ms
step:1140/1900 train_time:53165ms step_avg:46.64ms
step:1141/1900 train_time:53228ms step_avg:46.65ms
step:1142/1900 train_time:53289ms step_avg:46.66ms
step:1143/1900 train_time:53351ms step_avg:46.68ms
step:1144/1900 train_time:53412ms step_avg:46.69ms
step:1145/1900 train_time:53473ms step_avg:46.70ms
step:1146/1900 train_time:53534ms step_avg:46.71ms
step:1147/1900 train_time:53596ms step_avg:46.73ms
step:1148/1900 train_time:53657ms step_avg:46.74ms
step:1149/1900 train_time:53719ms step_avg:46.75ms
step:1150/1900 train_time:53779ms step_avg:46.76ms
step:1151/1900 train_time:53841ms step_avg:46.78ms
step:1152/1900 train_time:53902ms step_avg:46.79ms
step:1153/1900 train_time:53963ms step_avg:46.80ms
step:1154/1900 train_time:54025ms step_avg:46.82ms
step:1155/1900 train_time:54086ms step_avg:46.83ms
step:1156/1900 train_time:54147ms step_avg:46.84ms
step:1157/1900 train_time:54209ms step_avg:46.85ms
step:1158/1900 train_time:54270ms step_avg:46.87ms
step:1159/1900 train_time:54333ms step_avg:46.88ms
step:1160/1900 train_time:54394ms step_avg:46.89ms
step:1161/1900 train_time:54455ms step_avg:46.90ms
step:1162/1900 train_time:54517ms step_avg:46.92ms
step:1163/1900 train_time:54580ms step_avg:46.93ms
step:1164/1900 train_time:54640ms step_avg:46.94ms
step:1165/1900 train_time:54702ms step_avg:46.95ms
step:1166/1900 train_time:54763ms step_avg:46.97ms
step:1167/1900 train_time:54824ms step_avg:46.98ms
step:1168/1900 train_time:54885ms step_avg:46.99ms
step:1169/1900 train_time:54947ms step_avg:47.00ms
step:1170/1900 train_time:55008ms step_avg:47.02ms
step:1171/1900 train_time:55070ms step_avg:47.03ms
step:1172/1900 train_time:55131ms step_avg:47.04ms
step:1173/1900 train_time:55193ms step_avg:47.05ms
step:1174/1900 train_time:55254ms step_avg:47.06ms
step:1175/1900 train_time:55316ms step_avg:47.08ms
step:1176/1900 train_time:55377ms step_avg:47.09ms
step:1177/1900 train_time:55439ms step_avg:47.10ms
step:1178/1900 train_time:55500ms step_avg:47.11ms
step:1179/1900 train_time:55562ms step_avg:47.13ms
step:1180/1900 train_time:55623ms step_avg:47.14ms
step:1181/1900 train_time:55685ms step_avg:47.15ms
step:1182/1900 train_time:55747ms step_avg:47.16ms
step:1183/1900 train_time:55809ms step_avg:47.18ms
step:1184/1900 train_time:55870ms step_avg:47.19ms
step:1185/1900 train_time:55932ms step_avg:47.20ms
step:1186/1900 train_time:55993ms step_avg:47.21ms
step:1187/1900 train_time:56055ms step_avg:47.22ms
step:1188/1900 train_time:56116ms step_avg:47.24ms
step:1189/1900 train_time:56178ms step_avg:47.25ms
step:1190/1900 train_time:56239ms step_avg:47.26ms
step:1191/1900 train_time:56301ms step_avg:47.27ms
step:1192/1900 train_time:56363ms step_avg:47.28ms
step:1193/1900 train_time:56424ms step_avg:47.30ms
step:1194/1900 train_time:56486ms step_avg:47.31ms
step:1195/1900 train_time:56547ms step_avg:47.32ms
step:1196/1900 train_time:56608ms step_avg:47.33ms
step:1197/1900 train_time:56670ms step_avg:47.34ms
step:1198/1900 train_time:56731ms step_avg:47.35ms
step:1199/1900 train_time:56793ms step_avg:47.37ms
step:1200/1900 train_time:56855ms step_avg:47.38ms
step:1201/1900 train_time:56917ms step_avg:47.39ms
step:1202/1900 train_time:56979ms step_avg:47.40ms
step:1203/1900 train_time:57040ms step_avg:47.41ms
step:1204/1900 train_time:57101ms step_avg:47.43ms
step:1205/1900 train_time:57163ms step_avg:47.44ms
step:1206/1900 train_time:57224ms step_avg:47.45ms
step:1207/1900 train_time:57286ms step_avg:47.46ms
step:1208/1900 train_time:57346ms step_avg:47.47ms
step:1209/1900 train_time:57408ms step_avg:47.48ms
step:1210/1900 train_time:57469ms step_avg:47.50ms
step:1211/1900 train_time:57532ms step_avg:47.51ms
step:1212/1900 train_time:57593ms step_avg:47.52ms
step:1213/1900 train_time:57655ms step_avg:47.53ms
step:1214/1900 train_time:57716ms step_avg:47.54ms
step:1215/1900 train_time:57777ms step_avg:47.55ms
step:1216/1900 train_time:57838ms step_avg:47.56ms
step:1217/1900 train_time:57900ms step_avg:47.58ms
step:1218/1900 train_time:57961ms step_avg:47.59ms
step:1219/1900 train_time:58023ms step_avg:47.60ms
step:1220/1900 train_time:58084ms step_avg:47.61ms
step:1221/1900 train_time:58146ms step_avg:47.62ms
step:1222/1900 train_time:58208ms step_avg:47.63ms
step:1223/1900 train_time:58271ms step_avg:47.65ms
step:1224/1900 train_time:58332ms step_avg:47.66ms
step:1225/1900 train_time:58394ms step_avg:47.67ms
step:1226/1900 train_time:58455ms step_avg:47.68ms
step:1227/1900 train_time:58516ms step_avg:47.69ms
step:1228/1900 train_time:58577ms step_avg:47.70ms
step:1229/1900 train_time:58639ms step_avg:47.71ms
step:1230/1900 train_time:58700ms step_avg:47.72ms
step:1231/1900 train_time:58762ms step_avg:47.73ms
step:1232/1900 train_time:58822ms step_avg:47.75ms
step:1233/1900 train_time:58884ms step_avg:47.76ms
step:1234/1900 train_time:58945ms step_avg:47.77ms
step:1235/1900 train_time:59007ms step_avg:47.78ms
step:1236/1900 train_time:59068ms step_avg:47.79ms
step:1237/1900 train_time:59130ms step_avg:47.80ms
step:1238/1900 train_time:59191ms step_avg:47.81ms
step:1239/1900 train_time:59253ms step_avg:47.82ms
step:1240/1900 train_time:59314ms step_avg:47.83ms
step:1241/1900 train_time:59376ms step_avg:47.85ms
step:1242/1900 train_time:59464ms step_avg:47.88ms
step:1243/1900 train_time:59553ms step_avg:47.91ms
step:1244/1900 train_time:59641ms step_avg:47.94ms
step:1245/1900 train_time:59729ms step_avg:47.97ms
step:1246/1900 train_time:59817ms step_avg:48.01ms
step:1247/1900 train_time:59905ms step_avg:48.04ms
step:1248/1900 train_time:59993ms step_avg:48.07ms
step:1249/1900 train_time:60081ms step_avg:48.10ms
step:1250/1900 train_time:60169ms step_avg:48.14ms
step:1250/1900 val_loss:3.5467 train_time:60260ms step_avg:48.21ms
step:1251/1900 train_time:60280ms step_avg:48.19ms
step:1252/1900 train_time:60349ms step_avg:48.20ms
step:1253/1900 train_time:60443ms step_avg:48.24ms
step:1254/1900 train_time:60534ms step_avg:48.27ms
step:1255/1900 train_time:60621ms step_avg:48.30ms
step:1256/1900 train_time:60707ms step_avg:48.33ms
step:1257/1900 train_time:60795ms step_avg:48.37ms
step:1258/1900 train_time:60881ms step_avg:48.40ms
step:1259/1900 train_time:60969ms step_avg:48.43ms
step:1260/1900 train_time:61057ms step_avg:48.46ms
step:1261/1900 train_time:61145ms step_avg:48.49ms
step:1262/1900 train_time:61236ms step_avg:48.52ms
step:1263/1900 train_time:61327ms step_avg:48.56ms
step:1264/1900 train_time:61417ms step_avg:48.59ms
step:1265/1900 train_time:61506ms step_avg:48.62ms
step:1266/1900 train_time:61594ms step_avg:48.65ms
step:1267/1900 train_time:61681ms step_avg:48.68ms
step:1268/1900 train_time:61768ms step_avg:48.71ms
step:1269/1900 train_time:61855ms step_avg:48.74ms
step:1270/1900 train_time:61942ms step_avg:48.77ms
step:1271/1900 train_time:62029ms step_avg:48.80ms
step:1272/1900 train_time:62117ms step_avg:48.83ms
step:1273/1900 train_time:62207ms step_avg:48.87ms
step:1274/1900 train_time:62295ms step_avg:48.90ms
step:1275/1900 train_time:62387ms step_avg:48.93ms
step:1276/1900 train_time:62477ms step_avg:48.96ms
step:1277/1900 train_time:62567ms step_avg:49.00ms
step:1278/1900 train_time:62654ms step_avg:49.03ms
step:1279/1900 train_time:62742ms step_avg:49.06ms
step:1280/1900 train_time:62828ms step_avg:49.08ms
step:1281/1900 train_time:62916ms step_avg:49.11ms
step:1282/1900 train_time:63003ms step_avg:49.14ms
step:1283/1900 train_time:63090ms step_avg:49.17ms
step:1284/1900 train_time:63179ms step_avg:49.20ms
step:1285/1900 train_time:63268ms step_avg:49.24ms
step:1286/1900 train_time:63357ms step_avg:49.27ms
step:1287/1900 train_time:63446ms step_avg:49.30ms
step:1288/1900 train_time:63534ms step_avg:49.33ms
step:1289/1900 train_time:63623ms step_avg:49.36ms
step:1290/1900 train_time:63710ms step_avg:49.39ms
step:1291/1900 train_time:63798ms step_avg:49.42ms
step:1292/1900 train_time:63886ms step_avg:49.45ms
step:1293/1900 train_time:63974ms step_avg:49.48ms
step:1294/1900 train_time:64061ms step_avg:49.51ms
step:1295/1900 train_time:64149ms step_avg:49.54ms
step:1296/1900 train_time:64237ms step_avg:49.57ms
step:1297/1900 train_time:64326ms step_avg:49.60ms
step:1298/1900 train_time:64415ms step_avg:49.63ms
step:1299/1900 train_time:64504ms step_avg:49.66ms
step:1300/1900 train_time:64592ms step_avg:49.69ms
step:1301/1900 train_time:64681ms step_avg:49.72ms
step:1302/1900 train_time:64767ms step_avg:49.74ms
step:1303/1900 train_time:64856ms step_avg:49.77ms
step:1304/1900 train_time:64943ms step_avg:49.80ms
step:1305/1900 train_time:65031ms step_avg:49.83ms
step:1306/1900 train_time:65118ms step_avg:49.86ms
step:1307/1900 train_time:65207ms step_avg:49.89ms
step:1308/1900 train_time:65296ms step_avg:49.92ms
step:1309/1900 train_time:65386ms step_avg:49.95ms
step:1310/1900 train_time:65474ms step_avg:49.98ms
step:1311/1900 train_time:65563ms step_avg:50.01ms
step:1312/1900 train_time:65651ms step_avg:50.04ms
step:1313/1900 train_time:65740ms step_avg:50.07ms
step:1314/1900 train_time:65827ms step_avg:50.10ms
step:1315/1900 train_time:65916ms step_avg:50.13ms
step:1316/1900 train_time:66003ms step_avg:50.15ms
step:1317/1900 train_time:66091ms step_avg:50.18ms
step:1318/1900 train_time:66179ms step_avg:50.21ms
step:1319/1900 train_time:66268ms step_avg:50.24ms
step:1320/1900 train_time:66356ms step_avg:50.27ms
step:1321/1900 train_time:66446ms step_avg:50.30ms
step:1322/1900 train_time:66533ms step_avg:50.33ms
step:1323/1900 train_time:66622ms step_avg:50.36ms
step:1324/1900 train_time:66710ms step_avg:50.39ms
step:1325/1900 train_time:66799ms step_avg:50.41ms
step:1326/1900 train_time:66887ms step_avg:50.44ms
step:1327/1900 train_time:66976ms step_avg:50.47ms
step:1328/1900 train_time:67063ms step_avg:50.50ms
step:1329/1900 train_time:67151ms step_avg:50.53ms
step:1330/1900 train_time:67240ms step_avg:50.56ms
step:1331/1900 train_time:67329ms step_avg:50.58ms
step:1332/1900 train_time:67417ms step_avg:50.61ms
step:1333/1900 train_time:67506ms step_avg:50.64ms
step:1334/1900 train_time:67594ms step_avg:50.67ms
step:1335/1900 train_time:67683ms step_avg:50.70ms
step:1336/1900 train_time:67770ms step_avg:50.73ms
step:1337/1900 train_time:67858ms step_avg:50.75ms
step:1338/1900 train_time:67945ms step_avg:50.78ms
step:1339/1900 train_time:68034ms step_avg:50.81ms
step:1340/1900 train_time:68122ms step_avg:50.84ms
step:1341/1900 train_time:68210ms step_avg:50.87ms
step:1342/1900 train_time:68298ms step_avg:50.89ms
step:1343/1900 train_time:68388ms step_avg:50.92ms
step:1344/1900 train_time:68475ms step_avg:50.95ms
step:1345/1900 train_time:68565ms step_avg:50.98ms
step:1346/1900 train_time:68652ms step_avg:51.00ms
step:1347/1900 train_time:68741ms step_avg:51.03ms
step:1348/1900 train_time:68829ms step_avg:51.06ms
step:1349/1900 train_time:68917ms step_avg:51.09ms
step:1350/1900 train_time:69004ms step_avg:51.11ms
step:1351/1900 train_time:69092ms step_avg:51.14ms
step:1352/1900 train_time:69180ms step_avg:51.17ms
step:1353/1900 train_time:69268ms step_avg:51.20ms
step:1354/1900 train_time:69356ms step_avg:51.22ms
step:1355/1900 train_time:69445ms step_avg:51.25ms
step:1356/1900 train_time:69533ms step_avg:51.28ms
step:1357/1900 train_time:69621ms step_avg:51.31ms
step:1358/1900 train_time:69709ms step_avg:51.33ms
step:1359/1900 train_time:69798ms step_avg:51.36ms
step:1360/1900 train_time:69885ms step_avg:51.39ms
step:1361/1900 train_time:69974ms step_avg:51.41ms
step:1362/1900 train_time:70061ms step_avg:51.44ms
step:1363/1900 train_time:70150ms step_avg:51.47ms
step:1364/1900 train_time:70238ms step_avg:51.49ms
step:1365/1900 train_time:70327ms step_avg:51.52ms
step:1366/1900 train_time:70415ms step_avg:51.55ms
step:1367/1900 train_time:70503ms step_avg:51.57ms
step:1368/1900 train_time:70590ms step_avg:51.60ms
step:1369/1900 train_time:70679ms step_avg:51.63ms
step:1370/1900 train_time:70766ms step_avg:51.65ms
step:1371/1900 train_time:70855ms step_avg:51.68ms
step:1372/1900 train_time:70942ms step_avg:51.71ms
step:1373/1900 train_time:71031ms step_avg:51.73ms
step:1374/1900 train_time:71118ms step_avg:51.76ms
step:1375/1900 train_time:71207ms step_avg:51.79ms
step:1376/1900 train_time:71295ms step_avg:51.81ms
step:1377/1900 train_time:71384ms step_avg:51.84ms
step:1378/1900 train_time:71471ms step_avg:51.87ms
step:1379/1900 train_time:71560ms step_avg:51.89ms
step:1380/1900 train_time:71648ms step_avg:51.92ms
step:1381/1900 train_time:71736ms step_avg:51.94ms
step:1382/1900 train_time:71824ms step_avg:51.97ms
step:1383/1900 train_time:71912ms step_avg:52.00ms
step:1384/1900 train_time:71999ms step_avg:52.02ms
step:1385/1900 train_time:72088ms step_avg:52.05ms
step:1386/1900 train_time:72176ms step_avg:52.08ms
step:1387/1900 train_time:72266ms step_avg:52.10ms
step:1388/1900 train_time:72353ms step_avg:52.13ms
step:1389/1900 train_time:72442ms step_avg:52.15ms
step:1390/1900 train_time:72529ms step_avg:52.18ms
step:1391/1900 train_time:72618ms step_avg:52.21ms
step:1392/1900 train_time:72706ms step_avg:52.23ms
step:1393/1900 train_time:72794ms step_avg:52.26ms
step:1394/1900 train_time:72882ms step_avg:52.28ms
step:1395/1900 train_time:72970ms step_avg:52.31ms
step:1396/1900 train_time:73058ms step_avg:52.33ms
step:1397/1900 train_time:73146ms step_avg:52.36ms
step:1398/1900 train_time:73234ms step_avg:52.38ms
step:1399/1900 train_time:73323ms step_avg:52.41ms
step:1400/1900 train_time:73410ms step_avg:52.44ms
step:1401/1900 train_time:73499ms step_avg:52.46ms
step:1402/1900 train_time:73587ms step_avg:52.49ms
step:1403/1900 train_time:73677ms step_avg:52.51ms
step:1404/1900 train_time:73763ms step_avg:52.54ms
step:1405/1900 train_time:73852ms step_avg:52.56ms
step:1406/1900 train_time:73939ms step_avg:52.59ms
step:1407/1900 train_time:74028ms step_avg:52.61ms
step:1408/1900 train_time:74115ms step_avg:52.64ms
step:1409/1900 train_time:74204ms step_avg:52.66ms
step:1410/1900 train_time:74291ms step_avg:52.69ms
step:1411/1900 train_time:74380ms step_avg:52.71ms
step:1412/1900 train_time:74468ms step_avg:52.74ms
step:1413/1900 train_time:74556ms step_avg:52.76ms
step:1414/1900 train_time:74643ms step_avg:52.79ms
step:1415/1900 train_time:74732ms step_avg:52.81ms
step:1416/1900 train_time:74819ms step_avg:52.84ms
step:1417/1900 train_time:74908ms step_avg:52.86ms
step:1418/1900 train_time:74996ms step_avg:52.89ms
step:1419/1900 train_time:75085ms step_avg:52.91ms
step:1420/1900 train_time:75172ms step_avg:52.94ms
step:1421/1900 train_time:75261ms step_avg:52.96ms
step:1422/1900 train_time:75349ms step_avg:52.99ms
step:1423/1900 train_time:75437ms step_avg:53.01ms
step:1424/1900 train_time:75525ms step_avg:53.04ms
step:1425/1900 train_time:75613ms step_avg:53.06ms
step:1426/1900 train_time:75701ms step_avg:53.09ms
step:1427/1900 train_time:75789ms step_avg:53.11ms
step:1428/1900 train_time:75877ms step_avg:53.14ms
step:1429/1900 train_time:75966ms step_avg:53.16ms
step:1430/1900 train_time:76054ms step_avg:53.18ms
step:1431/1900 train_time:76142ms step_avg:53.21ms
step:1432/1900 train_time:76230ms step_avg:53.23ms
step:1433/1900 train_time:76319ms step_avg:53.26ms
step:1434/1900 train_time:76407ms step_avg:53.28ms
step:1435/1900 train_time:76495ms step_avg:53.31ms
step:1436/1900 train_time:76582ms step_avg:53.33ms
step:1437/1900 train_time:76670ms step_avg:53.35ms
step:1438/1900 train_time:76757ms step_avg:53.38ms
step:1439/1900 train_time:76846ms step_avg:53.40ms
step:1440/1900 train_time:76933ms step_avg:53.43ms
step:1441/1900 train_time:77021ms step_avg:53.45ms
step:1442/1900 train_time:77109ms step_avg:53.47ms
step:1443/1900 train_time:77198ms step_avg:53.50ms
step:1444/1900 train_time:77286ms step_avg:53.52ms
step:1445/1900 train_time:77375ms step_avg:53.55ms
step:1446/1900 train_time:77462ms step_avg:53.57ms
step:1447/1900 train_time:77551ms step_avg:53.59ms
step:1448/1900 train_time:77638ms step_avg:53.62ms
step:1449/1900 train_time:77727ms step_avg:53.64ms
step:1450/1900 train_time:77815ms step_avg:53.67ms
step:1451/1900 train_time:77903ms step_avg:53.69ms
step:1452/1900 train_time:77991ms step_avg:53.71ms
step:1453/1900 train_time:78079ms step_avg:53.74ms
step:1454/1900 train_time:78167ms step_avg:53.76ms
step:1455/1900 train_time:78255ms step_avg:53.78ms
step:1456/1900 train_time:78343ms step_avg:53.81ms
step:1457/1900 train_time:78430ms step_avg:53.83ms
step:1458/1900 train_time:78518ms step_avg:53.85ms
step:1459/1900 train_time:78607ms step_avg:53.88ms
step:1460/1900 train_time:78694ms step_avg:53.90ms
step:1461/1900 train_time:78782ms step_avg:53.92ms
step:1462/1900 train_time:78869ms step_avg:53.95ms
step:1463/1900 train_time:78958ms step_avg:53.97ms
step:1464/1900 train_time:79045ms step_avg:53.99ms
step:1465/1900 train_time:79133ms step_avg:54.02ms
step:1466/1900 train_time:79221ms step_avg:54.04ms
step:1467/1900 train_time:79310ms step_avg:54.06ms
step:1468/1900 train_time:79398ms step_avg:54.09ms
step:1469/1900 train_time:79486ms step_avg:54.11ms
step:1470/1900 train_time:79574ms step_avg:54.13ms
step:1471/1900 train_time:79662ms step_avg:54.16ms
step:1472/1900 train_time:79750ms step_avg:54.18ms
step:1473/1900 train_time:79839ms step_avg:54.20ms
step:1474/1900 train_time:79926ms step_avg:54.22ms
step:1475/1900 train_time:80015ms step_avg:54.25ms
step:1476/1900 train_time:80102ms step_avg:54.27ms
step:1477/1900 train_time:80190ms step_avg:54.29ms
step:1478/1900 train_time:80278ms step_avg:54.32ms
step:1479/1900 train_time:80367ms step_avg:54.34ms
step:1480/1900 train_time:80454ms step_avg:54.36ms
step:1481/1900 train_time:80543ms step_avg:54.38ms
step:1482/1900 train_time:80630ms step_avg:54.41ms
step:1483/1900 train_time:80719ms step_avg:54.43ms
step:1484/1900 train_time:80807ms step_avg:54.45ms
step:1485/1900 train_time:80895ms step_avg:54.47ms
step:1486/1900 train_time:80983ms step_avg:54.50ms
step:1487/1900 train_time:81072ms step_avg:54.52ms
step:1488/1900 train_time:81159ms step_avg:54.54ms
step:1489/1900 train_time:81247ms step_avg:54.57ms
step:1490/1900 train_time:81336ms step_avg:54.59ms
step:1491/1900 train_time:81425ms step_avg:54.61ms
step:1492/1900 train_time:81512ms step_avg:54.63ms
step:1493/1900 train_time:81600ms step_avg:54.66ms
step:1494/1900 train_time:81687ms step_avg:54.68ms
step:1495/1900 train_time:81776ms step_avg:54.70ms
step:1496/1900 train_time:81863ms step_avg:54.72ms
step:1497/1900 train_time:81952ms step_avg:54.74ms
step:1498/1900 train_time:82040ms step_avg:54.77ms
step:1499/1900 train_time:82128ms step_avg:54.79ms
step:1500/1900 train_time:82217ms step_avg:54.81ms
step:1500/1900 val_loss:3.4128 train_time:82307ms step_avg:54.87ms
step:1501/1900 train_time:82327ms step_avg:54.85ms
step:1502/1900 train_time:82397ms step_avg:54.86ms
step:1503/1900 train_time:82489ms step_avg:54.88ms
step:1504/1900 train_time:82577ms step_avg:54.91ms
step:1505/1900 train_time:82665ms step_avg:54.93ms
step:1506/1900 train_time:82752ms step_avg:54.95ms
step:1507/1900 train_time:82839ms step_avg:54.97ms
step:1508/1900 train_time:82925ms step_avg:54.99ms
step:1509/1900 train_time:83013ms step_avg:55.01ms
step:1510/1900 train_time:83099ms step_avg:55.03ms
step:1511/1900 train_time:83187ms step_avg:55.05ms
step:1512/1900 train_time:83276ms step_avg:55.08ms
step:1513/1900 train_time:83368ms step_avg:55.10ms
step:1514/1900 train_time:83458ms step_avg:55.12ms
step:1515/1900 train_time:83547ms step_avg:55.15ms
step:1516/1900 train_time:83635ms step_avg:55.17ms
step:1517/1900 train_time:83723ms step_avg:55.19ms
step:1518/1900 train_time:83810ms step_avg:55.21ms
step:1519/1900 train_time:83897ms step_avg:55.23ms
step:1520/1900 train_time:83984ms step_avg:55.25ms
step:1521/1900 train_time:84071ms step_avg:55.27ms
step:1522/1900 train_time:84158ms step_avg:55.29ms
step:1523/1900 train_time:84246ms step_avg:55.32ms
step:1524/1900 train_time:84335ms step_avg:55.34ms
step:1525/1900 train_time:84425ms step_avg:55.36ms
step:1526/1900 train_time:84514ms step_avg:55.38ms
step:1527/1900 train_time:84604ms step_avg:55.41ms
step:1528/1900 train_time:84692ms step_avg:55.43ms
step:1529/1900 train_time:84780ms step_avg:55.45ms
step:1530/1900 train_time:84867ms step_avg:55.47ms
step:1531/1900 train_time:84955ms step_avg:55.49ms
step:1532/1900 train_time:85041ms step_avg:55.51ms
step:1533/1900 train_time:85129ms step_avg:55.53ms
step:1534/1900 train_time:85217ms step_avg:55.55ms
step:1535/1900 train_time:85306ms step_avg:55.57ms
step:1536/1900 train_time:85395ms step_avg:55.60ms
step:1537/1900 train_time:85484ms step_avg:55.62ms
step:1538/1900 train_time:85572ms step_avg:55.64ms
step:1539/1900 train_time:85660ms step_avg:55.66ms
step:1540/1900 train_time:85748ms step_avg:55.68ms
step:1541/1900 train_time:85837ms step_avg:55.70ms
step:1542/1900 train_time:85924ms step_avg:55.72ms
step:1543/1900 train_time:86012ms step_avg:55.74ms
step:1544/1900 train_time:86099ms step_avg:55.76ms
step:1545/1900 train_time:86186ms step_avg:55.78ms
step:1546/1900 train_time:86274ms step_avg:55.80ms
step:1547/1900 train_time:86364ms step_avg:55.83ms
step:1548/1900 train_time:86452ms step_avg:55.85ms
step:1549/1900 train_time:86541ms step_avg:55.87ms
step:1550/1900 train_time:86628ms step_avg:55.89ms
step:1551/1900 train_time:86717ms step_avg:55.91ms
step:1552/1900 train_time:86804ms step_avg:55.93ms
step:1553/1900 train_time:86892ms step_avg:55.95ms
step:1554/1900 train_time:86979ms step_avg:55.97ms
step:1555/1900 train_time:87067ms step_avg:55.99ms
step:1556/1900 train_time:87154ms step_avg:56.01ms
step:1557/1900 train_time:87243ms step_avg:56.03ms
step:1558/1900 train_time:87331ms step_avg:56.05ms
step:1559/1900 train_time:87420ms step_avg:56.07ms
step:1560/1900 train_time:87508ms step_avg:56.09ms
step:1561/1900 train_time:87596ms step_avg:56.12ms
step:1562/1900 train_time:87684ms step_avg:56.14ms
step:1563/1900 train_time:87772ms step_avg:56.16ms
step:1564/1900 train_time:87860ms step_avg:56.18ms
step:1565/1900 train_time:87948ms step_avg:56.20ms
step:1566/1900 train_time:88035ms step_avg:56.22ms
step:1567/1900 train_time:88123ms step_avg:56.24ms
step:1568/1900 train_time:88210ms step_avg:56.26ms
step:1569/1900 train_time:88298ms step_avg:56.28ms
step:1570/1900 train_time:88386ms step_avg:56.30ms
step:1571/1900 train_time:88474ms step_avg:56.32ms
step:1572/1900 train_time:88563ms step_avg:56.34ms
step:1573/1900 train_time:88651ms step_avg:56.36ms
step:1574/1900 train_time:88739ms step_avg:56.38ms
step:1575/1900 train_time:88827ms step_avg:56.40ms
step:1576/1900 train_time:88915ms step_avg:56.42ms
step:1577/1900 train_time:89003ms step_avg:56.44ms
step:1578/1900 train_time:89090ms step_avg:56.46ms
step:1579/1900 train_time:89179ms step_avg:56.48ms
step:1580/1900 train_time:89267ms step_avg:56.50ms
step:1581/1900 train_time:89355ms step_avg:56.52ms
step:1582/1900 train_time:89443ms step_avg:56.54ms
step:1583/1900 train_time:89531ms step_avg:56.56ms
step:1584/1900 train_time:89620ms step_avg:56.58ms
step:1585/1900 train_time:89708ms step_avg:56.60ms
step:1586/1900 train_time:89796ms step_avg:56.62ms
step:1587/1900 train_time:89884ms step_avg:56.64ms
step:1588/1900 train_time:89972ms step_avg:56.66ms
step:1589/1900 train_time:90061ms step_avg:56.68ms
step:1590/1900 train_time:90149ms step_avg:56.70ms
step:1591/1900 train_time:90237ms step_avg:56.72ms
step:1592/1900 train_time:90325ms step_avg:56.74ms
step:1593/1900 train_time:90413ms step_avg:56.76ms
step:1594/1900 train_time:90501ms step_avg:56.78ms
step:1595/1900 train_time:90589ms step_avg:56.80ms
step:1596/1900 train_time:90676ms step_avg:56.81ms
step:1597/1900 train_time:90765ms step_avg:56.83ms
step:1598/1900 train_time:90853ms step_avg:56.85ms
step:1599/1900 train_time:90942ms step_avg:56.87ms
step:1600/1900 train_time:91030ms step_avg:56.89ms
step:1601/1900 train_time:91118ms step_avg:56.91ms
step:1602/1900 train_time:91205ms step_avg:56.93ms
step:1603/1900 train_time:91294ms step_avg:56.95ms
step:1604/1900 train_time:91381ms step_avg:56.97ms
step:1605/1900 train_time:91470ms step_avg:56.99ms
step:1606/1900 train_time:91557ms step_avg:57.01ms
step:1607/1900 train_time:91646ms step_avg:57.03ms
step:1608/1900 train_time:91733ms step_avg:57.05ms
step:1609/1900 train_time:91823ms step_avg:57.07ms
step:1610/1900 train_time:91910ms step_avg:57.09ms
step:1611/1900 train_time:92000ms step_avg:57.11ms
step:1612/1900 train_time:92087ms step_avg:57.13ms
step:1613/1900 train_time:92176ms step_avg:57.15ms
step:1614/1900 train_time:92263ms step_avg:57.16ms
step:1615/1900 train_time:92351ms step_avg:57.18ms
step:1616/1900 train_time:92439ms step_avg:57.20ms
step:1617/1900 train_time:92527ms step_avg:57.22ms
step:1618/1900 train_time:92615ms step_avg:57.24ms
step:1619/1900 train_time:92703ms step_avg:57.26ms
step:1620/1900 train_time:92790ms step_avg:57.28ms
step:1621/1900 train_time:92879ms step_avg:57.30ms
step:1622/1900 train_time:92968ms step_avg:57.32ms
step:1623/1900 train_time:93057ms step_avg:57.34ms
step:1624/1900 train_time:93145ms step_avg:57.36ms
step:1625/1900 train_time:93234ms step_avg:57.37ms
step:1626/1900 train_time:93321ms step_avg:57.39ms
step:1627/1900 train_time:93409ms step_avg:57.41ms
step:1628/1900 train_time:93497ms step_avg:57.43ms
step:1629/1900 train_time:93585ms step_avg:57.45ms
step:1630/1900 train_time:93673ms step_avg:57.47ms
step:1631/1900 train_time:93762ms step_avg:57.49ms
step:1632/1900 train_time:93850ms step_avg:57.51ms
step:1633/1900 train_time:93939ms step_avg:57.53ms
step:1634/1900 train_time:94026ms step_avg:57.54ms
step:1635/1900 train_time:94114ms step_avg:57.56ms
step:1636/1900 train_time:94202ms step_avg:57.58ms
step:1637/1900 train_time:94291ms step_avg:57.60ms
step:1638/1900 train_time:94378ms step_avg:57.62ms
step:1639/1900 train_time:94466ms step_avg:57.64ms
step:1640/1900 train_time:94553ms step_avg:57.65ms
step:1641/1900 train_time:94641ms step_avg:57.67ms
step:1642/1900 train_time:94729ms step_avg:57.69ms
step:1643/1900 train_time:94817ms step_avg:57.71ms
step:1644/1900 train_time:94905ms step_avg:57.73ms
step:1645/1900 train_time:94994ms step_avg:57.75ms
step:1646/1900 train_time:95082ms step_avg:57.77ms
step:1647/1900 train_time:95170ms step_avg:57.78ms
step:1648/1900 train_time:95257ms step_avg:57.80ms
step:1649/1900 train_time:95345ms step_avg:57.82ms
step:1650/1900 train_time:95433ms step_avg:57.84ms
step:1651/1900 train_time:95521ms step_avg:57.86ms
step:1652/1900 train_time:95609ms step_avg:57.87ms
step:1653/1900 train_time:95698ms step_avg:57.89ms
step:1654/1900 train_time:95785ms step_avg:57.91ms
step:1655/1900 train_time:95873ms step_avg:57.93ms
step:1656/1900 train_time:95962ms step_avg:57.95ms
step:1657/1900 train_time:96050ms step_avg:57.97ms
step:1658/1900 train_time:96138ms step_avg:57.98ms
step:1659/1900 train_time:96227ms step_avg:58.00ms
step:1660/1900 train_time:96314ms step_avg:58.02ms
step:1661/1900 train_time:96403ms step_avg:58.04ms
step:1662/1900 train_time:96490ms step_avg:58.06ms
step:1663/1900 train_time:96578ms step_avg:58.07ms
step:1664/1900 train_time:96666ms step_avg:58.09ms
step:1665/1900 train_time:96754ms step_avg:58.11ms
step:1666/1900 train_time:96842ms step_avg:58.13ms
step:1667/1900 train_time:96930ms step_avg:58.15ms
step:1668/1900 train_time:97019ms step_avg:58.16ms
step:1669/1900 train_time:97107ms step_avg:58.18ms
step:1670/1900 train_time:97195ms step_avg:58.20ms
step:1671/1900 train_time:97285ms step_avg:58.22ms
step:1672/1900 train_time:97373ms step_avg:58.24ms
step:1673/1900 train_time:97461ms step_avg:58.26ms
step:1674/1900 train_time:97548ms step_avg:58.27ms
step:1675/1900 train_time:97637ms step_avg:58.29ms
step:1676/1900 train_time:97725ms step_avg:58.31ms
step:1677/1900 train_time:97814ms step_avg:58.33ms
step:1678/1900 train_time:97901ms step_avg:58.34ms
step:1679/1900 train_time:97989ms step_avg:58.36ms
step:1680/1900 train_time:98077ms step_avg:58.38ms
step:1681/1900 train_time:98165ms step_avg:58.40ms
step:1682/1900 train_time:98254ms step_avg:58.41ms
step:1683/1900 train_time:98343ms step_avg:58.43ms
step:1684/1900 train_time:98431ms step_avg:58.45ms
step:1685/1900 train_time:98519ms step_avg:58.47ms
step:1686/1900 train_time:98607ms step_avg:58.49ms
step:1687/1900 train_time:98696ms step_avg:58.50ms
step:1688/1900 train_time:98783ms step_avg:58.52ms
step:1689/1900 train_time:98872ms step_avg:58.54ms
step:1690/1900 train_time:98959ms step_avg:58.56ms
step:1691/1900 train_time:99047ms step_avg:58.57ms
step:1692/1900 train_time:99134ms step_avg:58.59ms
step:1693/1900 train_time:99223ms step_avg:58.61ms
step:1694/1900 train_time:99311ms step_avg:58.63ms
step:1695/1900 train_time:99400ms step_avg:58.64ms
step:1696/1900 train_time:99487ms step_avg:58.66ms
step:1697/1900 train_time:99576ms step_avg:58.68ms
step:1698/1900 train_time:99664ms step_avg:58.69ms
step:1699/1900 train_time:99752ms step_avg:58.71ms
step:1700/1900 train_time:99839ms step_avg:58.73ms
step:1701/1900 train_time:99927ms step_avg:58.75ms
step:1702/1900 train_time:100015ms step_avg:58.76ms
step:1703/1900 train_time:100103ms step_avg:58.78ms
step:1704/1900 train_time:100191ms step_avg:58.80ms
step:1705/1900 train_time:100279ms step_avg:58.81ms
step:1706/1900 train_time:100367ms step_avg:58.83ms
step:1707/1900 train_time:100456ms step_avg:58.85ms
step:1708/1900 train_time:100543ms step_avg:58.87ms
step:1709/1900 train_time:100631ms step_avg:58.88ms
step:1710/1900 train_time:100719ms step_avg:58.90ms
step:1711/1900 train_time:100807ms step_avg:58.92ms
step:1712/1900 train_time:100895ms step_avg:58.93ms
step:1713/1900 train_time:100983ms step_avg:58.95ms
step:1714/1900 train_time:101071ms step_avg:58.97ms
step:1715/1900 train_time:101160ms step_avg:58.99ms
step:1716/1900 train_time:101248ms step_avg:59.00ms
step:1717/1900 train_time:101336ms step_avg:59.02ms
step:1718/1900 train_time:101424ms step_avg:59.04ms
step:1719/1900 train_time:101512ms step_avg:59.05ms
step:1720/1900 train_time:101599ms step_avg:59.07ms
step:1721/1900 train_time:101688ms step_avg:59.09ms
step:1722/1900 train_time:101775ms step_avg:59.10ms
step:1723/1900 train_time:101864ms step_avg:59.12ms
step:1724/1900 train_time:101952ms step_avg:59.14ms
step:1725/1900 train_time:102040ms step_avg:59.15ms
step:1726/1900 train_time:102127ms step_avg:59.17ms
step:1727/1900 train_time:102216ms step_avg:59.19ms
step:1728/1900 train_time:102303ms step_avg:59.20ms
step:1729/1900 train_time:102391ms step_avg:59.22ms
step:1730/1900 train_time:102478ms step_avg:59.24ms
step:1731/1900 train_time:102566ms step_avg:59.25ms
step:1732/1900 train_time:102655ms step_avg:59.27ms
step:1733/1900 train_time:102743ms step_avg:59.29ms
step:1734/1900 train_time:102831ms step_avg:59.30ms
step:1735/1900 train_time:102920ms step_avg:59.32ms
step:1736/1900 train_time:103007ms step_avg:59.34ms
step:1737/1900 train_time:103096ms step_avg:59.35ms
step:1738/1900 train_time:103184ms step_avg:59.37ms
step:1739/1900 train_time:103272ms step_avg:59.39ms
step:1740/1900 train_time:103360ms step_avg:59.40ms
step:1741/1900 train_time:103448ms step_avg:59.42ms
step:1742/1900 train_time:103536ms step_avg:59.44ms
step:1743/1900 train_time:103625ms step_avg:59.45ms
step:1744/1900 train_time:103713ms step_avg:59.47ms
step:1745/1900 train_time:103801ms step_avg:59.48ms
step:1746/1900 train_time:103888ms step_avg:59.50ms
step:1747/1900 train_time:103976ms step_avg:59.52ms
step:1748/1900 train_time:104063ms step_avg:59.53ms
step:1749/1900 train_time:104152ms step_avg:59.55ms
step:1750/1900 train_time:104239ms step_avg:59.57ms
step:1750/1900 val_loss:3.3172 train_time:104330ms step_avg:59.62ms
step:1751/1900 train_time:104349ms step_avg:59.59ms
step:1752/1900 train_time:104419ms step_avg:59.60ms
step:1753/1900 train_time:104511ms step_avg:59.62ms
step:1754/1900 train_time:104600ms step_avg:59.64ms
step:1755/1900 train_time:104688ms step_avg:59.65ms
step:1756/1900 train_time:104774ms step_avg:59.67ms
step:1757/1900 train_time:104861ms step_avg:59.68ms
step:1758/1900 train_time:104948ms step_avg:59.70ms
step:1759/1900 train_time:105035ms step_avg:59.71ms
step:1760/1900 train_time:105123ms step_avg:59.73ms
step:1761/1900 train_time:105210ms step_avg:59.74ms
step:1762/1900 train_time:105299ms step_avg:59.76ms
step:1763/1900 train_time:105389ms step_avg:59.78ms
step:1764/1900 train_time:105479ms step_avg:59.80ms
step:1765/1900 train_time:105568ms step_avg:59.81ms
step:1766/1900 train_time:105656ms step_avg:59.83ms
step:1767/1900 train_time:105743ms step_avg:59.84ms
step:1768/1900 train_time:105830ms step_avg:59.86ms
step:1769/1900 train_time:105919ms step_avg:59.87ms
step:1770/1900 train_time:106005ms step_avg:59.89ms
step:1771/1900 train_time:106093ms step_avg:59.91ms
step:1772/1900 train_time:106180ms step_avg:59.92ms
step:1773/1900 train_time:106269ms step_avg:59.94ms
step:1774/1900 train_time:106359ms step_avg:59.95ms
step:1775/1900 train_time:106449ms step_avg:59.97ms
step:1776/1900 train_time:106537ms step_avg:59.99ms
step:1777/1900 train_time:106626ms step_avg:60.00ms
step:1778/1900 train_time:106713ms step_avg:60.02ms
step:1779/1900 train_time:106801ms step_avg:60.03ms
step:1780/1900 train_time:106887ms step_avg:60.05ms
step:1781/1900 train_time:106976ms step_avg:60.06ms
step:1782/1900 train_time:107063ms step_avg:60.08ms
step:1783/1900 train_time:107152ms step_avg:60.10ms
step:1784/1900 train_time:107239ms step_avg:60.11ms
step:1785/1900 train_time:107328ms step_avg:60.13ms
step:1786/1900 train_time:107416ms step_avg:60.14ms
step:1787/1900 train_time:107506ms step_avg:60.16ms
step:1788/1900 train_time:107594ms step_avg:60.18ms
step:1789/1900 train_time:107682ms step_avg:60.19ms
step:1790/1900 train_time:107769ms step_avg:60.21ms
step:1791/1900 train_time:107858ms step_avg:60.22ms
step:1792/1900 train_time:107946ms step_avg:60.24ms
step:1793/1900 train_time:108034ms step_avg:60.25ms
step:1794/1900 train_time:108121ms step_avg:60.27ms
step:1795/1900 train_time:108210ms step_avg:60.28ms
step:1796/1900 train_time:108298ms step_avg:60.30ms
step:1797/1900 train_time:108387ms step_avg:60.32ms
step:1798/1900 train_time:108476ms step_avg:60.33ms
step:1799/1900 train_time:108565ms step_avg:60.35ms
step:1800/1900 train_time:108652ms step_avg:60.36ms
step:1801/1900 train_time:108740ms step_avg:60.38ms
step:1802/1900 train_time:108827ms step_avg:60.39ms
step:1803/1900 train_time:108916ms step_avg:60.41ms
step:1804/1900 train_time:109002ms step_avg:60.42ms
step:1805/1900 train_time:109091ms step_avg:60.44ms
step:1806/1900 train_time:109179ms step_avg:60.45ms
step:1807/1900 train_time:109267ms step_avg:60.47ms
step:1808/1900 train_time:109356ms step_avg:60.48ms
step:1809/1900 train_time:109445ms step_avg:60.50ms
step:1810/1900 train_time:109533ms step_avg:60.52ms
step:1811/1900 train_time:109621ms step_avg:60.53ms
step:1812/1900 train_time:109709ms step_avg:60.55ms
step:1813/1900 train_time:109798ms step_avg:60.56ms
step:1814/1900 train_time:109886ms step_avg:60.58ms
step:1815/1900 train_time:109973ms step_avg:60.59ms
step:1816/1900 train_time:110061ms step_avg:60.61ms
step:1817/1900 train_time:110149ms step_avg:60.62ms
step:1818/1900 train_time:110236ms step_avg:60.64ms
step:1819/1900 train_time:110325ms step_avg:60.65ms
step:1820/1900 train_time:110413ms step_avg:60.67ms
step:1821/1900 train_time:110501ms step_avg:60.68ms
step:1822/1900 train_time:110590ms step_avg:60.70ms
step:1823/1900 train_time:110679ms step_avg:60.71ms
step:1824/1900 train_time:110766ms step_avg:60.73ms
step:1825/1900 train_time:110855ms step_avg:60.74ms
step:1826/1900 train_time:110942ms step_avg:60.76ms
step:1827/1900 train_time:111031ms step_avg:60.77ms
step:1828/1900 train_time:111118ms step_avg:60.79ms
step:1829/1900 train_time:111207ms step_avg:60.80ms
step:1830/1900 train_time:111294ms step_avg:60.82ms
step:1831/1900 train_time:111382ms step_avg:60.83ms
step:1832/1900 train_time:111470ms step_avg:60.85ms
step:1833/1900 train_time:111559ms step_avg:60.86ms
step:1834/1900 train_time:111647ms step_avg:60.88ms
step:1835/1900 train_time:111735ms step_avg:60.89ms
step:1836/1900 train_time:111822ms step_avg:60.91ms
step:1837/1900 train_time:111911ms step_avg:60.92ms
step:1838/1900 train_time:111998ms step_avg:60.93ms
step:1839/1900 train_time:112087ms step_avg:60.95ms
step:1840/1900 train_time:112175ms step_avg:60.96ms
step:1841/1900 train_time:112263ms step_avg:60.98ms
step:1842/1900 train_time:112350ms step_avg:60.99ms
step:1843/1900 train_time:112438ms step_avg:61.01ms
step:1844/1900 train_time:112526ms step_avg:61.02ms
step:1845/1900 train_time:112615ms step_avg:61.04ms
step:1846/1900 train_time:112703ms step_avg:61.05ms
step:1847/1900 train_time:112791ms step_avg:61.07ms
step:1848/1900 train_time:112878ms step_avg:61.08ms
step:1849/1900 train_time:112966ms step_avg:61.10ms
step:1850/1900 train_time:113054ms step_avg:61.11ms
step:1851/1900 train_time:113142ms step_avg:61.12ms
step:1852/1900 train_time:113229ms step_avg:61.14ms
step:1853/1900 train_time:113318ms step_avg:61.15ms
step:1854/1900 train_time:113407ms step_avg:61.17ms
step:1855/1900 train_time:113496ms step_avg:61.18ms
step:1856/1900 train_time:113584ms step_avg:61.20ms
step:1857/1900 train_time:113673ms step_avg:61.21ms
step:1858/1900 train_time:113761ms step_avg:61.23ms
step:1859/1900 train_time:113849ms step_avg:61.24ms
step:1860/1900 train_time:113937ms step_avg:61.26ms
step:1861/1900 train_time:114025ms step_avg:61.27ms
step:1862/1900 train_time:114113ms step_avg:61.28ms
step:1863/1900 train_time:114201ms step_avg:61.30ms
step:1864/1900 train_time:114289ms step_avg:61.31ms
step:1865/1900 train_time:114378ms step_avg:61.33ms
step:1866/1900 train_time:114466ms step_avg:61.34ms
step:1867/1900 train_time:114555ms step_avg:61.36ms
step:1868/1900 train_time:114643ms step_avg:61.37ms
step:1869/1900 train_time:114732ms step_avg:61.39ms
step:1870/1900 train_time:114820ms step_avg:61.40ms
step:1871/1900 train_time:114909ms step_avg:61.42ms
step:1872/1900 train_time:114997ms step_avg:61.43ms
step:1873/1900 train_time:115085ms step_avg:61.44ms
step:1874/1900 train_time:115173ms step_avg:61.46ms
step:1875/1900 train_time:115261ms step_avg:61.47ms
step:1876/1900 train_time:115350ms step_avg:61.49ms
step:1877/1900 train_time:115438ms step_avg:61.50ms
step:1878/1900 train_time:115525ms step_avg:61.52ms
step:1879/1900 train_time:115614ms step_avg:61.53ms
step:1880/1900 train_time:115703ms step_avg:61.54ms
step:1881/1900 train_time:115791ms step_avg:61.56ms
step:1882/1900 train_time:115879ms step_avg:61.57ms
step:1883/1900 train_time:115967ms step_avg:61.59ms
step:1884/1900 train_time:116056ms step_avg:61.60ms
step:1885/1900 train_time:116144ms step_avg:61.62ms
step:1886/1900 train_time:116233ms step_avg:61.63ms
step:1887/1900 train_time:116321ms step_avg:61.64ms
step:1888/1900 train_time:116409ms step_avg:61.66ms
step:1889/1900 train_time:116499ms step_avg:61.67ms
step:1890/1900 train_time:116586ms step_avg:61.69ms
step:1891/1900 train_time:116674ms step_avg:61.70ms
step:1892/1900 train_time:116761ms step_avg:61.71ms
step:1893/1900 train_time:116851ms step_avg:61.73ms
step:1894/1900 train_time:116939ms step_avg:61.74ms
step:1895/1900 train_time:117028ms step_avg:61.76ms
step:1896/1900 train_time:117116ms step_avg:61.77ms
step:1897/1900 train_time:117205ms step_avg:61.78ms
step:1898/1900 train_time:117292ms step_avg:61.80ms
step:1899/1900 train_time:117380ms step_avg:61.81ms
step:1900/1900 train_time:117468ms step_avg:61.83ms
step:1900/1900 val_loss:3.2764 train_time:117559ms step_avg:61.87ms
peak memory allocated: 30033 MiB reserved: 44678 MiB

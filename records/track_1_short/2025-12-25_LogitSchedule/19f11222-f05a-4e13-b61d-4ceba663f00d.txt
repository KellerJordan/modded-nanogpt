import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int
    logit_range: torch.Tensor
    logit_slope: torch.Tensor

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, logit_range, logit_slope = schedule_cfg.mtp_weights, schedule_cfg.logit_range, schedule_cfg.logit_slope
        ws_short, ws_long = schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = logit_range * torch.sigmoid(logits / logit_slope)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Model Schedule Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_logit_slope(step: int, start_val=7.5, end_val=15.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

def get_logit_range(step: int, start_val=23.0, end_val=27.0, frac=0.7):
    # linear progression over initial frac
    step_complete = frac * args.num_scheduled_iterations
    if step > step_complete:
        return end_val
    return start_val + (end_val-start_val) * step / step_complete

class ModelScheduleManager():
    """
    Manages model architecture, data, and target that changes during training

    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Updates logit range and slope
        5. Split embed and lm head at 2/3 of training
        6. Batch size schedule of 8 -> 16 -> 24
        7. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        self.reset()

    def reset(self):
        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)
        self.model.yarn.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def update(self, step):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
        # create tensors to work with torch compile
        self.logit_slope = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_slope(0)
        self.logit_range = torch.ones(1, device=device, dtype = torch.bfloat16) * get_logit_range(0)

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long,
            logit_range = self.logit_range,
            logit_slope = self.logit_slope
        )

# -----------------------------------------------------------------------------
# Optimizer Management

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class OptimizerManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.

    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd
    """
    def __init__(self, model):
        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        self.model = model
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

    def _is_active_step(self, opt, step):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def step_optimizers(self, step):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state):
        for opt, opt_state in zip(self.optimizers, state):
            opt.should_sync = False
            opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1860  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
optimizer_manager = OptimizerManager(model)
model_schedule_manager = ModelScheduleManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=optimizer_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = model_schedule_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
for step in warmup_steps:
    model_schedule_manager.update(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer_manager.reset(initial_state["optimizers"])
model_schedule_manager.reset()
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    model_schedule_manager.update(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            model_schedule_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            optimizer_manager.activate_hooks(step)
        send_args = model_schedule_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, model_schedule_manager.get_forward_args()) / grad_accum_steps).backward()
    optimizer_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 25 08:28:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   43C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     54812      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54813      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54814      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54815      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54816      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54817      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54818      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     54819      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     54813      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     54814      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     54815      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     54816      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     54817      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     54818      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     54819      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
step:0/1900 val_loss:10.8307 train_time:0ms step_avg:0.16ms
step:1/1900 train_time:75ms step_avg:75.25ms
step:2/1900 train_time:99ms step_avg:49.60ms
step:3/1900 train_time:123ms step_avg:41.07ms
step:4/1900 train_time:157ms step_avg:39.26ms
step:5/1900 train_time:191ms step_avg:38.16ms
step:6/1900 train_time:277ms step_avg:46.13ms
step:7/1900 train_time:299ms step_avg:42.73ms
step:8/1900 train_time:333ms step_avg:41.64ms
step:9/1900 train_time:367ms step_avg:40.75ms
step:10/1900 train_time:401ms step_avg:40.06ms
step:11/1900 train_time:435ms step_avg:39.51ms
step:12/1900 train_time:469ms step_avg:39.05ms
step:13/1900 train_time:503ms step_avg:38.66ms
step:14/1900 train_time:537ms step_avg:38.33ms
step:15/1900 train_time:571ms step_avg:38.05ms
step:16/1900 train_time:605ms step_avg:37.80ms
step:17/1900 train_time:639ms step_avg:37.57ms
step:18/1900 train_time:673ms step_avg:37.38ms
step:19/1900 train_time:707ms step_avg:37.19ms
step:20/1900 train_time:741ms step_avg:37.03ms
step:21/1900 train_time:775ms step_avg:36.89ms
step:22/1900 train_time:809ms step_avg:36.76ms
step:23/1900 train_time:843ms step_avg:36.63ms
step:24/1900 train_time:877ms step_avg:36.53ms
step:25/1900 train_time:910ms step_avg:36.42ms
step:26/1900 train_time:944ms step_avg:36.32ms
step:27/1900 train_time:978ms step_avg:36.24ms
step:28/1900 train_time:1012ms step_avg:36.16ms
step:29/1900 train_time:1046ms step_avg:36.09ms
step:30/1900 train_time:1081ms step_avg:36.02ms
step:31/1900 train_time:1114ms step_avg:35.94ms
step:32/1900 train_time:1148ms step_avg:35.88ms
step:33/1900 train_time:1183ms step_avg:35.84ms
step:34/1900 train_time:1217ms step_avg:35.79ms
step:35/1900 train_time:1251ms step_avg:35.75ms
step:36/1900 train_time:1285ms step_avg:35.71ms
step:37/1900 train_time:1320ms step_avg:35.67ms
step:38/1900 train_time:1354ms step_avg:35.63ms
step:39/1900 train_time:1389ms step_avg:35.60ms
step:40/1900 train_time:1423ms step_avg:35.56ms
step:41/1900 train_time:1457ms step_avg:35.53ms
step:42/1900 train_time:1491ms step_avg:35.50ms
step:43/1900 train_time:1525ms step_avg:35.47ms
step:44/1900 train_time:1559ms step_avg:35.43ms
step:45/1900 train_time:1593ms step_avg:35.41ms
step:46/1900 train_time:1627ms step_avg:35.38ms
step:47/1900 train_time:1662ms step_avg:35.35ms
step:48/1900 train_time:1696ms step_avg:35.32ms
step:49/1900 train_time:1730ms step_avg:35.30ms
step:50/1900 train_time:1764ms step_avg:35.27ms
step:51/1900 train_time:1798ms step_avg:35.26ms
step:52/1900 train_time:1832ms step_avg:35.23ms
step:53/1900 train_time:1866ms step_avg:35.21ms
step:54/1900 train_time:1900ms step_avg:35.19ms
step:55/1900 train_time:1934ms step_avg:35.17ms
step:56/1900 train_time:1968ms step_avg:35.15ms
step:57/1900 train_time:2002ms step_avg:35.13ms
step:58/1900 train_time:2036ms step_avg:35.11ms
step:59/1900 train_time:2070ms step_avg:35.09ms
step:60/1900 train_time:2104ms step_avg:35.07ms
step:61/1900 train_time:2138ms step_avg:35.05ms
step:62/1900 train_time:2172ms step_avg:35.04ms
step:63/1900 train_time:2206ms step_avg:35.02ms
step:64/1900 train_time:2241ms step_avg:35.01ms
step:65/1900 train_time:2275ms step_avg:34.99ms
step:66/1900 train_time:2309ms step_avg:34.98ms
step:67/1900 train_time:2343ms step_avg:34.96ms
step:68/1900 train_time:2377ms step_avg:34.95ms
step:69/1900 train_time:2410ms step_avg:34.93ms
step:70/1900 train_time:2444ms step_avg:34.92ms
step:71/1900 train_time:2478ms step_avg:34.91ms
step:72/1900 train_time:2513ms step_avg:34.90ms
step:73/1900 train_time:2546ms step_avg:34.88ms
step:74/1900 train_time:2581ms step_avg:34.87ms
step:75/1900 train_time:2615ms step_avg:34.86ms
step:76/1900 train_time:2649ms step_avg:34.85ms
step:77/1900 train_time:2683ms step_avg:34.84ms
step:78/1900 train_time:2717ms step_avg:34.83ms
step:79/1900 train_time:2751ms step_avg:34.82ms
step:80/1900 train_time:2785ms step_avg:34.81ms
step:81/1900 train_time:2819ms step_avg:34.80ms
step:82/1900 train_time:2853ms step_avg:34.79ms
step:83/1900 train_time:2887ms step_avg:34.78ms
step:84/1900 train_time:2921ms step_avg:34.77ms
step:85/1900 train_time:2955ms step_avg:34.76ms
step:86/1900 train_time:2989ms step_avg:34.75ms
step:87/1900 train_time:3023ms step_avg:34.75ms
step:88/1900 train_time:3057ms step_avg:34.74ms
step:89/1900 train_time:3091ms step_avg:34.73ms
step:90/1900 train_time:3125ms step_avg:34.72ms
step:91/1900 train_time:3159ms step_avg:34.71ms
step:92/1900 train_time:3193ms step_avg:34.71ms
step:93/1900 train_time:3227ms step_avg:34.70ms
step:94/1900 train_time:3261ms step_avg:34.69ms
step:95/1900 train_time:3296ms step_avg:34.69ms
step:96/1900 train_time:3330ms step_avg:34.68ms
step:97/1900 train_time:3364ms step_avg:34.68ms
step:98/1900 train_time:3398ms step_avg:34.67ms
step:99/1900 train_time:3432ms step_avg:34.67ms
step:100/1900 train_time:3466ms step_avg:34.66ms
step:101/1900 train_time:3500ms step_avg:34.66ms
step:102/1900 train_time:3534ms step_avg:34.65ms
step:103/1900 train_time:3568ms step_avg:34.64ms
step:104/1900 train_time:3602ms step_avg:34.63ms
step:105/1900 train_time:3636ms step_avg:34.63ms
step:106/1900 train_time:3670ms step_avg:34.62ms
step:107/1900 train_time:3704ms step_avg:34.62ms
step:108/1900 train_time:3738ms step_avg:34.61ms
step:109/1900 train_time:3773ms step_avg:34.61ms
step:110/1900 train_time:3807ms step_avg:34.61ms
step:111/1900 train_time:3841ms step_avg:34.61ms
step:112/1900 train_time:3875ms step_avg:34.60ms
step:113/1900 train_time:3909ms step_avg:34.60ms
step:114/1900 train_time:3944ms step_avg:34.59ms
step:115/1900 train_time:3977ms step_avg:34.59ms
step:116/1900 train_time:4011ms step_avg:34.58ms
step:117/1900 train_time:4045ms step_avg:34.58ms
step:118/1900 train_time:4079ms step_avg:34.57ms
step:119/1900 train_time:4113ms step_avg:34.57ms
step:120/1900 train_time:4147ms step_avg:34.56ms
step:121/1900 train_time:4181ms step_avg:34.56ms
step:122/1900 train_time:4215ms step_avg:34.55ms
step:123/1900 train_time:4249ms step_avg:34.55ms
step:124/1900 train_time:4283ms step_avg:34.54ms
step:125/1900 train_time:4318ms step_avg:34.54ms
step:126/1900 train_time:4352ms step_avg:34.54ms
step:127/1900 train_time:4385ms step_avg:34.53ms
step:128/1900 train_time:4419ms step_avg:34.53ms
step:129/1900 train_time:4453ms step_avg:34.52ms
step:130/1900 train_time:4487ms step_avg:34.52ms
step:131/1900 train_time:4521ms step_avg:34.51ms
step:132/1900 train_time:4555ms step_avg:34.51ms
step:133/1900 train_time:4589ms step_avg:34.50ms
step:134/1900 train_time:4623ms step_avg:34.50ms
step:135/1900 train_time:4657ms step_avg:34.50ms
step:136/1900 train_time:4691ms step_avg:34.50ms
step:137/1900 train_time:4725ms step_avg:34.49ms
step:138/1900 train_time:4759ms step_avg:34.49ms
step:139/1900 train_time:4793ms step_avg:34.49ms
step:140/1900 train_time:4827ms step_avg:34.48ms
step:141/1900 train_time:4861ms step_avg:34.48ms
step:142/1900 train_time:4895ms step_avg:34.47ms
step:143/1900 train_time:4929ms step_avg:34.47ms
step:144/1900 train_time:4963ms step_avg:34.47ms
step:145/1900 train_time:4997ms step_avg:34.46ms
step:146/1900 train_time:5031ms step_avg:34.46ms
step:147/1900 train_time:5066ms step_avg:34.46ms
step:148/1900 train_time:5099ms step_avg:34.46ms
step:149/1900 train_time:5134ms step_avg:34.45ms
step:150/1900 train_time:5168ms step_avg:34.45ms
step:151/1900 train_time:5201ms step_avg:34.45ms
step:152/1900 train_time:5235ms step_avg:34.44ms
step:153/1900 train_time:5269ms step_avg:34.44ms
step:154/1900 train_time:5304ms step_avg:34.44ms
step:155/1900 train_time:5338ms step_avg:34.44ms
step:156/1900 train_time:5372ms step_avg:34.44ms
step:157/1900 train_time:5406ms step_avg:34.44ms
step:158/1900 train_time:5440ms step_avg:34.43ms
step:159/1900 train_time:5474ms step_avg:34.43ms
step:160/1900 train_time:5508ms step_avg:34.43ms
step:161/1900 train_time:5542ms step_avg:34.42ms
step:162/1900 train_time:5576ms step_avg:34.42ms
step:163/1900 train_time:5610ms step_avg:34.42ms
step:164/1900 train_time:5644ms step_avg:34.41ms
step:165/1900 train_time:5678ms step_avg:34.41ms
step:166/1900 train_time:5711ms step_avg:34.41ms
step:167/1900 train_time:5746ms step_avg:34.41ms
step:168/1900 train_time:5780ms step_avg:34.40ms
step:169/1900 train_time:5814ms step_avg:34.40ms
step:170/1900 train_time:5848ms step_avg:34.40ms
step:171/1900 train_time:5882ms step_avg:34.40ms
step:172/1900 train_time:5916ms step_avg:34.39ms
step:173/1900 train_time:5950ms step_avg:34.39ms
step:174/1900 train_time:5984ms step_avg:34.39ms
step:175/1900 train_time:6018ms step_avg:34.39ms
step:176/1900 train_time:6051ms step_avg:34.38ms
step:177/1900 train_time:6085ms step_avg:34.38ms
step:178/1900 train_time:6119ms step_avg:34.38ms
step:179/1900 train_time:6153ms step_avg:34.38ms
step:180/1900 train_time:6187ms step_avg:34.37ms
step:181/1900 train_time:6221ms step_avg:34.37ms
step:182/1900 train_time:6255ms step_avg:34.37ms
step:183/1900 train_time:6289ms step_avg:34.36ms
step:184/1900 train_time:6323ms step_avg:34.36ms
step:185/1900 train_time:6356ms step_avg:34.36ms
step:186/1900 train_time:6390ms step_avg:34.36ms
step:187/1900 train_time:6425ms step_avg:34.36ms
step:188/1900 train_time:6459ms step_avg:34.35ms
step:189/1900 train_time:6493ms step_avg:34.35ms
step:190/1900 train_time:6527ms step_avg:34.35ms
step:191/1900 train_time:6561ms step_avg:34.35ms
step:192/1900 train_time:6595ms step_avg:34.35ms
step:193/1900 train_time:6629ms step_avg:34.34ms
step:194/1900 train_time:6663ms step_avg:34.34ms
step:195/1900 train_time:6697ms step_avg:34.34ms
step:196/1900 train_time:6731ms step_avg:34.34ms
step:197/1900 train_time:6765ms step_avg:34.34ms
step:198/1900 train_time:6798ms step_avg:34.34ms
step:199/1900 train_time:6832ms step_avg:34.33ms
step:200/1900 train_time:6866ms step_avg:34.33ms
step:201/1900 train_time:6900ms step_avg:34.33ms
step:202/1900 train_time:6934ms step_avg:34.33ms
step:203/1900 train_time:6968ms step_avg:34.33ms
step:204/1900 train_time:7002ms step_avg:34.32ms
step:205/1900 train_time:7036ms step_avg:34.32ms
step:206/1900 train_time:7070ms step_avg:34.32ms
step:207/1900 train_time:7104ms step_avg:34.32ms
step:208/1900 train_time:7138ms step_avg:34.31ms
step:209/1900 train_time:7171ms step_avg:34.31ms
step:210/1900 train_time:7205ms step_avg:34.31ms
step:211/1900 train_time:7240ms step_avg:34.31ms
step:212/1900 train_time:7273ms step_avg:34.31ms
step:213/1900 train_time:7307ms step_avg:34.31ms
step:214/1900 train_time:7341ms step_avg:34.30ms
step:215/1900 train_time:7375ms step_avg:34.30ms
step:216/1900 train_time:7409ms step_avg:34.30ms
step:217/1900 train_time:7443ms step_avg:34.30ms
step:218/1900 train_time:7477ms step_avg:34.30ms
step:219/1900 train_time:7511ms step_avg:34.30ms
step:220/1900 train_time:7545ms step_avg:34.29ms
step:221/1900 train_time:7579ms step_avg:34.29ms
step:222/1900 train_time:7613ms step_avg:34.29ms
step:223/1900 train_time:7647ms step_avg:34.29ms
step:224/1900 train_time:7681ms step_avg:34.29ms
step:225/1900 train_time:7715ms step_avg:34.29ms
step:226/1900 train_time:7748ms step_avg:34.28ms
step:227/1900 train_time:7782ms step_avg:34.28ms
step:228/1900 train_time:7816ms step_avg:34.28ms
step:229/1900 train_time:7850ms step_avg:34.28ms
step:230/1900 train_time:7884ms step_avg:34.28ms
step:231/1900 train_time:7918ms step_avg:34.28ms
step:232/1900 train_time:7952ms step_avg:34.28ms
step:233/1900 train_time:7986ms step_avg:34.28ms
step:234/1900 train_time:8020ms step_avg:34.27ms
step:235/1900 train_time:8054ms step_avg:34.27ms
step:236/1900 train_time:8088ms step_avg:34.27ms
step:237/1900 train_time:8122ms step_avg:34.27ms
step:238/1900 train_time:8155ms step_avg:34.27ms
step:239/1900 train_time:8190ms step_avg:34.27ms
step:240/1900 train_time:8224ms step_avg:34.26ms
step:241/1900 train_time:8258ms step_avg:34.26ms
step:242/1900 train_time:8291ms step_avg:34.26ms
step:243/1900 train_time:8325ms step_avg:34.26ms
step:244/1900 train_time:8359ms step_avg:34.26ms
step:245/1900 train_time:8393ms step_avg:34.26ms
step:246/1900 train_time:8427ms step_avg:34.26ms
step:247/1900 train_time:8461ms step_avg:34.26ms
step:248/1900 train_time:8495ms step_avg:34.25ms
step:249/1900 train_time:8529ms step_avg:34.25ms
step:250/1900 train_time:8563ms step_avg:34.25ms
step:250/1900 val_loss:4.6111 train_time:8600ms step_avg:34.40ms
step:251/1900 train_time:8622ms step_avg:34.35ms
step:252/1900 train_time:8642ms step_avg:34.29ms
step:253/1900 train_time:8669ms step_avg:34.26ms
step:254/1900 train_time:8703ms step_avg:34.26ms
step:255/1900 train_time:8738ms step_avg:34.27ms
step:256/1900 train_time:8773ms step_avg:34.27ms
step:257/1900 train_time:8808ms step_avg:34.27ms
step:258/1900 train_time:8842ms step_avg:34.27ms
step:259/1900 train_time:8876ms step_avg:34.27ms
step:260/1900 train_time:8910ms step_avg:34.27ms
step:261/1900 train_time:8944ms step_avg:34.27ms
step:262/1900 train_time:8978ms step_avg:34.27ms
step:263/1900 train_time:9012ms step_avg:34.26ms
step:264/1900 train_time:9045ms step_avg:34.26ms
step:265/1900 train_time:9079ms step_avg:34.26ms
step:266/1900 train_time:9113ms step_avg:34.26ms
step:267/1900 train_time:9147ms step_avg:34.26ms
step:268/1900 train_time:9181ms step_avg:34.26ms
step:269/1900 train_time:9215ms step_avg:34.25ms
step:270/1900 train_time:9248ms step_avg:34.25ms
step:271/1900 train_time:9282ms step_avg:34.25ms
step:272/1900 train_time:9316ms step_avg:34.25ms
step:273/1900 train_time:9350ms step_avg:34.25ms
step:274/1900 train_time:9384ms step_avg:34.25ms
step:275/1900 train_time:9417ms step_avg:34.25ms
step:276/1900 train_time:9451ms step_avg:34.24ms
step:277/1900 train_time:9485ms step_avg:34.24ms
step:278/1900 train_time:9519ms step_avg:34.24ms
step:279/1900 train_time:9553ms step_avg:34.24ms
step:280/1900 train_time:9587ms step_avg:34.24ms
step:281/1900 train_time:9621ms step_avg:34.24ms
step:282/1900 train_time:9655ms step_avg:34.24ms
step:283/1900 train_time:9689ms step_avg:34.24ms
step:284/1900 train_time:9724ms step_avg:34.24ms
step:285/1900 train_time:9758ms step_avg:34.24ms
step:286/1900 train_time:9792ms step_avg:34.24ms
step:287/1900 train_time:9826ms step_avg:34.24ms
step:288/1900 train_time:9860ms step_avg:34.24ms
step:289/1900 train_time:9895ms step_avg:34.24ms
step:290/1900 train_time:9929ms step_avg:34.24ms
step:291/1900 train_time:9963ms step_avg:34.24ms
step:292/1900 train_time:9997ms step_avg:34.24ms
step:293/1900 train_time:10031ms step_avg:34.24ms
step:294/1900 train_time:10065ms step_avg:34.23ms
step:295/1900 train_time:10099ms step_avg:34.23ms
step:296/1900 train_time:10133ms step_avg:34.23ms
step:297/1900 train_time:10167ms step_avg:34.23ms
step:298/1900 train_time:10201ms step_avg:34.23ms
step:299/1900 train_time:10234ms step_avg:34.23ms
step:300/1900 train_time:10268ms step_avg:34.23ms
step:301/1900 train_time:10302ms step_avg:34.23ms
step:302/1900 train_time:10336ms step_avg:34.23ms
step:303/1900 train_time:10370ms step_avg:34.23ms
step:304/1900 train_time:10404ms step_avg:34.22ms
step:305/1900 train_time:10438ms step_avg:34.22ms
step:306/1900 train_time:10472ms step_avg:34.22ms
step:307/1900 train_time:10506ms step_avg:34.22ms
step:308/1900 train_time:10540ms step_avg:34.22ms
step:309/1900 train_time:10574ms step_avg:34.22ms
step:310/1900 train_time:10608ms step_avg:34.22ms
step:311/1900 train_time:10641ms step_avg:34.22ms
step:312/1900 train_time:10675ms step_avg:34.21ms
step:313/1900 train_time:10709ms step_avg:34.21ms
step:314/1900 train_time:10743ms step_avg:34.21ms
step:315/1900 train_time:10777ms step_avg:34.21ms
step:316/1900 train_time:10811ms step_avg:34.21ms
step:317/1900 train_time:10845ms step_avg:34.21ms
step:318/1900 train_time:10879ms step_avg:34.21ms
step:319/1900 train_time:10912ms step_avg:34.21ms
step:320/1900 train_time:10946ms step_avg:34.21ms
step:321/1900 train_time:10980ms step_avg:34.21ms
step:322/1900 train_time:11014ms step_avg:34.20ms
step:323/1900 train_time:11048ms step_avg:34.20ms
step:324/1900 train_time:11082ms step_avg:34.20ms
step:325/1900 train_time:11116ms step_avg:34.20ms
step:326/1900 train_time:11150ms step_avg:34.20ms
step:327/1900 train_time:11184ms step_avg:34.20ms
step:328/1900 train_time:11218ms step_avg:34.20ms
step:329/1900 train_time:11252ms step_avg:34.20ms
step:330/1900 train_time:11286ms step_avg:34.20ms
step:331/1900 train_time:11319ms step_avg:34.20ms
step:332/1900 train_time:11353ms step_avg:34.20ms
step:333/1900 train_time:11387ms step_avg:34.20ms
step:334/1900 train_time:11421ms step_avg:34.20ms
step:335/1900 train_time:11455ms step_avg:34.19ms
step:336/1900 train_time:11489ms step_avg:34.19ms
step:337/1900 train_time:11523ms step_avg:34.19ms
step:338/1900 train_time:11557ms step_avg:34.19ms
step:339/1900 train_time:11591ms step_avg:34.19ms
step:340/1900 train_time:11625ms step_avg:34.19ms
step:341/1900 train_time:11658ms step_avg:34.19ms
step:342/1900 train_time:11692ms step_avg:34.19ms
step:343/1900 train_time:11726ms step_avg:34.19ms
step:344/1900 train_time:11760ms step_avg:34.19ms
step:345/1900 train_time:11794ms step_avg:34.19ms
step:346/1900 train_time:11828ms step_avg:34.18ms
step:347/1900 train_time:11862ms step_avg:34.18ms
step:348/1900 train_time:11896ms step_avg:34.18ms
step:349/1900 train_time:11930ms step_avg:34.18ms
step:350/1900 train_time:11964ms step_avg:34.18ms
step:351/1900 train_time:11997ms step_avg:34.18ms
step:352/1900 train_time:12031ms step_avg:34.18ms
step:353/1900 train_time:12065ms step_avg:34.18ms
step:354/1900 train_time:12099ms step_avg:34.18ms
step:355/1900 train_time:12133ms step_avg:34.18ms
step:356/1900 train_time:12167ms step_avg:34.18ms
step:357/1900 train_time:12201ms step_avg:34.18ms
step:358/1900 train_time:12235ms step_avg:34.18ms
step:359/1900 train_time:12269ms step_avg:34.18ms
step:360/1900 train_time:12303ms step_avg:34.17ms
step:361/1900 train_time:12337ms step_avg:34.18ms
step:362/1900 train_time:12371ms step_avg:34.18ms
step:363/1900 train_time:12405ms step_avg:34.17ms
step:364/1900 train_time:12439ms step_avg:34.17ms
step:365/1900 train_time:12473ms step_avg:34.17ms
step:366/1900 train_time:12507ms step_avg:34.17ms
step:367/1900 train_time:12541ms step_avg:34.17ms
step:368/1900 train_time:12575ms step_avg:34.17ms
step:369/1900 train_time:12609ms step_avg:34.17ms
step:370/1900 train_time:12642ms step_avg:34.17ms
step:371/1900 train_time:12677ms step_avg:34.17ms
step:372/1900 train_time:12710ms step_avg:34.17ms
step:373/1900 train_time:12744ms step_avg:34.17ms
step:374/1900 train_time:12778ms step_avg:34.17ms
step:375/1900 train_time:12812ms step_avg:34.17ms
step:376/1900 train_time:12846ms step_avg:34.17ms
step:377/1900 train_time:12880ms step_avg:34.17ms
step:378/1900 train_time:12914ms step_avg:34.16ms
step:379/1900 train_time:12948ms step_avg:34.16ms
step:380/1900 train_time:12982ms step_avg:34.16ms
step:381/1900 train_time:13016ms step_avg:34.16ms
step:382/1900 train_time:13050ms step_avg:34.16ms
step:383/1900 train_time:13084ms step_avg:34.16ms
step:384/1900 train_time:13118ms step_avg:34.16ms
step:385/1900 train_time:13152ms step_avg:34.16ms
step:386/1900 train_time:13185ms step_avg:34.16ms
step:387/1900 train_time:13219ms step_avg:34.16ms
step:388/1900 train_time:13253ms step_avg:34.16ms
step:389/1900 train_time:13287ms step_avg:34.16ms
step:390/1900 train_time:13321ms step_avg:34.16ms
step:391/1900 train_time:13355ms step_avg:34.16ms
step:392/1900 train_time:13389ms step_avg:34.16ms
step:393/1900 train_time:13423ms step_avg:34.16ms
step:394/1900 train_time:13457ms step_avg:34.15ms
step:395/1900 train_time:13491ms step_avg:34.15ms
step:396/1900 train_time:13525ms step_avg:34.15ms
step:397/1900 train_time:13559ms step_avg:34.15ms
step:398/1900 train_time:13593ms step_avg:34.15ms
step:399/1900 train_time:13627ms step_avg:34.15ms
step:400/1900 train_time:13660ms step_avg:34.15ms
step:401/1900 train_time:13694ms step_avg:34.15ms
step:402/1900 train_time:13728ms step_avg:34.15ms
step:403/1900 train_time:13762ms step_avg:34.15ms
step:404/1900 train_time:13796ms step_avg:34.15ms
step:405/1900 train_time:13830ms step_avg:34.15ms
step:406/1900 train_time:13864ms step_avg:34.15ms
step:407/1900 train_time:13898ms step_avg:34.15ms
step:408/1900 train_time:13932ms step_avg:34.15ms
step:409/1900 train_time:13966ms step_avg:34.15ms
step:410/1900 train_time:14000ms step_avg:34.15ms
step:411/1900 train_time:14033ms step_avg:34.14ms
step:412/1900 train_time:14068ms step_avg:34.14ms
step:413/1900 train_time:14101ms step_avg:34.14ms
step:414/1900 train_time:14135ms step_avg:34.14ms
step:415/1900 train_time:14169ms step_avg:34.14ms
step:416/1900 train_time:14203ms step_avg:34.14ms
step:417/1900 train_time:14237ms step_avg:34.14ms
step:418/1900 train_time:14271ms step_avg:34.14ms
step:419/1900 train_time:14305ms step_avg:34.14ms
step:420/1900 train_time:14339ms step_avg:34.14ms
step:421/1900 train_time:14373ms step_avg:34.14ms
step:422/1900 train_time:14407ms step_avg:34.14ms
step:423/1900 train_time:14441ms step_avg:34.14ms
step:424/1900 train_time:14475ms step_avg:34.14ms
step:425/1900 train_time:14508ms step_avg:34.14ms
step:426/1900 train_time:14542ms step_avg:34.14ms
step:427/1900 train_time:14576ms step_avg:34.14ms
step:428/1900 train_time:14610ms step_avg:34.14ms
step:429/1900 train_time:14645ms step_avg:34.14ms
step:430/1900 train_time:14679ms step_avg:34.14ms
step:431/1900 train_time:14712ms step_avg:34.14ms
step:432/1900 train_time:14746ms step_avg:34.13ms
step:433/1900 train_time:14780ms step_avg:34.13ms
step:434/1900 train_time:14814ms step_avg:34.13ms
step:435/1900 train_time:14848ms step_avg:34.13ms
step:436/1900 train_time:14882ms step_avg:34.13ms
step:437/1900 train_time:14916ms step_avg:34.13ms
step:438/1900 train_time:14950ms step_avg:34.13ms
step:439/1900 train_time:14984ms step_avg:34.13ms
step:440/1900 train_time:15018ms step_avg:34.13ms
step:441/1900 train_time:15052ms step_avg:34.13ms
step:442/1900 train_time:15086ms step_avg:34.13ms
step:443/1900 train_time:15120ms step_avg:34.13ms
step:444/1900 train_time:15154ms step_avg:34.13ms
step:445/1900 train_time:15187ms step_avg:34.13ms
step:446/1900 train_time:15221ms step_avg:34.13ms
step:447/1900 train_time:15255ms step_avg:34.13ms
step:448/1900 train_time:15289ms step_avg:34.13ms
step:449/1900 train_time:15323ms step_avg:34.13ms
step:450/1900 train_time:15357ms step_avg:34.13ms
step:451/1900 train_time:15391ms step_avg:34.13ms
step:452/1900 train_time:15425ms step_avg:34.13ms
step:453/1900 train_time:15459ms step_avg:34.13ms
step:454/1900 train_time:15493ms step_avg:34.12ms
step:455/1900 train_time:15527ms step_avg:34.12ms
step:456/1900 train_time:15560ms step_avg:34.12ms
step:457/1900 train_time:15594ms step_avg:34.12ms
step:458/1900 train_time:15628ms step_avg:34.12ms
step:459/1900 train_time:15662ms step_avg:34.12ms
step:460/1900 train_time:15696ms step_avg:34.12ms
step:461/1900 train_time:15730ms step_avg:34.12ms
step:462/1900 train_time:15764ms step_avg:34.12ms
step:463/1900 train_time:15798ms step_avg:34.12ms
step:464/1900 train_time:15832ms step_avg:34.12ms
step:465/1900 train_time:15865ms step_avg:34.12ms
step:466/1900 train_time:15899ms step_avg:34.12ms
step:467/1900 train_time:15933ms step_avg:34.12ms
step:468/1900 train_time:15967ms step_avg:34.12ms
step:469/1900 train_time:16001ms step_avg:34.12ms
step:470/1900 train_time:16035ms step_avg:34.12ms
step:471/1900 train_time:16068ms step_avg:34.12ms
step:472/1900 train_time:16102ms step_avg:34.11ms
step:473/1900 train_time:16136ms step_avg:34.11ms
step:474/1900 train_time:16170ms step_avg:34.11ms
step:475/1900 train_time:16204ms step_avg:34.11ms
step:476/1900 train_time:16238ms step_avg:34.11ms
step:477/1900 train_time:16271ms step_avg:34.11ms
step:478/1900 train_time:16305ms step_avg:34.11ms
step:479/1900 train_time:16339ms step_avg:34.11ms
step:480/1900 train_time:16373ms step_avg:34.11ms
step:481/1900 train_time:16407ms step_avg:34.11ms
step:482/1900 train_time:16441ms step_avg:34.11ms
step:483/1900 train_time:16474ms step_avg:34.11ms
step:484/1900 train_time:16508ms step_avg:34.11ms
step:485/1900 train_time:16542ms step_avg:34.11ms
step:486/1900 train_time:16576ms step_avg:34.11ms
step:487/1900 train_time:16610ms step_avg:34.11ms
step:488/1900 train_time:16644ms step_avg:34.11ms
step:489/1900 train_time:16678ms step_avg:34.11ms
step:490/1900 train_time:16712ms step_avg:34.11ms
step:491/1900 train_time:16746ms step_avg:34.11ms
step:492/1900 train_time:16780ms step_avg:34.11ms
step:493/1900 train_time:16813ms step_avg:34.10ms
step:494/1900 train_time:16847ms step_avg:34.10ms
step:495/1900 train_time:16881ms step_avg:34.10ms
step:496/1900 train_time:16915ms step_avg:34.10ms
step:497/1900 train_time:16949ms step_avg:34.10ms
step:498/1900 train_time:16983ms step_avg:34.10ms
step:499/1900 train_time:17016ms step_avg:34.10ms
step:500/1900 train_time:17050ms step_avg:34.10ms
step:500/1900 val_loss:4.2869 train_time:17087ms step_avg:34.17ms
step:501/1900 train_time:17107ms step_avg:34.15ms
step:502/1900 train_time:17127ms step_avg:34.12ms
step:503/1900 train_time:17156ms step_avg:34.11ms
step:504/1900 train_time:17191ms step_avg:34.11ms
step:505/1900 train_time:17225ms step_avg:34.11ms
step:506/1900 train_time:17260ms step_avg:34.11ms
step:507/1900 train_time:17293ms step_avg:34.11ms
step:508/1900 train_time:17327ms step_avg:34.11ms
step:509/1900 train_time:17361ms step_avg:34.11ms
step:510/1900 train_time:17395ms step_avg:34.11ms
step:511/1900 train_time:17429ms step_avg:34.11ms
step:512/1900 train_time:17463ms step_avg:34.11ms
step:513/1900 train_time:17497ms step_avg:34.11ms
step:514/1900 train_time:17531ms step_avg:34.11ms
step:515/1900 train_time:17565ms step_avg:34.11ms
step:516/1900 train_time:17599ms step_avg:34.11ms
step:517/1900 train_time:17632ms step_avg:34.10ms
step:518/1900 train_time:17666ms step_avg:34.10ms
step:519/1900 train_time:17700ms step_avg:34.10ms
step:520/1900 train_time:17734ms step_avg:34.10ms
step:521/1900 train_time:17768ms step_avg:34.10ms
step:522/1900 train_time:17802ms step_avg:34.10ms
step:523/1900 train_time:17835ms step_avg:34.10ms
step:524/1900 train_time:17869ms step_avg:34.10ms
step:525/1900 train_time:17903ms step_avg:34.10ms
step:526/1900 train_time:17937ms step_avg:34.10ms
step:527/1900 train_time:17971ms step_avg:34.10ms
step:528/1900 train_time:18004ms step_avg:34.10ms
step:529/1900 train_time:18039ms step_avg:34.10ms
step:530/1900 train_time:18072ms step_avg:34.10ms
step:531/1900 train_time:18107ms step_avg:34.10ms
step:532/1900 train_time:18141ms step_avg:34.10ms
step:533/1900 train_time:18175ms step_avg:34.10ms
step:534/1900 train_time:18209ms step_avg:34.10ms
step:535/1900 train_time:18243ms step_avg:34.10ms
step:536/1900 train_time:18277ms step_avg:34.10ms
step:537/1900 train_time:18311ms step_avg:34.10ms
step:538/1900 train_time:18345ms step_avg:34.10ms
step:539/1900 train_time:18379ms step_avg:34.10ms
step:540/1900 train_time:18413ms step_avg:34.10ms
step:541/1900 train_time:18447ms step_avg:34.10ms
step:542/1900 train_time:18480ms step_avg:34.10ms
step:543/1900 train_time:18514ms step_avg:34.10ms
step:544/1900 train_time:18548ms step_avg:34.10ms
step:545/1900 train_time:18582ms step_avg:34.10ms
step:546/1900 train_time:18616ms step_avg:34.10ms
step:547/1900 train_time:18650ms step_avg:34.09ms
step:548/1900 train_time:18683ms step_avg:34.09ms
step:549/1900 train_time:18717ms step_avg:34.09ms
step:550/1900 train_time:18751ms step_avg:34.09ms
step:551/1900 train_time:18785ms step_avg:34.09ms
step:552/1900 train_time:18819ms step_avg:34.09ms
step:553/1900 train_time:18853ms step_avg:34.09ms
step:554/1900 train_time:18887ms step_avg:34.09ms
step:555/1900 train_time:18921ms step_avg:34.09ms
step:556/1900 train_time:18955ms step_avg:34.09ms
step:557/1900 train_time:18989ms step_avg:34.09ms
step:558/1900 train_time:19023ms step_avg:34.09ms
step:559/1900 train_time:19056ms step_avg:34.09ms
step:560/1900 train_time:19090ms step_avg:34.09ms
step:561/1900 train_time:19124ms step_avg:34.09ms
step:562/1900 train_time:19158ms step_avg:34.09ms
step:563/1900 train_time:19192ms step_avg:34.09ms
step:564/1900 train_time:19226ms step_avg:34.09ms
step:565/1900 train_time:19260ms step_avg:34.09ms
step:566/1900 train_time:19294ms step_avg:34.09ms
step:567/1900 train_time:19328ms step_avg:34.09ms
step:568/1900 train_time:19362ms step_avg:34.09ms
step:569/1900 train_time:19396ms step_avg:34.09ms
step:570/1900 train_time:19430ms step_avg:34.09ms
step:571/1900 train_time:19464ms step_avg:34.09ms
step:572/1900 train_time:19497ms step_avg:34.09ms
step:573/1900 train_time:19531ms step_avg:34.09ms
step:574/1900 train_time:19566ms step_avg:34.09ms
step:575/1900 train_time:19599ms step_avg:34.09ms
step:576/1900 train_time:19633ms step_avg:34.09ms
step:577/1900 train_time:19667ms step_avg:34.08ms
step:578/1900 train_time:19701ms step_avg:34.08ms
step:579/1900 train_time:19735ms step_avg:34.08ms
step:580/1900 train_time:19769ms step_avg:34.08ms
step:581/1900 train_time:19802ms step_avg:34.08ms
step:582/1900 train_time:19836ms step_avg:34.08ms
step:583/1900 train_time:19870ms step_avg:34.08ms
step:584/1900 train_time:19904ms step_avg:34.08ms
step:585/1900 train_time:19938ms step_avg:34.08ms
step:586/1900 train_time:19972ms step_avg:34.08ms
step:587/1900 train_time:20006ms step_avg:34.08ms
step:588/1900 train_time:20040ms step_avg:34.08ms
step:589/1900 train_time:20074ms step_avg:34.08ms
step:590/1900 train_time:20108ms step_avg:34.08ms
step:591/1900 train_time:20142ms step_avg:34.08ms
step:592/1900 train_time:20176ms step_avg:34.08ms
step:593/1900 train_time:20210ms step_avg:34.08ms
step:594/1900 train_time:20244ms step_avg:34.08ms
step:595/1900 train_time:20278ms step_avg:34.08ms
step:596/1900 train_time:20312ms step_avg:34.08ms
step:597/1900 train_time:20346ms step_avg:34.08ms
step:598/1900 train_time:20379ms step_avg:34.08ms
step:599/1900 train_time:20413ms step_avg:34.08ms
step:600/1900 train_time:20447ms step_avg:34.08ms
step:601/1900 train_time:20481ms step_avg:34.08ms
step:602/1900 train_time:20515ms step_avg:34.08ms
step:603/1900 train_time:20549ms step_avg:34.08ms
step:604/1900 train_time:20583ms step_avg:34.08ms
step:605/1900 train_time:20617ms step_avg:34.08ms
step:606/1900 train_time:20651ms step_avg:34.08ms
step:607/1900 train_time:20685ms step_avg:34.08ms
step:608/1900 train_time:20718ms step_avg:34.08ms
step:609/1900 train_time:20752ms step_avg:34.08ms
step:610/1900 train_time:20786ms step_avg:34.08ms
step:611/1900 train_time:20820ms step_avg:34.08ms
step:612/1900 train_time:20854ms step_avg:34.08ms
step:613/1900 train_time:20888ms step_avg:34.07ms
step:614/1900 train_time:20922ms step_avg:34.08ms
step:615/1900 train_time:20956ms step_avg:34.07ms
step:616/1900 train_time:20990ms step_avg:34.07ms
step:617/1900 train_time:21023ms step_avg:34.07ms
step:618/1900 train_time:21058ms step_avg:34.07ms
step:619/1900 train_time:21091ms step_avg:34.07ms
step:620/1900 train_time:21125ms step_avg:34.07ms
step:621/1900 train_time:21160ms step_avg:34.07ms
step:622/1900 train_time:21220ms step_avg:34.12ms
step:623/1900 train_time:21281ms step_avg:34.16ms
step:624/1900 train_time:21343ms step_avg:34.20ms
step:625/1900 train_time:21405ms step_avg:34.25ms
step:626/1900 train_time:21465ms step_avg:34.29ms
step:627/1900 train_time:21528ms step_avg:34.33ms
step:628/1900 train_time:21589ms step_avg:34.38ms
step:629/1900 train_time:21651ms step_avg:34.42ms
step:630/1900 train_time:21712ms step_avg:34.46ms
step:631/1900 train_time:21774ms step_avg:34.51ms
step:632/1900 train_time:21835ms step_avg:34.55ms
step:633/1900 train_time:21896ms step_avg:34.59ms
step:634/1900 train_time:21958ms step_avg:34.63ms
step:635/1900 train_time:22019ms step_avg:34.68ms
step:636/1900 train_time:22080ms step_avg:34.72ms
step:637/1900 train_time:22142ms step_avg:34.76ms
step:638/1900 train_time:22203ms step_avg:34.80ms
step:639/1900 train_time:22264ms step_avg:34.84ms
step:640/1900 train_time:22325ms step_avg:34.88ms
step:641/1900 train_time:22386ms step_avg:34.92ms
step:642/1900 train_time:22448ms step_avg:34.97ms
step:643/1900 train_time:22509ms step_avg:35.01ms
step:644/1900 train_time:22571ms step_avg:35.05ms
step:645/1900 train_time:22632ms step_avg:35.09ms
step:646/1900 train_time:22693ms step_avg:35.13ms
step:647/1900 train_time:22756ms step_avg:35.17ms
step:648/1900 train_time:22817ms step_avg:35.21ms
step:649/1900 train_time:22878ms step_avg:35.25ms
step:650/1900 train_time:22939ms step_avg:35.29ms
step:651/1900 train_time:23001ms step_avg:35.33ms
step:652/1900 train_time:23062ms step_avg:35.37ms
step:653/1900 train_time:23123ms step_avg:35.41ms
step:654/1900 train_time:23184ms step_avg:35.45ms
step:655/1900 train_time:23246ms step_avg:35.49ms
step:656/1900 train_time:23306ms step_avg:35.53ms
step:657/1900 train_time:23368ms step_avg:35.57ms
step:658/1900 train_time:23429ms step_avg:35.61ms
step:659/1900 train_time:23490ms step_avg:35.65ms
step:660/1900 train_time:23551ms step_avg:35.68ms
step:661/1900 train_time:23613ms step_avg:35.72ms
step:662/1900 train_time:23674ms step_avg:35.76ms
step:663/1900 train_time:23737ms step_avg:35.80ms
step:664/1900 train_time:23797ms step_avg:35.84ms
step:665/1900 train_time:23859ms step_avg:35.88ms
step:666/1900 train_time:23920ms step_avg:35.92ms
step:667/1900 train_time:23982ms step_avg:35.96ms
step:668/1900 train_time:24043ms step_avg:35.99ms
step:669/1900 train_time:24105ms step_avg:36.03ms
step:670/1900 train_time:24166ms step_avg:36.07ms
step:671/1900 train_time:24227ms step_avg:36.11ms
step:672/1900 train_time:24287ms step_avg:36.14ms
step:673/1900 train_time:24349ms step_avg:36.18ms
step:674/1900 train_time:24410ms step_avg:36.22ms
step:675/1900 train_time:24471ms step_avg:36.25ms
step:676/1900 train_time:24532ms step_avg:36.29ms
step:677/1900 train_time:24594ms step_avg:36.33ms
step:678/1900 train_time:24655ms step_avg:36.36ms
step:679/1900 train_time:24717ms step_avg:36.40ms
step:680/1900 train_time:24778ms step_avg:36.44ms
step:681/1900 train_time:24839ms step_avg:36.47ms
step:682/1900 train_time:24901ms step_avg:36.51ms
step:683/1900 train_time:24963ms step_avg:36.55ms
step:684/1900 train_time:25023ms step_avg:36.58ms
step:685/1900 train_time:25085ms step_avg:36.62ms
step:686/1900 train_time:25146ms step_avg:36.66ms
step:687/1900 train_time:25207ms step_avg:36.69ms
step:688/1900 train_time:25268ms step_avg:36.73ms
step:689/1900 train_time:25330ms step_avg:36.76ms
step:690/1900 train_time:25391ms step_avg:36.80ms
step:691/1900 train_time:25452ms step_avg:36.83ms
step:692/1900 train_time:25513ms step_avg:36.87ms
step:693/1900 train_time:25575ms step_avg:36.90ms
step:694/1900 train_time:25635ms step_avg:36.94ms
step:695/1900 train_time:25697ms step_avg:36.97ms
step:696/1900 train_time:25758ms step_avg:37.01ms
step:697/1900 train_time:25819ms step_avg:37.04ms
step:698/1900 train_time:25881ms step_avg:37.08ms
step:699/1900 train_time:25943ms step_avg:37.11ms
step:700/1900 train_time:26003ms step_avg:37.15ms
step:701/1900 train_time:26065ms step_avg:37.18ms
step:702/1900 train_time:26126ms step_avg:37.22ms
step:703/1900 train_time:26187ms step_avg:37.25ms
step:704/1900 train_time:26248ms step_avg:37.28ms
step:705/1900 train_time:26310ms step_avg:37.32ms
step:706/1900 train_time:26370ms step_avg:37.35ms
step:707/1900 train_time:26432ms step_avg:37.39ms
step:708/1900 train_time:26493ms step_avg:37.42ms
step:709/1900 train_time:26555ms step_avg:37.45ms
step:710/1900 train_time:26616ms step_avg:37.49ms
step:711/1900 train_time:26678ms step_avg:37.52ms
step:712/1900 train_time:26739ms step_avg:37.55ms
step:713/1900 train_time:26801ms step_avg:37.59ms
step:714/1900 train_time:26862ms step_avg:37.62ms
step:715/1900 train_time:26924ms step_avg:37.66ms
step:716/1900 train_time:26985ms step_avg:37.69ms
step:717/1900 train_time:27047ms step_avg:37.72ms
step:718/1900 train_time:27108ms step_avg:37.75ms
step:719/1900 train_time:27170ms step_avg:37.79ms
step:720/1900 train_time:27231ms step_avg:37.82ms
step:721/1900 train_time:27293ms step_avg:37.85ms
step:722/1900 train_time:27354ms step_avg:37.89ms
step:723/1900 train_time:27415ms step_avg:37.92ms
step:724/1900 train_time:27476ms step_avg:37.95ms
step:725/1900 train_time:27537ms step_avg:37.98ms
step:726/1900 train_time:27598ms step_avg:38.01ms
step:727/1900 train_time:27660ms step_avg:38.05ms
step:728/1900 train_time:27721ms step_avg:38.08ms
step:729/1900 train_time:27782ms step_avg:38.11ms
step:730/1900 train_time:27843ms step_avg:38.14ms
step:731/1900 train_time:27905ms step_avg:38.17ms
step:732/1900 train_time:27966ms step_avg:38.20ms
step:733/1900 train_time:28028ms step_avg:38.24ms
step:734/1900 train_time:28089ms step_avg:38.27ms
step:735/1900 train_time:28151ms step_avg:38.30ms
step:736/1900 train_time:28212ms step_avg:38.33ms
step:737/1900 train_time:28274ms step_avg:38.36ms
step:738/1900 train_time:28334ms step_avg:38.39ms
step:739/1900 train_time:28396ms step_avg:38.42ms
step:740/1900 train_time:28458ms step_avg:38.46ms
step:741/1900 train_time:28519ms step_avg:38.49ms
step:742/1900 train_time:28579ms step_avg:38.52ms
step:743/1900 train_time:28641ms step_avg:38.55ms
step:744/1900 train_time:28702ms step_avg:38.58ms
step:745/1900 train_time:28764ms step_avg:38.61ms
step:746/1900 train_time:28825ms step_avg:38.64ms
step:747/1900 train_time:28887ms step_avg:38.67ms
step:748/1900 train_time:28947ms step_avg:38.70ms
step:749/1900 train_time:29009ms step_avg:38.73ms
step:750/1900 train_time:29070ms step_avg:38.76ms
step:750/1900 val_loss:4.0237 train_time:29134ms step_avg:38.85ms
step:751/1900 train_time:29154ms step_avg:38.82ms
step:752/1900 train_time:29195ms step_avg:38.82ms
step:753/1900 train_time:29259ms step_avg:38.86ms
step:754/1900 train_time:29321ms step_avg:38.89ms
step:755/1900 train_time:29384ms step_avg:38.92ms
step:756/1900 train_time:29445ms step_avg:38.95ms
step:757/1900 train_time:29507ms step_avg:38.98ms
step:758/1900 train_time:29568ms step_avg:39.01ms
step:759/1900 train_time:29629ms step_avg:39.04ms
step:760/1900 train_time:29689ms step_avg:39.07ms
step:761/1900 train_time:29750ms step_avg:39.09ms
step:762/1900 train_time:29811ms step_avg:39.12ms
step:763/1900 train_time:29872ms step_avg:39.15ms
step:764/1900 train_time:29933ms step_avg:39.18ms
step:765/1900 train_time:29994ms step_avg:39.21ms
step:766/1900 train_time:30056ms step_avg:39.24ms
step:767/1900 train_time:30118ms step_avg:39.27ms
step:768/1900 train_time:30180ms step_avg:39.30ms
step:769/1900 train_time:30243ms step_avg:39.33ms
step:770/1900 train_time:30304ms step_avg:39.36ms
step:771/1900 train_time:30366ms step_avg:39.39ms
step:772/1900 train_time:30428ms step_avg:39.41ms
step:773/1900 train_time:30490ms step_avg:39.44ms
step:774/1900 train_time:30551ms step_avg:39.47ms
step:775/1900 train_time:30613ms step_avg:39.50ms
step:776/1900 train_time:30674ms step_avg:39.53ms
step:777/1900 train_time:30735ms step_avg:39.56ms
step:778/1900 train_time:30795ms step_avg:39.58ms
step:779/1900 train_time:30856ms step_avg:39.61ms
step:780/1900 train_time:30917ms step_avg:39.64ms
step:781/1900 train_time:30979ms step_avg:39.67ms
step:782/1900 train_time:31040ms step_avg:39.69ms
step:783/1900 train_time:31102ms step_avg:39.72ms
step:784/1900 train_time:31163ms step_avg:39.75ms
step:785/1900 train_time:31224ms step_avg:39.78ms
step:786/1900 train_time:31286ms step_avg:39.80ms
step:787/1900 train_time:31348ms step_avg:39.83ms
step:788/1900 train_time:31409ms step_avg:39.86ms
step:789/1900 train_time:31471ms step_avg:39.89ms
step:790/1900 train_time:31532ms step_avg:39.91ms
step:791/1900 train_time:31593ms step_avg:39.94ms
step:792/1900 train_time:31654ms step_avg:39.97ms
step:793/1900 train_time:31716ms step_avg:39.99ms
step:794/1900 train_time:31776ms step_avg:40.02ms
step:795/1900 train_time:31837ms step_avg:40.05ms
step:796/1900 train_time:31898ms step_avg:40.07ms
step:797/1900 train_time:31959ms step_avg:40.10ms
step:798/1900 train_time:32020ms step_avg:40.13ms
step:799/1900 train_time:32082ms step_avg:40.15ms
step:800/1900 train_time:32143ms step_avg:40.18ms
step:801/1900 train_time:32205ms step_avg:40.21ms
step:802/1900 train_time:32266ms step_avg:40.23ms
step:803/1900 train_time:32328ms step_avg:40.26ms
step:804/1900 train_time:32389ms step_avg:40.29ms
step:805/1900 train_time:32451ms step_avg:40.31ms
step:806/1900 train_time:32512ms step_avg:40.34ms
step:807/1900 train_time:32574ms step_avg:40.36ms
step:808/1900 train_time:32635ms step_avg:40.39ms
step:809/1900 train_time:32696ms step_avg:40.42ms
step:810/1900 train_time:32757ms step_avg:40.44ms
step:811/1900 train_time:32818ms step_avg:40.47ms
step:812/1900 train_time:32879ms step_avg:40.49ms
step:813/1900 train_time:32940ms step_avg:40.52ms
step:814/1900 train_time:33001ms step_avg:40.54ms
step:815/1900 train_time:33062ms step_avg:40.57ms
step:816/1900 train_time:33124ms step_avg:40.59ms
step:817/1900 train_time:33186ms step_avg:40.62ms
step:818/1900 train_time:33248ms step_avg:40.64ms
step:819/1900 train_time:33309ms step_avg:40.67ms
step:820/1900 train_time:33370ms step_avg:40.70ms
step:821/1900 train_time:33432ms step_avg:40.72ms
step:822/1900 train_time:33494ms step_avg:40.75ms
step:823/1900 train_time:33555ms step_avg:40.77ms
step:824/1900 train_time:33617ms step_avg:40.80ms
step:825/1900 train_time:33679ms step_avg:40.82ms
step:826/1900 train_time:33739ms step_avg:40.85ms
step:827/1900 train_time:33801ms step_avg:40.87ms
step:828/1900 train_time:33862ms step_avg:40.90ms
step:829/1900 train_time:33923ms step_avg:40.92ms
step:830/1900 train_time:33984ms step_avg:40.94ms
step:831/1900 train_time:34046ms step_avg:40.97ms
step:832/1900 train_time:34107ms step_avg:40.99ms
step:833/1900 train_time:34169ms step_avg:41.02ms
step:834/1900 train_time:34230ms step_avg:41.04ms
step:835/1900 train_time:34291ms step_avg:41.07ms
step:836/1900 train_time:34352ms step_avg:41.09ms
step:837/1900 train_time:34414ms step_avg:41.12ms
step:838/1900 train_time:34476ms step_avg:41.14ms
step:839/1900 train_time:34538ms step_avg:41.17ms
step:840/1900 train_time:34599ms step_avg:41.19ms
step:841/1900 train_time:34661ms step_avg:41.21ms
step:842/1900 train_time:34722ms step_avg:41.24ms
step:843/1900 train_time:34784ms step_avg:41.26ms
step:844/1900 train_time:34844ms step_avg:41.28ms
step:845/1900 train_time:34906ms step_avg:41.31ms
step:846/1900 train_time:34967ms step_avg:41.33ms
step:847/1900 train_time:35028ms step_avg:41.35ms
step:848/1900 train_time:35089ms step_avg:41.38ms
step:849/1900 train_time:35150ms step_avg:41.40ms
step:850/1900 train_time:35211ms step_avg:41.42ms
step:851/1900 train_time:35273ms step_avg:41.45ms
step:852/1900 train_time:35334ms step_avg:41.47ms
step:853/1900 train_time:35396ms step_avg:41.50ms
step:854/1900 train_time:35457ms step_avg:41.52ms
step:855/1900 train_time:35519ms step_avg:41.54ms
step:856/1900 train_time:35580ms step_avg:41.56ms
step:857/1900 train_time:35641ms step_avg:41.59ms
step:858/1900 train_time:35703ms step_avg:41.61ms
step:859/1900 train_time:35765ms step_avg:41.64ms
step:860/1900 train_time:35826ms step_avg:41.66ms
step:861/1900 train_time:35888ms step_avg:41.68ms
step:862/1900 train_time:35948ms step_avg:41.70ms
step:863/1900 train_time:36010ms step_avg:41.73ms
step:864/1900 train_time:36070ms step_avg:41.75ms
step:865/1900 train_time:36132ms step_avg:41.77ms
step:866/1900 train_time:36193ms step_avg:41.79ms
step:867/1900 train_time:36255ms step_avg:41.82ms
step:868/1900 train_time:36316ms step_avg:41.84ms
step:869/1900 train_time:36378ms step_avg:41.86ms
step:870/1900 train_time:36438ms step_avg:41.88ms
step:871/1900 train_time:36500ms step_avg:41.91ms
step:872/1900 train_time:36562ms step_avg:41.93ms
step:873/1900 train_time:36623ms step_avg:41.95ms
step:874/1900 train_time:36685ms step_avg:41.97ms
step:875/1900 train_time:36747ms step_avg:42.00ms
step:876/1900 train_time:36808ms step_avg:42.02ms
step:877/1900 train_time:36870ms step_avg:42.04ms
step:878/1900 train_time:36931ms step_avg:42.06ms
step:879/1900 train_time:36993ms step_avg:42.08ms
step:880/1900 train_time:37053ms step_avg:42.11ms
step:881/1900 train_time:37115ms step_avg:42.13ms
step:882/1900 train_time:37176ms step_avg:42.15ms
step:883/1900 train_time:37238ms step_avg:42.17ms
step:884/1900 train_time:37299ms step_avg:42.19ms
step:885/1900 train_time:37361ms step_avg:42.22ms
step:886/1900 train_time:37422ms step_avg:42.24ms
step:887/1900 train_time:37483ms step_avg:42.26ms
step:888/1900 train_time:37544ms step_avg:42.28ms
step:889/1900 train_time:37606ms step_avg:42.30ms
step:890/1900 train_time:37667ms step_avg:42.32ms
step:891/1900 train_time:37729ms step_avg:42.34ms
step:892/1900 train_time:37790ms step_avg:42.37ms
step:893/1900 train_time:37851ms step_avg:42.39ms
step:894/1900 train_time:37912ms step_avg:42.41ms
step:895/1900 train_time:37974ms step_avg:42.43ms
step:896/1900 train_time:38035ms step_avg:42.45ms
step:897/1900 train_time:38097ms step_avg:42.47ms
step:898/1900 train_time:38158ms step_avg:42.49ms
step:899/1900 train_time:38219ms step_avg:42.51ms
step:900/1900 train_time:38280ms step_avg:42.53ms
step:901/1900 train_time:38343ms step_avg:42.56ms
step:902/1900 train_time:38403ms step_avg:42.58ms
step:903/1900 train_time:38465ms step_avg:42.60ms
step:904/1900 train_time:38526ms step_avg:42.62ms
step:905/1900 train_time:38587ms step_avg:42.64ms
step:906/1900 train_time:38648ms step_avg:42.66ms
step:907/1900 train_time:38710ms step_avg:42.68ms
step:908/1900 train_time:38770ms step_avg:42.70ms
step:909/1900 train_time:38832ms step_avg:42.72ms
step:910/1900 train_time:38893ms step_avg:42.74ms
step:911/1900 train_time:38955ms step_avg:42.76ms
step:912/1900 train_time:39016ms step_avg:42.78ms
step:913/1900 train_time:39077ms step_avg:42.80ms
step:914/1900 train_time:39138ms step_avg:42.82ms
step:915/1900 train_time:39200ms step_avg:42.84ms
step:916/1900 train_time:39261ms step_avg:42.86ms
step:917/1900 train_time:39323ms step_avg:42.88ms
step:918/1900 train_time:39384ms step_avg:42.90ms
step:919/1900 train_time:39446ms step_avg:42.92ms
step:920/1900 train_time:39507ms step_avg:42.94ms
step:921/1900 train_time:39568ms step_avg:42.96ms
step:922/1900 train_time:39629ms step_avg:42.98ms
step:923/1900 train_time:39691ms step_avg:43.00ms
step:924/1900 train_time:39751ms step_avg:43.02ms
step:925/1900 train_time:39813ms step_avg:43.04ms
step:926/1900 train_time:39874ms step_avg:43.06ms
step:927/1900 train_time:39936ms step_avg:43.08ms
step:928/1900 train_time:39997ms step_avg:43.10ms
step:929/1900 train_time:40058ms step_avg:43.12ms
step:930/1900 train_time:40119ms step_avg:43.14ms
step:931/1900 train_time:40181ms step_avg:43.16ms
step:932/1900 train_time:40242ms step_avg:43.18ms
step:933/1900 train_time:40303ms step_avg:43.20ms
step:934/1900 train_time:40364ms step_avg:43.22ms
step:935/1900 train_time:40427ms step_avg:43.24ms
step:936/1900 train_time:40488ms step_avg:43.26ms
step:937/1900 train_time:40550ms step_avg:43.28ms
step:938/1900 train_time:40611ms step_avg:43.29ms
step:939/1900 train_time:40673ms step_avg:43.32ms
step:940/1900 train_time:40734ms step_avg:43.33ms
step:941/1900 train_time:40796ms step_avg:43.35ms
step:942/1900 train_time:40856ms step_avg:43.37ms
step:943/1900 train_time:40919ms step_avg:43.39ms
step:944/1900 train_time:40979ms step_avg:43.41ms
step:945/1900 train_time:41041ms step_avg:43.43ms
step:946/1900 train_time:41102ms step_avg:43.45ms
step:947/1900 train_time:41164ms step_avg:43.47ms
step:948/1900 train_time:41225ms step_avg:43.49ms
step:949/1900 train_time:41287ms step_avg:43.51ms
step:950/1900 train_time:41348ms step_avg:43.52ms
step:951/1900 train_time:41409ms step_avg:43.54ms
step:952/1900 train_time:41470ms step_avg:43.56ms
step:953/1900 train_time:41532ms step_avg:43.58ms
step:954/1900 train_time:41593ms step_avg:43.60ms
step:955/1900 train_time:41655ms step_avg:43.62ms
step:956/1900 train_time:41716ms step_avg:43.64ms
step:957/1900 train_time:41778ms step_avg:43.65ms
step:958/1900 train_time:41839ms step_avg:43.67ms
step:959/1900 train_time:41900ms step_avg:43.69ms
step:960/1900 train_time:41961ms step_avg:43.71ms
step:961/1900 train_time:42023ms step_avg:43.73ms
step:962/1900 train_time:42084ms step_avg:43.75ms
step:963/1900 train_time:42146ms step_avg:43.77ms
step:964/1900 train_time:42207ms step_avg:43.78ms
step:965/1900 train_time:42268ms step_avg:43.80ms
step:966/1900 train_time:42329ms step_avg:43.82ms
step:967/1900 train_time:42391ms step_avg:43.84ms
step:968/1900 train_time:42451ms step_avg:43.85ms
step:969/1900 train_time:42513ms step_avg:43.87ms
step:970/1900 train_time:42574ms step_avg:43.89ms
step:971/1900 train_time:42636ms step_avg:43.91ms
step:972/1900 train_time:42697ms step_avg:43.93ms
step:973/1900 train_time:42759ms step_avg:43.95ms
step:974/1900 train_time:42819ms step_avg:43.96ms
step:975/1900 train_time:42881ms step_avg:43.98ms
step:976/1900 train_time:42942ms step_avg:44.00ms
step:977/1900 train_time:43004ms step_avg:44.02ms
step:978/1900 train_time:43065ms step_avg:44.03ms
step:979/1900 train_time:43127ms step_avg:44.05ms
step:980/1900 train_time:43188ms step_avg:44.07ms
step:981/1900 train_time:43250ms step_avg:44.09ms
step:982/1900 train_time:43311ms step_avg:44.10ms
step:983/1900 train_time:43372ms step_avg:44.12ms
step:984/1900 train_time:43433ms step_avg:44.14ms
step:985/1900 train_time:43495ms step_avg:44.16ms
step:986/1900 train_time:43556ms step_avg:44.17ms
step:987/1900 train_time:43618ms step_avg:44.19ms
step:988/1900 train_time:43678ms step_avg:44.21ms
step:989/1900 train_time:43740ms step_avg:44.23ms
step:990/1900 train_time:43801ms step_avg:44.24ms
step:991/1900 train_time:43863ms step_avg:44.26ms
step:992/1900 train_time:43925ms step_avg:44.28ms
step:993/1900 train_time:43986ms step_avg:44.30ms
step:994/1900 train_time:44047ms step_avg:44.31ms
step:995/1900 train_time:44109ms step_avg:44.33ms
step:996/1900 train_time:44170ms step_avg:44.35ms
step:997/1900 train_time:44231ms step_avg:44.36ms
step:998/1900 train_time:44293ms step_avg:44.38ms
step:999/1900 train_time:44354ms step_avg:44.40ms
step:1000/1900 train_time:44414ms step_avg:44.41ms
step:1000/1900 val_loss:3.7838 train_time:44479ms step_avg:44.48ms
step:1001/1900 train_time:44499ms step_avg:44.45ms
step:1002/1900 train_time:44542ms step_avg:44.45ms
step:1003/1900 train_time:44605ms step_avg:44.47ms
step:1004/1900 train_time:44669ms step_avg:44.49ms
step:1005/1900 train_time:44730ms step_avg:44.51ms
step:1006/1900 train_time:44792ms step_avg:44.52ms
step:1007/1900 train_time:44853ms step_avg:44.54ms
step:1008/1900 train_time:44913ms step_avg:44.56ms
step:1009/1900 train_time:44974ms step_avg:44.57ms
step:1010/1900 train_time:45035ms step_avg:44.59ms
step:1011/1900 train_time:45096ms step_avg:44.61ms
step:1012/1900 train_time:45156ms step_avg:44.62ms
step:1013/1900 train_time:45217ms step_avg:44.64ms
step:1014/1900 train_time:45277ms step_avg:44.65ms
step:1015/1900 train_time:45338ms step_avg:44.67ms
step:1016/1900 train_time:45400ms step_avg:44.68ms
step:1017/1900 train_time:45463ms step_avg:44.70ms
step:1018/1900 train_time:45524ms step_avg:44.72ms
step:1019/1900 train_time:45587ms step_avg:44.74ms
step:1020/1900 train_time:45649ms step_avg:44.75ms
step:1021/1900 train_time:45711ms step_avg:44.77ms
step:1022/1900 train_time:45772ms step_avg:44.79ms
step:1023/1900 train_time:45834ms step_avg:44.80ms
step:1024/1900 train_time:45894ms step_avg:44.82ms
step:1025/1900 train_time:45955ms step_avg:44.83ms
step:1026/1900 train_time:46015ms step_avg:44.85ms
step:1027/1900 train_time:46077ms step_avg:44.87ms
step:1028/1900 train_time:46137ms step_avg:44.88ms
step:1029/1900 train_time:46199ms step_avg:44.90ms
step:1030/1900 train_time:46259ms step_avg:44.91ms
step:1031/1900 train_time:46321ms step_avg:44.93ms
step:1032/1900 train_time:46383ms step_avg:44.95ms
step:1033/1900 train_time:46446ms step_avg:44.96ms
step:1034/1900 train_time:46508ms step_avg:44.98ms
step:1035/1900 train_time:46570ms step_avg:45.00ms
step:1036/1900 train_time:46631ms step_avg:45.01ms
step:1037/1900 train_time:46693ms step_avg:45.03ms
step:1038/1900 train_time:46754ms step_avg:45.04ms
step:1039/1900 train_time:46817ms step_avg:45.06ms
step:1040/1900 train_time:46878ms step_avg:45.07ms
step:1041/1900 train_time:46940ms step_avg:45.09ms
step:1042/1900 train_time:47001ms step_avg:45.11ms
step:1043/1900 train_time:47063ms step_avg:45.12ms
step:1044/1900 train_time:47123ms step_avg:45.14ms
step:1045/1900 train_time:47185ms step_avg:45.15ms
step:1046/1900 train_time:47246ms step_avg:45.17ms
step:1047/1900 train_time:47308ms step_avg:45.18ms
step:1048/1900 train_time:47369ms step_avg:45.20ms
step:1049/1900 train_time:47431ms step_avg:45.22ms
step:1050/1900 train_time:47492ms step_avg:45.23ms
step:1051/1900 train_time:47555ms step_avg:45.25ms
step:1052/1900 train_time:47616ms step_avg:45.26ms
step:1053/1900 train_time:47678ms step_avg:45.28ms
step:1054/1900 train_time:47740ms step_avg:45.29ms
step:1055/1900 train_time:47802ms step_avg:45.31ms
step:1056/1900 train_time:47863ms step_avg:45.32ms
step:1057/1900 train_time:47925ms step_avg:45.34ms
step:1058/1900 train_time:47986ms step_avg:45.36ms
step:1059/1900 train_time:48047ms step_avg:45.37ms
step:1060/1900 train_time:48108ms step_avg:45.39ms
step:1061/1900 train_time:48170ms step_avg:45.40ms
step:1062/1900 train_time:48231ms step_avg:45.41ms
step:1063/1900 train_time:48293ms step_avg:45.43ms
step:1064/1900 train_time:48353ms step_avg:45.44ms
step:1065/1900 train_time:48416ms step_avg:45.46ms
step:1066/1900 train_time:48476ms step_avg:45.48ms
step:1067/1900 train_time:48538ms step_avg:45.49ms
step:1068/1900 train_time:48600ms step_avg:45.51ms
step:1069/1900 train_time:48662ms step_avg:45.52ms
step:1070/1900 train_time:48723ms step_avg:45.54ms
step:1071/1900 train_time:48785ms step_avg:45.55ms
step:1072/1900 train_time:48847ms step_avg:45.57ms
step:1073/1900 train_time:48909ms step_avg:45.58ms
step:1074/1900 train_time:48969ms step_avg:45.60ms
step:1075/1900 train_time:49031ms step_avg:45.61ms
step:1076/1900 train_time:49092ms step_avg:45.62ms
step:1077/1900 train_time:49154ms step_avg:45.64ms
step:1078/1900 train_time:49214ms step_avg:45.65ms
step:1079/1900 train_time:49276ms step_avg:45.67ms
step:1080/1900 train_time:49337ms step_avg:45.68ms
step:1081/1900 train_time:49399ms step_avg:45.70ms
step:1082/1900 train_time:49460ms step_avg:45.71ms
step:1083/1900 train_time:49522ms step_avg:45.73ms
step:1084/1900 train_time:49583ms step_avg:45.74ms
step:1085/1900 train_time:49646ms step_avg:45.76ms
step:1086/1900 train_time:49707ms step_avg:45.77ms
step:1087/1900 train_time:49769ms step_avg:45.79ms
step:1088/1900 train_time:49830ms step_avg:45.80ms
step:1089/1900 train_time:49892ms step_avg:45.81ms
step:1090/1900 train_time:49952ms step_avg:45.83ms
step:1091/1900 train_time:50014ms step_avg:45.84ms
step:1092/1900 train_time:50075ms step_avg:45.86ms
step:1093/1900 train_time:50136ms step_avg:45.87ms
step:1094/1900 train_time:50197ms step_avg:45.88ms
step:1095/1900 train_time:50258ms step_avg:45.90ms
step:1096/1900 train_time:50319ms step_avg:45.91ms
step:1097/1900 train_time:50380ms step_avg:45.93ms
step:1098/1900 train_time:50442ms step_avg:45.94ms
step:1099/1900 train_time:50504ms step_avg:45.95ms
step:1100/1900 train_time:50565ms step_avg:45.97ms
step:1101/1900 train_time:50627ms step_avg:45.98ms
step:1102/1900 train_time:50688ms step_avg:46.00ms
step:1103/1900 train_time:50750ms step_avg:46.01ms
step:1104/1900 train_time:50812ms step_avg:46.02ms
step:1105/1900 train_time:50874ms step_avg:46.04ms
step:1106/1900 train_time:50935ms step_avg:46.05ms
step:1107/1900 train_time:50997ms step_avg:46.07ms
step:1108/1900 train_time:51057ms step_avg:46.08ms
step:1109/1900 train_time:51119ms step_avg:46.09ms
step:1110/1900 train_time:51180ms step_avg:46.11ms
step:1111/1900 train_time:51242ms step_avg:46.12ms
step:1112/1900 train_time:51303ms step_avg:46.14ms
step:1113/1900 train_time:51365ms step_avg:46.15ms
step:1114/1900 train_time:51426ms step_avg:46.16ms
step:1115/1900 train_time:51488ms step_avg:46.18ms
step:1116/1900 train_time:51549ms step_avg:46.19ms
step:1117/1900 train_time:51611ms step_avg:46.21ms
step:1118/1900 train_time:51672ms step_avg:46.22ms
step:1119/1900 train_time:51734ms step_avg:46.23ms
step:1120/1900 train_time:51795ms step_avg:46.25ms
step:1121/1900 train_time:51857ms step_avg:46.26ms
step:1122/1900 train_time:51918ms step_avg:46.27ms
step:1123/1900 train_time:51980ms step_avg:46.29ms
step:1124/1900 train_time:52041ms step_avg:46.30ms
step:1125/1900 train_time:52103ms step_avg:46.31ms
step:1126/1900 train_time:52164ms step_avg:46.33ms
step:1127/1900 train_time:52225ms step_avg:46.34ms
step:1128/1900 train_time:52286ms step_avg:46.35ms
step:1129/1900 train_time:52348ms step_avg:46.37ms
step:1130/1900 train_time:52409ms step_avg:46.38ms
step:1131/1900 train_time:52471ms step_avg:46.39ms
step:1132/1900 train_time:52531ms step_avg:46.41ms
step:1133/1900 train_time:52593ms step_avg:46.42ms
step:1134/1900 train_time:52654ms step_avg:46.43ms
step:1135/1900 train_time:52716ms step_avg:46.45ms
step:1136/1900 train_time:52776ms step_avg:46.46ms
step:1137/1900 train_time:52839ms step_avg:46.47ms
step:1138/1900 train_time:52899ms step_avg:46.48ms
step:1139/1900 train_time:52961ms step_avg:46.50ms
step:1140/1900 train_time:53022ms step_avg:46.51ms
step:1141/1900 train_time:53084ms step_avg:46.52ms
step:1142/1900 train_time:53144ms step_avg:46.54ms
step:1143/1900 train_time:53206ms step_avg:46.55ms
step:1144/1900 train_time:53267ms step_avg:46.56ms
step:1145/1900 train_time:53328ms step_avg:46.57ms
step:1146/1900 train_time:53389ms step_avg:46.59ms
step:1147/1900 train_time:53451ms step_avg:46.60ms
step:1148/1900 train_time:53512ms step_avg:46.61ms
step:1149/1900 train_time:53573ms step_avg:46.63ms
step:1150/1900 train_time:53634ms step_avg:46.64ms
step:1151/1900 train_time:53696ms step_avg:46.65ms
step:1152/1900 train_time:53757ms step_avg:46.66ms
step:1153/1900 train_time:53819ms step_avg:46.68ms
step:1154/1900 train_time:53880ms step_avg:46.69ms
step:1155/1900 train_time:53942ms step_avg:46.70ms
step:1156/1900 train_time:54003ms step_avg:46.72ms
step:1157/1900 train_time:54065ms step_avg:46.73ms
step:1158/1900 train_time:54126ms step_avg:46.74ms
step:1159/1900 train_time:54187ms step_avg:46.75ms
step:1160/1900 train_time:54248ms step_avg:46.77ms
step:1161/1900 train_time:54309ms step_avg:46.78ms
step:1162/1900 train_time:54371ms step_avg:46.79ms
step:1163/1900 train_time:54432ms step_avg:46.80ms
step:1164/1900 train_time:54493ms step_avg:46.82ms
step:1165/1900 train_time:54555ms step_avg:46.83ms
step:1166/1900 train_time:54616ms step_avg:46.84ms
step:1167/1900 train_time:54677ms step_avg:46.85ms
step:1168/1900 train_time:54738ms step_avg:46.86ms
step:1169/1900 train_time:54800ms step_avg:46.88ms
step:1170/1900 train_time:54861ms step_avg:46.89ms
step:1171/1900 train_time:54923ms step_avg:46.90ms
step:1172/1900 train_time:54984ms step_avg:46.91ms
step:1173/1900 train_time:55046ms step_avg:46.93ms
step:1174/1900 train_time:55107ms step_avg:46.94ms
step:1175/1900 train_time:55168ms step_avg:46.95ms
step:1176/1900 train_time:55229ms step_avg:46.96ms
step:1177/1900 train_time:55291ms step_avg:46.98ms
step:1178/1900 train_time:55352ms step_avg:46.99ms
step:1179/1900 train_time:55414ms step_avg:47.00ms
step:1180/1900 train_time:55474ms step_avg:47.01ms
step:1181/1900 train_time:55536ms step_avg:47.02ms
step:1182/1900 train_time:55597ms step_avg:47.04ms
step:1183/1900 train_time:55659ms step_avg:47.05ms
step:1184/1900 train_time:55720ms step_avg:47.06ms
step:1185/1900 train_time:55782ms step_avg:47.07ms
step:1186/1900 train_time:55843ms step_avg:47.09ms
step:1187/1900 train_time:55905ms step_avg:47.10ms
step:1188/1900 train_time:55966ms step_avg:47.11ms
step:1189/1900 train_time:56028ms step_avg:47.12ms
step:1190/1900 train_time:56089ms step_avg:47.13ms
step:1191/1900 train_time:56151ms step_avg:47.15ms
step:1192/1900 train_time:56212ms step_avg:47.16ms
step:1193/1900 train_time:56274ms step_avg:47.17ms
step:1194/1900 train_time:56335ms step_avg:47.18ms
step:1195/1900 train_time:56397ms step_avg:47.19ms
step:1196/1900 train_time:56457ms step_avg:47.21ms
step:1197/1900 train_time:56519ms step_avg:47.22ms
step:1198/1900 train_time:56580ms step_avg:47.23ms
step:1199/1900 train_time:56642ms step_avg:47.24ms
step:1200/1900 train_time:56703ms step_avg:47.25ms
step:1201/1900 train_time:56765ms step_avg:47.26ms
step:1202/1900 train_time:56825ms step_avg:47.28ms
step:1203/1900 train_time:56887ms step_avg:47.29ms
step:1204/1900 train_time:56948ms step_avg:47.30ms
step:1205/1900 train_time:57010ms step_avg:47.31ms
step:1206/1900 train_time:57071ms step_avg:47.32ms
step:1207/1900 train_time:57133ms step_avg:47.33ms
step:1208/1900 train_time:57193ms step_avg:47.35ms
step:1209/1900 train_time:57255ms step_avg:47.36ms
step:1210/1900 train_time:57316ms step_avg:47.37ms
step:1211/1900 train_time:57378ms step_avg:47.38ms
step:1212/1900 train_time:57439ms step_avg:47.39ms
step:1213/1900 train_time:57500ms step_avg:47.40ms
step:1214/1900 train_time:57561ms step_avg:47.41ms
step:1215/1900 train_time:57623ms step_avg:47.43ms
step:1216/1900 train_time:57684ms step_avg:47.44ms
step:1217/1900 train_time:57746ms step_avg:47.45ms
step:1218/1900 train_time:57807ms step_avg:47.46ms
step:1219/1900 train_time:57869ms step_avg:47.47ms
step:1220/1900 train_time:57930ms step_avg:47.48ms
step:1221/1900 train_time:57992ms step_avg:47.50ms
step:1222/1900 train_time:58053ms step_avg:47.51ms
step:1223/1900 train_time:58115ms step_avg:47.52ms
step:1224/1900 train_time:58176ms step_avg:47.53ms
step:1225/1900 train_time:58238ms step_avg:47.54ms
step:1226/1900 train_time:58299ms step_avg:47.55ms
step:1227/1900 train_time:58361ms step_avg:47.56ms
step:1228/1900 train_time:58422ms step_avg:47.57ms
step:1229/1900 train_time:58484ms step_avg:47.59ms
step:1230/1900 train_time:58545ms step_avg:47.60ms
step:1231/1900 train_time:58607ms step_avg:47.61ms
step:1232/1900 train_time:58668ms step_avg:47.62ms
step:1233/1900 train_time:58730ms step_avg:47.63ms
step:1234/1900 train_time:58791ms step_avg:47.64ms
step:1235/1900 train_time:58853ms step_avg:47.65ms
step:1236/1900 train_time:58914ms step_avg:47.66ms
step:1237/1900 train_time:58975ms step_avg:47.68ms
step:1238/1900 train_time:59036ms step_avg:47.69ms
step:1239/1900 train_time:59098ms step_avg:47.70ms
step:1240/1900 train_time:59158ms step_avg:47.71ms
step:1241/1900 train_time:59221ms step_avg:47.72ms
step:1242/1900 train_time:59308ms step_avg:47.75ms
step:1243/1900 train_time:59396ms step_avg:47.78ms
step:1244/1900 train_time:59484ms step_avg:47.82ms
step:1245/1900 train_time:59572ms step_avg:47.85ms
step:1246/1900 train_time:59660ms step_avg:47.88ms
step:1247/1900 train_time:59748ms step_avg:47.91ms
step:1248/1900 train_time:59835ms step_avg:47.95ms
step:1249/1900 train_time:59924ms step_avg:47.98ms
step:1250/1900 train_time:60011ms step_avg:48.01ms
step:1250/1900 val_loss:3.5433 train_time:60102ms step_avg:48.08ms
step:1251/1900 train_time:60122ms step_avg:48.06ms
step:1252/1900 train_time:60190ms step_avg:48.08ms
step:1253/1900 train_time:60284ms step_avg:48.11ms
step:1254/1900 train_time:60371ms step_avg:48.14ms
step:1255/1900 train_time:60459ms step_avg:48.17ms
step:1256/1900 train_time:60546ms step_avg:48.21ms
step:1257/1900 train_time:60633ms step_avg:48.24ms
step:1258/1900 train_time:60720ms step_avg:48.27ms
step:1259/1900 train_time:60807ms step_avg:48.30ms
step:1260/1900 train_time:60894ms step_avg:48.33ms
step:1261/1900 train_time:60982ms step_avg:48.36ms
step:1262/1900 train_time:61071ms step_avg:48.39ms
step:1263/1900 train_time:61162ms step_avg:48.43ms
step:1264/1900 train_time:61253ms step_avg:48.46ms
step:1265/1900 train_time:61342ms step_avg:48.49ms
step:1266/1900 train_time:61429ms step_avg:48.52ms
step:1267/1900 train_time:61517ms step_avg:48.55ms
step:1268/1900 train_time:61603ms step_avg:48.58ms
step:1269/1900 train_time:61691ms step_avg:48.61ms
step:1270/1900 train_time:61777ms step_avg:48.64ms
step:1271/1900 train_time:61865ms step_avg:48.67ms
step:1272/1900 train_time:61951ms step_avg:48.70ms
step:1273/1900 train_time:62041ms step_avg:48.74ms
step:1274/1900 train_time:62128ms step_avg:48.77ms
step:1275/1900 train_time:62219ms step_avg:48.80ms
step:1276/1900 train_time:62308ms step_avg:48.83ms
step:1277/1900 train_time:62398ms step_avg:48.86ms
step:1278/1900 train_time:62485ms step_avg:48.89ms
step:1279/1900 train_time:62573ms step_avg:48.92ms
step:1280/1900 train_time:62660ms step_avg:48.95ms
step:1281/1900 train_time:62747ms step_avg:48.98ms
step:1282/1900 train_time:62834ms step_avg:49.01ms
step:1283/1900 train_time:62923ms step_avg:49.04ms
step:1284/1900 train_time:63010ms step_avg:49.07ms
step:1285/1900 train_time:63099ms step_avg:49.10ms
step:1286/1900 train_time:63188ms step_avg:49.14ms
step:1287/1900 train_time:63278ms step_avg:49.17ms
step:1288/1900 train_time:63366ms step_avg:49.20ms
step:1289/1900 train_time:63455ms step_avg:49.23ms
step:1290/1900 train_time:63542ms step_avg:49.26ms
step:1291/1900 train_time:63630ms step_avg:49.29ms
step:1292/1900 train_time:63716ms step_avg:49.32ms
step:1293/1900 train_time:63804ms step_avg:49.35ms
step:1294/1900 train_time:63892ms step_avg:49.38ms
step:1295/1900 train_time:63979ms step_avg:49.40ms
step:1296/1900 train_time:64067ms step_avg:49.43ms
step:1297/1900 train_time:64156ms step_avg:49.46ms
step:1298/1900 train_time:64244ms step_avg:49.49ms
step:1299/1900 train_time:64333ms step_avg:49.52ms
step:1300/1900 train_time:64421ms step_avg:49.55ms
step:1301/1900 train_time:64510ms step_avg:49.58ms
step:1302/1900 train_time:64598ms step_avg:49.61ms
step:1303/1900 train_time:64686ms step_avg:49.64ms
step:1304/1900 train_time:64772ms step_avg:49.67ms
step:1305/1900 train_time:64861ms step_avg:49.70ms
step:1306/1900 train_time:64948ms step_avg:49.73ms
step:1307/1900 train_time:65036ms step_avg:49.76ms
step:1308/1900 train_time:65124ms step_avg:49.79ms
step:1309/1900 train_time:65212ms step_avg:49.82ms
step:1310/1900 train_time:65301ms step_avg:49.85ms
step:1311/1900 train_time:65390ms step_avg:49.88ms
step:1312/1900 train_time:65478ms step_avg:49.91ms
step:1313/1900 train_time:65567ms step_avg:49.94ms
step:1314/1900 train_time:65655ms step_avg:49.97ms
step:1315/1900 train_time:65743ms step_avg:49.99ms
step:1316/1900 train_time:65829ms step_avg:50.02ms
step:1317/1900 train_time:65918ms step_avg:50.05ms
step:1318/1900 train_time:66005ms step_avg:50.08ms
step:1319/1900 train_time:66094ms step_avg:50.11ms
step:1320/1900 train_time:66181ms step_avg:50.14ms
step:1321/1900 train_time:66269ms step_avg:50.17ms
step:1322/1900 train_time:66357ms step_avg:50.19ms
step:1323/1900 train_time:66446ms step_avg:50.22ms
step:1324/1900 train_time:66534ms step_avg:50.25ms
step:1325/1900 train_time:66622ms step_avg:50.28ms
step:1326/1900 train_time:66709ms step_avg:50.31ms
step:1327/1900 train_time:66798ms step_avg:50.34ms
step:1328/1900 train_time:66885ms step_avg:50.37ms
step:1329/1900 train_time:66974ms step_avg:50.39ms
step:1330/1900 train_time:67061ms step_avg:50.42ms
step:1331/1900 train_time:67149ms step_avg:50.45ms
step:1332/1900 train_time:67237ms step_avg:50.48ms
step:1333/1900 train_time:67326ms step_avg:50.51ms
step:1334/1900 train_time:67414ms step_avg:50.53ms
step:1335/1900 train_time:67502ms step_avg:50.56ms
step:1336/1900 train_time:67590ms step_avg:50.59ms
step:1337/1900 train_time:67678ms step_avg:50.62ms
step:1338/1900 train_time:67765ms step_avg:50.65ms
step:1339/1900 train_time:67854ms step_avg:50.68ms
step:1340/1900 train_time:67942ms step_avg:50.70ms
step:1341/1900 train_time:68030ms step_avg:50.73ms
step:1342/1900 train_time:68118ms step_avg:50.76ms
step:1343/1900 train_time:68206ms step_avg:50.79ms
step:1344/1900 train_time:68294ms step_avg:50.81ms
step:1345/1900 train_time:68382ms step_avg:50.84ms
step:1346/1900 train_time:68469ms step_avg:50.87ms
step:1347/1900 train_time:68558ms step_avg:50.90ms
step:1348/1900 train_time:68645ms step_avg:50.92ms
step:1349/1900 train_time:68733ms step_avg:50.95ms
step:1350/1900 train_time:68821ms step_avg:50.98ms
step:1351/1900 train_time:68909ms step_avg:51.01ms
step:1352/1900 train_time:68996ms step_avg:51.03ms
step:1353/1900 train_time:69085ms step_avg:51.06ms
step:1354/1900 train_time:69172ms step_avg:51.09ms
step:1355/1900 train_time:69261ms step_avg:51.11ms
step:1356/1900 train_time:69348ms step_avg:51.14ms
step:1357/1900 train_time:69437ms step_avg:51.17ms
step:1358/1900 train_time:69525ms step_avg:51.20ms
step:1359/1900 train_time:69613ms step_avg:51.22ms
step:1360/1900 train_time:69701ms step_avg:51.25ms
step:1361/1900 train_time:69789ms step_avg:51.28ms
step:1362/1900 train_time:69877ms step_avg:51.30ms
step:1363/1900 train_time:69965ms step_avg:51.33ms
step:1364/1900 train_time:70054ms step_avg:51.36ms
step:1365/1900 train_time:70141ms step_avg:51.39ms
step:1366/1900 train_time:70229ms step_avg:51.41ms
step:1367/1900 train_time:70318ms step_avg:51.44ms
step:1368/1900 train_time:70406ms step_avg:51.47ms
step:1369/1900 train_time:70494ms step_avg:51.49ms
step:1370/1900 train_time:70581ms step_avg:51.52ms
step:1371/1900 train_time:70670ms step_avg:51.55ms
step:1372/1900 train_time:70758ms step_avg:51.57ms
step:1373/1900 train_time:70846ms step_avg:51.60ms
step:1374/1900 train_time:70934ms step_avg:51.63ms
step:1375/1900 train_time:71022ms step_avg:51.65ms
step:1376/1900 train_time:71109ms step_avg:51.68ms
step:1377/1900 train_time:71198ms step_avg:51.70ms
step:1378/1900 train_time:71285ms step_avg:51.73ms
step:1379/1900 train_time:71373ms step_avg:51.76ms
step:1380/1900 train_time:71461ms step_avg:51.78ms
step:1381/1900 train_time:71549ms step_avg:51.81ms
step:1382/1900 train_time:71638ms step_avg:51.84ms
step:1383/1900 train_time:71726ms step_avg:51.86ms
step:1384/1900 train_time:71813ms step_avg:51.89ms
step:1385/1900 train_time:71903ms step_avg:51.92ms
step:1386/1900 train_time:71989ms step_avg:51.94ms
step:1387/1900 train_time:72077ms step_avg:51.97ms
step:1388/1900 train_time:72165ms step_avg:51.99ms
step:1389/1900 train_time:72254ms step_avg:52.02ms
step:1390/1900 train_time:72341ms step_avg:52.04ms
step:1391/1900 train_time:72429ms step_avg:52.07ms
step:1392/1900 train_time:72517ms step_avg:52.10ms
step:1393/1900 train_time:72606ms step_avg:52.12ms
step:1394/1900 train_time:72694ms step_avg:52.15ms
step:1395/1900 train_time:72782ms step_avg:52.17ms
step:1396/1900 train_time:72870ms step_avg:52.20ms
step:1397/1900 train_time:72958ms step_avg:52.23ms
step:1398/1900 train_time:73046ms step_avg:52.25ms
step:1399/1900 train_time:73134ms step_avg:52.28ms
step:1400/1900 train_time:73223ms step_avg:52.30ms
step:1401/1900 train_time:73311ms step_avg:52.33ms
step:1402/1900 train_time:73399ms step_avg:52.35ms
step:1403/1900 train_time:73487ms step_avg:52.38ms
step:1404/1900 train_time:73575ms step_avg:52.40ms
step:1405/1900 train_time:73663ms step_avg:52.43ms
step:1406/1900 train_time:73750ms step_avg:52.45ms
step:1407/1900 train_time:73839ms step_avg:52.48ms
step:1408/1900 train_time:73926ms step_avg:52.50ms
step:1409/1900 train_time:74015ms step_avg:52.53ms
step:1410/1900 train_time:74102ms step_avg:52.55ms
step:1411/1900 train_time:74190ms step_avg:52.58ms
step:1412/1900 train_time:74277ms step_avg:52.60ms
step:1413/1900 train_time:74365ms step_avg:52.63ms
step:1414/1900 train_time:74452ms step_avg:52.65ms
step:1415/1900 train_time:74541ms step_avg:52.68ms
step:1416/1900 train_time:74629ms step_avg:52.70ms
step:1417/1900 train_time:74717ms step_avg:52.73ms
step:1418/1900 train_time:74805ms step_avg:52.75ms
step:1419/1900 train_time:74894ms step_avg:52.78ms
step:1420/1900 train_time:74981ms step_avg:52.80ms
step:1421/1900 train_time:75069ms step_avg:52.83ms
step:1422/1900 train_time:75157ms step_avg:52.85ms
step:1423/1900 train_time:75245ms step_avg:52.88ms
step:1424/1900 train_time:75333ms step_avg:52.90ms
step:1425/1900 train_time:75422ms step_avg:52.93ms
step:1426/1900 train_time:75510ms step_avg:52.95ms
step:1427/1900 train_time:75599ms step_avg:52.98ms
step:1428/1900 train_time:75686ms step_avg:53.00ms
step:1429/1900 train_time:75775ms step_avg:53.03ms
step:1430/1900 train_time:75863ms step_avg:53.05ms
step:1431/1900 train_time:75951ms step_avg:53.08ms
step:1432/1900 train_time:76038ms step_avg:53.10ms
step:1433/1900 train_time:76126ms step_avg:53.12ms
step:1434/1900 train_time:76213ms step_avg:53.15ms
step:1435/1900 train_time:76302ms step_avg:53.17ms
step:1436/1900 train_time:76389ms step_avg:53.20ms
step:1437/1900 train_time:76478ms step_avg:53.22ms
step:1438/1900 train_time:76565ms step_avg:53.24ms
step:1439/1900 train_time:76654ms step_avg:53.27ms
step:1440/1900 train_time:76742ms step_avg:53.29ms
step:1441/1900 train_time:76830ms step_avg:53.32ms
step:1442/1900 train_time:76917ms step_avg:53.34ms
step:1443/1900 train_time:77005ms step_avg:53.36ms
step:1444/1900 train_time:77093ms step_avg:53.39ms
step:1445/1900 train_time:77181ms step_avg:53.41ms
step:1446/1900 train_time:77268ms step_avg:53.44ms
step:1447/1900 train_time:77357ms step_avg:53.46ms
step:1448/1900 train_time:77445ms step_avg:53.48ms
step:1449/1900 train_time:77534ms step_avg:53.51ms
step:1450/1900 train_time:77622ms step_avg:53.53ms
step:1451/1900 train_time:77710ms step_avg:53.56ms
step:1452/1900 train_time:77798ms step_avg:53.58ms
step:1453/1900 train_time:77886ms step_avg:53.60ms
step:1454/1900 train_time:77973ms step_avg:53.63ms
step:1455/1900 train_time:78062ms step_avg:53.65ms
step:1456/1900 train_time:78149ms step_avg:53.67ms
step:1457/1900 train_time:78238ms step_avg:53.70ms
step:1458/1900 train_time:78325ms step_avg:53.72ms
step:1459/1900 train_time:78413ms step_avg:53.74ms
step:1460/1900 train_time:78501ms step_avg:53.77ms
step:1461/1900 train_time:78589ms step_avg:53.79ms
step:1462/1900 train_time:78677ms step_avg:53.81ms
step:1463/1900 train_time:78767ms step_avg:53.84ms
step:1464/1900 train_time:78855ms step_avg:53.86ms
step:1465/1900 train_time:78943ms step_avg:53.89ms
step:1466/1900 train_time:79030ms step_avg:53.91ms
step:1467/1900 train_time:79119ms step_avg:53.93ms
step:1468/1900 train_time:79207ms step_avg:53.96ms
step:1469/1900 train_time:79296ms step_avg:53.98ms
step:1470/1900 train_time:79383ms step_avg:54.00ms
step:1471/1900 train_time:79472ms step_avg:54.03ms
step:1472/1900 train_time:79559ms step_avg:54.05ms
step:1473/1900 train_time:79649ms step_avg:54.07ms
step:1474/1900 train_time:79736ms step_avg:54.09ms
step:1475/1900 train_time:79825ms step_avg:54.12ms
step:1476/1900 train_time:79912ms step_avg:54.14ms
step:1477/1900 train_time:80001ms step_avg:54.16ms
step:1478/1900 train_time:80088ms step_avg:54.19ms
step:1479/1900 train_time:80176ms step_avg:54.21ms
step:1480/1900 train_time:80264ms step_avg:54.23ms
step:1481/1900 train_time:80352ms step_avg:54.25ms
step:1482/1900 train_time:80439ms step_avg:54.28ms
step:1483/1900 train_time:80527ms step_avg:54.30ms
step:1484/1900 train_time:80615ms step_avg:54.32ms
step:1485/1900 train_time:80703ms step_avg:54.35ms
step:1486/1900 train_time:80790ms step_avg:54.37ms
step:1487/1900 train_time:80880ms step_avg:54.39ms
step:1488/1900 train_time:80967ms step_avg:54.41ms
step:1489/1900 train_time:81056ms step_avg:54.44ms
step:1490/1900 train_time:81143ms step_avg:54.46ms
step:1491/1900 train_time:81232ms step_avg:54.48ms
step:1492/1900 train_time:81319ms step_avg:54.50ms
step:1493/1900 train_time:81407ms step_avg:54.53ms
step:1494/1900 train_time:81495ms step_avg:54.55ms
step:1495/1900 train_time:81584ms step_avg:54.57ms
step:1496/1900 train_time:81671ms step_avg:54.59ms
step:1497/1900 train_time:81760ms step_avg:54.62ms
step:1498/1900 train_time:81848ms step_avg:54.64ms
step:1499/1900 train_time:81937ms step_avg:54.66ms
step:1500/1900 train_time:82024ms step_avg:54.68ms
step:1500/1900 val_loss:3.4131 train_time:82115ms step_avg:54.74ms
step:1501/1900 train_time:82135ms step_avg:54.72ms
step:1502/1900 train_time:82202ms step_avg:54.73ms
step:1503/1900 train_time:82295ms step_avg:54.75ms
step:1504/1900 train_time:82383ms step_avg:54.78ms
step:1505/1900 train_time:82470ms step_avg:54.80ms
step:1506/1900 train_time:82557ms step_avg:54.82ms
step:1507/1900 train_time:82645ms step_avg:54.84ms
step:1508/1900 train_time:82731ms step_avg:54.86ms
step:1509/1900 train_time:82818ms step_avg:54.88ms
step:1510/1900 train_time:82905ms step_avg:54.90ms
step:1511/1900 train_time:82993ms step_avg:54.93ms
step:1512/1900 train_time:83082ms step_avg:54.95ms
step:1513/1900 train_time:83172ms step_avg:54.97ms
step:1514/1900 train_time:83261ms step_avg:54.99ms
step:1515/1900 train_time:83350ms step_avg:55.02ms
step:1516/1900 train_time:83437ms step_avg:55.04ms
step:1517/1900 train_time:83526ms step_avg:55.06ms
step:1518/1900 train_time:83612ms step_avg:55.08ms
step:1519/1900 train_time:83701ms step_avg:55.10ms
step:1520/1900 train_time:83786ms step_avg:55.12ms
step:1521/1900 train_time:83874ms step_avg:55.14ms
step:1522/1900 train_time:83961ms step_avg:55.16ms
step:1523/1900 train_time:84049ms step_avg:55.19ms
step:1524/1900 train_time:84138ms step_avg:55.21ms
step:1525/1900 train_time:84227ms step_avg:55.23ms
step:1526/1900 train_time:84316ms step_avg:55.25ms
step:1527/1900 train_time:84404ms step_avg:55.27ms
step:1528/1900 train_time:84491ms step_avg:55.30ms
step:1529/1900 train_time:84579ms step_avg:55.32ms
step:1530/1900 train_time:84666ms step_avg:55.34ms
step:1531/1900 train_time:84753ms step_avg:55.36ms
step:1532/1900 train_time:84840ms step_avg:55.38ms
step:1533/1900 train_time:84929ms step_avg:55.40ms
step:1534/1900 train_time:85016ms step_avg:55.42ms
step:1535/1900 train_time:85105ms step_avg:55.44ms
step:1536/1900 train_time:85194ms step_avg:55.46ms
step:1537/1900 train_time:85282ms step_avg:55.49ms
step:1538/1900 train_time:85370ms step_avg:55.51ms
step:1539/1900 train_time:85458ms step_avg:55.53ms
step:1540/1900 train_time:85546ms step_avg:55.55ms
step:1541/1900 train_time:85634ms step_avg:55.57ms
step:1542/1900 train_time:85722ms step_avg:55.59ms
step:1543/1900 train_time:85809ms step_avg:55.61ms
step:1544/1900 train_time:85897ms step_avg:55.63ms
step:1545/1900 train_time:85986ms step_avg:55.65ms
step:1546/1900 train_time:86074ms step_avg:55.68ms
step:1547/1900 train_time:86164ms step_avg:55.70ms
step:1548/1900 train_time:86252ms step_avg:55.72ms
step:1549/1900 train_time:86341ms step_avg:55.74ms
step:1550/1900 train_time:86428ms step_avg:55.76ms
step:1551/1900 train_time:86516ms step_avg:55.78ms
step:1552/1900 train_time:86604ms step_avg:55.80ms
step:1553/1900 train_time:86693ms step_avg:55.82ms
step:1554/1900 train_time:86779ms step_avg:55.84ms
step:1555/1900 train_time:86868ms step_avg:55.86ms
step:1556/1900 train_time:86955ms step_avg:55.88ms
step:1557/1900 train_time:87044ms step_avg:55.91ms
step:1558/1900 train_time:87132ms step_avg:55.93ms
step:1559/1900 train_time:87221ms step_avg:55.95ms
step:1560/1900 train_time:87308ms step_avg:55.97ms
step:1561/1900 train_time:87397ms step_avg:55.99ms
step:1562/1900 train_time:87485ms step_avg:56.01ms
step:1563/1900 train_time:87574ms step_avg:56.03ms
step:1564/1900 train_time:87661ms step_avg:56.05ms
step:1565/1900 train_time:87749ms step_avg:56.07ms
step:1566/1900 train_time:87837ms step_avg:56.09ms
step:1567/1900 train_time:87926ms step_avg:56.11ms
step:1568/1900 train_time:88014ms step_avg:56.13ms
step:1569/1900 train_time:88102ms step_avg:56.15ms
step:1570/1900 train_time:88190ms step_avg:56.17ms
step:1571/1900 train_time:88279ms step_avg:56.19ms
step:1572/1900 train_time:88368ms step_avg:56.21ms
step:1573/1900 train_time:88457ms step_avg:56.23ms
step:1574/1900 train_time:88544ms step_avg:56.25ms
step:1575/1900 train_time:88632ms step_avg:56.27ms
step:1576/1900 train_time:88720ms step_avg:56.29ms
step:1577/1900 train_time:88808ms step_avg:56.31ms
step:1578/1900 train_time:88895ms step_avg:56.33ms
step:1579/1900 train_time:88984ms step_avg:56.35ms
step:1580/1900 train_time:89072ms step_avg:56.37ms
step:1581/1900 train_time:89161ms step_avg:56.40ms
step:1582/1900 train_time:89248ms step_avg:56.41ms
step:1583/1900 train_time:89337ms step_avg:56.44ms
step:1584/1900 train_time:89425ms step_avg:56.46ms
step:1585/1900 train_time:89514ms step_avg:56.48ms
step:1586/1900 train_time:89602ms step_avg:56.50ms
step:1587/1900 train_time:89690ms step_avg:56.52ms
step:1588/1900 train_time:89777ms step_avg:56.53ms
step:1589/1900 train_time:89867ms step_avg:56.56ms
step:1590/1900 train_time:89955ms step_avg:56.58ms
step:1591/1900 train_time:90043ms step_avg:56.60ms
step:1592/1900 train_time:90130ms step_avg:56.61ms
step:1593/1900 train_time:90220ms step_avg:56.64ms
step:1594/1900 train_time:90308ms step_avg:56.65ms
step:1595/1900 train_time:90397ms step_avg:56.68ms
step:1596/1900 train_time:90484ms step_avg:56.69ms
step:1597/1900 train_time:90573ms step_avg:56.71ms
step:1598/1900 train_time:90660ms step_avg:56.73ms
step:1599/1900 train_time:90748ms step_avg:56.75ms
step:1600/1900 train_time:90835ms step_avg:56.77ms
step:1601/1900 train_time:90925ms step_avg:56.79ms
step:1602/1900 train_time:91012ms step_avg:56.81ms
step:1603/1900 train_time:91101ms step_avg:56.83ms
step:1604/1900 train_time:91189ms step_avg:56.85ms
step:1605/1900 train_time:91277ms step_avg:56.87ms
step:1606/1900 train_time:91366ms step_avg:56.89ms
step:1607/1900 train_time:91455ms step_avg:56.91ms
step:1608/1900 train_time:91542ms step_avg:56.93ms
step:1609/1900 train_time:91630ms step_avg:56.95ms
step:1610/1900 train_time:91718ms step_avg:56.97ms
step:1611/1900 train_time:91806ms step_avg:56.99ms
step:1612/1900 train_time:91894ms step_avg:57.01ms
step:1613/1900 train_time:91982ms step_avg:57.03ms
step:1614/1900 train_time:92070ms step_avg:57.04ms
step:1615/1900 train_time:92158ms step_avg:57.06ms
step:1616/1900 train_time:92247ms step_avg:57.08ms
step:1617/1900 train_time:92336ms step_avg:57.10ms
step:1618/1900 train_time:92424ms step_avg:57.12ms
step:1619/1900 train_time:92511ms step_avg:57.14ms
step:1620/1900 train_time:92598ms step_avg:57.16ms
step:1621/1900 train_time:92687ms step_avg:57.18ms
step:1622/1900 train_time:92774ms step_avg:57.20ms
step:1623/1900 train_time:92863ms step_avg:57.22ms
step:1624/1900 train_time:92950ms step_avg:57.24ms
step:1625/1900 train_time:93039ms step_avg:57.25ms
step:1626/1900 train_time:93127ms step_avg:57.27ms
step:1627/1900 train_time:93215ms step_avg:57.29ms
step:1628/1900 train_time:93302ms step_avg:57.31ms
step:1629/1900 train_time:93391ms step_avg:57.33ms
step:1630/1900 train_time:93478ms step_avg:57.35ms
step:1631/1900 train_time:93567ms step_avg:57.37ms
step:1632/1900 train_time:93655ms step_avg:57.39ms
step:1633/1900 train_time:93744ms step_avg:57.41ms
step:1634/1900 train_time:93831ms step_avg:57.42ms
step:1635/1900 train_time:93920ms step_avg:57.44ms
step:1636/1900 train_time:94007ms step_avg:57.46ms
step:1637/1900 train_time:94096ms step_avg:57.48ms
step:1638/1900 train_time:94183ms step_avg:57.50ms
step:1639/1900 train_time:94271ms step_avg:57.52ms
step:1640/1900 train_time:94359ms step_avg:57.54ms
step:1641/1900 train_time:94447ms step_avg:57.55ms
step:1642/1900 train_time:94535ms step_avg:57.57ms
step:1643/1900 train_time:94624ms step_avg:57.59ms
step:1644/1900 train_time:94711ms step_avg:57.61ms
step:1645/1900 train_time:94799ms step_avg:57.63ms
step:1646/1900 train_time:94887ms step_avg:57.65ms
step:1647/1900 train_time:94975ms step_avg:57.67ms
step:1648/1900 train_time:95063ms step_avg:57.68ms
step:1649/1900 train_time:95151ms step_avg:57.70ms
step:1650/1900 train_time:95238ms step_avg:57.72ms
step:1651/1900 train_time:95327ms step_avg:57.74ms
step:1652/1900 train_time:95415ms step_avg:57.76ms
step:1653/1900 train_time:95504ms step_avg:57.78ms
step:1654/1900 train_time:95591ms step_avg:57.79ms
step:1655/1900 train_time:95680ms step_avg:57.81ms
step:1656/1900 train_time:95767ms step_avg:57.83ms
step:1657/1900 train_time:95857ms step_avg:57.85ms
step:1658/1900 train_time:95945ms step_avg:57.87ms
step:1659/1900 train_time:96032ms step_avg:57.89ms
step:1660/1900 train_time:96120ms step_avg:57.90ms
step:1661/1900 train_time:96209ms step_avg:57.92ms
step:1662/1900 train_time:96296ms step_avg:57.94ms
step:1663/1900 train_time:96385ms step_avg:57.96ms
step:1664/1900 train_time:96472ms step_avg:57.98ms
step:1665/1900 train_time:96561ms step_avg:57.99ms
step:1666/1900 train_time:96648ms step_avg:58.01ms
step:1667/1900 train_time:96737ms step_avg:58.03ms
step:1668/1900 train_time:96826ms step_avg:58.05ms
step:1669/1900 train_time:96914ms step_avg:58.07ms
step:1670/1900 train_time:97002ms step_avg:58.09ms
step:1671/1900 train_time:97090ms step_avg:58.10ms
step:1672/1900 train_time:97178ms step_avg:58.12ms
step:1673/1900 train_time:97266ms step_avg:58.14ms
step:1674/1900 train_time:97354ms step_avg:58.16ms
step:1675/1900 train_time:97444ms step_avg:58.18ms
step:1676/1900 train_time:97530ms step_avg:58.19ms
step:1677/1900 train_time:97619ms step_avg:58.21ms
step:1678/1900 train_time:97707ms step_avg:58.23ms
step:1679/1900 train_time:97796ms step_avg:58.25ms
step:1680/1900 train_time:97883ms step_avg:58.26ms
step:1681/1900 train_time:97971ms step_avg:58.28ms
step:1682/1900 train_time:98059ms step_avg:58.30ms
step:1683/1900 train_time:98147ms step_avg:58.32ms
step:1684/1900 train_time:98235ms step_avg:58.33ms
step:1685/1900 train_time:98323ms step_avg:58.35ms
step:1686/1900 train_time:98411ms step_avg:58.37ms
step:1687/1900 train_time:98499ms step_avg:58.39ms
step:1688/1900 train_time:98586ms step_avg:58.40ms
step:1689/1900 train_time:98675ms step_avg:58.42ms
step:1690/1900 train_time:98762ms step_avg:58.44ms
step:1691/1900 train_time:98851ms step_avg:58.46ms
step:1692/1900 train_time:98939ms step_avg:58.47ms
step:1693/1900 train_time:99028ms step_avg:58.49ms
step:1694/1900 train_time:99115ms step_avg:58.51ms
step:1695/1900 train_time:99204ms step_avg:58.53ms
step:1696/1900 train_time:99291ms step_avg:58.54ms
step:1697/1900 train_time:99379ms step_avg:58.56ms
step:1698/1900 train_time:99467ms step_avg:58.58ms
step:1699/1900 train_time:99556ms step_avg:58.60ms
step:1700/1900 train_time:99643ms step_avg:58.61ms
step:1701/1900 train_time:99732ms step_avg:58.63ms
step:1702/1900 train_time:99820ms step_avg:58.65ms
step:1703/1900 train_time:99907ms step_avg:58.67ms
step:1704/1900 train_time:99995ms step_avg:58.68ms
step:1705/1900 train_time:100084ms step_avg:58.70ms
step:1706/1900 train_time:100172ms step_avg:58.72ms
step:1707/1900 train_time:100260ms step_avg:58.73ms
step:1708/1900 train_time:100348ms step_avg:58.75ms
step:1709/1900 train_time:100437ms step_avg:58.77ms
step:1710/1900 train_time:100524ms step_avg:58.79ms
step:1711/1900 train_time:100612ms step_avg:58.80ms
step:1712/1900 train_time:100700ms step_avg:58.82ms
step:1713/1900 train_time:100789ms step_avg:58.84ms
step:1714/1900 train_time:100876ms step_avg:58.85ms
step:1715/1900 train_time:100965ms step_avg:58.87ms
step:1716/1900 train_time:101053ms step_avg:58.89ms
step:1717/1900 train_time:101142ms step_avg:58.91ms
step:1718/1900 train_time:101229ms step_avg:58.92ms
step:1719/1900 train_time:101319ms step_avg:58.94ms
step:1720/1900 train_time:101406ms step_avg:58.96ms
step:1721/1900 train_time:101495ms step_avg:58.97ms
step:1722/1900 train_time:101583ms step_avg:58.99ms
step:1723/1900 train_time:101671ms step_avg:59.01ms
step:1724/1900 train_time:101759ms step_avg:59.03ms
step:1725/1900 train_time:101848ms step_avg:59.04ms
step:1726/1900 train_time:101936ms step_avg:59.06ms
step:1727/1900 train_time:102025ms step_avg:59.08ms
step:1728/1900 train_time:102112ms step_avg:59.09ms
step:1729/1900 train_time:102200ms step_avg:59.11ms
step:1730/1900 train_time:102287ms step_avg:59.13ms
step:1731/1900 train_time:102377ms step_avg:59.14ms
step:1732/1900 train_time:102465ms step_avg:59.16ms
step:1733/1900 train_time:102554ms step_avg:59.18ms
step:1734/1900 train_time:102641ms step_avg:59.19ms
step:1735/1900 train_time:102730ms step_avg:59.21ms
step:1736/1900 train_time:102818ms step_avg:59.23ms
step:1737/1900 train_time:102907ms step_avg:59.24ms
step:1738/1900 train_time:102995ms step_avg:59.26ms
step:1739/1900 train_time:103083ms step_avg:59.28ms
step:1740/1900 train_time:103170ms step_avg:59.29ms
step:1741/1900 train_time:103258ms step_avg:59.31ms
step:1742/1900 train_time:103346ms step_avg:59.33ms
step:1743/1900 train_time:103434ms step_avg:59.34ms
step:1744/1900 train_time:103522ms step_avg:59.36ms
step:1745/1900 train_time:103610ms step_avg:59.38ms
step:1746/1900 train_time:103698ms step_avg:59.39ms
step:1747/1900 train_time:103786ms step_avg:59.41ms
step:1748/1900 train_time:103873ms step_avg:59.42ms
step:1749/1900 train_time:103962ms step_avg:59.44ms
step:1750/1900 train_time:104050ms step_avg:59.46ms
step:1750/1900 val_loss:3.3180 train_time:104141ms step_avg:59.51ms
step:1751/1900 train_time:104163ms step_avg:59.49ms
step:1752/1900 train_time:104229ms step_avg:59.49ms
step:1753/1900 train_time:104323ms step_avg:59.51ms
step:1754/1900 train_time:104413ms step_avg:59.53ms
step:1755/1900 train_time:104501ms step_avg:59.54ms
step:1756/1900 train_time:104588ms step_avg:59.56ms
step:1757/1900 train_time:104675ms step_avg:59.58ms
step:1758/1900 train_time:104762ms step_avg:59.59ms
step:1759/1900 train_time:104850ms step_avg:59.61ms
step:1760/1900 train_time:104937ms step_avg:59.62ms
step:1761/1900 train_time:105025ms step_avg:59.64ms
step:1762/1900 train_time:105114ms step_avg:59.66ms
step:1763/1900 train_time:105207ms step_avg:59.67ms
step:1764/1900 train_time:105297ms step_avg:59.69ms
step:1765/1900 train_time:105386ms step_avg:59.71ms
step:1766/1900 train_time:105473ms step_avg:59.72ms
step:1767/1900 train_time:105561ms step_avg:59.74ms
step:1768/1900 train_time:105647ms step_avg:59.76ms
step:1769/1900 train_time:105735ms step_avg:59.77ms
step:1770/1900 train_time:105821ms step_avg:59.79ms
step:1771/1900 train_time:105909ms step_avg:59.80ms
step:1772/1900 train_time:105995ms step_avg:59.82ms
step:1773/1900 train_time:106084ms step_avg:59.83ms
step:1774/1900 train_time:106173ms step_avg:59.85ms
step:1775/1900 train_time:106265ms step_avg:59.87ms
step:1776/1900 train_time:106354ms step_avg:59.88ms
step:1777/1900 train_time:106443ms step_avg:59.90ms
step:1778/1900 train_time:106530ms step_avg:59.92ms
step:1779/1900 train_time:106618ms step_avg:59.93ms
step:1780/1900 train_time:106705ms step_avg:59.95ms
step:1781/1900 train_time:106793ms step_avg:59.96ms
step:1782/1900 train_time:106880ms step_avg:59.98ms
step:1783/1900 train_time:106967ms step_avg:59.99ms
step:1784/1900 train_time:107055ms step_avg:60.01ms
step:1785/1900 train_time:107143ms step_avg:60.02ms
step:1786/1900 train_time:107232ms step_avg:60.04ms
step:1787/1900 train_time:107322ms step_avg:60.06ms
step:1788/1900 train_time:107411ms step_avg:60.07ms
step:1789/1900 train_time:107499ms step_avg:60.09ms
step:1790/1900 train_time:107588ms step_avg:60.11ms
step:1791/1900 train_time:107675ms step_avg:60.12ms
step:1792/1900 train_time:107763ms step_avg:60.14ms
step:1793/1900 train_time:107851ms step_avg:60.15ms
step:1794/1900 train_time:107938ms step_avg:60.17ms
step:1795/1900 train_time:108027ms step_avg:60.18ms
step:1796/1900 train_time:108114ms step_avg:60.20ms
step:1797/1900 train_time:108204ms step_avg:60.21ms
step:1798/1900 train_time:108292ms step_avg:60.23ms
step:1799/1900 train_time:108382ms step_avg:60.25ms
step:1800/1900 train_time:108469ms step_avg:60.26ms
step:1801/1900 train_time:108558ms step_avg:60.28ms
step:1802/1900 train_time:108645ms step_avg:60.29ms
step:1803/1900 train_time:108733ms step_avg:60.31ms
step:1804/1900 train_time:108821ms step_avg:60.32ms
step:1805/1900 train_time:108909ms step_avg:60.34ms
step:1806/1900 train_time:108996ms step_avg:60.35ms
step:1807/1900 train_time:109085ms step_avg:60.37ms
step:1808/1900 train_time:109173ms step_avg:60.38ms
step:1809/1900 train_time:109262ms step_avg:60.40ms
step:1810/1900 train_time:109350ms step_avg:60.41ms
step:1811/1900 train_time:109439ms step_avg:60.43ms
step:1812/1900 train_time:109526ms step_avg:60.44ms
step:1813/1900 train_time:109614ms step_avg:60.46ms
step:1814/1900 train_time:109701ms step_avg:60.47ms
step:1815/1900 train_time:109789ms step_avg:60.49ms
step:1816/1900 train_time:109876ms step_avg:60.50ms
step:1817/1900 train_time:109964ms step_avg:60.52ms
step:1818/1900 train_time:110051ms step_avg:60.53ms
step:1819/1900 train_time:110139ms step_avg:60.55ms
step:1820/1900 train_time:110228ms step_avg:60.56ms
step:1821/1900 train_time:110316ms step_avg:60.58ms
step:1822/1900 train_time:110404ms step_avg:60.59ms
step:1823/1900 train_time:110493ms step_avg:60.61ms
step:1824/1900 train_time:110580ms step_avg:60.63ms
step:1825/1900 train_time:110668ms step_avg:60.64ms
step:1826/1900 train_time:110755ms step_avg:60.65ms
step:1827/1900 train_time:110843ms step_avg:60.67ms
step:1828/1900 train_time:110932ms step_avg:60.68ms
step:1829/1900 train_time:111020ms step_avg:60.70ms
step:1830/1900 train_time:111108ms step_avg:60.71ms
step:1831/1900 train_time:111196ms step_avg:60.73ms
step:1832/1900 train_time:111284ms step_avg:60.74ms
step:1833/1900 train_time:111373ms step_avg:60.76ms
step:1834/1900 train_time:111461ms step_avg:60.77ms
step:1835/1900 train_time:111549ms step_avg:60.79ms
step:1836/1900 train_time:111636ms step_avg:60.80ms
step:1837/1900 train_time:111724ms step_avg:60.82ms
step:1838/1900 train_time:111812ms step_avg:60.83ms
step:1839/1900 train_time:111901ms step_avg:60.85ms
step:1840/1900 train_time:111989ms step_avg:60.86ms
step:1841/1900 train_time:112077ms step_avg:60.88ms
step:1842/1900 train_time:112164ms step_avg:60.89ms
step:1843/1900 train_time:112252ms step_avg:60.91ms
step:1844/1900 train_time:112341ms step_avg:60.92ms
step:1845/1900 train_time:112429ms step_avg:60.94ms
step:1846/1900 train_time:112516ms step_avg:60.95ms
step:1847/1900 train_time:112604ms step_avg:60.97ms
step:1848/1900 train_time:112692ms step_avg:60.98ms
step:1849/1900 train_time:112780ms step_avg:61.00ms
step:1850/1900 train_time:112868ms step_avg:61.01ms
step:1851/1900 train_time:112956ms step_avg:61.02ms
step:1852/1900 train_time:113043ms step_avg:61.04ms
step:1853/1900 train_time:113132ms step_avg:61.05ms
step:1854/1900 train_time:113220ms step_avg:61.07ms
step:1855/1900 train_time:113308ms step_avg:61.08ms
step:1856/1900 train_time:113396ms step_avg:61.10ms
step:1857/1900 train_time:113485ms step_avg:61.11ms
step:1858/1900 train_time:113573ms step_avg:61.13ms
step:1859/1900 train_time:113662ms step_avg:61.14ms
step:1860/1900 train_time:113749ms step_avg:61.16ms
step:1861/1900 train_time:113838ms step_avg:61.17ms
step:1862/1900 train_time:113926ms step_avg:61.18ms
step:1863/1900 train_time:114014ms step_avg:61.20ms
step:1864/1900 train_time:114102ms step_avg:61.21ms
step:1865/1900 train_time:114192ms step_avg:61.23ms
step:1866/1900 train_time:114280ms step_avg:61.24ms
step:1867/1900 train_time:114368ms step_avg:61.26ms
step:1868/1900 train_time:114455ms step_avg:61.27ms
step:1869/1900 train_time:114543ms step_avg:61.29ms
step:1870/1900 train_time:114631ms step_avg:61.30ms
step:1871/1900 train_time:114719ms step_avg:61.31ms
step:1872/1900 train_time:114808ms step_avg:61.33ms
step:1873/1900 train_time:114896ms step_avg:61.34ms
step:1874/1900 train_time:114984ms step_avg:61.36ms
step:1875/1900 train_time:115072ms step_avg:61.37ms
step:1876/1900 train_time:115160ms step_avg:61.39ms
step:1877/1900 train_time:115249ms step_avg:61.40ms
step:1878/1900 train_time:115337ms step_avg:61.41ms
step:1879/1900 train_time:115424ms step_avg:61.43ms
step:1880/1900 train_time:115512ms step_avg:61.44ms
step:1881/1900 train_time:115602ms step_avg:61.46ms
step:1882/1900 train_time:115690ms step_avg:61.47ms
step:1883/1900 train_time:115780ms step_avg:61.49ms
step:1884/1900 train_time:115867ms step_avg:61.50ms
step:1885/1900 train_time:115956ms step_avg:61.52ms
step:1886/1900 train_time:116043ms step_avg:61.53ms
step:1887/1900 train_time:116132ms step_avg:61.54ms
step:1888/1900 train_time:116220ms step_avg:61.56ms
step:1889/1900 train_time:116309ms step_avg:61.57ms
step:1890/1900 train_time:116396ms step_avg:61.59ms
step:1891/1900 train_time:116484ms step_avg:61.60ms
step:1892/1900 train_time:116572ms step_avg:61.61ms
step:1893/1900 train_time:116662ms step_avg:61.63ms
step:1894/1900 train_time:116750ms step_avg:61.64ms
step:1895/1900 train_time:116839ms step_avg:61.66ms
step:1896/1900 train_time:116926ms step_avg:61.67ms
step:1897/1900 train_time:117014ms step_avg:61.68ms
step:1898/1900 train_time:117102ms step_avg:61.70ms
step:1899/1900 train_time:117193ms step_avg:61.71ms
step:1900/1900 train_time:117281ms step_avg:61.73ms
step:1900/1900 val_loss:3.2776 train_time:117372ms step_avg:61.77ms
peak memory allocated: 29385 MiB reserved: 43838 MiB

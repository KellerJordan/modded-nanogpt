import os
import sys

# Read the current file and the kernels file code ASAP, for logging
with open(sys.argv[0], 'r') as f: 
    code = f.read()
with open(os.path.join(os.path.dirname(sys.argv[0]), 'triton_kernels.py'), 'r') as f:
    code += f"\n\n{'-'*40}\n# triton_kernels.py\n{'-'*40}\n\n" 
    code += f.read()

import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch
import triton

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
from kernels import get_kernel
from torch import Tensor, nn

from triton_kernels import XXT, ba_plus_cAA, FusedLinearReLUSquareFunction, FusedSoftcappedCrossEntropy

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng
# Transposed layout by @ChrisJMcCormick allows for faster gradient accumulation.

@torch.library.custom_op("nanogpt::mm_t", mutates_args=())
def mm_t_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    """Computes y = x @ w with F8 weights stored as (in_features, out_features)."""
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        assert x.shape[1] == w.shape[0]  # x: (batch, in), w: (in, out)

        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)

        # _scaled_mm requires column-major B. w_f8 is row-major (in, out).
        # .T.contiguous().T creates a column-major view without changing logical shape.
        w_f8_col_major = w_f8.T.contiguous().T

        out = torch._scaled_mm(
            x_f8,
            w_f8_col_major,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_t_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[0]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_t_backward", mutates_args=())
def mm_t_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        
        x_scale = grad.new_tensor(x_s, dtype=torch.float32)
        w_scale = grad.new_tensor(w_s, dtype=torch.float32)
        grad_scale = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        
        # grad_x = grad @ w.T
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T, 
            out_dtype=torch.bfloat16,
            scale_a=grad_scale,
            scale_b=w_scale,
            use_fast_accum=False,
        )
        
        # grad_w = x.T @ grad
        # Result is (in, out), naturally matching weight storage. No final .T needed.
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_scale,
            scale_b=grad_scale,
            use_fast_accum=False,
        )
        
        return grad_x, grad_w

    grad_x, grad_w = impl(g, x_f8, w_f8)

    return grad_x, grad_w

@mm_t_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward_t(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_t_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context_t(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_t_op.register_autograd(backward_t, setup_context=setup_context_t)

# -----------------------------------------------------------------------------
# Polar Express

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Combined NorMuon + Adam Optimizer

@dataclass
class ParamConfig:
    """Per-parameter configuration for NorMuonAndAdam optimizer."""
    label: str
    optim: str  # "adam" or "normuon"
    comms: str  # "none", "replicated", or "sharded"
    adam_betas: tuple[float, float] | None
    lr_mul: float
    wd_mul: float
    lr: float
    initial_lr: float
    weight_decay: float
    # Adam-specific
    eps: float | None = None
    # NorMuon-specific
    reshape: tuple | None = None
    chunk_size: int | None = None
    momentum: float | None = None
    beta2: float | None = None
    per_matrix_lr_mul: list[float] | None = None


class NorMuonAndAdam:
    """
    Combined optimizer that handles both NorMuon (for projection matrices) and 
    Adam (for embeddings/scalars/gate weights).

    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, Muon uses a Newton-Schulz iteration (replaced
    here with Polar Express), which has the advantage that it can be stably run in bfloat16 on the GPU.

    Muon is applied only to the projection matrices in the attention and MLP layers, and is not recommended
    for embeddings, scalars, or individual weight vectors (e.g., bias terms or gate weights). 

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - Cautious weight decay, a gated version of decoupled weight decay
    - Mantissa tracking for precision
    
    Adam (for embeddings/scalars/gates):
    - Standard Adam with bias correction
    - Cautious weight decay

    Configuration:
    Unlike torch.optim.Optimizer, this class uses per-parameter configs from a `param_table` dict
    and does not include parameter "groups". All parameters require a .label attribute, and a 
    corresponding entry in the param_table to specify their hyperparameters (lr_mul, wd_mul, adam_betas, etc.).

    Communication and ordering:
    Gradient communication is explicitly scheduled rather than hook-driven.
    Reductions are launched in `scatter_order`, while update math and final
    gathers are executed in `work_order`. These orders are independent and
    must each contain every parameter label exactly once.

    Two communication modes are supported per parameter:
    - 'replicated': Gradients are all-reduced and each rank computes the full update.
    - 'sharded': Gradients are reduce-scattered, each rank updates its shard,
      and results are all-gathered.

    Adam parameters may be freely sharded. NorMuon operates on full matrices; sharding is 
    supported by grouping matrices into parameter banks. NorMuon parameters must have a
    `.reshape` attribute that reshapes the bank so that the leading dimension is divisible 
    by world_size.

    # Contributors include @YouJiacheng, @KonstantinWilleke, @alexrgilbert, @adricarda,
    # @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
    """
    def __init__(self, named_params, param_table: dict, scatter_order: list, work_order: list,
                 adam_defaults: dict, normuon_defaults: dict):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        
        # Store defaults for each optimizer type
        self.adam_defaults = adam_defaults
        self.normuon_defaults = normuon_defaults
        self.param_table = param_table
        self.scatter_order = scatter_order
        self.work_order = work_order
        
        # Collect params by label and build config
        self.param_cfgs: dict[nn.Parameter, ParamConfig] = {}
        self.param_states: dict[nn.Parameter, dict] = {}
        self._param_by_label: dict[str, nn.Parameter] = {}
        for name, param in named_params:
            label = getattr(param, "label", None)
            assert label is not None and label in param_table  # all params must have valid label
            assert label not in self._param_by_label  # exactly one param per label
            self._param_by_label[label] = param
            self._build_param_cfg(param, label)
        
        # Assert scatter_order and work_order match present labels exactly
        present = set(self._param_by_label.keys())
        assert set(scatter_order) == present and set(work_order) == present
        
        # Handle world_size=1: overwrite comms to "none"
        if self.world_size == 1:
            for p_cfg in self.param_cfgs.values():
                p_cfg.comms = "none"
        
        # Initialize state for all params
        self._init_state()
        
        # 0-D CPU tensors to avoid recompilation
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_lr_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        
        # Track async operations
        self._reduce_futures: dict[nn.Parameter, tuple] = {}
        
        # Embed/lm_head tying state
        self.split_embed = False
        self._lm_head_param = self._param_by_label.get("lm_head")
        self._embed_param = self._param_by_label.get("embed")
    
    def _build_param_cfg(self, param: nn.Parameter, label: str):
        """Build config for a single parameter from param_table."""
        table_entry = self.param_table[label]
        optim = table_entry["optim"]
        comms = table_entry["comms"]
        adam_betas = table_entry.get("adam_betas")
        lr_mul = table_entry.get("lr_mul", 1.0)
        wd_mul = table_entry.get("wd_mul", 1.0)
        
        if optim == "adam":
            chunk_size = param.shape[0] // self.world_size if comms == "sharded" else None
            p_cfg = ParamConfig(
                label=label,
                optim=optim,
                comms=comms,
                adam_betas=tuple(adam_betas) if adam_betas else None,
                lr_mul=lr_mul,
                wd_mul=wd_mul,
                lr=self.adam_defaults["lr"],
                initial_lr=self.adam_defaults["lr"],
                weight_decay=self.adam_defaults["weight_decay"],
                eps=self.adam_defaults["eps"],
                chunk_size=chunk_size,
            )
        elif optim == "normuon":
            reshape = getattr(param, "reshape", None)
            if reshape is None:
                raise ValueError(f"NorMuon param {label} must have .reshape attribute")
            if reshape[0] % self.world_size != 0:
                raise ValueError(f"reshape[0]={reshape[0]} must be divisible by world_size")
            
            chunk_size = reshape[0] // self.world_size
            chunk_shape = (chunk_size, *reshape[1:])
            # Shape-based LR multiplier for NorMuon
            shape_mult = max(1.0, chunk_shape[-2] / chunk_shape[-1]) ** 0.5 if len(chunk_shape) >= 2 else 1.0
            lr_mul = shape_mult * lr_mul
            
            # Per-matrix LR multipliers for MLP c_proj (2x LR on odd indices)
            per_matrix_lr_mul = None
            if label == "mlp":
                rank = dist.get_rank() if dist.is_initialized() else 0
                start_idx = rank * chunk_size
                per_matrix_lr_mul = []
                for i in range(chunk_size):
                    global_idx = start_idx + i
                    is_c_proj = (global_idx % 2 == 1)
                    per_matrix_lr_mul.append(2.0 if is_c_proj else 1.0)
            
            p_cfg = ParamConfig(
                label=label,
                optim=optim,
                comms=comms,
                adam_betas=tuple(adam_betas) if adam_betas else None,
                lr_mul=lr_mul,
                wd_mul=wd_mul,
                lr=self.normuon_defaults["lr"],
                initial_lr=self.normuon_defaults["lr"],
                weight_decay=self.normuon_defaults["weight_decay"],
                reshape=reshape,
                chunk_size=chunk_size,
                momentum=self.normuon_defaults["momentum"],
                beta2=self.normuon_defaults["beta2"],
                per_matrix_lr_mul=per_matrix_lr_mul,
            )
        else:
            raise ValueError(f"Unknown optim type: {optim}")
        
        self.param_cfgs[param] = p_cfg
    
    def _init_state(self):
        """Initialize optimizer state for all parameters."""
        for param, p_cfg in self.param_cfgs.items():
            if p_cfg.optim == "adam":
                # Sharded params use chunk state, replicated use full state
                if p_cfg.comms == "sharded":
                    chunk = param[:p_cfg.chunk_size]
                else:
                    chunk = param
                exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=param.device)
                self.param_states[param] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
            
            elif p_cfg.optim == "normuon":
                chunk_shape = (p_cfg.chunk_size, *p_cfg.reshape[1:])
                
                # Momentum buffer (FP32 for precision)
                momentum_buffer = torch.zeros(
                    chunk_shape, dtype=torch.float32, device=param.device
                )
                
                # Second momentum buffer - reduced along one dimension
                if chunk_shape[-2] >= chunk_shape[-1]:
                    second_mom_shape = (*chunk_shape[:-1], 1)
                else:
                    second_mom_shape = (*chunk_shape[:-2], 1, chunk_shape[-1])
                second_momentum_buffer = torch.zeros(
                    second_mom_shape, dtype=torch.float32, device=param.device
                )
                
                # Mantissa buffer for precision tracking
                mantissa = torch.zeros(
                    chunk_shape, dtype=torch.uint16, device=param.device
                )
                
                self.param_states[param] = dict(
                    momentum_buffer=momentum_buffer,
                    second_momentum_buffer=second_momentum_buffer,
                    mantissa=mantissa,
                )

    # -----------------------------------
    # Reduce/Gather operations
    
    def _launch_reduce(self, param: nn.Parameter, grad: Tensor):
        """Launch async reduce for a parameter based on its comms policy."""
        p_cfg = self.param_cfgs[param]
        
        if p_cfg.comms == "none":
            if p_cfg.optim == "normuon":
                # NorMuon needs reshaped gradient even without communication
                grad = grad.view(p_cfg.reshape)
            self._reduce_futures[param] = (None, grad)
        elif p_cfg.comms == "replicated":
            future = dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future()
            self._reduce_futures[param] = (future, grad)
        elif p_cfg.comms == "sharded":
            if p_cfg.optim == "normuon":
                # NorMuon: reshape before reduce_scatter
                grad_reshaped = grad.view(p_cfg.reshape)
                grad_chunk = torch.empty(
                    (p_cfg.chunk_size, *grad_reshaped.shape[1:]),
                    dtype=grad.dtype,
                    device=grad.device
                )
                future = dist.reduce_scatter_tensor(
                    grad_chunk, grad_reshaped.contiguous(), op=dist.ReduceOp.AVG, async_op=True
                ).get_future()
                self._reduce_futures[param] = (future, grad_chunk)
            else:
                # Adam: simple reduce_scatter
                grad_chunk = torch.empty_like(grad[:p_cfg.chunk_size])
                future = dist.reduce_scatter_tensor(
                    grad_chunk, grad, op=dist.ReduceOp.AVG, async_op=True
                ).get_future()
                self._reduce_futures[param] = (future, grad_chunk)

    def _launch_gather(self, param: nn.Parameter, p_slice: Tensor) -> "torch.futures.Future":
        """Launch async all_gather for a sharded parameter."""
        p_cfg = self.param_cfgs[param]
        if p_cfg.optim == "normuon":
            full_param = param.data.view(p_cfg.reshape)
            assert full_param.is_contiguous()
            return dist.all_gather_into_tensor(
                full_param, p_slice.contiguous(), async_op=True
            ).get_future()
        else:
            return dist.all_gather_into_tensor(
                param, p_slice.contiguous(), async_op=True
            ).get_future()

    # -----------------------------------
    # State management
    
    def reset(self):
        """Reset NorMuon momentum buffers and split_embed state (called on training reset)."""
        self.split_embed = False
        for param, p_cfg in self.param_cfgs.items():
            if p_cfg.optim == "normuon":
                p_state = self.param_states[param]
                p_state["momentum_buffer"].zero_()
                p_state["mantissa"].zero_()
                p_state["second_momentum_buffer"].zero_()
    
    def copy_lm_state_to_embed(self):
        """
        Copy the optimizer state from the lm_head to the embed at the untie point.
        This requires an all-gather + reshard because of different sharding:
        - lm_head (768, 50304) is sharded to (96, 50304) per rank (along model_dim)
        - embed (50304, 768) is sharded to (6288, 768) per rank (along vocab_size)
        
        We all-gather the lm_head momentum, transpose it, then each rank takes their
        embed shard to get the correct momentum state.
        """
        lm_head = self._lm_head_param
        embed = self._embed_param        
        lm_state = self.param_states[lm_head]
        embed_state = self.param_states[embed]
        lm_cfg = self.param_cfgs[lm_head]
        embed_cfg = self.param_cfgs[embed]
        
        embed_state['step'] = lm_state['step'] # Preserve step count for bias correction        
        
        # Copy optimizer state with all-gather + transpose + reshard
        if self.world_size > 1:
            rank = dist.get_rank()
            lm_chunk_size = lm_cfg.chunk_size  # 96
            embed_chunk_size = embed_cfg.chunk_size  # 6288
            
            # All-gather lm_head momentum to get full (768, 50304) tensor
            for key in ["exp_avg", "exp_avg_sq"]:
                lm_chunk = lm_state[key]  # (96, 50304)
                full_lm = torch.empty(lm_head.shape[0], lm_head.shape[1], dtype=lm_chunk.dtype, device=lm_chunk.device)
                dist.all_gather_into_tensor(full_lm, lm_chunk.contiguous())
                embed_state[key].copy_(full_lm.T[rank * embed_chunk_size:(rank + 1) * embed_chunk_size])
        else:
            # Single GPU: simple transpose
            for key in ["exp_avg", "exp_avg_sq"]:
                embed_state[key].copy_(lm_state[key].T)
        
        # Mark as split
        self.split_embed = True
    
    def state_dict(self):
        """Return the optimizer state as a dict."""
        return {
            "param_states": {id(p): s for p, s in self.param_states.items()},
            "param_cfgs": {id(p): s for p, s in self.param_cfgs.items()},
        }
    
    def load_state_dict(self, state_dict):
        """Load optimizer state from a dict."""
        # Build id->param mapping
        id_to_param = {id(p): p for p in self.param_cfgs.keys()}
        
        # Load state, preserving dtypes
        for param_id, saved_p_state in state_dict["param_states"].items():
            if param_id in id_to_param:
                param = id_to_param[param_id]
                p_state = self.param_states[param]
                for k, v in saved_p_state.items():
                    if isinstance(v, torch.Tensor) and k in p_state:
                        target_dtype = p_state[k].dtype
                        p_state[k] = v.to(dtype=target_dtype, device=p_state[k].device)
                    else:
                        p_state[k] = v

    # -----------------------------------
    # Unified optimizer step with explicit ordering

    @torch.no_grad()
    def step(self, do_adam: bool = True):
        """
        Combined optimizer step with explicit ordering.
        
        Args:
            do_adam: If True, update Adam params. NorMuon params always updated.
        
        Flow:
        1. Scatter phase: Launch reduces in scatter_order
        2. Work phase: Process updates in work_order
           - Wait for reduce, compute update, launch gather
        3. Finalize phase: Wait for gathers
        
        While the embeddings are tied:
        - Comms and update math are only done on lm_head.
        - We add embed.grad.T into lm_head.grad before comms.
        - After lm_head gather, we copy lm_head.data.T --> embed.data        
        """
        rank = dist.get_rank() if dist.is_initialized() else 0
        lm_param, embed_param = self._lm_head_param, self._embed_param
        
        # ===== Phase 1: Launch reduces in scatter_order =====
        for label in self.scatter_order:
            param = self._param_by_label[label]
            p_cfg = self.param_cfgs[param]
            
            if p_cfg.optim == "adam" and not do_adam:
                continue
            if param.grad is None:
                continue
            
            # lm_head when tied: aggregate embed.grad.T (transposed shapes)
            if label == "lm_head" and do_adam and not self.split_embed:
                if embed_param is not None and embed_param.grad is not None:
                    param.grad.add_(embed_param.grad.T)
            
            # Skip embed when tied (copied from lm_head after gather)
            if label == "embed" and not self.split_embed:
                continue
            
            self._launch_reduce(param, param.grad)
        
        # ===== Phase 2: Process updates in work_order =====
        gather_futures = []
        lm_head_gather_future = None
        
        for label in self.work_order:
            param = self._param_by_label[label]
            if param not in self._reduce_futures:
                continue
            
            p_cfg = self.param_cfgs[param]
            if p_cfg.optim == "adam" and not do_adam:
                continue
            # Wait for reduce
            future, grad_chunk = self._reduce_futures[param]
            if future is not None:
                future.wait()
            # Apply update based on optim type
            if p_cfg.optim == "adam":
                p_slice = self._adam_update(param, grad_chunk, p_cfg, rank)
            else:
                p_slice = self._normuon_update(param, grad_chunk, p_cfg, rank)
            # Launch gather for sharded params
            if p_cfg.comms == "sharded" and self.world_size > 1:
                gather_fut = self._launch_gather(param, p_slice)
                if label == "lm_head":
                    lm_head_gather_future = gather_fut
                else:
                    gather_futures.append(gather_fut)
        
        # ===== Phase 3: Wait for gathers, sync embed if tied =====
        # Wait for lm_head gather first so we can copy to embed while other gathers complete
        if lm_head_gather_future is not None:
            lm_head_gather_future.wait()
        
        # When tied: copy lm_head.T to embed
        if do_adam and not self.split_embed and embed_param is not None and lm_param is not None:
            embed_param.data.copy_(lm_param.data.T)
        
        # Wait for remaining gathers
        for fut in gather_futures:
            fut.wait()
        
        self._reduce_futures.clear()
        
        # Clear grads for updated params
        for param, p_cfg in self.param_cfgs.items():
            if p_cfg.optim == "adam" and not do_adam:
                continue  # Don't clear Adam grads on even steps
            param.grad = None

    # -----------------------------------
    # Adam update

    def _adam_update(self, param: nn.Parameter, grad_chunk: Tensor, p_cfg: ParamConfig, rank: int) -> Tensor:
        """Apply Adam update to a parameter. Returns the updated p_slice."""
        beta1, beta2 = p_cfg.adam_betas
        lr = p_cfg.lr * p_cfg.lr_mul
        
        # Get parameter slice
        if p_cfg.comms == "sharded":
            p_slice = param[rank * p_cfg.chunk_size:(rank + 1) * p_cfg.chunk_size]
        else:
            p_slice = param
        
        p_state = self.param_states[param]
        p_state["step"] += 1
        t = p_state["step"]
        
        bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
        self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
        self._eff_wd_t.fill_(lr * lr * p_cfg.weight_decay * p_cfg.wd_mul)
        
        NorMuonAndAdam._adam_update_step(
            p_slice, grad_chunk, p_state["exp_avg"], p_state["exp_avg_sq"],
            beta1, beta2, p_cfg.eps, self._step_size_t, self._eff_wd_t
        )
        
        return p_slice

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _adam_update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)
        # Cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)
        p_slice.add_(other=update, alpha=-1.0)

    # -----------------------------------
    # NorMuon update

    def _normuon_update(self, param: nn.Parameter, grad_chunk: Tensor, p_cfg: ParamConfig, rank: int) -> Tensor:
        """Apply NorMuon update to a parameter. Returns the updated p_slice."""
        chunk_shape = grad_chunk.shape
        
        p_state = self.param_states[param]
        grad_chunk = grad_chunk.float()  # FP32 for momentum
        
        # Momentum update
        momentum_buffer = p_state["momentum_buffer"]
        momentum_buffer.lerp_(grad_chunk, 1 - p_cfg.momentum)
        updated_grads = grad_chunk.lerp_(momentum_buffer, p_cfg.momentum)
        
        self._eff_lr_t.fill_(p_cfg.lr_mul * p_cfg.lr)
        self._eff_wd_t.fill_(p_cfg.wd_mul * p_cfg.weight_decay * p_cfg.lr)
        
        # Polar Express orthogonalization
        is_large_matrix = chunk_shape[-2] > 1024
        v_chunk = polar_express(updated_grads, split_baddbmm=is_large_matrix)
        
        # Variance reduction
        red_dim = -1 if chunk_shape[-2] >= chunk_shape[-1] else -2
        v_chunk = NorMuonAndAdam._apply_normuon_variance_reduction(
            v_chunk, p_state["second_momentum_buffer"], p_cfg.beta2, red_dim
        )
        
        # Update parameter, in place, with cautious weight decay
        param_view = param.data.view(p_cfg.reshape)
        p_slice = param_view[rank * p_cfg.chunk_size:(rank + 1) * p_cfg.chunk_size]
        
        # MLP has per-matrix LR multipliers (c_proj gets 2x LR)
        if p_cfg.per_matrix_lr_mul is not None:
            for mat_idx in range(p_cfg.chunk_size):
                self._eff_lr_t.fill_(p_cfg.lr_mul * p_cfg.per_matrix_lr_mul[mat_idx] * p_cfg.lr)
                self._eff_wd_t.fill_(p_cfg.wd_mul * p_cfg.weight_decay * p_cfg.lr)
                NorMuonAndAdam._cautious_wd_and_update_inplace(
                    p_slice[mat_idx].view(torch.uint16), p_state["mantissa"][mat_idx], v_chunk[mat_idx],
                    self._eff_wd_t, self._eff_lr_t
                )
        else:
            NorMuonAndAdam._cautious_wd_and_update_inplace(
                p_slice.view(torch.uint16), p_state["mantissa"], v_chunk,
                self._eff_wd_t, self._eff_lr_t
            )
        
        return p_slice

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
        """
        Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
        Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
        bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
        float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
        """
        assert p.dtype == mantissa.dtype == torch.uint16
        grad = grad.float()
        wd_factor = wd_tensor.to(torch.float32)
        lr_factor = lr_tensor.to(torch.float32)
        p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
        p_precise = p_precise_raw.view(torch.float32)
        mask = (grad * p_precise) >= 0
        p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
        p.copy_((p_precise_raw >> 16).to(torch.uint16))
        mantissa.copy_(p_precise_raw.to(torch.uint16))

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
        """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
        v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
        red_dim_size = v_chunk.size(red_dim)
        v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
        v_norm = v_norm_sq.sqrt_()
        second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
        step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
        scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
        v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
        final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
        return v_chunk.mul_(final_scale.type_as(v_chunk))

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinearT(nn.Module):
    """
    Linear layer with transposed weight storage (in_features, out_features) which
    addresses the slow kernel that was used for gradient accumulation. @chrisjmccormick
    """
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s
        
        self.weight = nn.Parameter(torch.empty(in_features, out_features, dtype=torch.bfloat16))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        with torch.no_grad():
            nn.init.zeros_(self.weight) # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out = torch.ops.nanogpt.mm_t(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return x @ self.weight.type_as(x)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim
        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim
        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, c_fc: Tensor, c_proj: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        # Fused triton kernel for relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, c_fc, c_proj)

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, has_attn: bool, has_mlp: bool, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if has_attn:
            if use_paired_head:
                self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads)
            else:
                self.attn = CausalSelfAttention(dim, head_dim, num_heads)
        else:
            self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP() if has_mlp else None

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor = None, c_fc: Tensor = None, c_proj: Tensor = None):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args, qkvo_w)
        if self.mlp is not None:
            x = x + self.mlp(norm(x), c_fc, c_proj)
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = nn.Linear(12, 1, bias=False)
        nn.init.zeros_(self.smear_gate.weight)
        self.smear_gate.weight.label = 'smear_gate'

        self.skip_gate = nn.Linear(12, 1, bias=False)
        nn.init.zeros_(self.skip_gate.weight)
        self.skip_gate.weight.label = 'skip_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for i, ve in enumerate(self.value_embeds):
            ve.weight.label = f've{i}'  # ve0, ve1, ve2
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        # -----------------------------------
        # Parameter banks for sharded optimization, by @chrisjmccormick

        # Identify which layers have attention/MLP
        # Attention is skipped in layer 6 by @YouJiacheng
        self.attn_layer_indices = [i for i in range(num_layers) if i != 6]
        # All layers have MLP (At 11 layers--dropped first layer @EmelyanenkoK)
        self.mlp_layer_indices = list(range(num_layers))

        hdim = num_heads * head_dim
        mlp_hdim = 4 * model_dim

        # Create index mappings: layer_idx -> bank_idx
        self.layer_to_attn_idx = {layer_idx: bank_idx for bank_idx, layer_idx in enumerate(self.attn_layer_indices)}
        self.layer_to_mlp_idx = {layer_idx: bank_idx for bank_idx, layer_idx in enumerate(self.mlp_layer_indices)}

        # Attention bank: stores QKVO weights for all attention layers
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        # Shape: (num_attn_layers, 4*model_dim, hdim) = (10, 3072, 768)
        # Reshape for sharding: (40, 768, 768) for even distribution across 8 GPUs
        self.attn_bank = nn.Parameter(torch.empty(len(self.attn_layer_indices), 4 * model_dim, hdim))
        self.attn_bank.label = 'attn'
        self.attn_bank.reshape = (len(self.attn_layer_indices) * 4, hdim, hdim)  # (40, 768, 768)

        # MLP bank: stores c_fc and c_proj for all MLP layers
        # Shape: (num_mlp_layers + padding, 2, mlp_hdim, model_dim) = (12, 2, 3072, 768)
        # We add 1 padding layer (index 11) to get 12*2=24 matrices for even distribution across 8 GPUs
        # Reshape for sharding: (24, 3072, 768)
        num_mlp_with_padding = len(self.mlp_layer_indices) + 1  # 11 + 1 = 12
        self.mlp_bank = nn.Parameter(torch.empty(num_mlp_with_padding, 2, mlp_hdim, model_dim))
        self.mlp_bank.label = 'mlp'
        self.mlp_bank.reshape = (num_mlp_with_padding * 2, mlp_hdim, model_dim)  # (24, 3072, 768)

        # improved init scale by @YouJiacheng
        # Attention uses dim^-0.5, MLP uses 0.5 * dim^-0.5
        attn_std = model_dim ** -0.5
        attn_bound = (3 ** 0.5) * attn_std
        mlp_std = 0.5 * (model_dim ** -0.5)
        mlp_bound = (3 ** 0.5) * mlp_std
        with torch.no_grad():
            # Init attention bank (QKV uniform, O zero)
            self.attn_bank[:, :model_dim * 3, :].uniform_(-attn_bound, attn_bound)
            self.attn_bank[:, model_dim * 3:, :].zero_()
            # Init MLP bank (c_fc uniform, c_proj zero) 
            self.mlp_bank[:, 0, :, :].uniform_(-mlp_bound, mlp_bound)  # c_fc
            self.mlp_bank[:, 1, :, :].zero_()  # c_proj - zero init suggested by @Grad62304977

        # Create blocks with has_attn/has_mlp flags
        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([
            Block(model_dim, head_dim, num_heads, 
                  has_attn=(i in self.layer_to_attn_idx), 
                  has_mlp=(i in self.layer_to_mlp_idx),
                  use_paired_head=(i in self.paired_head_layers))
            for i in range(num_layers)
        ])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        # Transposed weight storage for faster gradient accumulation
        self.lm_head = CastedLinearT(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)

        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'
        with torch.no_grad():
            self.embed.weight.copy_(self.lm_head.weight.T)

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )
        self.scalars.label = 'scalars'

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # Embedding lookup - embed is synced from lm_head during tied phase by optimizer
        x = self.embed(input_seq)
        
        # Value embeddings - always computed (not precomputed)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        # unbind weight banks to avoid select_backwards kernel
        attn_weights = self.attn_bank.unbind(0)  # tuple of [4*dim, hdim] tensors
        mlp_fcs = self.mlp_bank[:, 0, :, :].unbind(0)  # tuple of [mlp_hdim, dim] tensors
        mlp_projs = self.mlp_bank[:, 1, :, :].unbind(0)  # tuple of [mlp_hdim, dim] tensors

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            
            # Get weights for this layer from banks
            qkvo_w = attn_weights[self.layer_to_attn_idx[i]] if i in self.layer_to_attn_idx else None
            c_fc = mlp_fcs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            c_proj = mlp_projs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            
            x = self.blocks[i](x, attn_args, qkvo_w, c_fc, c_proj)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss
# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages the NorMuonAndAdam for all parameters with explicit ordering.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Adam optimizers are only stepped on odd steps @classiclarryd
        3. Explicit scatter_order and work_order for communication scheduling (no backward hooks)
        4. Muon has a linear momentum warmup and cooldown schedule
        5. Learning rates follow a linear decay schedule
        6. Embed is tied to lm_head until split step (2/3 of training), then untied @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm_head at 2/3 of training (weights and optimizer state copied)
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        
        # - Ordering dictates when to launch reduce/reduce_scatter operations
        # - "sharded" parameters use reduce_scatter/all_gather and "replicated" ones use all_reduce
        # - lr_mul and wd_mul are per-parameter learning rate and weight decay multipliers
        self.param_table = {
            "attn":           {"optim": "normuon", "comms": "sharded",    "adam_betas": None},
            "mlp":            {"optim": "normuon", "comms": "sharded",    "adam_betas": None},         
            "scalars":        {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99], "lr_mul": 5.0,  "wd_mul": 0.0},
            "ve0":            {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.75, 0.95], "lr_mul": 75.,  "wd_mul": 5.0},
            "ve1":            {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.75, 0.95], "lr_mul": 75.,  "wd_mul": 5.0},
            "ve2":            {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.75, 0.95], "lr_mul": 75.,  "wd_mul": 5.0},
            "smear_gate":     {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99], "lr_mul": 0.01, "wd_mul": 0.0},
            "skip_gate":      {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99], "lr_mul": 0.05, "wd_mul": 0.0},
            "attn_gate_bank": {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99]},
            "ve_gate_bank":   {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99]},
            "x0_lambdas":     {"optim": "adam",    "comms": "replicated", "adam_betas": [0.65, 0.95], "lr_mul": 5.0,  "wd_mul": 0.0},
            "lm_head":        {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.5,  0.95], "wd_mul": 150.},
            "embed":          {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.5,  0.95], "wd_mul": 150.},
        }

        # - Process smaller/faster params first while large reduces complete
        # - lm_head must complete before embed sync (when tied)
        self.work_order = [
            "scalars", "smear_gate", "skip_gate", "attn_gate_bank", "ve_gate_bank", "x0_lambdas",  # Small, fast
            "ve0", "ve1", "ve2",  # Medium
            "lm_head", "embed",   # lm_head must complete before embed sync (when tied)
            "attn", "mlp",        # Large, polar express - process last to maximize overlap
        ]

        adam_defaults = dict(
            lr=0.008,
            eps=1e-10,
            weight_decay=0.005,
        )
        
        normuon_defaults = dict(
            lr=0.023,
            momentum=0.95,
            beta2=0.95,
            weight_decay=1.2,
        )
        
        self.optimizer = NorMuonAndAdam(
            model.named_parameters(),
            param_table=self.param_table,
            scatter_order=list(self.param_table.keys()),  # Dict order defines scatter priority
            work_order=self.work_order,
            adam_defaults=adam_defaults,
            normuon_defaults=normuon_defaults,
        )

        # Split embed from lm_head at 2/3 of training (on an odd step so Adam updates)
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_adam_step(self, step: int):
        """Adam params are only updated on odd steps."""
        return step % 2 == 1

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
            self.batch_size = new_batch_size
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        do_adam = self._is_adam_step(step)
        
        # Update learning rates and momentum for all params
        for param, p_cfg in self.optimizer.param_cfgs.items():
            p_cfg.lr = p_cfg.initial_lr * step_lr
            if p_cfg.optim == "normuon":
                p_cfg.momentum = muon_momentum
        
        # Step optimizer with do_adam flag
        self.optimizer.step(do_adam=do_adam)
        
        # At split step: copy lm_head optimizer state to embed and mark as split
        if step == self.split_step:
            self.optimizer.copy_lm_state_to_embed()

    def reset(self, state=None):
        if state is not None:
            self.optimizer.load_state_dict(state)

        # Reset NorMuon momentum buffers and split_embed state
        self.optimizer.reset()

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return copy.deepcopy(self.optimizer.state_dict())

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1725  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
model.attn_bank.data = model.attn_bank.data.bfloat16()
model.mlp_bank.data = model.mlp_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizer=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizer"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizer=training_manager.get_state())
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        inputs, targets, cum_seqlens = train_loader.send(training_manager.train_loader_send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


----------------------------------------
# triton_kernels.py
----------------------------------------

import torch
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# -----------------------------------------------------------------------------
# Triton kernel for MLP: relu(x @ W1.T)^2, by @andrewbriand, @jrauvola

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy


@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None
====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan 19 02:18:05 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:00:0B.0 Off |                    0 |
| N/A   30C    P0             108W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:00:0C.0 Off |                    0 |
| N/A   34C    P0             115W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:00:0D.0 Off |                    0 |
| N/A   35C    P0             114W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:00:0E.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:00:0F.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:00:10.0 Off |                    0 |
| N/A   36C    P0             114W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:00:11.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:00:12.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    189110      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    1   N/A  N/A    189111      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    2   N/A  N/A    189112      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    3   N/A  N/A    189113      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    4   N/A  N/A    189114      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    5   N/A  N/A    189115      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    6   N/A  N/A    189116      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    7   N/A  N/A    189117      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 574, 575, 576, 1149, 1150, 1151, 1724, 1725, 1726] for warmup
Resetting Model
step:0/1765 val_loss:10.8293 train_time:0ms step_avg:0.04ms
step:1/1765 train_time:80ms step_avg:80.39ms
step:2/1765 train_time:103ms step_avg:51.43ms
step:3/1765 train_time:123ms step_avg:40.87ms
step:4/1765 train_time:150ms step_avg:37.57ms
step:5/1765 train_time:180ms step_avg:36.05ms
step:6/1765 train_time:436ms step_avg:72.73ms
step:7/1765 train_time:455ms step_avg:65.01ms
step:8/1765 train_time:474ms step_avg:59.30ms
step:9/1765 train_time:495ms step_avg:55.01ms
step:10/1765 train_time:528ms step_avg:52.85ms
step:11/1765 train_time:558ms step_avg:50.76ms
step:12/1765 train_time:592ms step_avg:49.32ms
step:13/1765 train_time:622ms step_avg:47.85ms
step:14/1765 train_time:656ms step_avg:46.85ms
step:15/1765 train_time:687ms step_avg:45.77ms
step:16/1765 train_time:721ms step_avg:45.04ms
step:17/1765 train_time:751ms step_avg:44.17ms
step:18/1765 train_time:784ms step_avg:43.56ms
step:19/1765 train_time:814ms step_avg:42.86ms
step:20/1765 train_time:849ms step_avg:42.46ms
step:21/1765 train_time:879ms step_avg:41.84ms
step:22/1765 train_time:912ms step_avg:41.47ms
step:23/1765 train_time:942ms step_avg:40.97ms
step:24/1765 train_time:976ms step_avg:40.67ms
step:25/1765 train_time:1006ms step_avg:40.25ms
step:26/1765 train_time:1040ms step_avg:39.99ms
step:27/1765 train_time:1070ms step_avg:39.64ms
step:28/1765 train_time:1104ms step_avg:39.44ms
step:29/1765 train_time:1134ms step_avg:39.12ms
step:30/1765 train_time:1168ms step_avg:38.94ms
step:31/1765 train_time:1198ms step_avg:38.65ms
step:32/1765 train_time:1232ms step_avg:38.49ms
step:33/1765 train_time:1262ms step_avg:38.24ms
step:34/1765 train_time:1295ms step_avg:38.10ms
step:35/1765 train_time:1327ms step_avg:37.92ms
step:36/1765 train_time:1363ms step_avg:37.86ms
step:37/1765 train_time:1393ms step_avg:37.64ms
step:38/1765 train_time:1428ms step_avg:37.58ms
step:39/1765 train_time:1459ms step_avg:37.42ms
step:40/1765 train_time:1494ms step_avg:37.35ms
step:41/1765 train_time:1526ms step_avg:37.21ms
step:42/1765 train_time:1560ms step_avg:37.15ms
step:43/1765 train_time:1591ms step_avg:37.00ms
step:44/1765 train_time:1625ms step_avg:36.94ms
step:45/1765 train_time:1656ms step_avg:36.80ms
step:46/1765 train_time:1690ms step_avg:36.74ms
step:47/1765 train_time:1720ms step_avg:36.60ms
step:48/1765 train_time:1754ms step_avg:36.55ms
step:49/1765 train_time:1785ms step_avg:36.42ms
step:50/1765 train_time:1818ms step_avg:36.36ms
step:51/1765 train_time:1848ms step_avg:36.24ms
step:52/1765 train_time:1882ms step_avg:36.19ms
step:53/1765 train_time:1912ms step_avg:36.08ms
step:54/1765 train_time:1947ms step_avg:36.05ms
step:55/1765 train_time:1977ms step_avg:35.94ms
step:56/1765 train_time:2011ms step_avg:35.91ms
step:57/1765 train_time:2041ms step_avg:35.81ms
step:58/1765 train_time:2075ms step_avg:35.77ms
step:59/1765 train_time:2105ms step_avg:35.68ms
step:60/1765 train_time:2139ms step_avg:35.65ms
step:61/1765 train_time:2170ms step_avg:35.57ms
step:62/1765 train_time:2203ms step_avg:35.54ms
step:63/1765 train_time:2233ms step_avg:35.45ms
step:64/1765 train_time:2268ms step_avg:35.44ms
step:65/1765 train_time:2298ms step_avg:35.35ms
step:66/1765 train_time:2332ms step_avg:35.34ms
step:67/1765 train_time:2363ms step_avg:35.26ms
step:68/1765 train_time:2397ms step_avg:35.25ms
step:69/1765 train_time:2428ms step_avg:35.20ms
step:70/1765 train_time:2463ms step_avg:35.18ms
step:71/1765 train_time:2493ms step_avg:35.12ms
step:72/1765 train_time:2528ms step_avg:35.11ms
step:73/1765 train_time:2559ms step_avg:35.05ms
step:74/1765 train_time:2593ms step_avg:35.04ms
step:75/1765 train_time:2624ms step_avg:34.98ms
step:76/1765 train_time:2658ms step_avg:34.97ms
step:77/1765 train_time:2688ms step_avg:34.91ms
step:78/1765 train_time:2723ms step_avg:34.91ms
step:79/1765 train_time:2752ms step_avg:34.83ms
step:80/1765 train_time:2787ms step_avg:34.84ms
step:81/1765 train_time:2816ms step_avg:34.77ms
step:82/1765 train_time:2850ms step_avg:34.76ms
step:83/1765 train_time:2881ms step_avg:34.71ms
step:84/1765 train_time:2915ms step_avg:34.71ms
step:85/1765 train_time:2946ms step_avg:34.65ms
step:86/1765 train_time:2980ms step_avg:34.65ms
step:87/1765 train_time:3010ms step_avg:34.59ms
step:88/1765 train_time:3043ms step_avg:34.58ms
step:89/1765 train_time:3073ms step_avg:34.53ms
step:90/1765 train_time:3106ms step_avg:34.52ms
step:91/1765 train_time:3137ms step_avg:34.47ms
step:92/1765 train_time:3171ms step_avg:34.47ms
step:93/1765 train_time:3201ms step_avg:34.42ms
step:94/1765 train_time:3235ms step_avg:34.42ms
step:95/1765 train_time:3266ms step_avg:34.38ms
step:96/1765 train_time:3300ms step_avg:34.37ms
step:97/1765 train_time:3330ms step_avg:34.33ms
step:98/1765 train_time:3365ms step_avg:34.34ms
step:99/1765 train_time:3395ms step_avg:34.30ms
step:100/1765 train_time:3429ms step_avg:34.29ms
step:101/1765 train_time:3461ms step_avg:34.26ms
step:102/1765 train_time:3495ms step_avg:34.26ms
step:103/1765 train_time:3526ms step_avg:34.23ms
step:104/1765 train_time:3561ms step_avg:34.24ms
step:105/1765 train_time:3591ms step_avg:34.20ms
step:106/1765 train_time:3625ms step_avg:34.20ms
step:107/1765 train_time:3656ms step_avg:34.17ms
step:108/1765 train_time:3692ms step_avg:34.18ms
step:109/1765 train_time:3720ms step_avg:34.13ms
step:110/1765 train_time:3756ms step_avg:34.14ms
step:111/1765 train_time:3785ms step_avg:34.10ms
step:112/1765 train_time:3819ms step_avg:34.09ms
step:113/1765 train_time:3849ms step_avg:34.07ms
step:114/1765 train_time:3883ms step_avg:34.07ms
step:115/1765 train_time:3914ms step_avg:34.03ms
step:116/1765 train_time:3948ms step_avg:34.03ms
step:117/1765 train_time:3978ms step_avg:34.00ms
step:118/1765 train_time:4012ms step_avg:34.00ms
step:119/1765 train_time:4042ms step_avg:33.97ms
step:120/1765 train_time:4076ms step_avg:33.96ms
step:121/1765 train_time:4106ms step_avg:33.93ms
step:122/1765 train_time:4140ms step_avg:33.93ms
step:123/1765 train_time:4170ms step_avg:33.90ms
step:124/1765 train_time:4204ms step_avg:33.90ms
step:125/1765 train_time:4234ms step_avg:33.87ms
step:126/1765 train_time:4268ms step_avg:33.87ms
step:127/1765 train_time:4298ms step_avg:33.85ms
step:128/1765 train_time:4332ms step_avg:33.85ms
step:129/1765 train_time:4363ms step_avg:33.82ms
step:130/1765 train_time:4396ms step_avg:33.82ms
step:131/1765 train_time:4427ms step_avg:33.79ms
step:132/1765 train_time:4462ms step_avg:33.80ms
step:133/1765 train_time:4492ms step_avg:33.78ms
step:134/1765 train_time:4527ms step_avg:33.78ms
step:135/1765 train_time:4558ms step_avg:33.76ms
step:136/1765 train_time:4592ms step_avg:33.77ms
step:137/1765 train_time:4623ms step_avg:33.75ms
step:138/1765 train_time:4657ms step_avg:33.75ms
step:139/1765 train_time:4688ms step_avg:33.73ms
step:140/1765 train_time:4722ms step_avg:33.73ms
step:141/1765 train_time:4752ms step_avg:33.70ms
step:142/1765 train_time:4786ms step_avg:33.71ms
step:143/1765 train_time:4817ms step_avg:33.68ms
step:144/1765 train_time:4851ms step_avg:33.69ms
step:145/1765 train_time:4882ms step_avg:33.67ms
step:146/1765 train_time:4916ms step_avg:33.67ms
step:147/1765 train_time:4946ms step_avg:33.65ms
step:148/1765 train_time:4980ms step_avg:33.65ms
step:149/1765 train_time:5009ms step_avg:33.62ms
step:150/1765 train_time:5044ms step_avg:33.62ms
step:151/1765 train_time:5074ms step_avg:33.60ms
step:152/1765 train_time:5107ms step_avg:33.60ms
step:153/1765 train_time:5137ms step_avg:33.58ms
step:154/1765 train_time:5171ms step_avg:33.58ms
step:155/1765 train_time:5201ms step_avg:33.55ms
step:156/1765 train_time:5235ms step_avg:33.55ms
step:157/1765 train_time:5265ms step_avg:33.53ms
step:158/1765 train_time:5299ms step_avg:33.54ms
step:159/1765 train_time:5329ms step_avg:33.52ms
step:160/1765 train_time:5364ms step_avg:33.52ms
step:161/1765 train_time:5394ms step_avg:33.50ms
step:162/1765 train_time:5428ms step_avg:33.50ms
step:163/1765 train_time:5458ms step_avg:33.49ms
step:164/1765 train_time:5493ms step_avg:33.49ms
step:165/1765 train_time:5524ms step_avg:33.48ms
step:166/1765 train_time:5558ms step_avg:33.48ms
step:167/1765 train_time:5589ms step_avg:33.47ms
step:168/1765 train_time:5624ms step_avg:33.48ms
step:169/1765 train_time:5655ms step_avg:33.46ms
step:170/1765 train_time:5689ms step_avg:33.46ms
step:171/1765 train_time:5719ms step_avg:33.45ms
step:172/1765 train_time:5753ms step_avg:33.45ms
step:173/1765 train_time:5783ms step_avg:33.43ms
step:174/1765 train_time:5817ms step_avg:33.43ms
step:175/1765 train_time:5847ms step_avg:33.41ms
step:176/1765 train_time:5881ms step_avg:33.41ms
step:177/1765 train_time:5911ms step_avg:33.40ms
step:178/1765 train_time:5945ms step_avg:33.40ms
step:179/1765 train_time:5975ms step_avg:33.38ms
step:180/1765 train_time:6009ms step_avg:33.39ms
step:181/1765 train_time:6040ms step_avg:33.37ms
step:182/1765 train_time:6074ms step_avg:33.37ms
step:183/1765 train_time:6104ms step_avg:33.35ms
step:184/1765 train_time:6137ms step_avg:33.36ms
step:185/1765 train_time:6168ms step_avg:33.34ms
step:186/1765 train_time:6201ms step_avg:33.34ms
step:187/1765 train_time:6232ms step_avg:33.32ms
step:188/1765 train_time:6266ms step_avg:33.33ms
step:189/1765 train_time:6296ms step_avg:33.31ms
step:190/1765 train_time:6331ms step_avg:33.32ms
step:191/1765 train_time:6361ms step_avg:33.31ms
step:192/1765 train_time:6395ms step_avg:33.31ms
step:193/1765 train_time:6426ms step_avg:33.29ms
step:194/1765 train_time:6460ms step_avg:33.30ms
step:195/1765 train_time:6490ms step_avg:33.28ms
step:196/1765 train_time:6524ms step_avg:33.29ms
step:197/1765 train_time:6554ms step_avg:33.27ms
step:198/1765 train_time:6587ms step_avg:33.27ms
step:199/1765 train_time:6619ms step_avg:33.26ms
step:200/1765 train_time:6653ms step_avg:33.26ms
step:201/1765 train_time:6684ms step_avg:33.25ms
step:202/1765 train_time:6718ms step_avg:33.26ms
step:203/1765 train_time:6748ms step_avg:33.24ms
step:204/1765 train_time:6782ms step_avg:33.25ms
step:205/1765 train_time:6813ms step_avg:33.23ms
step:206/1765 train_time:6847ms step_avg:33.24ms
step:207/1765 train_time:6877ms step_avg:33.22ms
step:208/1765 train_time:6911ms step_avg:33.23ms
step:209/1765 train_time:6941ms step_avg:33.21ms
step:210/1765 train_time:6975ms step_avg:33.21ms
step:211/1765 train_time:7005ms step_avg:33.20ms
step:212/1765 train_time:7040ms step_avg:33.21ms
step:213/1765 train_time:7070ms step_avg:33.19ms
step:214/1765 train_time:7103ms step_avg:33.19ms
step:215/1765 train_time:7134ms step_avg:33.18ms
step:216/1765 train_time:7168ms step_avg:33.18ms
step:217/1765 train_time:7198ms step_avg:33.17ms
step:218/1765 train_time:7232ms step_avg:33.18ms
step:219/1765 train_time:7263ms step_avg:33.16ms
step:220/1765 train_time:7297ms step_avg:33.17ms
step:221/1765 train_time:7327ms step_avg:33.15ms
step:222/1765 train_time:7361ms step_avg:33.16ms
step:223/1765 train_time:7392ms step_avg:33.15ms
step:224/1765 train_time:7426ms step_avg:33.15ms
step:225/1765 train_time:7456ms step_avg:33.14ms
step:226/1765 train_time:7490ms step_avg:33.14ms
step:227/1765 train_time:7521ms step_avg:33.13ms
step:228/1765 train_time:7554ms step_avg:33.13ms
step:229/1765 train_time:7585ms step_avg:33.12ms
step:230/1765 train_time:7620ms step_avg:33.13ms
step:231/1765 train_time:7651ms step_avg:33.12ms
step:232/1765 train_time:7685ms step_avg:33.12ms
step:233/1765 train_time:7715ms step_avg:33.11ms
step:234/1765 train_time:7749ms step_avg:33.12ms
step:235/1765 train_time:7780ms step_avg:33.11ms
step:236/1765 train_time:7814ms step_avg:33.11ms
step:237/1765 train_time:7844ms step_avg:33.10ms
step:238/1765 train_time:7878ms step_avg:33.10ms
step:239/1765 train_time:7908ms step_avg:33.09ms
step:240/1765 train_time:7942ms step_avg:33.09ms
step:241/1765 train_time:7972ms step_avg:33.08ms
step:242/1765 train_time:8006ms step_avg:33.08ms
step:243/1765 train_time:8037ms step_avg:33.07ms
step:244/1765 train_time:8070ms step_avg:33.08ms
step:245/1765 train_time:8100ms step_avg:33.06ms
step:246/1765 train_time:8134ms step_avg:33.07ms
step:247/1765 train_time:8165ms step_avg:33.06ms
step:248/1765 train_time:8199ms step_avg:33.06ms
step:249/1765 train_time:8229ms step_avg:33.05ms
step:250/1765 train_time:8263ms step_avg:33.05ms
step:250/1765 val_loss:4.6179 train_time:8309ms step_avg:33.23ms
step:251/1765 train_time:8330ms step_avg:33.19ms
step:252/1765 train_time:8350ms step_avg:33.14ms
step:253/1765 train_time:8368ms step_avg:33.07ms
step:254/1765 train_time:8394ms step_avg:33.05ms
step:255/1765 train_time:8428ms step_avg:33.05ms
step:256/1765 train_time:8463ms step_avg:33.06ms
step:257/1765 train_time:8494ms step_avg:33.05ms
step:258/1765 train_time:8529ms step_avg:33.06ms
step:259/1765 train_time:8559ms step_avg:33.05ms
step:260/1765 train_time:8593ms step_avg:33.05ms
step:261/1765 train_time:8623ms step_avg:33.04ms
step:262/1765 train_time:8657ms step_avg:33.04ms
step:263/1765 train_time:8687ms step_avg:33.03ms
step:264/1765 train_time:8720ms step_avg:33.03ms
step:265/1765 train_time:8751ms step_avg:33.02ms
step:266/1765 train_time:8784ms step_avg:33.02ms
step:267/1765 train_time:8815ms step_avg:33.01ms
step:268/1765 train_time:8848ms step_avg:33.02ms
step:269/1765 train_time:8878ms step_avg:33.01ms
step:270/1765 train_time:8912ms step_avg:33.01ms
step:271/1765 train_time:8942ms step_avg:33.00ms
step:272/1765 train_time:8976ms step_avg:33.00ms
step:273/1765 train_time:9006ms step_avg:32.99ms
step:274/1765 train_time:9039ms step_avg:32.99ms
step:275/1765 train_time:9070ms step_avg:32.98ms
step:276/1765 train_time:9103ms step_avg:32.98ms
step:277/1765 train_time:9133ms step_avg:32.97ms
step:278/1765 train_time:9167ms step_avg:32.97ms
step:279/1765 train_time:9197ms step_avg:32.96ms
step:280/1765 train_time:9230ms step_avg:32.97ms
step:281/1765 train_time:9261ms step_avg:32.96ms
step:282/1765 train_time:9295ms step_avg:32.96ms
step:283/1765 train_time:9326ms step_avg:32.95ms
step:284/1765 train_time:9360ms step_avg:32.96ms
step:285/1765 train_time:9391ms step_avg:32.95ms
step:286/1765 train_time:9426ms step_avg:32.96ms
step:287/1765 train_time:9456ms step_avg:32.95ms
step:288/1765 train_time:9491ms step_avg:32.95ms
step:289/1765 train_time:9522ms step_avg:32.95ms
step:290/1765 train_time:9556ms step_avg:32.95ms
step:291/1765 train_time:9587ms step_avg:32.94ms
step:292/1765 train_time:9620ms step_avg:32.95ms
step:293/1765 train_time:9651ms step_avg:32.94ms
step:294/1765 train_time:9685ms step_avg:32.94ms
step:295/1765 train_time:9715ms step_avg:32.93ms
step:296/1765 train_time:9749ms step_avg:32.93ms
step:297/1765 train_time:9779ms step_avg:32.93ms
step:298/1765 train_time:9812ms step_avg:32.93ms
step:299/1765 train_time:9843ms step_avg:32.92ms
step:300/1765 train_time:9877ms step_avg:32.92ms
step:301/1765 train_time:9908ms step_avg:32.92ms
step:302/1765 train_time:9941ms step_avg:32.92ms
step:303/1765 train_time:9971ms step_avg:32.91ms
step:304/1765 train_time:10004ms step_avg:32.91ms
step:305/1765 train_time:10034ms step_avg:32.90ms
step:306/1765 train_time:10068ms step_avg:32.90ms
step:307/1765 train_time:10098ms step_avg:32.89ms
step:308/1765 train_time:10132ms step_avg:32.90ms
step:309/1765 train_time:10162ms step_avg:32.89ms
step:310/1765 train_time:10196ms step_avg:32.89ms
step:311/1765 train_time:10227ms step_avg:32.88ms
step:312/1765 train_time:10261ms step_avg:32.89ms
step:313/1765 train_time:10291ms step_avg:32.88ms
step:314/1765 train_time:10325ms step_avg:32.88ms
step:315/1765 train_time:10356ms step_avg:32.88ms
step:316/1765 train_time:10391ms step_avg:32.88ms
step:317/1765 train_time:10421ms step_avg:32.88ms
step:318/1765 train_time:10456ms step_avg:32.88ms
step:319/1765 train_time:10487ms step_avg:32.87ms
step:320/1765 train_time:10520ms step_avg:32.88ms
step:321/1765 train_time:10551ms step_avg:32.87ms
step:322/1765 train_time:10585ms step_avg:32.87ms
step:323/1765 train_time:10615ms step_avg:32.86ms
step:324/1765 train_time:10649ms step_avg:32.87ms
step:325/1765 train_time:10679ms step_avg:32.86ms
step:326/1765 train_time:10713ms step_avg:32.86ms
step:327/1765 train_time:10743ms step_avg:32.85ms
step:328/1765 train_time:10777ms step_avg:32.86ms
step:329/1765 train_time:10808ms step_avg:32.85ms
step:330/1765 train_time:10842ms step_avg:32.85ms
step:331/1765 train_time:10872ms step_avg:32.85ms
step:332/1765 train_time:10906ms step_avg:32.85ms
step:333/1765 train_time:10937ms step_avg:32.84ms
step:334/1765 train_time:10970ms step_avg:32.85ms
step:335/1765 train_time:11001ms step_avg:32.84ms
step:336/1765 train_time:11034ms step_avg:32.84ms
step:337/1765 train_time:11064ms step_avg:32.83ms
step:338/1765 train_time:11098ms step_avg:32.83ms
step:339/1765 train_time:11129ms step_avg:32.83ms
step:340/1765 train_time:11163ms step_avg:32.83ms
step:341/1765 train_time:11193ms step_avg:32.82ms
step:342/1765 train_time:11227ms step_avg:32.83ms
step:343/1765 train_time:11257ms step_avg:32.82ms
step:344/1765 train_time:11291ms step_avg:32.82ms
step:345/1765 train_time:11322ms step_avg:32.82ms
step:346/1765 train_time:11356ms step_avg:32.82ms
step:347/1765 train_time:11387ms step_avg:32.81ms
step:348/1765 train_time:11420ms step_avg:32.82ms
step:349/1765 train_time:11451ms step_avg:32.81ms
step:350/1765 train_time:11485ms step_avg:32.82ms
step:351/1765 train_time:11516ms step_avg:32.81ms
step:352/1765 train_time:11550ms step_avg:32.81ms
step:353/1765 train_time:11580ms step_avg:32.81ms
step:354/1765 train_time:11615ms step_avg:32.81ms
step:355/1765 train_time:11645ms step_avg:32.80ms
step:356/1765 train_time:11679ms step_avg:32.81ms
step:357/1765 train_time:11709ms step_avg:32.80ms
step:358/1765 train_time:11743ms step_avg:32.80ms
step:359/1765 train_time:11773ms step_avg:32.79ms
step:360/1765 train_time:11807ms step_avg:32.80ms
step:361/1765 train_time:11837ms step_avg:32.79ms
step:362/1765 train_time:11871ms step_avg:32.79ms
step:363/1765 train_time:11901ms step_avg:32.78ms
step:364/1765 train_time:11935ms step_avg:32.79ms
step:365/1765 train_time:11965ms step_avg:32.78ms
step:366/1765 train_time:11999ms step_avg:32.78ms
step:367/1765 train_time:12029ms step_avg:32.78ms
step:368/1765 train_time:12063ms step_avg:32.78ms
step:369/1765 train_time:12094ms step_avg:32.77ms
step:370/1765 train_time:12127ms step_avg:32.78ms
step:371/1765 train_time:12158ms step_avg:32.77ms
step:372/1765 train_time:12191ms step_avg:32.77ms
step:373/1765 train_time:12222ms step_avg:32.77ms
step:374/1765 train_time:12256ms step_avg:32.77ms
step:375/1765 train_time:12286ms step_avg:32.76ms
step:376/1765 train_time:12320ms step_avg:32.77ms
step:377/1765 train_time:12351ms step_avg:32.76ms
step:378/1765 train_time:12385ms step_avg:32.77ms
step:379/1765 train_time:12416ms step_avg:32.76ms
step:380/1765 train_time:12450ms step_avg:32.76ms
step:381/1765 train_time:12480ms step_avg:32.76ms
step:382/1765 train_time:12514ms step_avg:32.76ms
step:383/1765 train_time:12545ms step_avg:32.75ms
step:384/1765 train_time:12579ms step_avg:32.76ms
step:385/1765 train_time:12609ms step_avg:32.75ms
step:386/1765 train_time:12643ms step_avg:32.75ms
step:387/1765 train_time:12673ms step_avg:32.75ms
step:388/1765 train_time:12707ms step_avg:32.75ms
step:389/1765 train_time:12738ms step_avg:32.74ms
step:390/1765 train_time:12771ms step_avg:32.75ms
step:391/1765 train_time:12801ms step_avg:32.74ms
step:392/1765 train_time:12835ms step_avg:32.74ms
step:393/1765 train_time:12865ms step_avg:32.74ms
step:394/1765 train_time:12899ms step_avg:32.74ms
step:395/1765 train_time:12929ms step_avg:32.73ms
step:396/1765 train_time:12963ms step_avg:32.74ms
step:397/1765 train_time:12993ms step_avg:32.73ms
step:398/1765 train_time:13028ms step_avg:32.73ms
step:399/1765 train_time:13058ms step_avg:32.73ms
step:400/1765 train_time:13091ms step_avg:32.73ms
step:401/1765 train_time:13122ms step_avg:32.72ms
step:402/1765 train_time:13156ms step_avg:32.73ms
step:403/1765 train_time:13186ms step_avg:32.72ms
step:404/1765 train_time:13220ms step_avg:32.72ms
step:405/1765 train_time:13251ms step_avg:32.72ms
step:406/1765 train_time:13286ms step_avg:32.72ms
step:407/1765 train_time:13316ms step_avg:32.72ms
step:408/1765 train_time:13350ms step_avg:32.72ms
step:409/1765 train_time:13380ms step_avg:32.71ms
step:410/1765 train_time:13414ms step_avg:32.72ms
step:411/1765 train_time:13444ms step_avg:32.71ms
step:412/1765 train_time:13478ms step_avg:32.71ms
step:413/1765 train_time:13509ms step_avg:32.71ms
step:414/1765 train_time:13542ms step_avg:32.71ms
step:415/1765 train_time:13573ms step_avg:32.71ms
step:416/1765 train_time:13607ms step_avg:32.71ms
step:417/1765 train_time:13637ms step_avg:32.70ms
step:418/1765 train_time:13671ms step_avg:32.71ms
step:419/1765 train_time:13702ms step_avg:32.70ms
step:420/1765 train_time:13736ms step_avg:32.70ms
step:421/1765 train_time:13766ms step_avg:32.70ms
step:422/1765 train_time:13800ms step_avg:32.70ms
step:423/1765 train_time:13831ms step_avg:32.70ms
step:424/1765 train_time:13865ms step_avg:32.70ms
step:425/1765 train_time:13895ms step_avg:32.69ms
step:426/1765 train_time:13928ms step_avg:32.70ms
step:427/1765 train_time:13959ms step_avg:32.69ms
step:428/1765 train_time:13992ms step_avg:32.69ms
step:429/1765 train_time:14023ms step_avg:32.69ms
step:430/1765 train_time:14057ms step_avg:32.69ms
step:431/1765 train_time:14088ms step_avg:32.69ms
step:432/1765 train_time:14122ms step_avg:32.69ms
step:433/1765 train_time:14152ms step_avg:32.68ms
step:434/1765 train_time:14186ms step_avg:32.69ms
step:435/1765 train_time:14216ms step_avg:32.68ms
step:436/1765 train_time:14251ms step_avg:32.68ms
step:437/1765 train_time:14281ms step_avg:32.68ms
step:438/1765 train_time:14314ms step_avg:32.68ms
step:439/1765 train_time:14344ms step_avg:32.68ms
step:440/1765 train_time:14378ms step_avg:32.68ms
step:441/1765 train_time:14409ms step_avg:32.67ms
step:442/1765 train_time:14443ms step_avg:32.68ms
step:443/1765 train_time:14473ms step_avg:32.67ms
step:444/1765 train_time:14507ms step_avg:32.67ms
step:445/1765 train_time:14538ms step_avg:32.67ms
step:446/1765 train_time:14571ms step_avg:32.67ms
step:447/1765 train_time:14602ms step_avg:32.67ms
step:448/1765 train_time:14636ms step_avg:32.67ms
step:449/1765 train_time:14666ms step_avg:32.66ms
step:450/1765 train_time:14700ms step_avg:32.67ms
step:451/1765 train_time:14730ms step_avg:32.66ms
step:452/1765 train_time:14764ms step_avg:32.66ms
step:453/1765 train_time:14795ms step_avg:32.66ms
step:454/1765 train_time:14828ms step_avg:32.66ms
step:455/1765 train_time:14859ms step_avg:32.66ms
step:456/1765 train_time:14893ms step_avg:32.66ms
step:457/1765 train_time:14923ms step_avg:32.65ms
step:458/1765 train_time:14956ms step_avg:32.66ms
step:459/1765 train_time:14987ms step_avg:32.65ms
step:460/1765 train_time:15021ms step_avg:32.65ms
step:461/1765 train_time:15051ms step_avg:32.65ms
step:462/1765 train_time:15085ms step_avg:32.65ms
step:463/1765 train_time:15115ms step_avg:32.65ms
step:464/1765 train_time:15149ms step_avg:32.65ms
step:465/1765 train_time:15179ms step_avg:32.64ms
step:466/1765 train_time:15213ms step_avg:32.65ms
step:467/1765 train_time:15243ms step_avg:32.64ms
step:468/1765 train_time:15277ms step_avg:32.64ms
step:469/1765 train_time:15308ms step_avg:32.64ms
step:470/1765 train_time:15342ms step_avg:32.64ms
step:471/1765 train_time:15372ms step_avg:32.64ms
step:472/1765 train_time:15406ms step_avg:32.64ms
step:473/1765 train_time:15437ms step_avg:32.64ms
step:474/1765 train_time:15471ms step_avg:32.64ms
step:475/1765 train_time:15501ms step_avg:32.63ms
step:476/1765 train_time:15535ms step_avg:32.64ms
step:477/1765 train_time:15566ms step_avg:32.63ms
step:478/1765 train_time:15599ms step_avg:32.63ms
step:479/1765 train_time:15630ms step_avg:32.63ms
step:480/1765 train_time:15664ms step_avg:32.63ms
step:481/1765 train_time:15694ms step_avg:32.63ms
step:482/1765 train_time:15728ms step_avg:32.63ms
step:483/1765 train_time:15759ms step_avg:32.63ms
step:484/1765 train_time:15792ms step_avg:32.63ms
step:485/1765 train_time:15822ms step_avg:32.62ms
step:486/1765 train_time:15857ms step_avg:32.63ms
step:487/1765 train_time:15887ms step_avg:32.62ms
step:488/1765 train_time:15921ms step_avg:32.62ms
step:489/1765 train_time:15952ms step_avg:32.62ms
step:490/1765 train_time:15986ms step_avg:32.62ms
step:491/1765 train_time:16016ms step_avg:32.62ms
step:492/1765 train_time:16051ms step_avg:32.62ms
step:493/1765 train_time:16081ms step_avg:32.62ms
step:494/1765 train_time:16115ms step_avg:32.62ms
step:495/1765 train_time:16146ms step_avg:32.62ms
step:496/1765 train_time:16179ms step_avg:32.62ms
step:497/1765 train_time:16209ms step_avg:32.61ms
step:498/1765 train_time:16243ms step_avg:32.62ms
step:499/1765 train_time:16274ms step_avg:32.61ms
step:500/1765 train_time:16308ms step_avg:32.62ms
step:500/1765 val_loss:4.2825 train_time:16354ms step_avg:32.71ms
step:501/1765 train_time:16374ms step_avg:32.68ms
step:502/1765 train_time:16393ms step_avg:32.66ms
step:503/1765 train_time:16411ms step_avg:32.63ms
step:504/1765 train_time:16440ms step_avg:32.62ms
step:505/1765 train_time:16471ms step_avg:32.62ms
step:506/1765 train_time:16506ms step_avg:32.62ms
step:507/1765 train_time:16537ms step_avg:32.62ms
step:508/1765 train_time:16572ms step_avg:32.62ms
step:509/1765 train_time:16603ms step_avg:32.62ms
step:510/1765 train_time:16637ms step_avg:32.62ms
step:511/1765 train_time:16668ms step_avg:32.62ms
step:512/1765 train_time:16702ms step_avg:32.62ms
step:513/1765 train_time:16732ms step_avg:32.62ms
step:514/1765 train_time:16766ms step_avg:32.62ms
step:515/1765 train_time:16796ms step_avg:32.61ms
step:516/1765 train_time:16829ms step_avg:32.61ms
step:517/1765 train_time:16859ms step_avg:32.61ms
step:518/1765 train_time:16892ms step_avg:32.61ms
step:519/1765 train_time:16922ms step_avg:32.61ms
step:520/1765 train_time:16956ms step_avg:32.61ms
step:521/1765 train_time:16986ms step_avg:32.60ms
step:522/1765 train_time:17020ms step_avg:32.60ms
step:523/1765 train_time:17050ms step_avg:32.60ms
step:524/1765 train_time:17084ms step_avg:32.60ms
step:525/1765 train_time:17114ms step_avg:32.60ms
step:526/1765 train_time:17148ms step_avg:32.60ms
step:527/1765 train_time:17178ms step_avg:32.60ms
step:528/1765 train_time:17211ms step_avg:32.60ms
step:529/1765 train_time:17241ms step_avg:32.59ms
step:530/1765 train_time:17275ms step_avg:32.59ms
step:531/1765 train_time:17306ms step_avg:32.59ms
step:532/1765 train_time:17341ms step_avg:32.60ms
step:533/1765 train_time:17372ms step_avg:32.59ms
step:534/1765 train_time:17406ms step_avg:32.60ms
step:535/1765 train_time:17437ms step_avg:32.59ms
step:536/1765 train_time:17472ms step_avg:32.60ms
step:537/1765 train_time:17503ms step_avg:32.59ms
step:538/1765 train_time:17538ms step_avg:32.60ms
step:539/1765 train_time:17569ms step_avg:32.60ms
step:540/1765 train_time:17603ms step_avg:32.60ms
step:541/1765 train_time:17634ms step_avg:32.59ms
step:542/1765 train_time:17668ms step_avg:32.60ms
step:543/1765 train_time:17697ms step_avg:32.59ms
step:544/1765 train_time:17731ms step_avg:32.59ms
step:545/1765 train_time:17762ms step_avg:32.59ms
step:546/1765 train_time:17795ms step_avg:32.59ms
step:547/1765 train_time:17825ms step_avg:32.59ms
step:548/1765 train_time:17859ms step_avg:32.59ms
step:549/1765 train_time:17889ms step_avg:32.58ms
step:550/1765 train_time:17923ms step_avg:32.59ms
step:551/1765 train_time:17953ms step_avg:32.58ms
step:552/1765 train_time:17987ms step_avg:32.58ms
step:553/1765 train_time:18016ms step_avg:32.58ms
step:554/1765 train_time:18050ms step_avg:32.58ms
step:555/1765 train_time:18080ms step_avg:32.58ms
step:556/1765 train_time:18113ms step_avg:32.58ms
step:557/1765 train_time:18144ms step_avg:32.57ms
step:558/1765 train_time:18177ms step_avg:32.58ms
step:559/1765 train_time:18207ms step_avg:32.57ms
step:560/1765 train_time:18241ms step_avg:32.57ms
step:561/1765 train_time:18272ms step_avg:32.57ms
step:562/1765 train_time:18305ms step_avg:32.57ms
step:563/1765 train_time:18336ms step_avg:32.57ms
step:564/1765 train_time:18369ms step_avg:32.57ms
step:565/1765 train_time:18400ms step_avg:32.57ms
step:566/1765 train_time:18434ms step_avg:32.57ms
step:567/1765 train_time:18465ms step_avg:32.57ms
step:568/1765 train_time:18500ms step_avg:32.57ms
step:569/1765 train_time:18531ms step_avg:32.57ms
step:570/1765 train_time:18565ms step_avg:32.57ms
step:571/1765 train_time:18596ms step_avg:32.57ms
step:572/1765 train_time:18630ms step_avg:32.57ms
step:573/1765 train_time:18661ms step_avg:32.57ms
step:574/1765 train_time:18696ms step_avg:32.57ms
step:575/1765 train_time:18726ms step_avg:32.57ms
step:576/1765 train_time:18763ms step_avg:32.57ms
step:577/1765 train_time:18816ms step_avg:32.61ms
step:578/1765 train_time:18876ms step_avg:32.66ms
step:579/1765 train_time:18932ms step_avg:32.70ms
step:580/1765 train_time:18992ms step_avg:32.75ms
step:581/1765 train_time:19049ms step_avg:32.79ms
step:582/1765 train_time:19109ms step_avg:32.83ms
step:583/1765 train_time:19167ms step_avg:32.88ms
step:584/1765 train_time:19226ms step_avg:32.92ms
step:585/1765 train_time:19283ms step_avg:32.96ms
step:586/1765 train_time:19344ms step_avg:33.01ms
step:587/1765 train_time:19401ms step_avg:33.05ms
step:588/1765 train_time:19463ms step_avg:33.10ms
step:589/1765 train_time:19520ms step_avg:33.14ms
step:590/1765 train_time:19581ms step_avg:33.19ms
step:591/1765 train_time:19638ms step_avg:33.23ms
step:592/1765 train_time:19699ms step_avg:33.28ms
step:593/1765 train_time:19755ms step_avg:33.31ms
step:594/1765 train_time:19815ms step_avg:33.36ms
step:595/1765 train_time:19872ms step_avg:33.40ms
step:596/1765 train_time:19932ms step_avg:33.44ms
step:597/1765 train_time:19989ms step_avg:33.48ms
step:598/1765 train_time:20048ms step_avg:33.53ms
step:599/1765 train_time:20106ms step_avg:33.57ms
step:600/1765 train_time:20166ms step_avg:33.61ms
step:601/1765 train_time:20223ms step_avg:33.65ms
step:602/1765 train_time:20284ms step_avg:33.69ms
step:603/1765 train_time:20341ms step_avg:33.73ms
step:604/1765 train_time:20401ms step_avg:33.78ms
step:605/1765 train_time:20458ms step_avg:33.81ms
step:606/1765 train_time:20518ms step_avg:33.86ms
step:607/1765 train_time:20576ms step_avg:33.90ms
step:608/1765 train_time:20637ms step_avg:33.94ms
step:609/1765 train_time:20693ms step_avg:33.98ms
step:610/1765 train_time:20754ms step_avg:34.02ms
step:611/1765 train_time:20811ms step_avg:34.06ms
step:612/1765 train_time:20871ms step_avg:34.10ms
step:613/1765 train_time:20928ms step_avg:34.14ms
step:614/1765 train_time:20989ms step_avg:34.18ms
step:615/1765 train_time:21046ms step_avg:34.22ms
step:616/1765 train_time:21106ms step_avg:34.26ms
step:617/1765 train_time:21163ms step_avg:34.30ms
step:618/1765 train_time:21223ms step_avg:34.34ms
step:619/1765 train_time:21280ms step_avg:34.38ms
step:620/1765 train_time:21340ms step_avg:34.42ms
step:621/1765 train_time:21397ms step_avg:34.46ms
step:622/1765 train_time:21457ms step_avg:34.50ms
step:623/1765 train_time:21515ms step_avg:34.53ms
step:624/1765 train_time:21574ms step_avg:34.57ms
step:625/1765 train_time:21632ms step_avg:34.61ms
step:626/1765 train_time:21693ms step_avg:34.65ms
step:627/1765 train_time:21750ms step_avg:34.69ms
step:628/1765 train_time:21810ms step_avg:34.73ms
step:629/1765 train_time:21868ms step_avg:34.77ms
step:630/1765 train_time:21928ms step_avg:34.81ms
step:631/1765 train_time:21985ms step_avg:34.84ms
step:632/1765 train_time:22045ms step_avg:34.88ms
step:633/1765 train_time:22102ms step_avg:34.92ms
step:634/1765 train_time:22162ms step_avg:34.96ms
step:635/1765 train_time:22219ms step_avg:34.99ms
step:636/1765 train_time:22279ms step_avg:35.03ms
step:637/1765 train_time:22336ms step_avg:35.06ms
step:638/1765 train_time:22395ms step_avg:35.10ms
step:639/1765 train_time:22453ms step_avg:35.14ms
step:640/1765 train_time:22514ms step_avg:35.18ms
step:641/1765 train_time:22571ms step_avg:35.21ms
step:642/1765 train_time:22631ms step_avg:35.25ms
step:643/1765 train_time:22689ms step_avg:35.29ms
step:644/1765 train_time:22749ms step_avg:35.32ms
step:645/1765 train_time:22807ms step_avg:35.36ms
step:646/1765 train_time:22867ms step_avg:35.40ms
step:647/1765 train_time:22924ms step_avg:35.43ms
step:648/1765 train_time:22984ms step_avg:35.47ms
step:649/1765 train_time:23040ms step_avg:35.50ms
step:650/1765 train_time:23100ms step_avg:35.54ms
step:651/1765 train_time:23157ms step_avg:35.57ms
step:652/1765 train_time:23217ms step_avg:35.61ms
step:653/1765 train_time:23274ms step_avg:35.64ms
step:654/1765 train_time:23334ms step_avg:35.68ms
step:655/1765 train_time:23393ms step_avg:35.71ms
step:656/1765 train_time:23453ms step_avg:35.75ms
step:657/1765 train_time:23510ms step_avg:35.78ms
step:658/1765 train_time:23571ms step_avg:35.82ms
step:659/1765 train_time:23628ms step_avg:35.85ms
step:660/1765 train_time:23687ms step_avg:35.89ms
step:661/1765 train_time:23745ms step_avg:35.92ms
step:662/1765 train_time:23805ms step_avg:35.96ms
step:663/1765 train_time:23863ms step_avg:35.99ms
step:664/1765 train_time:23923ms step_avg:36.03ms
step:665/1765 train_time:23980ms step_avg:36.06ms
step:666/1765 train_time:24040ms step_avg:36.10ms
step:667/1765 train_time:24097ms step_avg:36.13ms
step:668/1765 train_time:24156ms step_avg:36.16ms
step:669/1765 train_time:24214ms step_avg:36.19ms
step:670/1765 train_time:24273ms step_avg:36.23ms
step:671/1765 train_time:24331ms step_avg:36.26ms
step:672/1765 train_time:24392ms step_avg:36.30ms
step:673/1765 train_time:24449ms step_avg:36.33ms
step:674/1765 train_time:24510ms step_avg:36.36ms
step:675/1765 train_time:24567ms step_avg:36.40ms
step:676/1765 train_time:24627ms step_avg:36.43ms
step:677/1765 train_time:24685ms step_avg:36.46ms
step:678/1765 train_time:24745ms step_avg:36.50ms
step:679/1765 train_time:24802ms step_avg:36.53ms
step:680/1765 train_time:24862ms step_avg:36.56ms
step:681/1765 train_time:24920ms step_avg:36.59ms
step:682/1765 train_time:24980ms step_avg:36.63ms
step:683/1765 train_time:25036ms step_avg:36.66ms
step:684/1765 train_time:25096ms step_avg:36.69ms
step:685/1765 train_time:25153ms step_avg:36.72ms
step:686/1765 train_time:25214ms step_avg:36.75ms
step:687/1765 train_time:25270ms step_avg:36.78ms
step:688/1765 train_time:25331ms step_avg:36.82ms
step:689/1765 train_time:25387ms step_avg:36.85ms
step:690/1765 train_time:25448ms step_avg:36.88ms
step:691/1765 train_time:25505ms step_avg:36.91ms
step:692/1765 train_time:25565ms step_avg:36.94ms
step:693/1765 train_time:25623ms step_avg:36.97ms
step:694/1765 train_time:25683ms step_avg:37.01ms
step:695/1765 train_time:25741ms step_avg:37.04ms
step:696/1765 train_time:25800ms step_avg:37.07ms
step:697/1765 train_time:25857ms step_avg:37.10ms
step:698/1765 train_time:25917ms step_avg:37.13ms
step:699/1765 train_time:25974ms step_avg:37.16ms
step:700/1765 train_time:26034ms step_avg:37.19ms
step:701/1765 train_time:26090ms step_avg:37.22ms
step:702/1765 train_time:26150ms step_avg:37.25ms
step:703/1765 train_time:26208ms step_avg:37.28ms
step:704/1765 train_time:26268ms step_avg:37.31ms
step:705/1765 train_time:26325ms step_avg:37.34ms
step:706/1765 train_time:26386ms step_avg:37.37ms
step:707/1765 train_time:26442ms step_avg:37.40ms
step:708/1765 train_time:26503ms step_avg:37.43ms
step:709/1765 train_time:26560ms step_avg:37.46ms
step:710/1765 train_time:26619ms step_avg:37.49ms
step:711/1765 train_time:26677ms step_avg:37.52ms
step:712/1765 train_time:26737ms step_avg:37.55ms
step:713/1765 train_time:26794ms step_avg:37.58ms
step:714/1765 train_time:26854ms step_avg:37.61ms
step:715/1765 train_time:26911ms step_avg:37.64ms
step:716/1765 train_time:26972ms step_avg:37.67ms
step:717/1765 train_time:27030ms step_avg:37.70ms
step:718/1765 train_time:27090ms step_avg:37.73ms
step:719/1765 train_time:27146ms step_avg:37.76ms
step:720/1765 train_time:27207ms step_avg:37.79ms
step:721/1765 train_time:27264ms step_avg:37.81ms
step:722/1765 train_time:27325ms step_avg:37.85ms
step:723/1765 train_time:27383ms step_avg:37.87ms
step:724/1765 train_time:27444ms step_avg:37.91ms
step:725/1765 train_time:27500ms step_avg:37.93ms
step:726/1765 train_time:27560ms step_avg:37.96ms
step:727/1765 train_time:27617ms step_avg:37.99ms
step:728/1765 train_time:27677ms step_avg:38.02ms
step:729/1765 train_time:27734ms step_avg:38.04ms
step:730/1765 train_time:27794ms step_avg:38.07ms
step:731/1765 train_time:27851ms step_avg:38.10ms
step:732/1765 train_time:27912ms step_avg:38.13ms
step:733/1765 train_time:27969ms step_avg:38.16ms
step:734/1765 train_time:28029ms step_avg:38.19ms
step:735/1765 train_time:28086ms step_avg:38.21ms
step:736/1765 train_time:28147ms step_avg:38.24ms
step:737/1765 train_time:28203ms step_avg:38.27ms
step:738/1765 train_time:28264ms step_avg:38.30ms
step:739/1765 train_time:28321ms step_avg:38.32ms
step:740/1765 train_time:28381ms step_avg:38.35ms
step:741/1765 train_time:28438ms step_avg:38.38ms
step:742/1765 train_time:28497ms step_avg:38.41ms
step:743/1765 train_time:28554ms step_avg:38.43ms
step:744/1765 train_time:28614ms step_avg:38.46ms
step:745/1765 train_time:28671ms step_avg:38.48ms
step:746/1765 train_time:28732ms step_avg:38.51ms
step:747/1765 train_time:28789ms step_avg:38.54ms
step:748/1765 train_time:28849ms step_avg:38.57ms
step:749/1765 train_time:28907ms step_avg:38.59ms
step:750/1765 train_time:28967ms step_avg:38.62ms
step:750/1765 val_loss:3.9856 train_time:29045ms step_avg:38.73ms
step:751/1765 train_time:29066ms step_avg:38.70ms
step:752/1765 train_time:29086ms step_avg:38.68ms
step:753/1765 train_time:29145ms step_avg:38.70ms
step:754/1765 train_time:29211ms step_avg:38.74ms
step:755/1765 train_time:29270ms step_avg:38.77ms
step:756/1765 train_time:29331ms step_avg:38.80ms
step:757/1765 train_time:29388ms step_avg:38.82ms
step:758/1765 train_time:29448ms step_avg:38.85ms
step:759/1765 train_time:29505ms step_avg:38.87ms
step:760/1765 train_time:29566ms step_avg:38.90ms
step:761/1765 train_time:29621ms step_avg:38.92ms
step:762/1765 train_time:29682ms step_avg:38.95ms
step:763/1765 train_time:29738ms step_avg:38.97ms
step:764/1765 train_time:29797ms step_avg:39.00ms
step:765/1765 train_time:29852ms step_avg:39.02ms
step:766/1765 train_time:29912ms step_avg:39.05ms
step:767/1765 train_time:29970ms step_avg:39.07ms
step:768/1765 train_time:30032ms step_avg:39.10ms
step:769/1765 train_time:30090ms step_avg:39.13ms
step:770/1765 train_time:30151ms step_avg:39.16ms
step:771/1765 train_time:30210ms step_avg:39.18ms
step:772/1765 train_time:30270ms step_avg:39.21ms
step:773/1765 train_time:30329ms step_avg:39.23ms
step:774/1765 train_time:30388ms step_avg:39.26ms
step:775/1765 train_time:30445ms step_avg:39.28ms
step:776/1765 train_time:30506ms step_avg:39.31ms
step:777/1765 train_time:30562ms step_avg:39.33ms
step:778/1765 train_time:30622ms step_avg:39.36ms
step:779/1765 train_time:30678ms step_avg:39.38ms
step:780/1765 train_time:30738ms step_avg:39.41ms
step:781/1765 train_time:30794ms step_avg:39.43ms
step:782/1765 train_time:30853ms step_avg:39.45ms
step:783/1765 train_time:30909ms step_avg:39.48ms
step:784/1765 train_time:30970ms step_avg:39.50ms
step:785/1765 train_time:31028ms step_avg:39.53ms
step:786/1765 train_time:31088ms step_avg:39.55ms
step:787/1765 train_time:31147ms step_avg:39.58ms
step:788/1765 train_time:31208ms step_avg:39.60ms
step:789/1765 train_time:31267ms step_avg:39.63ms
step:790/1765 train_time:31327ms step_avg:39.65ms
step:791/1765 train_time:31385ms step_avg:39.68ms
step:792/1765 train_time:31447ms step_avg:39.71ms
step:793/1765 train_time:31504ms step_avg:39.73ms
step:794/1765 train_time:31565ms step_avg:39.75ms
step:795/1765 train_time:31621ms step_avg:39.78ms
step:796/1765 train_time:31681ms step_avg:39.80ms
step:797/1765 train_time:31738ms step_avg:39.82ms
step:798/1765 train_time:31797ms step_avg:39.85ms
step:799/1765 train_time:31853ms step_avg:39.87ms
step:800/1765 train_time:31912ms step_avg:39.89ms
step:801/1765 train_time:31970ms step_avg:39.91ms
step:802/1765 train_time:32030ms step_avg:39.94ms
step:803/1765 train_time:32089ms step_avg:39.96ms
step:804/1765 train_time:32150ms step_avg:39.99ms
step:805/1765 train_time:32206ms step_avg:40.01ms
step:806/1765 train_time:32268ms step_avg:40.03ms
step:807/1765 train_time:32325ms step_avg:40.06ms
step:808/1765 train_time:32386ms step_avg:40.08ms
step:809/1765 train_time:32444ms step_avg:40.10ms
step:810/1765 train_time:32504ms step_avg:40.13ms
step:811/1765 train_time:32561ms step_avg:40.15ms
step:812/1765 train_time:32621ms step_avg:40.17ms
step:813/1765 train_time:32678ms step_avg:40.19ms
step:814/1765 train_time:32737ms step_avg:40.22ms
step:815/1765 train_time:32793ms step_avg:40.24ms
step:816/1765 train_time:32853ms step_avg:40.26ms
step:817/1765 train_time:32910ms step_avg:40.28ms
step:818/1765 train_time:32969ms step_avg:40.30ms
step:819/1765 train_time:33027ms step_avg:40.33ms
step:820/1765 train_time:33087ms step_avg:40.35ms
step:821/1765 train_time:33145ms step_avg:40.37ms
step:822/1765 train_time:33205ms step_avg:40.39ms
step:823/1765 train_time:33263ms step_avg:40.42ms
step:824/1765 train_time:33322ms step_avg:40.44ms
step:825/1765 train_time:33379ms step_avg:40.46ms
step:826/1765 train_time:33440ms step_avg:40.48ms
step:827/1765 train_time:33496ms step_avg:40.50ms
step:828/1765 train_time:33557ms step_avg:40.53ms
step:829/1765 train_time:33613ms step_avg:40.55ms
step:830/1765 train_time:33673ms step_avg:40.57ms
step:831/1765 train_time:33730ms step_avg:40.59ms
step:832/1765 train_time:33790ms step_avg:40.61ms
step:833/1765 train_time:33847ms step_avg:40.63ms
step:834/1765 train_time:33905ms step_avg:40.65ms
step:835/1765 train_time:33962ms step_avg:40.67ms
step:836/1765 train_time:34022ms step_avg:40.70ms
step:837/1765 train_time:34079ms step_avg:40.72ms
step:838/1765 train_time:34139ms step_avg:40.74ms
step:839/1765 train_time:34196ms step_avg:40.76ms
step:840/1765 train_time:34257ms step_avg:40.78ms
step:841/1765 train_time:34314ms step_avg:40.80ms
step:842/1765 train_time:34376ms step_avg:40.83ms
step:843/1765 train_time:34434ms step_avg:40.85ms
step:844/1765 train_time:34494ms step_avg:40.87ms
step:845/1765 train_time:34552ms step_avg:40.89ms
step:846/1765 train_time:34613ms step_avg:40.91ms
step:847/1765 train_time:34670ms step_avg:40.93ms
step:848/1765 train_time:34730ms step_avg:40.96ms
step:849/1765 train_time:34787ms step_avg:40.97ms
step:850/1765 train_time:34847ms step_avg:41.00ms
step:851/1765 train_time:34904ms step_avg:41.02ms
step:852/1765 train_time:34964ms step_avg:41.04ms
step:853/1765 train_time:35020ms step_avg:41.06ms
step:854/1765 train_time:35080ms step_avg:41.08ms
step:855/1765 train_time:35137ms step_avg:41.10ms
step:856/1765 train_time:35196ms step_avg:41.12ms
step:857/1765 train_time:35254ms step_avg:41.14ms
step:858/1765 train_time:35314ms step_avg:41.16ms
step:859/1765 train_time:35371ms step_avg:41.18ms
step:860/1765 train_time:35432ms step_avg:41.20ms
step:861/1765 train_time:35489ms step_avg:41.22ms
step:862/1765 train_time:35549ms step_avg:41.24ms
step:863/1765 train_time:35606ms step_avg:41.26ms
step:864/1765 train_time:35668ms step_avg:41.28ms
step:865/1765 train_time:35726ms step_avg:41.30ms
step:866/1765 train_time:35787ms step_avg:41.32ms
step:867/1765 train_time:35845ms step_avg:41.34ms
step:868/1765 train_time:35904ms step_avg:41.36ms
step:869/1765 train_time:35961ms step_avg:41.38ms
step:870/1765 train_time:36021ms step_avg:41.40ms
step:871/1765 train_time:36078ms step_avg:41.42ms
step:872/1765 train_time:36138ms step_avg:41.44ms
step:873/1765 train_time:36194ms step_avg:41.46ms
step:874/1765 train_time:36255ms step_avg:41.48ms
step:875/1765 train_time:36311ms step_avg:41.50ms
step:876/1765 train_time:36373ms step_avg:41.52ms
step:877/1765 train_time:36430ms step_avg:41.54ms
step:878/1765 train_time:36490ms step_avg:41.56ms
step:879/1765 train_time:36549ms step_avg:41.58ms
step:880/1765 train_time:36609ms step_avg:41.60ms
step:881/1765 train_time:36666ms step_avg:41.62ms
step:882/1765 train_time:36726ms step_avg:41.64ms
step:883/1765 train_time:36784ms step_avg:41.66ms
step:884/1765 train_time:36845ms step_avg:41.68ms
step:885/1765 train_time:36902ms step_avg:41.70ms
step:886/1765 train_time:36962ms step_avg:41.72ms
step:887/1765 train_time:37018ms step_avg:41.73ms
step:888/1765 train_time:37078ms step_avg:41.75ms
step:889/1765 train_time:37135ms step_avg:41.77ms
step:890/1765 train_time:37195ms step_avg:41.79ms
step:891/1765 train_time:37252ms step_avg:41.81ms
step:892/1765 train_time:37311ms step_avg:41.83ms
step:893/1765 train_time:37369ms step_avg:41.85ms
step:894/1765 train_time:37429ms step_avg:41.87ms
step:895/1765 train_time:37486ms step_avg:41.88ms
step:896/1765 train_time:37546ms step_avg:41.90ms
step:897/1765 train_time:37603ms step_avg:41.92ms
step:898/1765 train_time:37664ms step_avg:41.94ms
step:899/1765 train_time:37720ms step_avg:41.96ms
step:900/1765 train_time:37780ms step_avg:41.98ms
step:901/1765 train_time:37838ms step_avg:42.00ms
step:902/1765 train_time:37898ms step_avg:42.02ms
step:903/1765 train_time:37955ms step_avg:42.03ms
step:904/1765 train_time:38015ms step_avg:42.05ms
step:905/1765 train_time:38071ms step_avg:42.07ms
step:906/1765 train_time:38132ms step_avg:42.09ms
step:907/1765 train_time:38189ms step_avg:42.10ms
step:908/1765 train_time:38249ms step_avg:42.12ms
step:909/1765 train_time:38307ms step_avg:42.14ms
step:910/1765 train_time:38367ms step_avg:42.16ms
step:911/1765 train_time:38424ms step_avg:42.18ms
step:912/1765 train_time:38484ms step_avg:42.20ms
step:913/1765 train_time:38541ms step_avg:42.21ms
step:914/1765 train_time:38601ms step_avg:42.23ms
step:915/1765 train_time:38658ms step_avg:42.25ms
step:916/1765 train_time:38718ms step_avg:42.27ms
step:917/1765 train_time:38776ms step_avg:42.29ms
step:918/1765 train_time:38837ms step_avg:42.31ms
step:919/1765 train_time:38894ms step_avg:42.32ms
step:920/1765 train_time:38954ms step_avg:42.34ms
step:921/1765 train_time:39010ms step_avg:42.36ms
step:922/1765 train_time:39070ms step_avg:42.38ms
step:923/1765 train_time:39128ms step_avg:42.39ms
step:924/1765 train_time:39187ms step_avg:42.41ms
step:925/1765 train_time:39245ms step_avg:42.43ms
step:926/1765 train_time:39306ms step_avg:42.45ms
step:927/1765 train_time:39362ms step_avg:42.46ms
step:928/1765 train_time:39422ms step_avg:42.48ms
step:929/1765 train_time:39479ms step_avg:42.50ms
step:930/1765 train_time:39539ms step_avg:42.52ms
step:931/1765 train_time:39596ms step_avg:42.53ms
step:932/1765 train_time:39658ms step_avg:42.55ms
step:933/1765 train_time:39714ms step_avg:42.57ms
step:934/1765 train_time:39774ms step_avg:42.58ms
step:935/1765 train_time:39831ms step_avg:42.60ms
step:936/1765 train_time:39893ms step_avg:42.62ms
step:937/1765 train_time:39950ms step_avg:42.64ms
step:938/1765 train_time:40008ms step_avg:42.65ms
step:939/1765 train_time:40065ms step_avg:42.67ms
step:940/1765 train_time:40125ms step_avg:42.69ms
step:941/1765 train_time:40183ms step_avg:42.70ms
step:942/1765 train_time:40243ms step_avg:42.72ms
step:943/1765 train_time:40300ms step_avg:42.74ms
step:944/1765 train_time:40359ms step_avg:42.75ms
step:945/1765 train_time:40416ms step_avg:42.77ms
step:946/1765 train_time:40476ms step_avg:42.79ms
step:947/1765 train_time:40534ms step_avg:42.80ms
step:948/1765 train_time:40594ms step_avg:42.82ms
step:949/1765 train_time:40651ms step_avg:42.84ms
step:950/1765 train_time:40710ms step_avg:42.85ms
step:951/1765 train_time:40768ms step_avg:42.87ms
step:952/1765 train_time:40828ms step_avg:42.89ms
step:953/1765 train_time:40885ms step_avg:42.90ms
step:954/1765 train_time:40946ms step_avg:42.92ms
step:955/1765 train_time:41002ms step_avg:42.93ms
step:956/1765 train_time:41062ms step_avg:42.95ms
step:957/1765 train_time:41118ms step_avg:42.97ms
step:958/1765 train_time:41178ms step_avg:42.98ms
step:959/1765 train_time:41236ms step_avg:43.00ms
step:960/1765 train_time:41296ms step_avg:43.02ms
step:961/1765 train_time:41353ms step_avg:43.03ms
step:962/1765 train_time:41413ms step_avg:43.05ms
step:963/1765 train_time:41471ms step_avg:43.06ms
step:964/1765 train_time:41532ms step_avg:43.08ms
step:965/1765 train_time:41588ms step_avg:43.10ms
step:966/1765 train_time:41647ms step_avg:43.11ms
step:967/1765 train_time:41704ms step_avg:43.13ms
step:968/1765 train_time:41764ms step_avg:43.14ms
step:969/1765 train_time:41820ms step_avg:43.16ms
step:970/1765 train_time:41880ms step_avg:43.18ms
step:971/1765 train_time:41936ms step_avg:43.19ms
step:972/1765 train_time:41996ms step_avg:43.21ms
step:973/1765 train_time:42053ms step_avg:43.22ms
step:974/1765 train_time:42114ms step_avg:43.24ms
step:975/1765 train_time:42171ms step_avg:43.25ms
step:976/1765 train_time:42232ms step_avg:43.27ms
step:977/1765 train_time:42289ms step_avg:43.28ms
step:978/1765 train_time:42349ms step_avg:43.30ms
step:979/1765 train_time:42406ms step_avg:43.32ms
step:980/1765 train_time:42466ms step_avg:43.33ms
step:981/1765 train_time:42523ms step_avg:43.35ms
step:982/1765 train_time:42583ms step_avg:43.36ms
step:983/1765 train_time:42641ms step_avg:43.38ms
step:984/1765 train_time:42700ms step_avg:43.39ms
step:985/1765 train_time:42757ms step_avg:43.41ms
step:986/1765 train_time:42817ms step_avg:43.42ms
step:987/1765 train_time:42874ms step_avg:43.44ms
step:988/1765 train_time:42934ms step_avg:43.46ms
step:989/1765 train_time:42991ms step_avg:43.47ms
step:990/1765 train_time:43051ms step_avg:43.49ms
step:991/1765 train_time:43108ms step_avg:43.50ms
step:992/1765 train_time:43168ms step_avg:43.52ms
step:993/1765 train_time:43224ms step_avg:43.53ms
step:994/1765 train_time:43285ms step_avg:43.55ms
step:995/1765 train_time:43341ms step_avg:43.56ms
step:996/1765 train_time:43401ms step_avg:43.58ms
step:997/1765 train_time:43458ms step_avg:43.59ms
step:998/1765 train_time:43518ms step_avg:43.60ms
step:999/1765 train_time:43575ms step_avg:43.62ms
step:1000/1765 train_time:43636ms step_avg:43.64ms
step:1000/1765 val_loss:3.7199 train_time:43713ms step_avg:43.71ms
step:1001/1765 train_time:43733ms step_avg:43.69ms
step:1002/1765 train_time:43754ms step_avg:43.67ms
step:1003/1765 train_time:43814ms step_avg:43.68ms
step:1004/1765 train_time:43879ms step_avg:43.70ms
step:1005/1765 train_time:43937ms step_avg:43.72ms
step:1006/1765 train_time:43998ms step_avg:43.74ms
step:1007/1765 train_time:44055ms step_avg:43.75ms
step:1008/1765 train_time:44115ms step_avg:43.76ms
step:1009/1765 train_time:44172ms step_avg:43.78ms
step:1010/1765 train_time:44231ms step_avg:43.79ms
step:1011/1765 train_time:44287ms step_avg:43.80ms
step:1012/1765 train_time:44346ms step_avg:43.82ms
step:1013/1765 train_time:44402ms step_avg:43.83ms
step:1014/1765 train_time:44461ms step_avg:43.85ms
step:1015/1765 train_time:44517ms step_avg:43.86ms
step:1016/1765 train_time:44577ms step_avg:43.88ms
step:1017/1765 train_time:44636ms step_avg:43.89ms
step:1018/1765 train_time:44699ms step_avg:43.91ms
step:1019/1765 train_time:44758ms step_avg:43.92ms
step:1020/1765 train_time:44819ms step_avg:43.94ms
step:1021/1765 train_time:44878ms step_avg:43.95ms
step:1022/1765 train_time:44937ms step_avg:43.97ms
step:1023/1765 train_time:44996ms step_avg:43.98ms
step:1024/1765 train_time:45057ms step_avg:44.00ms
step:1025/1765 train_time:45114ms step_avg:44.01ms
step:1026/1765 train_time:45174ms step_avg:44.03ms
step:1027/1765 train_time:45231ms step_avg:44.04ms
step:1028/1765 train_time:45291ms step_avg:44.06ms
step:1029/1765 train_time:45346ms step_avg:44.07ms
step:1030/1765 train_time:45407ms step_avg:44.08ms
step:1031/1765 train_time:45464ms step_avg:44.10ms
step:1032/1765 train_time:45524ms step_avg:44.11ms
step:1033/1765 train_time:45580ms step_avg:44.12ms
step:1034/1765 train_time:45640ms step_avg:44.14ms
step:1035/1765 train_time:45697ms step_avg:44.15ms
step:1036/1765 train_time:45759ms step_avg:44.17ms
step:1037/1765 train_time:45816ms step_avg:44.18ms
step:1038/1765 train_time:45877ms step_avg:44.20ms
step:1039/1765 train_time:45933ms step_avg:44.21ms
step:1040/1765 train_time:45994ms step_avg:44.23ms
step:1041/1765 train_time:46052ms step_avg:44.24ms
step:1042/1765 train_time:46113ms step_avg:44.25ms
step:1043/1765 train_time:46170ms step_avg:44.27ms
step:1044/1765 train_time:46230ms step_avg:44.28ms
step:1045/1765 train_time:46286ms step_avg:44.29ms
step:1046/1765 train_time:46346ms step_avg:44.31ms
step:1047/1765 train_time:46402ms step_avg:44.32ms
step:1048/1765 train_time:46461ms step_avg:44.33ms
step:1049/1765 train_time:46519ms step_avg:44.35ms
step:1050/1765 train_time:46578ms step_avg:44.36ms
step:1051/1765 train_time:46634ms step_avg:44.37ms
step:1052/1765 train_time:46695ms step_avg:44.39ms
step:1053/1765 train_time:46753ms step_avg:44.40ms
step:1054/1765 train_time:46813ms step_avg:44.41ms
step:1055/1765 train_time:46870ms step_avg:44.43ms
step:1056/1765 train_time:46931ms step_avg:44.44ms
step:1057/1765 train_time:46988ms step_avg:44.45ms
step:1058/1765 train_time:47049ms step_avg:44.47ms
step:1059/1765 train_time:47106ms step_avg:44.48ms
step:1060/1765 train_time:47166ms step_avg:44.50ms
step:1061/1765 train_time:47222ms step_avg:44.51ms
step:1062/1765 train_time:47283ms step_avg:44.52ms
step:1063/1765 train_time:47339ms step_avg:44.53ms
step:1064/1765 train_time:47399ms step_avg:44.55ms
step:1065/1765 train_time:47456ms step_avg:44.56ms
step:1066/1765 train_time:47516ms step_avg:44.57ms
step:1067/1765 train_time:47572ms step_avg:44.59ms
step:1068/1765 train_time:47632ms step_avg:44.60ms
step:1069/1765 train_time:47689ms step_avg:44.61ms
step:1070/1765 train_time:47750ms step_avg:44.63ms
step:1071/1765 train_time:47807ms step_avg:44.64ms
step:1072/1765 train_time:47868ms step_avg:44.65ms
step:1073/1765 train_time:47925ms step_avg:44.66ms
step:1074/1765 train_time:47986ms step_avg:44.68ms
step:1075/1765 train_time:48042ms step_avg:44.69ms
step:1076/1765 train_time:48101ms step_avg:44.70ms
step:1077/1765 train_time:48158ms step_avg:44.72ms
step:1078/1765 train_time:48217ms step_avg:44.73ms
step:1079/1765 train_time:48274ms step_avg:44.74ms
step:1080/1765 train_time:48334ms step_avg:44.75ms
step:1081/1765 train_time:48392ms step_avg:44.77ms
step:1082/1765 train_time:48452ms step_avg:44.78ms
step:1083/1765 train_time:48509ms step_avg:44.79ms
step:1084/1765 train_time:48569ms step_avg:44.81ms
step:1085/1765 train_time:48627ms step_avg:44.82ms
step:1086/1765 train_time:48687ms step_avg:44.83ms
step:1087/1765 train_time:48744ms step_avg:44.84ms
step:1088/1765 train_time:48805ms step_avg:44.86ms
step:1089/1765 train_time:48862ms step_avg:44.87ms
step:1090/1765 train_time:48922ms step_avg:44.88ms
step:1091/1765 train_time:48979ms step_avg:44.89ms
step:1092/1765 train_time:49038ms step_avg:44.91ms
step:1093/1765 train_time:49095ms step_avg:44.92ms
step:1094/1765 train_time:49156ms step_avg:44.93ms
step:1095/1765 train_time:49212ms step_avg:44.94ms
step:1096/1765 train_time:49273ms step_avg:44.96ms
step:1097/1765 train_time:49331ms step_avg:44.97ms
step:1098/1765 train_time:49391ms step_avg:44.98ms
step:1099/1765 train_time:49448ms step_avg:44.99ms
step:1100/1765 train_time:49508ms step_avg:45.01ms
step:1101/1765 train_time:49565ms step_avg:45.02ms
step:1102/1765 train_time:49625ms step_avg:45.03ms
step:1103/1765 train_time:49683ms step_avg:45.04ms
step:1104/1765 train_time:49742ms step_avg:45.06ms
step:1105/1765 train_time:49799ms step_avg:45.07ms
step:1106/1765 train_time:49859ms step_avg:45.08ms
step:1107/1765 train_time:49916ms step_avg:45.09ms
step:1108/1765 train_time:49976ms step_avg:45.10ms
step:1109/1765 train_time:50034ms step_avg:45.12ms
step:1110/1765 train_time:50094ms step_avg:45.13ms
step:1111/1765 train_time:50152ms step_avg:45.14ms
step:1112/1765 train_time:50211ms step_avg:45.15ms
step:1113/1765 train_time:50268ms step_avg:45.16ms
step:1114/1765 train_time:50329ms step_avg:45.18ms
step:1115/1765 train_time:50385ms step_avg:45.19ms
step:1116/1765 train_time:50445ms step_avg:45.20ms
step:1117/1765 train_time:50502ms step_avg:45.21ms
step:1118/1765 train_time:50562ms step_avg:45.23ms
step:1119/1765 train_time:50619ms step_avg:45.24ms
step:1120/1765 train_time:50679ms step_avg:45.25ms
step:1121/1765 train_time:50736ms step_avg:45.26ms
step:1122/1765 train_time:50797ms step_avg:45.27ms
step:1123/1765 train_time:50853ms step_avg:45.28ms
step:1124/1765 train_time:50914ms step_avg:45.30ms
step:1125/1765 train_time:50971ms step_avg:45.31ms
step:1126/1765 train_time:51031ms step_avg:45.32ms
step:1127/1765 train_time:51088ms step_avg:45.33ms
step:1128/1765 train_time:51149ms step_avg:45.34ms
step:1129/1765 train_time:51207ms step_avg:45.36ms
step:1130/1765 train_time:51268ms step_avg:45.37ms
step:1131/1765 train_time:51325ms step_avg:45.38ms
step:1132/1765 train_time:51385ms step_avg:45.39ms
step:1133/1765 train_time:51442ms step_avg:45.40ms
step:1134/1765 train_time:51502ms step_avg:45.42ms
step:1135/1765 train_time:51559ms step_avg:45.43ms
step:1136/1765 train_time:51619ms step_avg:45.44ms
step:1137/1765 train_time:51676ms step_avg:45.45ms
step:1138/1765 train_time:51736ms step_avg:45.46ms
step:1139/1765 train_time:51794ms step_avg:45.47ms
step:1140/1765 train_time:51854ms step_avg:45.49ms
step:1141/1765 train_time:51911ms step_avg:45.50ms
step:1142/1765 train_time:51971ms step_avg:45.51ms
step:1143/1765 train_time:52028ms step_avg:45.52ms
step:1144/1765 train_time:52088ms step_avg:45.53ms
step:1145/1765 train_time:52146ms step_avg:45.54ms
step:1146/1765 train_time:52207ms step_avg:45.56ms
step:1147/1765 train_time:52263ms step_avg:45.56ms
step:1148/1765 train_time:52322ms step_avg:45.58ms
step:1149/1765 train_time:52380ms step_avg:45.59ms
step:1150/1765 train_time:52439ms step_avg:45.60ms
step:1151/1765 train_time:52499ms step_avg:45.61ms
step:1152/1765 train_time:52582ms step_avg:45.64ms
step:1153/1765 train_time:52663ms step_avg:45.68ms
step:1154/1765 train_time:52750ms step_avg:45.71ms
step:1155/1765 train_time:52834ms step_avg:45.74ms
step:1156/1765 train_time:52920ms step_avg:45.78ms
step:1157/1765 train_time:53003ms step_avg:45.81ms
step:1158/1765 train_time:53089ms step_avg:45.85ms
step:1159/1765 train_time:53173ms step_avg:45.88ms
step:1160/1765 train_time:53259ms step_avg:45.91ms
step:1161/1765 train_time:53342ms step_avg:45.95ms
step:1162/1765 train_time:53428ms step_avg:45.98ms
step:1163/1765 train_time:53511ms step_avg:46.01ms
step:1164/1765 train_time:53597ms step_avg:46.05ms
step:1165/1765 train_time:53680ms step_avg:46.08ms
step:1166/1765 train_time:53766ms step_avg:46.11ms
step:1167/1765 train_time:53849ms step_avg:46.14ms
step:1168/1765 train_time:53935ms step_avg:46.18ms
step:1169/1765 train_time:54019ms step_avg:46.21ms
step:1170/1765 train_time:54105ms step_avg:46.24ms
step:1171/1765 train_time:54188ms step_avg:46.27ms
step:1172/1765 train_time:54274ms step_avg:46.31ms
step:1173/1765 train_time:54357ms step_avg:46.34ms
step:1174/1765 train_time:54444ms step_avg:46.37ms
step:1175/1765 train_time:54527ms step_avg:46.41ms
step:1176/1765 train_time:54612ms step_avg:46.44ms
step:1177/1765 train_time:54695ms step_avg:46.47ms
step:1178/1765 train_time:54782ms step_avg:46.50ms
step:1179/1765 train_time:54864ms step_avg:46.53ms
step:1180/1765 train_time:54950ms step_avg:46.57ms
step:1181/1765 train_time:55033ms step_avg:46.60ms
step:1182/1765 train_time:55119ms step_avg:46.63ms
step:1183/1765 train_time:55202ms step_avg:46.66ms
step:1184/1765 train_time:55287ms step_avg:46.69ms
step:1185/1765 train_time:55371ms step_avg:46.73ms
step:1186/1765 train_time:55456ms step_avg:46.76ms
step:1187/1765 train_time:55539ms step_avg:46.79ms
step:1188/1765 train_time:55625ms step_avg:46.82ms
step:1189/1765 train_time:55708ms step_avg:46.85ms
step:1190/1765 train_time:55793ms step_avg:46.89ms
step:1191/1765 train_time:55876ms step_avg:46.92ms
step:1192/1765 train_time:55962ms step_avg:46.95ms
step:1193/1765 train_time:56045ms step_avg:46.98ms
step:1194/1765 train_time:56131ms step_avg:47.01ms
step:1195/1765 train_time:56215ms step_avg:47.04ms
step:1196/1765 train_time:56302ms step_avg:47.07ms
step:1197/1765 train_time:56384ms step_avg:47.10ms
step:1198/1765 train_time:56468ms step_avg:47.14ms
step:1199/1765 train_time:56551ms step_avg:47.17ms
step:1200/1765 train_time:56639ms step_avg:47.20ms
step:1201/1765 train_time:56723ms step_avg:47.23ms
step:1202/1765 train_time:56808ms step_avg:47.26ms
step:1203/1765 train_time:56892ms step_avg:47.29ms
step:1204/1765 train_time:56979ms step_avg:47.33ms
step:1205/1765 train_time:57063ms step_avg:47.36ms
step:1206/1765 train_time:57149ms step_avg:47.39ms
step:1207/1765 train_time:57232ms step_avg:47.42ms
step:1208/1765 train_time:57319ms step_avg:47.45ms
step:1209/1765 train_time:57402ms step_avg:47.48ms
step:1210/1765 train_time:57486ms step_avg:47.51ms
step:1211/1765 train_time:57570ms step_avg:47.54ms
step:1212/1765 train_time:57655ms step_avg:47.57ms
step:1213/1765 train_time:57740ms step_avg:47.60ms
step:1214/1765 train_time:57826ms step_avg:47.63ms
step:1215/1765 train_time:57909ms step_avg:47.66ms
step:1216/1765 train_time:57995ms step_avg:47.69ms
step:1217/1765 train_time:58078ms step_avg:47.72ms
step:1218/1765 train_time:58164ms step_avg:47.75ms
step:1219/1765 train_time:58246ms step_avg:47.78ms
step:1220/1765 train_time:58333ms step_avg:47.81ms
step:1221/1765 train_time:58416ms step_avg:47.84ms
step:1222/1765 train_time:58503ms step_avg:47.88ms
step:1223/1765 train_time:58586ms step_avg:47.90ms
step:1224/1765 train_time:58672ms step_avg:47.93ms
step:1225/1765 train_time:58755ms step_avg:47.96ms
step:1226/1765 train_time:58841ms step_avg:47.99ms
step:1227/1765 train_time:58923ms step_avg:48.02ms
step:1228/1765 train_time:59010ms step_avg:48.05ms
step:1229/1765 train_time:59094ms step_avg:48.08ms
step:1230/1765 train_time:59181ms step_avg:48.11ms
step:1231/1765 train_time:59263ms step_avg:48.14ms
step:1232/1765 train_time:59349ms step_avg:48.17ms
step:1233/1765 train_time:59432ms step_avg:48.20ms
step:1234/1765 train_time:59517ms step_avg:48.23ms
step:1235/1765 train_time:59601ms step_avg:48.26ms
step:1236/1765 train_time:59687ms step_avg:48.29ms
step:1237/1765 train_time:59770ms step_avg:48.32ms
step:1238/1765 train_time:59856ms step_avg:48.35ms
step:1239/1765 train_time:59941ms step_avg:48.38ms
step:1240/1765 train_time:60027ms step_avg:48.41ms
step:1241/1765 train_time:60111ms step_avg:48.44ms
step:1242/1765 train_time:60197ms step_avg:48.47ms
step:1243/1765 train_time:60280ms step_avg:48.50ms
step:1244/1765 train_time:60366ms step_avg:48.53ms
step:1245/1765 train_time:60449ms step_avg:48.55ms
step:1246/1765 train_time:60536ms step_avg:48.58ms
step:1247/1765 train_time:60619ms step_avg:48.61ms
step:1248/1765 train_time:60705ms step_avg:48.64ms
step:1249/1765 train_time:60787ms step_avg:48.67ms
step:1250/1765 train_time:60873ms step_avg:48.70ms
step:1250/1765 val_loss:3.4960 train_time:60982ms step_avg:48.79ms
step:1251/1765 train_time:61005ms step_avg:48.76ms
step:1252/1765 train_time:61044ms step_avg:48.76ms
step:1253/1765 train_time:61129ms step_avg:48.79ms
step:1254/1765 train_time:61219ms step_avg:48.82ms
step:1255/1765 train_time:61301ms step_avg:48.85ms
step:1256/1765 train_time:61387ms step_avg:48.87ms
step:1257/1765 train_time:61469ms step_avg:48.90ms
step:1258/1765 train_time:61553ms step_avg:48.93ms
step:1259/1765 train_time:61636ms step_avg:48.96ms
step:1260/1765 train_time:61721ms step_avg:48.98ms
step:1261/1765 train_time:61803ms step_avg:49.01ms
step:1262/1765 train_time:61887ms step_avg:49.04ms
step:1263/1765 train_time:61973ms step_avg:49.07ms
step:1264/1765 train_time:62063ms step_avg:49.10ms
step:1265/1765 train_time:62148ms step_avg:49.13ms
step:1266/1765 train_time:62234ms step_avg:49.16ms
step:1267/1765 train_time:62318ms step_avg:49.19ms
step:1268/1765 train_time:62403ms step_avg:49.21ms
step:1269/1765 train_time:62485ms step_avg:49.24ms
step:1270/1765 train_time:62569ms step_avg:49.27ms
step:1271/1765 train_time:62652ms step_avg:49.29ms
step:1272/1765 train_time:62735ms step_avg:49.32ms
step:1273/1765 train_time:62819ms step_avg:49.35ms
step:1274/1765 train_time:62904ms step_avg:49.38ms
step:1275/1765 train_time:62989ms step_avg:49.40ms
step:1276/1765 train_time:63075ms step_avg:49.43ms
step:1277/1765 train_time:63160ms step_avg:49.46ms
step:1278/1765 train_time:63246ms step_avg:49.49ms
step:1279/1765 train_time:63329ms step_avg:49.51ms
step:1280/1765 train_time:63415ms step_avg:49.54ms
step:1281/1765 train_time:63497ms step_avg:49.57ms
step:1282/1765 train_time:63584ms step_avg:49.60ms
step:1283/1765 train_time:63666ms step_avg:49.62ms
step:1284/1765 train_time:63750ms step_avg:49.65ms
step:1285/1765 train_time:63834ms step_avg:49.68ms
step:1286/1765 train_time:63920ms step_avg:49.70ms
step:1287/1765 train_time:64003ms step_avg:49.73ms
step:1288/1765 train_time:64089ms step_avg:49.76ms
step:1289/1765 train_time:64173ms step_avg:49.78ms
step:1290/1765 train_time:64260ms step_avg:49.81ms
step:1291/1765 train_time:64343ms step_avg:49.84ms
step:1292/1765 train_time:64427ms step_avg:49.87ms
step:1293/1765 train_time:64512ms step_avg:49.89ms
step:1294/1765 train_time:64598ms step_avg:49.92ms
step:1295/1765 train_time:64679ms step_avg:49.95ms
step:1296/1765 train_time:64765ms step_avg:49.97ms
step:1297/1765 train_time:64847ms step_avg:50.00ms
step:1298/1765 train_time:64933ms step_avg:50.03ms
step:1299/1765 train_time:65016ms step_avg:50.05ms
step:1300/1765 train_time:65103ms step_avg:50.08ms
step:1301/1765 train_time:65186ms step_avg:50.10ms
step:1302/1765 train_time:65272ms step_avg:50.13ms
step:1303/1765 train_time:65354ms step_avg:50.16ms
step:1304/1765 train_time:65440ms step_avg:50.18ms
step:1305/1765 train_time:65523ms step_avg:50.21ms
step:1306/1765 train_time:65609ms step_avg:50.24ms
step:1307/1765 train_time:65690ms step_avg:50.26ms
step:1308/1765 train_time:65775ms step_avg:50.29ms
step:1309/1765 train_time:65859ms step_avg:50.31ms
step:1310/1765 train_time:65945ms step_avg:50.34ms
step:1311/1765 train_time:66029ms step_avg:50.37ms
step:1312/1765 train_time:66114ms step_avg:50.39ms
step:1313/1765 train_time:66198ms step_avg:50.42ms
step:1314/1765 train_time:66284ms step_avg:50.44ms
step:1315/1765 train_time:66366ms step_avg:50.47ms
step:1316/1765 train_time:66451ms step_avg:50.49ms
step:1317/1765 train_time:66534ms step_avg:50.52ms
step:1318/1765 train_time:66622ms step_avg:50.55ms
step:1319/1765 train_time:66704ms step_avg:50.57ms
step:1320/1765 train_time:66790ms step_avg:50.60ms
step:1321/1765 train_time:66872ms step_avg:50.62ms
step:1322/1765 train_time:66958ms step_avg:50.65ms
step:1323/1765 train_time:67041ms step_avg:50.67ms
step:1324/1765 train_time:67127ms step_avg:50.70ms
step:1325/1765 train_time:67210ms step_avg:50.72ms
step:1326/1765 train_time:67296ms step_avg:50.75ms
step:1327/1765 train_time:67379ms step_avg:50.78ms
step:1328/1765 train_time:67465ms step_avg:50.80ms
step:1329/1765 train_time:67547ms step_avg:50.83ms
step:1330/1765 train_time:67632ms step_avg:50.85ms
step:1331/1765 train_time:67716ms step_avg:50.88ms
step:1332/1765 train_time:67801ms step_avg:50.90ms
step:1333/1765 train_time:67884ms step_avg:50.93ms
step:1334/1765 train_time:67969ms step_avg:50.95ms
step:1335/1765 train_time:68053ms step_avg:50.98ms
step:1336/1765 train_time:68138ms step_avg:51.00ms
step:1337/1765 train_time:68221ms step_avg:51.03ms
step:1338/1765 train_time:68307ms step_avg:51.05ms
step:1339/1765 train_time:68390ms step_avg:51.08ms
step:1340/1765 train_time:68474ms step_avg:51.10ms
step:1341/1765 train_time:68558ms step_avg:51.12ms
step:1342/1765 train_time:68644ms step_avg:51.15ms
step:1343/1765 train_time:68727ms step_avg:51.17ms
step:1344/1765 train_time:68813ms step_avg:51.20ms
step:1345/1765 train_time:68895ms step_avg:51.22ms
step:1346/1765 train_time:68981ms step_avg:51.25ms
step:1347/1765 train_time:69064ms step_avg:51.27ms
step:1348/1765 train_time:69149ms step_avg:51.30ms
step:1349/1765 train_time:69232ms step_avg:51.32ms
step:1350/1765 train_time:69319ms step_avg:51.35ms
step:1351/1765 train_time:69401ms step_avg:51.37ms
step:1352/1765 train_time:69486ms step_avg:51.39ms
step:1353/1765 train_time:69569ms step_avg:51.42ms
step:1354/1765 train_time:69654ms step_avg:51.44ms
step:1355/1765 train_time:69738ms step_avg:51.47ms
step:1356/1765 train_time:69823ms step_avg:51.49ms
step:1357/1765 train_time:69907ms step_avg:51.52ms
step:1358/1765 train_time:69992ms step_avg:51.54ms
step:1359/1765 train_time:70075ms step_avg:51.56ms
step:1360/1765 train_time:70161ms step_avg:51.59ms
step:1361/1765 train_time:70244ms step_avg:51.61ms
step:1362/1765 train_time:70330ms step_avg:51.64ms
step:1363/1765 train_time:70413ms step_avg:51.66ms
step:1364/1765 train_time:70499ms step_avg:51.69ms
step:1365/1765 train_time:70582ms step_avg:51.71ms
step:1366/1765 train_time:70667ms step_avg:51.73ms
step:1367/1765 train_time:70750ms step_avg:51.76ms
step:1368/1765 train_time:70835ms step_avg:51.78ms
step:1369/1765 train_time:70918ms step_avg:51.80ms
step:1370/1765 train_time:71004ms step_avg:51.83ms
step:1371/1765 train_time:71087ms step_avg:51.85ms
step:1372/1765 train_time:71171ms step_avg:51.87ms
step:1373/1765 train_time:71255ms step_avg:51.90ms
step:1374/1765 train_time:71342ms step_avg:51.92ms
step:1375/1765 train_time:71424ms step_avg:51.94ms
step:1376/1765 train_time:71510ms step_avg:51.97ms
step:1377/1765 train_time:71592ms step_avg:51.99ms
step:1378/1765 train_time:71676ms step_avg:52.01ms
step:1379/1765 train_time:71760ms step_avg:52.04ms
step:1380/1765 train_time:71845ms step_avg:52.06ms
step:1381/1765 train_time:71929ms step_avg:52.08ms
step:1382/1765 train_time:72015ms step_avg:52.11ms
step:1383/1765 train_time:72098ms step_avg:52.13ms
step:1384/1765 train_time:72185ms step_avg:52.16ms
step:1385/1765 train_time:72269ms step_avg:52.18ms
step:1386/1765 train_time:72354ms step_avg:52.20ms
step:1387/1765 train_time:72436ms step_avg:52.23ms
step:1388/1765 train_time:72523ms step_avg:52.25ms
step:1389/1765 train_time:72605ms step_avg:52.27ms
step:1390/1765 train_time:72692ms step_avg:52.30ms
step:1391/1765 train_time:72775ms step_avg:52.32ms
step:1392/1765 train_time:72859ms step_avg:52.34ms
step:1393/1765 train_time:72942ms step_avg:52.36ms
step:1394/1765 train_time:73027ms step_avg:52.39ms
step:1395/1765 train_time:73111ms step_avg:52.41ms
step:1396/1765 train_time:73197ms step_avg:52.43ms
step:1397/1765 train_time:73280ms step_avg:52.46ms
step:1398/1765 train_time:73365ms step_avg:52.48ms
step:1399/1765 train_time:73448ms step_avg:52.50ms
step:1400/1765 train_time:73533ms step_avg:52.52ms
step:1401/1765 train_time:73617ms step_avg:52.55ms
step:1402/1765 train_time:73703ms step_avg:52.57ms
step:1403/1765 train_time:73785ms step_avg:52.59ms
step:1404/1765 train_time:73870ms step_avg:52.61ms
step:1405/1765 train_time:73954ms step_avg:52.64ms
step:1406/1765 train_time:74040ms step_avg:52.66ms
step:1407/1765 train_time:74123ms step_avg:52.68ms
step:1408/1765 train_time:74209ms step_avg:52.71ms
step:1409/1765 train_time:74291ms step_avg:52.73ms
step:1410/1765 train_time:74377ms step_avg:52.75ms
step:1411/1765 train_time:74460ms step_avg:52.77ms
step:1412/1765 train_time:74545ms step_avg:52.79ms
step:1413/1765 train_time:74628ms step_avg:52.82ms
step:1414/1765 train_time:74714ms step_avg:52.84ms
step:1415/1765 train_time:74797ms step_avg:52.86ms
step:1416/1765 train_time:74883ms step_avg:52.88ms
step:1417/1765 train_time:74966ms step_avg:52.90ms
step:1418/1765 train_time:75052ms step_avg:52.93ms
step:1419/1765 train_time:75135ms step_avg:52.95ms
step:1420/1765 train_time:75221ms step_avg:52.97ms
step:1421/1765 train_time:75303ms step_avg:52.99ms
step:1422/1765 train_time:75390ms step_avg:53.02ms
step:1423/1765 train_time:75473ms step_avg:53.04ms
step:1424/1765 train_time:75559ms step_avg:53.06ms
step:1425/1765 train_time:75641ms step_avg:53.08ms
step:1426/1765 train_time:75728ms step_avg:53.11ms
step:1427/1765 train_time:75811ms step_avg:53.13ms
step:1428/1765 train_time:75897ms step_avg:53.15ms
step:1429/1765 train_time:75981ms step_avg:53.17ms
step:1430/1765 train_time:76066ms step_avg:53.19ms
step:1431/1765 train_time:76149ms step_avg:53.21ms
step:1432/1765 train_time:76236ms step_avg:53.24ms
step:1433/1765 train_time:76320ms step_avg:53.26ms
step:1434/1765 train_time:76405ms step_avg:53.28ms
step:1435/1765 train_time:76487ms step_avg:53.30ms
step:1436/1765 train_time:76572ms step_avg:53.32ms
step:1437/1765 train_time:76655ms step_avg:53.34ms
step:1438/1765 train_time:76741ms step_avg:53.37ms
step:1439/1765 train_time:76824ms step_avg:53.39ms
step:1440/1765 train_time:76908ms step_avg:53.41ms
step:1441/1765 train_time:76991ms step_avg:53.43ms
step:1442/1765 train_time:77077ms step_avg:53.45ms
step:1443/1765 train_time:77160ms step_avg:53.47ms
step:1444/1765 train_time:77245ms step_avg:53.49ms
step:1445/1765 train_time:77329ms step_avg:53.51ms
step:1446/1765 train_time:77414ms step_avg:53.54ms
step:1447/1765 train_time:77497ms step_avg:53.56ms
step:1448/1765 train_time:77583ms step_avg:53.58ms
step:1449/1765 train_time:77665ms step_avg:53.60ms
step:1450/1765 train_time:77751ms step_avg:53.62ms
step:1451/1765 train_time:77834ms step_avg:53.64ms
step:1452/1765 train_time:77920ms step_avg:53.66ms
step:1453/1765 train_time:78002ms step_avg:53.68ms
step:1454/1765 train_time:78088ms step_avg:53.71ms
step:1455/1765 train_time:78171ms step_avg:53.73ms
step:1456/1765 train_time:78258ms step_avg:53.75ms
step:1457/1765 train_time:78341ms step_avg:53.77ms
step:1458/1765 train_time:78427ms step_avg:53.79ms
step:1459/1765 train_time:78511ms step_avg:53.81ms
step:1460/1765 train_time:78596ms step_avg:53.83ms
step:1461/1765 train_time:78678ms step_avg:53.85ms
step:1462/1765 train_time:78764ms step_avg:53.87ms
step:1463/1765 train_time:78848ms step_avg:53.89ms
step:1464/1765 train_time:78933ms step_avg:53.92ms
step:1465/1765 train_time:79017ms step_avg:53.94ms
step:1466/1765 train_time:79101ms step_avg:53.96ms
step:1467/1765 train_time:79183ms step_avg:53.98ms
step:1468/1765 train_time:79269ms step_avg:54.00ms
step:1469/1765 train_time:79352ms step_avg:54.02ms
step:1470/1765 train_time:79438ms step_avg:54.04ms
step:1471/1765 train_time:79521ms step_avg:54.06ms
step:1472/1765 train_time:79606ms step_avg:54.08ms
step:1473/1765 train_time:79689ms step_avg:54.10ms
step:1474/1765 train_time:79774ms step_avg:54.12ms
step:1475/1765 train_time:79857ms step_avg:54.14ms
step:1476/1765 train_time:79944ms step_avg:54.16ms
step:1477/1765 train_time:80026ms step_avg:54.18ms
step:1478/1765 train_time:80111ms step_avg:54.20ms
step:1479/1765 train_time:80195ms step_avg:54.22ms
step:1480/1765 train_time:80280ms step_avg:54.24ms
step:1481/1765 train_time:80364ms step_avg:54.26ms
step:1482/1765 train_time:80450ms step_avg:54.28ms
step:1483/1765 train_time:80533ms step_avg:54.30ms
step:1484/1765 train_time:80620ms step_avg:54.33ms
step:1485/1765 train_time:80702ms step_avg:54.34ms
step:1486/1765 train_time:80788ms step_avg:54.37ms
step:1487/1765 train_time:80870ms step_avg:54.38ms
step:1488/1765 train_time:80955ms step_avg:54.41ms
step:1489/1765 train_time:81039ms step_avg:54.43ms
step:1490/1765 train_time:81124ms step_avg:54.45ms
step:1491/1765 train_time:81207ms step_avg:54.46ms
step:1492/1765 train_time:81292ms step_avg:54.49ms
step:1493/1765 train_time:81375ms step_avg:54.50ms
step:1494/1765 train_time:81462ms step_avg:54.53ms
step:1495/1765 train_time:81546ms step_avg:54.55ms
step:1496/1765 train_time:81631ms step_avg:54.57ms
step:1497/1765 train_time:81715ms step_avg:54.59ms
step:1498/1765 train_time:81800ms step_avg:54.61ms
step:1499/1765 train_time:81882ms step_avg:54.62ms
step:1500/1765 train_time:81967ms step_avg:54.64ms
step:1500/1765 val_loss:3.3707 train_time:82077ms step_avg:54.72ms
step:1501/1765 train_time:82098ms step_avg:54.70ms
step:1502/1765 train_time:82138ms step_avg:54.69ms
step:1503/1765 train_time:82222ms step_avg:54.71ms
step:1504/1765 train_time:82311ms step_avg:54.73ms
step:1505/1765 train_time:82394ms step_avg:54.75ms
step:1506/1765 train_time:82479ms step_avg:54.77ms
step:1507/1765 train_time:82560ms step_avg:54.78ms
step:1508/1765 train_time:82645ms step_avg:54.80ms
step:1509/1765 train_time:82727ms step_avg:54.82ms
step:1510/1765 train_time:82811ms step_avg:54.84ms
step:1511/1765 train_time:82892ms step_avg:54.86ms
step:1512/1765 train_time:82979ms step_avg:54.88ms
step:1513/1765 train_time:83063ms step_avg:54.90ms
step:1514/1765 train_time:83151ms step_avg:54.92ms
step:1515/1765 train_time:83238ms step_avg:54.94ms
step:1516/1765 train_time:83324ms step_avg:54.96ms
step:1517/1765 train_time:83407ms step_avg:54.98ms
step:1518/1765 train_time:83492ms step_avg:55.00ms
step:1519/1765 train_time:83574ms step_avg:55.02ms
step:1520/1765 train_time:83660ms step_avg:55.04ms
step:1521/1765 train_time:83742ms step_avg:55.06ms
step:1522/1765 train_time:83826ms step_avg:55.08ms
step:1523/1765 train_time:83908ms step_avg:55.09ms
step:1524/1765 train_time:83994ms step_avg:55.11ms
step:1525/1765 train_time:84078ms step_avg:55.13ms
step:1526/1765 train_time:84164ms step_avg:55.15ms
step:1527/1765 train_time:84251ms step_avg:55.17ms
step:1528/1765 train_time:84336ms step_avg:55.19ms
step:1529/1765 train_time:84417ms step_avg:55.21ms
step:1530/1765 train_time:84503ms step_avg:55.23ms
step:1531/1765 train_time:84584ms step_avg:55.25ms
step:1532/1765 train_time:84670ms step_avg:55.27ms
step:1533/1765 train_time:84753ms step_avg:55.29ms
step:1534/1765 train_time:84838ms step_avg:55.31ms
step:1535/1765 train_time:84920ms step_avg:55.32ms
step:1536/1765 train_time:85006ms step_avg:55.34ms
step:1537/1765 train_time:85089ms step_avg:55.36ms
step:1538/1765 train_time:85178ms step_avg:55.38ms
step:1539/1765 train_time:85260ms step_avg:55.40ms
step:1540/1765 train_time:85346ms step_avg:55.42ms
step:1541/1765 train_time:85429ms step_avg:55.44ms
step:1542/1765 train_time:85515ms step_avg:55.46ms
step:1543/1765 train_time:85597ms step_avg:55.47ms
step:1544/1765 train_time:85682ms step_avg:55.49ms
step:1545/1765 train_time:85765ms step_avg:55.51ms
step:1546/1765 train_time:85849ms step_avg:55.53ms
step:1547/1765 train_time:85932ms step_avg:55.55ms
step:1548/1765 train_time:86018ms step_avg:55.57ms
step:1549/1765 train_time:86100ms step_avg:55.58ms
step:1550/1765 train_time:86187ms step_avg:55.60ms
step:1551/1765 train_time:86271ms step_avg:55.62ms
step:1552/1765 train_time:86357ms step_avg:55.64ms
step:1553/1765 train_time:86440ms step_avg:55.66ms
step:1554/1765 train_time:86525ms step_avg:55.68ms
step:1555/1765 train_time:86608ms step_avg:55.70ms
step:1556/1765 train_time:86692ms step_avg:55.71ms
step:1557/1765 train_time:86776ms step_avg:55.73ms
step:1558/1765 train_time:86861ms step_avg:55.75ms
step:1559/1765 train_time:86943ms step_avg:55.77ms
step:1560/1765 train_time:87029ms step_avg:55.79ms
step:1561/1765 train_time:87112ms step_avg:55.81ms
step:1562/1765 train_time:87199ms step_avg:55.83ms
step:1563/1765 train_time:87282ms step_avg:55.84ms
step:1564/1765 train_time:87368ms step_avg:55.86ms
step:1565/1765 train_time:87450ms step_avg:55.88ms
step:1566/1765 train_time:87536ms step_avg:55.90ms
step:1567/1765 train_time:87618ms step_avg:55.91ms
step:1568/1765 train_time:87702ms step_avg:55.93ms
step:1569/1765 train_time:87785ms step_avg:55.95ms
step:1570/1765 train_time:87870ms step_avg:55.97ms
step:1571/1765 train_time:87952ms step_avg:55.98ms
step:1572/1765 train_time:88038ms step_avg:56.00ms
step:1573/1765 train_time:88120ms step_avg:56.02ms
step:1574/1765 train_time:88207ms step_avg:56.04ms
step:1575/1765 train_time:88291ms step_avg:56.06ms
step:1576/1765 train_time:88377ms step_avg:56.08ms
step:1577/1765 train_time:88460ms step_avg:56.09ms
step:1578/1765 train_time:88545ms step_avg:56.11ms
step:1579/1765 train_time:88628ms step_avg:56.13ms
step:1580/1765 train_time:88713ms step_avg:56.15ms
step:1581/1765 train_time:88795ms step_avg:56.16ms
step:1582/1765 train_time:88881ms step_avg:56.18ms
step:1583/1765 train_time:88963ms step_avg:56.20ms
step:1584/1765 train_time:89049ms step_avg:56.22ms
step:1585/1765 train_time:89133ms step_avg:56.24ms
step:1586/1765 train_time:89218ms step_avg:56.25ms
step:1587/1765 train_time:89301ms step_avg:56.27ms
step:1588/1765 train_time:89386ms step_avg:56.29ms
step:1589/1765 train_time:89469ms step_avg:56.31ms
step:1590/1765 train_time:89555ms step_avg:56.32ms
step:1591/1765 train_time:89639ms step_avg:56.34ms
step:1592/1765 train_time:89723ms step_avg:56.36ms
step:1593/1765 train_time:89806ms step_avg:56.38ms
step:1594/1765 train_time:89890ms step_avg:56.39ms
step:1595/1765 train_time:89974ms step_avg:56.41ms
step:1596/1765 train_time:90060ms step_avg:56.43ms
step:1597/1765 train_time:90142ms step_avg:56.44ms
step:1598/1765 train_time:90229ms step_avg:56.46ms
step:1599/1765 train_time:90311ms step_avg:56.48ms
step:1600/1765 train_time:90397ms step_avg:56.50ms
step:1601/1765 train_time:90480ms step_avg:56.51ms
step:1602/1765 train_time:90565ms step_avg:56.53ms
step:1603/1765 train_time:90648ms step_avg:56.55ms
step:1604/1765 train_time:90735ms step_avg:56.57ms
step:1605/1765 train_time:90817ms step_avg:56.58ms
step:1606/1765 train_time:90902ms step_avg:56.60ms
step:1607/1765 train_time:90985ms step_avg:56.62ms
step:1608/1765 train_time:91070ms step_avg:56.64ms
step:1609/1765 train_time:91153ms step_avg:56.65ms
step:1610/1765 train_time:91240ms step_avg:56.67ms
step:1611/1765 train_time:91323ms step_avg:56.69ms
step:1612/1765 train_time:91409ms step_avg:56.71ms
step:1613/1765 train_time:91491ms step_avg:56.72ms
step:1614/1765 train_time:91578ms step_avg:56.74ms
step:1615/1765 train_time:91660ms step_avg:56.76ms
step:1616/1765 train_time:91746ms step_avg:56.77ms
step:1617/1765 train_time:91829ms step_avg:56.79ms
step:1618/1765 train_time:91914ms step_avg:56.81ms
step:1619/1765 train_time:91997ms step_avg:56.82ms
step:1620/1765 train_time:92082ms step_avg:56.84ms
step:1621/1765 train_time:92165ms step_avg:56.86ms
step:1622/1765 train_time:92252ms step_avg:56.88ms
step:1623/1765 train_time:92335ms step_avg:56.89ms
step:1624/1765 train_time:92421ms step_avg:56.91ms
step:1625/1765 train_time:92504ms step_avg:56.93ms
step:1626/1765 train_time:92589ms step_avg:56.94ms
step:1627/1765 train_time:92672ms step_avg:56.96ms
step:1628/1765 train_time:92759ms step_avg:56.98ms
step:1629/1765 train_time:92842ms step_avg:56.99ms
step:1630/1765 train_time:92927ms step_avg:57.01ms
step:1631/1765 train_time:93009ms step_avg:57.03ms
step:1632/1765 train_time:93094ms step_avg:57.04ms
step:1633/1765 train_time:93178ms step_avg:57.06ms
step:1634/1765 train_time:93263ms step_avg:57.08ms
step:1635/1765 train_time:93346ms step_avg:57.09ms
step:1636/1765 train_time:93432ms step_avg:57.11ms
step:1637/1765 train_time:93514ms step_avg:57.13ms
step:1638/1765 train_time:93601ms step_avg:57.14ms
step:1639/1765 train_time:93683ms step_avg:57.16ms
step:1640/1765 train_time:93768ms step_avg:57.18ms
step:1641/1765 train_time:93852ms step_avg:57.19ms
step:1642/1765 train_time:93938ms step_avg:57.21ms
step:1643/1765 train_time:94020ms step_avg:57.22ms
step:1644/1765 train_time:94105ms step_avg:57.24ms
step:1645/1765 train_time:94188ms step_avg:57.26ms
step:1646/1765 train_time:94274ms step_avg:57.27ms
step:1647/1765 train_time:94357ms step_avg:57.29ms
step:1648/1765 train_time:94442ms step_avg:57.31ms
step:1649/1765 train_time:94525ms step_avg:57.32ms
step:1650/1765 train_time:94611ms step_avg:57.34ms
step:1651/1765 train_time:94693ms step_avg:57.36ms
step:1652/1765 train_time:94780ms step_avg:57.37ms
step:1653/1765 train_time:94862ms step_avg:57.39ms
step:1654/1765 train_time:94948ms step_avg:57.41ms
step:1655/1765 train_time:95031ms step_avg:57.42ms
step:1656/1765 train_time:95117ms step_avg:57.44ms
step:1657/1765 train_time:95200ms step_avg:57.45ms
step:1658/1765 train_time:95286ms step_avg:57.47ms
step:1659/1765 train_time:95369ms step_avg:57.49ms
step:1660/1765 train_time:95454ms step_avg:57.50ms
step:1661/1765 train_time:95538ms step_avg:57.52ms
step:1662/1765 train_time:95622ms step_avg:57.53ms
step:1663/1765 train_time:95705ms step_avg:57.55ms
step:1664/1765 train_time:95791ms step_avg:57.57ms
step:1665/1765 train_time:95875ms step_avg:57.58ms
step:1666/1765 train_time:95960ms step_avg:57.60ms
step:1667/1765 train_time:96043ms step_avg:57.61ms
step:1668/1765 train_time:96128ms step_avg:57.63ms
step:1669/1765 train_time:96211ms step_avg:57.65ms
step:1670/1765 train_time:96297ms step_avg:57.66ms
step:1671/1765 train_time:96380ms step_avg:57.68ms
step:1672/1765 train_time:96465ms step_avg:57.69ms
step:1673/1765 train_time:96548ms step_avg:57.71ms
step:1674/1765 train_time:96634ms step_avg:57.73ms
step:1675/1765 train_time:96716ms step_avg:57.74ms
step:1676/1765 train_time:96801ms step_avg:57.76ms
step:1677/1765 train_time:96884ms step_avg:57.77ms
step:1678/1765 train_time:96970ms step_avg:57.79ms
step:1679/1765 train_time:97053ms step_avg:57.80ms
step:1680/1765 train_time:97138ms step_avg:57.82ms
step:1681/1765 train_time:97222ms step_avg:57.84ms
step:1682/1765 train_time:97308ms step_avg:57.85ms
step:1683/1765 train_time:97391ms step_avg:57.87ms
step:1684/1765 train_time:97476ms step_avg:57.88ms
step:1685/1765 train_time:97557ms step_avg:57.90ms
step:1686/1765 train_time:97643ms step_avg:57.91ms
step:1687/1765 train_time:97726ms step_avg:57.93ms
step:1688/1765 train_time:97812ms step_avg:57.95ms
step:1689/1765 train_time:97895ms step_avg:57.96ms
step:1690/1765 train_time:97981ms step_avg:57.98ms
step:1691/1765 train_time:98064ms step_avg:57.99ms
step:1692/1765 train_time:98149ms step_avg:58.01ms
step:1693/1765 train_time:98232ms step_avg:58.02ms
step:1694/1765 train_time:98317ms step_avg:58.04ms
step:1695/1765 train_time:98399ms step_avg:58.05ms
step:1696/1765 train_time:98485ms step_avg:58.07ms
step:1697/1765 train_time:98568ms step_avg:58.08ms
step:1698/1765 train_time:98654ms step_avg:58.10ms
step:1699/1765 train_time:98737ms step_avg:58.11ms
step:1700/1765 train_time:98822ms step_avg:58.13ms
step:1701/1765 train_time:98904ms step_avg:58.14ms
step:1702/1765 train_time:98990ms step_avg:58.16ms
step:1703/1765 train_time:99074ms step_avg:58.18ms
step:1704/1765 train_time:99160ms step_avg:58.19ms
step:1705/1765 train_time:99243ms step_avg:58.21ms
step:1706/1765 train_time:99327ms step_avg:58.22ms
step:1707/1765 train_time:99411ms step_avg:58.24ms
step:1708/1765 train_time:99496ms step_avg:58.25ms
step:1709/1765 train_time:99578ms step_avg:58.27ms
step:1710/1765 train_time:99665ms step_avg:58.28ms
step:1711/1765 train_time:99747ms step_avg:58.30ms
step:1712/1765 train_time:99833ms step_avg:58.31ms
step:1713/1765 train_time:99915ms step_avg:58.33ms
step:1714/1765 train_time:100001ms step_avg:58.34ms
step:1715/1765 train_time:100083ms step_avg:58.36ms
step:1716/1765 train_time:100170ms step_avg:58.37ms
step:1717/1765 train_time:100252ms step_avg:58.39ms
step:1718/1765 train_time:100339ms step_avg:58.40ms
step:1719/1765 train_time:100421ms step_avg:58.42ms
step:1720/1765 train_time:100507ms step_avg:58.43ms
step:1721/1765 train_time:100590ms step_avg:58.45ms
step:1722/1765 train_time:100675ms step_avg:58.46ms
step:1723/1765 train_time:100758ms step_avg:58.48ms
step:1724/1765 train_time:100843ms step_avg:58.49ms
step:1725/1765 train_time:100927ms step_avg:58.51ms
step:1726/1765 train_time:101017ms step_avg:58.53ms
step:1727/1765 train_time:101101ms step_avg:58.54ms
step:1728/1765 train_time:101187ms step_avg:58.56ms
step:1729/1765 train_time:101271ms step_avg:58.57ms
step:1730/1765 train_time:101357ms step_avg:58.59ms
step:1731/1765 train_time:101441ms step_avg:58.60ms
step:1732/1765 train_time:101526ms step_avg:58.62ms
step:1733/1765 train_time:101609ms step_avg:58.63ms
step:1734/1765 train_time:101696ms step_avg:58.65ms
step:1735/1765 train_time:101779ms step_avg:58.66ms
step:1736/1765 train_time:101864ms step_avg:58.68ms
step:1737/1765 train_time:101948ms step_avg:58.69ms
step:1738/1765 train_time:102035ms step_avg:58.71ms
step:1739/1765 train_time:102117ms step_avg:58.72ms
step:1740/1765 train_time:102203ms step_avg:58.74ms
step:1741/1765 train_time:102287ms step_avg:58.75ms
step:1742/1765 train_time:102374ms step_avg:58.77ms
step:1743/1765 train_time:102457ms step_avg:58.78ms
step:1744/1765 train_time:102543ms step_avg:58.80ms
step:1745/1765 train_time:102627ms step_avg:58.81ms
step:1746/1765 train_time:102712ms step_avg:58.83ms
step:1747/1765 train_time:102795ms step_avg:58.84ms
step:1748/1765 train_time:102880ms step_avg:58.86ms
step:1749/1765 train_time:102964ms step_avg:58.87ms
step:1750/1765 train_time:103049ms step_avg:58.89ms
step:1750/1765 val_loss:3.2850 train_time:103159ms step_avg:58.95ms
step:1751/1765 train_time:103179ms step_avg:58.93ms
step:1752/1765 train_time:103218ms step_avg:58.91ms
step:1753/1765 train_time:103305ms step_avg:58.93ms
step:1754/1765 train_time:103395ms step_avg:58.95ms
step:1755/1765 train_time:103480ms step_avg:58.96ms
step:1756/1765 train_time:103566ms step_avg:58.98ms
step:1757/1765 train_time:103649ms step_avg:58.99ms
step:1758/1765 train_time:103734ms step_avg:59.01ms
step:1759/1765 train_time:103816ms step_avg:59.02ms
step:1760/1765 train_time:103900ms step_avg:59.03ms
step:1761/1765 train_time:103982ms step_avg:59.05ms
step:1762/1765 train_time:104070ms step_avg:59.06ms
step:1763/1765 train_time:104157ms step_avg:59.08ms
step:1764/1765 train_time:104246ms step_avg:59.10ms
step:1765/1765 train_time:104330ms step_avg:59.11ms
step:1765/1765 val_loss:3.2803 train_time:104442ms step_avg:59.17ms
peak memory allocated: 28992 MiB reserved: 44838 MiB

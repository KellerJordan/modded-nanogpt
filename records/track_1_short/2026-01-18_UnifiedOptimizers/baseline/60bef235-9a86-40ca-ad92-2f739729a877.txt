import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan 19 02:04:32 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:00:0B.0 Off |                    0 |
| N/A   31C    P0             109W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:00:0C.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:00:0D.0 Off |                    0 |
| N/A   37C    P0             115W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:00:0E.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:00:0F.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:00:10.0 Off |                    0 |
| N/A   38C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:00:11.0 Off |                    0 |
| N/A   35C    P0             114W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:00:12.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    145317      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    1   N/A  N/A    145318      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    2   N/A  N/A    145319      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    3   N/A  N/A    145320      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    4   N/A  N/A    145321      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    5   N/A  N/A    145322      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    6   N/A  N/A    145323      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    7   N/A  N/A    145324      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8284 train_time:0ms step_avg:0.04ms
step:1/1775 train_time:97ms step_avg:96.75ms
step:2/1775 train_time:122ms step_avg:60.78ms
step:3/1775 train_time:143ms step_avg:47.60ms
step:4/1775 train_time:165ms step_avg:41.33ms
step:5/1775 train_time:194ms step_avg:38.73ms
step:6/1775 train_time:338ms step_avg:56.32ms
step:7/1775 train_time:357ms step_avg:51.06ms
step:8/1775 train_time:379ms step_avg:47.36ms
step:9/1775 train_time:405ms step_avg:45.01ms
step:10/1775 train_time:438ms step_avg:43.81ms
step:11/1775 train_time:469ms step_avg:42.61ms
step:12/1775 train_time:502ms step_avg:41.85ms
step:13/1775 train_time:533ms step_avg:41.03ms
step:14/1775 train_time:567ms step_avg:40.48ms
step:15/1775 train_time:597ms step_avg:39.83ms
step:16/1775 train_time:630ms step_avg:39.40ms
step:17/1775 train_time:661ms step_avg:38.91ms
step:18/1775 train_time:695ms step_avg:38.59ms
step:19/1775 train_time:726ms step_avg:38.19ms
step:20/1775 train_time:759ms step_avg:37.94ms
step:21/1775 train_time:790ms step_avg:37.60ms
step:22/1775 train_time:823ms step_avg:37.40ms
step:23/1775 train_time:854ms step_avg:37.13ms
step:24/1775 train_time:887ms step_avg:36.97ms
step:25/1775 train_time:918ms step_avg:36.74ms
step:26/1775 train_time:952ms step_avg:36.61ms
step:27/1775 train_time:983ms step_avg:36.40ms
step:28/1775 train_time:1016ms step_avg:36.30ms
step:29/1775 train_time:1047ms step_avg:36.12ms
step:30/1775 train_time:1080ms step_avg:36.01ms
step:31/1775 train_time:1111ms step_avg:35.85ms
step:32/1775 train_time:1144ms step_avg:35.76ms
step:33/1775 train_time:1175ms step_avg:35.62ms
step:34/1775 train_time:1209ms step_avg:35.55ms
step:35/1775 train_time:1241ms step_avg:35.45ms
step:36/1775 train_time:1276ms step_avg:35.44ms
step:37/1775 train_time:1310ms step_avg:35.39ms
step:38/1775 train_time:1344ms step_avg:35.37ms
step:39/1775 train_time:1376ms step_avg:35.29ms
step:40/1775 train_time:1410ms step_avg:35.25ms
step:41/1775 train_time:1442ms step_avg:35.16ms
step:42/1775 train_time:1476ms step_avg:35.13ms
step:43/1775 train_time:1507ms step_avg:35.05ms
step:44/1775 train_time:1541ms step_avg:35.02ms
step:45/1775 train_time:1572ms step_avg:34.93ms
step:46/1775 train_time:1605ms step_avg:34.90ms
step:47/1775 train_time:1637ms step_avg:34.83ms
step:48/1775 train_time:1670ms step_avg:34.79ms
step:49/1775 train_time:1702ms step_avg:34.73ms
step:50/1775 train_time:1735ms step_avg:34.70ms
step:51/1775 train_time:1766ms step_avg:34.63ms
step:52/1775 train_time:1800ms step_avg:34.61ms
step:53/1775 train_time:1831ms step_avg:34.55ms
step:54/1775 train_time:1865ms step_avg:34.54ms
step:55/1775 train_time:1896ms step_avg:34.48ms
step:56/1775 train_time:1929ms step_avg:34.45ms
step:57/1775 train_time:1961ms step_avg:34.40ms
step:58/1775 train_time:1994ms step_avg:34.37ms
step:59/1775 train_time:2025ms step_avg:34.32ms
step:60/1775 train_time:2058ms step_avg:34.31ms
step:61/1775 train_time:2089ms step_avg:34.25ms
step:62/1775 train_time:2122ms step_avg:34.23ms
step:63/1775 train_time:2154ms step_avg:34.18ms
step:64/1775 train_time:2187ms step_avg:34.17ms
step:65/1775 train_time:2218ms step_avg:34.12ms
step:66/1775 train_time:2252ms step_avg:34.12ms
step:67/1775 train_time:2284ms step_avg:34.09ms
step:68/1775 train_time:2318ms step_avg:34.09ms
step:69/1775 train_time:2350ms step_avg:34.05ms
step:70/1775 train_time:2383ms step_avg:34.04ms
step:71/1775 train_time:2414ms step_avg:34.00ms
step:72/1775 train_time:2448ms step_avg:34.00ms
step:73/1775 train_time:2480ms step_avg:33.98ms
step:74/1775 train_time:2514ms step_avg:33.97ms
step:75/1775 train_time:2546ms step_avg:33.94ms
step:76/1775 train_time:2579ms step_avg:33.94ms
step:77/1775 train_time:2610ms step_avg:33.90ms
step:78/1775 train_time:2643ms step_avg:33.89ms
step:79/1775 train_time:2674ms step_avg:33.85ms
step:80/1775 train_time:2708ms step_avg:33.85ms
step:81/1775 train_time:2739ms step_avg:33.82ms
step:82/1775 train_time:2773ms step_avg:33.81ms
step:83/1775 train_time:2805ms step_avg:33.79ms
step:84/1775 train_time:2838ms step_avg:33.78ms
step:85/1775 train_time:2869ms step_avg:33.75ms
step:86/1775 train_time:2902ms step_avg:33.74ms
step:87/1775 train_time:2933ms step_avg:33.72ms
step:88/1775 train_time:2967ms step_avg:33.72ms
step:89/1775 train_time:2998ms step_avg:33.69ms
step:90/1775 train_time:3032ms step_avg:33.68ms
step:91/1775 train_time:3063ms step_avg:33.66ms
step:92/1775 train_time:3097ms step_avg:33.66ms
step:93/1775 train_time:3128ms step_avg:33.63ms
step:94/1775 train_time:3161ms step_avg:33.63ms
step:95/1775 train_time:3192ms step_avg:33.60ms
step:96/1775 train_time:3226ms step_avg:33.60ms
step:97/1775 train_time:3258ms step_avg:33.59ms
step:98/1775 train_time:3292ms step_avg:33.59ms
step:99/1775 train_time:3324ms step_avg:33.58ms
step:100/1775 train_time:3357ms step_avg:33.57ms
step:101/1775 train_time:3388ms step_avg:33.55ms
step:102/1775 train_time:3422ms step_avg:33.55ms
step:103/1775 train_time:3453ms step_avg:33.52ms
step:104/1775 train_time:3487ms step_avg:33.53ms
step:105/1775 train_time:3518ms step_avg:33.51ms
step:106/1775 train_time:3552ms step_avg:33.51ms
step:107/1775 train_time:3584ms step_avg:33.49ms
step:108/1775 train_time:3618ms step_avg:33.50ms
step:109/1775 train_time:3649ms step_avg:33.47ms
step:110/1775 train_time:3682ms step_avg:33.47ms
step:111/1775 train_time:3713ms step_avg:33.45ms
step:112/1775 train_time:3747ms step_avg:33.45ms
step:113/1775 train_time:3778ms step_avg:33.44ms
step:114/1775 train_time:3812ms step_avg:33.44ms
step:115/1775 train_time:3843ms step_avg:33.42ms
step:116/1775 train_time:3876ms step_avg:33.42ms
step:117/1775 train_time:3908ms step_avg:33.40ms
step:118/1775 train_time:3941ms step_avg:33.39ms
step:119/1775 train_time:3972ms step_avg:33.38ms
step:120/1775 train_time:4005ms step_avg:33.38ms
step:121/1775 train_time:4036ms step_avg:33.36ms
step:122/1775 train_time:4070ms step_avg:33.36ms
step:123/1775 train_time:4101ms step_avg:33.34ms
step:124/1775 train_time:4134ms step_avg:33.34ms
step:125/1775 train_time:4166ms step_avg:33.33ms
step:126/1775 train_time:4199ms step_avg:33.33ms
step:127/1775 train_time:4230ms step_avg:33.31ms
step:128/1775 train_time:4264ms step_avg:33.31ms
step:129/1775 train_time:4295ms step_avg:33.30ms
step:130/1775 train_time:4329ms step_avg:33.30ms
step:131/1775 train_time:4361ms step_avg:33.29ms
step:132/1775 train_time:4395ms step_avg:33.29ms
step:133/1775 train_time:4426ms step_avg:33.28ms
step:134/1775 train_time:4460ms step_avg:33.29ms
step:135/1775 train_time:4492ms step_avg:33.27ms
step:136/1775 train_time:4526ms step_avg:33.28ms
step:137/1775 train_time:4557ms step_avg:33.26ms
step:138/1775 train_time:4591ms step_avg:33.26ms
step:139/1775 train_time:4622ms step_avg:33.25ms
step:140/1775 train_time:4655ms step_avg:33.25ms
step:141/1775 train_time:4687ms step_avg:33.24ms
step:142/1775 train_time:4720ms step_avg:33.24ms
step:143/1775 train_time:4751ms step_avg:33.23ms
step:144/1775 train_time:4785ms step_avg:33.23ms
step:145/1775 train_time:4816ms step_avg:33.22ms
step:146/1775 train_time:4850ms step_avg:33.22ms
step:147/1775 train_time:4880ms step_avg:33.20ms
step:148/1775 train_time:4914ms step_avg:33.20ms
step:149/1775 train_time:4946ms step_avg:33.19ms
step:150/1775 train_time:4980ms step_avg:33.20ms
step:151/1775 train_time:5011ms step_avg:33.19ms
step:152/1775 train_time:5044ms step_avg:33.19ms
step:153/1775 train_time:5076ms step_avg:33.17ms
step:154/1775 train_time:5110ms step_avg:33.18ms
step:155/1775 train_time:5140ms step_avg:33.16ms
step:156/1775 train_time:5174ms step_avg:33.17ms
step:157/1775 train_time:5206ms step_avg:33.16ms
step:158/1775 train_time:5239ms step_avg:33.16ms
step:159/1775 train_time:5270ms step_avg:33.15ms
step:160/1775 train_time:5304ms step_avg:33.15ms
step:161/1775 train_time:5335ms step_avg:33.14ms
step:162/1775 train_time:5369ms step_avg:33.14ms
step:163/1775 train_time:5401ms step_avg:33.14ms
step:164/1775 train_time:5435ms step_avg:33.14ms
step:165/1775 train_time:5467ms step_avg:33.13ms
step:166/1775 train_time:5500ms step_avg:33.13ms
step:167/1775 train_time:5531ms step_avg:33.12ms
step:168/1775 train_time:5565ms step_avg:33.12ms
step:169/1775 train_time:5596ms step_avg:33.11ms
step:170/1775 train_time:5630ms step_avg:33.12ms
step:171/1775 train_time:5661ms step_avg:33.10ms
step:172/1775 train_time:5694ms step_avg:33.10ms
step:173/1775 train_time:5725ms step_avg:33.09ms
step:174/1775 train_time:5759ms step_avg:33.10ms
step:175/1775 train_time:5790ms step_avg:33.08ms
step:176/1775 train_time:5823ms step_avg:33.08ms
step:177/1775 train_time:5854ms step_avg:33.07ms
step:178/1775 train_time:5888ms step_avg:33.08ms
step:179/1775 train_time:5919ms step_avg:33.07ms
step:180/1775 train_time:5952ms step_avg:33.07ms
step:181/1775 train_time:5983ms step_avg:33.06ms
step:182/1775 train_time:6016ms step_avg:33.06ms
step:183/1775 train_time:6048ms step_avg:33.05ms
step:184/1775 train_time:6081ms step_avg:33.05ms
step:185/1775 train_time:6112ms step_avg:33.04ms
step:186/1775 train_time:6146ms step_avg:33.04ms
step:187/1775 train_time:6177ms step_avg:33.03ms
step:188/1775 train_time:6211ms step_avg:33.04ms
step:189/1775 train_time:6242ms step_avg:33.03ms
step:190/1775 train_time:6276ms step_avg:33.03ms
step:191/1775 train_time:6308ms step_avg:33.02ms
step:192/1775 train_time:6341ms step_avg:33.03ms
step:193/1775 train_time:6372ms step_avg:33.02ms
step:194/1775 train_time:6406ms step_avg:33.02ms
step:195/1775 train_time:6437ms step_avg:33.01ms
step:196/1775 train_time:6470ms step_avg:33.01ms
step:197/1775 train_time:6502ms step_avg:33.00ms
step:198/1775 train_time:6535ms step_avg:33.01ms
step:199/1775 train_time:6566ms step_avg:33.00ms
step:200/1775 train_time:6600ms step_avg:33.00ms
step:201/1775 train_time:6630ms step_avg:32.99ms
step:202/1775 train_time:6664ms step_avg:32.99ms
step:203/1775 train_time:6695ms step_avg:32.98ms
step:204/1775 train_time:6729ms step_avg:32.99ms
step:205/1775 train_time:6760ms step_avg:32.98ms
step:206/1775 train_time:6793ms step_avg:32.98ms
step:207/1775 train_time:6824ms step_avg:32.97ms
step:208/1775 train_time:6857ms step_avg:32.97ms
step:209/1775 train_time:6888ms step_avg:32.96ms
step:210/1775 train_time:6921ms step_avg:32.96ms
step:211/1775 train_time:6952ms step_avg:32.95ms
step:212/1775 train_time:6986ms step_avg:32.95ms
step:213/1775 train_time:7017ms step_avg:32.94ms
step:214/1775 train_time:7050ms step_avg:32.95ms
step:215/1775 train_time:7082ms step_avg:32.94ms
step:216/1775 train_time:7115ms step_avg:32.94ms
step:217/1775 train_time:7147ms step_avg:32.93ms
step:218/1775 train_time:7180ms step_avg:32.94ms
step:219/1775 train_time:7211ms step_avg:32.93ms
step:220/1775 train_time:7244ms step_avg:32.93ms
step:221/1775 train_time:7276ms step_avg:32.92ms
step:222/1775 train_time:7310ms step_avg:32.93ms
step:223/1775 train_time:7341ms step_avg:32.92ms
step:224/1775 train_time:7375ms step_avg:32.92ms
step:225/1775 train_time:7406ms step_avg:32.92ms
step:226/1775 train_time:7440ms step_avg:32.92ms
step:227/1775 train_time:7471ms step_avg:32.91ms
step:228/1775 train_time:7504ms step_avg:32.91ms
step:229/1775 train_time:7536ms step_avg:32.91ms
step:230/1775 train_time:7569ms step_avg:32.91ms
step:231/1775 train_time:7601ms step_avg:32.90ms
step:232/1775 train_time:7635ms step_avg:32.91ms
step:233/1775 train_time:7666ms step_avg:32.90ms
step:234/1775 train_time:7699ms step_avg:32.90ms
step:235/1775 train_time:7731ms step_avg:32.90ms
step:236/1775 train_time:7764ms step_avg:32.90ms
step:237/1775 train_time:7795ms step_avg:32.89ms
step:238/1775 train_time:7829ms step_avg:32.89ms
step:239/1775 train_time:7860ms step_avg:32.89ms
step:240/1775 train_time:7893ms step_avg:32.89ms
step:241/1775 train_time:7924ms step_avg:32.88ms
step:242/1775 train_time:7957ms step_avg:32.88ms
step:243/1775 train_time:7988ms step_avg:32.87ms
step:244/1775 train_time:8021ms step_avg:32.87ms
step:245/1775 train_time:8052ms step_avg:32.87ms
step:246/1775 train_time:8086ms step_avg:32.87ms
step:247/1775 train_time:8117ms step_avg:32.86ms
step:248/1775 train_time:8151ms step_avg:32.87ms
step:249/1775 train_time:8182ms step_avg:32.86ms
step:250/1775 train_time:8215ms step_avg:32.86ms
step:250/1775 val_loss:4.6264 train_time:8257ms step_avg:33.03ms
step:251/1775 train_time:8279ms step_avg:32.98ms
step:252/1775 train_time:8301ms step_avg:32.94ms
step:253/1775 train_time:8320ms step_avg:32.89ms
step:254/1775 train_time:8347ms step_avg:32.86ms
step:255/1775 train_time:8379ms step_avg:32.86ms
step:256/1775 train_time:8414ms step_avg:32.87ms
step:257/1775 train_time:8446ms step_avg:32.86ms
step:258/1775 train_time:8479ms step_avg:32.87ms
step:259/1775 train_time:8510ms step_avg:32.86ms
step:260/1775 train_time:8544ms step_avg:32.86ms
step:261/1775 train_time:8575ms step_avg:32.85ms
step:262/1775 train_time:8608ms step_avg:32.85ms
step:263/1775 train_time:8639ms step_avg:32.85ms
step:264/1775 train_time:8672ms step_avg:32.85ms
step:265/1775 train_time:8704ms step_avg:32.84ms
step:266/1775 train_time:8737ms step_avg:32.85ms
step:267/1775 train_time:8768ms step_avg:32.84ms
step:268/1775 train_time:8801ms step_avg:32.84ms
step:269/1775 train_time:8832ms step_avg:32.83ms
step:270/1775 train_time:8865ms step_avg:32.83ms
step:271/1775 train_time:8897ms step_avg:32.83ms
step:272/1775 train_time:8930ms step_avg:32.83ms
step:273/1775 train_time:8961ms step_avg:32.82ms
step:274/1775 train_time:8994ms step_avg:32.82ms
step:275/1775 train_time:9025ms step_avg:32.82ms
step:276/1775 train_time:9058ms step_avg:32.82ms
step:277/1775 train_time:9089ms step_avg:32.81ms
step:278/1775 train_time:9122ms step_avg:32.81ms
step:279/1775 train_time:9153ms step_avg:32.81ms
step:280/1775 train_time:9187ms step_avg:32.81ms
step:281/1775 train_time:9218ms step_avg:32.80ms
step:282/1775 train_time:9252ms step_avg:32.81ms
step:283/1775 train_time:9284ms step_avg:32.80ms
step:284/1775 train_time:9317ms step_avg:32.81ms
step:285/1775 train_time:9349ms step_avg:32.80ms
step:286/1775 train_time:9383ms step_avg:32.81ms
step:287/1775 train_time:9414ms step_avg:32.80ms
step:288/1775 train_time:9448ms step_avg:32.81ms
step:289/1775 train_time:9480ms step_avg:32.80ms
step:290/1775 train_time:9513ms step_avg:32.80ms
step:291/1775 train_time:9544ms step_avg:32.80ms
step:292/1775 train_time:9578ms step_avg:32.80ms
step:293/1775 train_time:9609ms step_avg:32.79ms
step:294/1775 train_time:9642ms step_avg:32.80ms
step:295/1775 train_time:9673ms step_avg:32.79ms
step:296/1775 train_time:9707ms step_avg:32.79ms
step:297/1775 train_time:9738ms step_avg:32.79ms
step:298/1775 train_time:9771ms step_avg:32.79ms
step:299/1775 train_time:9802ms step_avg:32.78ms
step:300/1775 train_time:9835ms step_avg:32.78ms
step:301/1775 train_time:9867ms step_avg:32.78ms
step:302/1775 train_time:9900ms step_avg:32.78ms
step:303/1775 train_time:9931ms step_avg:32.77ms
step:304/1775 train_time:9964ms step_avg:32.77ms
step:305/1775 train_time:9995ms step_avg:32.77ms
step:306/1775 train_time:10028ms step_avg:32.77ms
step:307/1775 train_time:10059ms step_avg:32.76ms
step:308/1775 train_time:10092ms step_avg:32.77ms
step:309/1775 train_time:10123ms step_avg:32.76ms
step:310/1775 train_time:10156ms step_avg:32.76ms
step:311/1775 train_time:10187ms step_avg:32.76ms
step:312/1775 train_time:10221ms step_avg:32.76ms
step:313/1775 train_time:10252ms step_avg:32.76ms
step:314/1775 train_time:10286ms step_avg:32.76ms
step:315/1775 train_time:10318ms step_avg:32.75ms
step:316/1775 train_time:10351ms step_avg:32.76ms
step:317/1775 train_time:10382ms step_avg:32.75ms
step:318/1775 train_time:10416ms step_avg:32.75ms
step:319/1775 train_time:10447ms step_avg:32.75ms
step:320/1775 train_time:10482ms step_avg:32.76ms
step:321/1775 train_time:10513ms step_avg:32.75ms
step:322/1775 train_time:10546ms step_avg:32.75ms
step:323/1775 train_time:10577ms step_avg:32.75ms
step:324/1775 train_time:10611ms step_avg:32.75ms
step:325/1775 train_time:10642ms step_avg:32.75ms
step:326/1775 train_time:10675ms step_avg:32.75ms
step:327/1775 train_time:10707ms step_avg:32.74ms
step:328/1775 train_time:10740ms step_avg:32.74ms
step:329/1775 train_time:10771ms step_avg:32.74ms
step:330/1775 train_time:10804ms step_avg:32.74ms
step:331/1775 train_time:10835ms step_avg:32.73ms
step:332/1775 train_time:10868ms step_avg:32.74ms
step:333/1775 train_time:10899ms step_avg:32.73ms
step:334/1775 train_time:10933ms step_avg:32.73ms
step:335/1775 train_time:10964ms step_avg:32.73ms
step:336/1775 train_time:10997ms step_avg:32.73ms
step:337/1775 train_time:11028ms step_avg:32.72ms
step:338/1775 train_time:11061ms step_avg:32.72ms
step:339/1775 train_time:11092ms step_avg:32.72ms
step:340/1775 train_time:11125ms step_avg:32.72ms
step:341/1775 train_time:11157ms step_avg:32.72ms
step:342/1775 train_time:11190ms step_avg:32.72ms
step:343/1775 train_time:11221ms step_avg:32.71ms
step:344/1775 train_time:11255ms step_avg:32.72ms
step:345/1775 train_time:11286ms step_avg:32.71ms
step:346/1775 train_time:11320ms step_avg:32.72ms
step:347/1775 train_time:11350ms step_avg:32.71ms
step:348/1775 train_time:11385ms step_avg:32.72ms
step:349/1775 train_time:11416ms step_avg:32.71ms
step:350/1775 train_time:11450ms step_avg:32.71ms
step:351/1775 train_time:11482ms step_avg:32.71ms
step:352/1775 train_time:11515ms step_avg:32.71ms
step:353/1775 train_time:11547ms step_avg:32.71ms
step:354/1775 train_time:11580ms step_avg:32.71ms
step:355/1775 train_time:11611ms step_avg:32.71ms
step:356/1775 train_time:11645ms step_avg:32.71ms
step:357/1775 train_time:11676ms step_avg:32.71ms
step:358/1775 train_time:11709ms step_avg:32.71ms
step:359/1775 train_time:11741ms step_avg:32.70ms
step:360/1775 train_time:11774ms step_avg:32.71ms
step:361/1775 train_time:11806ms step_avg:32.70ms
step:362/1775 train_time:11839ms step_avg:32.70ms
step:363/1775 train_time:11870ms step_avg:32.70ms
step:364/1775 train_time:11903ms step_avg:32.70ms
step:365/1775 train_time:11934ms step_avg:32.69ms
step:366/1775 train_time:11967ms step_avg:32.70ms
step:367/1775 train_time:11998ms step_avg:32.69ms
step:368/1775 train_time:12031ms step_avg:32.69ms
step:369/1775 train_time:12062ms step_avg:32.69ms
step:370/1775 train_time:12095ms step_avg:32.69ms
step:371/1775 train_time:12126ms step_avg:32.69ms
step:372/1775 train_time:12159ms step_avg:32.69ms
step:373/1775 train_time:12190ms step_avg:32.68ms
step:374/1775 train_time:12224ms step_avg:32.68ms
step:375/1775 train_time:12255ms step_avg:32.68ms
step:376/1775 train_time:12288ms step_avg:32.68ms
step:377/1775 train_time:12318ms step_avg:32.67ms
step:378/1775 train_time:12352ms step_avg:32.68ms
step:379/1775 train_time:12384ms step_avg:32.67ms
step:380/1775 train_time:12417ms step_avg:32.68ms
step:381/1775 train_time:12448ms step_avg:32.67ms
step:382/1775 train_time:12482ms step_avg:32.68ms
step:383/1775 train_time:12513ms step_avg:32.67ms
step:384/1775 train_time:12547ms step_avg:32.67ms
step:385/1775 train_time:12578ms step_avg:32.67ms
step:386/1775 train_time:12611ms step_avg:32.67ms
step:387/1775 train_time:12642ms step_avg:32.67ms
step:388/1775 train_time:12676ms step_avg:32.67ms
step:389/1775 train_time:12707ms step_avg:32.67ms
step:390/1775 train_time:12741ms step_avg:32.67ms
step:391/1775 train_time:12771ms step_avg:32.66ms
step:392/1775 train_time:12805ms step_avg:32.67ms
step:393/1775 train_time:12836ms step_avg:32.66ms
step:394/1775 train_time:12869ms step_avg:32.66ms
step:395/1775 train_time:12901ms step_avg:32.66ms
step:396/1775 train_time:12934ms step_avg:32.66ms
step:397/1775 train_time:12965ms step_avg:32.66ms
step:398/1775 train_time:12998ms step_avg:32.66ms
step:399/1775 train_time:13029ms step_avg:32.65ms
step:400/1775 train_time:13062ms step_avg:32.66ms
step:401/1775 train_time:13094ms step_avg:32.65ms
step:402/1775 train_time:13127ms step_avg:32.65ms
step:403/1775 train_time:13158ms step_avg:32.65ms
step:404/1775 train_time:13191ms step_avg:32.65ms
step:405/1775 train_time:13223ms step_avg:32.65ms
step:406/1775 train_time:13256ms step_avg:32.65ms
step:407/1775 train_time:13287ms step_avg:32.65ms
step:408/1775 train_time:13321ms step_avg:32.65ms
step:409/1775 train_time:13352ms step_avg:32.64ms
step:410/1775 train_time:13385ms step_avg:32.65ms
step:411/1775 train_time:13416ms step_avg:32.64ms
step:412/1775 train_time:13450ms step_avg:32.65ms
step:413/1775 train_time:13482ms step_avg:32.64ms
step:414/1775 train_time:13515ms step_avg:32.64ms
step:415/1775 train_time:13546ms step_avg:32.64ms
step:416/1775 train_time:13579ms step_avg:32.64ms
step:417/1775 train_time:13610ms step_avg:32.64ms
step:418/1775 train_time:13644ms step_avg:32.64ms
step:419/1775 train_time:13675ms step_avg:32.64ms
step:420/1775 train_time:13708ms step_avg:32.64ms
step:421/1775 train_time:13739ms step_avg:32.63ms
step:422/1775 train_time:13772ms step_avg:32.63ms
step:423/1775 train_time:13803ms step_avg:32.63ms
step:424/1775 train_time:13837ms step_avg:32.63ms
step:425/1775 train_time:13868ms step_avg:32.63ms
step:426/1775 train_time:13901ms step_avg:32.63ms
step:427/1775 train_time:13932ms step_avg:32.63ms
step:428/1775 train_time:13966ms step_avg:32.63ms
step:429/1775 train_time:13997ms step_avg:32.63ms
step:430/1775 train_time:14030ms step_avg:32.63ms
step:431/1775 train_time:14061ms step_avg:32.63ms
step:432/1775 train_time:14094ms step_avg:32.63ms
step:433/1775 train_time:14125ms step_avg:32.62ms
step:434/1775 train_time:14158ms step_avg:32.62ms
step:435/1775 train_time:14190ms step_avg:32.62ms
step:436/1775 train_time:14223ms step_avg:32.62ms
step:437/1775 train_time:14254ms step_avg:32.62ms
step:438/1775 train_time:14288ms step_avg:32.62ms
step:439/1775 train_time:14319ms step_avg:32.62ms
step:440/1775 train_time:14352ms step_avg:32.62ms
step:441/1775 train_time:14384ms step_avg:32.62ms
step:442/1775 train_time:14417ms step_avg:32.62ms
step:443/1775 train_time:14448ms step_avg:32.61ms
step:444/1775 train_time:14481ms step_avg:32.61ms
step:445/1775 train_time:14512ms step_avg:32.61ms
step:446/1775 train_time:14545ms step_avg:32.61ms
step:447/1775 train_time:14576ms step_avg:32.61ms
step:448/1775 train_time:14609ms step_avg:32.61ms
step:449/1775 train_time:14640ms step_avg:32.61ms
step:450/1775 train_time:14673ms step_avg:32.61ms
step:451/1775 train_time:14705ms step_avg:32.61ms
step:452/1775 train_time:14738ms step_avg:32.61ms
step:453/1775 train_time:14769ms step_avg:32.60ms
step:454/1775 train_time:14803ms step_avg:32.61ms
step:455/1775 train_time:14834ms step_avg:32.60ms
step:456/1775 train_time:14867ms step_avg:32.60ms
step:457/1775 train_time:14899ms step_avg:32.60ms
step:458/1775 train_time:14932ms step_avg:32.60ms
step:459/1775 train_time:14963ms step_avg:32.60ms
step:460/1775 train_time:14996ms step_avg:32.60ms
step:461/1775 train_time:15028ms step_avg:32.60ms
step:462/1775 train_time:15061ms step_avg:32.60ms
step:463/1775 train_time:15092ms step_avg:32.60ms
step:464/1775 train_time:15125ms step_avg:32.60ms
step:465/1775 train_time:15157ms step_avg:32.59ms
step:466/1775 train_time:15190ms step_avg:32.60ms
step:467/1775 train_time:15221ms step_avg:32.59ms
step:468/1775 train_time:15255ms step_avg:32.60ms
step:469/1775 train_time:15286ms step_avg:32.59ms
step:470/1775 train_time:15320ms step_avg:32.60ms
step:471/1775 train_time:15351ms step_avg:32.59ms
step:472/1775 train_time:15385ms step_avg:32.59ms
step:473/1775 train_time:15416ms step_avg:32.59ms
step:474/1775 train_time:15449ms step_avg:32.59ms
step:475/1775 train_time:15480ms step_avg:32.59ms
step:476/1775 train_time:15513ms step_avg:32.59ms
step:477/1775 train_time:15544ms step_avg:32.59ms
step:478/1775 train_time:15578ms step_avg:32.59ms
step:479/1775 train_time:15609ms step_avg:32.59ms
step:480/1775 train_time:15643ms step_avg:32.59ms
step:481/1775 train_time:15674ms step_avg:32.59ms
step:482/1775 train_time:15708ms step_avg:32.59ms
step:483/1775 train_time:15739ms step_avg:32.59ms
step:484/1775 train_time:15772ms step_avg:32.59ms
step:485/1775 train_time:15803ms step_avg:32.58ms
step:486/1775 train_time:15837ms step_avg:32.59ms
step:487/1775 train_time:15868ms step_avg:32.58ms
step:488/1775 train_time:15901ms step_avg:32.59ms
step:489/1775 train_time:15932ms step_avg:32.58ms
step:490/1775 train_time:15966ms step_avg:32.58ms
step:491/1775 train_time:15997ms step_avg:32.58ms
step:492/1775 train_time:16031ms step_avg:32.58ms
step:493/1775 train_time:16062ms step_avg:32.58ms
step:494/1775 train_time:16096ms step_avg:32.58ms
step:495/1775 train_time:16127ms step_avg:32.58ms
step:496/1775 train_time:16160ms step_avg:32.58ms
step:497/1775 train_time:16192ms step_avg:32.58ms
step:498/1775 train_time:16225ms step_avg:32.58ms
step:499/1775 train_time:16256ms step_avg:32.58ms
step:500/1775 train_time:16289ms step_avg:32.58ms
step:500/1775 val_loss:4.2746 train_time:16331ms step_avg:32.66ms
step:501/1775 train_time:16352ms step_avg:32.64ms
step:502/1775 train_time:16374ms step_avg:32.62ms
step:503/1775 train_time:16394ms step_avg:32.59ms
step:504/1775 train_time:16420ms step_avg:32.58ms
step:505/1775 train_time:16452ms step_avg:32.58ms
step:506/1775 train_time:16487ms step_avg:32.58ms
step:507/1775 train_time:16520ms step_avg:32.58ms
step:508/1775 train_time:16554ms step_avg:32.59ms
step:509/1775 train_time:16585ms step_avg:32.58ms
step:510/1775 train_time:16619ms step_avg:32.59ms
step:511/1775 train_time:16651ms step_avg:32.58ms
step:512/1775 train_time:16684ms step_avg:32.59ms
step:513/1775 train_time:16715ms step_avg:32.58ms
step:514/1775 train_time:16748ms step_avg:32.58ms
step:515/1775 train_time:16779ms step_avg:32.58ms
step:516/1775 train_time:16812ms step_avg:32.58ms
step:517/1775 train_time:16843ms step_avg:32.58ms
step:518/1775 train_time:16876ms step_avg:32.58ms
step:519/1775 train_time:16907ms step_avg:32.58ms
step:520/1775 train_time:16940ms step_avg:32.58ms
step:521/1775 train_time:16972ms step_avg:32.57ms
step:522/1775 train_time:17004ms step_avg:32.58ms
step:523/1775 train_time:17035ms step_avg:32.57ms
step:524/1775 train_time:17068ms step_avg:32.57ms
step:525/1775 train_time:17099ms step_avg:32.57ms
step:526/1775 train_time:17132ms step_avg:32.57ms
step:527/1775 train_time:17163ms step_avg:32.57ms
step:528/1775 train_time:17196ms step_avg:32.57ms
step:529/1775 train_time:17226ms step_avg:32.56ms
step:530/1775 train_time:17259ms step_avg:32.56ms
step:531/1775 train_time:17290ms step_avg:32.56ms
step:532/1775 train_time:17324ms step_avg:32.56ms
step:533/1775 train_time:17355ms step_avg:32.56ms
step:534/1775 train_time:17389ms step_avg:32.56ms
step:535/1775 train_time:17421ms step_avg:32.56ms
step:536/1775 train_time:17455ms step_avg:32.57ms
step:537/1775 train_time:17486ms step_avg:32.56ms
step:538/1775 train_time:17520ms step_avg:32.57ms
step:539/1775 train_time:17552ms step_avg:32.56ms
step:540/1775 train_time:17585ms step_avg:32.56ms
step:541/1775 train_time:17617ms step_avg:32.56ms
step:542/1775 train_time:17650ms step_avg:32.56ms
step:543/1775 train_time:17681ms step_avg:32.56ms
step:544/1775 train_time:17714ms step_avg:32.56ms
step:545/1775 train_time:17745ms step_avg:32.56ms
step:546/1775 train_time:17779ms step_avg:32.56ms
step:547/1775 train_time:17810ms step_avg:32.56ms
step:548/1775 train_time:17843ms step_avg:32.56ms
step:549/1775 train_time:17874ms step_avg:32.56ms
step:550/1775 train_time:17907ms step_avg:32.56ms
step:551/1775 train_time:17938ms step_avg:32.56ms
step:552/1775 train_time:17971ms step_avg:32.56ms
step:553/1775 train_time:18002ms step_avg:32.55ms
step:554/1775 train_time:18036ms step_avg:32.56ms
step:555/1775 train_time:18067ms step_avg:32.55ms
step:556/1775 train_time:18100ms step_avg:32.55ms
step:557/1775 train_time:18131ms step_avg:32.55ms
step:558/1775 train_time:18164ms step_avg:32.55ms
step:559/1775 train_time:18195ms step_avg:32.55ms
step:560/1775 train_time:18228ms step_avg:32.55ms
step:561/1775 train_time:18259ms step_avg:32.55ms
step:562/1775 train_time:18292ms step_avg:32.55ms
step:563/1775 train_time:18323ms step_avg:32.55ms
step:564/1775 train_time:18357ms step_avg:32.55ms
step:565/1775 train_time:18389ms step_avg:32.55ms
step:566/1775 train_time:18422ms step_avg:32.55ms
step:567/1775 train_time:18454ms step_avg:32.55ms
step:568/1775 train_time:18488ms step_avg:32.55ms
step:569/1775 train_time:18519ms step_avg:32.55ms
step:570/1775 train_time:18552ms step_avg:32.55ms
step:571/1775 train_time:18584ms step_avg:32.55ms
step:572/1775 train_time:18618ms step_avg:32.55ms
step:573/1775 train_time:18649ms step_avg:32.55ms
step:574/1775 train_time:18683ms step_avg:32.55ms
step:575/1775 train_time:18714ms step_avg:32.55ms
step:576/1775 train_time:18747ms step_avg:32.55ms
step:577/1775 train_time:18779ms step_avg:32.55ms
step:578/1775 train_time:18812ms step_avg:32.55ms
step:579/1775 train_time:18843ms step_avg:32.54ms
step:580/1775 train_time:18879ms step_avg:32.55ms
step:581/1775 train_time:18937ms step_avg:32.59ms
step:582/1775 train_time:18995ms step_avg:32.64ms
step:583/1775 train_time:19053ms step_avg:32.68ms
step:584/1775 train_time:19112ms step_avg:32.73ms
step:585/1775 train_time:19169ms step_avg:32.77ms
step:586/1775 train_time:19229ms step_avg:32.81ms
step:587/1775 train_time:19286ms step_avg:32.86ms
step:588/1775 train_time:19346ms step_avg:32.90ms
step:589/1775 train_time:19405ms step_avg:32.94ms
step:590/1775 train_time:19466ms step_avg:32.99ms
step:591/1775 train_time:19524ms step_avg:33.04ms
step:592/1775 train_time:19586ms step_avg:33.08ms
step:593/1775 train_time:19646ms step_avg:33.13ms
step:594/1775 train_time:19706ms step_avg:33.17ms
step:595/1775 train_time:19763ms step_avg:33.22ms
step:596/1775 train_time:19825ms step_avg:33.26ms
step:597/1775 train_time:19882ms step_avg:33.30ms
step:598/1775 train_time:19942ms step_avg:33.35ms
step:599/1775 train_time:20000ms step_avg:33.39ms
step:600/1775 train_time:20061ms step_avg:33.43ms
step:601/1775 train_time:20119ms step_avg:33.48ms
step:602/1775 train_time:20179ms step_avg:33.52ms
step:603/1775 train_time:20236ms step_avg:33.56ms
step:604/1775 train_time:20296ms step_avg:33.60ms
step:605/1775 train_time:20354ms step_avg:33.64ms
step:606/1775 train_time:20414ms step_avg:33.69ms
step:607/1775 train_time:20471ms step_avg:33.73ms
step:608/1775 train_time:20532ms step_avg:33.77ms
step:609/1775 train_time:20589ms step_avg:33.81ms
step:610/1775 train_time:20649ms step_avg:33.85ms
step:611/1775 train_time:20707ms step_avg:33.89ms
step:612/1775 train_time:20769ms step_avg:33.94ms
step:613/1775 train_time:20826ms step_avg:33.97ms
step:614/1775 train_time:20887ms step_avg:34.02ms
step:615/1775 train_time:20945ms step_avg:34.06ms
step:616/1775 train_time:21006ms step_avg:34.10ms
step:617/1775 train_time:21064ms step_avg:34.14ms
step:618/1775 train_time:21124ms step_avg:34.18ms
step:619/1775 train_time:21183ms step_avg:34.22ms
step:620/1775 train_time:21243ms step_avg:34.26ms
step:621/1775 train_time:21303ms step_avg:34.30ms
step:622/1775 train_time:21363ms step_avg:34.35ms
step:623/1775 train_time:21420ms step_avg:34.38ms
step:624/1775 train_time:21480ms step_avg:34.42ms
step:625/1775 train_time:21540ms step_avg:34.46ms
step:626/1775 train_time:21599ms step_avg:34.50ms
step:627/1775 train_time:21657ms step_avg:34.54ms
step:628/1775 train_time:21718ms step_avg:34.58ms
step:629/1775 train_time:21776ms step_avg:34.62ms
step:630/1775 train_time:21835ms step_avg:34.66ms
step:631/1775 train_time:21892ms step_avg:34.69ms
step:632/1775 train_time:21952ms step_avg:34.73ms
step:633/1775 train_time:22010ms step_avg:34.77ms
step:634/1775 train_time:22071ms step_avg:34.81ms
step:635/1775 train_time:22128ms step_avg:34.85ms
step:636/1775 train_time:22189ms step_avg:34.89ms
step:637/1775 train_time:22247ms step_avg:34.92ms
step:638/1775 train_time:22307ms step_avg:34.96ms
step:639/1775 train_time:22365ms step_avg:35.00ms
step:640/1775 train_time:22426ms step_avg:35.04ms
step:641/1775 train_time:22483ms step_avg:35.08ms
step:642/1775 train_time:22544ms step_avg:35.12ms
step:643/1775 train_time:22602ms step_avg:35.15ms
step:644/1775 train_time:22663ms step_avg:35.19ms
step:645/1775 train_time:22722ms step_avg:35.23ms
step:646/1775 train_time:22783ms step_avg:35.27ms
step:647/1775 train_time:22841ms step_avg:35.30ms
step:648/1775 train_time:22902ms step_avg:35.34ms
step:649/1775 train_time:22961ms step_avg:35.38ms
step:650/1775 train_time:23021ms step_avg:35.42ms
step:651/1775 train_time:23078ms step_avg:35.45ms
step:652/1775 train_time:23138ms step_avg:35.49ms
step:653/1775 train_time:23195ms step_avg:35.52ms
step:654/1775 train_time:23256ms step_avg:35.56ms
step:655/1775 train_time:23313ms step_avg:35.59ms
step:656/1775 train_time:23373ms step_avg:35.63ms
step:657/1775 train_time:23430ms step_avg:35.66ms
step:658/1775 train_time:23491ms step_avg:35.70ms
step:659/1775 train_time:23548ms step_avg:35.73ms
step:660/1775 train_time:23609ms step_avg:35.77ms
step:661/1775 train_time:23667ms step_avg:35.80ms
step:662/1775 train_time:23727ms step_avg:35.84ms
step:663/1775 train_time:23785ms step_avg:35.87ms
step:664/1775 train_time:23845ms step_avg:35.91ms
step:665/1775 train_time:23903ms step_avg:35.95ms
step:666/1775 train_time:23965ms step_avg:35.98ms
step:667/1775 train_time:24024ms step_avg:36.02ms
step:668/1775 train_time:24084ms step_avg:36.05ms
step:669/1775 train_time:24142ms step_avg:36.09ms
step:670/1775 train_time:24203ms step_avg:36.12ms
step:671/1775 train_time:24262ms step_avg:36.16ms
step:672/1775 train_time:24323ms step_avg:36.19ms
step:673/1775 train_time:24382ms step_avg:36.23ms
step:674/1775 train_time:24441ms step_avg:36.26ms
step:675/1775 train_time:24499ms step_avg:36.30ms
step:676/1775 train_time:24559ms step_avg:36.33ms
step:677/1775 train_time:24616ms step_avg:36.36ms
step:678/1775 train_time:24677ms step_avg:36.40ms
step:679/1775 train_time:24733ms step_avg:36.43ms
step:680/1775 train_time:24794ms step_avg:36.46ms
step:681/1775 train_time:24852ms step_avg:36.49ms
step:682/1775 train_time:24911ms step_avg:36.53ms
step:683/1775 train_time:24969ms step_avg:36.56ms
step:684/1775 train_time:25029ms step_avg:36.59ms
step:685/1775 train_time:25087ms step_avg:36.62ms
step:686/1775 train_time:25148ms step_avg:36.66ms
step:687/1775 train_time:25206ms step_avg:36.69ms
step:688/1775 train_time:25266ms step_avg:36.72ms
step:689/1775 train_time:25325ms step_avg:36.76ms
step:690/1775 train_time:25385ms step_avg:36.79ms
step:691/1775 train_time:25443ms step_avg:36.82ms
step:692/1775 train_time:25503ms step_avg:36.85ms
step:693/1775 train_time:25563ms step_avg:36.89ms
step:694/1775 train_time:25622ms step_avg:36.92ms
step:695/1775 train_time:25680ms step_avg:36.95ms
step:696/1775 train_time:25741ms step_avg:36.98ms
step:697/1775 train_time:25800ms step_avg:37.02ms
step:698/1775 train_time:25859ms step_avg:37.05ms
step:699/1775 train_time:25916ms step_avg:37.08ms
step:700/1775 train_time:25976ms step_avg:37.11ms
step:701/1775 train_time:26033ms step_avg:37.14ms
step:702/1775 train_time:26094ms step_avg:37.17ms
step:703/1775 train_time:26150ms step_avg:37.20ms
step:704/1775 train_time:26212ms step_avg:37.23ms
step:705/1775 train_time:26269ms step_avg:37.26ms
step:706/1775 train_time:26329ms step_avg:37.29ms
step:707/1775 train_time:26387ms step_avg:37.32ms
step:708/1775 train_time:26448ms step_avg:37.36ms
step:709/1775 train_time:26506ms step_avg:37.38ms
step:710/1775 train_time:26567ms step_avg:37.42ms
step:711/1775 train_time:26625ms step_avg:37.45ms
step:712/1775 train_time:26686ms step_avg:37.48ms
step:713/1775 train_time:26744ms step_avg:37.51ms
step:714/1775 train_time:26805ms step_avg:37.54ms
step:715/1775 train_time:26864ms step_avg:37.57ms
step:716/1775 train_time:26924ms step_avg:37.60ms
step:717/1775 train_time:26982ms step_avg:37.63ms
step:718/1775 train_time:27043ms step_avg:37.66ms
step:719/1775 train_time:27102ms step_avg:37.69ms
step:720/1775 train_time:27163ms step_avg:37.73ms
step:721/1775 train_time:27221ms step_avg:37.75ms
step:722/1775 train_time:27280ms step_avg:37.78ms
step:723/1775 train_time:27338ms step_avg:37.81ms
step:724/1775 train_time:27397ms step_avg:37.84ms
step:725/1775 train_time:27455ms step_avg:37.87ms
step:726/1775 train_time:27515ms step_avg:37.90ms
step:727/1775 train_time:27572ms step_avg:37.93ms
step:728/1775 train_time:27632ms step_avg:37.96ms
step:729/1775 train_time:27689ms step_avg:37.98ms
step:730/1775 train_time:27750ms step_avg:38.01ms
step:731/1775 train_time:27808ms step_avg:38.04ms
step:732/1775 train_time:27869ms step_avg:38.07ms
step:733/1775 train_time:27926ms step_avg:38.10ms
step:734/1775 train_time:27988ms step_avg:38.13ms
step:735/1775 train_time:28046ms step_avg:38.16ms
step:736/1775 train_time:28105ms step_avg:38.19ms
step:737/1775 train_time:28164ms step_avg:38.21ms
step:738/1775 train_time:28225ms step_avg:38.24ms
step:739/1775 train_time:28282ms step_avg:38.27ms
step:740/1775 train_time:28343ms step_avg:38.30ms
step:741/1775 train_time:28402ms step_avg:38.33ms
step:742/1775 train_time:28462ms step_avg:38.36ms
step:743/1775 train_time:28521ms step_avg:38.39ms
step:744/1775 train_time:28581ms step_avg:38.41ms
step:745/1775 train_time:28639ms step_avg:38.44ms
step:746/1775 train_time:28701ms step_avg:38.47ms
step:747/1775 train_time:28758ms step_avg:38.50ms
step:748/1775 train_time:28818ms step_avg:38.53ms
step:749/1775 train_time:28876ms step_avg:38.55ms
step:750/1775 train_time:28936ms step_avg:38.58ms
step:750/1775 val_loss:3.9980 train_time:29006ms step_avg:38.67ms
step:751/1775 train_time:29029ms step_avg:38.65ms
step:752/1775 train_time:29054ms step_avg:38.64ms
step:753/1775 train_time:29112ms step_avg:38.66ms
step:754/1775 train_time:29177ms step_avg:38.70ms
step:755/1775 train_time:29241ms step_avg:38.73ms
step:756/1775 train_time:29301ms step_avg:38.76ms
step:757/1775 train_time:29358ms step_avg:38.78ms
step:758/1775 train_time:29419ms step_avg:38.81ms
step:759/1775 train_time:29476ms step_avg:38.83ms
step:760/1775 train_time:29535ms step_avg:38.86ms
step:761/1775 train_time:29592ms step_avg:38.89ms
step:762/1775 train_time:29651ms step_avg:38.91ms
step:763/1775 train_time:29708ms step_avg:38.94ms
step:764/1775 train_time:29767ms step_avg:38.96ms
step:765/1775 train_time:29824ms step_avg:38.99ms
step:766/1775 train_time:29884ms step_avg:39.01ms
step:767/1775 train_time:29942ms step_avg:39.04ms
step:768/1775 train_time:30003ms step_avg:39.07ms
step:769/1775 train_time:30061ms step_avg:39.09ms
step:770/1775 train_time:30123ms step_avg:39.12ms
step:771/1775 train_time:30182ms step_avg:39.15ms
step:772/1775 train_time:30243ms step_avg:39.18ms
step:773/1775 train_time:30301ms step_avg:39.20ms
step:774/1775 train_time:30361ms step_avg:39.23ms
step:775/1775 train_time:30418ms step_avg:39.25ms
step:776/1775 train_time:30478ms step_avg:39.28ms
step:777/1775 train_time:30535ms step_avg:39.30ms
step:778/1775 train_time:30595ms step_avg:39.33ms
step:779/1775 train_time:30653ms step_avg:39.35ms
step:780/1775 train_time:30712ms step_avg:39.37ms
step:781/1775 train_time:30769ms step_avg:39.40ms
step:782/1775 train_time:30829ms step_avg:39.42ms
step:783/1775 train_time:30888ms step_avg:39.45ms
step:784/1775 train_time:30949ms step_avg:39.48ms
step:785/1775 train_time:31008ms step_avg:39.50ms
step:786/1775 train_time:31069ms step_avg:39.53ms
step:787/1775 train_time:31129ms step_avg:39.55ms
step:788/1775 train_time:31191ms step_avg:39.58ms
step:789/1775 train_time:31250ms step_avg:39.61ms
step:790/1775 train_time:31311ms step_avg:39.63ms
step:791/1775 train_time:31370ms step_avg:39.66ms
step:792/1775 train_time:31430ms step_avg:39.68ms
step:793/1775 train_time:31489ms step_avg:39.71ms
step:794/1775 train_time:31548ms step_avg:39.73ms
step:795/1775 train_time:31605ms step_avg:39.76ms
step:796/1775 train_time:31664ms step_avg:39.78ms
step:797/1775 train_time:31721ms step_avg:39.80ms
step:798/1775 train_time:31781ms step_avg:39.83ms
step:799/1775 train_time:31837ms step_avg:39.85ms
step:800/1775 train_time:31897ms step_avg:39.87ms
step:801/1775 train_time:31954ms step_avg:39.89ms
step:802/1775 train_time:32015ms step_avg:39.92ms
step:803/1775 train_time:32073ms step_avg:39.94ms
step:804/1775 train_time:32136ms step_avg:39.97ms
step:805/1775 train_time:32193ms step_avg:39.99ms
step:806/1775 train_time:32255ms step_avg:40.02ms
step:807/1775 train_time:32312ms step_avg:40.04ms
step:808/1775 train_time:32373ms step_avg:40.07ms
step:809/1775 train_time:32431ms step_avg:40.09ms
step:810/1775 train_time:32492ms step_avg:40.11ms
step:811/1775 train_time:32551ms step_avg:40.14ms
step:812/1775 train_time:32611ms step_avg:40.16ms
step:813/1775 train_time:32669ms step_avg:40.18ms
step:814/1775 train_time:32730ms step_avg:40.21ms
step:815/1775 train_time:32789ms step_avg:40.23ms
step:816/1775 train_time:32848ms step_avg:40.26ms
step:817/1775 train_time:32906ms step_avg:40.28ms
step:818/1775 train_time:32965ms step_avg:40.30ms
step:819/1775 train_time:33023ms step_avg:40.32ms
step:820/1775 train_time:33084ms step_avg:40.35ms
step:821/1775 train_time:33141ms step_avg:40.37ms
step:822/1775 train_time:33202ms step_avg:40.39ms
step:823/1775 train_time:33259ms step_avg:40.41ms
step:824/1775 train_time:33319ms step_avg:40.44ms
step:825/1775 train_time:33377ms step_avg:40.46ms
step:826/1775 train_time:33437ms step_avg:40.48ms
step:827/1775 train_time:33495ms step_avg:40.50ms
step:828/1775 train_time:33555ms step_avg:40.53ms
step:829/1775 train_time:33613ms step_avg:40.55ms
step:830/1775 train_time:33672ms step_avg:40.57ms
step:831/1775 train_time:33731ms step_avg:40.59ms
step:832/1775 train_time:33791ms step_avg:40.61ms
step:833/1775 train_time:33849ms step_avg:40.64ms
step:834/1775 train_time:33910ms step_avg:40.66ms
step:835/1775 train_time:33969ms step_avg:40.68ms
step:836/1775 train_time:34029ms step_avg:40.70ms
step:837/1775 train_time:34088ms step_avg:40.73ms
step:838/1775 train_time:34149ms step_avg:40.75ms
step:839/1775 train_time:34207ms step_avg:40.77ms
step:840/1775 train_time:34268ms step_avg:40.80ms
step:841/1775 train_time:34326ms step_avg:40.82ms
step:842/1775 train_time:34387ms step_avg:40.84ms
step:843/1775 train_time:34445ms step_avg:40.86ms
step:844/1775 train_time:34505ms step_avg:40.88ms
step:845/1775 train_time:34562ms step_avg:40.90ms
step:846/1775 train_time:34623ms step_avg:40.93ms
step:847/1775 train_time:34681ms step_avg:40.95ms
step:848/1775 train_time:34741ms step_avg:40.97ms
step:849/1775 train_time:34799ms step_avg:40.99ms
step:850/1775 train_time:34859ms step_avg:41.01ms
step:851/1775 train_time:34916ms step_avg:41.03ms
step:852/1775 train_time:34977ms step_avg:41.05ms
step:853/1775 train_time:35034ms step_avg:41.07ms
step:854/1775 train_time:35095ms step_avg:41.10ms
step:855/1775 train_time:35153ms step_avg:41.12ms
step:856/1775 train_time:35214ms step_avg:41.14ms
step:857/1775 train_time:35272ms step_avg:41.16ms
step:858/1775 train_time:35333ms step_avg:41.18ms
step:859/1775 train_time:35391ms step_avg:41.20ms
step:860/1775 train_time:35452ms step_avg:41.22ms
step:861/1775 train_time:35511ms step_avg:41.24ms
step:862/1775 train_time:35571ms step_avg:41.27ms
step:863/1775 train_time:35630ms step_avg:41.29ms
step:864/1775 train_time:35692ms step_avg:41.31ms
step:865/1775 train_time:35750ms step_avg:41.33ms
step:866/1775 train_time:35809ms step_avg:41.35ms
step:867/1775 train_time:35867ms step_avg:41.37ms
step:868/1775 train_time:35927ms step_avg:41.39ms
step:869/1775 train_time:35985ms step_avg:41.41ms
step:870/1775 train_time:36045ms step_avg:41.43ms
step:871/1775 train_time:36102ms step_avg:41.45ms
step:872/1775 train_time:36162ms step_avg:41.47ms
step:873/1775 train_time:36220ms step_avg:41.49ms
step:874/1775 train_time:36280ms step_avg:41.51ms
step:875/1775 train_time:36338ms step_avg:41.53ms
step:876/1775 train_time:36399ms step_avg:41.55ms
step:877/1775 train_time:36456ms step_avg:41.57ms
step:878/1775 train_time:36516ms step_avg:41.59ms
step:879/1775 train_time:36573ms step_avg:41.61ms
step:880/1775 train_time:36634ms step_avg:41.63ms
step:881/1775 train_time:36693ms step_avg:41.65ms
step:882/1775 train_time:36753ms step_avg:41.67ms
step:883/1775 train_time:36811ms step_avg:41.69ms
step:884/1775 train_time:36871ms step_avg:41.71ms
step:885/1775 train_time:36930ms step_avg:41.73ms
step:886/1775 train_time:36991ms step_avg:41.75ms
step:887/1775 train_time:37049ms step_avg:41.77ms
step:888/1775 train_time:37110ms step_avg:41.79ms
step:889/1775 train_time:37168ms step_avg:41.81ms
step:890/1775 train_time:37228ms step_avg:41.83ms
step:891/1775 train_time:37285ms step_avg:41.85ms
step:892/1775 train_time:37347ms step_avg:41.87ms
step:893/1775 train_time:37405ms step_avg:41.89ms
step:894/1775 train_time:37466ms step_avg:41.91ms
step:895/1775 train_time:37523ms step_avg:41.93ms
step:896/1775 train_time:37585ms step_avg:41.95ms
step:897/1775 train_time:37642ms step_avg:41.96ms
step:898/1775 train_time:37702ms step_avg:41.98ms
step:899/1775 train_time:37759ms step_avg:42.00ms
step:900/1775 train_time:37821ms step_avg:42.02ms
step:901/1775 train_time:37878ms step_avg:42.04ms
step:902/1775 train_time:37938ms step_avg:42.06ms
step:903/1775 train_time:37996ms step_avg:42.08ms
step:904/1775 train_time:38056ms step_avg:42.10ms
step:905/1775 train_time:38113ms step_avg:42.11ms
step:906/1775 train_time:38173ms step_avg:42.13ms
step:907/1775 train_time:38233ms step_avg:42.15ms
step:908/1775 train_time:38293ms step_avg:42.17ms
step:909/1775 train_time:38351ms step_avg:42.19ms
step:910/1775 train_time:38412ms step_avg:42.21ms
step:911/1775 train_time:38470ms step_avg:42.23ms
step:912/1775 train_time:38531ms step_avg:42.25ms
step:913/1775 train_time:38590ms step_avg:42.27ms
step:914/1775 train_time:38650ms step_avg:42.29ms
step:915/1775 train_time:38709ms step_avg:42.30ms
step:916/1775 train_time:38769ms step_avg:42.32ms
step:917/1775 train_time:38828ms step_avg:42.34ms
step:918/1775 train_time:38889ms step_avg:42.36ms
step:919/1775 train_time:38947ms step_avg:42.38ms
step:920/1775 train_time:39008ms step_avg:42.40ms
step:921/1775 train_time:39065ms step_avg:42.42ms
step:922/1775 train_time:39125ms step_avg:42.43ms
step:923/1775 train_time:39183ms step_avg:42.45ms
step:924/1775 train_time:39244ms step_avg:42.47ms
step:925/1775 train_time:39301ms step_avg:42.49ms
step:926/1775 train_time:39361ms step_avg:42.51ms
step:927/1775 train_time:39419ms step_avg:42.52ms
step:928/1775 train_time:39479ms step_avg:42.54ms
step:929/1775 train_time:39536ms step_avg:42.56ms
step:930/1775 train_time:39596ms step_avg:42.58ms
step:931/1775 train_time:39654ms step_avg:42.59ms
step:932/1775 train_time:39714ms step_avg:42.61ms
step:933/1775 train_time:39773ms step_avg:42.63ms
step:934/1775 train_time:39832ms step_avg:42.65ms
step:935/1775 train_time:39891ms step_avg:42.66ms
step:936/1775 train_time:39951ms step_avg:42.68ms
step:937/1775 train_time:40009ms step_avg:42.70ms
step:938/1775 train_time:40070ms step_avg:42.72ms
step:939/1775 train_time:40128ms step_avg:42.73ms
step:940/1775 train_time:40189ms step_avg:42.75ms
step:941/1775 train_time:40248ms step_avg:42.77ms
step:942/1775 train_time:40308ms step_avg:42.79ms
step:943/1775 train_time:40366ms step_avg:42.81ms
step:944/1775 train_time:40427ms step_avg:42.83ms
step:945/1775 train_time:40485ms step_avg:42.84ms
step:946/1775 train_time:40546ms step_avg:42.86ms
step:947/1775 train_time:40603ms step_avg:42.88ms
step:948/1775 train_time:40663ms step_avg:42.89ms
step:949/1775 train_time:40720ms step_avg:42.91ms
step:950/1775 train_time:40781ms step_avg:42.93ms
step:951/1775 train_time:40839ms step_avg:42.94ms
step:952/1775 train_time:40899ms step_avg:42.96ms
step:953/1775 train_time:40956ms step_avg:42.98ms
step:954/1775 train_time:41016ms step_avg:42.99ms
step:955/1775 train_time:41073ms step_avg:43.01ms
step:956/1775 train_time:41135ms step_avg:43.03ms
step:957/1775 train_time:41193ms step_avg:43.04ms
step:958/1775 train_time:41253ms step_avg:43.06ms
step:959/1775 train_time:41312ms step_avg:43.08ms
step:960/1775 train_time:41372ms step_avg:43.10ms
step:961/1775 train_time:41431ms step_avg:43.11ms
step:962/1775 train_time:41491ms step_avg:43.13ms
step:963/1775 train_time:41550ms step_avg:43.15ms
step:964/1775 train_time:41609ms step_avg:43.16ms
step:965/1775 train_time:41668ms step_avg:43.18ms
step:966/1775 train_time:41728ms step_avg:43.20ms
step:967/1775 train_time:41787ms step_avg:43.21ms
step:968/1775 train_time:41848ms step_avg:43.23ms
step:969/1775 train_time:41905ms step_avg:43.25ms
step:970/1775 train_time:41964ms step_avg:43.26ms
step:971/1775 train_time:42022ms step_avg:43.28ms
step:972/1775 train_time:42083ms step_avg:43.29ms
step:973/1775 train_time:42139ms step_avg:43.31ms
step:974/1775 train_time:42200ms step_avg:43.33ms
step:975/1775 train_time:42257ms step_avg:43.34ms
step:976/1775 train_time:42317ms step_avg:43.36ms
step:977/1775 train_time:42374ms step_avg:43.37ms
step:978/1775 train_time:42435ms step_avg:43.39ms
step:979/1775 train_time:42493ms step_avg:43.40ms
step:980/1775 train_time:42553ms step_avg:43.42ms
step:981/1775 train_time:42612ms step_avg:43.44ms
step:982/1775 train_time:42671ms step_avg:43.45ms
step:983/1775 train_time:42729ms step_avg:43.47ms
step:984/1775 train_time:42791ms step_avg:43.49ms
step:985/1775 train_time:42849ms step_avg:43.50ms
step:986/1775 train_time:42910ms step_avg:43.52ms
step:987/1775 train_time:42968ms step_avg:43.53ms
step:988/1775 train_time:43028ms step_avg:43.55ms
step:989/1775 train_time:43085ms step_avg:43.56ms
step:990/1775 train_time:43144ms step_avg:43.58ms
step:991/1775 train_time:43202ms step_avg:43.59ms
step:992/1775 train_time:43263ms step_avg:43.61ms
step:993/1775 train_time:43320ms step_avg:43.63ms
step:994/1775 train_time:43380ms step_avg:43.64ms
step:995/1775 train_time:43438ms step_avg:43.66ms
step:996/1775 train_time:43499ms step_avg:43.67ms
step:997/1775 train_time:43556ms step_avg:43.69ms
step:998/1775 train_time:43616ms step_avg:43.70ms
step:999/1775 train_time:43675ms step_avg:43.72ms
step:1000/1775 train_time:43735ms step_avg:43.73ms
step:1000/1775 val_loss:3.7390 train_time:43807ms step_avg:43.81ms
step:1001/1775 train_time:43829ms step_avg:43.79ms
step:1002/1775 train_time:43856ms step_avg:43.77ms
step:1003/1775 train_time:43914ms step_avg:43.78ms
step:1004/1775 train_time:43977ms step_avg:43.80ms
step:1005/1775 train_time:44038ms step_avg:43.82ms
step:1006/1775 train_time:44099ms step_avg:43.84ms
step:1007/1775 train_time:44157ms step_avg:43.85ms
step:1008/1775 train_time:44216ms step_avg:43.87ms
step:1009/1775 train_time:44273ms step_avg:43.88ms
step:1010/1775 train_time:44333ms step_avg:43.89ms
step:1011/1775 train_time:44389ms step_avg:43.91ms
step:1012/1775 train_time:44449ms step_avg:43.92ms
step:1013/1775 train_time:44506ms step_avg:43.93ms
step:1014/1775 train_time:44565ms step_avg:43.95ms
step:1015/1775 train_time:44623ms step_avg:43.96ms
step:1016/1775 train_time:44683ms step_avg:43.98ms
step:1017/1775 train_time:44741ms step_avg:43.99ms
step:1018/1775 train_time:44801ms step_avg:44.01ms
step:1019/1775 train_time:44861ms step_avg:44.02ms
step:1020/1775 train_time:44922ms step_avg:44.04ms
step:1021/1775 train_time:44980ms step_avg:44.05ms
step:1022/1775 train_time:45041ms step_avg:44.07ms
step:1023/1775 train_time:45099ms step_avg:44.08ms
step:1024/1775 train_time:45159ms step_avg:44.10ms
step:1025/1775 train_time:45218ms step_avg:44.11ms
step:1026/1775 train_time:45278ms step_avg:44.13ms
step:1027/1775 train_time:45337ms step_avg:44.14ms
step:1028/1775 train_time:45397ms step_avg:44.16ms
step:1029/1775 train_time:45454ms step_avg:44.17ms
step:1030/1775 train_time:45512ms step_avg:44.19ms
step:1031/1775 train_time:45569ms step_avg:44.20ms
step:1032/1775 train_time:45628ms step_avg:44.21ms
step:1033/1775 train_time:45685ms step_avg:44.23ms
step:1034/1775 train_time:45744ms step_avg:44.24ms
step:1035/1775 train_time:45802ms step_avg:44.25ms
step:1036/1775 train_time:45863ms step_avg:44.27ms
step:1037/1775 train_time:45922ms step_avg:44.28ms
step:1038/1775 train_time:45982ms step_avg:44.30ms
step:1039/1775 train_time:46041ms step_avg:44.31ms
step:1040/1775 train_time:46100ms step_avg:44.33ms
step:1041/1775 train_time:46159ms step_avg:44.34ms
step:1042/1775 train_time:46219ms step_avg:44.36ms
step:1043/1775 train_time:46276ms step_avg:44.37ms
step:1044/1775 train_time:46337ms step_avg:44.38ms
step:1045/1775 train_time:46394ms step_avg:44.40ms
step:1046/1775 train_time:46454ms step_avg:44.41ms
step:1047/1775 train_time:46511ms step_avg:44.42ms
step:1048/1775 train_time:46571ms step_avg:44.44ms
step:1049/1775 train_time:46628ms step_avg:44.45ms
step:1050/1775 train_time:46687ms step_avg:44.46ms
step:1051/1775 train_time:46744ms step_avg:44.48ms
step:1052/1775 train_time:46804ms step_avg:44.49ms
step:1053/1775 train_time:46861ms step_avg:44.50ms
step:1054/1775 train_time:46921ms step_avg:44.52ms
step:1055/1775 train_time:46979ms step_avg:44.53ms
step:1056/1775 train_time:47040ms step_avg:44.54ms
step:1057/1775 train_time:47097ms step_avg:44.56ms
step:1058/1775 train_time:47158ms step_avg:44.57ms
step:1059/1775 train_time:47215ms step_avg:44.58ms
step:1060/1775 train_time:47275ms step_avg:44.60ms
step:1061/1775 train_time:47332ms step_avg:44.61ms
step:1062/1775 train_time:47392ms step_avg:44.62ms
step:1063/1775 train_time:47450ms step_avg:44.64ms
step:1064/1775 train_time:47509ms step_avg:44.65ms
step:1065/1775 train_time:47565ms step_avg:44.66ms
step:1066/1775 train_time:47625ms step_avg:44.68ms
step:1067/1775 train_time:47683ms step_avg:44.69ms
step:1068/1775 train_time:47742ms step_avg:44.70ms
step:1069/1775 train_time:47801ms step_avg:44.72ms
step:1070/1775 train_time:47860ms step_avg:44.73ms
step:1071/1775 train_time:47918ms step_avg:44.74ms
step:1072/1775 train_time:47977ms step_avg:44.76ms
step:1073/1775 train_time:48034ms step_avg:44.77ms
step:1074/1775 train_time:48094ms step_avg:44.78ms
step:1075/1775 train_time:48152ms step_avg:44.79ms
step:1076/1775 train_time:48212ms step_avg:44.81ms
step:1077/1775 train_time:48270ms step_avg:44.82ms
step:1078/1775 train_time:48330ms step_avg:44.83ms
step:1079/1775 train_time:48387ms step_avg:44.84ms
step:1080/1775 train_time:48446ms step_avg:44.86ms
step:1081/1775 train_time:48504ms step_avg:44.87ms
step:1082/1775 train_time:48564ms step_avg:44.88ms
step:1083/1775 train_time:48621ms step_avg:44.90ms
step:1084/1775 train_time:48682ms step_avg:44.91ms
step:1085/1775 train_time:48740ms step_avg:44.92ms
step:1086/1775 train_time:48799ms step_avg:44.93ms
step:1087/1775 train_time:48857ms step_avg:44.95ms
step:1088/1775 train_time:48917ms step_avg:44.96ms
step:1089/1775 train_time:48975ms step_avg:44.97ms
step:1090/1775 train_time:49035ms step_avg:44.99ms
step:1091/1775 train_time:49092ms step_avg:45.00ms
step:1092/1775 train_time:49152ms step_avg:45.01ms
step:1093/1775 train_time:49209ms step_avg:45.02ms
step:1094/1775 train_time:49270ms step_avg:45.04ms
step:1095/1775 train_time:49327ms step_avg:45.05ms
step:1096/1775 train_time:49386ms step_avg:45.06ms
step:1097/1775 train_time:49444ms step_avg:45.07ms
step:1098/1775 train_time:49505ms step_avg:45.09ms
step:1099/1775 train_time:49562ms step_avg:45.10ms
step:1100/1775 train_time:49622ms step_avg:45.11ms
step:1101/1775 train_time:49679ms step_avg:45.12ms
step:1102/1775 train_time:49738ms step_avg:45.13ms
step:1103/1775 train_time:49796ms step_avg:45.15ms
step:1104/1775 train_time:49856ms step_avg:45.16ms
step:1105/1775 train_time:49912ms step_avg:45.17ms
step:1106/1775 train_time:49973ms step_avg:45.18ms
step:1107/1775 train_time:50030ms step_avg:45.19ms
step:1108/1775 train_time:50090ms step_avg:45.21ms
step:1109/1775 train_time:50147ms step_avg:45.22ms
step:1110/1775 train_time:50207ms step_avg:45.23ms
step:1111/1775 train_time:50265ms step_avg:45.24ms
step:1112/1775 train_time:50325ms step_avg:45.26ms
step:1113/1775 train_time:50382ms step_avg:45.27ms
step:1114/1775 train_time:50442ms step_avg:45.28ms
step:1115/1775 train_time:50500ms step_avg:45.29ms
step:1116/1775 train_time:50560ms step_avg:45.30ms
step:1117/1775 train_time:50617ms step_avg:45.32ms
step:1118/1775 train_time:50676ms step_avg:45.33ms
step:1119/1775 train_time:50734ms step_avg:45.34ms
step:1120/1775 train_time:50794ms step_avg:45.35ms
step:1121/1775 train_time:50850ms step_avg:45.36ms
step:1122/1775 train_time:50910ms step_avg:45.37ms
step:1123/1775 train_time:50968ms step_avg:45.39ms
step:1124/1775 train_time:51028ms step_avg:45.40ms
step:1125/1775 train_time:51086ms step_avg:45.41ms
step:1126/1775 train_time:51146ms step_avg:45.42ms
step:1127/1775 train_time:51204ms step_avg:45.43ms
step:1128/1775 train_time:51264ms step_avg:45.45ms
step:1129/1775 train_time:51322ms step_avg:45.46ms
step:1130/1775 train_time:51381ms step_avg:45.47ms
step:1131/1775 train_time:51439ms step_avg:45.48ms
step:1132/1775 train_time:51500ms step_avg:45.49ms
step:1133/1775 train_time:51557ms step_avg:45.51ms
step:1134/1775 train_time:51618ms step_avg:45.52ms
step:1135/1775 train_time:51675ms step_avg:45.53ms
step:1136/1775 train_time:51735ms step_avg:45.54ms
step:1137/1775 train_time:51792ms step_avg:45.55ms
step:1138/1775 train_time:51852ms step_avg:45.56ms
step:1139/1775 train_time:51909ms step_avg:45.57ms
step:1140/1775 train_time:51969ms step_avg:45.59ms
step:1141/1775 train_time:52027ms step_avg:45.60ms
step:1142/1775 train_time:52087ms step_avg:45.61ms
step:1143/1775 train_time:52144ms step_avg:45.62ms
step:1144/1775 train_time:52204ms step_avg:45.63ms
step:1145/1775 train_time:52262ms step_avg:45.64ms
step:1146/1775 train_time:52323ms step_avg:45.66ms
step:1147/1775 train_time:52380ms step_avg:45.67ms
step:1148/1775 train_time:52440ms step_avg:45.68ms
step:1149/1775 train_time:52498ms step_avg:45.69ms
step:1150/1775 train_time:52558ms step_avg:45.70ms
step:1151/1775 train_time:52616ms step_avg:45.71ms
step:1152/1775 train_time:52675ms step_avg:45.72ms
step:1153/1775 train_time:52732ms step_avg:45.73ms
step:1154/1775 train_time:52791ms step_avg:45.75ms
step:1155/1775 train_time:52848ms step_avg:45.76ms
step:1156/1775 train_time:52908ms step_avg:45.77ms
step:1157/1775 train_time:52966ms step_avg:45.78ms
step:1158/1775 train_time:53028ms step_avg:45.79ms
step:1159/1775 train_time:53111ms step_avg:45.83ms
step:1160/1775 train_time:53196ms step_avg:45.86ms
step:1161/1775 train_time:53279ms step_avg:45.89ms
step:1162/1775 train_time:53365ms step_avg:45.93ms
step:1163/1775 train_time:53448ms step_avg:45.96ms
step:1164/1775 train_time:53534ms step_avg:45.99ms
step:1165/1775 train_time:53619ms step_avg:46.02ms
step:1166/1775 train_time:53705ms step_avg:46.06ms
step:1167/1775 train_time:53790ms step_avg:46.09ms
step:1168/1775 train_time:53875ms step_avg:46.13ms
step:1169/1775 train_time:53959ms step_avg:46.16ms
step:1170/1775 train_time:54045ms step_avg:46.19ms
step:1171/1775 train_time:54127ms step_avg:46.22ms
step:1172/1775 train_time:54214ms step_avg:46.26ms
step:1173/1775 train_time:54297ms step_avg:46.29ms
step:1174/1775 train_time:54383ms step_avg:46.32ms
step:1175/1775 train_time:54467ms step_avg:46.35ms
step:1176/1775 train_time:54552ms step_avg:46.39ms
step:1177/1775 train_time:54635ms step_avg:46.42ms
step:1178/1775 train_time:54722ms step_avg:46.45ms
step:1179/1775 train_time:54806ms step_avg:46.48ms
step:1180/1775 train_time:54893ms step_avg:46.52ms
step:1181/1775 train_time:54975ms step_avg:46.55ms
step:1182/1775 train_time:55060ms step_avg:46.58ms
step:1183/1775 train_time:55144ms step_avg:46.61ms
step:1184/1775 train_time:55230ms step_avg:46.65ms
step:1185/1775 train_time:55314ms step_avg:46.68ms
step:1186/1775 train_time:55399ms step_avg:46.71ms
step:1187/1775 train_time:55481ms step_avg:46.74ms
step:1188/1775 train_time:55567ms step_avg:46.77ms
step:1189/1775 train_time:55651ms step_avg:46.80ms
step:1190/1775 train_time:55736ms step_avg:46.84ms
step:1191/1775 train_time:55820ms step_avg:46.87ms
step:1192/1775 train_time:55905ms step_avg:46.90ms
step:1193/1775 train_time:55990ms step_avg:46.93ms
step:1194/1775 train_time:56076ms step_avg:46.96ms
step:1195/1775 train_time:56159ms step_avg:47.00ms
step:1196/1775 train_time:56244ms step_avg:47.03ms
step:1197/1775 train_time:56327ms step_avg:47.06ms
step:1198/1775 train_time:56413ms step_avg:47.09ms
step:1199/1775 train_time:56496ms step_avg:47.12ms
step:1200/1775 train_time:56583ms step_avg:47.15ms
step:1201/1775 train_time:56667ms step_avg:47.18ms
step:1202/1775 train_time:56753ms step_avg:47.22ms
step:1203/1775 train_time:56836ms step_avg:47.24ms
step:1204/1775 train_time:56922ms step_avg:47.28ms
step:1205/1775 train_time:57005ms step_avg:47.31ms
step:1206/1775 train_time:57093ms step_avg:47.34ms
step:1207/1775 train_time:57176ms step_avg:47.37ms
step:1208/1775 train_time:57261ms step_avg:47.40ms
step:1209/1775 train_time:57344ms step_avg:47.43ms
step:1210/1775 train_time:57429ms step_avg:47.46ms
step:1211/1775 train_time:57513ms step_avg:47.49ms
step:1212/1775 train_time:57599ms step_avg:47.52ms
step:1213/1775 train_time:57682ms step_avg:47.55ms
step:1214/1775 train_time:57767ms step_avg:47.58ms
step:1215/1775 train_time:57851ms step_avg:47.61ms
step:1216/1775 train_time:57937ms step_avg:47.65ms
step:1217/1775 train_time:58021ms step_avg:47.68ms
step:1218/1775 train_time:58108ms step_avg:47.71ms
step:1219/1775 train_time:58192ms step_avg:47.74ms
step:1220/1775 train_time:58277ms step_avg:47.77ms
step:1221/1775 train_time:58360ms step_avg:47.80ms
step:1222/1775 train_time:58446ms step_avg:47.83ms
step:1223/1775 train_time:58528ms step_avg:47.86ms
step:1224/1775 train_time:58614ms step_avg:47.89ms
step:1225/1775 train_time:58698ms step_avg:47.92ms
step:1226/1775 train_time:58783ms step_avg:47.95ms
step:1227/1775 train_time:58867ms step_avg:47.98ms
step:1228/1775 train_time:58953ms step_avg:48.01ms
step:1229/1775 train_time:59036ms step_avg:48.04ms
step:1230/1775 train_time:59123ms step_avg:48.07ms
step:1231/1775 train_time:59206ms step_avg:48.10ms
step:1232/1775 train_time:59292ms step_avg:48.13ms
step:1233/1775 train_time:59375ms step_avg:48.15ms
step:1234/1775 train_time:59461ms step_avg:48.19ms
step:1235/1775 train_time:59544ms step_avg:48.21ms
step:1236/1775 train_time:59630ms step_avg:48.24ms
step:1237/1775 train_time:59712ms step_avg:48.27ms
step:1238/1775 train_time:59797ms step_avg:48.30ms
step:1239/1775 train_time:59881ms step_avg:48.33ms
step:1240/1775 train_time:59967ms step_avg:48.36ms
step:1241/1775 train_time:60052ms step_avg:48.39ms
step:1242/1775 train_time:60137ms step_avg:48.42ms
step:1243/1775 train_time:60220ms step_avg:48.45ms
step:1244/1775 train_time:60305ms step_avg:48.48ms
step:1245/1775 train_time:60390ms step_avg:48.51ms
step:1246/1775 train_time:60476ms step_avg:48.54ms
step:1247/1775 train_time:60558ms step_avg:48.56ms
step:1248/1775 train_time:60645ms step_avg:48.59ms
step:1249/1775 train_time:60727ms step_avg:48.62ms
step:1250/1775 train_time:60813ms step_avg:48.65ms
step:1250/1775 val_loss:3.5025 train_time:60912ms step_avg:48.73ms
step:1251/1775 train_time:60935ms step_avg:48.71ms
step:1252/1775 train_time:60984ms step_avg:48.71ms
step:1253/1775 train_time:61072ms step_avg:48.74ms
step:1254/1775 train_time:61161ms step_avg:48.77ms
step:1255/1775 train_time:61245ms step_avg:48.80ms
step:1256/1775 train_time:61330ms step_avg:48.83ms
step:1257/1775 train_time:61412ms step_avg:48.86ms
step:1258/1775 train_time:61498ms step_avg:48.89ms
step:1259/1775 train_time:61579ms step_avg:48.91ms
step:1260/1775 train_time:61663ms step_avg:48.94ms
step:1261/1775 train_time:61745ms step_avg:48.97ms
step:1262/1775 train_time:61831ms step_avg:48.99ms
step:1263/1775 train_time:61916ms step_avg:49.02ms
step:1264/1775 train_time:62003ms step_avg:49.05ms
step:1265/1775 train_time:62089ms step_avg:49.08ms
step:1266/1775 train_time:62178ms step_avg:49.11ms
step:1267/1775 train_time:62259ms step_avg:49.14ms
step:1268/1775 train_time:62346ms step_avg:49.17ms
step:1269/1775 train_time:62429ms step_avg:49.20ms
step:1270/1775 train_time:62513ms step_avg:49.22ms
step:1271/1775 train_time:62595ms step_avg:49.25ms
step:1272/1775 train_time:62679ms step_avg:49.28ms
step:1273/1775 train_time:62762ms step_avg:49.30ms
step:1274/1775 train_time:62849ms step_avg:49.33ms
step:1275/1775 train_time:62933ms step_avg:49.36ms
step:1276/1775 train_time:63021ms step_avg:49.39ms
step:1277/1775 train_time:63106ms step_avg:49.42ms
step:1278/1775 train_time:63192ms step_avg:49.45ms
step:1279/1775 train_time:63276ms step_avg:49.47ms
step:1280/1775 train_time:63360ms step_avg:49.50ms
step:1281/1775 train_time:63443ms step_avg:49.53ms
step:1282/1775 train_time:63528ms step_avg:49.55ms
step:1283/1775 train_time:63610ms step_avg:49.58ms
step:1284/1775 train_time:63695ms step_avg:49.61ms
step:1285/1775 train_time:63779ms step_avg:49.63ms
step:1286/1775 train_time:63865ms step_avg:49.66ms
step:1287/1775 train_time:63950ms step_avg:49.69ms
step:1288/1775 train_time:64037ms step_avg:49.72ms
step:1289/1775 train_time:64121ms step_avg:49.74ms
step:1290/1775 train_time:64209ms step_avg:49.77ms
step:1291/1775 train_time:64293ms step_avg:49.80ms
step:1292/1775 train_time:64377ms step_avg:49.83ms
step:1293/1775 train_time:64461ms step_avg:49.85ms
step:1294/1775 train_time:64548ms step_avg:49.88ms
step:1295/1775 train_time:64629ms step_avg:49.91ms
step:1296/1775 train_time:64714ms step_avg:49.93ms
step:1297/1775 train_time:64797ms step_avg:49.96ms
step:1298/1775 train_time:64882ms step_avg:49.99ms
step:1299/1775 train_time:64967ms step_avg:50.01ms
step:1300/1775 train_time:65054ms step_avg:50.04ms
step:1301/1775 train_time:65137ms step_avg:50.07ms
step:1302/1775 train_time:65223ms step_avg:50.09ms
step:1303/1775 train_time:65307ms step_avg:50.12ms
step:1304/1775 train_time:65393ms step_avg:50.15ms
step:1305/1775 train_time:65477ms step_avg:50.17ms
step:1306/1775 train_time:65562ms step_avg:50.20ms
step:1307/1775 train_time:65644ms step_avg:50.23ms
step:1308/1775 train_time:65730ms step_avg:50.25ms
step:1309/1775 train_time:65813ms step_avg:50.28ms
step:1310/1775 train_time:65900ms step_avg:50.31ms
step:1311/1775 train_time:65983ms step_avg:50.33ms
step:1312/1775 train_time:66069ms step_avg:50.36ms
step:1313/1775 train_time:66153ms step_avg:50.38ms
step:1314/1775 train_time:66240ms step_avg:50.41ms
step:1315/1775 train_time:66323ms step_avg:50.44ms
step:1316/1775 train_time:66409ms step_avg:50.46ms
step:1317/1775 train_time:66491ms step_avg:50.49ms
step:1318/1775 train_time:66578ms step_avg:50.51ms
step:1319/1775 train_time:66661ms step_avg:50.54ms
step:1320/1775 train_time:66747ms step_avg:50.57ms
step:1321/1775 train_time:66830ms step_avg:50.59ms
step:1322/1775 train_time:66916ms step_avg:50.62ms
step:1323/1775 train_time:66998ms step_avg:50.64ms
step:1324/1775 train_time:67085ms step_avg:50.67ms
step:1325/1775 train_time:67169ms step_avg:50.69ms
step:1326/1775 train_time:67255ms step_avg:50.72ms
step:1327/1775 train_time:67338ms step_avg:50.74ms
step:1328/1775 train_time:67423ms step_avg:50.77ms
step:1329/1775 train_time:67506ms step_avg:50.79ms
step:1330/1775 train_time:67591ms step_avg:50.82ms
step:1331/1775 train_time:67675ms step_avg:50.84ms
step:1332/1775 train_time:67760ms step_avg:50.87ms
step:1333/1775 train_time:67843ms step_avg:50.89ms
step:1334/1775 train_time:67928ms step_avg:50.92ms
step:1335/1775 train_time:68012ms step_avg:50.95ms
step:1336/1775 train_time:68098ms step_avg:50.97ms
step:1337/1775 train_time:68181ms step_avg:51.00ms
step:1338/1775 train_time:68268ms step_avg:51.02ms
step:1339/1775 train_time:68351ms step_avg:51.05ms
step:1340/1775 train_time:68438ms step_avg:51.07ms
step:1341/1775 train_time:68520ms step_avg:51.10ms
step:1342/1775 train_time:68606ms step_avg:51.12ms
step:1343/1775 train_time:68690ms step_avg:51.15ms
step:1344/1775 train_time:68776ms step_avg:51.17ms
step:1345/1775 train_time:68859ms step_avg:51.20ms
step:1346/1775 train_time:68945ms step_avg:51.22ms
step:1347/1775 train_time:69028ms step_avg:51.25ms
step:1348/1775 train_time:69115ms step_avg:51.27ms
step:1349/1775 train_time:69198ms step_avg:51.30ms
step:1350/1775 train_time:69284ms step_avg:51.32ms
step:1351/1775 train_time:69367ms step_avg:51.34ms
step:1352/1775 train_time:69453ms step_avg:51.37ms
step:1353/1775 train_time:69536ms step_avg:51.39ms
step:1354/1775 train_time:69621ms step_avg:51.42ms
step:1355/1775 train_time:69705ms step_avg:51.44ms
step:1356/1775 train_time:69791ms step_avg:51.47ms
step:1357/1775 train_time:69874ms step_avg:51.49ms
step:1358/1775 train_time:69959ms step_avg:51.52ms
step:1359/1775 train_time:70042ms step_avg:51.54ms
step:1360/1775 train_time:70127ms step_avg:51.56ms
step:1361/1775 train_time:70212ms step_avg:51.59ms
step:1362/1775 train_time:70298ms step_avg:51.61ms
step:1363/1775 train_time:70381ms step_avg:51.64ms
step:1364/1775 train_time:70467ms step_avg:51.66ms
step:1365/1775 train_time:70551ms step_avg:51.69ms
step:1366/1775 train_time:70637ms step_avg:51.71ms
step:1367/1775 train_time:70719ms step_avg:51.73ms
step:1368/1775 train_time:70805ms step_avg:51.76ms
step:1369/1775 train_time:70889ms step_avg:51.78ms
step:1370/1775 train_time:70974ms step_avg:51.81ms
step:1371/1775 train_time:71057ms step_avg:51.83ms
step:1372/1775 train_time:71143ms step_avg:51.85ms
step:1373/1775 train_time:71226ms step_avg:51.88ms
step:1374/1775 train_time:71311ms step_avg:51.90ms
step:1375/1775 train_time:71396ms step_avg:51.92ms
step:1376/1775 train_time:71482ms step_avg:51.95ms
step:1377/1775 train_time:71565ms step_avg:51.97ms
step:1378/1775 train_time:71651ms step_avg:52.00ms
step:1379/1775 train_time:71734ms step_avg:52.02ms
step:1380/1775 train_time:71820ms step_avg:52.04ms
step:1381/1775 train_time:71902ms step_avg:52.07ms
step:1382/1775 train_time:71989ms step_avg:52.09ms
step:1383/1775 train_time:72072ms step_avg:52.11ms
step:1384/1775 train_time:72156ms step_avg:52.14ms
step:1385/1775 train_time:72240ms step_avg:52.16ms
step:1386/1775 train_time:72326ms step_avg:52.18ms
step:1387/1775 train_time:72410ms step_avg:52.21ms
step:1388/1775 train_time:72496ms step_avg:52.23ms
step:1389/1775 train_time:72580ms step_avg:52.25ms
step:1390/1775 train_time:72665ms step_avg:52.28ms
step:1391/1775 train_time:72749ms step_avg:52.30ms
step:1392/1775 train_time:72834ms step_avg:52.32ms
step:1393/1775 train_time:72917ms step_avg:52.35ms
step:1394/1775 train_time:73002ms step_avg:52.37ms
step:1395/1775 train_time:73086ms step_avg:52.39ms
step:1396/1775 train_time:73171ms step_avg:52.41ms
step:1397/1775 train_time:73254ms step_avg:52.44ms
step:1398/1775 train_time:73340ms step_avg:52.46ms
step:1399/1775 train_time:73423ms step_avg:52.48ms
step:1400/1775 train_time:73510ms step_avg:52.51ms
step:1401/1775 train_time:73593ms step_avg:52.53ms
step:1402/1775 train_time:73679ms step_avg:52.55ms
step:1403/1775 train_time:73762ms step_avg:52.57ms
step:1404/1775 train_time:73848ms step_avg:52.60ms
step:1405/1775 train_time:73932ms step_avg:52.62ms
step:1406/1775 train_time:74019ms step_avg:52.64ms
step:1407/1775 train_time:74100ms step_avg:52.67ms
step:1408/1775 train_time:74187ms step_avg:52.69ms
step:1409/1775 train_time:74271ms step_avg:52.71ms
step:1410/1775 train_time:74357ms step_avg:52.74ms
step:1411/1775 train_time:74440ms step_avg:52.76ms
step:1412/1775 train_time:74526ms step_avg:52.78ms
step:1413/1775 train_time:74609ms step_avg:52.80ms
step:1414/1775 train_time:74695ms step_avg:52.83ms
step:1415/1775 train_time:74778ms step_avg:52.85ms
step:1416/1775 train_time:74864ms step_avg:52.87ms
step:1417/1775 train_time:74948ms step_avg:52.89ms
step:1418/1775 train_time:75035ms step_avg:52.92ms
step:1419/1775 train_time:75119ms step_avg:52.94ms
step:1420/1775 train_time:75204ms step_avg:52.96ms
step:1421/1775 train_time:75286ms step_avg:52.98ms
step:1422/1775 train_time:75373ms step_avg:53.00ms
step:1423/1775 train_time:75456ms step_avg:53.03ms
step:1424/1775 train_time:75541ms step_avg:53.05ms
step:1425/1775 train_time:75625ms step_avg:53.07ms
step:1426/1775 train_time:75711ms step_avg:53.09ms
step:1427/1775 train_time:75796ms step_avg:53.12ms
step:1428/1775 train_time:75881ms step_avg:53.14ms
step:1429/1775 train_time:75963ms step_avg:53.16ms
step:1430/1775 train_time:76050ms step_avg:53.18ms
step:1431/1775 train_time:76133ms step_avg:53.20ms
step:1432/1775 train_time:76220ms step_avg:53.23ms
step:1433/1775 train_time:76302ms step_avg:53.25ms
step:1434/1775 train_time:76388ms step_avg:53.27ms
step:1435/1775 train_time:76471ms step_avg:53.29ms
step:1436/1775 train_time:76558ms step_avg:53.31ms
step:1437/1775 train_time:76641ms step_avg:53.33ms
step:1438/1775 train_time:76727ms step_avg:53.36ms
step:1439/1775 train_time:76811ms step_avg:53.38ms
step:1440/1775 train_time:76897ms step_avg:53.40ms
step:1441/1775 train_time:76979ms step_avg:53.42ms
step:1442/1775 train_time:77067ms step_avg:53.44ms
step:1443/1775 train_time:77149ms step_avg:53.46ms
step:1444/1775 train_time:77234ms step_avg:53.49ms
step:1445/1775 train_time:77318ms step_avg:53.51ms
step:1446/1775 train_time:77404ms step_avg:53.53ms
step:1447/1775 train_time:77487ms step_avg:53.55ms
step:1448/1775 train_time:77573ms step_avg:53.57ms
step:1449/1775 train_time:77656ms step_avg:53.59ms
step:1450/1775 train_time:77741ms step_avg:53.61ms
step:1451/1775 train_time:77825ms step_avg:53.64ms
step:1452/1775 train_time:77912ms step_avg:53.66ms
step:1453/1775 train_time:77994ms step_avg:53.68ms
step:1454/1775 train_time:78080ms step_avg:53.70ms
step:1455/1775 train_time:78163ms step_avg:53.72ms
step:1456/1775 train_time:78249ms step_avg:53.74ms
step:1457/1775 train_time:78332ms step_avg:53.76ms
step:1458/1775 train_time:78418ms step_avg:53.78ms
step:1459/1775 train_time:78501ms step_avg:53.80ms
step:1460/1775 train_time:78587ms step_avg:53.83ms
step:1461/1775 train_time:78670ms step_avg:53.85ms
step:1462/1775 train_time:78756ms step_avg:53.87ms
step:1463/1775 train_time:78839ms step_avg:53.89ms
step:1464/1775 train_time:78925ms step_avg:53.91ms
step:1465/1775 train_time:79009ms step_avg:53.93ms
step:1466/1775 train_time:79095ms step_avg:53.95ms
step:1467/1775 train_time:79179ms step_avg:53.97ms
step:1468/1775 train_time:79266ms step_avg:54.00ms
step:1469/1775 train_time:79349ms step_avg:54.02ms
step:1470/1775 train_time:79435ms step_avg:54.04ms
step:1471/1775 train_time:79517ms step_avg:54.06ms
step:1472/1775 train_time:79603ms step_avg:54.08ms
step:1473/1775 train_time:79687ms step_avg:54.10ms
step:1474/1775 train_time:79772ms step_avg:54.12ms
step:1475/1775 train_time:79855ms step_avg:54.14ms
step:1476/1775 train_time:79941ms step_avg:54.16ms
step:1477/1775 train_time:80024ms step_avg:54.18ms
step:1478/1775 train_time:80111ms step_avg:54.20ms
step:1479/1775 train_time:80194ms step_avg:54.22ms
step:1480/1775 train_time:80282ms step_avg:54.24ms
step:1481/1775 train_time:80363ms step_avg:54.26ms
step:1482/1775 train_time:80449ms step_avg:54.28ms
step:1483/1775 train_time:80532ms step_avg:54.30ms
step:1484/1775 train_time:80617ms step_avg:54.32ms
step:1485/1775 train_time:80700ms step_avg:54.34ms
step:1486/1775 train_time:80786ms step_avg:54.36ms
step:1487/1775 train_time:80869ms step_avg:54.38ms
step:1488/1775 train_time:80955ms step_avg:54.41ms
step:1489/1775 train_time:81038ms step_avg:54.42ms
step:1490/1775 train_time:81123ms step_avg:54.44ms
step:1491/1775 train_time:81206ms step_avg:54.46ms
step:1492/1775 train_time:81292ms step_avg:54.49ms
step:1493/1775 train_time:81376ms step_avg:54.50ms
step:1494/1775 train_time:81460ms step_avg:54.52ms
step:1495/1775 train_time:81545ms step_avg:54.55ms
step:1496/1775 train_time:81630ms step_avg:54.57ms
step:1497/1775 train_time:81714ms step_avg:54.59ms
step:1498/1775 train_time:81799ms step_avg:54.61ms
step:1499/1775 train_time:81883ms step_avg:54.63ms
step:1500/1775 train_time:81969ms step_avg:54.65ms
step:1500/1775 val_loss:3.3755 train_time:82067ms step_avg:54.71ms
step:1501/1775 train_time:82089ms step_avg:54.69ms
step:1502/1775 train_time:82140ms step_avg:54.69ms
step:1503/1775 train_time:82227ms step_avg:54.71ms
step:1504/1775 train_time:82317ms step_avg:54.73ms
step:1505/1775 train_time:82400ms step_avg:54.75ms
step:1506/1775 train_time:82485ms step_avg:54.77ms
step:1507/1775 train_time:82568ms step_avg:54.79ms
step:1508/1775 train_time:82652ms step_avg:54.81ms
step:1509/1775 train_time:82734ms step_avg:54.83ms
step:1510/1775 train_time:82819ms step_avg:54.85ms
step:1511/1775 train_time:82901ms step_avg:54.86ms
step:1512/1775 train_time:82988ms step_avg:54.89ms
step:1513/1775 train_time:83074ms step_avg:54.91ms
step:1514/1775 train_time:83163ms step_avg:54.93ms
step:1515/1775 train_time:83246ms step_avg:54.95ms
step:1516/1775 train_time:83334ms step_avg:54.97ms
step:1517/1775 train_time:83417ms step_avg:54.99ms
step:1518/1775 train_time:83503ms step_avg:55.01ms
step:1519/1775 train_time:83585ms step_avg:55.03ms
step:1520/1775 train_time:83670ms step_avg:55.05ms
step:1521/1775 train_time:83752ms step_avg:55.06ms
step:1522/1775 train_time:83838ms step_avg:55.08ms
step:1523/1775 train_time:83919ms step_avg:55.10ms
step:1524/1775 train_time:84005ms step_avg:55.12ms
step:1525/1775 train_time:84091ms step_avg:55.14ms
step:1526/1775 train_time:84177ms step_avg:55.16ms
step:1527/1775 train_time:84261ms step_avg:55.18ms
step:1528/1775 train_time:84347ms step_avg:55.20ms
step:1529/1775 train_time:84431ms step_avg:55.22ms
step:1530/1775 train_time:84517ms step_avg:55.24ms
step:1531/1775 train_time:84600ms step_avg:55.26ms
step:1532/1775 train_time:84685ms step_avg:55.28ms
step:1533/1775 train_time:84767ms step_avg:55.29ms
step:1534/1775 train_time:84852ms step_avg:55.31ms
step:1535/1775 train_time:84935ms step_avg:55.33ms
step:1536/1775 train_time:85021ms step_avg:55.35ms
step:1537/1775 train_time:85105ms step_avg:55.37ms
step:1538/1775 train_time:85191ms step_avg:55.39ms
step:1539/1775 train_time:85275ms step_avg:55.41ms
step:1540/1775 train_time:85361ms step_avg:55.43ms
step:1541/1775 train_time:85444ms step_avg:55.45ms
step:1542/1775 train_time:85530ms step_avg:55.47ms
step:1543/1775 train_time:85614ms step_avg:55.49ms
step:1544/1775 train_time:85699ms step_avg:55.50ms
step:1545/1775 train_time:85781ms step_avg:55.52ms
step:1546/1775 train_time:85867ms step_avg:55.54ms
step:1547/1775 train_time:85950ms step_avg:55.56ms
step:1548/1775 train_time:86036ms step_avg:55.58ms
step:1549/1775 train_time:86120ms step_avg:55.60ms
step:1550/1775 train_time:86206ms step_avg:55.62ms
step:1551/1775 train_time:86290ms step_avg:55.63ms
step:1552/1775 train_time:86375ms step_avg:55.65ms
step:1553/1775 train_time:86459ms step_avg:55.67ms
step:1554/1775 train_time:86544ms step_avg:55.69ms
step:1555/1775 train_time:86628ms step_avg:55.71ms
step:1556/1775 train_time:86715ms step_avg:55.73ms
step:1557/1775 train_time:86797ms step_avg:55.75ms
step:1558/1775 train_time:86881ms step_avg:55.76ms
step:1559/1775 train_time:86964ms step_avg:55.78ms
step:1560/1775 train_time:87050ms step_avg:55.80ms
step:1561/1775 train_time:87135ms step_avg:55.82ms
step:1562/1775 train_time:87221ms step_avg:55.84ms
step:1563/1775 train_time:87304ms step_avg:55.86ms
step:1564/1775 train_time:87390ms step_avg:55.88ms
step:1565/1775 train_time:87472ms step_avg:55.89ms
step:1566/1775 train_time:87559ms step_avg:55.91ms
step:1567/1775 train_time:87641ms step_avg:55.93ms
step:1568/1775 train_time:87727ms step_avg:55.95ms
step:1569/1775 train_time:87810ms step_avg:55.97ms
step:1570/1775 train_time:87896ms step_avg:55.98ms
step:1571/1775 train_time:87978ms step_avg:56.00ms
step:1572/1775 train_time:88064ms step_avg:56.02ms
step:1573/1775 train_time:88148ms step_avg:56.04ms
step:1574/1775 train_time:88235ms step_avg:56.06ms
step:1575/1775 train_time:88318ms step_avg:56.08ms
step:1576/1775 train_time:88404ms step_avg:56.09ms
step:1577/1775 train_time:88487ms step_avg:56.11ms
step:1578/1775 train_time:88574ms step_avg:56.13ms
step:1579/1775 train_time:88657ms step_avg:56.15ms
step:1580/1775 train_time:88742ms step_avg:56.17ms
step:1581/1775 train_time:88825ms step_avg:56.18ms
step:1582/1775 train_time:88910ms step_avg:56.20ms
step:1583/1775 train_time:88994ms step_avg:56.22ms
step:1584/1775 train_time:89079ms step_avg:56.24ms
step:1585/1775 train_time:89161ms step_avg:56.25ms
step:1586/1775 train_time:89247ms step_avg:56.27ms
step:1587/1775 train_time:89332ms step_avg:56.29ms
step:1588/1775 train_time:89419ms step_avg:56.31ms
step:1589/1775 train_time:89502ms step_avg:56.33ms
step:1590/1775 train_time:89588ms step_avg:56.34ms
step:1591/1775 train_time:89671ms step_avg:56.36ms
step:1592/1775 train_time:89757ms step_avg:56.38ms
step:1593/1775 train_time:89840ms step_avg:56.40ms
step:1594/1775 train_time:89925ms step_avg:56.41ms
step:1595/1775 train_time:90008ms step_avg:56.43ms
step:1596/1775 train_time:90095ms step_avg:56.45ms
step:1597/1775 train_time:90178ms step_avg:56.47ms
step:1598/1775 train_time:90264ms step_avg:56.49ms
step:1599/1775 train_time:90347ms step_avg:56.50ms
step:1600/1775 train_time:90433ms step_avg:56.52ms
step:1601/1775 train_time:90516ms step_avg:56.54ms
step:1602/1775 train_time:90602ms step_avg:56.56ms
step:1603/1775 train_time:90686ms step_avg:56.57ms
step:1604/1775 train_time:90770ms step_avg:56.59ms
step:1605/1775 train_time:90854ms step_avg:56.61ms
step:1606/1775 train_time:90940ms step_avg:56.63ms
step:1607/1775 train_time:91022ms step_avg:56.64ms
step:1608/1775 train_time:91109ms step_avg:56.66ms
step:1609/1775 train_time:91191ms step_avg:56.68ms
step:1610/1775 train_time:91278ms step_avg:56.69ms
step:1611/1775 train_time:91360ms step_avg:56.71ms
step:1612/1775 train_time:91446ms step_avg:56.73ms
step:1613/1775 train_time:91530ms step_avg:56.75ms
step:1614/1775 train_time:91617ms step_avg:56.76ms
step:1615/1775 train_time:91700ms step_avg:56.78ms
step:1616/1775 train_time:91785ms step_avg:56.80ms
step:1617/1775 train_time:91869ms step_avg:56.81ms
step:1618/1775 train_time:91954ms step_avg:56.83ms
step:1619/1775 train_time:92036ms step_avg:56.85ms
step:1620/1775 train_time:92122ms step_avg:56.87ms
step:1621/1775 train_time:92206ms step_avg:56.88ms
step:1622/1775 train_time:92291ms step_avg:56.90ms
step:1623/1775 train_time:92375ms step_avg:56.92ms
step:1624/1775 train_time:92460ms step_avg:56.93ms
step:1625/1775 train_time:92543ms step_avg:56.95ms
step:1626/1775 train_time:92629ms step_avg:56.97ms
step:1627/1775 train_time:92712ms step_avg:56.98ms
step:1628/1775 train_time:92798ms step_avg:57.00ms
step:1629/1775 train_time:92881ms step_avg:57.02ms
step:1630/1775 train_time:92968ms step_avg:57.04ms
step:1631/1775 train_time:93051ms step_avg:57.05ms
step:1632/1775 train_time:93137ms step_avg:57.07ms
step:1633/1775 train_time:93220ms step_avg:57.09ms
step:1634/1775 train_time:93305ms step_avg:57.10ms
step:1635/1775 train_time:93389ms step_avg:57.12ms
step:1636/1775 train_time:93475ms step_avg:57.14ms
step:1637/1775 train_time:93559ms step_avg:57.15ms
step:1638/1775 train_time:93644ms step_avg:57.17ms
step:1639/1775 train_time:93728ms step_avg:57.19ms
step:1640/1775 train_time:93813ms step_avg:57.20ms
step:1641/1775 train_time:93898ms step_avg:57.22ms
step:1642/1775 train_time:93984ms step_avg:57.24ms
step:1643/1775 train_time:94067ms step_avg:57.25ms
step:1644/1775 train_time:94151ms step_avg:57.27ms
step:1645/1775 train_time:94236ms step_avg:57.29ms
step:1646/1775 train_time:94322ms step_avg:57.30ms
step:1647/1775 train_time:94406ms step_avg:57.32ms
step:1648/1775 train_time:94492ms step_avg:57.34ms
step:1649/1775 train_time:94575ms step_avg:57.35ms
step:1650/1775 train_time:94661ms step_avg:57.37ms
step:1651/1775 train_time:94744ms step_avg:57.39ms
step:1652/1775 train_time:94831ms step_avg:57.40ms
step:1653/1775 train_time:94914ms step_avg:57.42ms
step:1654/1775 train_time:95000ms step_avg:57.44ms
step:1655/1775 train_time:95082ms step_avg:57.45ms
step:1656/1775 train_time:95169ms step_avg:57.47ms
step:1657/1775 train_time:95251ms step_avg:57.48ms
step:1658/1775 train_time:95339ms step_avg:57.50ms
step:1659/1775 train_time:95421ms step_avg:57.52ms
step:1660/1775 train_time:95507ms step_avg:57.53ms
step:1661/1775 train_time:95590ms step_avg:57.55ms
step:1662/1775 train_time:95675ms step_avg:57.57ms
step:1663/1775 train_time:95757ms step_avg:57.58ms
step:1664/1775 train_time:95844ms step_avg:57.60ms
step:1665/1775 train_time:95928ms step_avg:57.61ms
step:1666/1775 train_time:96015ms step_avg:57.63ms
step:1667/1775 train_time:96097ms step_avg:57.65ms
step:1668/1775 train_time:96183ms step_avg:57.66ms
step:1669/1775 train_time:96267ms step_avg:57.68ms
step:1670/1775 train_time:96352ms step_avg:57.70ms
step:1671/1775 train_time:96437ms step_avg:57.71ms
step:1672/1775 train_time:96522ms step_avg:57.73ms
step:1673/1775 train_time:96607ms step_avg:57.74ms
step:1674/1775 train_time:96692ms step_avg:57.76ms
step:1675/1775 train_time:96776ms step_avg:57.78ms
step:1676/1775 train_time:96862ms step_avg:57.79ms
step:1677/1775 train_time:96945ms step_avg:57.81ms
step:1678/1775 train_time:97032ms step_avg:57.83ms
step:1679/1775 train_time:97115ms step_avg:57.84ms
step:1680/1775 train_time:97202ms step_avg:57.86ms
step:1681/1775 train_time:97283ms step_avg:57.87ms
step:1682/1775 train_time:97369ms step_avg:57.89ms
step:1683/1775 train_time:97451ms step_avg:57.90ms
step:1684/1775 train_time:97539ms step_avg:57.92ms
step:1685/1775 train_time:97622ms step_avg:57.94ms
step:1686/1775 train_time:97708ms step_avg:57.95ms
step:1687/1775 train_time:97791ms step_avg:57.97ms
step:1688/1775 train_time:97877ms step_avg:57.98ms
step:1689/1775 train_time:97959ms step_avg:58.00ms
step:1690/1775 train_time:98045ms step_avg:58.02ms
step:1691/1775 train_time:98130ms step_avg:58.03ms
step:1692/1775 train_time:98216ms step_avg:58.05ms
step:1693/1775 train_time:98299ms step_avg:58.06ms
step:1694/1775 train_time:98385ms step_avg:58.08ms
step:1695/1775 train_time:98468ms step_avg:58.09ms
step:1696/1775 train_time:98555ms step_avg:58.11ms
step:1697/1775 train_time:98637ms step_avg:58.12ms
step:1698/1775 train_time:98724ms step_avg:58.14ms
step:1699/1775 train_time:98807ms step_avg:58.16ms
step:1700/1775 train_time:98893ms step_avg:58.17ms
step:1701/1775 train_time:98976ms step_avg:58.19ms
step:1702/1775 train_time:99062ms step_avg:58.20ms
step:1703/1775 train_time:99146ms step_avg:58.22ms
step:1704/1775 train_time:99232ms step_avg:58.24ms
step:1705/1775 train_time:99317ms step_avg:58.25ms
step:1706/1775 train_time:99402ms step_avg:58.27ms
step:1707/1775 train_time:99485ms step_avg:58.28ms
step:1708/1775 train_time:99571ms step_avg:58.30ms
step:1709/1775 train_time:99655ms step_avg:58.31ms
step:1710/1775 train_time:99740ms step_avg:58.33ms
step:1711/1775 train_time:99824ms step_avg:58.34ms
step:1712/1775 train_time:99910ms step_avg:58.36ms
step:1713/1775 train_time:99993ms step_avg:58.37ms
step:1714/1775 train_time:100078ms step_avg:58.39ms
step:1715/1775 train_time:100161ms step_avg:58.40ms
step:1716/1775 train_time:100247ms step_avg:58.42ms
step:1717/1775 train_time:100330ms step_avg:58.43ms
step:1718/1775 train_time:100417ms step_avg:58.45ms
step:1719/1775 train_time:100500ms step_avg:58.46ms
step:1720/1775 train_time:100586ms step_avg:58.48ms
step:1721/1775 train_time:100669ms step_avg:58.49ms
step:1722/1775 train_time:100756ms step_avg:58.51ms
step:1723/1775 train_time:100838ms step_avg:58.52ms
step:1724/1775 train_time:100924ms step_avg:58.54ms
step:1725/1775 train_time:101008ms step_avg:58.56ms
step:1726/1775 train_time:101093ms step_avg:58.57ms
step:1727/1775 train_time:101176ms step_avg:58.58ms
step:1728/1775 train_time:101261ms step_avg:58.60ms
step:1729/1775 train_time:101345ms step_avg:58.61ms
step:1730/1775 train_time:101432ms step_avg:58.63ms
step:1731/1775 train_time:101516ms step_avg:58.65ms
step:1732/1775 train_time:101601ms step_avg:58.66ms
step:1733/1775 train_time:101684ms step_avg:58.67ms
step:1734/1775 train_time:101770ms step_avg:58.69ms
step:1735/1775 train_time:101852ms step_avg:58.70ms
step:1736/1775 train_time:101942ms step_avg:58.72ms
step:1737/1775 train_time:102027ms step_avg:58.74ms
step:1738/1775 train_time:102113ms step_avg:58.75ms
step:1739/1775 train_time:102197ms step_avg:58.77ms
step:1740/1775 train_time:102283ms step_avg:58.78ms
step:1741/1775 train_time:102367ms step_avg:58.80ms
step:1742/1775 train_time:102455ms step_avg:58.81ms
step:1743/1775 train_time:102539ms step_avg:58.83ms
step:1744/1775 train_time:102625ms step_avg:58.84ms
step:1745/1775 train_time:102708ms step_avg:58.86ms
step:1746/1775 train_time:102796ms step_avg:58.88ms
step:1747/1775 train_time:102879ms step_avg:58.89ms
step:1748/1775 train_time:102966ms step_avg:58.90ms
step:1749/1775 train_time:103049ms step_avg:58.92ms
step:1750/1775 train_time:103136ms step_avg:58.94ms
step:1750/1775 val_loss:3.2843 train_time:103235ms step_avg:58.99ms
step:1751/1775 train_time:103258ms step_avg:58.97ms
step:1752/1775 train_time:103308ms step_avg:58.97ms
step:1753/1775 train_time:103394ms step_avg:58.98ms
step:1754/1775 train_time:103483ms step_avg:59.00ms
step:1755/1775 train_time:103567ms step_avg:59.01ms
step:1756/1775 train_time:103653ms step_avg:59.03ms
step:1757/1775 train_time:103737ms step_avg:59.04ms
step:1758/1775 train_time:103822ms step_avg:59.06ms
step:1759/1775 train_time:103906ms step_avg:59.07ms
step:1760/1775 train_time:103992ms step_avg:59.09ms
step:1761/1775 train_time:104074ms step_avg:59.10ms
step:1762/1775 train_time:104160ms step_avg:59.11ms
step:1763/1775 train_time:104245ms step_avg:59.13ms
step:1764/1775 train_time:104335ms step_avg:59.15ms
step:1765/1775 train_time:104421ms step_avg:59.16ms
step:1766/1775 train_time:104509ms step_avg:59.18ms
step:1767/1775 train_time:104592ms step_avg:59.19ms
step:1768/1775 train_time:104677ms step_avg:59.21ms
step:1769/1775 train_time:104760ms step_avg:59.22ms
step:1770/1775 train_time:104845ms step_avg:59.23ms
step:1771/1775 train_time:104927ms step_avg:59.25ms
step:1772/1775 train_time:105013ms step_avg:59.26ms
step:1773/1775 train_time:105095ms step_avg:59.28ms
step:1774/1775 train_time:105181ms step_avg:59.29ms
step:1775/1775 train_time:105266ms step_avg:59.30ms
step:1775/1775 val_loss:3.2777 train_time:105369ms step_avg:59.36ms
peak memory allocated: 29245 MiB reserved: 44618 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan 19 01:49:27 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:00:0B.0 Off |                    0 |
| N/A   31C    P0             109W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:00:0C.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:00:0D.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:00:0E.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:00:0F.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:00:10.0 Off |                    0 |
| N/A   38C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:00:11.0 Off |                    0 |
| N/A   35C    P0             114W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:00:12.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     76247      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    1   N/A  N/A     76248      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    2   N/A  N/A     76249      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    3   N/A  N/A     76250      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    4   N/A  N/A     76251      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    5   N/A  N/A     76252      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    6   N/A  N/A     76253      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    7   N/A  N/A     76254      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8273 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:97ms step_avg:96.66ms
step:2/1775 train_time:121ms step_avg:60.61ms
step:3/1775 train_time:142ms step_avg:47.33ms
step:4/1775 train_time:173ms step_avg:43.21ms
step:5/1775 train_time:203ms step_avg:40.67ms
step:6/1775 train_time:335ms step_avg:55.87ms
step:7/1775 train_time:354ms step_avg:50.59ms
step:8/1775 train_time:534ms step_avg:66.75ms
step:9/1775 train_time:565ms step_avg:62.73ms
step:10/1775 train_time:597ms step_avg:59.72ms
step:11/1775 train_time:628ms step_avg:57.11ms
step:12/1775 train_time:661ms step_avg:55.11ms
step:13/1775 train_time:692ms step_avg:53.26ms
step:14/1775 train_time:726ms step_avg:51.84ms
step:15/1775 train_time:757ms step_avg:50.48ms
step:16/1775 train_time:790ms step_avg:49.38ms
step:17/1775 train_time:821ms step_avg:48.28ms
step:18/1775 train_time:854ms step_avg:47.42ms
step:19/1775 train_time:885ms step_avg:46.56ms
step:20/1775 train_time:918ms step_avg:45.92ms
step:21/1775 train_time:949ms step_avg:45.21ms
step:22/1775 train_time:982ms step_avg:44.66ms
step:23/1775 train_time:1013ms step_avg:44.05ms
step:24/1775 train_time:1046ms step_avg:43.60ms
step:25/1775 train_time:1077ms step_avg:43.09ms
step:26/1775 train_time:1111ms step_avg:42.72ms
step:27/1775 train_time:1142ms step_avg:42.29ms
step:28/1775 train_time:1176ms step_avg:41.99ms
step:29/1775 train_time:1207ms step_avg:41.61ms
step:30/1775 train_time:1240ms step_avg:41.33ms
step:31/1775 train_time:1270ms step_avg:40.98ms
step:32/1775 train_time:1304ms step_avg:40.73ms
step:33/1775 train_time:1335ms step_avg:40.45ms
step:34/1775 train_time:1368ms step_avg:40.23ms
step:35/1775 train_time:1400ms step_avg:39.99ms
step:36/1775 train_time:1436ms step_avg:39.88ms
step:37/1775 train_time:1469ms step_avg:39.70ms
step:38/1775 train_time:1504ms step_avg:39.57ms
step:39/1775 train_time:1536ms step_avg:39.38ms
step:40/1775 train_time:1570ms step_avg:39.24ms
step:41/1775 train_time:1601ms step_avg:39.06ms
step:42/1775 train_time:1635ms step_avg:38.94ms
step:43/1775 train_time:1667ms step_avg:38.76ms
step:44/1775 train_time:1700ms step_avg:38.64ms
step:45/1775 train_time:1731ms step_avg:38.48ms
step:46/1775 train_time:1765ms step_avg:38.38ms
step:47/1775 train_time:1796ms step_avg:38.22ms
step:48/1775 train_time:1830ms step_avg:38.12ms
step:49/1775 train_time:1861ms step_avg:37.97ms
step:50/1775 train_time:1894ms step_avg:37.87ms
step:51/1775 train_time:1925ms step_avg:37.74ms
step:52/1775 train_time:1958ms step_avg:37.65ms
step:53/1775 train_time:1989ms step_avg:37.53ms
step:54/1775 train_time:2022ms step_avg:37.45ms
step:55/1775 train_time:2053ms step_avg:37.33ms
step:56/1775 train_time:2086ms step_avg:37.25ms
step:57/1775 train_time:2117ms step_avg:37.15ms
step:58/1775 train_time:2151ms step_avg:37.08ms
step:59/1775 train_time:2182ms step_avg:36.97ms
step:60/1775 train_time:2215ms step_avg:36.91ms
step:61/1775 train_time:2245ms step_avg:36.81ms
step:62/1775 train_time:2279ms step_avg:36.76ms
step:63/1775 train_time:2310ms step_avg:36.66ms
step:64/1775 train_time:2343ms step_avg:36.61ms
step:65/1775 train_time:2375ms step_avg:36.54ms
step:66/1775 train_time:2409ms step_avg:36.50ms
step:67/1775 train_time:2440ms step_avg:36.42ms
step:68/1775 train_time:2474ms step_avg:36.39ms
step:69/1775 train_time:2506ms step_avg:36.32ms
step:70/1775 train_time:2540ms step_avg:36.29ms
step:71/1775 train_time:2573ms step_avg:36.23ms
step:72/1775 train_time:2607ms step_avg:36.20ms
step:73/1775 train_time:2638ms step_avg:36.14ms
step:74/1775 train_time:2672ms step_avg:36.11ms
step:75/1775 train_time:2703ms step_avg:36.04ms
step:76/1775 train_time:2737ms step_avg:36.01ms
step:77/1775 train_time:2769ms step_avg:35.96ms
step:78/1775 train_time:2802ms step_avg:35.93ms
step:79/1775 train_time:2833ms step_avg:35.86ms
step:80/1775 train_time:2867ms step_avg:35.83ms
step:81/1775 train_time:2898ms step_avg:35.78ms
step:82/1775 train_time:2932ms step_avg:35.75ms
step:83/1775 train_time:2962ms step_avg:35.69ms
step:84/1775 train_time:2996ms step_avg:35.67ms
step:85/1775 train_time:3027ms step_avg:35.62ms
step:86/1775 train_time:3061ms step_avg:35.59ms
step:87/1775 train_time:3092ms step_avg:35.54ms
step:88/1775 train_time:3125ms step_avg:35.51ms
step:89/1775 train_time:3156ms step_avg:35.46ms
step:90/1775 train_time:3189ms step_avg:35.44ms
step:91/1775 train_time:3220ms step_avg:35.39ms
step:92/1775 train_time:3254ms step_avg:35.36ms
step:93/1775 train_time:3285ms step_avg:35.32ms
step:94/1775 train_time:3318ms step_avg:35.30ms
step:95/1775 train_time:3349ms step_avg:35.26ms
step:96/1775 train_time:3383ms step_avg:35.24ms
step:97/1775 train_time:3415ms step_avg:35.20ms
step:98/1775 train_time:3448ms step_avg:35.18ms
step:99/1775 train_time:3480ms step_avg:35.15ms
step:100/1775 train_time:3513ms step_avg:35.13ms
step:101/1775 train_time:3544ms step_avg:35.09ms
step:102/1775 train_time:3578ms step_avg:35.08ms
step:103/1775 train_time:3610ms step_avg:35.05ms
step:104/1775 train_time:3643ms step_avg:35.03ms
step:105/1775 train_time:3675ms step_avg:35.00ms
step:106/1775 train_time:3709ms step_avg:34.99ms
step:107/1775 train_time:3740ms step_avg:34.96ms
step:108/1775 train_time:3774ms step_avg:34.94ms
step:109/1775 train_time:3805ms step_avg:34.91ms
step:110/1775 train_time:3838ms step_avg:34.89ms
step:111/1775 train_time:3870ms step_avg:34.86ms
step:112/1775 train_time:3903ms step_avg:34.85ms
step:113/1775 train_time:3934ms step_avg:34.82ms
step:114/1775 train_time:3968ms step_avg:34.81ms
step:115/1775 train_time:3999ms step_avg:34.77ms
step:116/1775 train_time:4032ms step_avg:34.76ms
step:117/1775 train_time:4063ms step_avg:34.73ms
step:118/1775 train_time:4096ms step_avg:34.71ms
step:119/1775 train_time:4128ms step_avg:34.69ms
step:120/1775 train_time:4161ms step_avg:34.68ms
step:121/1775 train_time:4192ms step_avg:34.65ms
step:122/1775 train_time:4225ms step_avg:34.63ms
step:123/1775 train_time:4256ms step_avg:34.60ms
step:124/1775 train_time:4289ms step_avg:34.59ms
step:125/1775 train_time:4320ms step_avg:34.56ms
step:126/1775 train_time:4354ms step_avg:34.56ms
step:127/1775 train_time:4385ms step_avg:34.53ms
step:128/1775 train_time:4418ms step_avg:34.52ms
step:129/1775 train_time:4450ms step_avg:34.49ms
step:130/1775 train_time:4483ms step_avg:34.48ms
step:131/1775 train_time:4515ms step_avg:34.46ms
step:132/1775 train_time:4549ms step_avg:34.46ms
step:133/1775 train_time:4580ms step_avg:34.44ms
step:134/1775 train_time:4614ms step_avg:34.43ms
step:135/1775 train_time:4645ms step_avg:34.41ms
step:136/1775 train_time:4679ms step_avg:34.40ms
step:137/1775 train_time:4710ms step_avg:34.38ms
step:138/1775 train_time:4743ms step_avg:34.37ms
step:139/1775 train_time:4774ms step_avg:34.35ms
step:140/1775 train_time:4808ms step_avg:34.34ms
step:141/1775 train_time:4839ms step_avg:34.32ms
step:142/1775 train_time:4873ms step_avg:34.31ms
step:143/1775 train_time:4904ms step_avg:34.29ms
step:144/1775 train_time:4937ms step_avg:34.29ms
step:145/1775 train_time:4968ms step_avg:34.26ms
step:146/1775 train_time:5001ms step_avg:34.26ms
step:147/1775 train_time:5032ms step_avg:34.23ms
step:148/1775 train_time:5065ms step_avg:34.23ms
step:149/1775 train_time:5097ms step_avg:34.21ms
step:150/1775 train_time:5131ms step_avg:34.20ms
step:151/1775 train_time:5162ms step_avg:34.18ms
step:152/1775 train_time:5195ms step_avg:34.18ms
step:153/1775 train_time:5226ms step_avg:34.16ms
step:154/1775 train_time:5259ms step_avg:34.15ms
step:155/1775 train_time:5291ms step_avg:34.13ms
step:156/1775 train_time:5324ms step_avg:34.13ms
step:157/1775 train_time:5355ms step_avg:34.11ms
step:158/1775 train_time:5388ms step_avg:34.10ms
step:159/1775 train_time:5419ms step_avg:34.08ms
step:160/1775 train_time:5452ms step_avg:34.08ms
step:161/1775 train_time:5483ms step_avg:34.06ms
step:162/1775 train_time:5517ms step_avg:34.06ms
step:163/1775 train_time:5549ms step_avg:34.05ms
step:164/1775 train_time:5583ms step_avg:34.04ms
step:165/1775 train_time:5615ms step_avg:34.03ms
step:166/1775 train_time:5648ms step_avg:34.02ms
step:167/1775 train_time:5679ms step_avg:34.00ms
step:168/1775 train_time:5712ms step_avg:34.00ms
step:169/1775 train_time:5743ms step_avg:33.98ms
step:170/1775 train_time:5777ms step_avg:33.98ms
step:171/1775 train_time:5809ms step_avg:33.97ms
step:172/1775 train_time:5842ms step_avg:33.97ms
step:173/1775 train_time:5873ms step_avg:33.95ms
step:174/1775 train_time:5906ms step_avg:33.95ms
step:175/1775 train_time:5938ms step_avg:33.93ms
step:176/1775 train_time:5971ms step_avg:33.93ms
step:177/1775 train_time:6002ms step_avg:33.91ms
step:178/1775 train_time:6036ms step_avg:33.91ms
step:179/1775 train_time:6067ms step_avg:33.90ms
step:180/1775 train_time:6101ms step_avg:33.89ms
step:181/1775 train_time:6132ms step_avg:33.88ms
step:182/1775 train_time:6165ms step_avg:33.87ms
step:183/1775 train_time:6197ms step_avg:33.86ms
step:184/1775 train_time:6230ms step_avg:33.86ms
step:185/1775 train_time:6261ms step_avg:33.84ms
step:186/1775 train_time:6294ms step_avg:33.84ms
step:187/1775 train_time:6325ms step_avg:33.82ms
step:188/1775 train_time:6359ms step_avg:33.82ms
step:189/1775 train_time:6390ms step_avg:33.81ms
step:190/1775 train_time:6423ms step_avg:33.81ms
step:191/1775 train_time:6455ms step_avg:33.80ms
step:192/1775 train_time:6489ms step_avg:33.80ms
step:193/1775 train_time:6521ms step_avg:33.79ms
step:194/1775 train_time:6554ms step_avg:33.79ms
step:195/1775 train_time:6586ms step_avg:33.77ms
step:196/1775 train_time:6619ms step_avg:33.77ms
step:197/1775 train_time:6652ms step_avg:33.76ms
step:198/1775 train_time:6685ms step_avg:33.76ms
step:199/1775 train_time:6717ms step_avg:33.75ms
step:200/1775 train_time:6751ms step_avg:33.75ms
step:201/1775 train_time:6782ms step_avg:33.74ms
step:202/1775 train_time:6816ms step_avg:33.74ms
step:203/1775 train_time:6847ms step_avg:33.73ms
step:204/1775 train_time:6881ms step_avg:33.73ms
step:205/1775 train_time:6912ms step_avg:33.72ms
step:206/1775 train_time:6946ms step_avg:33.72ms
step:207/1775 train_time:6977ms step_avg:33.71ms
step:208/1775 train_time:7011ms step_avg:33.70ms
step:209/1775 train_time:7042ms step_avg:33.69ms
step:210/1775 train_time:7075ms step_avg:33.69ms
step:211/1775 train_time:7106ms step_avg:33.68ms
step:212/1775 train_time:7139ms step_avg:33.68ms
step:213/1775 train_time:7170ms step_avg:33.66ms
step:214/1775 train_time:7203ms step_avg:33.66ms
step:215/1775 train_time:7235ms step_avg:33.65ms
step:216/1775 train_time:7268ms step_avg:33.65ms
step:217/1775 train_time:7299ms step_avg:33.64ms
step:218/1775 train_time:7332ms step_avg:33.63ms
step:219/1775 train_time:7364ms step_avg:33.62ms
step:220/1775 train_time:7397ms step_avg:33.62ms
step:221/1775 train_time:7428ms step_avg:33.61ms
step:222/1775 train_time:7462ms step_avg:33.61ms
step:223/1775 train_time:7492ms step_avg:33.60ms
step:224/1775 train_time:7526ms step_avg:33.60ms
step:225/1775 train_time:7558ms step_avg:33.59ms
step:226/1775 train_time:7591ms step_avg:33.59ms
step:227/1775 train_time:7623ms step_avg:33.58ms
step:228/1775 train_time:7657ms step_avg:33.58ms
step:229/1775 train_time:7688ms step_avg:33.57ms
step:230/1775 train_time:7722ms step_avg:33.57ms
step:231/1775 train_time:7753ms step_avg:33.56ms
step:232/1775 train_time:7787ms step_avg:33.56ms
step:233/1775 train_time:7818ms step_avg:33.55ms
step:234/1775 train_time:7851ms step_avg:33.55ms
step:235/1775 train_time:7882ms step_avg:33.54ms
step:236/1775 train_time:7916ms step_avg:33.54ms
step:237/1775 train_time:7947ms step_avg:33.53ms
step:238/1775 train_time:7980ms step_avg:33.53ms
step:239/1775 train_time:8011ms step_avg:33.52ms
step:240/1775 train_time:8044ms step_avg:33.52ms
step:241/1775 train_time:8076ms step_avg:33.51ms
step:242/1775 train_time:8109ms step_avg:33.51ms
step:243/1775 train_time:8141ms step_avg:33.50ms
step:244/1775 train_time:8174ms step_avg:33.50ms
step:245/1775 train_time:8205ms step_avg:33.49ms
step:246/1775 train_time:8238ms step_avg:33.49ms
step:247/1775 train_time:8270ms step_avg:33.48ms
step:248/1775 train_time:8303ms step_avg:33.48ms
step:249/1775 train_time:8334ms step_avg:33.47ms
step:250/1775 train_time:8367ms step_avg:33.47ms
step:250/1775 val_loss:4.6065 train_time:8408ms step_avg:33.63ms
step:251/1775 train_time:8429ms step_avg:33.58ms
step:252/1775 train_time:8450ms step_avg:33.53ms
step:253/1775 train_time:8469ms step_avg:33.47ms
step:254/1775 train_time:8497ms step_avg:33.45ms
step:255/1775 train_time:8530ms step_avg:33.45ms
step:256/1775 train_time:8564ms step_avg:33.45ms
step:257/1775 train_time:8596ms step_avg:33.45ms
step:258/1775 train_time:8629ms step_avg:33.45ms
step:259/1775 train_time:8660ms step_avg:33.44ms
step:260/1775 train_time:8694ms step_avg:33.44ms
step:261/1775 train_time:8725ms step_avg:33.43ms
step:262/1775 train_time:8758ms step_avg:33.43ms
step:263/1775 train_time:8789ms step_avg:33.42ms
step:264/1775 train_time:8822ms step_avg:33.42ms
step:265/1775 train_time:8853ms step_avg:33.41ms
step:266/1775 train_time:8886ms step_avg:33.41ms
step:267/1775 train_time:8917ms step_avg:33.40ms
step:268/1775 train_time:8950ms step_avg:33.40ms
step:269/1775 train_time:8981ms step_avg:33.39ms
step:270/1775 train_time:9014ms step_avg:33.39ms
step:271/1775 train_time:9045ms step_avg:33.38ms
step:272/1775 train_time:9078ms step_avg:33.38ms
step:273/1775 train_time:9109ms step_avg:33.37ms
step:274/1775 train_time:9142ms step_avg:33.37ms
step:275/1775 train_time:9173ms step_avg:33.36ms
step:276/1775 train_time:9207ms step_avg:33.36ms
step:277/1775 train_time:9237ms step_avg:33.35ms
step:278/1775 train_time:9270ms step_avg:33.35ms
step:279/1775 train_time:9301ms step_avg:33.34ms
step:280/1775 train_time:9335ms step_avg:33.34ms
step:281/1775 train_time:9366ms step_avg:33.33ms
step:282/1775 train_time:9400ms step_avg:33.33ms
step:283/1775 train_time:9432ms step_avg:33.33ms
step:284/1775 train_time:9466ms step_avg:33.33ms
step:285/1775 train_time:9497ms step_avg:33.32ms
step:286/1775 train_time:9531ms step_avg:33.33ms
step:287/1775 train_time:9563ms step_avg:33.32ms
step:288/1775 train_time:9597ms step_avg:33.32ms
step:289/1775 train_time:9628ms step_avg:33.31ms
step:290/1775 train_time:9662ms step_avg:33.32ms
step:291/1775 train_time:9694ms step_avg:33.31ms
step:292/1775 train_time:9728ms step_avg:33.31ms
step:293/1775 train_time:9759ms step_avg:33.31ms
step:294/1775 train_time:9793ms step_avg:33.31ms
step:295/1775 train_time:9824ms step_avg:33.30ms
step:296/1775 train_time:9857ms step_avg:33.30ms
step:297/1775 train_time:9888ms step_avg:33.29ms
step:298/1775 train_time:9922ms step_avg:33.29ms
step:299/1775 train_time:9953ms step_avg:33.29ms
step:300/1775 train_time:9986ms step_avg:33.29ms
step:301/1775 train_time:10016ms step_avg:33.28ms
step:302/1775 train_time:10049ms step_avg:33.28ms
step:303/1775 train_time:10081ms step_avg:33.27ms
step:304/1775 train_time:10115ms step_avg:33.27ms
step:305/1775 train_time:10145ms step_avg:33.26ms
step:306/1775 train_time:10179ms step_avg:33.26ms
step:307/1775 train_time:10209ms step_avg:33.25ms
step:308/1775 train_time:10242ms step_avg:33.25ms
step:309/1775 train_time:10273ms step_avg:33.25ms
step:310/1775 train_time:10306ms step_avg:33.25ms
step:311/1775 train_time:10337ms step_avg:33.24ms
step:312/1775 train_time:10371ms step_avg:33.24ms
step:313/1775 train_time:10402ms step_avg:33.23ms
step:314/1775 train_time:10436ms step_avg:33.24ms
step:315/1775 train_time:10467ms step_avg:33.23ms
step:316/1775 train_time:10501ms step_avg:33.23ms
step:317/1775 train_time:10533ms step_avg:33.23ms
step:318/1775 train_time:10567ms step_avg:33.23ms
step:319/1775 train_time:10598ms step_avg:33.22ms
step:320/1775 train_time:10632ms step_avg:33.22ms
step:321/1775 train_time:10663ms step_avg:33.22ms
step:322/1775 train_time:10697ms step_avg:33.22ms
step:323/1775 train_time:10728ms step_avg:33.21ms
step:324/1775 train_time:10762ms step_avg:33.21ms
step:325/1775 train_time:10793ms step_avg:33.21ms
step:326/1775 train_time:10826ms step_avg:33.21ms
step:327/1775 train_time:10858ms step_avg:33.21ms
step:328/1775 train_time:10891ms step_avg:33.21ms
step:329/1775 train_time:10922ms step_avg:33.20ms
step:330/1775 train_time:10956ms step_avg:33.20ms
step:331/1775 train_time:10986ms step_avg:33.19ms
step:332/1775 train_time:11020ms step_avg:33.19ms
step:333/1775 train_time:11050ms step_avg:33.18ms
step:334/1775 train_time:11084ms step_avg:33.18ms
step:335/1775 train_time:11115ms step_avg:33.18ms
step:336/1775 train_time:11148ms step_avg:33.18ms
step:337/1775 train_time:11179ms step_avg:33.17ms
step:338/1775 train_time:11212ms step_avg:33.17ms
step:339/1775 train_time:11243ms step_avg:33.17ms
step:340/1775 train_time:11276ms step_avg:33.16ms
step:341/1775 train_time:11307ms step_avg:33.16ms
step:342/1775 train_time:11341ms step_avg:33.16ms
step:343/1775 train_time:11372ms step_avg:33.15ms
step:344/1775 train_time:11405ms step_avg:33.15ms
step:345/1775 train_time:11437ms step_avg:33.15ms
step:346/1775 train_time:11470ms step_avg:33.15ms
step:347/1775 train_time:11502ms step_avg:33.15ms
step:348/1775 train_time:11536ms step_avg:33.15ms
step:349/1775 train_time:11567ms step_avg:33.14ms
step:350/1775 train_time:11601ms step_avg:33.15ms
step:351/1775 train_time:11632ms step_avg:33.14ms
step:352/1775 train_time:11666ms step_avg:33.14ms
step:353/1775 train_time:11697ms step_avg:33.14ms
step:354/1775 train_time:11731ms step_avg:33.14ms
step:355/1775 train_time:11762ms step_avg:33.13ms
step:356/1775 train_time:11795ms step_avg:33.13ms
step:357/1775 train_time:11827ms step_avg:33.13ms
step:358/1775 train_time:11860ms step_avg:33.13ms
step:359/1775 train_time:11891ms step_avg:33.12ms
step:360/1775 train_time:11924ms step_avg:33.12ms
step:361/1775 train_time:11955ms step_avg:33.12ms
step:362/1775 train_time:11988ms step_avg:33.12ms
step:363/1775 train_time:12019ms step_avg:33.11ms
step:364/1775 train_time:12053ms step_avg:33.11ms
step:365/1775 train_time:12083ms step_avg:33.11ms
step:366/1775 train_time:12116ms step_avg:33.10ms
step:367/1775 train_time:12148ms step_avg:33.10ms
step:368/1775 train_time:12181ms step_avg:33.10ms
step:369/1775 train_time:12212ms step_avg:33.09ms
step:370/1775 train_time:12245ms step_avg:33.10ms
step:371/1775 train_time:12277ms step_avg:33.09ms
step:372/1775 train_time:12310ms step_avg:33.09ms
step:373/1775 train_time:12341ms step_avg:33.09ms
step:374/1775 train_time:12375ms step_avg:33.09ms
step:375/1775 train_time:12406ms step_avg:33.08ms
step:376/1775 train_time:12439ms step_avg:33.08ms
step:377/1775 train_time:12469ms step_avg:33.08ms
step:378/1775 train_time:12503ms step_avg:33.08ms
step:379/1775 train_time:12534ms step_avg:33.07ms
step:380/1775 train_time:12567ms step_avg:33.07ms
step:381/1775 train_time:12599ms step_avg:33.07ms
step:382/1775 train_time:12632ms step_avg:33.07ms
step:383/1775 train_time:12664ms step_avg:33.07ms
step:384/1775 train_time:12697ms step_avg:33.07ms
step:385/1775 train_time:12728ms step_avg:33.06ms
step:386/1775 train_time:12761ms step_avg:33.06ms
step:387/1775 train_time:12793ms step_avg:33.06ms
step:388/1775 train_time:12826ms step_avg:33.06ms
step:389/1775 train_time:12858ms step_avg:33.05ms
step:390/1775 train_time:12892ms step_avg:33.06ms
step:391/1775 train_time:12923ms step_avg:33.05ms
step:392/1775 train_time:12956ms step_avg:33.05ms
step:393/1775 train_time:12987ms step_avg:33.05ms
step:394/1775 train_time:13020ms step_avg:33.05ms
step:395/1775 train_time:13051ms step_avg:33.04ms
step:396/1775 train_time:13085ms step_avg:33.04ms
step:397/1775 train_time:13116ms step_avg:33.04ms
step:398/1775 train_time:13149ms step_avg:33.04ms
step:399/1775 train_time:13180ms step_avg:33.03ms
step:400/1775 train_time:13213ms step_avg:33.03ms
step:401/1775 train_time:13244ms step_avg:33.03ms
step:402/1775 train_time:13277ms step_avg:33.03ms
step:403/1775 train_time:13308ms step_avg:33.02ms
step:404/1775 train_time:13341ms step_avg:33.02ms
step:405/1775 train_time:13373ms step_avg:33.02ms
step:406/1775 train_time:13406ms step_avg:33.02ms
step:407/1775 train_time:13438ms step_avg:33.02ms
step:408/1775 train_time:13471ms step_avg:33.02ms
step:409/1775 train_time:13503ms step_avg:33.01ms
step:410/1775 train_time:13536ms step_avg:33.01ms
step:411/1775 train_time:13567ms step_avg:33.01ms
step:412/1775 train_time:13600ms step_avg:33.01ms
step:413/1775 train_time:13632ms step_avg:33.01ms
step:414/1775 train_time:13665ms step_avg:33.01ms
step:415/1775 train_time:13696ms step_avg:33.00ms
step:416/1775 train_time:13730ms step_avg:33.00ms
step:417/1775 train_time:13761ms step_avg:33.00ms
step:418/1775 train_time:13794ms step_avg:33.00ms
step:419/1775 train_time:13825ms step_avg:33.00ms
step:420/1775 train_time:13859ms step_avg:33.00ms
step:421/1775 train_time:13890ms step_avg:32.99ms
step:422/1775 train_time:13923ms step_avg:32.99ms
step:423/1775 train_time:13954ms step_avg:32.99ms
step:424/1775 train_time:13987ms step_avg:32.99ms
step:425/1775 train_time:14018ms step_avg:32.98ms
step:426/1775 train_time:14052ms step_avg:32.99ms
step:427/1775 train_time:14084ms step_avg:32.98ms
step:428/1775 train_time:14117ms step_avg:32.98ms
step:429/1775 train_time:14148ms step_avg:32.98ms
step:430/1775 train_time:14181ms step_avg:32.98ms
step:431/1775 train_time:14212ms step_avg:32.98ms
step:432/1775 train_time:14245ms step_avg:32.98ms
step:433/1775 train_time:14276ms step_avg:32.97ms
step:434/1775 train_time:14309ms step_avg:32.97ms
step:435/1775 train_time:14341ms step_avg:32.97ms
step:436/1775 train_time:14375ms step_avg:32.97ms
step:437/1775 train_time:14406ms step_avg:32.96ms
step:438/1775 train_time:14439ms step_avg:32.97ms
step:439/1775 train_time:14471ms step_avg:32.96ms
step:440/1775 train_time:14504ms step_avg:32.96ms
step:441/1775 train_time:14536ms step_avg:32.96ms
step:442/1775 train_time:14569ms step_avg:32.96ms
step:443/1775 train_time:14600ms step_avg:32.96ms
step:444/1775 train_time:14634ms step_avg:32.96ms
step:445/1775 train_time:14665ms step_avg:32.95ms
step:446/1775 train_time:14698ms step_avg:32.95ms
step:447/1775 train_time:14729ms step_avg:32.95ms
step:448/1775 train_time:14762ms step_avg:32.95ms
step:449/1775 train_time:14794ms step_avg:32.95ms
step:450/1775 train_time:14827ms step_avg:32.95ms
step:451/1775 train_time:14858ms step_avg:32.95ms
step:452/1775 train_time:14892ms step_avg:32.95ms
step:453/1775 train_time:14923ms step_avg:32.94ms
step:454/1775 train_time:14956ms step_avg:32.94ms
step:455/1775 train_time:14987ms step_avg:32.94ms
step:456/1775 train_time:15021ms step_avg:32.94ms
step:457/1775 train_time:15052ms step_avg:32.94ms
step:458/1775 train_time:15085ms step_avg:32.94ms
step:459/1775 train_time:15116ms step_avg:32.93ms
step:460/1775 train_time:15150ms step_avg:32.93ms
step:461/1775 train_time:15181ms step_avg:32.93ms
step:462/1775 train_time:15214ms step_avg:32.93ms
step:463/1775 train_time:15245ms step_avg:32.93ms
step:464/1775 train_time:15279ms step_avg:32.93ms
step:465/1775 train_time:15310ms step_avg:32.92ms
step:466/1775 train_time:15343ms step_avg:32.93ms
step:467/1775 train_time:15374ms step_avg:32.92ms
step:468/1775 train_time:15407ms step_avg:32.92ms
step:469/1775 train_time:15438ms step_avg:32.92ms
step:470/1775 train_time:15471ms step_avg:32.92ms
step:471/1775 train_time:15502ms step_avg:32.91ms
step:472/1775 train_time:15535ms step_avg:32.91ms
step:473/1775 train_time:15567ms step_avg:32.91ms
step:474/1775 train_time:15601ms step_avg:32.91ms
step:475/1775 train_time:15632ms step_avg:32.91ms
step:476/1775 train_time:15665ms step_avg:32.91ms
step:477/1775 train_time:15696ms step_avg:32.91ms
step:478/1775 train_time:15730ms step_avg:32.91ms
step:479/1775 train_time:15761ms step_avg:32.90ms
step:480/1775 train_time:15795ms step_avg:32.91ms
step:481/1775 train_time:15826ms step_avg:32.90ms
step:482/1775 train_time:15860ms step_avg:32.90ms
step:483/1775 train_time:15891ms step_avg:32.90ms
step:484/1775 train_time:15925ms step_avg:32.90ms
step:485/1775 train_time:15956ms step_avg:32.90ms
step:486/1775 train_time:15989ms step_avg:32.90ms
step:487/1775 train_time:16020ms step_avg:32.89ms
step:488/1775 train_time:16053ms step_avg:32.90ms
step:489/1775 train_time:16084ms step_avg:32.89ms
step:490/1775 train_time:16117ms step_avg:32.89ms
step:491/1775 train_time:16148ms step_avg:32.89ms
step:492/1775 train_time:16182ms step_avg:32.89ms
step:493/1775 train_time:16214ms step_avg:32.89ms
step:494/1775 train_time:16247ms step_avg:32.89ms
step:495/1775 train_time:16278ms step_avg:32.88ms
step:496/1775 train_time:16311ms step_avg:32.89ms
step:497/1775 train_time:16342ms step_avg:32.88ms
step:498/1775 train_time:16376ms step_avg:32.88ms
step:499/1775 train_time:16407ms step_avg:32.88ms
step:500/1775 train_time:16440ms step_avg:32.88ms
step:500/1775 val_loss:4.2776 train_time:16482ms step_avg:32.96ms
step:501/1775 train_time:16503ms step_avg:32.94ms
step:502/1775 train_time:16524ms step_avg:32.92ms
step:503/1775 train_time:16543ms step_avg:32.89ms
step:504/1775 train_time:16571ms step_avg:32.88ms
step:505/1775 train_time:16604ms step_avg:32.88ms
step:506/1775 train_time:16639ms step_avg:32.88ms
step:507/1775 train_time:16671ms step_avg:32.88ms
step:508/1775 train_time:16705ms step_avg:32.88ms
step:509/1775 train_time:16736ms step_avg:32.88ms
step:510/1775 train_time:16769ms step_avg:32.88ms
step:511/1775 train_time:16800ms step_avg:32.88ms
step:512/1775 train_time:16833ms step_avg:32.88ms
step:513/1775 train_time:16864ms step_avg:32.87ms
step:514/1775 train_time:16897ms step_avg:32.87ms
step:515/1775 train_time:16928ms step_avg:32.87ms
step:516/1775 train_time:16961ms step_avg:32.87ms
step:517/1775 train_time:16991ms step_avg:32.87ms
step:518/1775 train_time:17024ms step_avg:32.87ms
step:519/1775 train_time:17055ms step_avg:32.86ms
step:520/1775 train_time:17088ms step_avg:32.86ms
step:521/1775 train_time:17119ms step_avg:32.86ms
step:522/1775 train_time:17152ms step_avg:32.86ms
step:523/1775 train_time:17182ms step_avg:32.85ms
step:524/1775 train_time:17215ms step_avg:32.85ms
step:525/1775 train_time:17246ms step_avg:32.85ms
step:526/1775 train_time:17279ms step_avg:32.85ms
step:527/1775 train_time:17310ms step_avg:32.85ms
step:528/1775 train_time:17343ms step_avg:32.85ms
step:529/1775 train_time:17374ms step_avg:32.84ms
step:530/1775 train_time:17408ms step_avg:32.84ms
step:531/1775 train_time:17440ms step_avg:32.84ms
step:532/1775 train_time:17473ms step_avg:32.84ms
step:533/1775 train_time:17504ms step_avg:32.84ms
step:534/1775 train_time:17538ms step_avg:32.84ms
step:535/1775 train_time:17570ms step_avg:32.84ms
step:536/1775 train_time:17603ms step_avg:32.84ms
step:537/1775 train_time:17635ms step_avg:32.84ms
step:538/1775 train_time:17668ms step_avg:32.84ms
step:539/1775 train_time:17700ms step_avg:32.84ms
step:540/1775 train_time:17733ms step_avg:32.84ms
step:541/1775 train_time:17764ms step_avg:32.84ms
step:542/1775 train_time:17798ms step_avg:32.84ms
step:543/1775 train_time:17830ms step_avg:32.84ms
step:544/1775 train_time:17863ms step_avg:32.84ms
step:545/1775 train_time:17894ms step_avg:32.83ms
step:546/1775 train_time:17927ms step_avg:32.83ms
step:547/1775 train_time:17958ms step_avg:32.83ms
step:548/1775 train_time:17991ms step_avg:32.83ms
step:549/1775 train_time:18022ms step_avg:32.83ms
step:550/1775 train_time:18056ms step_avg:32.83ms
step:551/1775 train_time:18087ms step_avg:32.82ms
step:552/1775 train_time:18120ms step_avg:32.83ms
step:553/1775 train_time:18151ms step_avg:32.82ms
step:554/1775 train_time:18184ms step_avg:32.82ms
step:555/1775 train_time:18215ms step_avg:32.82ms
step:556/1775 train_time:18248ms step_avg:32.82ms
step:557/1775 train_time:18279ms step_avg:32.82ms
step:558/1775 train_time:18313ms step_avg:32.82ms
step:559/1775 train_time:18344ms step_avg:32.82ms
step:560/1775 train_time:18377ms step_avg:32.82ms
step:561/1775 train_time:18408ms step_avg:32.81ms
step:562/1775 train_time:18441ms step_avg:32.81ms
step:563/1775 train_time:18473ms step_avg:32.81ms
step:564/1775 train_time:18507ms step_avg:32.81ms
step:565/1775 train_time:18538ms step_avg:32.81ms
step:566/1775 train_time:18572ms step_avg:32.81ms
step:567/1775 train_time:18603ms step_avg:32.81ms
step:568/1775 train_time:18636ms step_avg:32.81ms
step:569/1775 train_time:18668ms step_avg:32.81ms
step:570/1775 train_time:18702ms step_avg:32.81ms
step:571/1775 train_time:18733ms step_avg:32.81ms
step:572/1775 train_time:18767ms step_avg:32.81ms
step:573/1775 train_time:18798ms step_avg:32.81ms
step:574/1775 train_time:18831ms step_avg:32.81ms
step:575/1775 train_time:18862ms step_avg:32.80ms
step:576/1775 train_time:18895ms step_avg:32.80ms
step:577/1775 train_time:18926ms step_avg:32.80ms
step:578/1775 train_time:18960ms step_avg:32.80ms
step:579/1775 train_time:18991ms step_avg:32.80ms
step:580/1775 train_time:19027ms step_avg:32.80ms
step:581/1775 train_time:19084ms step_avg:32.85ms
step:582/1775 train_time:19144ms step_avg:32.89ms
step:583/1775 train_time:19200ms step_avg:32.93ms
step:584/1775 train_time:19260ms step_avg:32.98ms
step:585/1775 train_time:19317ms step_avg:33.02ms
step:586/1775 train_time:19376ms step_avg:33.07ms
step:587/1775 train_time:19434ms step_avg:33.11ms
step:588/1775 train_time:19494ms step_avg:33.15ms
step:589/1775 train_time:19553ms step_avg:33.20ms
step:590/1775 train_time:19615ms step_avg:33.25ms
step:591/1775 train_time:19672ms step_avg:33.29ms
step:592/1775 train_time:19734ms step_avg:33.33ms
step:593/1775 train_time:19792ms step_avg:33.38ms
step:594/1775 train_time:19853ms step_avg:33.42ms
step:595/1775 train_time:19911ms step_avg:33.46ms
step:596/1775 train_time:19971ms step_avg:33.51ms
step:597/1775 train_time:20029ms step_avg:33.55ms
step:598/1775 train_time:20090ms step_avg:33.60ms
step:599/1775 train_time:20150ms step_avg:33.64ms
step:600/1775 train_time:20211ms step_avg:33.69ms
step:601/1775 train_time:20270ms step_avg:33.73ms
step:602/1775 train_time:20331ms step_avg:33.77ms
step:603/1775 train_time:20388ms step_avg:33.81ms
step:604/1775 train_time:20448ms step_avg:33.85ms
step:605/1775 train_time:20506ms step_avg:33.89ms
step:606/1775 train_time:20567ms step_avg:33.94ms
step:607/1775 train_time:20625ms step_avg:33.98ms
step:608/1775 train_time:20686ms step_avg:34.02ms
step:609/1775 train_time:20744ms step_avg:34.06ms
step:610/1775 train_time:20804ms step_avg:34.10ms
step:611/1775 train_time:20861ms step_avg:34.14ms
step:612/1775 train_time:20922ms step_avg:34.19ms
step:613/1775 train_time:20980ms step_avg:34.23ms
step:614/1775 train_time:21039ms step_avg:34.27ms
step:615/1775 train_time:21098ms step_avg:34.31ms
step:616/1775 train_time:21158ms step_avg:34.35ms
step:617/1775 train_time:21216ms step_avg:34.39ms
step:618/1775 train_time:21276ms step_avg:34.43ms
step:619/1775 train_time:21334ms step_avg:34.47ms
step:620/1775 train_time:21394ms step_avg:34.51ms
step:621/1775 train_time:21452ms step_avg:34.54ms
step:622/1775 train_time:21513ms step_avg:34.59ms
step:623/1775 train_time:21571ms step_avg:34.62ms
step:624/1775 train_time:21633ms step_avg:34.67ms
step:625/1775 train_time:21692ms step_avg:34.71ms
step:626/1775 train_time:21754ms step_avg:34.75ms
step:627/1775 train_time:21813ms step_avg:34.79ms
step:628/1775 train_time:21874ms step_avg:34.83ms
step:629/1775 train_time:21933ms step_avg:34.87ms
step:630/1775 train_time:21992ms step_avg:34.91ms
step:631/1775 train_time:22051ms step_avg:34.95ms
step:632/1775 train_time:22111ms step_avg:34.99ms
step:633/1775 train_time:22169ms step_avg:35.02ms
step:634/1775 train_time:22228ms step_avg:35.06ms
step:635/1775 train_time:22286ms step_avg:35.10ms
step:636/1775 train_time:22347ms step_avg:35.14ms
step:637/1775 train_time:22404ms step_avg:35.17ms
step:638/1775 train_time:22464ms step_avg:35.21ms
step:639/1775 train_time:22522ms step_avg:35.25ms
step:640/1775 train_time:22582ms step_avg:35.29ms
step:641/1775 train_time:22640ms step_avg:35.32ms
step:642/1775 train_time:22700ms step_avg:35.36ms
step:643/1775 train_time:22757ms step_avg:35.39ms
step:644/1775 train_time:22817ms step_avg:35.43ms
step:645/1775 train_time:22877ms step_avg:35.47ms
step:646/1775 train_time:22937ms step_avg:35.51ms
step:647/1775 train_time:22995ms step_avg:35.54ms
step:648/1775 train_time:23055ms step_avg:35.58ms
step:649/1775 train_time:23112ms step_avg:35.61ms
step:650/1775 train_time:23173ms step_avg:35.65ms
step:651/1775 train_time:23231ms step_avg:35.68ms
step:652/1775 train_time:23290ms step_avg:35.72ms
step:653/1775 train_time:23348ms step_avg:35.76ms
step:654/1775 train_time:23410ms step_avg:35.80ms
step:655/1775 train_time:23469ms step_avg:35.83ms
step:656/1775 train_time:23529ms step_avg:35.87ms
step:657/1775 train_time:23588ms step_avg:35.90ms
step:658/1775 train_time:23649ms step_avg:35.94ms
step:659/1775 train_time:23707ms step_avg:35.97ms
step:660/1775 train_time:23767ms step_avg:36.01ms
step:661/1775 train_time:23826ms step_avg:36.05ms
step:662/1775 train_time:23886ms step_avg:36.08ms
step:663/1775 train_time:23944ms step_avg:36.11ms
step:664/1775 train_time:24003ms step_avg:36.15ms
step:665/1775 train_time:24060ms step_avg:36.18ms
step:666/1775 train_time:24121ms step_avg:36.22ms
step:667/1775 train_time:24178ms step_avg:36.25ms
step:668/1775 train_time:24238ms step_avg:36.29ms
step:669/1775 train_time:24296ms step_avg:36.32ms
step:670/1775 train_time:24356ms step_avg:36.35ms
step:671/1775 train_time:24414ms step_avg:36.38ms
step:672/1775 train_time:24475ms step_avg:36.42ms
step:673/1775 train_time:24534ms step_avg:36.45ms
step:674/1775 train_time:24594ms step_avg:36.49ms
step:675/1775 train_time:24652ms step_avg:36.52ms
step:676/1775 train_time:24713ms step_avg:36.56ms
step:677/1775 train_time:24771ms step_avg:36.59ms
step:678/1775 train_time:24832ms step_avg:36.63ms
step:679/1775 train_time:24890ms step_avg:36.66ms
step:680/1775 train_time:24951ms step_avg:36.69ms
step:681/1775 train_time:25010ms step_avg:36.73ms
step:682/1775 train_time:25070ms step_avg:36.76ms
step:683/1775 train_time:25128ms step_avg:36.79ms
step:684/1775 train_time:25188ms step_avg:36.82ms
step:685/1775 train_time:25246ms step_avg:36.86ms
step:686/1775 train_time:25308ms step_avg:36.89ms
step:687/1775 train_time:25365ms step_avg:36.92ms
step:688/1775 train_time:25425ms step_avg:36.96ms
step:689/1775 train_time:25483ms step_avg:36.98ms
step:690/1775 train_time:25542ms step_avg:37.02ms
step:691/1775 train_time:25599ms step_avg:37.05ms
step:692/1775 train_time:25660ms step_avg:37.08ms
step:693/1775 train_time:25717ms step_avg:37.11ms
step:694/1775 train_time:25778ms step_avg:37.14ms
step:695/1775 train_time:25836ms step_avg:37.17ms
step:696/1775 train_time:25896ms step_avg:37.21ms
step:697/1775 train_time:25955ms step_avg:37.24ms
step:698/1775 train_time:26015ms step_avg:37.27ms
step:699/1775 train_time:26073ms step_avg:37.30ms
step:700/1775 train_time:26135ms step_avg:37.34ms
step:701/1775 train_time:26192ms step_avg:37.36ms
step:702/1775 train_time:26253ms step_avg:37.40ms
step:703/1775 train_time:26311ms step_avg:37.43ms
step:704/1775 train_time:26370ms step_avg:37.46ms
step:705/1775 train_time:26429ms step_avg:37.49ms
step:706/1775 train_time:26488ms step_avg:37.52ms
step:707/1775 train_time:26547ms step_avg:37.55ms
step:708/1775 train_time:26608ms step_avg:37.58ms
step:709/1775 train_time:26666ms step_avg:37.61ms
step:710/1775 train_time:26726ms step_avg:37.64ms
step:711/1775 train_time:26784ms step_avg:37.67ms
step:712/1775 train_time:26844ms step_avg:37.70ms
step:713/1775 train_time:26901ms step_avg:37.73ms
step:714/1775 train_time:26961ms step_avg:37.76ms
step:715/1775 train_time:27018ms step_avg:37.79ms
step:716/1775 train_time:27079ms step_avg:37.82ms
step:717/1775 train_time:27137ms step_avg:37.85ms
step:718/1775 train_time:27196ms step_avg:37.88ms
step:719/1775 train_time:27254ms step_avg:37.91ms
step:720/1775 train_time:27315ms step_avg:37.94ms
step:721/1775 train_time:27372ms step_avg:37.96ms
step:722/1775 train_time:27434ms step_avg:38.00ms
step:723/1775 train_time:27492ms step_avg:38.02ms
step:724/1775 train_time:27552ms step_avg:38.06ms
step:725/1775 train_time:27611ms step_avg:38.08ms
step:726/1775 train_time:27672ms step_avg:38.12ms
step:727/1775 train_time:27731ms step_avg:38.14ms
step:728/1775 train_time:27792ms step_avg:38.18ms
step:729/1775 train_time:27850ms step_avg:38.20ms
step:730/1775 train_time:27911ms step_avg:38.23ms
step:731/1775 train_time:27970ms step_avg:38.26ms
step:732/1775 train_time:28031ms step_avg:38.29ms
step:733/1775 train_time:28089ms step_avg:38.32ms
step:734/1775 train_time:28151ms step_avg:38.35ms
step:735/1775 train_time:28208ms step_avg:38.38ms
step:736/1775 train_time:28268ms step_avg:38.41ms
step:737/1775 train_time:28325ms step_avg:38.43ms
step:738/1775 train_time:28386ms step_avg:38.46ms
step:739/1775 train_time:28443ms step_avg:38.49ms
step:740/1775 train_time:28503ms step_avg:38.52ms
step:741/1775 train_time:28560ms step_avg:38.54ms
step:742/1775 train_time:28621ms step_avg:38.57ms
step:743/1775 train_time:28678ms step_avg:38.60ms
step:744/1775 train_time:28738ms step_avg:38.63ms
step:745/1775 train_time:28795ms step_avg:38.65ms
step:746/1775 train_time:28855ms step_avg:38.68ms
step:747/1775 train_time:28914ms step_avg:38.71ms
step:748/1775 train_time:28975ms step_avg:38.74ms
step:749/1775 train_time:29032ms step_avg:38.76ms
step:750/1775 train_time:29094ms step_avg:38.79ms
step:750/1775 val_loss:4.0072 train_time:29163ms step_avg:38.88ms
step:751/1775 train_time:29185ms step_avg:38.86ms
step:752/1775 train_time:29213ms step_avg:38.85ms
step:753/1775 train_time:29271ms step_avg:38.87ms
step:754/1775 train_time:29335ms step_avg:38.91ms
step:755/1775 train_time:29394ms step_avg:38.93ms
step:756/1775 train_time:29455ms step_avg:38.96ms
step:757/1775 train_time:29512ms step_avg:38.99ms
step:758/1775 train_time:29572ms step_avg:39.01ms
step:759/1775 train_time:29629ms step_avg:39.04ms
step:760/1775 train_time:29688ms step_avg:39.06ms
step:761/1775 train_time:29744ms step_avg:39.09ms
step:762/1775 train_time:29804ms step_avg:39.11ms
step:763/1775 train_time:29860ms step_avg:39.14ms
step:764/1775 train_time:29919ms step_avg:39.16ms
step:765/1775 train_time:29976ms step_avg:39.18ms
step:766/1775 train_time:30036ms step_avg:39.21ms
step:767/1775 train_time:30093ms step_avg:39.24ms
step:768/1775 train_time:30155ms step_avg:39.26ms
step:769/1775 train_time:30214ms step_avg:39.29ms
step:770/1775 train_time:30276ms step_avg:39.32ms
step:771/1775 train_time:30334ms step_avg:39.34ms
step:772/1775 train_time:30396ms step_avg:39.37ms
step:773/1775 train_time:30454ms step_avg:39.40ms
step:774/1775 train_time:30514ms step_avg:39.42ms
step:775/1775 train_time:30572ms step_avg:39.45ms
step:776/1775 train_time:30633ms step_avg:39.48ms
step:777/1775 train_time:30690ms step_avg:39.50ms
step:778/1775 train_time:30751ms step_avg:39.53ms
step:779/1775 train_time:30808ms step_avg:39.55ms
step:780/1775 train_time:30868ms step_avg:39.57ms
step:781/1775 train_time:30924ms step_avg:39.59ms
step:782/1775 train_time:30984ms step_avg:39.62ms
step:783/1775 train_time:31040ms step_avg:39.64ms
step:784/1775 train_time:31100ms step_avg:39.67ms
step:785/1775 train_time:31157ms step_avg:39.69ms
step:786/1775 train_time:31217ms step_avg:39.72ms
step:787/1775 train_time:31275ms step_avg:39.74ms
step:788/1775 train_time:31336ms step_avg:39.77ms
step:789/1775 train_time:31394ms step_avg:39.79ms
step:790/1775 train_time:31455ms step_avg:39.82ms
step:791/1775 train_time:31513ms step_avg:39.84ms
step:792/1775 train_time:31573ms step_avg:39.87ms
step:793/1775 train_time:31631ms step_avg:39.89ms
step:794/1775 train_time:31692ms step_avg:39.91ms
step:795/1775 train_time:31750ms step_avg:39.94ms
step:796/1775 train_time:31810ms step_avg:39.96ms
step:797/1775 train_time:31867ms step_avg:39.98ms
step:798/1775 train_time:31926ms step_avg:40.01ms
step:799/1775 train_time:31983ms step_avg:40.03ms
step:800/1775 train_time:32042ms step_avg:40.05ms
step:801/1775 train_time:32100ms step_avg:40.08ms
step:802/1775 train_time:32159ms step_avg:40.10ms
step:803/1775 train_time:32217ms step_avg:40.12ms
step:804/1775 train_time:32277ms step_avg:40.15ms
step:805/1775 train_time:32335ms step_avg:40.17ms
step:806/1775 train_time:32396ms step_avg:40.19ms
step:807/1775 train_time:32454ms step_avg:40.22ms
step:808/1775 train_time:32514ms step_avg:40.24ms
step:809/1775 train_time:32572ms step_avg:40.26ms
step:810/1775 train_time:32633ms step_avg:40.29ms
step:811/1775 train_time:32691ms step_avg:40.31ms
step:812/1775 train_time:32751ms step_avg:40.33ms
step:813/1775 train_time:32810ms step_avg:40.36ms
step:814/1775 train_time:32870ms step_avg:40.38ms
step:815/1775 train_time:32928ms step_avg:40.40ms
step:816/1775 train_time:32988ms step_avg:40.43ms
step:817/1775 train_time:33045ms step_avg:40.45ms
step:818/1775 train_time:33105ms step_avg:40.47ms
step:819/1775 train_time:33163ms step_avg:40.49ms
step:820/1775 train_time:33223ms step_avg:40.52ms
step:821/1775 train_time:33281ms step_avg:40.54ms
step:822/1775 train_time:33342ms step_avg:40.56ms
step:823/1775 train_time:33400ms step_avg:40.58ms
step:824/1775 train_time:33460ms step_avg:40.61ms
step:825/1775 train_time:33518ms step_avg:40.63ms
step:826/1775 train_time:33578ms step_avg:40.65ms
step:827/1775 train_time:33636ms step_avg:40.67ms
step:828/1775 train_time:33697ms step_avg:40.70ms
step:829/1775 train_time:33755ms step_avg:40.72ms
step:830/1775 train_time:33816ms step_avg:40.74ms
step:831/1775 train_time:33874ms step_avg:40.76ms
step:832/1775 train_time:33935ms step_avg:40.79ms
step:833/1775 train_time:33993ms step_avg:40.81ms
step:834/1775 train_time:34054ms step_avg:40.83ms
step:835/1775 train_time:34113ms step_avg:40.85ms
step:836/1775 train_time:34173ms step_avg:40.88ms
step:837/1775 train_time:34232ms step_avg:40.90ms
step:838/1775 train_time:34292ms step_avg:40.92ms
step:839/1775 train_time:34350ms step_avg:40.94ms
step:840/1775 train_time:34411ms step_avg:40.97ms
step:841/1775 train_time:34468ms step_avg:40.98ms
step:842/1775 train_time:34527ms step_avg:41.01ms
step:843/1775 train_time:34586ms step_avg:41.03ms
step:844/1775 train_time:34646ms step_avg:41.05ms
step:845/1775 train_time:34703ms step_avg:41.07ms
step:846/1775 train_time:34764ms step_avg:41.09ms
step:847/1775 train_time:34822ms step_avg:41.11ms
step:848/1775 train_time:34882ms step_avg:41.13ms
step:849/1775 train_time:34939ms step_avg:41.15ms
step:850/1775 train_time:35000ms step_avg:41.18ms
step:851/1775 train_time:35059ms step_avg:41.20ms
step:852/1775 train_time:35119ms step_avg:41.22ms
step:853/1775 train_time:35177ms step_avg:41.24ms
step:854/1775 train_time:35237ms step_avg:41.26ms
step:855/1775 train_time:35296ms step_avg:41.28ms
step:856/1775 train_time:35357ms step_avg:41.30ms
step:857/1775 train_time:35414ms step_avg:41.32ms
step:858/1775 train_time:35475ms step_avg:41.35ms
step:859/1775 train_time:35534ms step_avg:41.37ms
step:860/1775 train_time:35593ms step_avg:41.39ms
step:861/1775 train_time:35652ms step_avg:41.41ms
step:862/1775 train_time:35711ms step_avg:41.43ms
step:863/1775 train_time:35770ms step_avg:41.45ms
step:864/1775 train_time:35830ms step_avg:41.47ms
step:865/1775 train_time:35888ms step_avg:41.49ms
step:866/1775 train_time:35947ms step_avg:41.51ms
step:867/1775 train_time:36004ms step_avg:41.53ms
step:868/1775 train_time:36065ms step_avg:41.55ms
step:869/1775 train_time:36123ms step_avg:41.57ms
step:870/1775 train_time:36183ms step_avg:41.59ms
step:871/1775 train_time:36240ms step_avg:41.61ms
step:872/1775 train_time:36301ms step_avg:41.63ms
step:873/1775 train_time:36358ms step_avg:41.65ms
step:874/1775 train_time:36418ms step_avg:41.67ms
step:875/1775 train_time:36475ms step_avg:41.69ms
step:876/1775 train_time:36537ms step_avg:41.71ms
step:877/1775 train_time:36595ms step_avg:41.73ms
step:878/1775 train_time:36655ms step_avg:41.75ms
step:879/1775 train_time:36714ms step_avg:41.77ms
step:880/1775 train_time:36774ms step_avg:41.79ms
step:881/1775 train_time:36832ms step_avg:41.81ms
step:882/1775 train_time:36893ms step_avg:41.83ms
step:883/1775 train_time:36952ms step_avg:41.85ms
step:884/1775 train_time:37013ms step_avg:41.87ms
step:885/1775 train_time:37072ms step_avg:41.89ms
step:886/1775 train_time:37133ms step_avg:41.91ms
step:887/1775 train_time:37192ms step_avg:41.93ms
step:888/1775 train_time:37253ms step_avg:41.95ms
step:889/1775 train_time:37310ms step_avg:41.97ms
step:890/1775 train_time:37370ms step_avg:41.99ms
step:891/1775 train_time:37426ms step_avg:42.00ms
step:892/1775 train_time:37485ms step_avg:42.02ms
step:893/1775 train_time:37543ms step_avg:42.04ms
step:894/1775 train_time:37603ms step_avg:42.06ms
step:895/1775 train_time:37661ms step_avg:42.08ms
step:896/1775 train_time:37720ms step_avg:42.10ms
step:897/1775 train_time:37778ms step_avg:42.12ms
step:898/1775 train_time:37838ms step_avg:42.14ms
step:899/1775 train_time:37896ms step_avg:42.15ms
step:900/1775 train_time:37957ms step_avg:42.17ms
step:901/1775 train_time:38015ms step_avg:42.19ms
step:902/1775 train_time:38075ms step_avg:42.21ms
step:903/1775 train_time:38134ms step_avg:42.23ms
step:904/1775 train_time:38194ms step_avg:42.25ms
step:905/1775 train_time:38252ms step_avg:42.27ms
step:906/1775 train_time:38313ms step_avg:42.29ms
step:907/1775 train_time:38371ms step_avg:42.30ms
step:908/1775 train_time:38431ms step_avg:42.32ms
step:909/1775 train_time:38489ms step_avg:42.34ms
step:910/1775 train_time:38548ms step_avg:42.36ms
step:911/1775 train_time:38606ms step_avg:42.38ms
step:912/1775 train_time:38666ms step_avg:42.40ms
step:913/1775 train_time:38722ms step_avg:42.41ms
step:914/1775 train_time:38783ms step_avg:42.43ms
step:915/1775 train_time:38840ms step_avg:42.45ms
step:916/1775 train_time:38901ms step_avg:42.47ms
step:917/1775 train_time:38958ms step_avg:42.48ms
step:918/1775 train_time:39019ms step_avg:42.50ms
step:919/1775 train_time:39076ms step_avg:42.52ms
step:920/1775 train_time:39137ms step_avg:42.54ms
step:921/1775 train_time:39196ms step_avg:42.56ms
step:922/1775 train_time:39257ms step_avg:42.58ms
step:923/1775 train_time:39315ms step_avg:42.59ms
step:924/1775 train_time:39375ms step_avg:42.61ms
step:925/1775 train_time:39432ms step_avg:42.63ms
step:926/1775 train_time:39493ms step_avg:42.65ms
step:927/1775 train_time:39553ms step_avg:42.67ms
step:928/1775 train_time:39613ms step_avg:42.69ms
step:929/1775 train_time:39671ms step_avg:42.70ms
step:930/1775 train_time:39731ms step_avg:42.72ms
step:931/1775 train_time:39790ms step_avg:42.74ms
step:932/1775 train_time:39850ms step_avg:42.76ms
step:933/1775 train_time:39908ms step_avg:42.77ms
step:934/1775 train_time:39967ms step_avg:42.79ms
step:935/1775 train_time:40024ms step_avg:42.81ms
step:936/1775 train_time:40085ms step_avg:42.83ms
step:937/1775 train_time:40142ms step_avg:42.84ms
step:938/1775 train_time:40203ms step_avg:42.86ms
step:939/1775 train_time:40260ms step_avg:42.88ms
step:940/1775 train_time:40321ms step_avg:42.90ms
step:941/1775 train_time:40379ms step_avg:42.91ms
step:942/1775 train_time:40440ms step_avg:42.93ms
step:943/1775 train_time:40498ms step_avg:42.95ms
step:944/1775 train_time:40559ms step_avg:42.96ms
step:945/1775 train_time:40616ms step_avg:42.98ms
step:946/1775 train_time:40677ms step_avg:43.00ms
step:947/1775 train_time:40735ms step_avg:43.01ms
step:948/1775 train_time:40795ms step_avg:43.03ms
step:949/1775 train_time:40854ms step_avg:43.05ms
step:950/1775 train_time:40914ms step_avg:43.07ms
step:951/1775 train_time:40972ms step_avg:43.08ms
step:952/1775 train_time:41033ms step_avg:43.10ms
step:953/1775 train_time:41091ms step_avg:43.12ms
step:954/1775 train_time:41152ms step_avg:43.14ms
step:955/1775 train_time:41210ms step_avg:43.15ms
step:956/1775 train_time:41271ms step_avg:43.17ms
step:957/1775 train_time:41329ms step_avg:43.19ms
step:958/1775 train_time:41388ms step_avg:43.20ms
step:959/1775 train_time:41445ms step_avg:43.22ms
step:960/1775 train_time:41505ms step_avg:43.23ms
step:961/1775 train_time:41563ms step_avg:43.25ms
step:962/1775 train_time:41622ms step_avg:43.27ms
step:963/1775 train_time:41679ms step_avg:43.28ms
step:964/1775 train_time:41740ms step_avg:43.30ms
step:965/1775 train_time:41798ms step_avg:43.31ms
step:966/1775 train_time:41858ms step_avg:43.33ms
step:967/1775 train_time:41916ms step_avg:43.35ms
step:968/1775 train_time:41975ms step_avg:43.36ms
step:969/1775 train_time:42034ms step_avg:43.38ms
step:970/1775 train_time:42094ms step_avg:43.40ms
step:971/1775 train_time:42152ms step_avg:43.41ms
step:972/1775 train_time:42213ms step_avg:43.43ms
step:973/1775 train_time:42271ms step_avg:43.44ms
step:974/1775 train_time:42332ms step_avg:43.46ms
step:975/1775 train_time:42391ms step_avg:43.48ms
step:976/1775 train_time:42451ms step_avg:43.49ms
step:977/1775 train_time:42509ms step_avg:43.51ms
step:978/1775 train_time:42569ms step_avg:43.53ms
step:979/1775 train_time:42625ms step_avg:43.54ms
step:980/1775 train_time:42686ms step_avg:43.56ms
step:981/1775 train_time:42744ms step_avg:43.57ms
step:982/1775 train_time:42805ms step_avg:43.59ms
step:983/1775 train_time:42862ms step_avg:43.60ms
step:984/1775 train_time:42921ms step_avg:43.62ms
step:985/1775 train_time:42979ms step_avg:43.63ms
step:986/1775 train_time:43039ms step_avg:43.65ms
step:987/1775 train_time:43098ms step_avg:43.67ms
step:988/1775 train_time:43159ms step_avg:43.68ms
step:989/1775 train_time:43216ms step_avg:43.70ms
step:990/1775 train_time:43277ms step_avg:43.71ms
step:991/1775 train_time:43335ms step_avg:43.73ms
step:992/1775 train_time:43396ms step_avg:43.75ms
step:993/1775 train_time:43454ms step_avg:43.76ms
step:994/1775 train_time:43514ms step_avg:43.78ms
step:995/1775 train_time:43572ms step_avg:43.79ms
step:996/1775 train_time:43634ms step_avg:43.81ms
step:997/1775 train_time:43691ms step_avg:43.82ms
step:998/1775 train_time:43751ms step_avg:43.84ms
step:999/1775 train_time:43810ms step_avg:43.85ms
step:1000/1775 train_time:43869ms step_avg:43.87ms
step:1000/1775 val_loss:3.7313 train_time:43938ms step_avg:43.94ms
step:1001/1775 train_time:43961ms step_avg:43.92ms
step:1002/1775 train_time:43987ms step_avg:43.90ms
step:1003/1775 train_time:44045ms step_avg:43.91ms
step:1004/1775 train_time:44111ms step_avg:43.93ms
step:1005/1775 train_time:44170ms step_avg:43.95ms
step:1006/1775 train_time:44230ms step_avg:43.97ms
step:1007/1775 train_time:44287ms step_avg:43.98ms
step:1008/1775 train_time:44346ms step_avg:43.99ms
step:1009/1775 train_time:44404ms step_avg:44.01ms
step:1010/1775 train_time:44463ms step_avg:44.02ms
step:1011/1775 train_time:44521ms step_avg:44.04ms
step:1012/1775 train_time:44581ms step_avg:44.05ms
step:1013/1775 train_time:44637ms step_avg:44.06ms
step:1014/1775 train_time:44696ms step_avg:44.08ms
step:1015/1775 train_time:44751ms step_avg:44.09ms
step:1016/1775 train_time:44810ms step_avg:44.10ms
step:1017/1775 train_time:44868ms step_avg:44.12ms
step:1018/1775 train_time:44928ms step_avg:44.13ms
step:1019/1775 train_time:44988ms step_avg:44.15ms
step:1020/1775 train_time:45050ms step_avg:44.17ms
step:1021/1775 train_time:45108ms step_avg:44.18ms
step:1022/1775 train_time:45169ms step_avg:44.20ms
step:1023/1775 train_time:45227ms step_avg:44.21ms
step:1024/1775 train_time:45288ms step_avg:44.23ms
step:1025/1775 train_time:45345ms step_avg:44.24ms
step:1026/1775 train_time:45405ms step_avg:44.25ms
step:1027/1775 train_time:45463ms step_avg:44.27ms
step:1028/1775 train_time:45522ms step_avg:44.28ms
step:1029/1775 train_time:45580ms step_avg:44.30ms
step:1030/1775 train_time:45640ms step_avg:44.31ms
step:1031/1775 train_time:45697ms step_avg:44.32ms
step:1032/1775 train_time:45756ms step_avg:44.34ms
step:1033/1775 train_time:45813ms step_avg:44.35ms
step:1034/1775 train_time:45873ms step_avg:44.36ms
step:1035/1775 train_time:45931ms step_avg:44.38ms
step:1036/1775 train_time:45991ms step_avg:44.39ms
step:1037/1775 train_time:46049ms step_avg:44.41ms
step:1038/1775 train_time:46110ms step_avg:44.42ms
step:1039/1775 train_time:46167ms step_avg:44.43ms
step:1040/1775 train_time:46228ms step_avg:44.45ms
step:1041/1775 train_time:46286ms step_avg:44.46ms
step:1042/1775 train_time:46346ms step_avg:44.48ms
step:1043/1775 train_time:46404ms step_avg:44.49ms
step:1044/1775 train_time:46464ms step_avg:44.51ms
step:1045/1775 train_time:46522ms step_avg:44.52ms
step:1046/1775 train_time:46583ms step_avg:44.53ms
step:1047/1775 train_time:46640ms step_avg:44.55ms
step:1048/1775 train_time:46700ms step_avg:44.56ms
step:1049/1775 train_time:46757ms step_avg:44.57ms
step:1050/1775 train_time:46817ms step_avg:44.59ms
step:1051/1775 train_time:46875ms step_avg:44.60ms
step:1052/1775 train_time:46935ms step_avg:44.61ms
step:1053/1775 train_time:46992ms step_avg:44.63ms
step:1054/1775 train_time:47052ms step_avg:44.64ms
step:1055/1775 train_time:47109ms step_avg:44.65ms
step:1056/1775 train_time:47169ms step_avg:44.67ms
step:1057/1775 train_time:47227ms step_avg:44.68ms
step:1058/1775 train_time:47288ms step_avg:44.70ms
step:1059/1775 train_time:47345ms step_avg:44.71ms
step:1060/1775 train_time:47405ms step_avg:44.72ms
step:1061/1775 train_time:47463ms step_avg:44.73ms
step:1062/1775 train_time:47524ms step_avg:44.75ms
step:1063/1775 train_time:47581ms step_avg:44.76ms
step:1064/1775 train_time:47642ms step_avg:44.78ms
step:1065/1775 train_time:47700ms step_avg:44.79ms
step:1066/1775 train_time:47760ms step_avg:44.80ms
step:1067/1775 train_time:47819ms step_avg:44.82ms
step:1068/1775 train_time:47879ms step_avg:44.83ms
step:1069/1775 train_time:47936ms step_avg:44.84ms
step:1070/1775 train_time:47997ms step_avg:44.86ms
step:1071/1775 train_time:48053ms step_avg:44.87ms
step:1072/1775 train_time:48116ms step_avg:44.88ms
step:1073/1775 train_time:48173ms step_avg:44.90ms
step:1074/1775 train_time:48233ms step_avg:44.91ms
step:1075/1775 train_time:48290ms step_avg:44.92ms
step:1076/1775 train_time:48350ms step_avg:44.93ms
step:1077/1775 train_time:48408ms step_avg:44.95ms
step:1078/1775 train_time:48469ms step_avg:44.96ms
step:1079/1775 train_time:48528ms step_avg:44.97ms
step:1080/1775 train_time:48587ms step_avg:44.99ms
step:1081/1775 train_time:48646ms step_avg:45.00ms
step:1082/1775 train_time:48707ms step_avg:45.02ms
step:1083/1775 train_time:48764ms step_avg:45.03ms
step:1084/1775 train_time:48824ms step_avg:45.04ms
step:1085/1775 train_time:48886ms step_avg:45.06ms
step:1086/1775 train_time:48945ms step_avg:45.07ms
step:1087/1775 train_time:49003ms step_avg:45.08ms
step:1088/1775 train_time:49065ms step_avg:45.10ms
step:1089/1775 train_time:49125ms step_avg:45.11ms
step:1090/1775 train_time:49185ms step_avg:45.12ms
step:1091/1775 train_time:49242ms step_avg:45.13ms
step:1092/1775 train_time:49302ms step_avg:45.15ms
step:1093/1775 train_time:49360ms step_avg:45.16ms
step:1094/1775 train_time:49421ms step_avg:45.17ms
step:1095/1775 train_time:49478ms step_avg:45.19ms
step:1096/1775 train_time:49538ms step_avg:45.20ms
step:1097/1775 train_time:49595ms step_avg:45.21ms
step:1098/1775 train_time:49654ms step_avg:45.22ms
step:1099/1775 train_time:49712ms step_avg:45.23ms
step:1100/1775 train_time:49772ms step_avg:45.25ms
step:1101/1775 train_time:49831ms step_avg:45.26ms
step:1102/1775 train_time:49891ms step_avg:45.27ms
step:1103/1775 train_time:49948ms step_avg:45.28ms
step:1104/1775 train_time:50008ms step_avg:45.30ms
step:1105/1775 train_time:50066ms step_avg:45.31ms
step:1106/1775 train_time:50127ms step_avg:45.32ms
step:1107/1775 train_time:50185ms step_avg:45.33ms
step:1108/1775 train_time:50245ms step_avg:45.35ms
step:1109/1775 train_time:50302ms step_avg:45.36ms
step:1110/1775 train_time:50361ms step_avg:45.37ms
step:1111/1775 train_time:50419ms step_avg:45.38ms
step:1112/1775 train_time:50480ms step_avg:45.40ms
step:1113/1775 train_time:50538ms step_avg:45.41ms
step:1114/1775 train_time:50597ms step_avg:45.42ms
step:1115/1775 train_time:50654ms step_avg:45.43ms
step:1116/1775 train_time:50714ms step_avg:45.44ms
step:1117/1775 train_time:50772ms step_avg:45.45ms
step:1118/1775 train_time:50832ms step_avg:45.47ms
step:1119/1775 train_time:50888ms step_avg:45.48ms
step:1120/1775 train_time:50949ms step_avg:45.49ms
step:1121/1775 train_time:51007ms step_avg:45.50ms
step:1122/1775 train_time:51067ms step_avg:45.51ms
step:1123/1775 train_time:51125ms step_avg:45.53ms
step:1124/1775 train_time:51185ms step_avg:45.54ms
step:1125/1775 train_time:51242ms step_avg:45.55ms
step:1126/1775 train_time:51303ms step_avg:45.56ms
step:1127/1775 train_time:51362ms step_avg:45.57ms
step:1128/1775 train_time:51423ms step_avg:45.59ms
step:1129/1775 train_time:51481ms step_avg:45.60ms
step:1130/1775 train_time:51541ms step_avg:45.61ms
step:1131/1775 train_time:51598ms step_avg:45.62ms
step:1132/1775 train_time:51657ms step_avg:45.63ms
step:1133/1775 train_time:51714ms step_avg:45.64ms
step:1134/1775 train_time:51775ms step_avg:45.66ms
step:1135/1775 train_time:51832ms step_avg:45.67ms
step:1136/1775 train_time:51892ms step_avg:45.68ms
step:1137/1775 train_time:51949ms step_avg:45.69ms
step:1138/1775 train_time:52008ms step_avg:45.70ms
step:1139/1775 train_time:52066ms step_avg:45.71ms
step:1140/1775 train_time:52127ms step_avg:45.73ms
step:1141/1775 train_time:52185ms step_avg:45.74ms
step:1142/1775 train_time:52245ms step_avg:45.75ms
step:1143/1775 train_time:52303ms step_avg:45.76ms
step:1144/1775 train_time:52363ms step_avg:45.77ms
step:1145/1775 train_time:52422ms step_avg:45.78ms
step:1146/1775 train_time:52483ms step_avg:45.80ms
step:1147/1775 train_time:52541ms step_avg:45.81ms
step:1148/1775 train_time:52602ms step_avg:45.82ms
step:1149/1775 train_time:52660ms step_avg:45.83ms
step:1150/1775 train_time:52720ms step_avg:45.84ms
step:1151/1775 train_time:52779ms step_avg:45.85ms
step:1152/1775 train_time:52838ms step_avg:45.87ms
step:1153/1775 train_time:52895ms step_avg:45.88ms
step:1154/1775 train_time:52955ms step_avg:45.89ms
step:1155/1775 train_time:53011ms step_avg:45.90ms
step:1156/1775 train_time:53072ms step_avg:45.91ms
step:1157/1775 train_time:53130ms step_avg:45.92ms
step:1158/1775 train_time:53194ms step_avg:45.94ms
step:1159/1775 train_time:53277ms step_avg:45.97ms
step:1160/1775 train_time:53361ms step_avg:46.00ms
step:1161/1775 train_time:53446ms step_avg:46.03ms
step:1162/1775 train_time:53532ms step_avg:46.07ms
step:1163/1775 train_time:53617ms step_avg:46.10ms
step:1164/1775 train_time:53703ms step_avg:46.14ms
step:1165/1775 train_time:53788ms step_avg:46.17ms
step:1166/1775 train_time:53873ms step_avg:46.20ms
step:1167/1775 train_time:53958ms step_avg:46.24ms
step:1168/1775 train_time:54043ms step_avg:46.27ms
step:1169/1775 train_time:54126ms step_avg:46.30ms
step:1170/1775 train_time:54213ms step_avg:46.34ms
step:1171/1775 train_time:54295ms step_avg:46.37ms
step:1172/1775 train_time:54379ms step_avg:46.40ms
step:1173/1775 train_time:54462ms step_avg:46.43ms
step:1174/1775 train_time:54549ms step_avg:46.46ms
step:1175/1775 train_time:54633ms step_avg:46.50ms
step:1176/1775 train_time:54720ms step_avg:46.53ms
step:1177/1775 train_time:54804ms step_avg:46.56ms
step:1178/1775 train_time:54890ms step_avg:46.60ms
step:1179/1775 train_time:54973ms step_avg:46.63ms
step:1180/1775 train_time:55059ms step_avg:46.66ms
step:1181/1775 train_time:55142ms step_avg:46.69ms
step:1182/1775 train_time:55227ms step_avg:46.72ms
step:1183/1775 train_time:55311ms step_avg:46.75ms
step:1184/1775 train_time:55396ms step_avg:46.79ms
step:1185/1775 train_time:55479ms step_avg:46.82ms
step:1186/1775 train_time:55567ms step_avg:46.85ms
step:1187/1775 train_time:55651ms step_avg:46.88ms
step:1188/1775 train_time:55737ms step_avg:46.92ms
step:1189/1775 train_time:55820ms step_avg:46.95ms
step:1190/1775 train_time:55906ms step_avg:46.98ms
step:1191/1775 train_time:55990ms step_avg:47.01ms
step:1192/1775 train_time:56076ms step_avg:47.04ms
step:1193/1775 train_time:56160ms step_avg:47.07ms
step:1194/1775 train_time:56245ms step_avg:47.11ms
step:1195/1775 train_time:56328ms step_avg:47.14ms
step:1196/1775 train_time:56414ms step_avg:47.17ms
step:1197/1775 train_time:56497ms step_avg:47.20ms
step:1198/1775 train_time:56582ms step_avg:47.23ms
step:1199/1775 train_time:56665ms step_avg:47.26ms
step:1200/1775 train_time:56752ms step_avg:47.29ms
step:1201/1775 train_time:56836ms step_avg:47.32ms
step:1202/1775 train_time:56922ms step_avg:47.36ms
step:1203/1775 train_time:57004ms step_avg:47.39ms
step:1204/1775 train_time:57091ms step_avg:47.42ms
step:1205/1775 train_time:57173ms step_avg:47.45ms
step:1206/1775 train_time:57261ms step_avg:47.48ms
step:1207/1775 train_time:57344ms step_avg:47.51ms
step:1208/1775 train_time:57429ms step_avg:47.54ms
step:1209/1775 train_time:57512ms step_avg:47.57ms
step:1210/1775 train_time:57598ms step_avg:47.60ms
step:1211/1775 train_time:57680ms step_avg:47.63ms
step:1212/1775 train_time:57767ms step_avg:47.66ms
step:1213/1775 train_time:57852ms step_avg:47.69ms
step:1214/1775 train_time:57937ms step_avg:47.72ms
step:1215/1775 train_time:58020ms step_avg:47.75ms
step:1216/1775 train_time:58108ms step_avg:47.79ms
step:1217/1775 train_time:58191ms step_avg:47.82ms
step:1218/1775 train_time:58277ms step_avg:47.85ms
step:1219/1775 train_time:58360ms step_avg:47.88ms
step:1220/1775 train_time:58446ms step_avg:47.91ms
step:1221/1775 train_time:58529ms step_avg:47.94ms
step:1222/1775 train_time:58616ms step_avg:47.97ms
step:1223/1775 train_time:58699ms step_avg:48.00ms
step:1224/1775 train_time:58785ms step_avg:48.03ms
step:1225/1775 train_time:58868ms step_avg:48.06ms
step:1226/1775 train_time:58956ms step_avg:48.09ms
step:1227/1775 train_time:59039ms step_avg:48.12ms
step:1228/1775 train_time:59124ms step_avg:48.15ms
step:1229/1775 train_time:59208ms step_avg:48.18ms
step:1230/1775 train_time:59293ms step_avg:48.21ms
step:1231/1775 train_time:59376ms step_avg:48.23ms
step:1232/1775 train_time:59462ms step_avg:48.26ms
step:1233/1775 train_time:59545ms step_avg:48.29ms
step:1234/1775 train_time:59631ms step_avg:48.32ms
step:1235/1775 train_time:59715ms step_avg:48.35ms
step:1236/1775 train_time:59800ms step_avg:48.38ms
step:1237/1775 train_time:59884ms step_avg:48.41ms
step:1238/1775 train_time:59970ms step_avg:48.44ms
step:1239/1775 train_time:60055ms step_avg:48.47ms
step:1240/1775 train_time:60141ms step_avg:48.50ms
step:1241/1775 train_time:60224ms step_avg:48.53ms
step:1242/1775 train_time:60310ms step_avg:48.56ms
step:1243/1775 train_time:60392ms step_avg:48.59ms
step:1244/1775 train_time:60479ms step_avg:48.62ms
step:1245/1775 train_time:60561ms step_avg:48.64ms
step:1246/1775 train_time:60648ms step_avg:48.67ms
step:1247/1775 train_time:60732ms step_avg:48.70ms
step:1248/1775 train_time:60818ms step_avg:48.73ms
step:1249/1775 train_time:60900ms step_avg:48.76ms
step:1250/1775 train_time:60986ms step_avg:48.79ms
step:1250/1775 val_loss:3.5062 train_time:61085ms step_avg:48.87ms
step:1251/1775 train_time:61108ms step_avg:48.85ms
step:1252/1775 train_time:61160ms step_avg:48.85ms
step:1253/1775 train_time:61247ms step_avg:48.88ms
step:1254/1775 train_time:61333ms step_avg:48.91ms
step:1255/1775 train_time:61416ms step_avg:48.94ms
step:1256/1775 train_time:61502ms step_avg:48.97ms
step:1257/1775 train_time:61583ms step_avg:48.99ms
step:1258/1775 train_time:61670ms step_avg:49.02ms
step:1259/1775 train_time:61751ms step_avg:49.05ms
step:1260/1775 train_time:61836ms step_avg:49.08ms
step:1261/1775 train_time:61918ms step_avg:49.10ms
step:1262/1775 train_time:62003ms step_avg:49.13ms
step:1263/1775 train_time:62087ms step_avg:49.16ms
step:1264/1775 train_time:62177ms step_avg:49.19ms
step:1265/1775 train_time:62262ms step_avg:49.22ms
step:1266/1775 train_time:62350ms step_avg:49.25ms
step:1267/1775 train_time:62434ms step_avg:49.28ms
step:1268/1775 train_time:62518ms step_avg:49.30ms
step:1269/1775 train_time:62601ms step_avg:49.33ms
step:1270/1775 train_time:62686ms step_avg:49.36ms
step:1271/1775 train_time:62768ms step_avg:49.38ms
step:1272/1775 train_time:62853ms step_avg:49.41ms
step:1273/1775 train_time:62935ms step_avg:49.44ms
step:1274/1775 train_time:63020ms step_avg:49.47ms
step:1275/1775 train_time:63104ms step_avg:49.49ms
step:1276/1775 train_time:63192ms step_avg:49.52ms
step:1277/1775 train_time:63277ms step_avg:49.55ms
step:1278/1775 train_time:63363ms step_avg:49.58ms
step:1279/1775 train_time:63447ms step_avg:49.61ms
step:1280/1775 train_time:63532ms step_avg:49.63ms
step:1281/1775 train_time:63615ms step_avg:49.66ms
step:1282/1775 train_time:63701ms step_avg:49.69ms
step:1283/1775 train_time:63782ms step_avg:49.71ms
step:1284/1775 train_time:63867ms step_avg:49.74ms
step:1285/1775 train_time:63951ms step_avg:49.77ms
step:1286/1775 train_time:64038ms step_avg:49.80ms
step:1287/1775 train_time:64121ms step_avg:49.82ms
step:1288/1775 train_time:64208ms step_avg:49.85ms
step:1289/1775 train_time:64294ms step_avg:49.88ms
step:1290/1775 train_time:64380ms step_avg:49.91ms
step:1291/1775 train_time:64465ms step_avg:49.93ms
step:1292/1775 train_time:64549ms step_avg:49.96ms
step:1293/1775 train_time:64633ms step_avg:49.99ms
step:1294/1775 train_time:64718ms step_avg:50.01ms
step:1295/1775 train_time:64801ms step_avg:50.04ms
step:1296/1775 train_time:64885ms step_avg:50.07ms
step:1297/1775 train_time:64969ms step_avg:50.09ms
step:1298/1775 train_time:65056ms step_avg:50.12ms
step:1299/1775 train_time:65140ms step_avg:50.15ms
step:1300/1775 train_time:65228ms step_avg:50.18ms
step:1301/1775 train_time:65312ms step_avg:50.20ms
step:1302/1775 train_time:65400ms step_avg:50.23ms
step:1303/1775 train_time:65483ms step_avg:50.26ms
step:1304/1775 train_time:65568ms step_avg:50.28ms
step:1305/1775 train_time:65651ms step_avg:50.31ms
step:1306/1775 train_time:65737ms step_avg:50.33ms
step:1307/1775 train_time:65819ms step_avg:50.36ms
step:1308/1775 train_time:65905ms step_avg:50.39ms
step:1309/1775 train_time:65988ms step_avg:50.41ms
step:1310/1775 train_time:66074ms step_avg:50.44ms
step:1311/1775 train_time:66157ms step_avg:50.46ms
step:1312/1775 train_time:66247ms step_avg:50.49ms
step:1313/1775 train_time:66331ms step_avg:50.52ms
step:1314/1775 train_time:66416ms step_avg:50.55ms
step:1315/1775 train_time:66500ms step_avg:50.57ms
step:1316/1775 train_time:66585ms step_avg:50.60ms
step:1317/1775 train_time:66668ms step_avg:50.62ms
step:1318/1775 train_time:66755ms step_avg:50.65ms
step:1319/1775 train_time:66837ms step_avg:50.67ms
step:1320/1775 train_time:66923ms step_avg:50.70ms
step:1321/1775 train_time:67006ms step_avg:50.72ms
step:1322/1775 train_time:67092ms step_avg:50.75ms
step:1323/1775 train_time:67176ms step_avg:50.78ms
step:1324/1775 train_time:67263ms step_avg:50.80ms
step:1325/1775 train_time:67347ms step_avg:50.83ms
step:1326/1775 train_time:67433ms step_avg:50.85ms
step:1327/1775 train_time:67517ms step_avg:50.88ms
step:1328/1775 train_time:67602ms step_avg:50.91ms
step:1329/1775 train_time:67686ms step_avg:50.93ms
step:1330/1775 train_time:67772ms step_avg:50.96ms
step:1331/1775 train_time:67855ms step_avg:50.98ms
step:1332/1775 train_time:67941ms step_avg:51.01ms
step:1333/1775 train_time:68023ms step_avg:51.03ms
step:1334/1775 train_time:68108ms step_avg:51.06ms
step:1335/1775 train_time:68192ms step_avg:51.08ms
step:1336/1775 train_time:68277ms step_avg:51.11ms
step:1337/1775 train_time:68361ms step_avg:51.13ms
step:1338/1775 train_time:68448ms step_avg:51.16ms
step:1339/1775 train_time:68531ms step_avg:51.18ms
step:1340/1775 train_time:68616ms step_avg:51.21ms
step:1341/1775 train_time:68699ms step_avg:51.23ms
step:1342/1775 train_time:68784ms step_avg:51.25ms
step:1343/1775 train_time:68868ms step_avg:51.28ms
step:1344/1775 train_time:68953ms step_avg:51.30ms
step:1345/1775 train_time:69035ms step_avg:51.33ms
step:1346/1775 train_time:69122ms step_avg:51.35ms
step:1347/1775 train_time:69206ms step_avg:51.38ms
step:1348/1775 train_time:69292ms step_avg:51.40ms
step:1349/1775 train_time:69376ms step_avg:51.43ms
step:1350/1775 train_time:69461ms step_avg:51.45ms
step:1351/1775 train_time:69544ms step_avg:51.48ms
step:1352/1775 train_time:69629ms step_avg:51.50ms
step:1353/1775 train_time:69712ms step_avg:51.52ms
step:1354/1775 train_time:69798ms step_avg:51.55ms
step:1355/1775 train_time:69881ms step_avg:51.57ms
step:1356/1775 train_time:69968ms step_avg:51.60ms
step:1357/1775 train_time:70051ms step_avg:51.62ms
step:1358/1775 train_time:70137ms step_avg:51.65ms
step:1359/1775 train_time:70220ms step_avg:51.67ms
step:1360/1775 train_time:70307ms step_avg:51.70ms
step:1361/1775 train_time:70391ms step_avg:51.72ms
step:1362/1775 train_time:70476ms step_avg:51.74ms
step:1363/1775 train_time:70560ms step_avg:51.77ms
step:1364/1775 train_time:70645ms step_avg:51.79ms
step:1365/1775 train_time:70728ms step_avg:51.82ms
step:1366/1775 train_time:70813ms step_avg:51.84ms
step:1367/1775 train_time:70896ms step_avg:51.86ms
step:1368/1775 train_time:70983ms step_avg:51.89ms
step:1369/1775 train_time:71067ms step_avg:51.91ms
step:1370/1775 train_time:71153ms step_avg:51.94ms
step:1371/1775 train_time:71235ms step_avg:51.96ms
step:1372/1775 train_time:71322ms step_avg:51.98ms
step:1373/1775 train_time:71405ms step_avg:52.01ms
step:1374/1775 train_time:71490ms step_avg:52.03ms
step:1375/1775 train_time:71574ms step_avg:52.05ms
step:1376/1775 train_time:71659ms step_avg:52.08ms
step:1377/1775 train_time:71742ms step_avg:52.10ms
step:1378/1775 train_time:71829ms step_avg:52.13ms
step:1379/1775 train_time:71912ms step_avg:52.15ms
step:1380/1775 train_time:71998ms step_avg:52.17ms
step:1381/1775 train_time:72080ms step_avg:52.19ms
step:1382/1775 train_time:72167ms step_avg:52.22ms
step:1383/1775 train_time:72250ms step_avg:52.24ms
step:1384/1775 train_time:72336ms step_avg:52.27ms
step:1385/1775 train_time:72419ms step_avg:52.29ms
step:1386/1775 train_time:72506ms step_avg:52.31ms
step:1387/1775 train_time:72588ms step_avg:52.33ms
step:1388/1775 train_time:72675ms step_avg:52.36ms
step:1389/1775 train_time:72758ms step_avg:52.38ms
step:1390/1775 train_time:72844ms step_avg:52.41ms
step:1391/1775 train_time:72927ms step_avg:52.43ms
step:1392/1775 train_time:73012ms step_avg:52.45ms
step:1393/1775 train_time:73095ms step_avg:52.47ms
step:1394/1775 train_time:73181ms step_avg:52.50ms
step:1395/1775 train_time:73265ms step_avg:52.52ms
step:1396/1775 train_time:73351ms step_avg:52.54ms
step:1397/1775 train_time:73435ms step_avg:52.57ms
step:1398/1775 train_time:73520ms step_avg:52.59ms
step:1399/1775 train_time:73603ms step_avg:52.61ms
step:1400/1775 train_time:73689ms step_avg:52.63ms
step:1401/1775 train_time:73773ms step_avg:52.66ms
step:1402/1775 train_time:73858ms step_avg:52.68ms
step:1403/1775 train_time:73941ms step_avg:52.70ms
step:1404/1775 train_time:74027ms step_avg:52.73ms
step:1405/1775 train_time:74110ms step_avg:52.75ms
step:1406/1775 train_time:74197ms step_avg:52.77ms
step:1407/1775 train_time:74280ms step_avg:52.79ms
step:1408/1775 train_time:74367ms step_avg:52.82ms
step:1409/1775 train_time:74450ms step_avg:52.84ms
step:1410/1775 train_time:74536ms step_avg:52.86ms
step:1411/1775 train_time:74618ms step_avg:52.88ms
step:1412/1775 train_time:74706ms step_avg:52.91ms
step:1413/1775 train_time:74787ms step_avg:52.93ms
step:1414/1775 train_time:74874ms step_avg:52.95ms
step:1415/1775 train_time:74957ms step_avg:52.97ms
step:1416/1775 train_time:75043ms step_avg:53.00ms
step:1417/1775 train_time:75127ms step_avg:53.02ms
step:1418/1775 train_time:75213ms step_avg:53.04ms
step:1419/1775 train_time:75295ms step_avg:53.06ms
step:1420/1775 train_time:75382ms step_avg:53.09ms
step:1421/1775 train_time:75466ms step_avg:53.11ms
step:1422/1775 train_time:75551ms step_avg:53.13ms
step:1423/1775 train_time:75635ms step_avg:53.15ms
step:1424/1775 train_time:75719ms step_avg:53.17ms
step:1425/1775 train_time:75804ms step_avg:53.20ms
step:1426/1775 train_time:75889ms step_avg:53.22ms
step:1427/1775 train_time:75972ms step_avg:53.24ms
step:1428/1775 train_time:76057ms step_avg:53.26ms
step:1429/1775 train_time:76141ms step_avg:53.28ms
step:1430/1775 train_time:76227ms step_avg:53.31ms
step:1431/1775 train_time:76309ms step_avg:53.33ms
step:1432/1775 train_time:76396ms step_avg:53.35ms
step:1433/1775 train_time:76479ms step_avg:53.37ms
step:1434/1775 train_time:76566ms step_avg:53.39ms
step:1435/1775 train_time:76650ms step_avg:53.41ms
step:1436/1775 train_time:76737ms step_avg:53.44ms
step:1437/1775 train_time:76819ms step_avg:53.46ms
step:1438/1775 train_time:76905ms step_avg:53.48ms
step:1439/1775 train_time:76988ms step_avg:53.50ms
step:1440/1775 train_time:77075ms step_avg:53.52ms
step:1441/1775 train_time:77158ms step_avg:53.54ms
step:1442/1775 train_time:77244ms step_avg:53.57ms
step:1443/1775 train_time:77328ms step_avg:53.59ms
step:1444/1775 train_time:77413ms step_avg:53.61ms
step:1445/1775 train_time:77496ms step_avg:53.63ms
step:1446/1775 train_time:77582ms step_avg:53.65ms
step:1447/1775 train_time:77666ms step_avg:53.67ms
step:1448/1775 train_time:77753ms step_avg:53.70ms
step:1449/1775 train_time:77836ms step_avg:53.72ms
step:1450/1775 train_time:77921ms step_avg:53.74ms
step:1451/1775 train_time:78004ms step_avg:53.76ms
step:1452/1775 train_time:78091ms step_avg:53.78ms
step:1453/1775 train_time:78175ms step_avg:53.80ms
step:1454/1775 train_time:78261ms step_avg:53.82ms
step:1455/1775 train_time:78344ms step_avg:53.84ms
step:1456/1775 train_time:78431ms step_avg:53.87ms
step:1457/1775 train_time:78512ms step_avg:53.89ms
step:1458/1775 train_time:78598ms step_avg:53.91ms
step:1459/1775 train_time:78682ms step_avg:53.93ms
step:1460/1775 train_time:78768ms step_avg:53.95ms
step:1461/1775 train_time:78852ms step_avg:53.97ms
step:1462/1775 train_time:78938ms step_avg:53.99ms
step:1463/1775 train_time:79021ms step_avg:54.01ms
step:1464/1775 train_time:79106ms step_avg:54.03ms
step:1465/1775 train_time:79190ms step_avg:54.05ms
step:1466/1775 train_time:79276ms step_avg:54.08ms
step:1467/1775 train_time:79360ms step_avg:54.10ms
step:1468/1775 train_time:79446ms step_avg:54.12ms
step:1469/1775 train_time:79528ms step_avg:54.14ms
step:1470/1775 train_time:79614ms step_avg:54.16ms
step:1471/1775 train_time:79697ms step_avg:54.18ms
step:1472/1775 train_time:79783ms step_avg:54.20ms
step:1473/1775 train_time:79868ms step_avg:54.22ms
step:1474/1775 train_time:79953ms step_avg:54.24ms
step:1475/1775 train_time:80037ms step_avg:54.26ms
step:1476/1775 train_time:80123ms step_avg:54.28ms
step:1477/1775 train_time:80206ms step_avg:54.30ms
step:1478/1775 train_time:80293ms step_avg:54.33ms
step:1479/1775 train_time:80375ms step_avg:54.34ms
step:1480/1775 train_time:80462ms step_avg:54.37ms
step:1481/1775 train_time:80544ms step_avg:54.39ms
step:1482/1775 train_time:80630ms step_avg:54.41ms
step:1483/1775 train_time:80713ms step_avg:54.43ms
step:1484/1775 train_time:80797ms step_avg:54.45ms
step:1485/1775 train_time:80881ms step_avg:54.47ms
step:1486/1775 train_time:80967ms step_avg:54.49ms
step:1487/1775 train_time:81050ms step_avg:54.51ms
step:1488/1775 train_time:81137ms step_avg:54.53ms
step:1489/1775 train_time:81219ms step_avg:54.55ms
step:1490/1775 train_time:81306ms step_avg:54.57ms
step:1491/1775 train_time:81388ms step_avg:54.59ms
step:1492/1775 train_time:81474ms step_avg:54.61ms
step:1493/1775 train_time:81557ms step_avg:54.63ms
step:1494/1775 train_time:81644ms step_avg:54.65ms
step:1495/1775 train_time:81727ms step_avg:54.67ms
step:1496/1775 train_time:81812ms step_avg:54.69ms
step:1497/1775 train_time:81895ms step_avg:54.71ms
step:1498/1775 train_time:81981ms step_avg:54.73ms
step:1499/1775 train_time:82065ms step_avg:54.75ms
step:1500/1775 train_time:82151ms step_avg:54.77ms
step:1500/1775 val_loss:3.3759 train_time:82250ms step_avg:54.83ms
step:1501/1775 train_time:82271ms step_avg:54.81ms
step:1502/1775 train_time:82324ms step_avg:54.81ms
step:1503/1775 train_time:82412ms step_avg:54.83ms
step:1504/1775 train_time:82497ms step_avg:54.85ms
step:1505/1775 train_time:82580ms step_avg:54.87ms
step:1506/1775 train_time:82666ms step_avg:54.89ms
step:1507/1775 train_time:82749ms step_avg:54.91ms
step:1508/1775 train_time:82833ms step_avg:54.93ms
step:1509/1775 train_time:82915ms step_avg:54.95ms
step:1510/1775 train_time:83000ms step_avg:54.97ms
step:1511/1775 train_time:83081ms step_avg:54.98ms
step:1512/1775 train_time:83168ms step_avg:55.01ms
step:1513/1775 train_time:83253ms step_avg:55.02ms
step:1514/1775 train_time:83340ms step_avg:55.05ms
step:1515/1775 train_time:83425ms step_avg:55.07ms
step:1516/1775 train_time:83511ms step_avg:55.09ms
step:1517/1775 train_time:83595ms step_avg:55.11ms
step:1518/1775 train_time:83681ms step_avg:55.13ms
step:1519/1775 train_time:83765ms step_avg:55.14ms
step:1520/1775 train_time:83851ms step_avg:55.17ms
step:1521/1775 train_time:83932ms step_avg:55.18ms
step:1522/1775 train_time:84018ms step_avg:55.20ms
step:1523/1775 train_time:84099ms step_avg:55.22ms
step:1524/1775 train_time:84185ms step_avg:55.24ms
step:1525/1775 train_time:84271ms step_avg:55.26ms
step:1526/1775 train_time:84359ms step_avg:55.28ms
step:1527/1775 train_time:84445ms step_avg:55.30ms
step:1528/1775 train_time:84530ms step_avg:55.32ms
step:1529/1775 train_time:84613ms step_avg:55.34ms
step:1530/1775 train_time:84699ms step_avg:55.36ms
step:1531/1775 train_time:84782ms step_avg:55.38ms
step:1532/1775 train_time:84868ms step_avg:55.40ms
step:1533/1775 train_time:84951ms step_avg:55.41ms
step:1534/1775 train_time:85035ms step_avg:55.43ms
step:1535/1775 train_time:85116ms step_avg:55.45ms
step:1536/1775 train_time:85202ms step_avg:55.47ms
step:1537/1775 train_time:85287ms step_avg:55.49ms
step:1538/1775 train_time:85375ms step_avg:55.51ms
step:1539/1775 train_time:85459ms step_avg:55.53ms
step:1540/1775 train_time:85546ms step_avg:55.55ms
step:1541/1775 train_time:85629ms step_avg:55.57ms
step:1542/1775 train_time:85714ms step_avg:55.59ms
step:1543/1775 train_time:85796ms step_avg:55.60ms
step:1544/1775 train_time:85882ms step_avg:55.62ms
step:1545/1775 train_time:85965ms step_avg:55.64ms
step:1546/1775 train_time:86051ms step_avg:55.66ms
step:1547/1775 train_time:86132ms step_avg:55.68ms
step:1548/1775 train_time:86218ms step_avg:55.70ms
step:1549/1775 train_time:86302ms step_avg:55.71ms
step:1550/1775 train_time:86390ms step_avg:55.74ms
step:1551/1775 train_time:86474ms step_avg:55.75ms
step:1552/1775 train_time:86560ms step_avg:55.77ms
step:1553/1775 train_time:86644ms step_avg:55.79ms
step:1554/1775 train_time:86730ms step_avg:55.81ms
step:1555/1775 train_time:86813ms step_avg:55.83ms
step:1556/1775 train_time:86898ms step_avg:55.85ms
step:1557/1775 train_time:86981ms step_avg:55.86ms
step:1558/1775 train_time:87067ms step_avg:55.88ms
step:1559/1775 train_time:87149ms step_avg:55.90ms
step:1560/1775 train_time:87235ms step_avg:55.92ms
step:1561/1775 train_time:87319ms step_avg:55.94ms
step:1562/1775 train_time:87404ms step_avg:55.96ms
step:1563/1775 train_time:87489ms step_avg:55.98ms
step:1564/1775 train_time:87575ms step_avg:55.99ms
step:1565/1775 train_time:87657ms step_avg:56.01ms
step:1566/1775 train_time:87745ms step_avg:56.03ms
step:1567/1775 train_time:87828ms step_avg:56.05ms
step:1568/1775 train_time:87913ms step_avg:56.07ms
step:1569/1775 train_time:87994ms step_avg:56.08ms
step:1570/1775 train_time:88081ms step_avg:56.10ms
step:1571/1775 train_time:88165ms step_avg:56.12ms
step:1572/1775 train_time:88251ms step_avg:56.14ms
step:1573/1775 train_time:88334ms step_avg:56.16ms
step:1574/1775 train_time:88420ms step_avg:56.18ms
step:1575/1775 train_time:88503ms step_avg:56.19ms
step:1576/1775 train_time:88591ms step_avg:56.21ms
step:1577/1775 train_time:88674ms step_avg:56.23ms
step:1578/1775 train_time:88761ms step_avg:56.25ms
step:1579/1775 train_time:88844ms step_avg:56.27ms
step:1580/1775 train_time:88930ms step_avg:56.28ms
step:1581/1775 train_time:89012ms step_avg:56.30ms
step:1582/1775 train_time:89098ms step_avg:56.32ms
step:1583/1775 train_time:89182ms step_avg:56.34ms
step:1584/1775 train_time:89267ms step_avg:56.36ms
step:1585/1775 train_time:89351ms step_avg:56.37ms
step:1586/1775 train_time:89437ms step_avg:56.39ms
step:1587/1775 train_time:89521ms step_avg:56.41ms
step:1588/1775 train_time:89608ms step_avg:56.43ms
step:1589/1775 train_time:89691ms step_avg:56.44ms
step:1590/1775 train_time:89776ms step_avg:56.46ms
step:1591/1775 train_time:89860ms step_avg:56.48ms
step:1592/1775 train_time:89945ms step_avg:56.50ms
step:1593/1775 train_time:90028ms step_avg:56.51ms
step:1594/1775 train_time:90114ms step_avg:56.53ms
step:1595/1775 train_time:90197ms step_avg:56.55ms
step:1596/1775 train_time:90283ms step_avg:56.57ms
step:1597/1775 train_time:90367ms step_avg:56.59ms
step:1598/1775 train_time:90453ms step_avg:56.60ms
step:1599/1775 train_time:90535ms step_avg:56.62ms
step:1600/1775 train_time:90623ms step_avg:56.64ms
step:1601/1775 train_time:90706ms step_avg:56.66ms
step:1602/1775 train_time:90793ms step_avg:56.68ms
step:1603/1775 train_time:90876ms step_avg:56.69ms
step:1604/1775 train_time:90961ms step_avg:56.71ms
step:1605/1775 train_time:91044ms step_avg:56.73ms
step:1606/1775 train_time:91129ms step_avg:56.74ms
step:1607/1775 train_time:91212ms step_avg:56.76ms
step:1608/1775 train_time:91298ms step_avg:56.78ms
step:1609/1775 train_time:91382ms step_avg:56.79ms
step:1610/1775 train_time:91468ms step_avg:56.81ms
step:1611/1775 train_time:91553ms step_avg:56.83ms
step:1612/1775 train_time:91639ms step_avg:56.85ms
step:1613/1775 train_time:91722ms step_avg:56.86ms
step:1614/1775 train_time:91807ms step_avg:56.88ms
step:1615/1775 train_time:91891ms step_avg:56.90ms
step:1616/1775 train_time:91977ms step_avg:56.92ms
step:1617/1775 train_time:92060ms step_avg:56.93ms
step:1618/1775 train_time:92146ms step_avg:56.95ms
step:1619/1775 train_time:92228ms step_avg:56.97ms
step:1620/1775 train_time:92314ms step_avg:56.98ms
step:1621/1775 train_time:92397ms step_avg:57.00ms
step:1622/1775 train_time:92482ms step_avg:57.02ms
step:1623/1775 train_time:92565ms step_avg:57.03ms
step:1624/1775 train_time:92653ms step_avg:57.05ms
step:1625/1775 train_time:92735ms step_avg:57.07ms
step:1626/1775 train_time:92822ms step_avg:57.09ms
step:1627/1775 train_time:92905ms step_avg:57.10ms
step:1628/1775 train_time:92991ms step_avg:57.12ms
step:1629/1775 train_time:93074ms step_avg:57.14ms
step:1630/1775 train_time:93160ms step_avg:57.15ms
step:1631/1775 train_time:93243ms step_avg:57.17ms
step:1632/1775 train_time:93329ms step_avg:57.19ms
step:1633/1775 train_time:93412ms step_avg:57.20ms
step:1634/1775 train_time:93498ms step_avg:57.22ms
step:1635/1775 train_time:93582ms step_avg:57.24ms
step:1636/1775 train_time:93668ms step_avg:57.25ms
step:1637/1775 train_time:93751ms step_avg:57.27ms
step:1638/1775 train_time:93838ms step_avg:57.29ms
step:1639/1775 train_time:93922ms step_avg:57.30ms
step:1640/1775 train_time:94007ms step_avg:57.32ms
step:1641/1775 train_time:94090ms step_avg:57.34ms
step:1642/1775 train_time:94176ms step_avg:57.35ms
step:1643/1775 train_time:94259ms step_avg:57.37ms
step:1644/1775 train_time:94345ms step_avg:57.39ms
step:1645/1775 train_time:94428ms step_avg:57.40ms
step:1646/1775 train_time:94514ms step_avg:57.42ms
step:1647/1775 train_time:94597ms step_avg:57.44ms
step:1648/1775 train_time:94683ms step_avg:57.45ms
step:1649/1775 train_time:94767ms step_avg:57.47ms
step:1650/1775 train_time:94853ms step_avg:57.49ms
step:1651/1775 train_time:94935ms step_avg:57.50ms
step:1652/1775 train_time:95022ms step_avg:57.52ms
step:1653/1775 train_time:95104ms step_avg:57.53ms
step:1654/1775 train_time:95190ms step_avg:57.55ms
step:1655/1775 train_time:95273ms step_avg:57.57ms
step:1656/1775 train_time:95359ms step_avg:57.58ms
step:1657/1775 train_time:95442ms step_avg:57.60ms
step:1658/1775 train_time:95527ms step_avg:57.62ms
step:1659/1775 train_time:95611ms step_avg:57.63ms
step:1660/1775 train_time:95698ms step_avg:57.65ms
step:1661/1775 train_time:95782ms step_avg:57.67ms
step:1662/1775 train_time:95868ms step_avg:57.68ms
step:1663/1775 train_time:95951ms step_avg:57.70ms
step:1664/1775 train_time:96037ms step_avg:57.71ms
step:1665/1775 train_time:96120ms step_avg:57.73ms
step:1666/1775 train_time:96206ms step_avg:57.75ms
step:1667/1775 train_time:96290ms step_avg:57.76ms
step:1668/1775 train_time:96376ms step_avg:57.78ms
step:1669/1775 train_time:96459ms step_avg:57.79ms
step:1670/1775 train_time:96545ms step_avg:57.81ms
step:1671/1775 train_time:96628ms step_avg:57.83ms
step:1672/1775 train_time:96715ms step_avg:57.84ms
step:1673/1775 train_time:96797ms step_avg:57.86ms
step:1674/1775 train_time:96884ms step_avg:57.88ms
step:1675/1775 train_time:96968ms step_avg:57.89ms
step:1676/1775 train_time:97054ms step_avg:57.91ms
step:1677/1775 train_time:97136ms step_avg:57.92ms
step:1678/1775 train_time:97222ms step_avg:57.94ms
step:1679/1775 train_time:97305ms step_avg:57.95ms
step:1680/1775 train_time:97392ms step_avg:57.97ms
step:1681/1775 train_time:97475ms step_avg:57.99ms
step:1682/1775 train_time:97562ms step_avg:58.00ms
step:1683/1775 train_time:97645ms step_avg:58.02ms
step:1684/1775 train_time:97732ms step_avg:58.04ms
step:1685/1775 train_time:97813ms step_avg:58.05ms
step:1686/1775 train_time:97900ms step_avg:58.07ms
step:1687/1775 train_time:97984ms step_avg:58.08ms
step:1688/1775 train_time:98070ms step_avg:58.10ms
step:1689/1775 train_time:98154ms step_avg:58.11ms
step:1690/1775 train_time:98240ms step_avg:58.13ms
step:1691/1775 train_time:98323ms step_avg:58.15ms
step:1692/1775 train_time:98410ms step_avg:58.16ms
step:1693/1775 train_time:98494ms step_avg:58.18ms
step:1694/1775 train_time:98580ms step_avg:58.19ms
step:1695/1775 train_time:98662ms step_avg:58.21ms
step:1696/1775 train_time:98749ms step_avg:58.22ms
step:1697/1775 train_time:98832ms step_avg:58.24ms
step:1698/1775 train_time:98917ms step_avg:58.26ms
step:1699/1775 train_time:99000ms step_avg:58.27ms
step:1700/1775 train_time:99086ms step_avg:58.29ms
step:1701/1775 train_time:99170ms step_avg:58.30ms
step:1702/1775 train_time:99256ms step_avg:58.32ms
step:1703/1775 train_time:99339ms step_avg:58.33ms
step:1704/1775 train_time:99426ms step_avg:58.35ms
step:1705/1775 train_time:99509ms step_avg:58.36ms
step:1706/1775 train_time:99595ms step_avg:58.38ms
step:1707/1775 train_time:99679ms step_avg:58.39ms
step:1708/1775 train_time:99765ms step_avg:58.41ms
step:1709/1775 train_time:99847ms step_avg:58.42ms
step:1710/1775 train_time:99933ms step_avg:58.44ms
step:1711/1775 train_time:100016ms step_avg:58.45ms
step:1712/1775 train_time:100102ms step_avg:58.47ms
step:1713/1775 train_time:100185ms step_avg:58.49ms
step:1714/1775 train_time:100272ms step_avg:58.50ms
step:1715/1775 train_time:100355ms step_avg:58.52ms
step:1716/1775 train_time:100441ms step_avg:58.53ms
step:1717/1775 train_time:100524ms step_avg:58.55ms
step:1718/1775 train_time:100611ms step_avg:58.56ms
step:1719/1775 train_time:100694ms step_avg:58.58ms
step:1720/1775 train_time:100781ms step_avg:58.59ms
step:1721/1775 train_time:100864ms step_avg:58.61ms
step:1722/1775 train_time:100950ms step_avg:58.62ms
step:1723/1775 train_time:101033ms step_avg:58.64ms
step:1724/1775 train_time:101119ms step_avg:58.65ms
step:1725/1775 train_time:101201ms step_avg:58.67ms
step:1726/1775 train_time:101287ms step_avg:58.68ms
step:1727/1775 train_time:101370ms step_avg:58.70ms
step:1728/1775 train_time:101457ms step_avg:58.71ms
step:1729/1775 train_time:101540ms step_avg:58.73ms
step:1730/1775 train_time:101626ms step_avg:58.74ms
step:1731/1775 train_time:101709ms step_avg:58.76ms
step:1732/1775 train_time:101795ms step_avg:58.77ms
step:1733/1775 train_time:101878ms step_avg:58.79ms
step:1734/1775 train_time:101965ms step_avg:58.80ms
step:1735/1775 train_time:102048ms step_avg:58.82ms
step:1736/1775 train_time:102137ms step_avg:58.83ms
step:1737/1775 train_time:102221ms step_avg:58.85ms
step:1738/1775 train_time:102309ms step_avg:58.87ms
step:1739/1775 train_time:102393ms step_avg:58.88ms
step:1740/1775 train_time:102478ms step_avg:58.90ms
step:1741/1775 train_time:102563ms step_avg:58.91ms
step:1742/1775 train_time:102649ms step_avg:58.93ms
step:1743/1775 train_time:102732ms step_avg:58.94ms
step:1744/1775 train_time:102817ms step_avg:58.95ms
step:1745/1775 train_time:102900ms step_avg:58.97ms
step:1746/1775 train_time:102987ms step_avg:58.98ms
step:1747/1775 train_time:103071ms step_avg:59.00ms
step:1748/1775 train_time:103157ms step_avg:59.01ms
step:1749/1775 train_time:103240ms step_avg:59.03ms
step:1750/1775 train_time:103327ms step_avg:59.04ms
step:1750/1775 val_loss:3.2851 train_time:103426ms step_avg:59.10ms
step:1751/1775 train_time:103449ms step_avg:59.08ms
step:1752/1775 train_time:103501ms step_avg:59.08ms
step:1753/1775 train_time:103585ms step_avg:59.09ms
step:1754/1775 train_time:103677ms step_avg:59.11ms
step:1755/1775 train_time:103762ms step_avg:59.12ms
step:1756/1775 train_time:103847ms step_avg:59.14ms
step:1757/1775 train_time:103930ms step_avg:59.15ms
step:1758/1775 train_time:104014ms step_avg:59.17ms
step:1759/1775 train_time:104097ms step_avg:59.18ms
step:1760/1775 train_time:104184ms step_avg:59.20ms
step:1761/1775 train_time:104266ms step_avg:59.21ms
step:1762/1775 train_time:104352ms step_avg:59.22ms
step:1763/1775 train_time:104437ms step_avg:59.24ms
step:1764/1775 train_time:104525ms step_avg:59.25ms
step:1765/1775 train_time:104611ms step_avg:59.27ms
step:1766/1775 train_time:104700ms step_avg:59.29ms
step:1767/1775 train_time:104783ms step_avg:59.30ms
step:1768/1775 train_time:104869ms step_avg:59.32ms
step:1769/1775 train_time:104952ms step_avg:59.33ms
step:1770/1775 train_time:105038ms step_avg:59.34ms
step:1771/1775 train_time:105120ms step_avg:59.36ms
step:1772/1775 train_time:105206ms step_avg:59.37ms
step:1773/1775 train_time:105289ms step_avg:59.38ms
step:1774/1775 train_time:105376ms step_avg:59.40ms
step:1775/1775 train_time:105461ms step_avg:59.41ms
step:1775/1775 val_loss:3.2788 train_time:105561ms step_avg:59.47ms
peak memory allocated: 29407 MiB reserved: 45078 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan 19 01:53:13 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:00:0B.0 Off |                    0 |
| N/A   32C    P0             109W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:00:0C.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:00:0D.0 Off |                    0 |
| N/A   38C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:00:0E.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:00:0F.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:00:10.0 Off |                    0 |
| N/A   39C    P0             116W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:00:11.0 Off |                    0 |
| N/A   35C    P0             114W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:00:12.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1515MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     93601      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    1   N/A  N/A     93602      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    2   N/A  N/A     93603      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    3   N/A  N/A     93604      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    4   N/A  N/A     93605      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    5   N/A  N/A     93606      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    6   N/A  N/A     93607      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
|    7   N/A  N/A     93608      C   ...omamba/envs/speedrun/bin/python3.12     1506MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8257 train_time:0ms step_avg:0.04ms
step:1/1775 train_time:81ms step_avg:80.63ms
step:2/1775 train_time:106ms step_avg:52.79ms
step:3/1775 train_time:127ms step_avg:42.24ms
step:4/1775 train_time:150ms step_avg:37.42ms
step:5/1775 train_time:174ms step_avg:34.84ms
step:6/1775 train_time:322ms step_avg:53.65ms
step:7/1775 train_time:343ms step_avg:48.94ms
step:8/1775 train_time:400ms step_avg:50.01ms
step:9/1775 train_time:431ms step_avg:47.84ms
step:10/1775 train_time:463ms step_avg:46.32ms
step:11/1775 train_time:494ms step_avg:44.92ms
step:12/1775 train_time:527ms step_avg:43.94ms
step:13/1775 train_time:558ms step_avg:42.92ms
step:14/1775 train_time:591ms step_avg:42.23ms
step:15/1775 train_time:623ms step_avg:41.51ms
step:16/1775 train_time:656ms step_avg:40.99ms
step:17/1775 train_time:687ms step_avg:40.43ms
step:18/1775 train_time:720ms step_avg:40.02ms
step:19/1775 train_time:751ms step_avg:39.55ms
step:20/1775 train_time:785ms step_avg:39.23ms
step:21/1775 train_time:816ms step_avg:38.85ms
step:22/1775 train_time:849ms step_avg:38.58ms
step:23/1775 train_time:880ms step_avg:38.26ms
step:24/1775 train_time:913ms step_avg:38.05ms
step:25/1775 train_time:945ms step_avg:37.79ms
step:26/1775 train_time:978ms step_avg:37.60ms
step:27/1775 train_time:1008ms step_avg:37.34ms
step:28/1775 train_time:1041ms step_avg:37.19ms
step:29/1775 train_time:1072ms step_avg:36.98ms
step:30/1775 train_time:1105ms step_avg:36.85ms
step:31/1775 train_time:1137ms step_avg:36.68ms
step:32/1775 train_time:1170ms step_avg:36.58ms
step:33/1775 train_time:1201ms step_avg:36.41ms
step:34/1775 train_time:1235ms step_avg:36.31ms
step:35/1775 train_time:1266ms step_avg:36.17ms
step:36/1775 train_time:1301ms step_avg:36.15ms
step:37/1775 train_time:1335ms step_avg:36.07ms
step:38/1775 train_time:1370ms step_avg:36.06ms
step:39/1775 train_time:1403ms step_avg:35.97ms
step:40/1775 train_time:1437ms step_avg:35.93ms
step:41/1775 train_time:1469ms step_avg:35.83ms
step:42/1775 train_time:1503ms step_avg:35.78ms
step:43/1775 train_time:1534ms step_avg:35.67ms
step:44/1775 train_time:1567ms step_avg:35.62ms
step:45/1775 train_time:1598ms step_avg:35.51ms
step:46/1775 train_time:1631ms step_avg:35.47ms
step:47/1775 train_time:1663ms step_avg:35.38ms
step:48/1775 train_time:1697ms step_avg:35.35ms
step:49/1775 train_time:1728ms step_avg:35.27ms
step:50/1775 train_time:1761ms step_avg:35.23ms
step:51/1775 train_time:1792ms step_avg:35.14ms
step:52/1775 train_time:1825ms step_avg:35.10ms
step:53/1775 train_time:1856ms step_avg:35.03ms
step:54/1775 train_time:1889ms step_avg:34.99ms
step:55/1775 train_time:1921ms step_avg:34.93ms
step:56/1775 train_time:1954ms step_avg:34.89ms
step:57/1775 train_time:1985ms step_avg:34.83ms
step:58/1775 train_time:2019ms step_avg:34.80ms
step:59/1775 train_time:2050ms step_avg:34.74ms
step:60/1775 train_time:2083ms step_avg:34.71ms
step:61/1775 train_time:2114ms step_avg:34.65ms
step:62/1775 train_time:2147ms step_avg:34.63ms
step:63/1775 train_time:2178ms step_avg:34.58ms
step:64/1775 train_time:2212ms step_avg:34.56ms
step:65/1775 train_time:2243ms step_avg:34.51ms
step:66/1775 train_time:2278ms step_avg:34.52ms
step:67/1775 train_time:2310ms step_avg:34.48ms
step:68/1775 train_time:2344ms step_avg:34.47ms
step:69/1775 train_time:2376ms step_avg:34.44ms
step:70/1775 train_time:2410ms step_avg:34.43ms
step:71/1775 train_time:2442ms step_avg:34.40ms
step:72/1775 train_time:2476ms step_avg:34.39ms
step:73/1775 train_time:2508ms step_avg:34.36ms
step:74/1775 train_time:2542ms step_avg:34.35ms
step:75/1775 train_time:2573ms step_avg:34.31ms
step:76/1775 train_time:2607ms step_avg:34.30ms
step:77/1775 train_time:2639ms step_avg:34.27ms
step:78/1775 train_time:2672ms step_avg:34.26ms
step:79/1775 train_time:2703ms step_avg:34.22ms
step:80/1775 train_time:2737ms step_avg:34.21ms
step:81/1775 train_time:2768ms step_avg:34.17ms
step:82/1775 train_time:2801ms step_avg:34.16ms
step:83/1775 train_time:2832ms step_avg:34.12ms
step:84/1775 train_time:2865ms step_avg:34.11ms
step:85/1775 train_time:2897ms step_avg:34.08ms
step:86/1775 train_time:2930ms step_avg:34.07ms
step:87/1775 train_time:2961ms step_avg:34.04ms
step:88/1775 train_time:2994ms step_avg:34.03ms
step:89/1775 train_time:3026ms step_avg:34.00ms
step:90/1775 train_time:3059ms step_avg:33.99ms
step:91/1775 train_time:3090ms step_avg:33.96ms
step:92/1775 train_time:3123ms step_avg:33.94ms
step:93/1775 train_time:3154ms step_avg:33.91ms
step:94/1775 train_time:3187ms step_avg:33.90ms
step:95/1775 train_time:3219ms step_avg:33.88ms
step:96/1775 train_time:3253ms step_avg:33.89ms
step:97/1775 train_time:3285ms step_avg:33.87ms
step:98/1775 train_time:3319ms step_avg:33.86ms
step:99/1775 train_time:3350ms step_avg:33.84ms
step:100/1775 train_time:3384ms step_avg:33.84ms
step:101/1775 train_time:3415ms step_avg:33.81ms
step:102/1775 train_time:3449ms step_avg:33.82ms
step:103/1775 train_time:3481ms step_avg:33.79ms
step:104/1775 train_time:3514ms step_avg:33.79ms
step:105/1775 train_time:3546ms step_avg:33.77ms
step:106/1775 train_time:3579ms step_avg:33.77ms
step:107/1775 train_time:3611ms step_avg:33.74ms
step:108/1775 train_time:3644ms step_avg:33.74ms
step:109/1775 train_time:3675ms step_avg:33.72ms
step:110/1775 train_time:3709ms step_avg:33.72ms
step:111/1775 train_time:3740ms step_avg:33.70ms
step:112/1775 train_time:3774ms step_avg:33.69ms
step:113/1775 train_time:3805ms step_avg:33.67ms
step:114/1775 train_time:3839ms step_avg:33.67ms
step:115/1775 train_time:3869ms step_avg:33.65ms
step:116/1775 train_time:3902ms step_avg:33.64ms
step:117/1775 train_time:3934ms step_avg:33.62ms
step:118/1775 train_time:3968ms step_avg:33.63ms
step:119/1775 train_time:3998ms step_avg:33.60ms
step:120/1775 train_time:4031ms step_avg:33.60ms
step:121/1775 train_time:4063ms step_avg:33.58ms
step:122/1775 train_time:4096ms step_avg:33.57ms
step:123/1775 train_time:4127ms step_avg:33.55ms
step:124/1775 train_time:4160ms step_avg:33.55ms
step:125/1775 train_time:4191ms step_avg:33.53ms
step:126/1775 train_time:4225ms step_avg:33.54ms
step:127/1775 train_time:4256ms step_avg:33.52ms
step:128/1775 train_time:4290ms step_avg:33.52ms
step:129/1775 train_time:4322ms step_avg:33.50ms
step:130/1775 train_time:4355ms step_avg:33.50ms
step:131/1775 train_time:4386ms step_avg:33.48ms
step:132/1775 train_time:4420ms step_avg:33.48ms
step:133/1775 train_time:4451ms step_avg:33.47ms
step:134/1775 train_time:4485ms step_avg:33.47ms
step:135/1775 train_time:4517ms step_avg:33.46ms
step:136/1775 train_time:4550ms step_avg:33.46ms
step:137/1775 train_time:4582ms step_avg:33.45ms
step:138/1775 train_time:4615ms step_avg:33.44ms
step:139/1775 train_time:4647ms step_avg:33.43ms
step:140/1775 train_time:4680ms step_avg:33.43ms
step:141/1775 train_time:4711ms step_avg:33.41ms
step:142/1775 train_time:4744ms step_avg:33.41ms
step:143/1775 train_time:4775ms step_avg:33.39ms
step:144/1775 train_time:4809ms step_avg:33.39ms
step:145/1775 train_time:4840ms step_avg:33.38ms
step:146/1775 train_time:4873ms step_avg:33.38ms
step:147/1775 train_time:4904ms step_avg:33.36ms
step:148/1775 train_time:4937ms step_avg:33.36ms
step:149/1775 train_time:4968ms step_avg:33.34ms
step:150/1775 train_time:5001ms step_avg:33.34ms
step:151/1775 train_time:5032ms step_avg:33.33ms
step:152/1775 train_time:5066ms step_avg:33.33ms
step:153/1775 train_time:5097ms step_avg:33.31ms
step:154/1775 train_time:5130ms step_avg:33.31ms
step:155/1775 train_time:5162ms step_avg:33.30ms
step:156/1775 train_time:5195ms step_avg:33.30ms
step:157/1775 train_time:5227ms step_avg:33.29ms
step:158/1775 train_time:5260ms step_avg:33.29ms
step:159/1775 train_time:5291ms step_avg:33.28ms
step:160/1775 train_time:5325ms step_avg:33.28ms
step:161/1775 train_time:5356ms step_avg:33.27ms
step:162/1775 train_time:5390ms step_avg:33.27ms
step:163/1775 train_time:5422ms step_avg:33.26ms
step:164/1775 train_time:5456ms step_avg:33.27ms
step:165/1775 train_time:5487ms step_avg:33.26ms
step:166/1775 train_time:5521ms step_avg:33.26ms
step:167/1775 train_time:5552ms step_avg:33.25ms
step:168/1775 train_time:5586ms step_avg:33.25ms
step:169/1775 train_time:5617ms step_avg:33.24ms
step:170/1775 train_time:5651ms step_avg:33.24ms
step:171/1775 train_time:5683ms step_avg:33.23ms
step:172/1775 train_time:5716ms step_avg:33.24ms
step:173/1775 train_time:5748ms step_avg:33.22ms
step:174/1775 train_time:5781ms step_avg:33.22ms
step:175/1775 train_time:5812ms step_avg:33.21ms
step:176/1775 train_time:5845ms step_avg:33.21ms
step:177/1775 train_time:5876ms step_avg:33.20ms
step:178/1775 train_time:5909ms step_avg:33.20ms
step:179/1775 train_time:5941ms step_avg:33.19ms
step:180/1775 train_time:5974ms step_avg:33.19ms
step:181/1775 train_time:6005ms step_avg:33.17ms
step:182/1775 train_time:6038ms step_avg:33.17ms
step:183/1775 train_time:6069ms step_avg:33.16ms
step:184/1775 train_time:6102ms step_avg:33.16ms
step:185/1775 train_time:6133ms step_avg:33.15ms
step:186/1775 train_time:6167ms step_avg:33.16ms
step:187/1775 train_time:6199ms step_avg:33.15ms
step:188/1775 train_time:6232ms step_avg:33.15ms
step:189/1775 train_time:6263ms step_avg:33.14ms
step:190/1775 train_time:6297ms step_avg:33.14ms
step:191/1775 train_time:6328ms step_avg:33.13ms
step:192/1775 train_time:6361ms step_avg:33.13ms
step:193/1775 train_time:6393ms step_avg:33.12ms
step:194/1775 train_time:6426ms step_avg:33.12ms
step:195/1775 train_time:6458ms step_avg:33.12ms
step:196/1775 train_time:6491ms step_avg:33.12ms
step:197/1775 train_time:6523ms step_avg:33.11ms
step:198/1775 train_time:6556ms step_avg:33.11ms
step:199/1775 train_time:6587ms step_avg:33.10ms
step:200/1775 train_time:6621ms step_avg:33.10ms
step:201/1775 train_time:6652ms step_avg:33.09ms
step:202/1775 train_time:6685ms step_avg:33.09ms
step:203/1775 train_time:6717ms step_avg:33.09ms
step:204/1775 train_time:6750ms step_avg:33.09ms
step:205/1775 train_time:6783ms step_avg:33.09ms
step:206/1775 train_time:6816ms step_avg:33.09ms
step:207/1775 train_time:6848ms step_avg:33.08ms
step:208/1775 train_time:6881ms step_avg:33.08ms
step:209/1775 train_time:6911ms step_avg:33.07ms
step:210/1775 train_time:6945ms step_avg:33.07ms
step:211/1775 train_time:6976ms step_avg:33.06ms
step:212/1775 train_time:7009ms step_avg:33.06ms
step:213/1775 train_time:7040ms step_avg:33.05ms
step:214/1775 train_time:7073ms step_avg:33.05ms
step:215/1775 train_time:7104ms step_avg:33.04ms
step:216/1775 train_time:7137ms step_avg:33.04ms
step:217/1775 train_time:7168ms step_avg:33.03ms
step:218/1775 train_time:7202ms step_avg:33.03ms
step:219/1775 train_time:7233ms step_avg:33.03ms
step:220/1775 train_time:7266ms step_avg:33.03ms
step:221/1775 train_time:7297ms step_avg:33.02ms
step:222/1775 train_time:7330ms step_avg:33.02ms
step:223/1775 train_time:7361ms step_avg:33.01ms
step:224/1775 train_time:7394ms step_avg:33.01ms
step:225/1775 train_time:7426ms step_avg:33.00ms
step:226/1775 train_time:7459ms step_avg:33.01ms
step:227/1775 train_time:7490ms step_avg:33.00ms
step:228/1775 train_time:7524ms step_avg:33.00ms
step:229/1775 train_time:7555ms step_avg:32.99ms
step:230/1775 train_time:7589ms step_avg:33.00ms
step:231/1775 train_time:7620ms step_avg:32.99ms
step:232/1775 train_time:7653ms step_avg:32.99ms
step:233/1775 train_time:7685ms step_avg:32.98ms
step:234/1775 train_time:7719ms step_avg:32.99ms
step:235/1775 train_time:7751ms step_avg:32.98ms
step:236/1775 train_time:7784ms step_avg:32.99ms
step:237/1775 train_time:7816ms step_avg:32.98ms
step:238/1775 train_time:7850ms step_avg:32.98ms
step:239/1775 train_time:7881ms step_avg:32.97ms
step:240/1775 train_time:7914ms step_avg:32.98ms
step:241/1775 train_time:7946ms step_avg:32.97ms
step:242/1775 train_time:7979ms step_avg:32.97ms
step:243/1775 train_time:8010ms step_avg:32.96ms
step:244/1775 train_time:8042ms step_avg:32.96ms
step:245/1775 train_time:8074ms step_avg:32.95ms
step:246/1775 train_time:8107ms step_avg:32.95ms
step:247/1775 train_time:8138ms step_avg:32.95ms
step:248/1775 train_time:8171ms step_avg:32.95ms
step:249/1775 train_time:8202ms step_avg:32.94ms
step:250/1775 train_time:8236ms step_avg:32.94ms
step:250/1775 val_loss:4.6168 train_time:8278ms step_avg:33.11ms
step:251/1775 train_time:8299ms step_avg:33.07ms
step:252/1775 train_time:8321ms step_avg:33.02ms
step:253/1775 train_time:8341ms step_avg:32.97ms
step:254/1775 train_time:8368ms step_avg:32.94ms
step:255/1775 train_time:8401ms step_avg:32.95ms
step:256/1775 train_time:8437ms step_avg:32.96ms
step:257/1775 train_time:8469ms step_avg:32.95ms
step:258/1775 train_time:8503ms step_avg:32.96ms
step:259/1775 train_time:8534ms step_avg:32.95ms
step:260/1775 train_time:8568ms step_avg:32.95ms
step:261/1775 train_time:8600ms step_avg:32.95ms
step:262/1775 train_time:8634ms step_avg:32.95ms
step:263/1775 train_time:8665ms step_avg:32.95ms
step:264/1775 train_time:8698ms step_avg:32.95ms
step:265/1775 train_time:8729ms step_avg:32.94ms
step:266/1775 train_time:8762ms step_avg:32.94ms
step:267/1775 train_time:8793ms step_avg:32.93ms
step:268/1775 train_time:8827ms step_avg:32.94ms
step:269/1775 train_time:8857ms step_avg:32.93ms
step:270/1775 train_time:8890ms step_avg:32.93ms
step:271/1775 train_time:8921ms step_avg:32.92ms
step:272/1775 train_time:8955ms step_avg:32.92ms
step:273/1775 train_time:8985ms step_avg:32.91ms
step:274/1775 train_time:9018ms step_avg:32.91ms
step:275/1775 train_time:9049ms step_avg:32.91ms
step:276/1775 train_time:9082ms step_avg:32.91ms
step:277/1775 train_time:9113ms step_avg:32.90ms
step:278/1775 train_time:9145ms step_avg:32.90ms
step:279/1775 train_time:9177ms step_avg:32.89ms
step:280/1775 train_time:9210ms step_avg:32.89ms
step:281/1775 train_time:9241ms step_avg:32.89ms
step:282/1775 train_time:9275ms step_avg:32.89ms
step:283/1775 train_time:9307ms step_avg:32.89ms
step:284/1775 train_time:9341ms step_avg:32.89ms
step:285/1775 train_time:9372ms step_avg:32.89ms
step:286/1775 train_time:9407ms step_avg:32.89ms
step:287/1775 train_time:9439ms step_avg:32.89ms
step:288/1775 train_time:9472ms step_avg:32.89ms
step:289/1775 train_time:9504ms step_avg:32.89ms
step:290/1775 train_time:9538ms step_avg:32.89ms
step:291/1775 train_time:9570ms step_avg:32.89ms
step:292/1775 train_time:9603ms step_avg:32.89ms
step:293/1775 train_time:9635ms step_avg:32.88ms
step:294/1775 train_time:9668ms step_avg:32.88ms
step:295/1775 train_time:9699ms step_avg:32.88ms
step:296/1775 train_time:9732ms step_avg:32.88ms
step:297/1775 train_time:9763ms step_avg:32.87ms
step:298/1775 train_time:9797ms step_avg:32.87ms
step:299/1775 train_time:9827ms step_avg:32.87ms
step:300/1775 train_time:9861ms step_avg:32.87ms
step:301/1775 train_time:9892ms step_avg:32.86ms
step:302/1775 train_time:9926ms step_avg:32.87ms
step:303/1775 train_time:9956ms step_avg:32.86ms
step:304/1775 train_time:9989ms step_avg:32.86ms
step:305/1775 train_time:10020ms step_avg:32.85ms
step:306/1775 train_time:10053ms step_avg:32.85ms
step:307/1775 train_time:10084ms step_avg:32.85ms
step:308/1775 train_time:10118ms step_avg:32.85ms
step:309/1775 train_time:10149ms step_avg:32.85ms
step:310/1775 train_time:10182ms step_avg:32.85ms
step:311/1775 train_time:10213ms step_avg:32.84ms
step:312/1775 train_time:10247ms step_avg:32.84ms
step:313/1775 train_time:10278ms step_avg:32.84ms
step:314/1775 train_time:10311ms step_avg:32.84ms
step:315/1775 train_time:10343ms step_avg:32.84ms
step:316/1775 train_time:10377ms step_avg:32.84ms
step:317/1775 train_time:10408ms step_avg:32.83ms
step:318/1775 train_time:10442ms step_avg:32.84ms
step:319/1775 train_time:10474ms step_avg:32.83ms
step:320/1775 train_time:10507ms step_avg:32.83ms
step:321/1775 train_time:10539ms step_avg:32.83ms
step:322/1775 train_time:10572ms step_avg:32.83ms
step:323/1775 train_time:10603ms step_avg:32.83ms
step:324/1775 train_time:10637ms step_avg:32.83ms
step:325/1775 train_time:10668ms step_avg:32.82ms
step:326/1775 train_time:10702ms step_avg:32.83ms
step:327/1775 train_time:10733ms step_avg:32.82ms
step:328/1775 train_time:10766ms step_avg:32.82ms
step:329/1775 train_time:10797ms step_avg:32.82ms
step:330/1775 train_time:10830ms step_avg:32.82ms
step:331/1775 train_time:10862ms step_avg:32.81ms
step:332/1775 train_time:10895ms step_avg:32.82ms
step:333/1775 train_time:10926ms step_avg:32.81ms
step:334/1775 train_time:10958ms step_avg:32.81ms
step:335/1775 train_time:10989ms step_avg:32.80ms
step:336/1775 train_time:11023ms step_avg:32.81ms
step:337/1775 train_time:11053ms step_avg:32.80ms
step:338/1775 train_time:11087ms step_avg:32.80ms
step:339/1775 train_time:11118ms step_avg:32.80ms
step:340/1775 train_time:11151ms step_avg:32.80ms
step:341/1775 train_time:11183ms step_avg:32.79ms
step:342/1775 train_time:11216ms step_avg:32.79ms
step:343/1775 train_time:11247ms step_avg:32.79ms
step:344/1775 train_time:11280ms step_avg:32.79ms
step:345/1775 train_time:11311ms step_avg:32.79ms
step:346/1775 train_time:11344ms step_avg:32.79ms
step:347/1775 train_time:11376ms step_avg:32.78ms
step:348/1775 train_time:11410ms step_avg:32.79ms
step:349/1775 train_time:11441ms step_avg:32.78ms
step:350/1775 train_time:11475ms step_avg:32.79ms
step:351/1775 train_time:11506ms step_avg:32.78ms
step:352/1775 train_time:11540ms step_avg:32.78ms
step:353/1775 train_time:11571ms step_avg:32.78ms
step:354/1775 train_time:11605ms step_avg:32.78ms
step:355/1775 train_time:11636ms step_avg:32.78ms
step:356/1775 train_time:11669ms step_avg:32.78ms
step:357/1775 train_time:11701ms step_avg:32.78ms
step:358/1775 train_time:11734ms step_avg:32.78ms
step:359/1775 train_time:11765ms step_avg:32.77ms
step:360/1775 train_time:11799ms step_avg:32.77ms
step:361/1775 train_time:11830ms step_avg:32.77ms
step:362/1775 train_time:11863ms step_avg:32.77ms
step:363/1775 train_time:11894ms step_avg:32.77ms
step:364/1775 train_time:11927ms step_avg:32.77ms
step:365/1775 train_time:11959ms step_avg:32.76ms
step:366/1775 train_time:11992ms step_avg:32.76ms
step:367/1775 train_time:12022ms step_avg:32.76ms
step:368/1775 train_time:12056ms step_avg:32.76ms
step:369/1775 train_time:12087ms step_avg:32.76ms
step:370/1775 train_time:12120ms step_avg:32.76ms
step:371/1775 train_time:12151ms step_avg:32.75ms
step:372/1775 train_time:12184ms step_avg:32.75ms
step:373/1775 train_time:12216ms step_avg:32.75ms
step:374/1775 train_time:12249ms step_avg:32.75ms
step:375/1775 train_time:12280ms step_avg:32.75ms
step:376/1775 train_time:12314ms step_avg:32.75ms
step:377/1775 train_time:12345ms step_avg:32.75ms
step:378/1775 train_time:12378ms step_avg:32.75ms
step:379/1775 train_time:12409ms step_avg:32.74ms
step:380/1775 train_time:12444ms step_avg:32.75ms
step:381/1775 train_time:12475ms step_avg:32.74ms
step:382/1775 train_time:12508ms step_avg:32.74ms
step:383/1775 train_time:12540ms step_avg:32.74ms
step:384/1775 train_time:12573ms step_avg:32.74ms
step:385/1775 train_time:12604ms step_avg:32.74ms
step:386/1775 train_time:12638ms step_avg:32.74ms
step:387/1775 train_time:12669ms step_avg:32.74ms
step:388/1775 train_time:12703ms step_avg:32.74ms
step:389/1775 train_time:12734ms step_avg:32.74ms
step:390/1775 train_time:12768ms step_avg:32.74ms
step:391/1775 train_time:12799ms step_avg:32.73ms
step:392/1775 train_time:12832ms step_avg:32.74ms
step:393/1775 train_time:12864ms step_avg:32.73ms
step:394/1775 train_time:12897ms step_avg:32.73ms
step:395/1775 train_time:12928ms step_avg:32.73ms
step:396/1775 train_time:12962ms step_avg:32.73ms
step:397/1775 train_time:12993ms step_avg:32.73ms
step:398/1775 train_time:13027ms step_avg:32.73ms
step:399/1775 train_time:13057ms step_avg:32.73ms
step:400/1775 train_time:13090ms step_avg:32.73ms
step:401/1775 train_time:13122ms step_avg:32.72ms
step:402/1775 train_time:13155ms step_avg:32.72ms
step:403/1775 train_time:13186ms step_avg:32.72ms
step:404/1775 train_time:13219ms step_avg:32.72ms
step:405/1775 train_time:13250ms step_avg:32.72ms
step:406/1775 train_time:13284ms step_avg:32.72ms
step:407/1775 train_time:13315ms step_avg:32.72ms
step:408/1775 train_time:13348ms step_avg:32.72ms
step:409/1775 train_time:13379ms step_avg:32.71ms
step:410/1775 train_time:13412ms step_avg:32.71ms
step:411/1775 train_time:13444ms step_avg:32.71ms
step:412/1775 train_time:13477ms step_avg:32.71ms
step:413/1775 train_time:13508ms step_avg:32.71ms
step:414/1775 train_time:13541ms step_avg:32.71ms
step:415/1775 train_time:13572ms step_avg:32.70ms
step:416/1775 train_time:13605ms step_avg:32.71ms
step:417/1775 train_time:13637ms step_avg:32.70ms
step:418/1775 train_time:13670ms step_avg:32.70ms
step:419/1775 train_time:13702ms step_avg:32.70ms
step:420/1775 train_time:13735ms step_avg:32.70ms
step:421/1775 train_time:13767ms step_avg:32.70ms
step:422/1775 train_time:13800ms step_avg:32.70ms
step:423/1775 train_time:13831ms step_avg:32.70ms
step:424/1775 train_time:13864ms step_avg:32.70ms
step:425/1775 train_time:13895ms step_avg:32.69ms
step:426/1775 train_time:13929ms step_avg:32.70ms
step:427/1775 train_time:13960ms step_avg:32.69ms
step:428/1775 train_time:13993ms step_avg:32.69ms
step:429/1775 train_time:14024ms step_avg:32.69ms
step:430/1775 train_time:14058ms step_avg:32.69ms
step:431/1775 train_time:14089ms step_avg:32.69ms
step:432/1775 train_time:14122ms step_avg:32.69ms
step:433/1775 train_time:14153ms step_avg:32.69ms
step:434/1775 train_time:14187ms step_avg:32.69ms
step:435/1775 train_time:14218ms step_avg:32.68ms
step:436/1775 train_time:14251ms step_avg:32.69ms
step:437/1775 train_time:14282ms step_avg:32.68ms
step:438/1775 train_time:14315ms step_avg:32.68ms
step:439/1775 train_time:14346ms step_avg:32.68ms
step:440/1775 train_time:14379ms step_avg:32.68ms
step:441/1775 train_time:14410ms step_avg:32.68ms
step:442/1775 train_time:14443ms step_avg:32.68ms
step:443/1775 train_time:14475ms step_avg:32.67ms
step:444/1775 train_time:14508ms step_avg:32.68ms
step:445/1775 train_time:14540ms step_avg:32.67ms
step:446/1775 train_time:14573ms step_avg:32.67ms
step:447/1775 train_time:14604ms step_avg:32.67ms
step:448/1775 train_time:14638ms step_avg:32.67ms
step:449/1775 train_time:14669ms step_avg:32.67ms
step:450/1775 train_time:14702ms step_avg:32.67ms
step:451/1775 train_time:14734ms step_avg:32.67ms
step:452/1775 train_time:14767ms step_avg:32.67ms
step:453/1775 train_time:14798ms step_avg:32.67ms
step:454/1775 train_time:14831ms step_avg:32.67ms
step:455/1775 train_time:14863ms step_avg:32.67ms
step:456/1775 train_time:14896ms step_avg:32.67ms
step:457/1775 train_time:14927ms step_avg:32.66ms
step:458/1775 train_time:14961ms step_avg:32.67ms
step:459/1775 train_time:14992ms step_avg:32.66ms
step:460/1775 train_time:15025ms step_avg:32.66ms
step:461/1775 train_time:15057ms step_avg:32.66ms
step:462/1775 train_time:15090ms step_avg:32.66ms
step:463/1775 train_time:15121ms step_avg:32.66ms
step:464/1775 train_time:15155ms step_avg:32.66ms
step:465/1775 train_time:15186ms step_avg:32.66ms
step:466/1775 train_time:15220ms step_avg:32.66ms
step:467/1775 train_time:15251ms step_avg:32.66ms
step:468/1775 train_time:15284ms step_avg:32.66ms
step:469/1775 train_time:15315ms step_avg:32.65ms
step:470/1775 train_time:15348ms step_avg:32.66ms
step:471/1775 train_time:15380ms step_avg:32.65ms
step:472/1775 train_time:15413ms step_avg:32.65ms
step:473/1775 train_time:15444ms step_avg:32.65ms
step:474/1775 train_time:15478ms step_avg:32.65ms
step:475/1775 train_time:15509ms step_avg:32.65ms
step:476/1775 train_time:15542ms step_avg:32.65ms
step:477/1775 train_time:15573ms step_avg:32.65ms
step:478/1775 train_time:15607ms step_avg:32.65ms
step:479/1775 train_time:15638ms step_avg:32.65ms
step:480/1775 train_time:15671ms step_avg:32.65ms
step:481/1775 train_time:15703ms step_avg:32.65ms
step:482/1775 train_time:15737ms step_avg:32.65ms
step:483/1775 train_time:15768ms step_avg:32.65ms
step:484/1775 train_time:15802ms step_avg:32.65ms
step:485/1775 train_time:15833ms step_avg:32.64ms
step:486/1775 train_time:15866ms step_avg:32.65ms
step:487/1775 train_time:15898ms step_avg:32.64ms
step:488/1775 train_time:15931ms step_avg:32.64ms
step:489/1775 train_time:15962ms step_avg:32.64ms
step:490/1775 train_time:15995ms step_avg:32.64ms
step:491/1775 train_time:16026ms step_avg:32.64ms
step:492/1775 train_time:16060ms step_avg:32.64ms
step:493/1775 train_time:16090ms step_avg:32.64ms
step:494/1775 train_time:16124ms step_avg:32.64ms
step:495/1775 train_time:16155ms step_avg:32.64ms
step:496/1775 train_time:16189ms step_avg:32.64ms
step:497/1775 train_time:16221ms step_avg:32.64ms
step:498/1775 train_time:16254ms step_avg:32.64ms
step:499/1775 train_time:16285ms step_avg:32.64ms
step:500/1775 train_time:16319ms step_avg:32.64ms
step:500/1775 val_loss:4.2756 train_time:16360ms step_avg:32.72ms
step:501/1775 train_time:16382ms step_avg:32.70ms
step:502/1775 train_time:16404ms step_avg:32.68ms
step:503/1775 train_time:16424ms step_avg:32.65ms
step:504/1775 train_time:16450ms step_avg:32.64ms
step:505/1775 train_time:16482ms step_avg:32.64ms
step:506/1775 train_time:16517ms step_avg:32.64ms
step:507/1775 train_time:16549ms step_avg:32.64ms
step:508/1775 train_time:16582ms step_avg:32.64ms
step:509/1775 train_time:16612ms step_avg:32.64ms
step:510/1775 train_time:16646ms step_avg:32.64ms
step:511/1775 train_time:16677ms step_avg:32.64ms
step:512/1775 train_time:16710ms step_avg:32.64ms
step:513/1775 train_time:16740ms step_avg:32.63ms
step:514/1775 train_time:16773ms step_avg:32.63ms
step:515/1775 train_time:16804ms step_avg:32.63ms
step:516/1775 train_time:16837ms step_avg:32.63ms
step:517/1775 train_time:16868ms step_avg:32.63ms
step:518/1775 train_time:16901ms step_avg:32.63ms
step:519/1775 train_time:16932ms step_avg:32.62ms
step:520/1775 train_time:16965ms step_avg:32.62ms
step:521/1775 train_time:16995ms step_avg:32.62ms
step:522/1775 train_time:17029ms step_avg:32.62ms
step:523/1775 train_time:17059ms step_avg:32.62ms
step:524/1775 train_time:17092ms step_avg:32.62ms
step:525/1775 train_time:17123ms step_avg:32.61ms
step:526/1775 train_time:17156ms step_avg:32.62ms
step:527/1775 train_time:17187ms step_avg:32.61ms
step:528/1775 train_time:17219ms step_avg:32.61ms
step:529/1775 train_time:17250ms step_avg:32.61ms
step:530/1775 train_time:17283ms step_avg:32.61ms
step:531/1775 train_time:17314ms step_avg:32.61ms
step:532/1775 train_time:17348ms step_avg:32.61ms
step:533/1775 train_time:17379ms step_avg:32.61ms
step:534/1775 train_time:17413ms step_avg:32.61ms
step:535/1775 train_time:17445ms step_avg:32.61ms
step:536/1775 train_time:17479ms step_avg:32.61ms
step:537/1775 train_time:17510ms step_avg:32.61ms
step:538/1775 train_time:17544ms step_avg:32.61ms
step:539/1775 train_time:17575ms step_avg:32.61ms
step:540/1775 train_time:17608ms step_avg:32.61ms
step:541/1775 train_time:17639ms step_avg:32.61ms
step:542/1775 train_time:17673ms step_avg:32.61ms
step:543/1775 train_time:17703ms step_avg:32.60ms
step:544/1775 train_time:17737ms step_avg:32.60ms
step:545/1775 train_time:17767ms step_avg:32.60ms
step:546/1775 train_time:17801ms step_avg:32.60ms
step:547/1775 train_time:17832ms step_avg:32.60ms
step:548/1775 train_time:17865ms step_avg:32.60ms
step:549/1775 train_time:17896ms step_avg:32.60ms
step:550/1775 train_time:17930ms step_avg:32.60ms
step:551/1775 train_time:17961ms step_avg:32.60ms
step:552/1775 train_time:17994ms step_avg:32.60ms
step:553/1775 train_time:18025ms step_avg:32.60ms
step:554/1775 train_time:18059ms step_avg:32.60ms
step:555/1775 train_time:18089ms step_avg:32.59ms
step:556/1775 train_time:18123ms step_avg:32.59ms
step:557/1775 train_time:18153ms step_avg:32.59ms
step:558/1775 train_time:18187ms step_avg:32.59ms
step:559/1775 train_time:18219ms step_avg:32.59ms
step:560/1775 train_time:18252ms step_avg:32.59ms
step:561/1775 train_time:18284ms step_avg:32.59ms
step:562/1775 train_time:18317ms step_avg:32.59ms
step:563/1775 train_time:18348ms step_avg:32.59ms
step:564/1775 train_time:18381ms step_avg:32.59ms
step:565/1775 train_time:18413ms step_avg:32.59ms
step:566/1775 train_time:18446ms step_avg:32.59ms
step:567/1775 train_time:18477ms step_avg:32.59ms
step:568/1775 train_time:18511ms step_avg:32.59ms
step:569/1775 train_time:18542ms step_avg:32.59ms
step:570/1775 train_time:18576ms step_avg:32.59ms
step:571/1775 train_time:18607ms step_avg:32.59ms
step:572/1775 train_time:18641ms step_avg:32.59ms
step:573/1775 train_time:18671ms step_avg:32.59ms
step:574/1775 train_time:18705ms step_avg:32.59ms
step:575/1775 train_time:18736ms step_avg:32.58ms
step:576/1775 train_time:18769ms step_avg:32.58ms
step:577/1775 train_time:18801ms step_avg:32.58ms
step:578/1775 train_time:18834ms step_avg:32.58ms
step:579/1775 train_time:18865ms step_avg:32.58ms
step:580/1775 train_time:18901ms step_avg:32.59ms
step:581/1775 train_time:18959ms step_avg:32.63ms
step:582/1775 train_time:19017ms step_avg:32.68ms
step:583/1775 train_time:19075ms step_avg:32.72ms
step:584/1775 train_time:19135ms step_avg:32.76ms
step:585/1775 train_time:19192ms step_avg:32.81ms
step:586/1775 train_time:19253ms step_avg:32.86ms
step:587/1775 train_time:19311ms step_avg:32.90ms
step:588/1775 train_time:19370ms step_avg:32.94ms
step:589/1775 train_time:19429ms step_avg:32.99ms
step:590/1775 train_time:19490ms step_avg:33.03ms
step:591/1775 train_time:19549ms step_avg:33.08ms
step:592/1775 train_time:19610ms step_avg:33.13ms
step:593/1775 train_time:19669ms step_avg:33.17ms
step:594/1775 train_time:19728ms step_avg:33.21ms
step:595/1775 train_time:19785ms step_avg:33.25ms
step:596/1775 train_time:19844ms step_avg:33.30ms
step:597/1775 train_time:19901ms step_avg:33.33ms
step:598/1775 train_time:19961ms step_avg:33.38ms
step:599/1775 train_time:20018ms step_avg:33.42ms
step:600/1775 train_time:20078ms step_avg:33.46ms
step:601/1775 train_time:20135ms step_avg:33.50ms
step:602/1775 train_time:20195ms step_avg:33.55ms
step:603/1775 train_time:20255ms step_avg:33.59ms
step:604/1775 train_time:20315ms step_avg:33.63ms
step:605/1775 train_time:20373ms step_avg:33.67ms
step:606/1775 train_time:20434ms step_avg:33.72ms
step:607/1775 train_time:20494ms step_avg:33.76ms
step:608/1775 train_time:20555ms step_avg:33.81ms
step:609/1775 train_time:20613ms step_avg:33.85ms
step:610/1775 train_time:20674ms step_avg:33.89ms
step:611/1775 train_time:20733ms step_avg:33.93ms
step:612/1775 train_time:20794ms step_avg:33.98ms
step:613/1775 train_time:20852ms step_avg:34.02ms
step:614/1775 train_time:20912ms step_avg:34.06ms
step:615/1775 train_time:20969ms step_avg:34.10ms
step:616/1775 train_time:21029ms step_avg:34.14ms
step:617/1775 train_time:21086ms step_avg:34.17ms
step:618/1775 train_time:21145ms step_avg:34.22ms
step:619/1775 train_time:21202ms step_avg:34.25ms
step:620/1775 train_time:21262ms step_avg:34.29ms
step:621/1775 train_time:21319ms step_avg:34.33ms
step:622/1775 train_time:21379ms step_avg:34.37ms
step:623/1775 train_time:21437ms step_avg:34.41ms
step:624/1775 train_time:21498ms step_avg:34.45ms
step:625/1775 train_time:21557ms step_avg:34.49ms
step:626/1775 train_time:21616ms step_avg:34.53ms
step:627/1775 train_time:21675ms step_avg:34.57ms
step:628/1775 train_time:21736ms step_avg:34.61ms
step:629/1775 train_time:21795ms step_avg:34.65ms
step:630/1775 train_time:21854ms step_avg:34.69ms
step:631/1775 train_time:21912ms step_avg:34.73ms
step:632/1775 train_time:21973ms step_avg:34.77ms
step:633/1775 train_time:22031ms step_avg:34.80ms
step:634/1775 train_time:22092ms step_avg:34.84ms
step:635/1775 train_time:22150ms step_avg:34.88ms
step:636/1775 train_time:22210ms step_avg:34.92ms
step:637/1775 train_time:22269ms step_avg:34.96ms
step:638/1775 train_time:22329ms step_avg:35.00ms
step:639/1775 train_time:22387ms step_avg:35.03ms
step:640/1775 train_time:22447ms step_avg:35.07ms
step:641/1775 train_time:22505ms step_avg:35.11ms
step:642/1775 train_time:22565ms step_avg:35.15ms
step:643/1775 train_time:22622ms step_avg:35.18ms
step:644/1775 train_time:22683ms step_avg:35.22ms
step:645/1775 train_time:22741ms step_avg:35.26ms
step:646/1775 train_time:22802ms step_avg:35.30ms
step:647/1775 train_time:22859ms step_avg:35.33ms
step:648/1775 train_time:22918ms step_avg:35.37ms
step:649/1775 train_time:22977ms step_avg:35.40ms
step:650/1775 train_time:23038ms step_avg:35.44ms
step:651/1775 train_time:23095ms step_avg:35.48ms
step:652/1775 train_time:23155ms step_avg:35.51ms
step:653/1775 train_time:23213ms step_avg:35.55ms
step:654/1775 train_time:23274ms step_avg:35.59ms
step:655/1775 train_time:23332ms step_avg:35.62ms
step:656/1775 train_time:23393ms step_avg:35.66ms
step:657/1775 train_time:23451ms step_avg:35.69ms
step:658/1775 train_time:23513ms step_avg:35.73ms
step:659/1775 train_time:23572ms step_avg:35.77ms
step:660/1775 train_time:23632ms step_avg:35.81ms
step:661/1775 train_time:23691ms step_avg:35.84ms
step:662/1775 train_time:23751ms step_avg:35.88ms
step:663/1775 train_time:23808ms step_avg:35.91ms
step:664/1775 train_time:23869ms step_avg:35.95ms
step:665/1775 train_time:23927ms step_avg:35.98ms
step:666/1775 train_time:23986ms step_avg:36.02ms
step:667/1775 train_time:24044ms step_avg:36.05ms
step:668/1775 train_time:24104ms step_avg:36.08ms
step:669/1775 train_time:24161ms step_avg:36.11ms
step:670/1775 train_time:24220ms step_avg:36.15ms
step:671/1775 train_time:24278ms step_avg:36.18ms
step:672/1775 train_time:24338ms step_avg:36.22ms
step:673/1775 train_time:24396ms step_avg:36.25ms
step:674/1775 train_time:24456ms step_avg:36.29ms
step:675/1775 train_time:24515ms step_avg:36.32ms
step:676/1775 train_time:24574ms step_avg:36.35ms
step:677/1775 train_time:24634ms step_avg:36.39ms
step:678/1775 train_time:24695ms step_avg:36.42ms
step:679/1775 train_time:24753ms step_avg:36.46ms
step:680/1775 train_time:24813ms step_avg:36.49ms
step:681/1775 train_time:24871ms step_avg:36.52ms
step:682/1775 train_time:24932ms step_avg:36.56ms
step:683/1775 train_time:24991ms step_avg:36.59ms
step:684/1775 train_time:25051ms step_avg:36.62ms
step:685/1775 train_time:25109ms step_avg:36.66ms
step:686/1775 train_time:25169ms step_avg:36.69ms
step:687/1775 train_time:25226ms step_avg:36.72ms
step:688/1775 train_time:25285ms step_avg:36.75ms
step:689/1775 train_time:25343ms step_avg:36.78ms
step:690/1775 train_time:25404ms step_avg:36.82ms
step:691/1775 train_time:25460ms step_avg:36.84ms
step:692/1775 train_time:25520ms step_avg:36.88ms
step:693/1775 train_time:25578ms step_avg:36.91ms
step:694/1775 train_time:25639ms step_avg:36.94ms
step:695/1775 train_time:25697ms step_avg:36.97ms
step:696/1775 train_time:25758ms step_avg:37.01ms
step:697/1775 train_time:25816ms step_avg:37.04ms
step:698/1775 train_time:25877ms step_avg:37.07ms
step:699/1775 train_time:25936ms step_avg:37.10ms
step:700/1775 train_time:25997ms step_avg:37.14ms
step:701/1775 train_time:26054ms step_avg:37.17ms
step:702/1775 train_time:26114ms step_avg:37.20ms
step:703/1775 train_time:26172ms step_avg:37.23ms
step:704/1775 train_time:26233ms step_avg:37.26ms
step:705/1775 train_time:26291ms step_avg:37.29ms
step:706/1775 train_time:26351ms step_avg:37.32ms
step:707/1775 train_time:26409ms step_avg:37.35ms
step:708/1775 train_time:26470ms step_avg:37.39ms
step:709/1775 train_time:26527ms step_avg:37.41ms
step:710/1775 train_time:26587ms step_avg:37.45ms
step:711/1775 train_time:26644ms step_avg:37.47ms
step:712/1775 train_time:26705ms step_avg:37.51ms
step:713/1775 train_time:26762ms step_avg:37.53ms
step:714/1775 train_time:26822ms step_avg:37.57ms
step:715/1775 train_time:26880ms step_avg:37.59ms
step:716/1775 train_time:26940ms step_avg:37.63ms
step:717/1775 train_time:26999ms step_avg:37.66ms
step:718/1775 train_time:27058ms step_avg:37.69ms
step:719/1775 train_time:27116ms step_avg:37.71ms
step:720/1775 train_time:27176ms step_avg:37.74ms
step:721/1775 train_time:27235ms step_avg:37.77ms
step:722/1775 train_time:27295ms step_avg:37.80ms
step:723/1775 train_time:27353ms step_avg:37.83ms
step:724/1775 train_time:27413ms step_avg:37.86ms
step:725/1775 train_time:27471ms step_avg:37.89ms
step:726/1775 train_time:27532ms step_avg:37.92ms
step:727/1775 train_time:27591ms step_avg:37.95ms
step:728/1775 train_time:27653ms step_avg:37.98ms
step:729/1775 train_time:27711ms step_avg:38.01ms
step:730/1775 train_time:27772ms step_avg:38.04ms
step:731/1775 train_time:27830ms step_avg:38.07ms
step:732/1775 train_time:27890ms step_avg:38.10ms
step:733/1775 train_time:27947ms step_avg:38.13ms
step:734/1775 train_time:28006ms step_avg:38.16ms
step:735/1775 train_time:28063ms step_avg:38.18ms
step:736/1775 train_time:28124ms step_avg:38.21ms
step:737/1775 train_time:28181ms step_avg:38.24ms
step:738/1775 train_time:28242ms step_avg:38.27ms
step:739/1775 train_time:28299ms step_avg:38.29ms
step:740/1775 train_time:28358ms step_avg:38.32ms
step:741/1775 train_time:28416ms step_avg:38.35ms
step:742/1775 train_time:28477ms step_avg:38.38ms
step:743/1775 train_time:28535ms step_avg:38.41ms
step:744/1775 train_time:28596ms step_avg:38.44ms
step:745/1775 train_time:28654ms step_avg:38.46ms
step:746/1775 train_time:28715ms step_avg:38.49ms
step:747/1775 train_time:28775ms step_avg:38.52ms
step:748/1775 train_time:28836ms step_avg:38.55ms
step:749/1775 train_time:28893ms step_avg:38.58ms
step:750/1775 train_time:28954ms step_avg:38.61ms
step:750/1775 val_loss:4.0010 train_time:29023ms step_avg:38.70ms
step:751/1775 train_time:29045ms step_avg:38.68ms
step:752/1775 train_time:29071ms step_avg:38.66ms
step:753/1775 train_time:29129ms step_avg:38.68ms
step:754/1775 train_time:29192ms step_avg:38.72ms
step:755/1775 train_time:29251ms step_avg:38.74ms
step:756/1775 train_time:29311ms step_avg:38.77ms
step:757/1775 train_time:29368ms step_avg:38.80ms
step:758/1775 train_time:29428ms step_avg:38.82ms
step:759/1775 train_time:29485ms step_avg:38.85ms
step:760/1775 train_time:29545ms step_avg:38.87ms
step:761/1775 train_time:29602ms step_avg:38.90ms
step:762/1775 train_time:29661ms step_avg:38.93ms
step:763/1775 train_time:29717ms step_avg:38.95ms
step:764/1775 train_time:29776ms step_avg:38.97ms
step:765/1775 train_time:29833ms step_avg:39.00ms
step:766/1775 train_time:29892ms step_avg:39.02ms
step:767/1775 train_time:29951ms step_avg:39.05ms
step:768/1775 train_time:30013ms step_avg:39.08ms
step:769/1775 train_time:30072ms step_avg:39.11ms
step:770/1775 train_time:30134ms step_avg:39.13ms
step:771/1775 train_time:30192ms step_avg:39.16ms
step:772/1775 train_time:30252ms step_avg:39.19ms
step:773/1775 train_time:30310ms step_avg:39.21ms
step:774/1775 train_time:30369ms step_avg:39.24ms
step:775/1775 train_time:30427ms step_avg:39.26ms
step:776/1775 train_time:30487ms step_avg:39.29ms
step:777/1775 train_time:30545ms step_avg:39.31ms
step:778/1775 train_time:30605ms step_avg:39.34ms
step:779/1775 train_time:30663ms step_avg:39.36ms
step:780/1775 train_time:30722ms step_avg:39.39ms
step:781/1775 train_time:30779ms step_avg:39.41ms
step:782/1775 train_time:30839ms step_avg:39.44ms
step:783/1775 train_time:30896ms step_avg:39.46ms
step:784/1775 train_time:30956ms step_avg:39.48ms
step:785/1775 train_time:31015ms step_avg:39.51ms
step:786/1775 train_time:31074ms step_avg:39.53ms
step:787/1775 train_time:31134ms step_avg:39.56ms
step:788/1775 train_time:31192ms step_avg:39.58ms
step:789/1775 train_time:31250ms step_avg:39.61ms
step:790/1775 train_time:31310ms step_avg:39.63ms
step:791/1775 train_time:31368ms step_avg:39.66ms
step:792/1775 train_time:31429ms step_avg:39.68ms
step:793/1775 train_time:31486ms step_avg:39.70ms
step:794/1775 train_time:31546ms step_avg:39.73ms
step:795/1775 train_time:31604ms step_avg:39.75ms
step:796/1775 train_time:31664ms step_avg:39.78ms
step:797/1775 train_time:31722ms step_avg:39.80ms
step:798/1775 train_time:31783ms step_avg:39.83ms
step:799/1775 train_time:31840ms step_avg:39.85ms
step:800/1775 train_time:31900ms step_avg:39.87ms
step:801/1775 train_time:31957ms step_avg:39.90ms
step:802/1775 train_time:32017ms step_avg:39.92ms
step:803/1775 train_time:32075ms step_avg:39.94ms
step:804/1775 train_time:32136ms step_avg:39.97ms
step:805/1775 train_time:32193ms step_avg:39.99ms
step:806/1775 train_time:32254ms step_avg:40.02ms
step:807/1775 train_time:32311ms step_avg:40.04ms
step:808/1775 train_time:32371ms step_avg:40.06ms
step:809/1775 train_time:32429ms step_avg:40.09ms
step:810/1775 train_time:32489ms step_avg:40.11ms
step:811/1775 train_time:32547ms step_avg:40.13ms
step:812/1775 train_time:32608ms step_avg:40.16ms
step:813/1775 train_time:32665ms step_avg:40.18ms
step:814/1775 train_time:32726ms step_avg:40.20ms
step:815/1775 train_time:32785ms step_avg:40.23ms
step:816/1775 train_time:32845ms step_avg:40.25ms
step:817/1775 train_time:32905ms step_avg:40.27ms
step:818/1775 train_time:32966ms step_avg:40.30ms
step:819/1775 train_time:33024ms step_avg:40.32ms
step:820/1775 train_time:33084ms step_avg:40.35ms
step:821/1775 train_time:33143ms step_avg:40.37ms
step:822/1775 train_time:33203ms step_avg:40.39ms
step:823/1775 train_time:33262ms step_avg:40.42ms
step:824/1775 train_time:33321ms step_avg:40.44ms
step:825/1775 train_time:33379ms step_avg:40.46ms
step:826/1775 train_time:33440ms step_avg:40.48ms
step:827/1775 train_time:33497ms step_avg:40.50ms
step:828/1775 train_time:33558ms step_avg:40.53ms
step:829/1775 train_time:33615ms step_avg:40.55ms
step:830/1775 train_time:33675ms step_avg:40.57ms
step:831/1775 train_time:33733ms step_avg:40.59ms
step:832/1775 train_time:33792ms step_avg:40.62ms
step:833/1775 train_time:33850ms step_avg:40.64ms
step:834/1775 train_time:33910ms step_avg:40.66ms
step:835/1775 train_time:33969ms step_avg:40.68ms
step:836/1775 train_time:34029ms step_avg:40.70ms
step:837/1775 train_time:34088ms step_avg:40.73ms
step:838/1775 train_time:34148ms step_avg:40.75ms
step:839/1775 train_time:34206ms step_avg:40.77ms
step:840/1775 train_time:34266ms step_avg:40.79ms
step:841/1775 train_time:34325ms step_avg:40.81ms
step:842/1775 train_time:34385ms step_avg:40.84ms
step:843/1775 train_time:34444ms step_avg:40.86ms
step:844/1775 train_time:34504ms step_avg:40.88ms
step:845/1775 train_time:34562ms step_avg:40.90ms
step:846/1775 train_time:34622ms step_avg:40.92ms
step:847/1775 train_time:34679ms step_avg:40.94ms
step:848/1775 train_time:34738ms step_avg:40.96ms
step:849/1775 train_time:34794ms step_avg:40.98ms
step:850/1775 train_time:34855ms step_avg:41.01ms
step:851/1775 train_time:34912ms step_avg:41.03ms
step:852/1775 train_time:34973ms step_avg:41.05ms
step:853/1775 train_time:35030ms step_avg:41.07ms
step:854/1775 train_time:35090ms step_avg:41.09ms
step:855/1775 train_time:35148ms step_avg:41.11ms
step:856/1775 train_time:35208ms step_avg:41.13ms
step:857/1775 train_time:35267ms step_avg:41.15ms
step:858/1775 train_time:35328ms step_avg:41.17ms
step:859/1775 train_time:35386ms step_avg:41.19ms
step:860/1775 train_time:35446ms step_avg:41.22ms
step:861/1775 train_time:35504ms step_avg:41.24ms
step:862/1775 train_time:35565ms step_avg:41.26ms
step:863/1775 train_time:35623ms step_avg:41.28ms
step:864/1775 train_time:35684ms step_avg:41.30ms
step:865/1775 train_time:35742ms step_avg:41.32ms
step:866/1775 train_time:35801ms step_avg:41.34ms
step:867/1775 train_time:35859ms step_avg:41.36ms
step:868/1775 train_time:35919ms step_avg:41.38ms
step:869/1775 train_time:35976ms step_avg:41.40ms
step:870/1775 train_time:36037ms step_avg:41.42ms
step:871/1775 train_time:36094ms step_avg:41.44ms
step:872/1775 train_time:36155ms step_avg:41.46ms
step:873/1775 train_time:36212ms step_avg:41.48ms
step:874/1775 train_time:36273ms step_avg:41.50ms
step:875/1775 train_time:36331ms step_avg:41.52ms
step:876/1775 train_time:36391ms step_avg:41.54ms
step:877/1775 train_time:36449ms step_avg:41.56ms
step:878/1775 train_time:36509ms step_avg:41.58ms
step:879/1775 train_time:36567ms step_avg:41.60ms
step:880/1775 train_time:36628ms step_avg:41.62ms
step:881/1775 train_time:36687ms step_avg:41.64ms
step:882/1775 train_time:36748ms step_avg:41.66ms
step:883/1775 train_time:36806ms step_avg:41.68ms
step:884/1775 train_time:36866ms step_avg:41.70ms
step:885/1775 train_time:36925ms step_avg:41.72ms
step:886/1775 train_time:36985ms step_avg:41.74ms
step:887/1775 train_time:37044ms step_avg:41.76ms
step:888/1775 train_time:37103ms step_avg:41.78ms
step:889/1775 train_time:37160ms step_avg:41.80ms
step:890/1775 train_time:37219ms step_avg:41.82ms
step:891/1775 train_time:37276ms step_avg:41.84ms
step:892/1775 train_time:37336ms step_avg:41.86ms
step:893/1775 train_time:37393ms step_avg:41.87ms
step:894/1775 train_time:37453ms step_avg:41.89ms
step:895/1775 train_time:37511ms step_avg:41.91ms
step:896/1775 train_time:37572ms step_avg:41.93ms
step:897/1775 train_time:37629ms step_avg:41.95ms
step:898/1775 train_time:37690ms step_avg:41.97ms
step:899/1775 train_time:37748ms step_avg:41.99ms
step:900/1775 train_time:37809ms step_avg:42.01ms
step:901/1775 train_time:37867ms step_avg:42.03ms
step:902/1775 train_time:37929ms step_avg:42.05ms
step:903/1775 train_time:37986ms step_avg:42.07ms
step:904/1775 train_time:38047ms step_avg:42.09ms
step:905/1775 train_time:38105ms step_avg:42.11ms
step:906/1775 train_time:38166ms step_avg:42.13ms
step:907/1775 train_time:38225ms step_avg:42.14ms
step:908/1775 train_time:38284ms step_avg:42.16ms
step:909/1775 train_time:38341ms step_avg:42.18ms
step:910/1775 train_time:38401ms step_avg:42.20ms
step:911/1775 train_time:38459ms step_avg:42.22ms
step:912/1775 train_time:38520ms step_avg:42.24ms
step:913/1775 train_time:38577ms step_avg:42.25ms
step:914/1775 train_time:38637ms step_avg:42.27ms
step:915/1775 train_time:38694ms step_avg:42.29ms
step:916/1775 train_time:38755ms step_avg:42.31ms
step:917/1775 train_time:38812ms step_avg:42.32ms
step:918/1775 train_time:38872ms step_avg:42.34ms
step:919/1775 train_time:38930ms step_avg:42.36ms
step:920/1775 train_time:38990ms step_avg:42.38ms
step:921/1775 train_time:39049ms step_avg:42.40ms
step:922/1775 train_time:39109ms step_avg:42.42ms
step:923/1775 train_time:39168ms step_avg:42.44ms
step:924/1775 train_time:39228ms step_avg:42.45ms
step:925/1775 train_time:39286ms step_avg:42.47ms
step:926/1775 train_time:39347ms step_avg:42.49ms
step:927/1775 train_time:39405ms step_avg:42.51ms
step:928/1775 train_time:39466ms step_avg:42.53ms
step:929/1775 train_time:39524ms step_avg:42.54ms
step:930/1775 train_time:39584ms step_avg:42.56ms
step:931/1775 train_time:39642ms step_avg:42.58ms
step:932/1775 train_time:39701ms step_avg:42.60ms
step:933/1775 train_time:39759ms step_avg:42.61ms
step:934/1775 train_time:39819ms step_avg:42.63ms
step:935/1775 train_time:39876ms step_avg:42.65ms
step:936/1775 train_time:39937ms step_avg:42.67ms
step:937/1775 train_time:39993ms step_avg:42.68ms
step:938/1775 train_time:40054ms step_avg:42.70ms
step:939/1775 train_time:40112ms step_avg:42.72ms
step:940/1775 train_time:40172ms step_avg:42.74ms
step:941/1775 train_time:40231ms step_avg:42.75ms
step:942/1775 train_time:40291ms step_avg:42.77ms
step:943/1775 train_time:40349ms step_avg:42.79ms
step:944/1775 train_time:40411ms step_avg:42.81ms
step:945/1775 train_time:40468ms step_avg:42.82ms
step:946/1775 train_time:40529ms step_avg:42.84ms
step:947/1775 train_time:40587ms step_avg:42.86ms
step:948/1775 train_time:40647ms step_avg:42.88ms
step:949/1775 train_time:40705ms step_avg:42.89ms
step:950/1775 train_time:40766ms step_avg:42.91ms
step:951/1775 train_time:40824ms step_avg:42.93ms
step:952/1775 train_time:40884ms step_avg:42.95ms
step:953/1775 train_time:40942ms step_avg:42.96ms
step:954/1775 train_time:41002ms step_avg:42.98ms
step:955/1775 train_time:41058ms step_avg:42.99ms
step:956/1775 train_time:41120ms step_avg:43.01ms
step:957/1775 train_time:41177ms step_avg:43.03ms
step:958/1775 train_time:41237ms step_avg:43.05ms
step:959/1775 train_time:41294ms step_avg:43.06ms
step:960/1775 train_time:41354ms step_avg:43.08ms
step:961/1775 train_time:41412ms step_avg:43.09ms
step:962/1775 train_time:41473ms step_avg:43.11ms
step:963/1775 train_time:41531ms step_avg:43.13ms
step:964/1775 train_time:41591ms step_avg:43.14ms
step:965/1775 train_time:41649ms step_avg:43.16ms
step:966/1775 train_time:41709ms step_avg:43.18ms
step:967/1775 train_time:41768ms step_avg:43.19ms
step:968/1775 train_time:41828ms step_avg:43.21ms
step:969/1775 train_time:41886ms step_avg:43.23ms
step:970/1775 train_time:41946ms step_avg:43.24ms
step:971/1775 train_time:42004ms step_avg:43.26ms
step:972/1775 train_time:42064ms step_avg:43.28ms
step:973/1775 train_time:42123ms step_avg:43.29ms
step:974/1775 train_time:42183ms step_avg:43.31ms
step:975/1775 train_time:42239ms step_avg:43.32ms
step:976/1775 train_time:42299ms step_avg:43.34ms
step:977/1775 train_time:42356ms step_avg:43.35ms
step:978/1775 train_time:42416ms step_avg:43.37ms
step:979/1775 train_time:42474ms step_avg:43.38ms
step:980/1775 train_time:42534ms step_avg:43.40ms
step:981/1775 train_time:42591ms step_avg:43.42ms
step:982/1775 train_time:42651ms step_avg:43.43ms
step:983/1775 train_time:42708ms step_avg:43.45ms
step:984/1775 train_time:42768ms step_avg:43.46ms
step:985/1775 train_time:42826ms step_avg:43.48ms
step:986/1775 train_time:42887ms step_avg:43.50ms
step:987/1775 train_time:42946ms step_avg:43.51ms
step:988/1775 train_time:43007ms step_avg:43.53ms
step:989/1775 train_time:43065ms step_avg:43.54ms
step:990/1775 train_time:43126ms step_avg:43.56ms
step:991/1775 train_time:43184ms step_avg:43.58ms
step:992/1775 train_time:43245ms step_avg:43.59ms
step:993/1775 train_time:43303ms step_avg:43.61ms
step:994/1775 train_time:43364ms step_avg:43.63ms
step:995/1775 train_time:43423ms step_avg:43.64ms
step:996/1775 train_time:43484ms step_avg:43.66ms
step:997/1775 train_time:43540ms step_avg:43.67ms
step:998/1775 train_time:43601ms step_avg:43.69ms
step:999/1775 train_time:43658ms step_avg:43.70ms
step:1000/1775 train_time:43718ms step_avg:43.72ms
step:1000/1775 val_loss:3.7371 train_time:43789ms step_avg:43.79ms
step:1001/1775 train_time:43812ms step_avg:43.77ms
step:1002/1775 train_time:43838ms step_avg:43.75ms
step:1003/1775 train_time:43895ms step_avg:43.76ms
step:1004/1775 train_time:43961ms step_avg:43.79ms
step:1005/1775 train_time:44021ms step_avg:43.80ms
step:1006/1775 train_time:44081ms step_avg:43.82ms
step:1007/1775 train_time:44139ms step_avg:43.83ms
step:1008/1775 train_time:44198ms step_avg:43.85ms
step:1009/1775 train_time:44256ms step_avg:43.86ms
step:1010/1775 train_time:44316ms step_avg:43.88ms
step:1011/1775 train_time:44373ms step_avg:43.89ms
step:1012/1775 train_time:44433ms step_avg:43.91ms
step:1013/1775 train_time:44489ms step_avg:43.92ms
step:1014/1775 train_time:44549ms step_avg:43.93ms
step:1015/1775 train_time:44606ms step_avg:43.95ms
step:1016/1775 train_time:44666ms step_avg:43.96ms
step:1017/1775 train_time:44723ms step_avg:43.98ms
step:1018/1775 train_time:44785ms step_avg:43.99ms
step:1019/1775 train_time:44843ms step_avg:44.01ms
step:1020/1775 train_time:44906ms step_avg:44.03ms
step:1021/1775 train_time:44963ms step_avg:44.04ms
step:1022/1775 train_time:45024ms step_avg:44.05ms
step:1023/1775 train_time:45082ms step_avg:44.07ms
step:1024/1775 train_time:45142ms step_avg:44.08ms
step:1025/1775 train_time:45200ms step_avg:44.10ms
step:1026/1775 train_time:45261ms step_avg:44.11ms
step:1027/1775 train_time:45317ms step_avg:44.13ms
step:1028/1775 train_time:45378ms step_avg:44.14ms
step:1029/1775 train_time:45434ms step_avg:44.15ms
step:1030/1775 train_time:45494ms step_avg:44.17ms
step:1031/1775 train_time:45552ms step_avg:44.18ms
step:1032/1775 train_time:45612ms step_avg:44.20ms
step:1033/1775 train_time:45670ms step_avg:44.21ms
step:1034/1775 train_time:45730ms step_avg:44.23ms
step:1035/1775 train_time:45790ms step_avg:44.24ms
step:1036/1775 train_time:45853ms step_avg:44.26ms
step:1037/1775 train_time:45912ms step_avg:44.27ms
step:1038/1775 train_time:45974ms step_avg:44.29ms
step:1039/1775 train_time:46034ms step_avg:44.31ms
step:1040/1775 train_time:46094ms step_avg:44.32ms
step:1041/1775 train_time:46153ms step_avg:44.33ms
step:1042/1775 train_time:46214ms step_avg:44.35ms
step:1043/1775 train_time:46271ms step_avg:44.36ms
step:1044/1775 train_time:46332ms step_avg:44.38ms
step:1045/1775 train_time:46388ms step_avg:44.39ms
step:1046/1775 train_time:46448ms step_avg:44.41ms
step:1047/1775 train_time:46505ms step_avg:44.42ms
step:1048/1775 train_time:46565ms step_avg:44.43ms
step:1049/1775 train_time:46620ms step_avg:44.44ms
step:1050/1775 train_time:46681ms step_avg:44.46ms
step:1051/1775 train_time:46738ms step_avg:44.47ms
step:1052/1775 train_time:46799ms step_avg:44.49ms
step:1053/1775 train_time:46858ms step_avg:44.50ms
step:1054/1775 train_time:46918ms step_avg:44.51ms
step:1055/1775 train_time:46977ms step_avg:44.53ms
step:1056/1775 train_time:47039ms step_avg:44.54ms
step:1057/1775 train_time:47097ms step_avg:44.56ms
step:1058/1775 train_time:47157ms step_avg:44.57ms
step:1059/1775 train_time:47215ms step_avg:44.58ms
step:1060/1775 train_time:47275ms step_avg:44.60ms
step:1061/1775 train_time:47332ms step_avg:44.61ms
step:1062/1775 train_time:47393ms step_avg:44.63ms
step:1063/1775 train_time:47452ms step_avg:44.64ms
step:1064/1775 train_time:47512ms step_avg:44.65ms
step:1065/1775 train_time:47571ms step_avg:44.67ms
step:1066/1775 train_time:47630ms step_avg:44.68ms
step:1067/1775 train_time:47688ms step_avg:44.69ms
step:1068/1775 train_time:47748ms step_avg:44.71ms
step:1069/1775 train_time:47806ms step_avg:44.72ms
step:1070/1775 train_time:47866ms step_avg:44.73ms
step:1071/1775 train_time:47924ms step_avg:44.75ms
step:1072/1775 train_time:47985ms step_avg:44.76ms
step:1073/1775 train_time:48042ms step_avg:44.77ms
step:1074/1775 train_time:48103ms step_avg:44.79ms
step:1075/1775 train_time:48160ms step_avg:44.80ms
step:1076/1775 train_time:48220ms step_avg:44.81ms
step:1077/1775 train_time:48278ms step_avg:44.83ms
step:1078/1775 train_time:48338ms step_avg:44.84ms
step:1079/1775 train_time:48396ms step_avg:44.85ms
step:1080/1775 train_time:48456ms step_avg:44.87ms
step:1081/1775 train_time:48514ms step_avg:44.88ms
step:1082/1775 train_time:48574ms step_avg:44.89ms
step:1083/1775 train_time:48632ms step_avg:44.90ms
step:1084/1775 train_time:48693ms step_avg:44.92ms
step:1085/1775 train_time:48751ms step_avg:44.93ms
step:1086/1775 train_time:48812ms step_avg:44.95ms
step:1087/1775 train_time:48871ms step_avg:44.96ms
step:1088/1775 train_time:48931ms step_avg:44.97ms
step:1089/1775 train_time:48990ms step_avg:44.99ms
step:1090/1775 train_time:49051ms step_avg:45.00ms
step:1091/1775 train_time:49110ms step_avg:45.01ms
step:1092/1775 train_time:49170ms step_avg:45.03ms
step:1093/1775 train_time:49228ms step_avg:45.04ms
step:1094/1775 train_time:49290ms step_avg:45.05ms
step:1095/1775 train_time:49347ms step_avg:45.07ms
step:1096/1775 train_time:49407ms step_avg:45.08ms
step:1097/1775 train_time:49464ms step_avg:45.09ms
step:1098/1775 train_time:49525ms step_avg:45.10ms
step:1099/1775 train_time:49581ms step_avg:45.11ms
step:1100/1775 train_time:49641ms step_avg:45.13ms
step:1101/1775 train_time:49699ms step_avg:45.14ms
step:1102/1775 train_time:49759ms step_avg:45.15ms
step:1103/1775 train_time:49817ms step_avg:45.16ms
step:1104/1775 train_time:49879ms step_avg:45.18ms
step:1105/1775 train_time:49937ms step_avg:45.19ms
step:1106/1775 train_time:49998ms step_avg:45.21ms
step:1107/1775 train_time:50056ms step_avg:45.22ms
step:1108/1775 train_time:50116ms step_avg:45.23ms
step:1109/1775 train_time:50174ms step_avg:45.24ms
step:1110/1775 train_time:50234ms step_avg:45.26ms
step:1111/1775 train_time:50292ms step_avg:45.27ms
step:1112/1775 train_time:50353ms step_avg:45.28ms
step:1113/1775 train_time:50411ms step_avg:45.29ms
step:1114/1775 train_time:50472ms step_avg:45.31ms
step:1115/1775 train_time:50530ms step_avg:45.32ms
step:1116/1775 train_time:50589ms step_avg:45.33ms
step:1117/1775 train_time:50646ms step_avg:45.34ms
step:1118/1775 train_time:50706ms step_avg:45.35ms
step:1119/1775 train_time:50764ms step_avg:45.37ms
step:1120/1775 train_time:50824ms step_avg:45.38ms
step:1121/1775 train_time:50881ms step_avg:45.39ms
step:1122/1775 train_time:50941ms step_avg:45.40ms
step:1123/1775 train_time:50999ms step_avg:45.41ms
step:1124/1775 train_time:51059ms step_avg:45.43ms
step:1125/1775 train_time:51117ms step_avg:45.44ms
step:1126/1775 train_time:51177ms step_avg:45.45ms
step:1127/1775 train_time:51234ms step_avg:45.46ms
step:1128/1775 train_time:51295ms step_avg:45.47ms
step:1129/1775 train_time:51353ms step_avg:45.49ms
step:1130/1775 train_time:51414ms step_avg:45.50ms
step:1131/1775 train_time:51471ms step_avg:45.51ms
step:1132/1775 train_time:51533ms step_avg:45.52ms
step:1133/1775 train_time:51592ms step_avg:45.54ms
step:1134/1775 train_time:51653ms step_avg:45.55ms
step:1135/1775 train_time:51710ms step_avg:45.56ms
step:1136/1775 train_time:51771ms step_avg:45.57ms
step:1137/1775 train_time:51829ms step_avg:45.58ms
step:1138/1775 train_time:51890ms step_avg:45.60ms
step:1139/1775 train_time:51947ms step_avg:45.61ms
step:1140/1775 train_time:52008ms step_avg:45.62ms
step:1141/1775 train_time:52065ms step_avg:45.63ms
step:1142/1775 train_time:52126ms step_avg:45.64ms
step:1143/1775 train_time:52183ms step_avg:45.65ms
step:1144/1775 train_time:52243ms step_avg:45.67ms
step:1145/1775 train_time:52300ms step_avg:45.68ms
step:1146/1775 train_time:52361ms step_avg:45.69ms
step:1147/1775 train_time:52417ms step_avg:45.70ms
step:1148/1775 train_time:52478ms step_avg:45.71ms
step:1149/1775 train_time:52536ms step_avg:45.72ms
step:1150/1775 train_time:52596ms step_avg:45.74ms
step:1151/1775 train_time:52655ms step_avg:45.75ms
step:1152/1775 train_time:52715ms step_avg:45.76ms
step:1153/1775 train_time:52774ms step_avg:45.77ms
step:1154/1775 train_time:52835ms step_avg:45.78ms
step:1155/1775 train_time:52894ms step_avg:45.80ms
step:1156/1775 train_time:52955ms step_avg:45.81ms
step:1157/1775 train_time:53013ms step_avg:45.82ms
step:1158/1775 train_time:53076ms step_avg:45.83ms
step:1159/1775 train_time:53160ms step_avg:45.87ms
step:1160/1775 train_time:53247ms step_avg:45.90ms
step:1161/1775 train_time:53330ms step_avg:45.93ms
step:1162/1775 train_time:53416ms step_avg:45.97ms
step:1163/1775 train_time:53500ms step_avg:46.00ms
step:1164/1775 train_time:53587ms step_avg:46.04ms
step:1165/1775 train_time:53670ms step_avg:46.07ms
step:1166/1775 train_time:53757ms step_avg:46.10ms
step:1167/1775 train_time:53839ms step_avg:46.13ms
step:1168/1775 train_time:53925ms step_avg:46.17ms
step:1169/1775 train_time:54009ms step_avg:46.20ms
step:1170/1775 train_time:54097ms step_avg:46.24ms
step:1171/1775 train_time:54181ms step_avg:46.27ms
step:1172/1775 train_time:54267ms step_avg:46.30ms
step:1173/1775 train_time:54350ms step_avg:46.33ms
step:1174/1775 train_time:54437ms step_avg:46.37ms
step:1175/1775 train_time:54520ms step_avg:46.40ms
step:1176/1775 train_time:54607ms step_avg:46.43ms
step:1177/1775 train_time:54691ms step_avg:46.47ms
step:1178/1775 train_time:54779ms step_avg:46.50ms
step:1179/1775 train_time:54862ms step_avg:46.53ms
step:1180/1775 train_time:54948ms step_avg:46.57ms
step:1181/1775 train_time:55032ms step_avg:46.60ms
step:1182/1775 train_time:55119ms step_avg:46.63ms
step:1183/1775 train_time:55203ms step_avg:46.66ms
step:1184/1775 train_time:55291ms step_avg:46.70ms
step:1185/1775 train_time:55374ms step_avg:46.73ms
step:1186/1775 train_time:55459ms step_avg:46.76ms
step:1187/1775 train_time:55543ms step_avg:46.79ms
step:1188/1775 train_time:55628ms step_avg:46.83ms
step:1189/1775 train_time:55712ms step_avg:46.86ms
step:1190/1775 train_time:55799ms step_avg:46.89ms
step:1191/1775 train_time:55882ms step_avg:46.92ms
step:1192/1775 train_time:55969ms step_avg:46.95ms
step:1193/1775 train_time:56052ms step_avg:46.98ms
step:1194/1775 train_time:56139ms step_avg:47.02ms
step:1195/1775 train_time:56223ms step_avg:47.05ms
step:1196/1775 train_time:56309ms step_avg:47.08ms
step:1197/1775 train_time:56393ms step_avg:47.11ms
step:1198/1775 train_time:56479ms step_avg:47.14ms
step:1199/1775 train_time:56562ms step_avg:47.17ms
step:1200/1775 train_time:56648ms step_avg:47.21ms
step:1201/1775 train_time:56731ms step_avg:47.24ms
step:1202/1775 train_time:56818ms step_avg:47.27ms
step:1203/1775 train_time:56902ms step_avg:47.30ms
step:1204/1775 train_time:56988ms step_avg:47.33ms
step:1205/1775 train_time:57071ms step_avg:47.36ms
step:1206/1775 train_time:57158ms step_avg:47.39ms
step:1207/1775 train_time:57241ms step_avg:47.42ms
step:1208/1775 train_time:57328ms step_avg:47.46ms
step:1209/1775 train_time:57411ms step_avg:47.49ms
step:1210/1775 train_time:57497ms step_avg:47.52ms
step:1211/1775 train_time:57580ms step_avg:47.55ms
step:1212/1775 train_time:57667ms step_avg:47.58ms
step:1213/1775 train_time:57750ms step_avg:47.61ms
step:1214/1775 train_time:57836ms step_avg:47.64ms
step:1215/1775 train_time:57921ms step_avg:47.67ms
step:1216/1775 train_time:58007ms step_avg:47.70ms
step:1217/1775 train_time:58092ms step_avg:47.73ms
step:1218/1775 train_time:58178ms step_avg:47.76ms
step:1219/1775 train_time:58261ms step_avg:47.79ms
step:1220/1775 train_time:58349ms step_avg:47.83ms
step:1221/1775 train_time:58431ms step_avg:47.86ms
step:1222/1775 train_time:58518ms step_avg:47.89ms
step:1223/1775 train_time:58603ms step_avg:47.92ms
step:1224/1775 train_time:58690ms step_avg:47.95ms
step:1225/1775 train_time:58772ms step_avg:47.98ms
step:1226/1775 train_time:58860ms step_avg:48.01ms
step:1227/1775 train_time:58943ms step_avg:48.04ms
step:1228/1775 train_time:59030ms step_avg:48.07ms
step:1229/1775 train_time:59113ms step_avg:48.10ms
step:1230/1775 train_time:59200ms step_avg:48.13ms
step:1231/1775 train_time:59284ms step_avg:48.16ms
step:1232/1775 train_time:59371ms step_avg:48.19ms
step:1233/1775 train_time:59454ms step_avg:48.22ms
step:1234/1775 train_time:59540ms step_avg:48.25ms
step:1235/1775 train_time:59624ms step_avg:48.28ms
step:1236/1775 train_time:59710ms step_avg:48.31ms
step:1237/1775 train_time:59793ms step_avg:48.34ms
step:1238/1775 train_time:59880ms step_avg:48.37ms
step:1239/1775 train_time:59964ms step_avg:48.40ms
step:1240/1775 train_time:60050ms step_avg:48.43ms
step:1241/1775 train_time:60134ms step_avg:48.46ms
step:1242/1775 train_time:60220ms step_avg:48.49ms
step:1243/1775 train_time:60304ms step_avg:48.52ms
step:1244/1775 train_time:60391ms step_avg:48.55ms
step:1245/1775 train_time:60475ms step_avg:48.57ms
step:1246/1775 train_time:60561ms step_avg:48.60ms
step:1247/1775 train_time:60645ms step_avg:48.63ms
step:1248/1775 train_time:60731ms step_avg:48.66ms
step:1249/1775 train_time:60815ms step_avg:48.69ms
step:1250/1775 train_time:60902ms step_avg:48.72ms
step:1250/1775 val_loss:3.5036 train_time:61002ms step_avg:48.80ms
step:1251/1775 train_time:61024ms step_avg:48.78ms
step:1252/1775 train_time:61073ms step_avg:48.78ms
step:1253/1775 train_time:61159ms step_avg:48.81ms
step:1254/1775 train_time:61249ms step_avg:48.84ms
step:1255/1775 train_time:61332ms step_avg:48.87ms
step:1256/1775 train_time:61417ms step_avg:48.90ms
step:1257/1775 train_time:61500ms step_avg:48.93ms
step:1258/1775 train_time:61586ms step_avg:48.96ms
step:1259/1775 train_time:61667ms step_avg:48.98ms
step:1260/1775 train_time:61753ms step_avg:49.01ms
step:1261/1775 train_time:61836ms step_avg:49.04ms
step:1262/1775 train_time:61924ms step_avg:49.07ms
step:1263/1775 train_time:62010ms step_avg:49.10ms
step:1264/1775 train_time:62096ms step_avg:49.13ms
step:1265/1775 train_time:62181ms step_avg:49.15ms
step:1266/1775 train_time:62269ms step_avg:49.19ms
step:1267/1775 train_time:62353ms step_avg:49.21ms
step:1268/1775 train_time:62439ms step_avg:49.24ms
step:1269/1775 train_time:62523ms step_avg:49.27ms
step:1270/1775 train_time:62609ms step_avg:49.30ms
step:1271/1775 train_time:62691ms step_avg:49.32ms
step:1272/1775 train_time:62776ms step_avg:49.35ms
step:1273/1775 train_time:62860ms step_avg:49.38ms
step:1274/1775 train_time:62947ms step_avg:49.41ms
step:1275/1775 train_time:63031ms step_avg:49.44ms
step:1276/1775 train_time:63119ms step_avg:49.47ms
step:1277/1775 train_time:63204ms step_avg:49.49ms
step:1278/1775 train_time:63290ms step_avg:49.52ms
step:1279/1775 train_time:63373ms step_avg:49.55ms
step:1280/1775 train_time:63457ms step_avg:49.58ms
step:1281/1775 train_time:63541ms step_avg:49.60ms
step:1282/1775 train_time:63627ms step_avg:49.63ms
step:1283/1775 train_time:63709ms step_avg:49.66ms
step:1284/1775 train_time:63795ms step_avg:49.68ms
step:1285/1775 train_time:63878ms step_avg:49.71ms
step:1286/1775 train_time:63965ms step_avg:49.74ms
step:1287/1775 train_time:64050ms step_avg:49.77ms
step:1288/1775 train_time:64137ms step_avg:49.80ms
step:1289/1775 train_time:64221ms step_avg:49.82ms
step:1290/1775 train_time:64308ms step_avg:49.85ms
step:1291/1775 train_time:64392ms step_avg:49.88ms
step:1292/1775 train_time:64477ms step_avg:49.90ms
step:1293/1775 train_time:64561ms step_avg:49.93ms
step:1294/1775 train_time:64646ms step_avg:49.96ms
step:1295/1775 train_time:64730ms step_avg:49.98ms
step:1296/1775 train_time:64817ms step_avg:50.01ms
step:1297/1775 train_time:64901ms step_avg:50.04ms
step:1298/1775 train_time:64987ms step_avg:50.07ms
step:1299/1775 train_time:65073ms step_avg:50.09ms
step:1300/1775 train_time:65160ms step_avg:50.12ms
step:1301/1775 train_time:65244ms step_avg:50.15ms
step:1302/1775 train_time:65331ms step_avg:50.18ms
step:1303/1775 train_time:65414ms step_avg:50.20ms
step:1304/1775 train_time:65500ms step_avg:50.23ms
step:1305/1775 train_time:65583ms step_avg:50.25ms
step:1306/1775 train_time:65668ms step_avg:50.28ms
step:1307/1775 train_time:65753ms step_avg:50.31ms
step:1308/1775 train_time:65838ms step_avg:50.33ms
step:1309/1775 train_time:65922ms step_avg:50.36ms
step:1310/1775 train_time:66009ms step_avg:50.39ms
step:1311/1775 train_time:66093ms step_avg:50.41ms
step:1312/1775 train_time:66180ms step_avg:50.44ms
step:1313/1775 train_time:66263ms step_avg:50.47ms
step:1314/1775 train_time:66352ms step_avg:50.50ms
step:1315/1775 train_time:66435ms step_avg:50.52ms
step:1316/1775 train_time:66520ms step_avg:50.55ms
step:1317/1775 train_time:66603ms step_avg:50.57ms
step:1318/1775 train_time:66692ms step_avg:50.60ms
step:1319/1775 train_time:66774ms step_avg:50.62ms
step:1320/1775 train_time:66860ms step_avg:50.65ms
step:1321/1775 train_time:66944ms step_avg:50.68ms
step:1322/1775 train_time:67031ms step_avg:50.70ms
step:1323/1775 train_time:67115ms step_avg:50.73ms
step:1324/1775 train_time:67202ms step_avg:50.76ms
step:1325/1775 train_time:67285ms step_avg:50.78ms
step:1326/1775 train_time:67371ms step_avg:50.81ms
step:1327/1775 train_time:67455ms step_avg:50.83ms
step:1328/1775 train_time:67540ms step_avg:50.86ms
step:1329/1775 train_time:67625ms step_avg:50.88ms
step:1330/1775 train_time:67711ms step_avg:50.91ms
step:1331/1775 train_time:67794ms step_avg:50.93ms
step:1332/1775 train_time:67879ms step_avg:50.96ms
step:1333/1775 train_time:67962ms step_avg:50.98ms
step:1334/1775 train_time:68049ms step_avg:51.01ms
step:1335/1775 train_time:68133ms step_avg:51.04ms
step:1336/1775 train_time:68220ms step_avg:51.06ms
step:1337/1775 train_time:68304ms step_avg:51.09ms
step:1338/1775 train_time:68391ms step_avg:51.11ms
step:1339/1775 train_time:68474ms step_avg:51.14ms
step:1340/1775 train_time:68560ms step_avg:51.16ms
step:1341/1775 train_time:68644ms step_avg:51.19ms
step:1342/1775 train_time:68730ms step_avg:51.21ms
step:1343/1775 train_time:68815ms step_avg:51.24ms
step:1344/1775 train_time:68899ms step_avg:51.26ms
step:1345/1775 train_time:68983ms step_avg:51.29ms
step:1346/1775 train_time:69069ms step_avg:51.31ms
step:1347/1775 train_time:69154ms step_avg:51.34ms
step:1348/1775 train_time:69239ms step_avg:51.36ms
step:1349/1775 train_time:69322ms step_avg:51.39ms
step:1350/1775 train_time:69409ms step_avg:51.41ms
step:1351/1775 train_time:69491ms step_avg:51.44ms
step:1352/1775 train_time:69578ms step_avg:51.46ms
step:1353/1775 train_time:69661ms step_avg:51.49ms
step:1354/1775 train_time:69747ms step_avg:51.51ms
step:1355/1775 train_time:69832ms step_avg:51.54ms
step:1356/1775 train_time:69918ms step_avg:51.56ms
step:1357/1775 train_time:70002ms step_avg:51.59ms
step:1358/1775 train_time:70089ms step_avg:51.61ms
step:1359/1775 train_time:70173ms step_avg:51.64ms
step:1360/1775 train_time:70259ms step_avg:51.66ms
step:1361/1775 train_time:70343ms step_avg:51.68ms
step:1362/1775 train_time:70428ms step_avg:51.71ms
step:1363/1775 train_time:70513ms step_avg:51.73ms
step:1364/1775 train_time:70597ms step_avg:51.76ms
step:1365/1775 train_time:70680ms step_avg:51.78ms
step:1366/1775 train_time:70765ms step_avg:51.80ms
step:1367/1775 train_time:70850ms step_avg:51.83ms
step:1368/1775 train_time:70937ms step_avg:51.85ms
step:1369/1775 train_time:71021ms step_avg:51.88ms
step:1370/1775 train_time:71107ms step_avg:51.90ms
step:1371/1775 train_time:71190ms step_avg:51.93ms
step:1372/1775 train_time:71276ms step_avg:51.95ms
step:1373/1775 train_time:71359ms step_avg:51.97ms
step:1374/1775 train_time:71446ms step_avg:52.00ms
step:1375/1775 train_time:71530ms step_avg:52.02ms
step:1376/1775 train_time:71616ms step_avg:52.05ms
step:1377/1775 train_time:71699ms step_avg:52.07ms
step:1378/1775 train_time:71785ms step_avg:52.09ms
step:1379/1775 train_time:71868ms step_avg:52.12ms
step:1380/1775 train_time:71955ms step_avg:52.14ms
step:1381/1775 train_time:72037ms step_avg:52.16ms
step:1382/1775 train_time:72124ms step_avg:52.19ms
step:1383/1775 train_time:72208ms step_avg:52.21ms
step:1384/1775 train_time:72295ms step_avg:52.24ms
step:1385/1775 train_time:72378ms step_avg:52.26ms
step:1386/1775 train_time:72464ms step_avg:52.28ms
step:1387/1775 train_time:72547ms step_avg:52.31ms
step:1388/1775 train_time:72633ms step_avg:52.33ms
step:1389/1775 train_time:72717ms step_avg:52.35ms
step:1390/1775 train_time:72803ms step_avg:52.38ms
step:1391/1775 train_time:72887ms step_avg:52.40ms
step:1392/1775 train_time:72973ms step_avg:52.42ms
step:1393/1775 train_time:73056ms step_avg:52.44ms
step:1394/1775 train_time:73143ms step_avg:52.47ms
step:1395/1775 train_time:73227ms step_avg:52.49ms
step:1396/1775 train_time:73313ms step_avg:52.52ms
step:1397/1775 train_time:73395ms step_avg:52.54ms
step:1398/1775 train_time:73481ms step_avg:52.56ms
step:1399/1775 train_time:73564ms step_avg:52.58ms
step:1400/1775 train_time:73651ms step_avg:52.61ms
step:1401/1775 train_time:73734ms step_avg:52.63ms
step:1402/1775 train_time:73820ms step_avg:52.65ms
step:1403/1775 train_time:73903ms step_avg:52.67ms
step:1404/1775 train_time:73990ms step_avg:52.70ms
step:1405/1775 train_time:74072ms step_avg:52.72ms
step:1406/1775 train_time:74158ms step_avg:52.74ms
step:1407/1775 train_time:74242ms step_avg:52.77ms
step:1408/1775 train_time:74328ms step_avg:52.79ms
step:1409/1775 train_time:74411ms step_avg:52.81ms
step:1410/1775 train_time:74497ms step_avg:52.83ms
step:1411/1775 train_time:74581ms step_avg:52.86ms
step:1412/1775 train_time:74667ms step_avg:52.88ms
step:1413/1775 train_time:74751ms step_avg:52.90ms
step:1414/1775 train_time:74837ms step_avg:52.93ms
step:1415/1775 train_time:74921ms step_avg:52.95ms
step:1416/1775 train_time:75007ms step_avg:52.97ms
step:1417/1775 train_time:75090ms step_avg:52.99ms
step:1418/1775 train_time:75177ms step_avg:53.02ms
step:1419/1775 train_time:75260ms step_avg:53.04ms
step:1420/1775 train_time:75347ms step_avg:53.06ms
step:1421/1775 train_time:75430ms step_avg:53.08ms
step:1422/1775 train_time:75516ms step_avg:53.11ms
step:1423/1775 train_time:75598ms step_avg:53.13ms
step:1424/1775 train_time:75685ms step_avg:53.15ms
step:1425/1775 train_time:75768ms step_avg:53.17ms
step:1426/1775 train_time:75854ms step_avg:53.19ms
step:1427/1775 train_time:75937ms step_avg:53.21ms
step:1428/1775 train_time:76024ms step_avg:53.24ms
step:1429/1775 train_time:76107ms step_avg:53.26ms
step:1430/1775 train_time:76193ms step_avg:53.28ms
step:1431/1775 train_time:76275ms step_avg:53.30ms
step:1432/1775 train_time:76361ms step_avg:53.32ms
step:1433/1775 train_time:76446ms step_avg:53.35ms
step:1434/1775 train_time:76533ms step_avg:53.37ms
step:1435/1775 train_time:76616ms step_avg:53.39ms
step:1436/1775 train_time:76702ms step_avg:53.41ms
step:1437/1775 train_time:76785ms step_avg:53.43ms
step:1438/1775 train_time:76871ms step_avg:53.46ms
step:1439/1775 train_time:76955ms step_avg:53.48ms
step:1440/1775 train_time:77041ms step_avg:53.50ms
step:1441/1775 train_time:77125ms step_avg:53.52ms
step:1442/1775 train_time:77211ms step_avg:53.54ms
step:1443/1775 train_time:77294ms step_avg:53.56ms
step:1444/1775 train_time:77380ms step_avg:53.59ms
step:1445/1775 train_time:77464ms step_avg:53.61ms
step:1446/1775 train_time:77551ms step_avg:53.63ms
step:1447/1775 train_time:77635ms step_avg:53.65ms
step:1448/1775 train_time:77721ms step_avg:53.67ms
step:1449/1775 train_time:77806ms step_avg:53.70ms
step:1450/1775 train_time:77892ms step_avg:53.72ms
step:1451/1775 train_time:77974ms step_avg:53.74ms
step:1452/1775 train_time:78061ms step_avg:53.76ms
step:1453/1775 train_time:78145ms step_avg:53.78ms
step:1454/1775 train_time:78232ms step_avg:53.80ms
step:1455/1775 train_time:78316ms step_avg:53.83ms
step:1456/1775 train_time:78402ms step_avg:53.85ms
step:1457/1775 train_time:78485ms step_avg:53.87ms
step:1458/1775 train_time:78571ms step_avg:53.89ms
step:1459/1775 train_time:78655ms step_avg:53.91ms
step:1460/1775 train_time:78741ms step_avg:53.93ms
step:1461/1775 train_time:78825ms step_avg:53.95ms
step:1462/1775 train_time:78912ms step_avg:53.98ms
step:1463/1775 train_time:78995ms step_avg:54.00ms
step:1464/1775 train_time:79081ms step_avg:54.02ms
step:1465/1775 train_time:79164ms step_avg:54.04ms
step:1466/1775 train_time:79252ms step_avg:54.06ms
step:1467/1775 train_time:79336ms step_avg:54.08ms
step:1468/1775 train_time:79422ms step_avg:54.10ms
step:1469/1775 train_time:79505ms step_avg:54.12ms
step:1470/1775 train_time:79591ms step_avg:54.14ms
step:1471/1775 train_time:79674ms step_avg:54.16ms
step:1472/1775 train_time:79761ms step_avg:54.19ms
step:1473/1775 train_time:79845ms step_avg:54.21ms
step:1474/1775 train_time:79932ms step_avg:54.23ms
step:1475/1775 train_time:80015ms step_avg:54.25ms
step:1476/1775 train_time:80100ms step_avg:54.27ms
step:1477/1775 train_time:80184ms step_avg:54.29ms
step:1478/1775 train_time:80271ms step_avg:54.31ms
step:1479/1775 train_time:80354ms step_avg:54.33ms
step:1480/1775 train_time:80439ms step_avg:54.35ms
step:1481/1775 train_time:80524ms step_avg:54.37ms
step:1482/1775 train_time:80611ms step_avg:54.39ms
step:1483/1775 train_time:80693ms step_avg:54.41ms
step:1484/1775 train_time:80778ms step_avg:54.43ms
step:1485/1775 train_time:80862ms step_avg:54.45ms
step:1486/1775 train_time:80950ms step_avg:54.47ms
step:1487/1775 train_time:81034ms step_avg:54.49ms
step:1488/1775 train_time:81120ms step_avg:54.52ms
step:1489/1775 train_time:81203ms step_avg:54.53ms
step:1490/1775 train_time:81289ms step_avg:54.56ms
step:1491/1775 train_time:81372ms step_avg:54.58ms
step:1492/1775 train_time:81458ms step_avg:54.60ms
step:1493/1775 train_time:81542ms step_avg:54.62ms
step:1494/1775 train_time:81628ms step_avg:54.64ms
step:1495/1775 train_time:81712ms step_avg:54.66ms
step:1496/1775 train_time:81797ms step_avg:54.68ms
step:1497/1775 train_time:81881ms step_avg:54.70ms
step:1498/1775 train_time:81966ms step_avg:54.72ms
step:1499/1775 train_time:82053ms step_avg:54.74ms
step:1500/1775 train_time:82139ms step_avg:54.76ms
step:1500/1775 val_loss:3.3741 train_time:82237ms step_avg:54.82ms
step:1501/1775 train_time:82260ms step_avg:54.80ms
step:1502/1775 train_time:82310ms step_avg:54.80ms
step:1503/1775 train_time:82395ms step_avg:54.82ms
step:1504/1775 train_time:82484ms step_avg:54.84ms
step:1505/1775 train_time:82567ms step_avg:54.86ms
step:1506/1775 train_time:82653ms step_avg:54.88ms
step:1507/1775 train_time:82736ms step_avg:54.90ms
step:1508/1775 train_time:82821ms step_avg:54.92ms
step:1509/1775 train_time:82903ms step_avg:54.94ms
step:1510/1775 train_time:82989ms step_avg:54.96ms
step:1511/1775 train_time:83071ms step_avg:54.98ms
step:1512/1775 train_time:83161ms step_avg:55.00ms
step:1513/1775 train_time:83245ms step_avg:55.02ms
step:1514/1775 train_time:83335ms step_avg:55.04ms
step:1515/1775 train_time:83419ms step_avg:55.06ms
step:1516/1775 train_time:83505ms step_avg:55.08ms
step:1517/1775 train_time:83589ms step_avg:55.10ms
step:1518/1775 train_time:83675ms step_avg:55.12ms
step:1519/1775 train_time:83757ms step_avg:55.14ms
step:1520/1775 train_time:83843ms step_avg:55.16ms
step:1521/1775 train_time:83925ms step_avg:55.18ms
step:1522/1775 train_time:84011ms step_avg:55.20ms
step:1523/1775 train_time:84094ms step_avg:55.22ms
step:1524/1775 train_time:84182ms step_avg:55.24ms
step:1525/1775 train_time:84267ms step_avg:55.26ms
step:1526/1775 train_time:84354ms step_avg:55.28ms
step:1527/1775 train_time:84439ms step_avg:55.30ms
step:1528/1775 train_time:84525ms step_avg:55.32ms
step:1529/1775 train_time:84610ms step_avg:55.34ms
step:1530/1775 train_time:84695ms step_avg:55.36ms
step:1531/1775 train_time:84779ms step_avg:55.37ms
step:1532/1775 train_time:84864ms step_avg:55.39ms
step:1533/1775 train_time:84947ms step_avg:55.41ms
step:1534/1775 train_time:85034ms step_avg:55.43ms
step:1535/1775 train_time:85116ms step_avg:55.45ms
step:1536/1775 train_time:85203ms step_avg:55.47ms
step:1537/1775 train_time:85287ms step_avg:55.49ms
step:1538/1775 train_time:85374ms step_avg:55.51ms
step:1539/1775 train_time:85457ms step_avg:55.53ms
step:1540/1775 train_time:85545ms step_avg:55.55ms
step:1541/1775 train_time:85628ms step_avg:55.57ms
step:1542/1775 train_time:85714ms step_avg:55.59ms
step:1543/1775 train_time:85797ms step_avg:55.60ms
step:1544/1775 train_time:85883ms step_avg:55.62ms
step:1545/1775 train_time:85966ms step_avg:55.64ms
step:1546/1775 train_time:86053ms step_avg:55.66ms
step:1547/1775 train_time:86137ms step_avg:55.68ms
step:1548/1775 train_time:86223ms step_avg:55.70ms
step:1549/1775 train_time:86307ms step_avg:55.72ms
step:1550/1775 train_time:86393ms step_avg:55.74ms
step:1551/1775 train_time:86477ms step_avg:55.76ms
step:1552/1775 train_time:86564ms step_avg:55.78ms
step:1553/1775 train_time:86646ms step_avg:55.79ms
step:1554/1775 train_time:86732ms step_avg:55.81ms
step:1555/1775 train_time:86814ms step_avg:55.83ms
step:1556/1775 train_time:86900ms step_avg:55.85ms
step:1557/1775 train_time:86983ms step_avg:55.87ms
step:1558/1775 train_time:87069ms step_avg:55.89ms
step:1559/1775 train_time:87152ms step_avg:55.90ms
step:1560/1775 train_time:87239ms step_avg:55.92ms
step:1561/1775 train_time:87321ms step_avg:55.94ms
step:1562/1775 train_time:87409ms step_avg:55.96ms
step:1563/1775 train_time:87494ms step_avg:55.98ms
step:1564/1775 train_time:87580ms step_avg:56.00ms
step:1565/1775 train_time:87663ms step_avg:56.01ms
step:1566/1775 train_time:87750ms step_avg:56.03ms
step:1567/1775 train_time:87832ms step_avg:56.05ms
step:1568/1775 train_time:87918ms step_avg:56.07ms
step:1569/1775 train_time:88001ms step_avg:56.09ms
step:1570/1775 train_time:88086ms step_avg:56.11ms
step:1571/1775 train_time:88171ms step_avg:56.12ms
step:1572/1775 train_time:88258ms step_avg:56.14ms
step:1573/1775 train_time:88341ms step_avg:56.16ms
step:1574/1775 train_time:88427ms step_avg:56.18ms
step:1575/1775 train_time:88510ms step_avg:56.20ms
step:1576/1775 train_time:88597ms step_avg:56.22ms
step:1577/1775 train_time:88681ms step_avg:56.23ms
step:1578/1775 train_time:88766ms step_avg:56.25ms
step:1579/1775 train_time:88850ms step_avg:56.27ms
step:1580/1775 train_time:88937ms step_avg:56.29ms
step:1581/1775 train_time:89019ms step_avg:56.31ms
step:1582/1775 train_time:89104ms step_avg:56.32ms
step:1583/1775 train_time:89188ms step_avg:56.34ms
step:1584/1775 train_time:89274ms step_avg:56.36ms
step:1585/1775 train_time:89358ms step_avg:56.38ms
step:1586/1775 train_time:89444ms step_avg:56.40ms
step:1587/1775 train_time:89528ms step_avg:56.41ms
step:1588/1775 train_time:89614ms step_avg:56.43ms
step:1589/1775 train_time:89697ms step_avg:56.45ms
step:1590/1775 train_time:89783ms step_avg:56.47ms
step:1591/1775 train_time:89866ms step_avg:56.48ms
step:1592/1775 train_time:89953ms step_avg:56.50ms
step:1593/1775 train_time:90037ms step_avg:56.52ms
step:1594/1775 train_time:90122ms step_avg:56.54ms
step:1595/1775 train_time:90205ms step_avg:56.56ms
step:1596/1775 train_time:90291ms step_avg:56.57ms
step:1597/1775 train_time:90377ms step_avg:56.59ms
step:1598/1775 train_time:90463ms step_avg:56.61ms
step:1599/1775 train_time:90546ms step_avg:56.63ms
step:1600/1775 train_time:90632ms step_avg:56.65ms
step:1601/1775 train_time:90716ms step_avg:56.66ms
step:1602/1775 train_time:90801ms step_avg:56.68ms
step:1603/1775 train_time:90883ms step_avg:56.70ms
step:1604/1775 train_time:90970ms step_avg:56.71ms
step:1605/1775 train_time:91053ms step_avg:56.73ms
step:1606/1775 train_time:91139ms step_avg:56.75ms
step:1607/1775 train_time:91221ms step_avg:56.76ms
step:1608/1775 train_time:91307ms step_avg:56.78ms
step:1609/1775 train_time:91391ms step_avg:56.80ms
step:1610/1775 train_time:91479ms step_avg:56.82ms
step:1611/1775 train_time:91562ms step_avg:56.84ms
step:1612/1775 train_time:91649ms step_avg:56.85ms
step:1613/1775 train_time:91733ms step_avg:56.87ms
step:1614/1775 train_time:91818ms step_avg:56.89ms
step:1615/1775 train_time:91902ms step_avg:56.91ms
step:1616/1775 train_time:91988ms step_avg:56.92ms
step:1617/1775 train_time:92071ms step_avg:56.94ms
step:1618/1775 train_time:92158ms step_avg:56.96ms
step:1619/1775 train_time:92242ms step_avg:56.97ms
step:1620/1775 train_time:92328ms step_avg:56.99ms
step:1621/1775 train_time:92412ms step_avg:57.01ms
step:1622/1775 train_time:92498ms step_avg:57.03ms
step:1623/1775 train_time:92582ms step_avg:57.04ms
step:1624/1775 train_time:92668ms step_avg:57.06ms
step:1625/1775 train_time:92752ms step_avg:57.08ms
step:1626/1775 train_time:92838ms step_avg:57.10ms
step:1627/1775 train_time:92920ms step_avg:57.11ms
step:1628/1775 train_time:93007ms step_avg:57.13ms
step:1629/1775 train_time:93090ms step_avg:57.15ms
step:1630/1775 train_time:93178ms step_avg:57.16ms
step:1631/1775 train_time:93261ms step_avg:57.18ms
step:1632/1775 train_time:93348ms step_avg:57.20ms
step:1633/1775 train_time:93432ms step_avg:57.21ms
step:1634/1775 train_time:93518ms step_avg:57.23ms
step:1635/1775 train_time:93601ms step_avg:57.25ms
step:1636/1775 train_time:93688ms step_avg:57.27ms
step:1637/1775 train_time:93771ms step_avg:57.28ms
step:1638/1775 train_time:93859ms step_avg:57.30ms
step:1639/1775 train_time:93942ms step_avg:57.32ms
step:1640/1775 train_time:94028ms step_avg:57.33ms
step:1641/1775 train_time:94111ms step_avg:57.35ms
step:1642/1775 train_time:94197ms step_avg:57.37ms
step:1643/1775 train_time:94280ms step_avg:57.38ms
step:1644/1775 train_time:94365ms step_avg:57.40ms
step:1645/1775 train_time:94450ms step_avg:57.42ms
step:1646/1775 train_time:94538ms step_avg:57.44ms
step:1647/1775 train_time:94621ms step_avg:57.45ms
step:1648/1775 train_time:94706ms step_avg:57.47ms
step:1649/1775 train_time:94790ms step_avg:57.48ms
step:1650/1775 train_time:94876ms step_avg:57.50ms
step:1651/1775 train_time:94960ms step_avg:57.52ms
step:1652/1775 train_time:95046ms step_avg:57.53ms
step:1653/1775 train_time:95129ms step_avg:57.55ms
step:1654/1775 train_time:95215ms step_avg:57.57ms
step:1655/1775 train_time:95299ms step_avg:57.58ms
step:1656/1775 train_time:95384ms step_avg:57.60ms
step:1657/1775 train_time:95469ms step_avg:57.62ms
step:1658/1775 train_time:95555ms step_avg:57.63ms
step:1659/1775 train_time:95640ms step_avg:57.65ms
step:1660/1775 train_time:95725ms step_avg:57.67ms
step:1661/1775 train_time:95810ms step_avg:57.68ms
step:1662/1775 train_time:95895ms step_avg:57.70ms
step:1663/1775 train_time:95978ms step_avg:57.71ms
step:1664/1775 train_time:96064ms step_avg:57.73ms
step:1665/1775 train_time:96148ms step_avg:57.75ms
step:1666/1775 train_time:96235ms step_avg:57.76ms
step:1667/1775 train_time:96317ms step_avg:57.78ms
step:1668/1775 train_time:96404ms step_avg:57.80ms
step:1669/1775 train_time:96487ms step_avg:57.81ms
step:1670/1775 train_time:96574ms step_avg:57.83ms
step:1671/1775 train_time:96657ms step_avg:57.84ms
step:1672/1775 train_time:96744ms step_avg:57.86ms
step:1673/1775 train_time:96826ms step_avg:57.88ms
step:1674/1775 train_time:96913ms step_avg:57.89ms
step:1675/1775 train_time:96996ms step_avg:57.91ms
step:1676/1775 train_time:97083ms step_avg:57.93ms
step:1677/1775 train_time:97166ms step_avg:57.94ms
step:1678/1775 train_time:97252ms step_avg:57.96ms
step:1679/1775 train_time:97336ms step_avg:57.97ms
step:1680/1775 train_time:97422ms step_avg:57.99ms
step:1681/1775 train_time:97504ms step_avg:58.00ms
step:1682/1775 train_time:97589ms step_avg:58.02ms
step:1683/1775 train_time:97673ms step_avg:58.04ms
step:1684/1775 train_time:97760ms step_avg:58.05ms
step:1685/1775 train_time:97844ms step_avg:58.07ms
step:1686/1775 train_time:97930ms step_avg:58.08ms
step:1687/1775 train_time:98014ms step_avg:58.10ms
step:1688/1775 train_time:98101ms step_avg:58.12ms
step:1689/1775 train_time:98184ms step_avg:58.13ms
step:1690/1775 train_time:98271ms step_avg:58.15ms
step:1691/1775 train_time:98354ms step_avg:58.16ms
step:1692/1775 train_time:98440ms step_avg:58.18ms
step:1693/1775 train_time:98522ms step_avg:58.19ms
step:1694/1775 train_time:98610ms step_avg:58.21ms
step:1695/1775 train_time:98693ms step_avg:58.23ms
step:1696/1775 train_time:98780ms step_avg:58.24ms
step:1697/1775 train_time:98863ms step_avg:58.26ms
step:1698/1775 train_time:98949ms step_avg:58.27ms
step:1699/1775 train_time:99034ms step_avg:58.29ms
step:1700/1775 train_time:99118ms step_avg:58.30ms
step:1701/1775 train_time:99201ms step_avg:58.32ms
step:1702/1775 train_time:99287ms step_avg:58.34ms
step:1703/1775 train_time:99372ms step_avg:58.35ms
step:1704/1775 train_time:99458ms step_avg:58.37ms
step:1705/1775 train_time:99542ms step_avg:58.38ms
step:1706/1775 train_time:99628ms step_avg:58.40ms
step:1707/1775 train_time:99712ms step_avg:58.41ms
step:1708/1775 train_time:99798ms step_avg:58.43ms
step:1709/1775 train_time:99882ms step_avg:58.44ms
step:1710/1775 train_time:99970ms step_avg:58.46ms
step:1711/1775 train_time:100054ms step_avg:58.48ms
step:1712/1775 train_time:100140ms step_avg:58.49ms
step:1713/1775 train_time:100223ms step_avg:58.51ms
step:1714/1775 train_time:100308ms step_avg:58.52ms
step:1715/1775 train_time:100392ms step_avg:58.54ms
step:1716/1775 train_time:100478ms step_avg:58.55ms
step:1717/1775 train_time:100562ms step_avg:58.57ms
step:1718/1775 train_time:100649ms step_avg:58.58ms
step:1719/1775 train_time:100732ms step_avg:58.60ms
step:1720/1775 train_time:100818ms step_avg:58.62ms
step:1721/1775 train_time:100902ms step_avg:58.63ms
step:1722/1775 train_time:100988ms step_avg:58.65ms
step:1723/1775 train_time:101072ms step_avg:58.66ms
step:1724/1775 train_time:101158ms step_avg:58.68ms
step:1725/1775 train_time:101241ms step_avg:58.69ms
step:1726/1775 train_time:101327ms step_avg:58.71ms
step:1727/1775 train_time:101410ms step_avg:58.72ms
step:1728/1775 train_time:101496ms step_avg:58.74ms
step:1729/1775 train_time:101580ms step_avg:58.75ms
step:1730/1775 train_time:101666ms step_avg:58.77ms
step:1731/1775 train_time:101750ms step_avg:58.78ms
step:1732/1775 train_time:101836ms step_avg:58.80ms
step:1733/1775 train_time:101919ms step_avg:58.81ms
step:1734/1775 train_time:102005ms step_avg:58.83ms
step:1735/1775 train_time:102089ms step_avg:58.84ms
step:1736/1775 train_time:102179ms step_avg:58.86ms
step:1737/1775 train_time:102264ms step_avg:58.87ms
step:1738/1775 train_time:102350ms step_avg:58.89ms
step:1739/1775 train_time:102433ms step_avg:58.90ms
step:1740/1775 train_time:102519ms step_avg:58.92ms
step:1741/1775 train_time:102602ms step_avg:58.93ms
step:1742/1775 train_time:102690ms step_avg:58.95ms
step:1743/1775 train_time:102774ms step_avg:58.96ms
step:1744/1775 train_time:102862ms step_avg:58.98ms
step:1745/1775 train_time:102945ms step_avg:58.99ms
step:1746/1775 train_time:103031ms step_avg:59.01ms
step:1747/1775 train_time:103115ms step_avg:59.02ms
step:1748/1775 train_time:103201ms step_avg:59.04ms
step:1749/1775 train_time:103284ms step_avg:59.05ms
step:1750/1775 train_time:103370ms step_avg:59.07ms
step:1750/1775 val_loss:3.2830 train_time:103470ms step_avg:59.13ms
step:1751/1775 train_time:103493ms step_avg:59.11ms
step:1752/1775 train_time:103542ms step_avg:59.10ms
step:1753/1775 train_time:103627ms step_avg:59.11ms
step:1754/1775 train_time:103720ms step_avg:59.13ms
step:1755/1775 train_time:103804ms step_avg:59.15ms
step:1756/1775 train_time:103890ms step_avg:59.16ms
step:1757/1775 train_time:103973ms step_avg:59.18ms
step:1758/1775 train_time:104058ms step_avg:59.19ms
step:1759/1775 train_time:104139ms step_avg:59.20ms
step:1760/1775 train_time:104226ms step_avg:59.22ms
step:1761/1775 train_time:104308ms step_avg:59.23ms
step:1762/1775 train_time:104396ms step_avg:59.25ms
step:1763/1775 train_time:104481ms step_avg:59.26ms
step:1764/1775 train_time:104568ms step_avg:59.28ms
step:1765/1775 train_time:104654ms step_avg:59.29ms
step:1766/1775 train_time:104742ms step_avg:59.31ms
step:1767/1775 train_time:104825ms step_avg:59.32ms
step:1768/1775 train_time:104912ms step_avg:59.34ms
step:1769/1775 train_time:104997ms step_avg:59.35ms
step:1770/1775 train_time:105083ms step_avg:59.37ms
step:1771/1775 train_time:105165ms step_avg:59.38ms
step:1772/1775 train_time:105252ms step_avg:59.40ms
step:1773/1775 train_time:105335ms step_avg:59.41ms
step:1774/1775 train_time:105423ms step_avg:59.43ms
step:1775/1775 train_time:105507ms step_avg:59.44ms
step:1775/1775 val_loss:3.2766 train_time:105609ms step_avg:59.50ms
peak memory allocated: 29148 MiB reserved: 44858 MiB

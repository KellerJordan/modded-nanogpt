import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    logs_dir: str = f"logs/12-19-Baseline"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 15:02:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     90837      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A     90838      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     90839      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     90840      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     90841      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     90842      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     90843      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     90844      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A     90838      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A     90839      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A     90840      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A     90841      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A     90842      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A     90843      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A     90844      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8344 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:79ms step_avg:78.80ms
step:2/2035 train_time:104ms step_avg:52.21ms
step:3/2035 train_time:125ms step_avg:41.71ms
step:4/2035 train_time:155ms step_avg:38.84ms
step:5/2035 train_time:188ms step_avg:37.57ms
step:6/2035 train_time:270ms step_avg:44.98ms
step:7/2035 train_time:290ms step_avg:41.46ms
step:8/2035 train_time:323ms step_avg:40.42ms
step:9/2035 train_time:356ms step_avg:39.55ms
step:10/2035 train_time:389ms step_avg:38.91ms
step:11/2035 train_time:422ms step_avg:38.36ms
step:12/2035 train_time:455ms step_avg:37.91ms
step:13/2035 train_time:488ms step_avg:37.53ms
step:14/2035 train_time:521ms step_avg:37.23ms
step:15/2035 train_time:554ms step_avg:36.94ms
step:16/2035 train_time:587ms step_avg:36.70ms
step:17/2035 train_time:620ms step_avg:36.48ms
step:18/2035 train_time:653ms step_avg:36.30ms
step:19/2035 train_time:686ms step_avg:36.12ms
step:20/2035 train_time:719ms step_avg:35.97ms
step:21/2035 train_time:752ms step_avg:35.82ms
step:22/2035 train_time:785ms step_avg:35.70ms
step:23/2035 train_time:818ms step_avg:35.59ms
step:24/2035 train_time:852ms step_avg:35.49ms
step:25/2035 train_time:885ms step_avg:35.40ms
step:26/2035 train_time:918ms step_avg:35.32ms
step:27/2035 train_time:951ms step_avg:35.24ms
step:28/2035 train_time:985ms step_avg:35.17ms
step:29/2035 train_time:1018ms step_avg:35.10ms
step:30/2035 train_time:1051ms step_avg:35.04ms
step:31/2035 train_time:1084ms step_avg:34.97ms
step:32/2035 train_time:1117ms step_avg:34.92ms
step:33/2035 train_time:1152ms step_avg:34.90ms
step:34/2035 train_time:1186ms step_avg:34.88ms
step:35/2035 train_time:1220ms step_avg:34.85ms
step:36/2035 train_time:1254ms step_avg:34.84ms
step:37/2035 train_time:1288ms step_avg:34.81ms
step:38/2035 train_time:1322ms step_avg:34.78ms
step:39/2035 train_time:1355ms step_avg:34.75ms
step:40/2035 train_time:1389ms step_avg:34.72ms
step:41/2035 train_time:1422ms step_avg:34.68ms
step:42/2035 train_time:1455ms step_avg:34.65ms
step:43/2035 train_time:1489ms step_avg:34.62ms
step:44/2035 train_time:1522ms step_avg:34.59ms
step:45/2035 train_time:1555ms step_avg:34.56ms
step:46/2035 train_time:1588ms step_avg:34.53ms
step:47/2035 train_time:1621ms step_avg:34.50ms
step:48/2035 train_time:1655ms step_avg:34.47ms
step:49/2035 train_time:1688ms step_avg:34.44ms
step:50/2035 train_time:1721ms step_avg:34.42ms
step:51/2035 train_time:1754ms step_avg:34.39ms
step:52/2035 train_time:1787ms step_avg:34.36ms
step:53/2035 train_time:1820ms step_avg:34.34ms
step:54/2035 train_time:1853ms step_avg:34.32ms
step:55/2035 train_time:1886ms step_avg:34.29ms
step:56/2035 train_time:1919ms step_avg:34.27ms
step:57/2035 train_time:1952ms step_avg:34.25ms
step:58/2035 train_time:1985ms step_avg:34.23ms
step:59/2035 train_time:2019ms step_avg:34.21ms
step:60/2035 train_time:2052ms step_avg:34.20ms
step:61/2035 train_time:2085ms step_avg:34.18ms
step:62/2035 train_time:2118ms step_avg:34.17ms
step:63/2035 train_time:2151ms step_avg:34.15ms
step:64/2035 train_time:2185ms step_avg:34.14ms
step:65/2035 train_time:2218ms step_avg:34.13ms
step:66/2035 train_time:2251ms step_avg:34.11ms
step:67/2035 train_time:2285ms step_avg:34.10ms
step:68/2035 train_time:2318ms step_avg:34.09ms
step:69/2035 train_time:2352ms step_avg:34.08ms
step:70/2035 train_time:2385ms step_avg:34.07ms
step:71/2035 train_time:2418ms step_avg:34.06ms
step:72/2035 train_time:2452ms step_avg:34.05ms
step:73/2035 train_time:2485ms step_avg:34.04ms
step:74/2035 train_time:2518ms step_avg:34.03ms
step:75/2035 train_time:2551ms step_avg:34.02ms
step:76/2035 train_time:2584ms step_avg:34.01ms
step:77/2035 train_time:2618ms step_avg:34.00ms
step:78/2035 train_time:2651ms step_avg:33.99ms
step:79/2035 train_time:2684ms step_avg:33.98ms
step:80/2035 train_time:2718ms step_avg:33.97ms
step:81/2035 train_time:2751ms step_avg:33.96ms
step:82/2035 train_time:2784ms step_avg:33.95ms
step:83/2035 train_time:2817ms step_avg:33.94ms
step:84/2035 train_time:2851ms step_avg:33.94ms
step:85/2035 train_time:2884ms step_avg:33.93ms
step:86/2035 train_time:2917ms step_avg:33.92ms
step:87/2035 train_time:2950ms step_avg:33.91ms
step:88/2035 train_time:2984ms step_avg:33.90ms
step:89/2035 train_time:3017ms step_avg:33.90ms
step:90/2035 train_time:3050ms step_avg:33.89ms
step:91/2035 train_time:3083ms step_avg:33.88ms
step:92/2035 train_time:3116ms step_avg:33.87ms
step:93/2035 train_time:3149ms step_avg:33.86ms
step:94/2035 train_time:3183ms step_avg:33.86ms
step:95/2035 train_time:3216ms step_avg:33.85ms
step:96/2035 train_time:3249ms step_avg:33.85ms
step:97/2035 train_time:3283ms step_avg:33.84ms
step:98/2035 train_time:3316ms step_avg:33.84ms
step:99/2035 train_time:3349ms step_avg:33.83ms
step:100/2035 train_time:3382ms step_avg:33.82ms
step:101/2035 train_time:3415ms step_avg:33.82ms
step:102/2035 train_time:3449ms step_avg:33.81ms
step:103/2035 train_time:3482ms step_avg:33.81ms
step:104/2035 train_time:3516ms step_avg:33.81ms
step:105/2035 train_time:3549ms step_avg:33.80ms
step:106/2035 train_time:3582ms step_avg:33.80ms
step:107/2035 train_time:3615ms step_avg:33.79ms
step:108/2035 train_time:3649ms step_avg:33.78ms
step:109/2035 train_time:3682ms step_avg:33.78ms
step:110/2035 train_time:3715ms step_avg:33.77ms
step:111/2035 train_time:3748ms step_avg:33.76ms
step:112/2035 train_time:3781ms step_avg:33.76ms
step:113/2035 train_time:3814ms step_avg:33.75ms
step:114/2035 train_time:3848ms step_avg:33.75ms
step:115/2035 train_time:3881ms step_avg:33.74ms
step:116/2035 train_time:3914ms step_avg:33.74ms
step:117/2035 train_time:3947ms step_avg:33.73ms
step:118/2035 train_time:3980ms step_avg:33.73ms
step:119/2035 train_time:4013ms step_avg:33.72ms
step:120/2035 train_time:4046ms step_avg:33.71ms
step:121/2035 train_time:4079ms step_avg:33.71ms
step:122/2035 train_time:4112ms step_avg:33.71ms
step:123/2035 train_time:4145ms step_avg:33.70ms
step:124/2035 train_time:4178ms step_avg:33.70ms
step:125/2035 train_time:4211ms step_avg:33.69ms
step:126/2035 train_time:4244ms step_avg:33.69ms
step:127/2035 train_time:4278ms step_avg:33.68ms
step:128/2035 train_time:4311ms step_avg:33.68ms
step:129/2035 train_time:4344ms step_avg:33.67ms
step:130/2035 train_time:4377ms step_avg:33.67ms
step:131/2035 train_time:4410ms step_avg:33.67ms
step:132/2035 train_time:4443ms step_avg:33.66ms
step:133/2035 train_time:4477ms step_avg:33.66ms
step:134/2035 train_time:4510ms step_avg:33.66ms
step:135/2035 train_time:4543ms step_avg:33.65ms
step:136/2035 train_time:4577ms step_avg:33.65ms
step:137/2035 train_time:4609ms step_avg:33.64ms
step:138/2035 train_time:4642ms step_avg:33.64ms
step:139/2035 train_time:4675ms step_avg:33.63ms
step:140/2035 train_time:4708ms step_avg:33.63ms
step:141/2035 train_time:4741ms step_avg:33.63ms
step:142/2035 train_time:4775ms step_avg:33.62ms
step:143/2035 train_time:4808ms step_avg:33.62ms
step:144/2035 train_time:4841ms step_avg:33.62ms
step:145/2035 train_time:4874ms step_avg:33.62ms
step:146/2035 train_time:4907ms step_avg:33.61ms
step:147/2035 train_time:4940ms step_avg:33.61ms
step:148/2035 train_time:4974ms step_avg:33.61ms
step:149/2035 train_time:5007ms step_avg:33.60ms
step:150/2035 train_time:5040ms step_avg:33.60ms
step:151/2035 train_time:5073ms step_avg:33.60ms
step:152/2035 train_time:5106ms step_avg:33.59ms
step:153/2035 train_time:5140ms step_avg:33.59ms
step:154/2035 train_time:5173ms step_avg:33.59ms
step:155/2035 train_time:5206ms step_avg:33.59ms
step:156/2035 train_time:5239ms step_avg:33.58ms
step:157/2035 train_time:5273ms step_avg:33.58ms
step:158/2035 train_time:5306ms step_avg:33.58ms
step:159/2035 train_time:5339ms step_avg:33.58ms
step:160/2035 train_time:5372ms step_avg:33.58ms
step:161/2035 train_time:5405ms step_avg:33.57ms
step:162/2035 train_time:5438ms step_avg:33.57ms
step:163/2035 train_time:5472ms step_avg:33.57ms
step:164/2035 train_time:5505ms step_avg:33.57ms
step:165/2035 train_time:5538ms step_avg:33.56ms
step:166/2035 train_time:5571ms step_avg:33.56ms
step:167/2035 train_time:5604ms step_avg:33.56ms
step:168/2035 train_time:5638ms step_avg:33.56ms
step:169/2035 train_time:5671ms step_avg:33.56ms
step:170/2035 train_time:5704ms step_avg:33.55ms
step:171/2035 train_time:5737ms step_avg:33.55ms
step:172/2035 train_time:5770ms step_avg:33.55ms
step:173/2035 train_time:5804ms step_avg:33.55ms
step:174/2035 train_time:5837ms step_avg:33.55ms
step:175/2035 train_time:5870ms step_avg:33.54ms
step:176/2035 train_time:5903ms step_avg:33.54ms
step:177/2035 train_time:5937ms step_avg:33.54ms
step:178/2035 train_time:5970ms step_avg:33.54ms
step:179/2035 train_time:6003ms step_avg:33.54ms
step:180/2035 train_time:6036ms step_avg:33.53ms
step:181/2035 train_time:6069ms step_avg:33.53ms
step:182/2035 train_time:6102ms step_avg:33.53ms
step:183/2035 train_time:6135ms step_avg:33.53ms
step:184/2035 train_time:6168ms step_avg:33.52ms
step:185/2035 train_time:6201ms step_avg:33.52ms
step:186/2035 train_time:6235ms step_avg:33.52ms
step:187/2035 train_time:6268ms step_avg:33.52ms
step:188/2035 train_time:6302ms step_avg:33.52ms
step:189/2035 train_time:6335ms step_avg:33.52ms
step:190/2035 train_time:6368ms step_avg:33.51ms
step:191/2035 train_time:6401ms step_avg:33.51ms
step:192/2035 train_time:6434ms step_avg:33.51ms
step:193/2035 train_time:6467ms step_avg:33.51ms
step:194/2035 train_time:6500ms step_avg:33.51ms
step:195/2035 train_time:6534ms step_avg:33.51ms
step:196/2035 train_time:6567ms step_avg:33.51ms
step:197/2035 train_time:6600ms step_avg:33.50ms
step:198/2035 train_time:6633ms step_avg:33.50ms
step:199/2035 train_time:6666ms step_avg:33.50ms
step:200/2035 train_time:6700ms step_avg:33.50ms
step:201/2035 train_time:6733ms step_avg:33.50ms
step:202/2035 train_time:6767ms step_avg:33.50ms
step:203/2035 train_time:6799ms step_avg:33.49ms
step:204/2035 train_time:6832ms step_avg:33.49ms
step:205/2035 train_time:6865ms step_avg:33.49ms
step:206/2035 train_time:6899ms step_avg:33.49ms
step:207/2035 train_time:6932ms step_avg:33.49ms
step:208/2035 train_time:6965ms step_avg:33.48ms
step:209/2035 train_time:6998ms step_avg:33.48ms
step:210/2035 train_time:7031ms step_avg:33.48ms
step:211/2035 train_time:7064ms step_avg:33.48ms
step:212/2035 train_time:7097ms step_avg:33.48ms
step:213/2035 train_time:7130ms step_avg:33.48ms
step:214/2035 train_time:7163ms step_avg:33.47ms
step:215/2035 train_time:7197ms step_avg:33.47ms
step:216/2035 train_time:7230ms step_avg:33.47ms
step:217/2035 train_time:7263ms step_avg:33.47ms
step:218/2035 train_time:7296ms step_avg:33.47ms
step:219/2035 train_time:7329ms step_avg:33.47ms
step:220/2035 train_time:7362ms step_avg:33.46ms
step:221/2035 train_time:7395ms step_avg:33.46ms
step:222/2035 train_time:7428ms step_avg:33.46ms
step:223/2035 train_time:7461ms step_avg:33.46ms
step:224/2035 train_time:7494ms step_avg:33.45ms
step:225/2035 train_time:7527ms step_avg:33.45ms
step:226/2035 train_time:7560ms step_avg:33.45ms
step:227/2035 train_time:7593ms step_avg:33.45ms
step:228/2035 train_time:7626ms step_avg:33.45ms
step:229/2035 train_time:7659ms step_avg:33.44ms
step:230/2035 train_time:7692ms step_avg:33.44ms
step:231/2035 train_time:7725ms step_avg:33.44ms
step:232/2035 train_time:7759ms step_avg:33.44ms
step:233/2035 train_time:7792ms step_avg:33.44ms
step:234/2035 train_time:7825ms step_avg:33.44ms
step:235/2035 train_time:7858ms step_avg:33.44ms
step:236/2035 train_time:7892ms step_avg:33.44ms
step:237/2035 train_time:7925ms step_avg:33.44ms
step:238/2035 train_time:7958ms step_avg:33.44ms
step:239/2035 train_time:7991ms step_avg:33.44ms
step:240/2035 train_time:8024ms step_avg:33.43ms
step:241/2035 train_time:8057ms step_avg:33.43ms
step:242/2035 train_time:8091ms step_avg:33.43ms
step:243/2035 train_time:8123ms step_avg:33.43ms
step:244/2035 train_time:8157ms step_avg:33.43ms
step:245/2035 train_time:8190ms step_avg:33.43ms
step:246/2035 train_time:8223ms step_avg:33.43ms
step:247/2035 train_time:8256ms step_avg:33.43ms
step:248/2035 train_time:8289ms step_avg:33.42ms
step:249/2035 train_time:8322ms step_avg:33.42ms
step:250/2035 train_time:8355ms step_avg:33.42ms
step:250/2035 val_loss:4.2725 train_time:8391ms step_avg:33.56ms
step:251/2035 train_time:8413ms step_avg:33.52ms
step:252/2035 train_time:8432ms step_avg:33.46ms
step:253/2035 train_time:8458ms step_avg:33.43ms
step:254/2035 train_time:8491ms step_avg:33.43ms
step:255/2035 train_time:8526ms step_avg:33.44ms
step:256/2035 train_time:8561ms step_avg:33.44ms
step:257/2035 train_time:8594ms step_avg:33.44ms
step:258/2035 train_time:8628ms step_avg:33.44ms
step:259/2035 train_time:8660ms step_avg:33.44ms
step:260/2035 train_time:8693ms step_avg:33.44ms
step:261/2035 train_time:8727ms step_avg:33.44ms
step:262/2035 train_time:8760ms step_avg:33.43ms
step:263/2035 train_time:8792ms step_avg:33.43ms
step:264/2035 train_time:8825ms step_avg:33.43ms
step:265/2035 train_time:8858ms step_avg:33.43ms
step:266/2035 train_time:8891ms step_avg:33.42ms
step:267/2035 train_time:8924ms step_avg:33.42ms
step:268/2035 train_time:8957ms step_avg:33.42ms
step:269/2035 train_time:8990ms step_avg:33.42ms
step:270/2035 train_time:9023ms step_avg:33.42ms
step:271/2035 train_time:9055ms step_avg:33.41ms
step:272/2035 train_time:9088ms step_avg:33.41ms
step:273/2035 train_time:9121ms step_avg:33.41ms
step:274/2035 train_time:9154ms step_avg:33.41ms
step:275/2035 train_time:9187ms step_avg:33.41ms
step:276/2035 train_time:9220ms step_avg:33.41ms
step:277/2035 train_time:9253ms step_avg:33.40ms
step:278/2035 train_time:9286ms step_avg:33.40ms
step:279/2035 train_time:9319ms step_avg:33.40ms
step:280/2035 train_time:9353ms step_avg:33.40ms
step:281/2035 train_time:9386ms step_avg:33.40ms
step:282/2035 train_time:9419ms step_avg:33.40ms
step:283/2035 train_time:9453ms step_avg:33.40ms
step:284/2035 train_time:9487ms step_avg:33.40ms
step:285/2035 train_time:9520ms step_avg:33.40ms
step:286/2035 train_time:9553ms step_avg:33.40ms
step:287/2035 train_time:9587ms step_avg:33.40ms
step:288/2035 train_time:9620ms step_avg:33.40ms
step:289/2035 train_time:9653ms step_avg:33.40ms
step:290/2035 train_time:9686ms step_avg:33.40ms
step:291/2035 train_time:9719ms step_avg:33.40ms
step:292/2035 train_time:9752ms step_avg:33.40ms
step:293/2035 train_time:9785ms step_avg:33.40ms
step:294/2035 train_time:9819ms step_avg:33.40ms
step:295/2035 train_time:9852ms step_avg:33.40ms
step:296/2035 train_time:9885ms step_avg:33.40ms
step:297/2035 train_time:9918ms step_avg:33.39ms
step:298/2035 train_time:9951ms step_avg:33.39ms
step:299/2035 train_time:9984ms step_avg:33.39ms
step:300/2035 train_time:10017ms step_avg:33.39ms
step:301/2035 train_time:10049ms step_avg:33.39ms
step:302/2035 train_time:10082ms step_avg:33.39ms
step:303/2035 train_time:10115ms step_avg:33.38ms
step:304/2035 train_time:10148ms step_avg:33.38ms
step:305/2035 train_time:10181ms step_avg:33.38ms
step:306/2035 train_time:10214ms step_avg:33.38ms
step:307/2035 train_time:10247ms step_avg:33.38ms
step:308/2035 train_time:10280ms step_avg:33.38ms
step:309/2035 train_time:10313ms step_avg:33.38ms
step:310/2035 train_time:10346ms step_avg:33.38ms
step:311/2035 train_time:10380ms step_avg:33.38ms
step:312/2035 train_time:10413ms step_avg:33.38ms
step:313/2035 train_time:10446ms step_avg:33.37ms
step:314/2035 train_time:10480ms step_avg:33.38ms
step:315/2035 train_time:10513ms step_avg:33.37ms
step:316/2035 train_time:10546ms step_avg:33.37ms
step:317/2035 train_time:10579ms step_avg:33.37ms
step:318/2035 train_time:10612ms step_avg:33.37ms
step:319/2035 train_time:10645ms step_avg:33.37ms
step:320/2035 train_time:10678ms step_avg:33.37ms
step:321/2035 train_time:10711ms step_avg:33.37ms
step:322/2035 train_time:10745ms step_avg:33.37ms
step:323/2035 train_time:10777ms step_avg:33.37ms
step:324/2035 train_time:10811ms step_avg:33.37ms
step:325/2035 train_time:10844ms step_avg:33.36ms
step:326/2035 train_time:10877ms step_avg:33.36ms
step:327/2035 train_time:10910ms step_avg:33.36ms
step:328/2035 train_time:10943ms step_avg:33.36ms
step:329/2035 train_time:10976ms step_avg:33.36ms
step:330/2035 train_time:11009ms step_avg:33.36ms
step:331/2035 train_time:11042ms step_avg:33.36ms
step:332/2035 train_time:11075ms step_avg:33.36ms
step:333/2035 train_time:11108ms step_avg:33.36ms
step:334/2035 train_time:11141ms step_avg:33.36ms
step:335/2035 train_time:11174ms step_avg:33.36ms
step:336/2035 train_time:11208ms step_avg:33.36ms
step:337/2035 train_time:11240ms step_avg:33.35ms
step:338/2035 train_time:11273ms step_avg:33.35ms
step:339/2035 train_time:11306ms step_avg:33.35ms
step:340/2035 train_time:11339ms step_avg:33.35ms
step:341/2035 train_time:11372ms step_avg:33.35ms
step:342/2035 train_time:11405ms step_avg:33.35ms
step:343/2035 train_time:11439ms step_avg:33.35ms
step:344/2035 train_time:11472ms step_avg:33.35ms
step:345/2035 train_time:11505ms step_avg:33.35ms
step:346/2035 train_time:11539ms step_avg:33.35ms
step:347/2035 train_time:11572ms step_avg:33.35ms
step:348/2035 train_time:11605ms step_avg:33.35ms
step:349/2035 train_time:11638ms step_avg:33.35ms
step:350/2035 train_time:11671ms step_avg:33.35ms
step:351/2035 train_time:11704ms step_avg:33.35ms
step:352/2035 train_time:11738ms step_avg:33.35ms
step:353/2035 train_time:11771ms step_avg:33.34ms
step:354/2035 train_time:11804ms step_avg:33.34ms
step:355/2035 train_time:11837ms step_avg:33.34ms
step:356/2035 train_time:11870ms step_avg:33.34ms
step:357/2035 train_time:11904ms step_avg:33.34ms
step:358/2035 train_time:11937ms step_avg:33.34ms
step:359/2035 train_time:11970ms step_avg:33.34ms
step:360/2035 train_time:12003ms step_avg:33.34ms
step:361/2035 train_time:12036ms step_avg:33.34ms
step:362/2035 train_time:12069ms step_avg:33.34ms
step:363/2035 train_time:12102ms step_avg:33.34ms
step:364/2035 train_time:12135ms step_avg:33.34ms
step:365/2035 train_time:12168ms step_avg:33.34ms
step:366/2035 train_time:12201ms step_avg:33.34ms
step:367/2035 train_time:12235ms step_avg:33.34ms
step:368/2035 train_time:12268ms step_avg:33.34ms
step:369/2035 train_time:12301ms step_avg:33.34ms
step:370/2035 train_time:12334ms step_avg:33.34ms
step:371/2035 train_time:12367ms step_avg:33.33ms
step:372/2035 train_time:12401ms step_avg:33.33ms
step:373/2035 train_time:12434ms step_avg:33.33ms
step:374/2035 train_time:12467ms step_avg:33.33ms
step:375/2035 train_time:12500ms step_avg:33.33ms
step:376/2035 train_time:12534ms step_avg:33.33ms
step:377/2035 train_time:12567ms step_avg:33.33ms
step:378/2035 train_time:12600ms step_avg:33.33ms
step:379/2035 train_time:12633ms step_avg:33.33ms
step:380/2035 train_time:12666ms step_avg:33.33ms
step:381/2035 train_time:12700ms step_avg:33.33ms
step:382/2035 train_time:12733ms step_avg:33.33ms
step:383/2035 train_time:12766ms step_avg:33.33ms
step:384/2035 train_time:12799ms step_avg:33.33ms
step:385/2035 train_time:12832ms step_avg:33.33ms
step:386/2035 train_time:12866ms step_avg:33.33ms
step:387/2035 train_time:12898ms step_avg:33.33ms
step:388/2035 train_time:12931ms step_avg:33.33ms
step:389/2035 train_time:12965ms step_avg:33.33ms
step:390/2035 train_time:12998ms step_avg:33.33ms
step:391/2035 train_time:13031ms step_avg:33.33ms
step:392/2035 train_time:13064ms step_avg:33.33ms
step:393/2035 train_time:13097ms step_avg:33.33ms
step:394/2035 train_time:13130ms step_avg:33.32ms
step:395/2035 train_time:13163ms step_avg:33.32ms
step:396/2035 train_time:13196ms step_avg:33.32ms
step:397/2035 train_time:13229ms step_avg:33.32ms
step:398/2035 train_time:13262ms step_avg:33.32ms
step:399/2035 train_time:13295ms step_avg:33.32ms
step:400/2035 train_time:13328ms step_avg:33.32ms
step:401/2035 train_time:13361ms step_avg:33.32ms
step:402/2035 train_time:13394ms step_avg:33.32ms
step:403/2035 train_time:13427ms step_avg:33.32ms
step:404/2035 train_time:13461ms step_avg:33.32ms
step:405/2035 train_time:13494ms step_avg:33.32ms
step:406/2035 train_time:13527ms step_avg:33.32ms
step:407/2035 train_time:13560ms step_avg:33.32ms
step:408/2035 train_time:13593ms step_avg:33.32ms
step:409/2035 train_time:13626ms step_avg:33.32ms
step:410/2035 train_time:13660ms step_avg:33.32ms
step:411/2035 train_time:13693ms step_avg:33.32ms
step:412/2035 train_time:13726ms step_avg:33.32ms
step:413/2035 train_time:13759ms step_avg:33.32ms
step:414/2035 train_time:13793ms step_avg:33.32ms
step:415/2035 train_time:13826ms step_avg:33.32ms
step:416/2035 train_time:13859ms step_avg:33.32ms
step:417/2035 train_time:13892ms step_avg:33.31ms
step:418/2035 train_time:13925ms step_avg:33.31ms
step:419/2035 train_time:13958ms step_avg:33.31ms
step:420/2035 train_time:13991ms step_avg:33.31ms
step:421/2035 train_time:14024ms step_avg:33.31ms
step:422/2035 train_time:14058ms step_avg:33.31ms
step:423/2035 train_time:14091ms step_avg:33.31ms
step:424/2035 train_time:14124ms step_avg:33.31ms
step:425/2035 train_time:14157ms step_avg:33.31ms
step:426/2035 train_time:14191ms step_avg:33.31ms
step:427/2035 train_time:14223ms step_avg:33.31ms
step:428/2035 train_time:14257ms step_avg:33.31ms
step:429/2035 train_time:14289ms step_avg:33.31ms
step:430/2035 train_time:14322ms step_avg:33.31ms
step:431/2035 train_time:14355ms step_avg:33.31ms
step:432/2035 train_time:14388ms step_avg:33.31ms
step:433/2035 train_time:14421ms step_avg:33.30ms
step:434/2035 train_time:14454ms step_avg:33.30ms
step:435/2035 train_time:14487ms step_avg:33.30ms
step:436/2035 train_time:14520ms step_avg:33.30ms
step:437/2035 train_time:14553ms step_avg:33.30ms
step:438/2035 train_time:14587ms step_avg:33.30ms
step:439/2035 train_time:14620ms step_avg:33.30ms
step:440/2035 train_time:14653ms step_avg:33.30ms
step:441/2035 train_time:14686ms step_avg:33.30ms
step:442/2035 train_time:14720ms step_avg:33.30ms
step:443/2035 train_time:14753ms step_avg:33.30ms
step:444/2035 train_time:14786ms step_avg:33.30ms
step:445/2035 train_time:14819ms step_avg:33.30ms
step:446/2035 train_time:14852ms step_avg:33.30ms
step:447/2035 train_time:14885ms step_avg:33.30ms
step:448/2035 train_time:14919ms step_avg:33.30ms
step:449/2035 train_time:14952ms step_avg:33.30ms
step:450/2035 train_time:14985ms step_avg:33.30ms
step:451/2035 train_time:15018ms step_avg:33.30ms
step:452/2035 train_time:15051ms step_avg:33.30ms
step:453/2035 train_time:15085ms step_avg:33.30ms
step:454/2035 train_time:15118ms step_avg:33.30ms
step:455/2035 train_time:15151ms step_avg:33.30ms
step:456/2035 train_time:15184ms step_avg:33.30ms
step:457/2035 train_time:15218ms step_avg:33.30ms
step:458/2035 train_time:15251ms step_avg:33.30ms
step:459/2035 train_time:15284ms step_avg:33.30ms
step:460/2035 train_time:15317ms step_avg:33.30ms
step:461/2035 train_time:15350ms step_avg:33.30ms
step:462/2035 train_time:15383ms step_avg:33.30ms
step:463/2035 train_time:15416ms step_avg:33.30ms
step:464/2035 train_time:15449ms step_avg:33.29ms
step:465/2035 train_time:15481ms step_avg:33.29ms
step:466/2035 train_time:15515ms step_avg:33.29ms
step:467/2035 train_time:15548ms step_avg:33.29ms
step:468/2035 train_time:15581ms step_avg:33.29ms
step:469/2035 train_time:15614ms step_avg:33.29ms
step:470/2035 train_time:15648ms step_avg:33.29ms
step:471/2035 train_time:15680ms step_avg:33.29ms
step:472/2035 train_time:15714ms step_avg:33.29ms
step:473/2035 train_time:15747ms step_avg:33.29ms
step:474/2035 train_time:15780ms step_avg:33.29ms
step:475/2035 train_time:15813ms step_avg:33.29ms
step:476/2035 train_time:15847ms step_avg:33.29ms
step:477/2035 train_time:15880ms step_avg:33.29ms
step:478/2035 train_time:15913ms step_avg:33.29ms
step:479/2035 train_time:15946ms step_avg:33.29ms
step:480/2035 train_time:15980ms step_avg:33.29ms
step:481/2035 train_time:16013ms step_avg:33.29ms
step:482/2035 train_time:16046ms step_avg:33.29ms
step:483/2035 train_time:16079ms step_avg:33.29ms
step:484/2035 train_time:16112ms step_avg:33.29ms
step:485/2035 train_time:16145ms step_avg:33.29ms
step:486/2035 train_time:16179ms step_avg:33.29ms
step:487/2035 train_time:16212ms step_avg:33.29ms
step:488/2035 train_time:16245ms step_avg:33.29ms
step:489/2035 train_time:16278ms step_avg:33.29ms
step:490/2035 train_time:16311ms step_avg:33.29ms
step:491/2035 train_time:16344ms step_avg:33.29ms
step:492/2035 train_time:16378ms step_avg:33.29ms
step:493/2035 train_time:16411ms step_avg:33.29ms
step:494/2035 train_time:16444ms step_avg:33.29ms
step:495/2035 train_time:16477ms step_avg:33.29ms
step:496/2035 train_time:16510ms step_avg:33.29ms
step:497/2035 train_time:16543ms step_avg:33.29ms
step:498/2035 train_time:16576ms step_avg:33.29ms
step:499/2035 train_time:16610ms step_avg:33.29ms
step:500/2035 train_time:16642ms step_avg:33.28ms
step:500/2035 val_loss:4.0012 train_time:16678ms step_avg:33.36ms
step:501/2035 train_time:16701ms step_avg:33.33ms
step:502/2035 train_time:16720ms step_avg:33.31ms
step:503/2035 train_time:16748ms step_avg:33.30ms
step:504/2035 train_time:16782ms step_avg:33.30ms
step:505/2035 train_time:16816ms step_avg:33.30ms
step:506/2035 train_time:16850ms step_avg:33.30ms
step:507/2035 train_time:16883ms step_avg:33.30ms
step:508/2035 train_time:16916ms step_avg:33.30ms
step:509/2035 train_time:16950ms step_avg:33.30ms
step:510/2035 train_time:16983ms step_avg:33.30ms
step:511/2035 train_time:17016ms step_avg:33.30ms
step:512/2035 train_time:17049ms step_avg:33.30ms
step:513/2035 train_time:17083ms step_avg:33.30ms
step:514/2035 train_time:17116ms step_avg:33.30ms
step:515/2035 train_time:17149ms step_avg:33.30ms
step:516/2035 train_time:17182ms step_avg:33.30ms
step:517/2035 train_time:17215ms step_avg:33.30ms
step:518/2035 train_time:17248ms step_avg:33.30ms
step:519/2035 train_time:17280ms step_avg:33.30ms
step:520/2035 train_time:17313ms step_avg:33.30ms
step:521/2035 train_time:17346ms step_avg:33.29ms
step:522/2035 train_time:17379ms step_avg:33.29ms
step:523/2035 train_time:17412ms step_avg:33.29ms
step:524/2035 train_time:17445ms step_avg:33.29ms
step:525/2035 train_time:17478ms step_avg:33.29ms
step:526/2035 train_time:17511ms step_avg:33.29ms
step:527/2035 train_time:17544ms step_avg:33.29ms
step:528/2035 train_time:17577ms step_avg:33.29ms
step:529/2035 train_time:17610ms step_avg:33.29ms
step:530/2035 train_time:17643ms step_avg:33.29ms
step:531/2035 train_time:17677ms step_avg:33.29ms
step:532/2035 train_time:17710ms step_avg:33.29ms
step:533/2035 train_time:17743ms step_avg:33.29ms
step:534/2035 train_time:17777ms step_avg:33.29ms
step:535/2035 train_time:17810ms step_avg:33.29ms
step:536/2035 train_time:17843ms step_avg:33.29ms
step:537/2035 train_time:17877ms step_avg:33.29ms
step:538/2035 train_time:17910ms step_avg:33.29ms
step:539/2035 train_time:17944ms step_avg:33.29ms
step:540/2035 train_time:17977ms step_avg:33.29ms
step:541/2035 train_time:18010ms step_avg:33.29ms
step:542/2035 train_time:18044ms step_avg:33.29ms
step:543/2035 train_time:18077ms step_avg:33.29ms
step:544/2035 train_time:18110ms step_avg:33.29ms
step:545/2035 train_time:18143ms step_avg:33.29ms
step:546/2035 train_time:18176ms step_avg:33.29ms
step:547/2035 train_time:18209ms step_avg:33.29ms
step:548/2035 train_time:18242ms step_avg:33.29ms
step:549/2035 train_time:18275ms step_avg:33.29ms
step:550/2035 train_time:18308ms step_avg:33.29ms
step:551/2035 train_time:18341ms step_avg:33.29ms
step:552/2035 train_time:18374ms step_avg:33.29ms
step:553/2035 train_time:18407ms step_avg:33.29ms
step:554/2035 train_time:18440ms step_avg:33.29ms
step:555/2035 train_time:18473ms step_avg:33.28ms
step:556/2035 train_time:18506ms step_avg:33.28ms
step:557/2035 train_time:18539ms step_avg:33.28ms
step:558/2035 train_time:18572ms step_avg:33.28ms
step:559/2035 train_time:18605ms step_avg:33.28ms
step:560/2035 train_time:18639ms step_avg:33.28ms
step:561/2035 train_time:18671ms step_avg:33.28ms
step:562/2035 train_time:18705ms step_avg:33.28ms
step:563/2035 train_time:18738ms step_avg:33.28ms
step:564/2035 train_time:18772ms step_avg:33.28ms
step:565/2035 train_time:18804ms step_avg:33.28ms
step:566/2035 train_time:18837ms step_avg:33.28ms
step:567/2035 train_time:18871ms step_avg:33.28ms
step:568/2035 train_time:18904ms step_avg:33.28ms
step:569/2035 train_time:18937ms step_avg:33.28ms
step:570/2035 train_time:18971ms step_avg:33.28ms
step:571/2035 train_time:19004ms step_avg:33.28ms
step:572/2035 train_time:19037ms step_avg:33.28ms
step:573/2035 train_time:19070ms step_avg:33.28ms
step:574/2035 train_time:19103ms step_avg:33.28ms
step:575/2035 train_time:19136ms step_avg:33.28ms
step:576/2035 train_time:19169ms step_avg:33.28ms
step:577/2035 train_time:19202ms step_avg:33.28ms
step:578/2035 train_time:19235ms step_avg:33.28ms
step:579/2035 train_time:19268ms step_avg:33.28ms
step:580/2035 train_time:19301ms step_avg:33.28ms
step:581/2035 train_time:19334ms step_avg:33.28ms
step:582/2035 train_time:19367ms step_avg:33.28ms
step:583/2035 train_time:19401ms step_avg:33.28ms
step:584/2035 train_time:19434ms step_avg:33.28ms
step:585/2035 train_time:19467ms step_avg:33.28ms
step:586/2035 train_time:19501ms step_avg:33.28ms
step:587/2035 train_time:19534ms step_avg:33.28ms
step:588/2035 train_time:19567ms step_avg:33.28ms
step:589/2035 train_time:19600ms step_avg:33.28ms
step:590/2035 train_time:19633ms step_avg:33.28ms
step:591/2035 train_time:19666ms step_avg:33.28ms
step:592/2035 train_time:19700ms step_avg:33.28ms
step:593/2035 train_time:19733ms step_avg:33.28ms
step:594/2035 train_time:19766ms step_avg:33.28ms
step:595/2035 train_time:19799ms step_avg:33.28ms
step:596/2035 train_time:19832ms step_avg:33.28ms
step:597/2035 train_time:19865ms step_avg:33.28ms
step:598/2035 train_time:19899ms step_avg:33.28ms
step:599/2035 train_time:19932ms step_avg:33.28ms
step:600/2035 train_time:19965ms step_avg:33.28ms
step:601/2035 train_time:19999ms step_avg:33.28ms
step:602/2035 train_time:20032ms step_avg:33.28ms
step:603/2035 train_time:20065ms step_avg:33.28ms
step:604/2035 train_time:20099ms step_avg:33.28ms
step:605/2035 train_time:20132ms step_avg:33.28ms
step:606/2035 train_time:20165ms step_avg:33.28ms
step:607/2035 train_time:20198ms step_avg:33.27ms
step:608/2035 train_time:20231ms step_avg:33.27ms
step:609/2035 train_time:20264ms step_avg:33.27ms
step:610/2035 train_time:20297ms step_avg:33.27ms
step:611/2035 train_time:20330ms step_avg:33.27ms
step:612/2035 train_time:20364ms step_avg:33.27ms
step:613/2035 train_time:20397ms step_avg:33.27ms
step:614/2035 train_time:20430ms step_avg:33.27ms
step:615/2035 train_time:20463ms step_avg:33.27ms
step:616/2035 train_time:20497ms step_avg:33.27ms
step:617/2035 train_time:20529ms step_avg:33.27ms
step:618/2035 train_time:20563ms step_avg:33.27ms
step:619/2035 train_time:20596ms step_avg:33.27ms
step:620/2035 train_time:20629ms step_avg:33.27ms
step:621/2035 train_time:20663ms step_avg:33.27ms
step:622/2035 train_time:20696ms step_avg:33.27ms
step:623/2035 train_time:20729ms step_avg:33.27ms
step:624/2035 train_time:20763ms step_avg:33.27ms
step:625/2035 train_time:20795ms step_avg:33.27ms
step:626/2035 train_time:20828ms step_avg:33.27ms
step:627/2035 train_time:20861ms step_avg:33.27ms
step:628/2035 train_time:20895ms step_avg:33.27ms
step:629/2035 train_time:20927ms step_avg:33.27ms
step:630/2035 train_time:20961ms step_avg:33.27ms
step:631/2035 train_time:20994ms step_avg:33.27ms
step:632/2035 train_time:21027ms step_avg:33.27ms
step:633/2035 train_time:21060ms step_avg:33.27ms
step:634/2035 train_time:21093ms step_avg:33.27ms
step:635/2035 train_time:21126ms step_avg:33.27ms
step:636/2035 train_time:21160ms step_avg:33.27ms
step:637/2035 train_time:21193ms step_avg:33.27ms
step:638/2035 train_time:21226ms step_avg:33.27ms
step:639/2035 train_time:21259ms step_avg:33.27ms
step:640/2035 train_time:21292ms step_avg:33.27ms
step:641/2035 train_time:21325ms step_avg:33.27ms
step:642/2035 train_time:21359ms step_avg:33.27ms
step:643/2035 train_time:21392ms step_avg:33.27ms
step:644/2035 train_time:21425ms step_avg:33.27ms
step:645/2035 train_time:21458ms step_avg:33.27ms
step:646/2035 train_time:21492ms step_avg:33.27ms
step:647/2035 train_time:21524ms step_avg:33.27ms
step:648/2035 train_time:21558ms step_avg:33.27ms
step:649/2035 train_time:21591ms step_avg:33.27ms
step:650/2035 train_time:21625ms step_avg:33.27ms
step:651/2035 train_time:21658ms step_avg:33.27ms
step:652/2035 train_time:21691ms step_avg:33.27ms
step:653/2035 train_time:21724ms step_avg:33.27ms
step:654/2035 train_time:21757ms step_avg:33.27ms
step:655/2035 train_time:21790ms step_avg:33.27ms
step:656/2035 train_time:21824ms step_avg:33.27ms
step:657/2035 train_time:21857ms step_avg:33.27ms
step:658/2035 train_time:21890ms step_avg:33.27ms
step:659/2035 train_time:21923ms step_avg:33.27ms
step:660/2035 train_time:21956ms step_avg:33.27ms
step:661/2035 train_time:21990ms step_avg:33.27ms
step:662/2035 train_time:22023ms step_avg:33.27ms
step:663/2035 train_time:22056ms step_avg:33.27ms
step:664/2035 train_time:22089ms step_avg:33.27ms
step:665/2035 train_time:22122ms step_avg:33.27ms
step:666/2035 train_time:22156ms step_avg:33.27ms
step:667/2035 train_time:22215ms step_avg:33.31ms
step:668/2035 train_time:22275ms step_avg:33.35ms
step:669/2035 train_time:22336ms step_avg:33.39ms
step:670/2035 train_time:22395ms step_avg:33.43ms
step:671/2035 train_time:22456ms step_avg:33.47ms
step:672/2035 train_time:22515ms step_avg:33.50ms
step:673/2035 train_time:22576ms step_avg:33.54ms
step:674/2035 train_time:22636ms step_avg:33.58ms
step:675/2035 train_time:22696ms step_avg:33.62ms
step:676/2035 train_time:22755ms step_avg:33.66ms
step:677/2035 train_time:22816ms step_avg:33.70ms
step:678/2035 train_time:22875ms step_avg:33.74ms
step:679/2035 train_time:22935ms step_avg:33.78ms
step:680/2035 train_time:22995ms step_avg:33.82ms
step:681/2035 train_time:23055ms step_avg:33.85ms
step:682/2035 train_time:23115ms step_avg:33.89ms
step:683/2035 train_time:23175ms step_avg:33.93ms
step:684/2035 train_time:23235ms step_avg:33.97ms
step:685/2035 train_time:23295ms step_avg:34.01ms
step:686/2035 train_time:23355ms step_avg:34.05ms
step:687/2035 train_time:23415ms step_avg:34.08ms
step:688/2035 train_time:23475ms step_avg:34.12ms
step:689/2035 train_time:23536ms step_avg:34.16ms
step:690/2035 train_time:23596ms step_avg:34.20ms
step:691/2035 train_time:23657ms step_avg:34.24ms
step:692/2035 train_time:23717ms step_avg:34.27ms
step:693/2035 train_time:23777ms step_avg:34.31ms
step:694/2035 train_time:23836ms step_avg:34.35ms
step:695/2035 train_time:23896ms step_avg:34.38ms
step:696/2035 train_time:23956ms step_avg:34.42ms
step:697/2035 train_time:24017ms step_avg:34.46ms
step:698/2035 train_time:24076ms step_avg:34.49ms
step:699/2035 train_time:24137ms step_avg:34.53ms
step:700/2035 train_time:24196ms step_avg:34.57ms
step:701/2035 train_time:24257ms step_avg:34.60ms
step:702/2035 train_time:24316ms step_avg:34.64ms
step:703/2035 train_time:24377ms step_avg:34.68ms
step:704/2035 train_time:24437ms step_avg:34.71ms
step:705/2035 train_time:24497ms step_avg:34.75ms
step:706/2035 train_time:24556ms step_avg:34.78ms
step:707/2035 train_time:24617ms step_avg:34.82ms
step:708/2035 train_time:24676ms step_avg:34.85ms
step:709/2035 train_time:24737ms step_avg:34.89ms
step:710/2035 train_time:24797ms step_avg:34.92ms
step:711/2035 train_time:24857ms step_avg:34.96ms
step:712/2035 train_time:24917ms step_avg:35.00ms
step:713/2035 train_time:24977ms step_avg:35.03ms
step:714/2035 train_time:25037ms step_avg:35.07ms
step:715/2035 train_time:25098ms step_avg:35.10ms
step:716/2035 train_time:25158ms step_avg:35.14ms
step:717/2035 train_time:25218ms step_avg:35.17ms
step:718/2035 train_time:25277ms step_avg:35.20ms
step:719/2035 train_time:25338ms step_avg:35.24ms
step:720/2035 train_time:25397ms step_avg:35.27ms
step:721/2035 train_time:25458ms step_avg:35.31ms
step:722/2035 train_time:25518ms step_avg:35.34ms
step:723/2035 train_time:25578ms step_avg:35.38ms
step:724/2035 train_time:25638ms step_avg:35.41ms
step:725/2035 train_time:25699ms step_avg:35.45ms
step:726/2035 train_time:25758ms step_avg:35.48ms
step:727/2035 train_time:25819ms step_avg:35.51ms
step:728/2035 train_time:25878ms step_avg:35.55ms
step:729/2035 train_time:25938ms step_avg:35.58ms
step:730/2035 train_time:25997ms step_avg:35.61ms
step:731/2035 train_time:26057ms step_avg:35.65ms
step:732/2035 train_time:26117ms step_avg:35.68ms
step:733/2035 train_time:26178ms step_avg:35.71ms
step:734/2035 train_time:26237ms step_avg:35.75ms
step:735/2035 train_time:26297ms step_avg:35.78ms
step:736/2035 train_time:26357ms step_avg:35.81ms
step:737/2035 train_time:26417ms step_avg:35.84ms
step:738/2035 train_time:26477ms step_avg:35.88ms
step:739/2035 train_time:26538ms step_avg:35.91ms
step:740/2035 train_time:26597ms step_avg:35.94ms
step:741/2035 train_time:26658ms step_avg:35.98ms
step:742/2035 train_time:26718ms step_avg:36.01ms
step:743/2035 train_time:26778ms step_avg:36.04ms
step:744/2035 train_time:26838ms step_avg:36.07ms
step:745/2035 train_time:26899ms step_avg:36.11ms
step:746/2035 train_time:26958ms step_avg:36.14ms
step:747/2035 train_time:27019ms step_avg:36.17ms
step:748/2035 train_time:27078ms step_avg:36.20ms
step:749/2035 train_time:27139ms step_avg:36.23ms
step:750/2035 train_time:27198ms step_avg:36.26ms
step:750/2035 val_loss:3.8359 train_time:27260ms step_avg:36.35ms
step:751/2035 train_time:27286ms step_avg:36.33ms
step:752/2035 train_time:27321ms step_avg:36.33ms
step:753/2035 train_time:27385ms step_avg:36.37ms
step:754/2035 train_time:27447ms step_avg:36.40ms
step:755/2035 train_time:27507ms step_avg:36.43ms
step:756/2035 train_time:27567ms step_avg:36.46ms
step:757/2035 train_time:27627ms step_avg:36.49ms
step:758/2035 train_time:27685ms step_avg:36.52ms
step:759/2035 train_time:27746ms step_avg:36.56ms
step:760/2035 train_time:27804ms step_avg:36.58ms
step:761/2035 train_time:27864ms step_avg:36.62ms
step:762/2035 train_time:27923ms step_avg:36.64ms
step:763/2035 train_time:27984ms step_avg:36.68ms
step:764/2035 train_time:28043ms step_avg:36.71ms
step:765/2035 train_time:28103ms step_avg:36.74ms
step:766/2035 train_time:28163ms step_avg:36.77ms
step:767/2035 train_time:28226ms step_avg:36.80ms
step:768/2035 train_time:28287ms step_avg:36.83ms
step:769/2035 train_time:28349ms step_avg:36.86ms
step:770/2035 train_time:28410ms step_avg:36.90ms
step:771/2035 train_time:28471ms step_avg:36.93ms
step:772/2035 train_time:28531ms step_avg:36.96ms
step:773/2035 train_time:28592ms step_avg:36.99ms
step:774/2035 train_time:28652ms step_avg:37.02ms
step:775/2035 train_time:28714ms step_avg:37.05ms
step:776/2035 train_time:28773ms step_avg:37.08ms
step:777/2035 train_time:28834ms step_avg:37.11ms
step:778/2035 train_time:28893ms step_avg:37.14ms
step:779/2035 train_time:28953ms step_avg:37.17ms
step:780/2035 train_time:29013ms step_avg:37.20ms
step:781/2035 train_time:29074ms step_avg:37.23ms
step:782/2035 train_time:29134ms step_avg:37.26ms
step:783/2035 train_time:29196ms step_avg:37.29ms
step:784/2035 train_time:29256ms step_avg:37.32ms
step:785/2035 train_time:29318ms step_avg:37.35ms
step:786/2035 train_time:29379ms step_avg:37.38ms
step:787/2035 train_time:29440ms step_avg:37.41ms
step:788/2035 train_time:29501ms step_avg:37.44ms
step:789/2035 train_time:29561ms step_avg:37.47ms
step:790/2035 train_time:29620ms step_avg:37.49ms
step:791/2035 train_time:29680ms step_avg:37.52ms
step:792/2035 train_time:29740ms step_avg:37.55ms
step:793/2035 train_time:29801ms step_avg:37.58ms
step:794/2035 train_time:29860ms step_avg:37.61ms
step:795/2035 train_time:29920ms step_avg:37.64ms
step:796/2035 train_time:29980ms step_avg:37.66ms
step:797/2035 train_time:30040ms step_avg:37.69ms
step:798/2035 train_time:30100ms step_avg:37.72ms
step:799/2035 train_time:30160ms step_avg:37.75ms
step:800/2035 train_time:30220ms step_avg:37.78ms
step:801/2035 train_time:30281ms step_avg:37.80ms
step:802/2035 train_time:30342ms step_avg:37.83ms
step:803/2035 train_time:30402ms step_avg:37.86ms
step:804/2035 train_time:30462ms step_avg:37.89ms
step:805/2035 train_time:30522ms step_avg:37.92ms
step:806/2035 train_time:30582ms step_avg:37.94ms
step:807/2035 train_time:30643ms step_avg:37.97ms
step:808/2035 train_time:30702ms step_avg:38.00ms
step:809/2035 train_time:30762ms step_avg:38.03ms
step:810/2035 train_time:30821ms step_avg:38.05ms
step:811/2035 train_time:30882ms step_avg:38.08ms
step:812/2035 train_time:30942ms step_avg:38.11ms
step:813/2035 train_time:31002ms step_avg:38.13ms
step:814/2035 train_time:31062ms step_avg:38.16ms
step:815/2035 train_time:31122ms step_avg:38.19ms
step:816/2035 train_time:31182ms step_avg:38.21ms
step:817/2035 train_time:31243ms step_avg:38.24ms
step:818/2035 train_time:31303ms step_avg:38.27ms
step:819/2035 train_time:31363ms step_avg:38.29ms
step:820/2035 train_time:31423ms step_avg:38.32ms
step:821/2035 train_time:31484ms step_avg:38.35ms
step:822/2035 train_time:31543ms step_avg:38.37ms
step:823/2035 train_time:31604ms step_avg:38.40ms
step:824/2035 train_time:31664ms step_avg:38.43ms
step:825/2035 train_time:31725ms step_avg:38.45ms
step:826/2035 train_time:31784ms step_avg:38.48ms
step:827/2035 train_time:31844ms step_avg:38.51ms
step:828/2035 train_time:31903ms step_avg:38.53ms
step:829/2035 train_time:31964ms step_avg:38.56ms
step:830/2035 train_time:32022ms step_avg:38.58ms
step:831/2035 train_time:32083ms step_avg:38.61ms
step:832/2035 train_time:32142ms step_avg:38.63ms
step:833/2035 train_time:32203ms step_avg:38.66ms
step:834/2035 train_time:32263ms step_avg:38.68ms
step:835/2035 train_time:32324ms step_avg:38.71ms
step:836/2035 train_time:32384ms step_avg:38.74ms
step:837/2035 train_time:32444ms step_avg:38.76ms
step:838/2035 train_time:32504ms step_avg:38.79ms
step:839/2035 train_time:32565ms step_avg:38.81ms
step:840/2035 train_time:32624ms step_avg:38.84ms
step:841/2035 train_time:32685ms step_avg:38.86ms
step:842/2035 train_time:32744ms step_avg:38.89ms
step:843/2035 train_time:32806ms step_avg:38.92ms
step:844/2035 train_time:32864ms step_avg:38.94ms
step:845/2035 train_time:32924ms step_avg:38.96ms
step:846/2035 train_time:32984ms step_avg:38.99ms
step:847/2035 train_time:33044ms step_avg:39.01ms
step:848/2035 train_time:33104ms step_avg:39.04ms
step:849/2035 train_time:33165ms step_avg:39.06ms
step:850/2035 train_time:33224ms step_avg:39.09ms
step:851/2035 train_time:33284ms step_avg:39.11ms
step:852/2035 train_time:33344ms step_avg:39.14ms
step:853/2035 train_time:33405ms step_avg:39.16ms
step:854/2035 train_time:33465ms step_avg:39.19ms
step:855/2035 train_time:33526ms step_avg:39.21ms
step:856/2035 train_time:33587ms step_avg:39.24ms
step:857/2035 train_time:33646ms step_avg:39.26ms
step:858/2035 train_time:33706ms step_avg:39.28ms
step:859/2035 train_time:33767ms step_avg:39.31ms
step:860/2035 train_time:33826ms step_avg:39.33ms
step:861/2035 train_time:33886ms step_avg:39.36ms
step:862/2035 train_time:33945ms step_avg:39.38ms
step:863/2035 train_time:34005ms step_avg:39.40ms
step:864/2035 train_time:34064ms step_avg:39.43ms
step:865/2035 train_time:34125ms step_avg:39.45ms
step:866/2035 train_time:34185ms step_avg:39.47ms
step:867/2035 train_time:34245ms step_avg:39.50ms
step:868/2035 train_time:34305ms step_avg:39.52ms
step:869/2035 train_time:34366ms step_avg:39.55ms
step:870/2035 train_time:34425ms step_avg:39.57ms
step:871/2035 train_time:34486ms step_avg:39.59ms
step:872/2035 train_time:34545ms step_avg:39.62ms
step:873/2035 train_time:34606ms step_avg:39.64ms
step:874/2035 train_time:34665ms step_avg:39.66ms
step:875/2035 train_time:34727ms step_avg:39.69ms
step:876/2035 train_time:34786ms step_avg:39.71ms
step:877/2035 train_time:34846ms step_avg:39.73ms
step:878/2035 train_time:34906ms step_avg:39.76ms
step:879/2035 train_time:34967ms step_avg:39.78ms
step:880/2035 train_time:35026ms step_avg:39.80ms
step:881/2035 train_time:35086ms step_avg:39.83ms
step:882/2035 train_time:35146ms step_avg:39.85ms
step:883/2035 train_time:35206ms step_avg:39.87ms
step:884/2035 train_time:35266ms step_avg:39.89ms
step:885/2035 train_time:35327ms step_avg:39.92ms
step:886/2035 train_time:35386ms step_avg:39.94ms
step:887/2035 train_time:35448ms step_avg:39.96ms
step:888/2035 train_time:35508ms step_avg:39.99ms
step:889/2035 train_time:35569ms step_avg:40.01ms
step:890/2035 train_time:35628ms step_avg:40.03ms
step:891/2035 train_time:35689ms step_avg:40.05ms
step:892/2035 train_time:35748ms step_avg:40.08ms
step:893/2035 train_time:35809ms step_avg:40.10ms
step:894/2035 train_time:35868ms step_avg:40.12ms
step:895/2035 train_time:35929ms step_avg:40.14ms
step:896/2035 train_time:35989ms step_avg:40.17ms
step:897/2035 train_time:36050ms step_avg:40.19ms
step:898/2035 train_time:36109ms step_avg:40.21ms
step:899/2035 train_time:36170ms step_avg:40.23ms
step:900/2035 train_time:36230ms step_avg:40.26ms
step:901/2035 train_time:36291ms step_avg:40.28ms
step:902/2035 train_time:36351ms step_avg:40.30ms
step:903/2035 train_time:36412ms step_avg:40.32ms
step:904/2035 train_time:36473ms step_avg:40.35ms
step:905/2035 train_time:36535ms step_avg:40.37ms
step:906/2035 train_time:36595ms step_avg:40.39ms
step:907/2035 train_time:36655ms step_avg:40.41ms
step:908/2035 train_time:36714ms step_avg:40.43ms
step:909/2035 train_time:36775ms step_avg:40.46ms
step:910/2035 train_time:36836ms step_avg:40.48ms
step:911/2035 train_time:36897ms step_avg:40.50ms
step:912/2035 train_time:36957ms step_avg:40.52ms
step:913/2035 train_time:37018ms step_avg:40.54ms
step:914/2035 train_time:37078ms step_avg:40.57ms
step:915/2035 train_time:37139ms step_avg:40.59ms
step:916/2035 train_time:37199ms step_avg:40.61ms
step:917/2035 train_time:37260ms step_avg:40.63ms
step:918/2035 train_time:37320ms step_avg:40.65ms
step:919/2035 train_time:37381ms step_avg:40.68ms
step:920/2035 train_time:37441ms step_avg:40.70ms
step:921/2035 train_time:37501ms step_avg:40.72ms
step:922/2035 train_time:37561ms step_avg:40.74ms
step:923/2035 train_time:37621ms step_avg:40.76ms
step:924/2035 train_time:37681ms step_avg:40.78ms
step:925/2035 train_time:37741ms step_avg:40.80ms
step:926/2035 train_time:37801ms step_avg:40.82ms
step:927/2035 train_time:37861ms step_avg:40.84ms
step:928/2035 train_time:37921ms step_avg:40.86ms
step:929/2035 train_time:37982ms step_avg:40.88ms
step:930/2035 train_time:38041ms step_avg:40.90ms
step:931/2035 train_time:38102ms step_avg:40.93ms
step:932/2035 train_time:38162ms step_avg:40.95ms
step:933/2035 train_time:38223ms step_avg:40.97ms
step:934/2035 train_time:38283ms step_avg:40.99ms
step:935/2035 train_time:38343ms step_avg:41.01ms
step:936/2035 train_time:38402ms step_avg:41.03ms
step:937/2035 train_time:38463ms step_avg:41.05ms
step:938/2035 train_time:38522ms step_avg:41.07ms
step:939/2035 train_time:38581ms step_avg:41.09ms
step:940/2035 train_time:38641ms step_avg:41.11ms
step:941/2035 train_time:38702ms step_avg:41.13ms
step:942/2035 train_time:38762ms step_avg:41.15ms
step:943/2035 train_time:38823ms step_avg:41.17ms
step:944/2035 train_time:38882ms step_avg:41.19ms
step:945/2035 train_time:38943ms step_avg:41.21ms
step:946/2035 train_time:39003ms step_avg:41.23ms
step:947/2035 train_time:39064ms step_avg:41.25ms
step:948/2035 train_time:39123ms step_avg:41.27ms
step:949/2035 train_time:39184ms step_avg:41.29ms
step:950/2035 train_time:39243ms step_avg:41.31ms
step:951/2035 train_time:39304ms step_avg:41.33ms
step:952/2035 train_time:39363ms step_avg:41.35ms
step:953/2035 train_time:39423ms step_avg:41.37ms
step:954/2035 train_time:39483ms step_avg:41.39ms
step:955/2035 train_time:39543ms step_avg:41.41ms
step:956/2035 train_time:39603ms step_avg:41.43ms
step:957/2035 train_time:39664ms step_avg:41.45ms
step:958/2035 train_time:39723ms step_avg:41.46ms
step:959/2035 train_time:39784ms step_avg:41.48ms
step:960/2035 train_time:39844ms step_avg:41.50ms
step:961/2035 train_time:39905ms step_avg:41.52ms
step:962/2035 train_time:39965ms step_avg:41.54ms
step:963/2035 train_time:40026ms step_avg:41.56ms
step:964/2035 train_time:40085ms step_avg:41.58ms
step:965/2035 train_time:40146ms step_avg:41.60ms
step:966/2035 train_time:40206ms step_avg:41.62ms
step:967/2035 train_time:40267ms step_avg:41.64ms
step:968/2035 train_time:40327ms step_avg:41.66ms
step:969/2035 train_time:40387ms step_avg:41.68ms
step:970/2035 train_time:40446ms step_avg:41.70ms
step:971/2035 train_time:40507ms step_avg:41.72ms
step:972/2035 train_time:40566ms step_avg:41.73ms
step:973/2035 train_time:40627ms step_avg:41.75ms
step:974/2035 train_time:40687ms step_avg:41.77ms
step:975/2035 train_time:40747ms step_avg:41.79ms
step:976/2035 train_time:40806ms step_avg:41.81ms
step:977/2035 train_time:40868ms step_avg:41.83ms
step:978/2035 train_time:40927ms step_avg:41.85ms
step:979/2035 train_time:40988ms step_avg:41.87ms
step:980/2035 train_time:41048ms step_avg:41.89ms
step:981/2035 train_time:41109ms step_avg:41.91ms
step:982/2035 train_time:41169ms step_avg:41.92ms
step:983/2035 train_time:41229ms step_avg:41.94ms
step:984/2035 train_time:41289ms step_avg:41.96ms
step:985/2035 train_time:41349ms step_avg:41.98ms
step:986/2035 train_time:41408ms step_avg:42.00ms
step:987/2035 train_time:41469ms step_avg:42.02ms
step:988/2035 train_time:41529ms step_avg:42.03ms
step:989/2035 train_time:41590ms step_avg:42.05ms
step:990/2035 train_time:41650ms step_avg:42.07ms
step:991/2035 train_time:41710ms step_avg:42.09ms
step:992/2035 train_time:41770ms step_avg:42.11ms
step:993/2035 train_time:41831ms step_avg:42.13ms
step:994/2035 train_time:41891ms step_avg:42.14ms
step:995/2035 train_time:41952ms step_avg:42.16ms
step:996/2035 train_time:42012ms step_avg:42.18ms
step:997/2035 train_time:42073ms step_avg:42.20ms
step:998/2035 train_time:42133ms step_avg:42.22ms
step:999/2035 train_time:42193ms step_avg:42.24ms
step:1000/2035 train_time:42253ms step_avg:42.25ms
step:1000/2035 val_loss:3.6946 train_time:42315ms step_avg:42.32ms
step:1001/2035 train_time:42336ms step_avg:42.29ms
step:1002/2035 train_time:42374ms step_avg:42.29ms
step:1003/2035 train_time:42436ms step_avg:42.31ms
step:1004/2035 train_time:42497ms step_avg:42.33ms
step:1005/2035 train_time:42558ms step_avg:42.35ms
step:1006/2035 train_time:42617ms step_avg:42.36ms
step:1007/2035 train_time:42677ms step_avg:42.38ms
step:1008/2035 train_time:42736ms step_avg:42.40ms
step:1009/2035 train_time:42796ms step_avg:42.41ms
step:1010/2035 train_time:42854ms step_avg:42.43ms
step:1011/2035 train_time:42914ms step_avg:42.45ms
step:1012/2035 train_time:42973ms step_avg:42.46ms
step:1013/2035 train_time:43033ms step_avg:42.48ms
step:1014/2035 train_time:43092ms step_avg:42.50ms
step:1015/2035 train_time:43152ms step_avg:42.51ms
step:1016/2035 train_time:43212ms step_avg:42.53ms
step:1017/2035 train_time:43275ms step_avg:42.55ms
step:1018/2035 train_time:43335ms step_avg:42.57ms
step:1019/2035 train_time:43397ms step_avg:42.59ms
step:1020/2035 train_time:43457ms step_avg:42.61ms
step:1021/2035 train_time:43520ms step_avg:42.62ms
step:1022/2035 train_time:43579ms step_avg:42.64ms
step:1023/2035 train_time:43639ms step_avg:42.66ms
step:1024/2035 train_time:43698ms step_avg:42.67ms
step:1025/2035 train_time:43758ms step_avg:42.69ms
step:1026/2035 train_time:43817ms step_avg:42.71ms
step:1027/2035 train_time:43878ms step_avg:42.72ms
step:1028/2035 train_time:43937ms step_avg:42.74ms
step:1029/2035 train_time:43997ms step_avg:42.76ms
step:1030/2035 train_time:44057ms step_avg:42.77ms
step:1031/2035 train_time:44117ms step_avg:42.79ms
step:1032/2035 train_time:44177ms step_avg:42.81ms
step:1033/2035 train_time:44238ms step_avg:42.83ms
step:1034/2035 train_time:44298ms step_avg:42.84ms
step:1035/2035 train_time:44360ms step_avg:42.86ms
step:1036/2035 train_time:44420ms step_avg:42.88ms
step:1037/2035 train_time:44481ms step_avg:42.89ms
step:1038/2035 train_time:44541ms step_avg:42.91ms
step:1039/2035 train_time:44602ms step_avg:42.93ms
step:1040/2035 train_time:44662ms step_avg:42.94ms
step:1041/2035 train_time:44723ms step_avg:42.96ms
step:1042/2035 train_time:44782ms step_avg:42.98ms
step:1043/2035 train_time:44843ms step_avg:42.99ms
step:1044/2035 train_time:44902ms step_avg:43.01ms
step:1045/2035 train_time:44963ms step_avg:43.03ms
step:1046/2035 train_time:45024ms step_avg:43.04ms
step:1047/2035 train_time:45084ms step_avg:43.06ms
step:1048/2035 train_time:45144ms step_avg:43.08ms
step:1049/2035 train_time:45205ms step_avg:43.09ms
step:1050/2035 train_time:45264ms step_avg:43.11ms
step:1051/2035 train_time:45326ms step_avg:43.13ms
step:1052/2035 train_time:45387ms step_avg:43.14ms
step:1053/2035 train_time:45448ms step_avg:43.16ms
step:1054/2035 train_time:45508ms step_avg:43.18ms
step:1055/2035 train_time:45568ms step_avg:43.19ms
step:1056/2035 train_time:45629ms step_avg:43.21ms
step:1057/2035 train_time:45689ms step_avg:43.23ms
step:1058/2035 train_time:45749ms step_avg:43.24ms
step:1059/2035 train_time:45809ms step_avg:43.26ms
step:1060/2035 train_time:45868ms step_avg:43.27ms
step:1061/2035 train_time:45929ms step_avg:43.29ms
step:1062/2035 train_time:45990ms step_avg:43.30ms
step:1063/2035 train_time:46049ms step_avg:43.32ms
step:1064/2035 train_time:46108ms step_avg:43.34ms
step:1065/2035 train_time:46170ms step_avg:43.35ms
step:1066/2035 train_time:46230ms step_avg:43.37ms
step:1067/2035 train_time:46290ms step_avg:43.38ms
step:1068/2035 train_time:46350ms step_avg:43.40ms
step:1069/2035 train_time:46411ms step_avg:43.42ms
step:1070/2035 train_time:46471ms step_avg:43.43ms
step:1071/2035 train_time:46532ms step_avg:43.45ms
step:1072/2035 train_time:46592ms step_avg:43.46ms
step:1073/2035 train_time:46653ms step_avg:43.48ms
step:1074/2035 train_time:46712ms step_avg:43.49ms
step:1075/2035 train_time:46772ms step_avg:43.51ms
step:1076/2035 train_time:46831ms step_avg:43.52ms
step:1077/2035 train_time:46892ms step_avg:43.54ms
step:1078/2035 train_time:46952ms step_avg:43.55ms
step:1079/2035 train_time:47012ms step_avg:43.57ms
step:1080/2035 train_time:47072ms step_avg:43.58ms
step:1081/2035 train_time:47132ms step_avg:43.60ms
step:1082/2035 train_time:47191ms step_avg:43.61ms
step:1083/2035 train_time:47252ms step_avg:43.63ms
step:1084/2035 train_time:47312ms step_avg:43.65ms
step:1085/2035 train_time:47372ms step_avg:43.66ms
step:1086/2035 train_time:47432ms step_avg:43.68ms
step:1087/2035 train_time:47493ms step_avg:43.69ms
step:1088/2035 train_time:47554ms step_avg:43.71ms
step:1089/2035 train_time:47615ms step_avg:43.72ms
step:1090/2035 train_time:47675ms step_avg:43.74ms
step:1091/2035 train_time:47735ms step_avg:43.75ms
step:1092/2035 train_time:47794ms step_avg:43.77ms
step:1093/2035 train_time:47855ms step_avg:43.78ms
step:1094/2035 train_time:47914ms step_avg:43.80ms
step:1095/2035 train_time:47974ms step_avg:43.81ms
step:1096/2035 train_time:48033ms step_avg:43.83ms
step:1097/2035 train_time:48094ms step_avg:43.84ms
step:1098/2035 train_time:48154ms step_avg:43.86ms
step:1099/2035 train_time:48216ms step_avg:43.87ms
step:1100/2035 train_time:48275ms step_avg:43.89ms
step:1101/2035 train_time:48336ms step_avg:43.90ms
step:1102/2035 train_time:48395ms step_avg:43.92ms
step:1103/2035 train_time:48456ms step_avg:43.93ms
step:1104/2035 train_time:48516ms step_avg:43.95ms
step:1105/2035 train_time:48577ms step_avg:43.96ms
step:1106/2035 train_time:48637ms step_avg:43.98ms
step:1107/2035 train_time:48698ms step_avg:43.99ms
step:1108/2035 train_time:48757ms step_avg:44.00ms
step:1109/2035 train_time:48818ms step_avg:44.02ms
step:1110/2035 train_time:48878ms step_avg:44.03ms
step:1111/2035 train_time:48938ms step_avg:44.05ms
step:1112/2035 train_time:48997ms step_avg:44.06ms
step:1113/2035 train_time:49058ms step_avg:44.08ms
step:1114/2035 train_time:49117ms step_avg:44.09ms
step:1115/2035 train_time:49178ms step_avg:44.11ms
step:1116/2035 train_time:49237ms step_avg:44.12ms
step:1117/2035 train_time:49298ms step_avg:44.13ms
step:1118/2035 train_time:49358ms step_avg:44.15ms
step:1119/2035 train_time:49418ms step_avg:44.16ms
step:1120/2035 train_time:49477ms step_avg:44.18ms
step:1121/2035 train_time:49538ms step_avg:44.19ms
step:1122/2035 train_time:49598ms step_avg:44.21ms
step:1123/2035 train_time:49659ms step_avg:44.22ms
step:1124/2035 train_time:49718ms step_avg:44.23ms
step:1125/2035 train_time:49779ms step_avg:44.25ms
step:1126/2035 train_time:49839ms step_avg:44.26ms
step:1127/2035 train_time:49899ms step_avg:44.28ms
step:1128/2035 train_time:49959ms step_avg:44.29ms
step:1129/2035 train_time:50020ms step_avg:44.30ms
step:1130/2035 train_time:50080ms step_avg:44.32ms
step:1131/2035 train_time:50141ms step_avg:44.33ms
step:1132/2035 train_time:50202ms step_avg:44.35ms
step:1133/2035 train_time:50262ms step_avg:44.36ms
step:1134/2035 train_time:50321ms step_avg:44.38ms
step:1135/2035 train_time:50382ms step_avg:44.39ms
step:1136/2035 train_time:50442ms step_avg:44.40ms
step:1137/2035 train_time:50503ms step_avg:44.42ms
step:1138/2035 train_time:50564ms step_avg:44.43ms
step:1139/2035 train_time:50625ms step_avg:44.45ms
step:1140/2035 train_time:50685ms step_avg:44.46ms
step:1141/2035 train_time:50746ms step_avg:44.48ms
step:1142/2035 train_time:50806ms step_avg:44.49ms
step:1143/2035 train_time:50866ms step_avg:44.50ms
step:1144/2035 train_time:50927ms step_avg:44.52ms
step:1145/2035 train_time:50987ms step_avg:44.53ms
step:1146/2035 train_time:51047ms step_avg:44.54ms
step:1147/2035 train_time:51108ms step_avg:44.56ms
step:1148/2035 train_time:51168ms step_avg:44.57ms
step:1149/2035 train_time:51228ms step_avg:44.59ms
step:1150/2035 train_time:51288ms step_avg:44.60ms
step:1151/2035 train_time:51349ms step_avg:44.61ms
step:1152/2035 train_time:51409ms step_avg:44.63ms
step:1153/2035 train_time:51469ms step_avg:44.64ms
step:1154/2035 train_time:51531ms step_avg:44.65ms
step:1155/2035 train_time:51591ms step_avg:44.67ms
step:1156/2035 train_time:51650ms step_avg:44.68ms
step:1157/2035 train_time:51711ms step_avg:44.69ms
step:1158/2035 train_time:51770ms step_avg:44.71ms
step:1159/2035 train_time:51831ms step_avg:44.72ms
step:1160/2035 train_time:51890ms step_avg:44.73ms
step:1161/2035 train_time:51951ms step_avg:44.75ms
step:1162/2035 train_time:52010ms step_avg:44.76ms
step:1163/2035 train_time:52072ms step_avg:44.77ms
step:1164/2035 train_time:52131ms step_avg:44.79ms
step:1165/2035 train_time:52191ms step_avg:44.80ms
step:1166/2035 train_time:52250ms step_avg:44.81ms
step:1167/2035 train_time:52310ms step_avg:44.82ms
step:1168/2035 train_time:52369ms step_avg:44.84ms
step:1169/2035 train_time:52430ms step_avg:44.85ms
step:1170/2035 train_time:52490ms step_avg:44.86ms
step:1171/2035 train_time:52550ms step_avg:44.88ms
step:1172/2035 train_time:52610ms step_avg:44.89ms
step:1173/2035 train_time:52671ms step_avg:44.90ms
step:1174/2035 train_time:52731ms step_avg:44.92ms
step:1175/2035 train_time:52791ms step_avg:44.93ms
step:1176/2035 train_time:52851ms step_avg:44.94ms
step:1177/2035 train_time:52912ms step_avg:44.95ms
step:1178/2035 train_time:52971ms step_avg:44.97ms
step:1179/2035 train_time:53032ms step_avg:44.98ms
step:1180/2035 train_time:53092ms step_avg:44.99ms
step:1181/2035 train_time:53153ms step_avg:45.01ms
step:1182/2035 train_time:53212ms step_avg:45.02ms
step:1183/2035 train_time:53273ms step_avg:45.03ms
step:1184/2035 train_time:53332ms step_avg:45.04ms
step:1185/2035 train_time:53392ms step_avg:45.06ms
step:1186/2035 train_time:53452ms step_avg:45.07ms
step:1187/2035 train_time:53513ms step_avg:45.08ms
step:1188/2035 train_time:53573ms step_avg:45.09ms
step:1189/2035 train_time:53634ms step_avg:45.11ms
step:1190/2035 train_time:53693ms step_avg:45.12ms
step:1191/2035 train_time:53754ms step_avg:45.13ms
step:1192/2035 train_time:53813ms step_avg:45.14ms
step:1193/2035 train_time:53874ms step_avg:45.16ms
step:1194/2035 train_time:53933ms step_avg:45.17ms
step:1195/2035 train_time:53995ms step_avg:45.18ms
step:1196/2035 train_time:54054ms step_avg:45.20ms
step:1197/2035 train_time:54115ms step_avg:45.21ms
step:1198/2035 train_time:54175ms step_avg:45.22ms
step:1199/2035 train_time:54234ms step_avg:45.23ms
step:1200/2035 train_time:54293ms step_avg:45.24ms
step:1201/2035 train_time:54354ms step_avg:45.26ms
step:1202/2035 train_time:54413ms step_avg:45.27ms
step:1203/2035 train_time:54474ms step_avg:45.28ms
step:1204/2035 train_time:54534ms step_avg:45.29ms
step:1205/2035 train_time:54594ms step_avg:45.31ms
step:1206/2035 train_time:54654ms step_avg:45.32ms
step:1207/2035 train_time:54715ms step_avg:45.33ms
step:1208/2035 train_time:54775ms step_avg:45.34ms
step:1209/2035 train_time:54836ms step_avg:45.36ms
step:1210/2035 train_time:54895ms step_avg:45.37ms
step:1211/2035 train_time:54956ms step_avg:45.38ms
step:1212/2035 train_time:55015ms step_avg:45.39ms
step:1213/2035 train_time:55076ms step_avg:45.41ms
step:1214/2035 train_time:55136ms step_avg:45.42ms
step:1215/2035 train_time:55197ms step_avg:45.43ms
step:1216/2035 train_time:55256ms step_avg:45.44ms
step:1217/2035 train_time:55316ms step_avg:45.45ms
step:1218/2035 train_time:55375ms step_avg:45.46ms
step:1219/2035 train_time:55436ms step_avg:45.48ms
step:1220/2035 train_time:55496ms step_avg:45.49ms
step:1221/2035 train_time:55557ms step_avg:45.50ms
step:1222/2035 train_time:55616ms step_avg:45.51ms
step:1223/2035 train_time:55678ms step_avg:45.53ms
step:1224/2035 train_time:55737ms step_avg:45.54ms
step:1225/2035 train_time:55798ms step_avg:45.55ms
step:1226/2035 train_time:55858ms step_avg:45.56ms
step:1227/2035 train_time:55919ms step_avg:45.57ms
step:1228/2035 train_time:55978ms step_avg:45.58ms
step:1229/2035 train_time:56039ms step_avg:45.60ms
step:1230/2035 train_time:56098ms step_avg:45.61ms
step:1231/2035 train_time:56159ms step_avg:45.62ms
step:1232/2035 train_time:56219ms step_avg:45.63ms
step:1233/2035 train_time:56279ms step_avg:45.64ms
step:1234/2035 train_time:56340ms step_avg:45.66ms
step:1235/2035 train_time:56400ms step_avg:45.67ms
step:1236/2035 train_time:56460ms step_avg:45.68ms
step:1237/2035 train_time:56521ms step_avg:45.69ms
step:1238/2035 train_time:56581ms step_avg:45.70ms
step:1239/2035 train_time:56642ms step_avg:45.72ms
step:1240/2035 train_time:56701ms step_avg:45.73ms
step:1241/2035 train_time:56762ms step_avg:45.74ms
step:1242/2035 train_time:56822ms step_avg:45.75ms
step:1243/2035 train_time:56882ms step_avg:45.76ms
step:1244/2035 train_time:56942ms step_avg:45.77ms
step:1245/2035 train_time:57003ms step_avg:45.79ms
step:1246/2035 train_time:57063ms step_avg:45.80ms
step:1247/2035 train_time:57124ms step_avg:45.81ms
step:1248/2035 train_time:57184ms step_avg:45.82ms
step:1249/2035 train_time:57245ms step_avg:45.83ms
step:1250/2035 train_time:57305ms step_avg:45.84ms
step:1250/2035 val_loss:3.5708 train_time:57367ms step_avg:45.89ms
step:1251/2035 train_time:57388ms step_avg:45.87ms
step:1252/2035 train_time:57427ms step_avg:45.87ms
step:1253/2035 train_time:57491ms step_avg:45.88ms
step:1254/2035 train_time:57552ms step_avg:45.89ms
step:1255/2035 train_time:57613ms step_avg:45.91ms
step:1256/2035 train_time:57674ms step_avg:45.92ms
step:1257/2035 train_time:57733ms step_avg:45.93ms
step:1258/2035 train_time:57793ms step_avg:45.94ms
step:1259/2035 train_time:57852ms step_avg:45.95ms
step:1260/2035 train_time:57911ms step_avg:45.96ms
step:1261/2035 train_time:57971ms step_avg:45.97ms
step:1262/2035 train_time:58031ms step_avg:45.98ms
step:1263/2035 train_time:58091ms step_avg:45.99ms
step:1264/2035 train_time:58151ms step_avg:46.01ms
step:1265/2035 train_time:58211ms step_avg:46.02ms
step:1266/2035 train_time:58271ms step_avg:46.03ms
step:1267/2035 train_time:58332ms step_avg:46.04ms
step:1268/2035 train_time:58393ms step_avg:46.05ms
step:1269/2035 train_time:58454ms step_avg:46.06ms
step:1270/2035 train_time:58515ms step_avg:46.07ms
step:1271/2035 train_time:58576ms step_avg:46.09ms
step:1272/2035 train_time:58635ms step_avg:46.10ms
step:1273/2035 train_time:58695ms step_avg:46.11ms
step:1274/2035 train_time:58755ms step_avg:46.12ms
step:1275/2035 train_time:58815ms step_avg:46.13ms
step:1276/2035 train_time:58874ms step_avg:46.14ms
step:1277/2035 train_time:58934ms step_avg:46.15ms
step:1278/2035 train_time:58993ms step_avg:46.16ms
step:1279/2035 train_time:59053ms step_avg:46.17ms
step:1280/2035 train_time:59113ms step_avg:46.18ms
step:1281/2035 train_time:59173ms step_avg:46.19ms
step:1282/2035 train_time:59233ms step_avg:46.20ms
step:1283/2035 train_time:59294ms step_avg:46.21ms
step:1284/2035 train_time:59354ms step_avg:46.23ms
step:1285/2035 train_time:59415ms step_avg:46.24ms
step:1286/2035 train_time:59476ms step_avg:46.25ms
step:1287/2035 train_time:59537ms step_avg:46.26ms
step:1288/2035 train_time:59596ms step_avg:46.27ms
step:1289/2035 train_time:59656ms step_avg:46.28ms
step:1290/2035 train_time:59716ms step_avg:46.29ms
step:1291/2035 train_time:59776ms step_avg:46.30ms
step:1292/2035 train_time:59835ms step_avg:46.31ms
step:1293/2035 train_time:59895ms step_avg:46.32ms
step:1294/2035 train_time:59954ms step_avg:46.33ms
step:1295/2035 train_time:60015ms step_avg:46.34ms
step:1296/2035 train_time:60074ms step_avg:46.35ms
step:1297/2035 train_time:60135ms step_avg:46.37ms
step:1298/2035 train_time:60195ms step_avg:46.38ms
step:1299/2035 train_time:60256ms step_avg:46.39ms
step:1300/2035 train_time:60316ms step_avg:46.40ms
step:1301/2035 train_time:60376ms step_avg:46.41ms
step:1302/2035 train_time:60436ms step_avg:46.42ms
step:1303/2035 train_time:60497ms step_avg:46.43ms
step:1304/2035 train_time:60557ms step_avg:46.44ms
step:1305/2035 train_time:60618ms step_avg:46.45ms
step:1306/2035 train_time:60677ms step_avg:46.46ms
step:1307/2035 train_time:60738ms step_avg:46.47ms
step:1308/2035 train_time:60797ms step_avg:46.48ms
step:1309/2035 train_time:60858ms step_avg:46.49ms
step:1310/2035 train_time:60916ms step_avg:46.50ms
step:1311/2035 train_time:60977ms step_avg:46.51ms
step:1312/2035 train_time:61036ms step_avg:46.52ms
step:1313/2035 train_time:61097ms step_avg:46.53ms
step:1314/2035 train_time:61156ms step_avg:46.54ms
step:1315/2035 train_time:61218ms step_avg:46.55ms
step:1316/2035 train_time:61277ms step_avg:46.56ms
step:1317/2035 train_time:61338ms step_avg:46.57ms
step:1318/2035 train_time:61398ms step_avg:46.58ms
step:1319/2035 train_time:61458ms step_avg:46.59ms
step:1320/2035 train_time:61518ms step_avg:46.60ms
step:1321/2035 train_time:61579ms step_avg:46.62ms
step:1322/2035 train_time:61638ms step_avg:46.62ms
step:1323/2035 train_time:61698ms step_avg:46.64ms
step:1324/2035 train_time:61758ms step_avg:46.64ms
step:1325/2035 train_time:61818ms step_avg:46.65ms
step:1326/2035 train_time:61876ms step_avg:46.66ms
step:1327/2035 train_time:61937ms step_avg:46.67ms
step:1328/2035 train_time:61997ms step_avg:46.68ms
step:1329/2035 train_time:62057ms step_avg:46.69ms
step:1330/2035 train_time:62116ms step_avg:46.70ms
step:1331/2035 train_time:62178ms step_avg:46.71ms
step:1332/2035 train_time:62265ms step_avg:46.75ms
step:1333/2035 train_time:62353ms step_avg:46.78ms
step:1334/2035 train_time:62441ms step_avg:46.81ms
step:1335/2035 train_time:62530ms step_avg:46.84ms
step:1336/2035 train_time:62618ms step_avg:46.87ms
step:1337/2035 train_time:62706ms step_avg:46.90ms
step:1338/2035 train_time:62794ms step_avg:46.93ms
step:1339/2035 train_time:62882ms step_avg:46.96ms
step:1340/2035 train_time:62969ms step_avg:46.99ms
step:1341/2035 train_time:63057ms step_avg:47.02ms
step:1342/2035 train_time:63145ms step_avg:47.05ms
step:1343/2035 train_time:63232ms step_avg:47.08ms
step:1344/2035 train_time:63319ms step_avg:47.11ms
step:1345/2035 train_time:63408ms step_avg:47.14ms
step:1346/2035 train_time:63496ms step_avg:47.17ms
step:1347/2035 train_time:63585ms step_avg:47.20ms
step:1348/2035 train_time:63672ms step_avg:47.23ms
step:1349/2035 train_time:63760ms step_avg:47.26ms
step:1350/2035 train_time:63849ms step_avg:47.30ms
step:1351/2035 train_time:63937ms step_avg:47.33ms
step:1352/2035 train_time:64025ms step_avg:47.36ms
step:1353/2035 train_time:64113ms step_avg:47.39ms
step:1354/2035 train_time:64200ms step_avg:47.42ms
step:1355/2035 train_time:64288ms step_avg:47.45ms
step:1356/2035 train_time:64376ms step_avg:47.48ms
step:1357/2035 train_time:64464ms step_avg:47.50ms
step:1358/2035 train_time:64552ms step_avg:47.53ms
step:1359/2035 train_time:64641ms step_avg:47.57ms
step:1360/2035 train_time:64728ms step_avg:47.59ms
step:1361/2035 train_time:64816ms step_avg:47.62ms
step:1362/2035 train_time:64904ms step_avg:47.65ms
step:1363/2035 train_time:64992ms step_avg:47.68ms
step:1364/2035 train_time:65079ms step_avg:47.71ms
step:1365/2035 train_time:65166ms step_avg:47.74ms
step:1366/2035 train_time:65254ms step_avg:47.77ms
step:1367/2035 train_time:65343ms step_avg:47.80ms
step:1368/2035 train_time:65430ms step_avg:47.83ms
step:1369/2035 train_time:65519ms step_avg:47.86ms
step:1370/2035 train_time:65607ms step_avg:47.89ms
step:1371/2035 train_time:65694ms step_avg:47.92ms
step:1372/2035 train_time:65782ms step_avg:47.95ms
step:1373/2035 train_time:65870ms step_avg:47.98ms
step:1374/2035 train_time:65957ms step_avg:48.00ms
step:1375/2035 train_time:66047ms step_avg:48.03ms
step:1376/2035 train_time:66135ms step_avg:48.06ms
step:1377/2035 train_time:66223ms step_avg:48.09ms
step:1378/2035 train_time:66310ms step_avg:48.12ms
step:1379/2035 train_time:66399ms step_avg:48.15ms
step:1380/2035 train_time:66487ms step_avg:48.18ms
step:1381/2035 train_time:66575ms step_avg:48.21ms
step:1382/2035 train_time:66662ms step_avg:48.24ms
step:1383/2035 train_time:66750ms step_avg:48.26ms
step:1384/2035 train_time:66838ms step_avg:48.29ms
step:1385/2035 train_time:66926ms step_avg:48.32ms
step:1386/2035 train_time:67014ms step_avg:48.35ms
step:1387/2035 train_time:67103ms step_avg:48.38ms
step:1388/2035 train_time:67189ms step_avg:48.41ms
step:1389/2035 train_time:67278ms step_avg:48.44ms
step:1390/2035 train_time:67366ms step_avg:48.46ms
step:1391/2035 train_time:67454ms step_avg:48.49ms
step:1392/2035 train_time:67542ms step_avg:48.52ms
step:1393/2035 train_time:67631ms step_avg:48.55ms
step:1394/2035 train_time:67719ms step_avg:48.58ms
step:1395/2035 train_time:67807ms step_avg:48.61ms
step:1396/2035 train_time:67895ms step_avg:48.64ms
step:1397/2035 train_time:67984ms step_avg:48.66ms
step:1398/2035 train_time:68072ms step_avg:48.69ms
step:1399/2035 train_time:68160ms step_avg:48.72ms
step:1400/2035 train_time:68247ms step_avg:48.75ms
step:1401/2035 train_time:68336ms step_avg:48.78ms
step:1402/2035 train_time:68424ms step_avg:48.80ms
step:1403/2035 train_time:68513ms step_avg:48.83ms
step:1404/2035 train_time:68600ms step_avg:48.86ms
step:1405/2035 train_time:68688ms step_avg:48.89ms
step:1406/2035 train_time:68775ms step_avg:48.92ms
step:1407/2035 train_time:68864ms step_avg:48.94ms
step:1408/2035 train_time:68950ms step_avg:48.97ms
step:1409/2035 train_time:69040ms step_avg:49.00ms
step:1410/2035 train_time:69129ms step_avg:49.03ms
step:1411/2035 train_time:69216ms step_avg:49.05ms
step:1412/2035 train_time:69304ms step_avg:49.08ms
step:1413/2035 train_time:69392ms step_avg:49.11ms
step:1414/2035 train_time:69481ms step_avg:49.14ms
step:1415/2035 train_time:69570ms step_avg:49.17ms
step:1416/2035 train_time:69657ms step_avg:49.19ms
step:1417/2035 train_time:69746ms step_avg:49.22ms
step:1418/2035 train_time:69833ms step_avg:49.25ms
step:1419/2035 train_time:69922ms step_avg:49.28ms
step:1420/2035 train_time:70009ms step_avg:49.30ms
step:1421/2035 train_time:70098ms step_avg:49.33ms
step:1422/2035 train_time:70186ms step_avg:49.36ms
step:1423/2035 train_time:70274ms step_avg:49.38ms
step:1424/2035 train_time:70361ms step_avg:49.41ms
step:1425/2035 train_time:70449ms step_avg:49.44ms
step:1426/2035 train_time:70536ms step_avg:49.46ms
step:1427/2035 train_time:70624ms step_avg:49.49ms
step:1428/2035 train_time:70712ms step_avg:49.52ms
step:1429/2035 train_time:70800ms step_avg:49.55ms
step:1430/2035 train_time:70887ms step_avg:49.57ms
step:1431/2035 train_time:70977ms step_avg:49.60ms
step:1432/2035 train_time:71065ms step_avg:49.63ms
step:1433/2035 train_time:71153ms step_avg:49.65ms
step:1434/2035 train_time:71240ms step_avg:49.68ms
step:1435/2035 train_time:71328ms step_avg:49.71ms
step:1436/2035 train_time:71415ms step_avg:49.73ms
step:1437/2035 train_time:71503ms step_avg:49.76ms
step:1438/2035 train_time:71591ms step_avg:49.78ms
step:1439/2035 train_time:71680ms step_avg:49.81ms
step:1440/2035 train_time:71768ms step_avg:49.84ms
step:1441/2035 train_time:71856ms step_avg:49.87ms
step:1442/2035 train_time:71944ms step_avg:49.89ms
step:1443/2035 train_time:72033ms step_avg:49.92ms
step:1444/2035 train_time:72121ms step_avg:49.95ms
step:1445/2035 train_time:72209ms step_avg:49.97ms
step:1446/2035 train_time:72297ms step_avg:50.00ms
step:1447/2035 train_time:72386ms step_avg:50.02ms
step:1448/2035 train_time:72473ms step_avg:50.05ms
step:1449/2035 train_time:72561ms step_avg:50.08ms
step:1450/2035 train_time:72649ms step_avg:50.10ms
step:1451/2035 train_time:72737ms step_avg:50.13ms
step:1452/2035 train_time:72825ms step_avg:50.16ms
step:1453/2035 train_time:72913ms step_avg:50.18ms
step:1454/2035 train_time:73001ms step_avg:50.21ms
step:1455/2035 train_time:73089ms step_avg:50.23ms
step:1456/2035 train_time:73177ms step_avg:50.26ms
step:1457/2035 train_time:73266ms step_avg:50.29ms
step:1458/2035 train_time:73353ms step_avg:50.31ms
step:1459/2035 train_time:73441ms step_avg:50.34ms
step:1460/2035 train_time:73528ms step_avg:50.36ms
step:1461/2035 train_time:73617ms step_avg:50.39ms
step:1462/2035 train_time:73704ms step_avg:50.41ms
step:1463/2035 train_time:73792ms step_avg:50.44ms
step:1464/2035 train_time:73879ms step_avg:50.46ms
step:1465/2035 train_time:73967ms step_avg:50.49ms
step:1466/2035 train_time:74055ms step_avg:50.51ms
step:1467/2035 train_time:74143ms step_avg:50.54ms
step:1468/2035 train_time:74231ms step_avg:50.57ms
step:1469/2035 train_time:74319ms step_avg:50.59ms
step:1470/2035 train_time:74408ms step_avg:50.62ms
step:1471/2035 train_time:74495ms step_avg:50.64ms
step:1472/2035 train_time:74582ms step_avg:50.67ms
step:1473/2035 train_time:74671ms step_avg:50.69ms
step:1474/2035 train_time:74759ms step_avg:50.72ms
step:1475/2035 train_time:74847ms step_avg:50.74ms
step:1476/2035 train_time:74935ms step_avg:50.77ms
step:1477/2035 train_time:75023ms step_avg:50.79ms
step:1478/2035 train_time:75111ms step_avg:50.82ms
step:1479/2035 train_time:75198ms step_avg:50.84ms
step:1480/2035 train_time:75285ms step_avg:50.87ms
step:1481/2035 train_time:75373ms step_avg:50.89ms
step:1482/2035 train_time:75461ms step_avg:50.92ms
step:1483/2035 train_time:75549ms step_avg:50.94ms
step:1484/2035 train_time:75637ms step_avg:50.97ms
step:1485/2035 train_time:75725ms step_avg:50.99ms
step:1486/2035 train_time:75812ms step_avg:51.02ms
step:1487/2035 train_time:75901ms step_avg:51.04ms
step:1488/2035 train_time:75988ms step_avg:51.07ms
step:1489/2035 train_time:76077ms step_avg:51.09ms
step:1490/2035 train_time:76167ms step_avg:51.12ms
step:1491/2035 train_time:76254ms step_avg:51.14ms
step:1492/2035 train_time:76341ms step_avg:51.17ms
step:1493/2035 train_time:76429ms step_avg:51.19ms
step:1494/2035 train_time:76517ms step_avg:51.22ms
step:1495/2035 train_time:76605ms step_avg:51.24ms
step:1496/2035 train_time:76693ms step_avg:51.27ms
step:1497/2035 train_time:76781ms step_avg:51.29ms
step:1498/2035 train_time:76868ms step_avg:51.31ms
step:1499/2035 train_time:76955ms step_avg:51.34ms
step:1500/2035 train_time:77043ms step_avg:51.36ms
step:1500/2035 val_loss:3.4553 train_time:77133ms step_avg:51.42ms
step:1501/2035 train_time:77153ms step_avg:51.40ms
step:1502/2035 train_time:77222ms step_avg:51.41ms
step:1503/2035 train_time:77315ms step_avg:51.44ms
step:1504/2035 train_time:77403ms step_avg:51.46ms
step:1505/2035 train_time:77491ms step_avg:51.49ms
step:1506/2035 train_time:77577ms step_avg:51.51ms
step:1507/2035 train_time:77665ms step_avg:51.54ms
step:1508/2035 train_time:77752ms step_avg:51.56ms
step:1509/2035 train_time:77839ms step_avg:51.58ms
step:1510/2035 train_time:77927ms step_avg:51.61ms
step:1511/2035 train_time:78015ms step_avg:51.63ms
step:1512/2035 train_time:78103ms step_avg:51.66ms
step:1513/2035 train_time:78194ms step_avg:51.68ms
step:1514/2035 train_time:78283ms step_avg:51.71ms
step:1515/2035 train_time:78373ms step_avg:51.73ms
step:1516/2035 train_time:78461ms step_avg:51.76ms
step:1517/2035 train_time:78549ms step_avg:51.78ms
step:1518/2035 train_time:78636ms step_avg:51.80ms
step:1519/2035 train_time:78723ms step_avg:51.83ms
step:1520/2035 train_time:78809ms step_avg:51.85ms
step:1521/2035 train_time:78897ms step_avg:51.87ms
step:1522/2035 train_time:78986ms step_avg:51.90ms
step:1523/2035 train_time:79075ms step_avg:51.92ms
step:1524/2035 train_time:79163ms step_avg:51.94ms
step:1525/2035 train_time:79252ms step_avg:51.97ms
step:1526/2035 train_time:79340ms step_avg:51.99ms
step:1527/2035 train_time:79429ms step_avg:52.02ms
step:1528/2035 train_time:79517ms step_avg:52.04ms
step:1529/2035 train_time:79606ms step_avg:52.06ms
step:1530/2035 train_time:79693ms step_avg:52.09ms
step:1531/2035 train_time:79781ms step_avg:52.11ms
step:1532/2035 train_time:79868ms step_avg:52.13ms
step:1533/2035 train_time:79956ms step_avg:52.16ms
step:1534/2035 train_time:80045ms step_avg:52.18ms
step:1535/2035 train_time:80133ms step_avg:52.20ms
step:1536/2035 train_time:80220ms step_avg:52.23ms
step:1537/2035 train_time:80310ms step_avg:52.25ms
step:1538/2035 train_time:80399ms step_avg:52.27ms
step:1539/2035 train_time:80488ms step_avg:52.30ms
step:1540/2035 train_time:80576ms step_avg:52.32ms
step:1541/2035 train_time:80664ms step_avg:52.35ms
step:1542/2035 train_time:80751ms step_avg:52.37ms
step:1543/2035 train_time:80839ms step_avg:52.39ms
step:1544/2035 train_time:80926ms step_avg:52.41ms
step:1545/2035 train_time:81014ms step_avg:52.44ms
step:1546/2035 train_time:81102ms step_avg:52.46ms
step:1547/2035 train_time:81190ms step_avg:52.48ms
step:1548/2035 train_time:81279ms step_avg:52.51ms
step:1549/2035 train_time:81368ms step_avg:52.53ms
step:1550/2035 train_time:81456ms step_avg:52.55ms
step:1551/2035 train_time:81546ms step_avg:52.58ms
step:1552/2035 train_time:81633ms step_avg:52.60ms
step:1553/2035 train_time:81720ms step_avg:52.62ms
step:1554/2035 train_time:81807ms step_avg:52.64ms
step:1555/2035 train_time:81894ms step_avg:52.67ms
step:1556/2035 train_time:81982ms step_avg:52.69ms
step:1557/2035 train_time:82071ms step_avg:52.71ms
step:1558/2035 train_time:82158ms step_avg:52.73ms
step:1559/2035 train_time:82248ms step_avg:52.76ms
step:1560/2035 train_time:82337ms step_avg:52.78ms
step:1561/2035 train_time:82426ms step_avg:52.80ms
step:1562/2035 train_time:82515ms step_avg:52.83ms
step:1563/2035 train_time:82603ms step_avg:52.85ms
step:1564/2035 train_time:82690ms step_avg:52.87ms
step:1565/2035 train_time:82777ms step_avg:52.89ms
step:1566/2035 train_time:82866ms step_avg:52.92ms
step:1567/2035 train_time:82954ms step_avg:52.94ms
step:1568/2035 train_time:83041ms step_avg:52.96ms
step:1569/2035 train_time:83129ms step_avg:52.98ms
step:1570/2035 train_time:83218ms step_avg:53.00ms
step:1571/2035 train_time:83307ms step_avg:53.03ms
step:1572/2035 train_time:83396ms step_avg:53.05ms
step:1573/2035 train_time:83483ms step_avg:53.07ms
step:1574/2035 train_time:83571ms step_avg:53.09ms
step:1575/2035 train_time:83659ms step_avg:53.12ms
step:1576/2035 train_time:83747ms step_avg:53.14ms
step:1577/2035 train_time:83836ms step_avg:53.16ms
step:1578/2035 train_time:83923ms step_avg:53.18ms
step:1579/2035 train_time:84011ms step_avg:53.21ms
step:1580/2035 train_time:84098ms step_avg:53.23ms
step:1581/2035 train_time:84188ms step_avg:53.25ms
step:1582/2035 train_time:84276ms step_avg:53.27ms
step:1583/2035 train_time:84365ms step_avg:53.29ms
step:1584/2035 train_time:84452ms step_avg:53.32ms
step:1585/2035 train_time:84540ms step_avg:53.34ms
step:1586/2035 train_time:84628ms step_avg:53.36ms
step:1587/2035 train_time:84716ms step_avg:53.38ms
step:1588/2035 train_time:84805ms step_avg:53.40ms
step:1589/2035 train_time:84892ms step_avg:53.42ms
step:1590/2035 train_time:84980ms step_avg:53.45ms
step:1591/2035 train_time:85069ms step_avg:53.47ms
step:1592/2035 train_time:85157ms step_avg:53.49ms
step:1593/2035 train_time:85245ms step_avg:53.51ms
step:1594/2035 train_time:85332ms step_avg:53.53ms
step:1595/2035 train_time:85420ms step_avg:53.56ms
step:1596/2035 train_time:85508ms step_avg:53.58ms
step:1597/2035 train_time:85596ms step_avg:53.60ms
step:1598/2035 train_time:85684ms step_avg:53.62ms
step:1599/2035 train_time:85772ms step_avg:53.64ms
step:1600/2035 train_time:85860ms step_avg:53.66ms
step:1601/2035 train_time:85949ms step_avg:53.68ms
step:1602/2035 train_time:86038ms step_avg:53.71ms
step:1603/2035 train_time:86127ms step_avg:53.73ms
step:1604/2035 train_time:86216ms step_avg:53.75ms
step:1605/2035 train_time:86305ms step_avg:53.77ms
step:1606/2035 train_time:86391ms step_avg:53.79ms
step:1607/2035 train_time:86480ms step_avg:53.81ms
step:1608/2035 train_time:86567ms step_avg:53.84ms
step:1609/2035 train_time:86655ms step_avg:53.86ms
step:1610/2035 train_time:86743ms step_avg:53.88ms
step:1611/2035 train_time:86831ms step_avg:53.90ms
step:1612/2035 train_time:86920ms step_avg:53.92ms
step:1613/2035 train_time:87008ms step_avg:53.94ms
step:1614/2035 train_time:87096ms step_avg:53.96ms
step:1615/2035 train_time:87185ms step_avg:53.98ms
step:1616/2035 train_time:87273ms step_avg:54.01ms
step:1617/2035 train_time:87362ms step_avg:54.03ms
step:1618/2035 train_time:87449ms step_avg:54.05ms
step:1619/2035 train_time:87538ms step_avg:54.07ms
step:1620/2035 train_time:87626ms step_avg:54.09ms
step:1621/2035 train_time:87713ms step_avg:54.11ms
step:1622/2035 train_time:87801ms step_avg:54.13ms
step:1623/2035 train_time:87890ms step_avg:54.15ms
step:1624/2035 train_time:87977ms step_avg:54.17ms
step:1625/2035 train_time:88066ms step_avg:54.19ms
step:1626/2035 train_time:88154ms step_avg:54.22ms
step:1627/2035 train_time:88243ms step_avg:54.24ms
step:1628/2035 train_time:88330ms step_avg:54.26ms
step:1629/2035 train_time:88419ms step_avg:54.28ms
step:1630/2035 train_time:88506ms step_avg:54.30ms
step:1631/2035 train_time:88594ms step_avg:54.32ms
step:1632/2035 train_time:88681ms step_avg:54.34ms
step:1633/2035 train_time:88770ms step_avg:54.36ms
step:1634/2035 train_time:88857ms step_avg:54.38ms
step:1635/2035 train_time:88946ms step_avg:54.40ms
step:1636/2035 train_time:89034ms step_avg:54.42ms
step:1637/2035 train_time:89122ms step_avg:54.44ms
step:1638/2035 train_time:89210ms step_avg:54.46ms
step:1639/2035 train_time:89299ms step_avg:54.48ms
step:1640/2035 train_time:89388ms step_avg:54.50ms
step:1641/2035 train_time:89477ms step_avg:54.53ms
step:1642/2035 train_time:89564ms step_avg:54.55ms
step:1643/2035 train_time:89652ms step_avg:54.57ms
step:1644/2035 train_time:89740ms step_avg:54.59ms
step:1645/2035 train_time:89828ms step_avg:54.61ms
step:1646/2035 train_time:89916ms step_avg:54.63ms
step:1647/2035 train_time:90005ms step_avg:54.65ms
step:1648/2035 train_time:90092ms step_avg:54.67ms
step:1649/2035 train_time:90181ms step_avg:54.69ms
step:1650/2035 train_time:90269ms step_avg:54.71ms
step:1651/2035 train_time:90358ms step_avg:54.73ms
step:1652/2035 train_time:90446ms step_avg:54.75ms
step:1653/2035 train_time:90534ms step_avg:54.77ms
step:1654/2035 train_time:90623ms step_avg:54.79ms
step:1655/2035 train_time:90711ms step_avg:54.81ms
step:1656/2035 train_time:90799ms step_avg:54.83ms
step:1657/2035 train_time:90887ms step_avg:54.85ms
step:1658/2035 train_time:90976ms step_avg:54.87ms
step:1659/2035 train_time:91065ms step_avg:54.89ms
step:1660/2035 train_time:91152ms step_avg:54.91ms
step:1661/2035 train_time:91240ms step_avg:54.93ms
step:1662/2035 train_time:91327ms step_avg:54.95ms
step:1663/2035 train_time:91416ms step_avg:54.97ms
step:1664/2035 train_time:91505ms step_avg:54.99ms
step:1665/2035 train_time:91593ms step_avg:55.01ms
step:1666/2035 train_time:91680ms step_avg:55.03ms
step:1667/2035 train_time:91769ms step_avg:55.05ms
step:1668/2035 train_time:91857ms step_avg:55.07ms
step:1669/2035 train_time:91945ms step_avg:55.09ms
step:1670/2035 train_time:92032ms step_avg:55.11ms
step:1671/2035 train_time:92120ms step_avg:55.13ms
step:1672/2035 train_time:92208ms step_avg:55.15ms
step:1673/2035 train_time:92296ms step_avg:55.17ms
step:1674/2035 train_time:92385ms step_avg:55.19ms
step:1675/2035 train_time:92474ms step_avg:55.21ms
step:1676/2035 train_time:92562ms step_avg:55.23ms
step:1677/2035 train_time:92651ms step_avg:55.25ms
step:1678/2035 train_time:92738ms step_avg:55.27ms
step:1679/2035 train_time:92827ms step_avg:55.29ms
step:1680/2035 train_time:92916ms step_avg:55.31ms
step:1681/2035 train_time:93005ms step_avg:55.33ms
step:1682/2035 train_time:93092ms step_avg:55.35ms
step:1683/2035 train_time:93181ms step_avg:55.37ms
step:1684/2035 train_time:93269ms step_avg:55.39ms
step:1685/2035 train_time:93357ms step_avg:55.40ms
step:1686/2035 train_time:93445ms step_avg:55.42ms
step:1687/2035 train_time:93532ms step_avg:55.44ms
step:1688/2035 train_time:93620ms step_avg:55.46ms
step:1689/2035 train_time:93708ms step_avg:55.48ms
step:1690/2035 train_time:93796ms step_avg:55.50ms
step:1691/2035 train_time:93885ms step_avg:55.52ms
step:1692/2035 train_time:93972ms step_avg:55.54ms
step:1693/2035 train_time:94060ms step_avg:55.56ms
step:1694/2035 train_time:94148ms step_avg:55.58ms
step:1695/2035 train_time:94236ms step_avg:55.60ms
step:1696/2035 train_time:94324ms step_avg:55.62ms
step:1697/2035 train_time:94412ms step_avg:55.63ms
step:1698/2035 train_time:94499ms step_avg:55.65ms
step:1699/2035 train_time:94588ms step_avg:55.67ms
step:1700/2035 train_time:94677ms step_avg:55.69ms
step:1701/2035 train_time:94766ms step_avg:55.71ms
step:1702/2035 train_time:94854ms step_avg:55.73ms
step:1703/2035 train_time:94942ms step_avg:55.75ms
step:1704/2035 train_time:95029ms step_avg:55.77ms
step:1705/2035 train_time:95118ms step_avg:55.79ms
step:1706/2035 train_time:95206ms step_avg:55.81ms
step:1707/2035 train_time:95295ms step_avg:55.83ms
step:1708/2035 train_time:95384ms step_avg:55.85ms
step:1709/2035 train_time:95472ms step_avg:55.86ms
step:1710/2035 train_time:95560ms step_avg:55.88ms
step:1711/2035 train_time:95648ms step_avg:55.90ms
step:1712/2035 train_time:95736ms step_avg:55.92ms
step:1713/2035 train_time:95825ms step_avg:55.94ms
step:1714/2035 train_time:95913ms step_avg:55.96ms
step:1715/2035 train_time:96001ms step_avg:55.98ms
step:1716/2035 train_time:96088ms step_avg:56.00ms
step:1717/2035 train_time:96177ms step_avg:56.01ms
step:1718/2035 train_time:96265ms step_avg:56.03ms
step:1719/2035 train_time:96353ms step_avg:56.05ms
step:1720/2035 train_time:96442ms step_avg:56.07ms
step:1721/2035 train_time:96529ms step_avg:56.09ms
step:1722/2035 train_time:96617ms step_avg:56.11ms
step:1723/2035 train_time:96706ms step_avg:56.13ms
step:1724/2035 train_time:96792ms step_avg:56.14ms
step:1725/2035 train_time:96881ms step_avg:56.16ms
step:1726/2035 train_time:96969ms step_avg:56.18ms
step:1727/2035 train_time:97057ms step_avg:56.20ms
step:1728/2035 train_time:97145ms step_avg:56.22ms
step:1729/2035 train_time:97233ms step_avg:56.24ms
step:1730/2035 train_time:97321ms step_avg:56.25ms
step:1731/2035 train_time:97409ms step_avg:56.27ms
step:1732/2035 train_time:97497ms step_avg:56.29ms
step:1733/2035 train_time:97588ms step_avg:56.31ms
step:1734/2035 train_time:97675ms step_avg:56.33ms
step:1735/2035 train_time:97763ms step_avg:56.35ms
step:1736/2035 train_time:97852ms step_avg:56.37ms
step:1737/2035 train_time:97941ms step_avg:56.38ms
step:1738/2035 train_time:98028ms step_avg:56.40ms
step:1739/2035 train_time:98117ms step_avg:56.42ms
step:1740/2035 train_time:98204ms step_avg:56.44ms
step:1741/2035 train_time:98292ms step_avg:56.46ms
step:1742/2035 train_time:98380ms step_avg:56.48ms
step:1743/2035 train_time:98468ms step_avg:56.49ms
step:1744/2035 train_time:98556ms step_avg:56.51ms
step:1745/2035 train_time:98645ms step_avg:56.53ms
step:1746/2035 train_time:98732ms step_avg:56.55ms
step:1747/2035 train_time:98820ms step_avg:56.57ms
step:1748/2035 train_time:98908ms step_avg:56.58ms
step:1749/2035 train_time:98996ms step_avg:56.60ms
step:1750/2035 train_time:99085ms step_avg:56.62ms
step:1750/2035 val_loss:3.3598 train_time:99175ms step_avg:56.67ms
step:1751/2035 train_time:99195ms step_avg:56.65ms
step:1752/2035 train_time:99265ms step_avg:56.66ms
step:1753/2035 train_time:99357ms step_avg:56.68ms
step:1754/2035 train_time:99446ms step_avg:56.70ms
step:1755/2035 train_time:99534ms step_avg:56.71ms
step:1756/2035 train_time:99621ms step_avg:56.73ms
step:1757/2035 train_time:99708ms step_avg:56.75ms
step:1758/2035 train_time:99795ms step_avg:56.77ms
step:1759/2035 train_time:99882ms step_avg:56.78ms
step:1760/2035 train_time:99968ms step_avg:56.80ms
step:1761/2035 train_time:100055ms step_avg:56.82ms
step:1762/2035 train_time:100144ms step_avg:56.84ms
step:1763/2035 train_time:100234ms step_avg:56.85ms
step:1764/2035 train_time:100325ms step_avg:56.87ms
step:1765/2035 train_time:100414ms step_avg:56.89ms
step:1766/2035 train_time:100502ms step_avg:56.91ms
step:1767/2035 train_time:100591ms step_avg:56.93ms
step:1768/2035 train_time:100678ms step_avg:56.94ms
step:1769/2035 train_time:100766ms step_avg:56.96ms
step:1770/2035 train_time:100852ms step_avg:56.98ms
step:1771/2035 train_time:100939ms step_avg:57.00ms
step:1772/2035 train_time:101027ms step_avg:57.01ms
step:1773/2035 train_time:101115ms step_avg:57.03ms
step:1774/2035 train_time:101204ms step_avg:57.05ms
step:1775/2035 train_time:101293ms step_avg:57.07ms
step:1776/2035 train_time:101383ms step_avg:57.09ms
step:1777/2035 train_time:101471ms step_avg:57.10ms
step:1778/2035 train_time:101559ms step_avg:57.12ms
step:1779/2035 train_time:101647ms step_avg:57.14ms
step:1780/2035 train_time:101734ms step_avg:57.15ms
step:1781/2035 train_time:101822ms step_avg:57.17ms
step:1782/2035 train_time:101909ms step_avg:57.19ms
step:1783/2035 train_time:101996ms step_avg:57.20ms
step:1784/2035 train_time:102083ms step_avg:57.22ms
step:1785/2035 train_time:102171ms step_avg:57.24ms
step:1786/2035 train_time:102260ms step_avg:57.26ms
step:1787/2035 train_time:102350ms step_avg:57.27ms
step:1788/2035 train_time:102439ms step_avg:57.29ms
step:1789/2035 train_time:102528ms step_avg:57.31ms
step:1790/2035 train_time:102616ms step_avg:57.33ms
step:1791/2035 train_time:102705ms step_avg:57.34ms
step:1792/2035 train_time:102792ms step_avg:57.36ms
step:1793/2035 train_time:102879ms step_avg:57.38ms
step:1794/2035 train_time:102967ms step_avg:57.39ms
step:1795/2035 train_time:103055ms step_avg:57.41ms
step:1796/2035 train_time:103142ms step_avg:57.43ms
step:1797/2035 train_time:103232ms step_avg:57.45ms
step:1798/2035 train_time:103320ms step_avg:57.46ms
step:1799/2035 train_time:103409ms step_avg:57.48ms
step:1800/2035 train_time:103497ms step_avg:57.50ms
step:1801/2035 train_time:103585ms step_avg:57.52ms
step:1802/2035 train_time:103672ms step_avg:57.53ms
step:1803/2035 train_time:103760ms step_avg:57.55ms
step:1804/2035 train_time:103848ms step_avg:57.57ms
step:1805/2035 train_time:103936ms step_avg:57.58ms
step:1806/2035 train_time:104024ms step_avg:57.60ms
step:1807/2035 train_time:104112ms step_avg:57.62ms
step:1808/2035 train_time:104201ms step_avg:57.63ms
step:1809/2035 train_time:104290ms step_avg:57.65ms
step:1810/2035 train_time:104378ms step_avg:57.67ms
step:1811/2035 train_time:104467ms step_avg:57.68ms
step:1812/2035 train_time:104554ms step_avg:57.70ms
step:1813/2035 train_time:104643ms step_avg:57.72ms
step:1814/2035 train_time:104730ms step_avg:57.73ms
step:1815/2035 train_time:104818ms step_avg:57.75ms
step:1816/2035 train_time:104905ms step_avg:57.77ms
step:1817/2035 train_time:104993ms step_avg:57.78ms
step:1818/2035 train_time:105081ms step_avg:57.80ms
step:1819/2035 train_time:105170ms step_avg:57.82ms
step:1820/2035 train_time:105258ms step_avg:57.83ms
step:1821/2035 train_time:105347ms step_avg:57.85ms
step:1822/2035 train_time:105434ms step_avg:57.87ms
step:1823/2035 train_time:105523ms step_avg:57.88ms
step:1824/2035 train_time:105610ms step_avg:57.90ms
step:1825/2035 train_time:105699ms step_avg:57.92ms
step:1826/2035 train_time:105787ms step_avg:57.93ms
step:1827/2035 train_time:105875ms step_avg:57.95ms
step:1828/2035 train_time:105962ms step_avg:57.97ms
step:1829/2035 train_time:106050ms step_avg:57.98ms
step:1830/2035 train_time:106138ms step_avg:58.00ms
step:1831/2035 train_time:106228ms step_avg:58.02ms
step:1832/2035 train_time:106316ms step_avg:58.03ms
step:1833/2035 train_time:106404ms step_avg:58.05ms
step:1834/2035 train_time:106492ms step_avg:58.07ms
step:1835/2035 train_time:106580ms step_avg:58.08ms
step:1836/2035 train_time:106667ms step_avg:58.10ms
step:1837/2035 train_time:106756ms step_avg:58.11ms
step:1838/2035 train_time:106844ms step_avg:58.13ms
step:1839/2035 train_time:106932ms step_avg:58.15ms
step:1840/2035 train_time:107019ms step_avg:58.16ms
step:1841/2035 train_time:107108ms step_avg:58.18ms
step:1842/2035 train_time:107195ms step_avg:58.20ms
step:1843/2035 train_time:107284ms step_avg:58.21ms
step:1844/2035 train_time:107372ms step_avg:58.23ms
step:1845/2035 train_time:107461ms step_avg:58.24ms
step:1846/2035 train_time:107549ms step_avg:58.26ms
step:1847/2035 train_time:107637ms step_avg:58.28ms
step:1848/2035 train_time:107725ms step_avg:58.29ms
step:1849/2035 train_time:107813ms step_avg:58.31ms
step:1850/2035 train_time:107901ms step_avg:58.33ms
step:1851/2035 train_time:107990ms step_avg:58.34ms
step:1852/2035 train_time:108078ms step_avg:58.36ms
step:1853/2035 train_time:108165ms step_avg:58.37ms
step:1854/2035 train_time:108253ms step_avg:58.39ms
step:1855/2035 train_time:108341ms step_avg:58.40ms
step:1856/2035 train_time:108429ms step_avg:58.42ms
step:1857/2035 train_time:108518ms step_avg:58.44ms
step:1858/2035 train_time:108605ms step_avg:58.45ms
step:1859/2035 train_time:108694ms step_avg:58.47ms
step:1860/2035 train_time:108782ms step_avg:58.48ms
step:1861/2035 train_time:108869ms step_avg:58.50ms
step:1862/2035 train_time:108958ms step_avg:58.52ms
step:1863/2035 train_time:109046ms step_avg:58.53ms
step:1864/2035 train_time:109133ms step_avg:58.55ms
step:1865/2035 train_time:109223ms step_avg:58.56ms
step:1866/2035 train_time:109309ms step_avg:58.58ms
step:1867/2035 train_time:109397ms step_avg:58.60ms
step:1868/2035 train_time:109486ms step_avg:58.61ms
step:1869/2035 train_time:109573ms step_avg:58.63ms
step:1870/2035 train_time:109661ms step_avg:58.64ms
step:1871/2035 train_time:109750ms step_avg:58.66ms
step:1872/2035 train_time:109837ms step_avg:58.67ms
step:1873/2035 train_time:109926ms step_avg:58.69ms
step:1874/2035 train_time:110013ms step_avg:58.71ms
step:1875/2035 train_time:110103ms step_avg:58.72ms
step:1876/2035 train_time:110191ms step_avg:58.74ms
step:1877/2035 train_time:110278ms step_avg:58.75ms
step:1878/2035 train_time:110366ms step_avg:58.77ms
step:1879/2035 train_time:110454ms step_avg:58.78ms
step:1880/2035 train_time:110541ms step_avg:58.80ms
step:1881/2035 train_time:110630ms step_avg:58.81ms
step:1882/2035 train_time:110718ms step_avg:58.83ms
step:1883/2035 train_time:110807ms step_avg:58.85ms
step:1884/2035 train_time:110894ms step_avg:58.86ms
step:1885/2035 train_time:110983ms step_avg:58.88ms
step:1886/2035 train_time:111071ms step_avg:58.89ms
step:1887/2035 train_time:111159ms step_avg:58.91ms
step:1888/2035 train_time:111248ms step_avg:58.92ms
step:1889/2035 train_time:111335ms step_avg:58.94ms
step:1890/2035 train_time:111423ms step_avg:58.95ms
step:1891/2035 train_time:111511ms step_avg:58.97ms
step:1892/2035 train_time:111600ms step_avg:58.99ms
step:1893/2035 train_time:111689ms step_avg:59.00ms
step:1894/2035 train_time:111777ms step_avg:59.02ms
step:1895/2035 train_time:111864ms step_avg:59.03ms
step:1896/2035 train_time:111951ms step_avg:59.05ms
step:1897/2035 train_time:112039ms step_avg:59.06ms
step:1898/2035 train_time:112128ms step_avg:59.08ms
step:1899/2035 train_time:112217ms step_avg:59.09ms
step:1900/2035 train_time:112305ms step_avg:59.11ms
step:1901/2035 train_time:112393ms step_avg:59.12ms
step:1902/2035 train_time:112481ms step_avg:59.14ms
step:1903/2035 train_time:112570ms step_avg:59.15ms
step:1904/2035 train_time:112657ms step_avg:59.17ms
step:1905/2035 train_time:112746ms step_avg:59.18ms
step:1906/2035 train_time:112834ms step_avg:59.20ms
step:1907/2035 train_time:112922ms step_avg:59.21ms
step:1908/2035 train_time:113009ms step_avg:59.23ms
step:1909/2035 train_time:113098ms step_avg:59.24ms
step:1910/2035 train_time:113187ms step_avg:59.26ms
step:1911/2035 train_time:113275ms step_avg:59.28ms
step:1912/2035 train_time:113363ms step_avg:59.29ms
step:1913/2035 train_time:113450ms step_avg:59.30ms
step:1914/2035 train_time:113537ms step_avg:59.32ms
step:1915/2035 train_time:113626ms step_avg:59.33ms
step:1916/2035 train_time:113714ms step_avg:59.35ms
step:1917/2035 train_time:113802ms step_avg:59.36ms
step:1918/2035 train_time:113890ms step_avg:59.38ms
step:1919/2035 train_time:113978ms step_avg:59.39ms
step:1920/2035 train_time:114066ms step_avg:59.41ms
step:1921/2035 train_time:114154ms step_avg:59.42ms
step:1922/2035 train_time:114243ms step_avg:59.44ms
step:1923/2035 train_time:114331ms step_avg:59.45ms
step:1924/2035 train_time:114419ms step_avg:59.47ms
step:1925/2035 train_time:114507ms step_avg:59.48ms
step:1926/2035 train_time:114595ms step_avg:59.50ms
step:1927/2035 train_time:114683ms step_avg:59.51ms
step:1928/2035 train_time:114771ms step_avg:59.53ms
step:1929/2035 train_time:114859ms step_avg:59.54ms
step:1930/2035 train_time:114949ms step_avg:59.56ms
step:1931/2035 train_time:115038ms step_avg:59.57ms
step:1932/2035 train_time:115126ms step_avg:59.59ms
step:1933/2035 train_time:115213ms step_avg:59.60ms
step:1934/2035 train_time:115301ms step_avg:59.62ms
step:1935/2035 train_time:115390ms step_avg:59.63ms
step:1936/2035 train_time:115478ms step_avg:59.65ms
step:1937/2035 train_time:115566ms step_avg:59.66ms
step:1938/2035 train_time:115654ms step_avg:59.68ms
step:1939/2035 train_time:115742ms step_avg:59.69ms
step:1940/2035 train_time:115830ms step_avg:59.71ms
step:1941/2035 train_time:115919ms step_avg:59.72ms
step:1942/2035 train_time:116007ms step_avg:59.74ms
step:1943/2035 train_time:116095ms step_avg:59.75ms
step:1944/2035 train_time:116183ms step_avg:59.76ms
step:1945/2035 train_time:116270ms step_avg:59.78ms
step:1946/2035 train_time:116359ms step_avg:59.79ms
step:1947/2035 train_time:116448ms step_avg:59.81ms
step:1948/2035 train_time:116535ms step_avg:59.82ms
step:1949/2035 train_time:116623ms step_avg:59.84ms
step:1950/2035 train_time:116711ms step_avg:59.85ms
step:1951/2035 train_time:116799ms step_avg:59.87ms
step:1952/2035 train_time:116887ms step_avg:59.88ms
step:1953/2035 train_time:116974ms step_avg:59.89ms
step:1954/2035 train_time:117062ms step_avg:59.91ms
step:1955/2035 train_time:117151ms step_avg:59.92ms
step:1956/2035 train_time:117238ms step_avg:59.94ms
step:1957/2035 train_time:117326ms step_avg:59.95ms
step:1958/2035 train_time:117414ms step_avg:59.97ms
step:1959/2035 train_time:117501ms step_avg:59.98ms
step:1960/2035 train_time:117590ms step_avg:59.99ms
step:1961/2035 train_time:117678ms step_avg:60.01ms
step:1962/2035 train_time:117765ms step_avg:60.02ms
step:1963/2035 train_time:117854ms step_avg:60.04ms
step:1964/2035 train_time:117941ms step_avg:60.05ms
step:1965/2035 train_time:118029ms step_avg:60.07ms
step:1966/2035 train_time:118117ms step_avg:60.08ms
step:1967/2035 train_time:118206ms step_avg:60.09ms
step:1968/2035 train_time:118293ms step_avg:60.11ms
step:1969/2035 train_time:118382ms step_avg:60.12ms
step:1970/2035 train_time:118471ms step_avg:60.14ms
step:1971/2035 train_time:118559ms step_avg:60.15ms
step:1972/2035 train_time:118648ms step_avg:60.17ms
step:1973/2035 train_time:118735ms step_avg:60.18ms
step:1974/2035 train_time:118823ms step_avg:60.19ms
step:1975/2035 train_time:118911ms step_avg:60.21ms
step:1976/2035 train_time:118999ms step_avg:60.22ms
step:1977/2035 train_time:119088ms step_avg:60.24ms
step:1978/2035 train_time:119176ms step_avg:60.25ms
step:1979/2035 train_time:119264ms step_avg:60.26ms
step:1980/2035 train_time:119352ms step_avg:60.28ms
step:1981/2035 train_time:119440ms step_avg:60.29ms
step:1982/2035 train_time:119529ms step_avg:60.31ms
step:1983/2035 train_time:119618ms step_avg:60.32ms
step:1984/2035 train_time:119706ms step_avg:60.34ms
step:1985/2035 train_time:119795ms step_avg:60.35ms
step:1986/2035 train_time:119882ms step_avg:60.36ms
step:1987/2035 train_time:119971ms step_avg:60.38ms
step:1988/2035 train_time:120059ms step_avg:60.39ms
step:1989/2035 train_time:120147ms step_avg:60.41ms
step:1990/2035 train_time:120234ms step_avg:60.42ms
step:1991/2035 train_time:120323ms step_avg:60.43ms
step:1992/2035 train_time:120410ms step_avg:60.45ms
step:1993/2035 train_time:120499ms step_avg:60.46ms
step:1994/2035 train_time:120589ms step_avg:60.48ms
step:1995/2035 train_time:120677ms step_avg:60.49ms
step:1996/2035 train_time:120765ms step_avg:60.50ms
step:1997/2035 train_time:120853ms step_avg:60.52ms
step:1998/2035 train_time:120941ms step_avg:60.53ms
step:1999/2035 train_time:121030ms step_avg:60.55ms
step:2000/2035 train_time:121117ms step_avg:60.56ms
step:2000/2035 val_loss:3.2861 train_time:121208ms step_avg:60.60ms
step:2001/2035 train_time:121228ms step_avg:60.58ms
step:2002/2035 train_time:121296ms step_avg:60.59ms
step:2003/2035 train_time:121389ms step_avg:60.60ms
step:2004/2035 train_time:121480ms step_avg:60.62ms
step:2005/2035 train_time:121569ms step_avg:60.63ms
step:2006/2035 train_time:121658ms step_avg:60.65ms
step:2007/2035 train_time:121745ms step_avg:60.66ms
step:2008/2035 train_time:121832ms step_avg:60.67ms
step:2009/2035 train_time:121920ms step_avg:60.69ms
step:2010/2035 train_time:122007ms step_avg:60.70ms
step:2011/2035 train_time:122094ms step_avg:60.71ms
step:2012/2035 train_time:122183ms step_avg:60.73ms
step:2013/2035 train_time:122274ms step_avg:60.74ms
step:2014/2035 train_time:122364ms step_avg:60.76ms
step:2015/2035 train_time:122453ms step_avg:60.77ms
step:2016/2035 train_time:122542ms step_avg:60.78ms
step:2017/2035 train_time:122630ms step_avg:60.80ms
step:2018/2035 train_time:122718ms step_avg:60.81ms
step:2019/2035 train_time:122806ms step_avg:60.82ms
step:2020/2035 train_time:122892ms step_avg:60.84ms
step:2021/2035 train_time:122980ms step_avg:60.85ms
step:2022/2035 train_time:123067ms step_avg:60.86ms
step:2023/2035 train_time:123157ms step_avg:60.88ms
step:2024/2035 train_time:123246ms step_avg:60.89ms
step:2025/2035 train_time:123335ms step_avg:60.91ms
step:2026/2035 train_time:123424ms step_avg:60.92ms
step:2027/2035 train_time:123513ms step_avg:60.93ms
step:2028/2035 train_time:123601ms step_avg:60.95ms
step:2029/2035 train_time:123689ms step_avg:60.96ms
step:2030/2035 train_time:123776ms step_avg:60.97ms
step:2031/2035 train_time:123864ms step_avg:60.99ms
step:2032/2035 train_time:123951ms step_avg:61.00ms
step:2033/2035 train_time:124039ms step_avg:61.01ms
step:2034/2035 train_time:124128ms step_avg:61.03ms
step:2035/2035 train_time:124216ms step_avg:61.04ms
step:2035/2035 val_loss:3.2788 train_time:124305ms step_avg:61.08ms
peak memory allocated: 29475 MiB reserved: 43976 MiB

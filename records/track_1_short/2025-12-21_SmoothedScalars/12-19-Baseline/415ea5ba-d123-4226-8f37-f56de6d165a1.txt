import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    logs_dir: str = f"logs/12-19-Baseline"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 15:05:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    105542      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    105543      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    105544      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    105545      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    105546      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    105547      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    105548      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    105549      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    105543      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    105544      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    105545      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    105546      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    105547      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    105548      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    105549      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8355 train_time:0ms step_avg:0.04ms
step:1/2035 train_time:116ms step_avg:115.68ms
step:2/2035 train_time:142ms step_avg:70.90ms
step:3/2035 train_time:164ms step_avg:54.62ms
step:4/2035 train_time:186ms step_avg:46.58ms
step:5/2035 train_time:216ms step_avg:43.11ms
step:6/2035 train_time:316ms step_avg:52.74ms
step:7/2035 train_time:335ms step_avg:47.90ms
step:8/2035 train_time:361ms step_avg:45.15ms
step:9/2035 train_time:394ms step_avg:43.77ms
step:10/2035 train_time:427ms step_avg:42.72ms
step:11/2035 train_time:460ms step_avg:41.85ms
step:12/2035 train_time:494ms step_avg:41.15ms
step:13/2035 train_time:527ms step_avg:40.53ms
step:14/2035 train_time:560ms step_avg:40.00ms
step:15/2035 train_time:593ms step_avg:39.56ms
step:16/2035 train_time:627ms step_avg:39.17ms
step:17/2035 train_time:660ms step_avg:38.81ms
step:18/2035 train_time:693ms step_avg:38.50ms
step:19/2035 train_time:726ms step_avg:38.22ms
step:20/2035 train_time:759ms step_avg:37.97ms
step:21/2035 train_time:793ms step_avg:37.74ms
step:22/2035 train_time:826ms step_avg:37.54ms
step:23/2035 train_time:859ms step_avg:37.36ms
step:24/2035 train_time:893ms step_avg:37.19ms
step:25/2035 train_time:926ms step_avg:37.04ms
step:26/2035 train_time:959ms step_avg:36.89ms
step:27/2035 train_time:992ms step_avg:36.76ms
step:28/2035 train_time:1026ms step_avg:36.63ms
step:29/2035 train_time:1059ms step_avg:36.51ms
step:30/2035 train_time:1092ms step_avg:36.40ms
step:31/2035 train_time:1125ms step_avg:36.30ms
step:32/2035 train_time:1158ms step_avg:36.20ms
step:33/2035 train_time:1192ms step_avg:36.11ms
step:34/2035 train_time:1226ms step_avg:36.06ms
step:35/2035 train_time:1260ms step_avg:35.99ms
step:36/2035 train_time:1294ms step_avg:35.93ms
step:37/2035 train_time:1327ms step_avg:35.88ms
step:38/2035 train_time:1361ms step_avg:35.81ms
step:39/2035 train_time:1395ms step_avg:35.77ms
step:40/2035 train_time:1429ms step_avg:35.71ms
step:41/2035 train_time:1462ms step_avg:35.66ms
step:42/2035 train_time:1496ms step_avg:35.61ms
step:43/2035 train_time:1529ms step_avg:35.55ms
step:44/2035 train_time:1562ms step_avg:35.50ms
step:45/2035 train_time:1595ms step_avg:35.45ms
step:46/2035 train_time:1629ms step_avg:35.41ms
step:47/2035 train_time:1662ms step_avg:35.36ms
step:48/2035 train_time:1696ms step_avg:35.32ms
step:49/2035 train_time:1729ms step_avg:35.28ms
step:50/2035 train_time:1762ms step_avg:35.24ms
step:51/2035 train_time:1795ms step_avg:35.20ms
step:52/2035 train_time:1828ms step_avg:35.16ms
step:53/2035 train_time:1862ms step_avg:35.12ms
step:54/2035 train_time:1895ms step_avg:35.09ms
step:55/2035 train_time:1929ms step_avg:35.07ms
step:56/2035 train_time:1962ms step_avg:35.04ms
step:57/2035 train_time:1995ms step_avg:35.00ms
step:58/2035 train_time:2029ms step_avg:34.99ms
step:59/2035 train_time:2063ms step_avg:34.96ms
step:60/2035 train_time:2096ms step_avg:34.94ms
step:61/2035 train_time:2129ms step_avg:34.91ms
step:62/2035 train_time:2163ms step_avg:34.88ms
step:63/2035 train_time:2196ms step_avg:34.85ms
step:64/2035 train_time:2229ms step_avg:34.83ms
step:65/2035 train_time:2263ms step_avg:34.81ms
step:66/2035 train_time:2296ms step_avg:34.79ms
step:67/2035 train_time:2329ms step_avg:34.77ms
step:68/2035 train_time:2363ms step_avg:34.75ms
step:69/2035 train_time:2396ms step_avg:34.73ms
step:70/2035 train_time:2430ms step_avg:34.72ms
step:71/2035 train_time:2464ms step_avg:34.70ms
step:72/2035 train_time:2497ms step_avg:34.68ms
step:73/2035 train_time:2530ms step_avg:34.66ms
step:74/2035 train_time:2564ms step_avg:34.65ms
step:75/2035 train_time:2598ms step_avg:34.63ms
step:76/2035 train_time:2631ms step_avg:34.62ms
step:77/2035 train_time:2665ms step_avg:34.61ms
step:78/2035 train_time:2698ms step_avg:34.59ms
step:79/2035 train_time:2732ms step_avg:34.58ms
step:80/2035 train_time:2765ms step_avg:34.56ms
step:81/2035 train_time:2798ms step_avg:34.54ms
step:82/2035 train_time:2831ms step_avg:34.53ms
step:83/2035 train_time:2865ms step_avg:34.51ms
step:84/2035 train_time:2898ms step_avg:34.50ms
step:85/2035 train_time:2931ms step_avg:34.48ms
step:86/2035 train_time:2964ms step_avg:34.46ms
step:87/2035 train_time:2997ms step_avg:34.45ms
step:88/2035 train_time:3031ms step_avg:34.44ms
step:89/2035 train_time:3064ms step_avg:34.42ms
step:90/2035 train_time:3097ms step_avg:34.41ms
step:91/2035 train_time:3130ms step_avg:34.40ms
step:92/2035 train_time:3163ms step_avg:34.39ms
step:93/2035 train_time:3196ms step_avg:34.37ms
step:94/2035 train_time:3230ms step_avg:34.36ms
step:95/2035 train_time:3263ms step_avg:34.34ms
step:96/2035 train_time:3296ms step_avg:34.33ms
step:97/2035 train_time:3329ms step_avg:34.32ms
step:98/2035 train_time:3362ms step_avg:34.31ms
step:99/2035 train_time:3395ms step_avg:34.30ms
step:100/2035 train_time:3429ms step_avg:34.29ms
step:101/2035 train_time:3462ms step_avg:34.28ms
step:102/2035 train_time:3496ms step_avg:34.27ms
step:103/2035 train_time:3529ms step_avg:34.26ms
step:104/2035 train_time:3563ms step_avg:34.26ms
step:105/2035 train_time:3596ms step_avg:34.25ms
step:106/2035 train_time:3630ms step_avg:34.24ms
step:107/2035 train_time:3663ms step_avg:34.23ms
step:108/2035 train_time:3696ms step_avg:34.22ms
step:109/2035 train_time:3729ms step_avg:34.21ms
step:110/2035 train_time:3762ms step_avg:34.20ms
step:111/2035 train_time:3795ms step_avg:34.19ms
step:112/2035 train_time:3829ms step_avg:34.19ms
step:113/2035 train_time:3862ms step_avg:34.18ms
step:114/2035 train_time:3895ms step_avg:34.17ms
step:115/2035 train_time:3928ms step_avg:34.16ms
step:116/2035 train_time:3961ms step_avg:34.15ms
step:117/2035 train_time:3995ms step_avg:34.14ms
step:118/2035 train_time:4029ms step_avg:34.14ms
step:119/2035 train_time:4062ms step_avg:34.13ms
step:120/2035 train_time:4095ms step_avg:34.12ms
step:121/2035 train_time:4128ms step_avg:34.12ms
step:122/2035 train_time:4161ms step_avg:34.11ms
step:123/2035 train_time:4194ms step_avg:34.10ms
step:124/2035 train_time:4228ms step_avg:34.10ms
step:125/2035 train_time:4261ms step_avg:34.09ms
step:126/2035 train_time:4294ms step_avg:34.08ms
step:127/2035 train_time:4327ms step_avg:34.07ms
step:128/2035 train_time:4360ms step_avg:34.06ms
step:129/2035 train_time:4393ms step_avg:34.06ms
step:130/2035 train_time:4427ms step_avg:34.05ms
step:131/2035 train_time:4460ms step_avg:34.05ms
step:132/2035 train_time:4493ms step_avg:34.04ms
step:133/2035 train_time:4526ms step_avg:34.03ms
step:134/2035 train_time:4560ms step_avg:34.03ms
step:135/2035 train_time:4593ms step_avg:34.02ms
step:136/2035 train_time:4626ms step_avg:34.02ms
step:137/2035 train_time:4660ms step_avg:34.01ms
step:138/2035 train_time:4693ms step_avg:34.00ms
step:139/2035 train_time:4726ms step_avg:34.00ms
step:140/2035 train_time:4759ms step_avg:33.99ms
step:141/2035 train_time:4792ms step_avg:33.98ms
step:142/2035 train_time:4825ms step_avg:33.98ms
step:143/2035 train_time:4858ms step_avg:33.97ms
step:144/2035 train_time:4892ms step_avg:33.97ms
step:145/2035 train_time:4925ms step_avg:33.96ms
step:146/2035 train_time:4958ms step_avg:33.96ms
step:147/2035 train_time:4991ms step_avg:33.95ms
step:148/2035 train_time:5024ms step_avg:33.95ms
step:149/2035 train_time:5057ms step_avg:33.94ms
step:150/2035 train_time:5091ms step_avg:33.94ms
step:151/2035 train_time:5124ms step_avg:33.93ms
step:152/2035 train_time:5157ms step_avg:33.93ms
step:153/2035 train_time:5190ms step_avg:33.92ms
step:154/2035 train_time:5224ms step_avg:33.92ms
step:155/2035 train_time:5257ms step_avg:33.92ms
step:156/2035 train_time:5291ms step_avg:33.92ms
step:157/2035 train_time:5324ms step_avg:33.91ms
step:158/2035 train_time:5357ms step_avg:33.91ms
step:159/2035 train_time:5390ms step_avg:33.90ms
step:160/2035 train_time:5424ms step_avg:33.90ms
step:161/2035 train_time:5456ms step_avg:33.89ms
step:162/2035 train_time:5490ms step_avg:33.89ms
step:163/2035 train_time:5523ms step_avg:33.88ms
step:164/2035 train_time:5556ms step_avg:33.88ms
step:165/2035 train_time:5590ms step_avg:33.88ms
step:166/2035 train_time:5624ms step_avg:33.88ms
step:167/2035 train_time:5656ms step_avg:33.87ms
step:168/2035 train_time:5689ms step_avg:33.87ms
step:169/2035 train_time:5722ms step_avg:33.86ms
step:170/2035 train_time:5756ms step_avg:33.86ms
step:171/2035 train_time:5789ms step_avg:33.85ms
step:172/2035 train_time:5822ms step_avg:33.85ms
step:173/2035 train_time:5855ms step_avg:33.84ms
step:174/2035 train_time:5888ms step_avg:33.84ms
step:175/2035 train_time:5921ms step_avg:33.84ms
step:176/2035 train_time:5955ms step_avg:33.83ms
step:177/2035 train_time:5988ms step_avg:33.83ms
step:178/2035 train_time:6021ms step_avg:33.83ms
step:179/2035 train_time:6054ms step_avg:33.82ms
step:180/2035 train_time:6088ms step_avg:33.82ms
step:181/2035 train_time:6121ms step_avg:33.82ms
step:182/2035 train_time:6154ms step_avg:33.81ms
step:183/2035 train_time:6187ms step_avg:33.81ms
step:184/2035 train_time:6220ms step_avg:33.81ms
step:185/2035 train_time:6253ms step_avg:33.80ms
step:186/2035 train_time:6287ms step_avg:33.80ms
step:187/2035 train_time:6320ms step_avg:33.80ms
step:188/2035 train_time:6353ms step_avg:33.79ms
step:189/2035 train_time:6386ms step_avg:33.79ms
step:190/2035 train_time:6419ms step_avg:33.79ms
step:191/2035 train_time:6452ms step_avg:33.78ms
step:192/2035 train_time:6486ms step_avg:33.78ms
step:193/2035 train_time:6519ms step_avg:33.78ms
step:194/2035 train_time:6552ms step_avg:33.77ms
step:195/2035 train_time:6586ms step_avg:33.77ms
step:196/2035 train_time:6619ms step_avg:33.77ms
step:197/2035 train_time:6653ms step_avg:33.77ms
step:198/2035 train_time:6686ms step_avg:33.77ms
step:199/2035 train_time:6719ms step_avg:33.76ms
step:200/2035 train_time:6752ms step_avg:33.76ms
step:201/2035 train_time:6786ms step_avg:33.76ms
step:202/2035 train_time:6819ms step_avg:33.76ms
step:203/2035 train_time:6852ms step_avg:33.75ms
step:204/2035 train_time:6885ms step_avg:33.75ms
step:205/2035 train_time:6918ms step_avg:33.75ms
step:206/2035 train_time:6951ms step_avg:33.74ms
step:207/2035 train_time:6984ms step_avg:33.74ms
step:208/2035 train_time:7018ms step_avg:33.74ms
step:209/2035 train_time:7051ms step_avg:33.74ms
step:210/2035 train_time:7084ms step_avg:33.74ms
step:211/2035 train_time:7118ms step_avg:33.73ms
step:212/2035 train_time:7151ms step_avg:33.73ms
step:213/2035 train_time:7184ms step_avg:33.73ms
step:214/2035 train_time:7217ms step_avg:33.73ms
step:215/2035 train_time:7251ms step_avg:33.72ms
step:216/2035 train_time:7284ms step_avg:33.72ms
step:217/2035 train_time:7317ms step_avg:33.72ms
step:218/2035 train_time:7350ms step_avg:33.72ms
step:219/2035 train_time:7383ms step_avg:33.71ms
step:220/2035 train_time:7417ms step_avg:33.71ms
step:221/2035 train_time:7450ms step_avg:33.71ms
step:222/2035 train_time:7483ms step_avg:33.71ms
step:223/2035 train_time:7516ms step_avg:33.71ms
step:224/2035 train_time:7549ms step_avg:33.70ms
step:225/2035 train_time:7583ms step_avg:33.70ms
step:226/2035 train_time:7616ms step_avg:33.70ms
step:227/2035 train_time:7649ms step_avg:33.70ms
step:228/2035 train_time:7683ms step_avg:33.70ms
step:229/2035 train_time:7716ms step_avg:33.69ms
step:230/2035 train_time:7749ms step_avg:33.69ms
step:231/2035 train_time:7782ms step_avg:33.69ms
step:232/2035 train_time:7815ms step_avg:33.69ms
step:233/2035 train_time:7848ms step_avg:33.68ms
step:234/2035 train_time:7882ms step_avg:33.68ms
step:235/2035 train_time:7915ms step_avg:33.68ms
step:236/2035 train_time:7948ms step_avg:33.68ms
step:237/2035 train_time:7981ms step_avg:33.68ms
step:238/2035 train_time:8014ms step_avg:33.67ms
step:239/2035 train_time:8047ms step_avg:33.67ms
step:240/2035 train_time:8080ms step_avg:33.67ms
step:241/2035 train_time:8113ms step_avg:33.66ms
step:242/2035 train_time:8147ms step_avg:33.66ms
step:243/2035 train_time:8180ms step_avg:33.66ms
step:244/2035 train_time:8213ms step_avg:33.66ms
step:245/2035 train_time:8246ms step_avg:33.66ms
step:246/2035 train_time:8279ms step_avg:33.65ms
step:247/2035 train_time:8312ms step_avg:33.65ms
step:248/2035 train_time:8346ms step_avg:33.65ms
step:249/2035 train_time:8378ms step_avg:33.65ms
step:250/2035 train_time:8412ms step_avg:33.65ms
step:250/2035 val_loss:4.2716 train_time:8447ms step_avg:33.79ms
step:251/2035 train_time:8469ms step_avg:33.74ms
step:252/2035 train_time:8489ms step_avg:33.68ms
step:253/2035 train_time:8515ms step_avg:33.66ms
step:254/2035 train_time:8549ms step_avg:33.66ms
step:255/2035 train_time:8583ms step_avg:33.66ms
step:256/2035 train_time:8618ms step_avg:33.66ms
step:257/2035 train_time:8651ms step_avg:33.66ms
step:258/2035 train_time:8684ms step_avg:33.66ms
step:259/2035 train_time:8717ms step_avg:33.66ms
step:260/2035 train_time:8750ms step_avg:33.65ms
step:261/2035 train_time:8783ms step_avg:33.65ms
step:262/2035 train_time:8816ms step_avg:33.65ms
step:263/2035 train_time:8849ms step_avg:33.65ms
step:264/2035 train_time:8882ms step_avg:33.64ms
step:265/2035 train_time:8915ms step_avg:33.64ms
step:266/2035 train_time:8948ms step_avg:33.64ms
step:267/2035 train_time:8981ms step_avg:33.64ms
step:268/2035 train_time:9014ms step_avg:33.64ms
step:269/2035 train_time:9047ms step_avg:33.63ms
step:270/2035 train_time:9080ms step_avg:33.63ms
step:271/2035 train_time:9113ms step_avg:33.63ms
step:272/2035 train_time:9146ms step_avg:33.63ms
step:273/2035 train_time:9179ms step_avg:33.62ms
step:274/2035 train_time:9213ms step_avg:33.62ms
step:275/2035 train_time:9245ms step_avg:33.62ms
step:276/2035 train_time:9278ms step_avg:33.62ms
step:277/2035 train_time:9311ms step_avg:33.61ms
step:278/2035 train_time:9344ms step_avg:33.61ms
step:279/2035 train_time:9378ms step_avg:33.61ms
step:280/2035 train_time:9411ms step_avg:33.61ms
step:281/2035 train_time:9444ms step_avg:33.61ms
step:282/2035 train_time:9478ms step_avg:33.61ms
step:283/2035 train_time:9511ms step_avg:33.61ms
step:284/2035 train_time:9545ms step_avg:33.61ms
step:285/2035 train_time:9578ms step_avg:33.61ms
step:286/2035 train_time:9612ms step_avg:33.61ms
step:287/2035 train_time:9645ms step_avg:33.61ms
step:288/2035 train_time:9679ms step_avg:33.61ms
step:289/2035 train_time:9712ms step_avg:33.61ms
step:290/2035 train_time:9746ms step_avg:33.61ms
step:291/2035 train_time:9779ms step_avg:33.60ms
step:292/2035 train_time:9812ms step_avg:33.60ms
step:293/2035 train_time:9845ms step_avg:33.60ms
step:294/2035 train_time:9878ms step_avg:33.60ms
step:295/2035 train_time:9911ms step_avg:33.60ms
step:296/2035 train_time:9945ms step_avg:33.60ms
step:297/2035 train_time:9978ms step_avg:33.60ms
step:298/2035 train_time:10011ms step_avg:33.60ms
step:299/2035 train_time:10044ms step_avg:33.59ms
step:300/2035 train_time:10077ms step_avg:33.59ms
step:301/2035 train_time:10110ms step_avg:33.59ms
step:302/2035 train_time:10143ms step_avg:33.59ms
step:303/2035 train_time:10176ms step_avg:33.58ms
step:304/2035 train_time:10209ms step_avg:33.58ms
step:305/2035 train_time:10242ms step_avg:33.58ms
step:306/2035 train_time:10275ms step_avg:33.58ms
step:307/2035 train_time:10308ms step_avg:33.58ms
step:308/2035 train_time:10342ms step_avg:33.58ms
step:309/2035 train_time:10375ms step_avg:33.57ms
step:310/2035 train_time:10408ms step_avg:33.57ms
step:311/2035 train_time:10441ms step_avg:33.57ms
step:312/2035 train_time:10475ms step_avg:33.57ms
step:313/2035 train_time:10508ms step_avg:33.57ms
step:314/2035 train_time:10542ms step_avg:33.57ms
step:315/2035 train_time:10575ms step_avg:33.57ms
step:316/2035 train_time:10608ms step_avg:33.57ms
step:317/2035 train_time:10642ms step_avg:33.57ms
step:318/2035 train_time:10675ms step_avg:33.57ms
step:319/2035 train_time:10708ms step_avg:33.57ms
step:320/2035 train_time:10742ms step_avg:33.57ms
step:321/2035 train_time:10775ms step_avg:33.57ms
step:322/2035 train_time:10808ms step_avg:33.56ms
step:323/2035 train_time:10841ms step_avg:33.56ms
step:324/2035 train_time:10874ms step_avg:33.56ms
step:325/2035 train_time:10907ms step_avg:33.56ms
step:326/2035 train_time:10941ms step_avg:33.56ms
step:327/2035 train_time:10974ms step_avg:33.56ms
step:328/2035 train_time:11007ms step_avg:33.56ms
step:329/2035 train_time:11040ms step_avg:33.56ms
step:330/2035 train_time:11074ms step_avg:33.56ms
step:331/2035 train_time:11107ms step_avg:33.56ms
step:332/2035 train_time:11140ms step_avg:33.55ms
step:333/2035 train_time:11173ms step_avg:33.55ms
step:334/2035 train_time:11206ms step_avg:33.55ms
step:335/2035 train_time:11239ms step_avg:33.55ms
step:336/2035 train_time:11273ms step_avg:33.55ms
step:337/2035 train_time:11305ms step_avg:33.55ms
step:338/2035 train_time:11339ms step_avg:33.55ms
step:339/2035 train_time:11372ms step_avg:33.55ms
step:340/2035 train_time:11406ms step_avg:33.55ms
step:341/2035 train_time:11439ms step_avg:33.54ms
step:342/2035 train_time:11472ms step_avg:33.54ms
step:343/2035 train_time:11505ms step_avg:33.54ms
step:344/2035 train_time:11539ms step_avg:33.54ms
step:345/2035 train_time:11572ms step_avg:33.54ms
step:346/2035 train_time:11605ms step_avg:33.54ms
step:347/2035 train_time:11639ms step_avg:33.54ms
step:348/2035 train_time:11673ms step_avg:33.54ms
step:349/2035 train_time:11706ms step_avg:33.54ms
step:350/2035 train_time:11739ms step_avg:33.54ms
step:351/2035 train_time:11772ms step_avg:33.54ms
step:352/2035 train_time:11806ms step_avg:33.54ms
step:353/2035 train_time:11838ms step_avg:33.54ms
step:354/2035 train_time:11872ms step_avg:33.54ms
step:355/2035 train_time:11905ms step_avg:33.54ms
step:356/2035 train_time:11939ms step_avg:33.54ms
step:357/2035 train_time:11972ms step_avg:33.53ms
step:358/2035 train_time:12005ms step_avg:33.53ms
step:359/2035 train_time:12038ms step_avg:33.53ms
step:360/2035 train_time:12071ms step_avg:33.53ms
step:361/2035 train_time:12104ms step_avg:33.53ms
step:362/2035 train_time:12138ms step_avg:33.53ms
step:363/2035 train_time:12170ms step_avg:33.53ms
step:364/2035 train_time:12203ms step_avg:33.53ms
step:365/2035 train_time:12236ms step_avg:33.52ms
step:366/2035 train_time:12269ms step_avg:33.52ms
step:367/2035 train_time:12302ms step_avg:33.52ms
step:368/2035 train_time:12335ms step_avg:33.52ms
step:369/2035 train_time:12368ms step_avg:33.52ms
step:370/2035 train_time:12401ms step_avg:33.52ms
step:371/2035 train_time:12434ms step_avg:33.52ms
step:372/2035 train_time:12467ms step_avg:33.51ms
step:373/2035 train_time:12500ms step_avg:33.51ms
step:374/2035 train_time:12534ms step_avg:33.51ms
step:375/2035 train_time:12567ms step_avg:33.51ms
step:376/2035 train_time:12600ms step_avg:33.51ms
step:377/2035 train_time:12633ms step_avg:33.51ms
step:378/2035 train_time:12666ms step_avg:33.51ms
step:379/2035 train_time:12699ms step_avg:33.51ms
step:380/2035 train_time:12733ms step_avg:33.51ms
step:381/2035 train_time:12766ms step_avg:33.51ms
step:382/2035 train_time:12799ms step_avg:33.50ms
step:383/2035 train_time:12832ms step_avg:33.50ms
step:384/2035 train_time:12865ms step_avg:33.50ms
step:385/2035 train_time:12898ms step_avg:33.50ms
step:386/2035 train_time:12932ms step_avg:33.50ms
step:387/2035 train_time:12965ms step_avg:33.50ms
step:388/2035 train_time:12998ms step_avg:33.50ms
step:389/2035 train_time:13031ms step_avg:33.50ms
step:390/2035 train_time:13064ms step_avg:33.50ms
step:391/2035 train_time:13098ms step_avg:33.50ms
step:392/2035 train_time:13131ms step_avg:33.50ms
step:393/2035 train_time:13164ms step_avg:33.50ms
step:394/2035 train_time:13197ms step_avg:33.50ms
step:395/2035 train_time:13230ms step_avg:33.49ms
step:396/2035 train_time:13263ms step_avg:33.49ms
step:397/2035 train_time:13296ms step_avg:33.49ms
step:398/2035 train_time:13329ms step_avg:33.49ms
step:399/2035 train_time:13362ms step_avg:33.49ms
step:400/2035 train_time:13396ms step_avg:33.49ms
step:401/2035 train_time:13428ms step_avg:33.49ms
step:402/2035 train_time:13462ms step_avg:33.49ms
step:403/2035 train_time:13495ms step_avg:33.49ms
step:404/2035 train_time:13528ms step_avg:33.49ms
step:405/2035 train_time:13562ms step_avg:33.49ms
step:406/2035 train_time:13595ms step_avg:33.49ms
step:407/2035 train_time:13628ms step_avg:33.48ms
step:408/2035 train_time:13661ms step_avg:33.48ms
step:409/2035 train_time:13694ms step_avg:33.48ms
step:410/2035 train_time:13727ms step_avg:33.48ms
step:411/2035 train_time:13761ms step_avg:33.48ms
step:412/2035 train_time:13794ms step_avg:33.48ms
step:413/2035 train_time:13827ms step_avg:33.48ms
step:414/2035 train_time:13861ms step_avg:33.48ms
step:415/2035 train_time:13894ms step_avg:33.48ms
step:416/2035 train_time:13927ms step_avg:33.48ms
step:417/2035 train_time:13960ms step_avg:33.48ms
step:418/2035 train_time:13993ms step_avg:33.48ms
step:419/2035 train_time:14026ms step_avg:33.48ms
step:420/2035 train_time:14059ms step_avg:33.47ms
step:421/2035 train_time:14092ms step_avg:33.47ms
step:422/2035 train_time:14126ms step_avg:33.47ms
step:423/2035 train_time:14159ms step_avg:33.47ms
step:424/2035 train_time:14193ms step_avg:33.47ms
step:425/2035 train_time:14226ms step_avg:33.47ms
step:426/2035 train_time:14259ms step_avg:33.47ms
step:427/2035 train_time:14292ms step_avg:33.47ms
step:428/2035 train_time:14325ms step_avg:33.47ms
step:429/2035 train_time:14359ms step_avg:33.47ms
step:430/2035 train_time:14392ms step_avg:33.47ms
step:431/2035 train_time:14425ms step_avg:33.47ms
step:432/2035 train_time:14459ms step_avg:33.47ms
step:433/2035 train_time:14492ms step_avg:33.47ms
step:434/2035 train_time:14525ms step_avg:33.47ms
step:435/2035 train_time:14559ms step_avg:33.47ms
step:436/2035 train_time:14592ms step_avg:33.47ms
step:437/2035 train_time:14625ms step_avg:33.47ms
step:438/2035 train_time:14659ms step_avg:33.47ms
step:439/2035 train_time:14692ms step_avg:33.47ms
step:440/2035 train_time:14725ms step_avg:33.47ms
step:441/2035 train_time:14758ms step_avg:33.47ms
step:442/2035 train_time:14792ms step_avg:33.47ms
step:443/2035 train_time:14825ms step_avg:33.46ms
step:444/2035 train_time:14858ms step_avg:33.46ms
step:445/2035 train_time:14891ms step_avg:33.46ms
step:446/2035 train_time:14925ms step_avg:33.46ms
step:447/2035 train_time:14957ms step_avg:33.46ms
step:448/2035 train_time:14991ms step_avg:33.46ms
step:449/2035 train_time:15024ms step_avg:33.46ms
step:450/2035 train_time:15057ms step_avg:33.46ms
step:451/2035 train_time:15090ms step_avg:33.46ms
step:452/2035 train_time:15124ms step_avg:33.46ms
step:453/2035 train_time:15157ms step_avg:33.46ms
step:454/2035 train_time:15190ms step_avg:33.46ms
step:455/2035 train_time:15223ms step_avg:33.46ms
step:456/2035 train_time:15256ms step_avg:33.46ms
step:457/2035 train_time:15289ms step_avg:33.46ms
step:458/2035 train_time:15323ms step_avg:33.46ms
step:459/2035 train_time:15356ms step_avg:33.45ms
step:460/2035 train_time:15389ms step_avg:33.45ms
step:461/2035 train_time:15422ms step_avg:33.45ms
step:462/2035 train_time:15456ms step_avg:33.45ms
step:463/2035 train_time:15489ms step_avg:33.45ms
step:464/2035 train_time:15522ms step_avg:33.45ms
step:465/2035 train_time:15555ms step_avg:33.45ms
step:466/2035 train_time:15588ms step_avg:33.45ms
step:467/2035 train_time:15621ms step_avg:33.45ms
step:468/2035 train_time:15655ms step_avg:33.45ms
step:469/2035 train_time:15687ms step_avg:33.45ms
step:470/2035 train_time:15721ms step_avg:33.45ms
step:471/2035 train_time:15754ms step_avg:33.45ms
step:472/2035 train_time:15788ms step_avg:33.45ms
step:473/2035 train_time:15821ms step_avg:33.45ms
step:474/2035 train_time:15854ms step_avg:33.45ms
step:475/2035 train_time:15887ms step_avg:33.45ms
step:476/2035 train_time:15920ms step_avg:33.45ms
step:477/2035 train_time:15953ms step_avg:33.44ms
step:478/2035 train_time:15987ms step_avg:33.44ms
step:479/2035 train_time:16020ms step_avg:33.44ms
step:480/2035 train_time:16054ms step_avg:33.45ms
step:481/2035 train_time:16086ms step_avg:33.44ms
step:482/2035 train_time:16119ms step_avg:33.44ms
step:483/2035 train_time:16152ms step_avg:33.44ms
step:484/2035 train_time:16186ms step_avg:33.44ms
step:485/2035 train_time:16219ms step_avg:33.44ms
step:486/2035 train_time:16252ms step_avg:33.44ms
step:487/2035 train_time:16286ms step_avg:33.44ms
step:488/2035 train_time:16319ms step_avg:33.44ms
step:489/2035 train_time:16352ms step_avg:33.44ms
step:490/2035 train_time:16385ms step_avg:33.44ms
step:491/2035 train_time:16418ms step_avg:33.44ms
step:492/2035 train_time:16451ms step_avg:33.44ms
step:493/2035 train_time:16484ms step_avg:33.44ms
step:494/2035 train_time:16518ms step_avg:33.44ms
step:495/2035 train_time:16551ms step_avg:33.44ms
step:496/2035 train_time:16584ms step_avg:33.44ms
step:497/2035 train_time:16617ms step_avg:33.44ms
step:498/2035 train_time:16650ms step_avg:33.43ms
step:499/2035 train_time:16683ms step_avg:33.43ms
step:500/2035 train_time:16717ms step_avg:33.43ms
step:500/2035 val_loss:4.0044 train_time:16753ms step_avg:33.51ms
step:501/2035 train_time:16775ms step_avg:33.48ms
step:502/2035 train_time:16795ms step_avg:33.46ms
step:503/2035 train_time:16821ms step_avg:33.44ms
step:504/2035 train_time:16855ms step_avg:33.44ms
step:505/2035 train_time:16891ms step_avg:33.45ms
step:506/2035 train_time:16926ms step_avg:33.45ms
step:507/2035 train_time:16960ms step_avg:33.45ms
step:508/2035 train_time:16994ms step_avg:33.45ms
step:509/2035 train_time:17027ms step_avg:33.45ms
step:510/2035 train_time:17060ms step_avg:33.45ms
step:511/2035 train_time:17093ms step_avg:33.45ms
step:512/2035 train_time:17126ms step_avg:33.45ms
step:513/2035 train_time:17159ms step_avg:33.45ms
step:514/2035 train_time:17192ms step_avg:33.45ms
step:515/2035 train_time:17225ms step_avg:33.45ms
step:516/2035 train_time:17258ms step_avg:33.45ms
step:517/2035 train_time:17291ms step_avg:33.44ms
step:518/2035 train_time:17324ms step_avg:33.44ms
step:519/2035 train_time:17356ms step_avg:33.44ms
step:520/2035 train_time:17389ms step_avg:33.44ms
step:521/2035 train_time:17422ms step_avg:33.44ms
step:522/2035 train_time:17455ms step_avg:33.44ms
step:523/2035 train_time:17488ms step_avg:33.44ms
step:524/2035 train_time:17521ms step_avg:33.44ms
step:525/2035 train_time:17554ms step_avg:33.44ms
step:526/2035 train_time:17588ms step_avg:33.44ms
step:527/2035 train_time:17621ms step_avg:33.44ms
step:528/2035 train_time:17654ms step_avg:33.44ms
step:529/2035 train_time:17687ms step_avg:33.43ms
step:530/2035 train_time:17720ms step_avg:33.43ms
step:531/2035 train_time:17754ms step_avg:33.43ms
step:532/2035 train_time:17788ms step_avg:33.44ms
step:533/2035 train_time:17822ms step_avg:33.44ms
step:534/2035 train_time:17856ms step_avg:33.44ms
step:535/2035 train_time:17890ms step_avg:33.44ms
step:536/2035 train_time:17924ms step_avg:33.44ms
step:537/2035 train_time:17957ms step_avg:33.44ms
step:538/2035 train_time:17991ms step_avg:33.44ms
step:539/2035 train_time:18024ms step_avg:33.44ms
step:540/2035 train_time:18057ms step_avg:33.44ms
step:541/2035 train_time:18090ms step_avg:33.44ms
step:542/2035 train_time:18124ms step_avg:33.44ms
step:543/2035 train_time:18157ms step_avg:33.44ms
step:544/2035 train_time:18190ms step_avg:33.44ms
step:545/2035 train_time:18223ms step_avg:33.44ms
step:546/2035 train_time:18257ms step_avg:33.44ms
step:547/2035 train_time:18289ms step_avg:33.44ms
step:548/2035 train_time:18322ms step_avg:33.44ms
step:549/2035 train_time:18355ms step_avg:33.43ms
step:550/2035 train_time:18388ms step_avg:33.43ms
step:551/2035 train_time:18421ms step_avg:33.43ms
step:552/2035 train_time:18454ms step_avg:33.43ms
step:553/2035 train_time:18487ms step_avg:33.43ms
step:554/2035 train_time:18520ms step_avg:33.43ms
step:555/2035 train_time:18554ms step_avg:33.43ms
step:556/2035 train_time:18587ms step_avg:33.43ms
step:557/2035 train_time:18620ms step_avg:33.43ms
step:558/2035 train_time:18653ms step_avg:33.43ms
step:559/2035 train_time:18686ms step_avg:33.43ms
step:560/2035 train_time:18719ms step_avg:33.43ms
step:561/2035 train_time:18752ms step_avg:33.43ms
step:562/2035 train_time:18786ms step_avg:33.43ms
step:563/2035 train_time:18819ms step_avg:33.43ms
step:564/2035 train_time:18853ms step_avg:33.43ms
step:565/2035 train_time:18887ms step_avg:33.43ms
step:566/2035 train_time:18920ms step_avg:33.43ms
step:567/2035 train_time:18953ms step_avg:33.43ms
step:568/2035 train_time:18987ms step_avg:33.43ms
step:569/2035 train_time:19020ms step_avg:33.43ms
step:570/2035 train_time:19053ms step_avg:33.43ms
step:571/2035 train_time:19086ms step_avg:33.43ms
step:572/2035 train_time:19119ms step_avg:33.43ms
step:573/2035 train_time:19152ms step_avg:33.42ms
step:574/2035 train_time:19186ms step_avg:33.42ms
step:575/2035 train_time:19219ms step_avg:33.42ms
step:576/2035 train_time:19252ms step_avg:33.42ms
step:577/2035 train_time:19285ms step_avg:33.42ms
step:578/2035 train_time:19319ms step_avg:33.42ms
step:579/2035 train_time:19351ms step_avg:33.42ms
step:580/2035 train_time:19385ms step_avg:33.42ms
step:581/2035 train_time:19417ms step_avg:33.42ms
step:582/2035 train_time:19450ms step_avg:33.42ms
step:583/2035 train_time:19483ms step_avg:33.42ms
step:584/2035 train_time:19517ms step_avg:33.42ms
step:585/2035 train_time:19550ms step_avg:33.42ms
step:586/2035 train_time:19583ms step_avg:33.42ms
step:587/2035 train_time:19616ms step_avg:33.42ms
step:588/2035 train_time:19649ms step_avg:33.42ms
step:589/2035 train_time:19682ms step_avg:33.42ms
step:590/2035 train_time:19715ms step_avg:33.42ms
step:591/2035 train_time:19748ms step_avg:33.42ms
step:592/2035 train_time:19782ms step_avg:33.42ms
step:593/2035 train_time:19815ms step_avg:33.41ms
step:594/2035 train_time:19848ms step_avg:33.41ms
step:595/2035 train_time:19881ms step_avg:33.41ms
step:596/2035 train_time:19915ms step_avg:33.41ms
step:597/2035 train_time:19948ms step_avg:33.41ms
step:598/2035 train_time:19981ms step_avg:33.41ms
step:599/2035 train_time:20014ms step_avg:33.41ms
step:600/2035 train_time:20048ms step_avg:33.41ms
step:601/2035 train_time:20082ms step_avg:33.41ms
step:602/2035 train_time:20115ms step_avg:33.41ms
step:603/2035 train_time:20148ms step_avg:33.41ms
step:604/2035 train_time:20182ms step_avg:33.41ms
step:605/2035 train_time:20215ms step_avg:33.41ms
step:606/2035 train_time:20248ms step_avg:33.41ms
step:607/2035 train_time:20281ms step_avg:33.41ms
step:608/2035 train_time:20315ms step_avg:33.41ms
step:609/2035 train_time:20347ms step_avg:33.41ms
step:610/2035 train_time:20381ms step_avg:33.41ms
step:611/2035 train_time:20414ms step_avg:33.41ms
step:612/2035 train_time:20447ms step_avg:33.41ms
step:613/2035 train_time:20480ms step_avg:33.41ms
step:614/2035 train_time:20514ms step_avg:33.41ms
step:615/2035 train_time:20547ms step_avg:33.41ms
step:616/2035 train_time:20580ms step_avg:33.41ms
step:617/2035 train_time:20613ms step_avg:33.41ms
step:618/2035 train_time:20646ms step_avg:33.41ms
step:619/2035 train_time:20679ms step_avg:33.41ms
step:620/2035 train_time:20713ms step_avg:33.41ms
step:621/2035 train_time:20746ms step_avg:33.41ms
step:622/2035 train_time:20779ms step_avg:33.41ms
step:623/2035 train_time:20812ms step_avg:33.41ms
step:624/2035 train_time:20846ms step_avg:33.41ms
step:625/2035 train_time:20879ms step_avg:33.41ms
step:626/2035 train_time:20912ms step_avg:33.41ms
step:627/2035 train_time:20945ms step_avg:33.41ms
step:628/2035 train_time:20978ms step_avg:33.41ms
step:629/2035 train_time:21011ms step_avg:33.40ms
step:630/2035 train_time:21045ms step_avg:33.40ms
step:631/2035 train_time:21078ms step_avg:33.40ms
step:632/2035 train_time:21112ms step_avg:33.40ms
step:633/2035 train_time:21145ms step_avg:33.40ms
step:634/2035 train_time:21178ms step_avg:33.40ms
step:635/2035 train_time:21212ms step_avg:33.40ms
step:636/2035 train_time:21245ms step_avg:33.40ms
step:637/2035 train_time:21278ms step_avg:33.40ms
step:638/2035 train_time:21311ms step_avg:33.40ms
step:639/2035 train_time:21344ms step_avg:33.40ms
step:640/2035 train_time:21377ms step_avg:33.40ms
step:641/2035 train_time:21411ms step_avg:33.40ms
step:642/2035 train_time:21444ms step_avg:33.40ms
step:643/2035 train_time:21477ms step_avg:33.40ms
step:644/2035 train_time:21510ms step_avg:33.40ms
step:645/2035 train_time:21543ms step_avg:33.40ms
step:646/2035 train_time:21577ms step_avg:33.40ms
step:647/2035 train_time:21610ms step_avg:33.40ms
step:648/2035 train_time:21643ms step_avg:33.40ms
step:649/2035 train_time:21676ms step_avg:33.40ms
step:650/2035 train_time:21710ms step_avg:33.40ms
step:651/2035 train_time:21743ms step_avg:33.40ms
step:652/2035 train_time:21776ms step_avg:33.40ms
step:653/2035 train_time:21809ms step_avg:33.40ms
step:654/2035 train_time:21842ms step_avg:33.40ms
step:655/2035 train_time:21875ms step_avg:33.40ms
step:656/2035 train_time:21909ms step_avg:33.40ms
step:657/2035 train_time:21942ms step_avg:33.40ms
step:658/2035 train_time:21975ms step_avg:33.40ms
step:659/2035 train_time:22008ms step_avg:33.40ms
step:660/2035 train_time:22042ms step_avg:33.40ms
step:661/2035 train_time:22075ms step_avg:33.40ms
step:662/2035 train_time:22109ms step_avg:33.40ms
step:663/2035 train_time:22142ms step_avg:33.40ms
step:664/2035 train_time:22175ms step_avg:33.40ms
step:665/2035 train_time:22208ms step_avg:33.40ms
step:666/2035 train_time:22242ms step_avg:33.40ms
step:667/2035 train_time:22301ms step_avg:33.43ms
step:668/2035 train_time:22361ms step_avg:33.47ms
step:669/2035 train_time:22422ms step_avg:33.52ms
step:670/2035 train_time:22482ms step_avg:33.55ms
step:671/2035 train_time:22542ms step_avg:33.59ms
step:672/2035 train_time:22601ms step_avg:33.63ms
step:673/2035 train_time:22662ms step_avg:33.67ms
step:674/2035 train_time:22722ms step_avg:33.71ms
step:675/2035 train_time:22783ms step_avg:33.75ms
step:676/2035 train_time:22842ms step_avg:33.79ms
step:677/2035 train_time:22903ms step_avg:33.83ms
step:678/2035 train_time:22962ms step_avg:33.87ms
step:679/2035 train_time:23023ms step_avg:33.91ms
step:680/2035 train_time:23083ms step_avg:33.95ms
step:681/2035 train_time:23144ms step_avg:33.99ms
step:682/2035 train_time:23203ms step_avg:34.02ms
step:683/2035 train_time:23264ms step_avg:34.06ms
step:684/2035 train_time:23323ms step_avg:34.10ms
step:685/2035 train_time:23385ms step_avg:34.14ms
step:686/2035 train_time:23444ms step_avg:34.17ms
step:687/2035 train_time:23504ms step_avg:34.21ms
step:688/2035 train_time:23564ms step_avg:34.25ms
step:689/2035 train_time:23624ms step_avg:34.29ms
step:690/2035 train_time:23684ms step_avg:34.32ms
step:691/2035 train_time:23745ms step_avg:34.36ms
step:692/2035 train_time:23805ms step_avg:34.40ms
step:693/2035 train_time:23866ms step_avg:34.44ms
step:694/2035 train_time:23925ms step_avg:34.47ms
step:695/2035 train_time:23986ms step_avg:34.51ms
step:696/2035 train_time:24046ms step_avg:34.55ms
step:697/2035 train_time:24107ms step_avg:34.59ms
step:698/2035 train_time:24167ms step_avg:34.62ms
step:699/2035 train_time:24227ms step_avg:34.66ms
step:700/2035 train_time:24287ms step_avg:34.70ms
step:701/2035 train_time:24347ms step_avg:34.73ms
step:702/2035 train_time:24407ms step_avg:34.77ms
step:703/2035 train_time:24468ms step_avg:34.81ms
step:704/2035 train_time:24528ms step_avg:34.84ms
step:705/2035 train_time:24590ms step_avg:34.88ms
step:706/2035 train_time:24650ms step_avg:34.91ms
step:707/2035 train_time:24711ms step_avg:34.95ms
step:708/2035 train_time:24771ms step_avg:34.99ms
step:709/2035 train_time:24832ms step_avg:35.02ms
step:710/2035 train_time:24893ms step_avg:35.06ms
step:711/2035 train_time:24953ms step_avg:35.10ms
step:712/2035 train_time:25013ms step_avg:35.13ms
step:713/2035 train_time:25075ms step_avg:35.17ms
step:714/2035 train_time:25136ms step_avg:35.20ms
step:715/2035 train_time:25197ms step_avg:35.24ms
step:716/2035 train_time:25257ms step_avg:35.28ms
step:717/2035 train_time:25317ms step_avg:35.31ms
step:718/2035 train_time:25377ms step_avg:35.34ms
step:719/2035 train_time:25438ms step_avg:35.38ms
step:720/2035 train_time:25497ms step_avg:35.41ms
step:721/2035 train_time:25558ms step_avg:35.45ms
step:722/2035 train_time:25618ms step_avg:35.48ms
step:723/2035 train_time:25679ms step_avg:35.52ms
step:724/2035 train_time:25739ms step_avg:35.55ms
step:725/2035 train_time:25801ms step_avg:35.59ms
step:726/2035 train_time:25860ms step_avg:35.62ms
step:727/2035 train_time:25921ms step_avg:35.65ms
step:728/2035 train_time:25981ms step_avg:35.69ms
step:729/2035 train_time:26042ms step_avg:35.72ms
step:730/2035 train_time:26101ms step_avg:35.75ms
step:731/2035 train_time:26162ms step_avg:35.79ms
step:732/2035 train_time:26221ms step_avg:35.82ms
step:733/2035 train_time:26282ms step_avg:35.86ms
step:734/2035 train_time:26341ms step_avg:35.89ms
step:735/2035 train_time:26402ms step_avg:35.92ms
step:736/2035 train_time:26461ms step_avg:35.95ms
step:737/2035 train_time:26522ms step_avg:35.99ms
step:738/2035 train_time:26581ms step_avg:36.02ms
step:739/2035 train_time:26642ms step_avg:36.05ms
step:740/2035 train_time:26701ms step_avg:36.08ms
step:741/2035 train_time:26763ms step_avg:36.12ms
step:742/2035 train_time:26823ms step_avg:36.15ms
step:743/2035 train_time:26885ms step_avg:36.18ms
step:744/2035 train_time:26944ms step_avg:36.22ms
step:745/2035 train_time:27004ms step_avg:36.25ms
step:746/2035 train_time:27064ms step_avg:36.28ms
step:747/2035 train_time:27125ms step_avg:36.31ms
step:748/2035 train_time:27185ms step_avg:36.34ms
step:749/2035 train_time:27246ms step_avg:36.38ms
step:750/2035 train_time:27305ms step_avg:36.41ms
step:750/2035 val_loss:3.8341 train_time:27368ms step_avg:36.49ms
step:751/2035 train_time:27388ms step_avg:36.47ms
step:752/2035 train_time:27428ms step_avg:36.47ms
step:753/2035 train_time:27492ms step_avg:36.51ms
step:754/2035 train_time:27555ms step_avg:36.54ms
step:755/2035 train_time:27616ms step_avg:36.58ms
step:756/2035 train_time:27677ms step_avg:36.61ms
step:757/2035 train_time:27736ms step_avg:36.64ms
step:758/2035 train_time:27795ms step_avg:36.67ms
step:759/2035 train_time:27854ms step_avg:36.70ms
step:760/2035 train_time:27913ms step_avg:36.73ms
step:761/2035 train_time:27973ms step_avg:36.76ms
step:762/2035 train_time:28032ms step_avg:36.79ms
step:763/2035 train_time:28092ms step_avg:36.82ms
step:764/2035 train_time:28152ms step_avg:36.85ms
step:765/2035 train_time:28211ms step_avg:36.88ms
step:766/2035 train_time:28271ms step_avg:36.91ms
step:767/2035 train_time:28333ms step_avg:36.94ms
step:768/2035 train_time:28394ms step_avg:36.97ms
step:769/2035 train_time:28456ms step_avg:37.00ms
step:770/2035 train_time:28517ms step_avg:37.04ms
step:771/2035 train_time:28578ms step_avg:37.07ms
step:772/2035 train_time:28637ms step_avg:37.09ms
step:773/2035 train_time:28697ms step_avg:37.12ms
step:774/2035 train_time:28757ms step_avg:37.15ms
step:775/2035 train_time:28817ms step_avg:37.18ms
step:776/2035 train_time:28876ms step_avg:37.21ms
step:777/2035 train_time:28935ms step_avg:37.24ms
step:778/2035 train_time:28994ms step_avg:37.27ms
step:779/2035 train_time:29055ms step_avg:37.30ms
step:780/2035 train_time:29114ms step_avg:37.33ms
step:781/2035 train_time:29174ms step_avg:37.35ms
step:782/2035 train_time:29234ms step_avg:37.38ms
step:783/2035 train_time:29296ms step_avg:37.41ms
step:784/2035 train_time:29356ms step_avg:37.44ms
step:785/2035 train_time:29418ms step_avg:37.47ms
step:786/2035 train_time:29478ms step_avg:37.50ms
step:787/2035 train_time:29540ms step_avg:37.53ms
step:788/2035 train_time:29600ms step_avg:37.56ms
step:789/2035 train_time:29661ms step_avg:37.59ms
step:790/2035 train_time:29721ms step_avg:37.62ms
step:791/2035 train_time:29781ms step_avg:37.65ms
step:792/2035 train_time:29840ms step_avg:37.68ms
step:793/2035 train_time:29901ms step_avg:37.71ms
step:794/2035 train_time:29961ms step_avg:37.73ms
step:795/2035 train_time:30021ms step_avg:37.76ms
step:796/2035 train_time:30080ms step_avg:37.79ms
step:797/2035 train_time:30141ms step_avg:37.82ms
step:798/2035 train_time:30200ms step_avg:37.84ms
step:799/2035 train_time:30262ms step_avg:37.88ms
step:800/2035 train_time:30323ms step_avg:37.90ms
step:801/2035 train_time:30384ms step_avg:37.93ms
step:802/2035 train_time:30443ms step_avg:37.96ms
step:803/2035 train_time:30505ms step_avg:37.99ms
step:804/2035 train_time:30565ms step_avg:38.02ms
step:805/2035 train_time:30626ms step_avg:38.04ms
step:806/2035 train_time:30686ms step_avg:38.07ms
step:807/2035 train_time:30747ms step_avg:38.10ms
step:808/2035 train_time:30807ms step_avg:38.13ms
step:809/2035 train_time:30868ms step_avg:38.16ms
step:810/2035 train_time:30929ms step_avg:38.18ms
step:811/2035 train_time:30989ms step_avg:38.21ms
step:812/2035 train_time:31049ms step_avg:38.24ms
step:813/2035 train_time:31110ms step_avg:38.27ms
step:814/2035 train_time:31170ms step_avg:38.29ms
step:815/2035 train_time:31231ms step_avg:38.32ms
step:816/2035 train_time:31293ms step_avg:38.35ms
step:817/2035 train_time:31353ms step_avg:38.38ms
step:818/2035 train_time:31413ms step_avg:38.40ms
step:819/2035 train_time:31474ms step_avg:38.43ms
step:820/2035 train_time:31534ms step_avg:38.46ms
step:821/2035 train_time:31595ms step_avg:38.48ms
step:822/2035 train_time:31655ms step_avg:38.51ms
step:823/2035 train_time:31716ms step_avg:38.54ms
step:824/2035 train_time:31775ms step_avg:38.56ms
step:825/2035 train_time:31836ms step_avg:38.59ms
step:826/2035 train_time:31896ms step_avg:38.62ms
step:827/2035 train_time:31957ms step_avg:38.64ms
step:828/2035 train_time:32016ms step_avg:38.67ms
step:829/2035 train_time:32076ms step_avg:38.69ms
step:830/2035 train_time:32136ms step_avg:38.72ms
step:831/2035 train_time:32197ms step_avg:38.74ms
step:832/2035 train_time:32257ms step_avg:38.77ms
step:833/2035 train_time:32317ms step_avg:38.80ms
step:834/2035 train_time:32377ms step_avg:38.82ms
step:835/2035 train_time:32437ms step_avg:38.85ms
step:836/2035 train_time:32496ms step_avg:38.87ms
step:837/2035 train_time:32556ms step_avg:38.90ms
step:838/2035 train_time:32616ms step_avg:38.92ms
step:839/2035 train_time:32677ms step_avg:38.95ms
step:840/2035 train_time:32737ms step_avg:38.97ms
step:841/2035 train_time:32798ms step_avg:39.00ms
step:842/2035 train_time:32857ms step_avg:39.02ms
step:843/2035 train_time:32918ms step_avg:39.05ms
step:844/2035 train_time:32978ms step_avg:39.07ms
step:845/2035 train_time:33039ms step_avg:39.10ms
step:846/2035 train_time:33099ms step_avg:39.12ms
step:847/2035 train_time:33159ms step_avg:39.15ms
step:848/2035 train_time:33218ms step_avg:39.17ms
step:849/2035 train_time:33279ms step_avg:39.20ms
step:850/2035 train_time:33339ms step_avg:39.22ms
step:851/2035 train_time:33400ms step_avg:39.25ms
step:852/2035 train_time:33459ms step_avg:39.27ms
step:853/2035 train_time:33521ms step_avg:39.30ms
step:854/2035 train_time:33580ms step_avg:39.32ms
step:855/2035 train_time:33641ms step_avg:39.35ms
step:856/2035 train_time:33700ms step_avg:39.37ms
step:857/2035 train_time:33762ms step_avg:39.40ms
step:858/2035 train_time:33821ms step_avg:39.42ms
step:859/2035 train_time:33882ms step_avg:39.44ms
step:860/2035 train_time:33941ms step_avg:39.47ms
step:861/2035 train_time:34002ms step_avg:39.49ms
step:862/2035 train_time:34062ms step_avg:39.52ms
step:863/2035 train_time:34123ms step_avg:39.54ms
step:864/2035 train_time:34184ms step_avg:39.56ms
step:865/2035 train_time:34245ms step_avg:39.59ms
step:866/2035 train_time:34305ms step_avg:39.61ms
step:867/2035 train_time:34366ms step_avg:39.64ms
step:868/2035 train_time:34426ms step_avg:39.66ms
step:869/2035 train_time:34486ms step_avg:39.69ms
step:870/2035 train_time:34546ms step_avg:39.71ms
step:871/2035 train_time:34607ms step_avg:39.73ms
step:872/2035 train_time:34667ms step_avg:39.76ms
step:873/2035 train_time:34729ms step_avg:39.78ms
step:874/2035 train_time:34789ms step_avg:39.80ms
step:875/2035 train_time:34849ms step_avg:39.83ms
step:876/2035 train_time:34909ms step_avg:39.85ms
step:877/2035 train_time:34969ms step_avg:39.87ms
step:878/2035 train_time:35029ms step_avg:39.90ms
step:879/2035 train_time:35090ms step_avg:39.92ms
step:880/2035 train_time:35151ms step_avg:39.94ms
step:881/2035 train_time:35211ms step_avg:39.97ms
step:882/2035 train_time:35271ms step_avg:39.99ms
step:883/2035 train_time:35332ms step_avg:40.01ms
step:884/2035 train_time:35394ms step_avg:40.04ms
step:885/2035 train_time:35455ms step_avg:40.06ms
step:886/2035 train_time:35515ms step_avg:40.08ms
step:887/2035 train_time:35574ms step_avg:40.11ms
step:888/2035 train_time:35634ms step_avg:40.13ms
step:889/2035 train_time:35695ms step_avg:40.15ms
step:890/2035 train_time:35755ms step_avg:40.17ms
step:891/2035 train_time:35816ms step_avg:40.20ms
step:892/2035 train_time:35877ms step_avg:40.22ms
step:893/2035 train_time:35938ms step_avg:40.24ms
step:894/2035 train_time:35997ms step_avg:40.27ms
step:895/2035 train_time:36058ms step_avg:40.29ms
step:896/2035 train_time:36118ms step_avg:40.31ms
step:897/2035 train_time:36178ms step_avg:40.33ms
step:898/2035 train_time:36238ms step_avg:40.35ms
step:899/2035 train_time:36298ms step_avg:40.38ms
step:900/2035 train_time:36358ms step_avg:40.40ms
step:901/2035 train_time:36418ms step_avg:40.42ms
step:902/2035 train_time:36478ms step_avg:40.44ms
step:903/2035 train_time:36538ms step_avg:40.46ms
step:904/2035 train_time:36598ms step_avg:40.48ms
step:905/2035 train_time:36658ms step_avg:40.51ms
step:906/2035 train_time:36718ms step_avg:40.53ms
step:907/2035 train_time:36779ms step_avg:40.55ms
step:908/2035 train_time:36838ms step_avg:40.57ms
step:909/2035 train_time:36899ms step_avg:40.59ms
step:910/2035 train_time:36959ms step_avg:40.61ms
step:911/2035 train_time:37021ms step_avg:40.64ms
step:912/2035 train_time:37080ms step_avg:40.66ms
step:913/2035 train_time:37140ms step_avg:40.68ms
step:914/2035 train_time:37200ms step_avg:40.70ms
step:915/2035 train_time:37261ms step_avg:40.72ms
step:916/2035 train_time:37321ms step_avg:40.74ms
step:917/2035 train_time:37382ms step_avg:40.77ms
step:918/2035 train_time:37441ms step_avg:40.79ms
step:919/2035 train_time:37502ms step_avg:40.81ms
step:920/2035 train_time:37562ms step_avg:40.83ms
step:921/2035 train_time:37623ms step_avg:40.85ms
step:922/2035 train_time:37683ms step_avg:40.87ms
step:923/2035 train_time:37744ms step_avg:40.89ms
step:924/2035 train_time:37804ms step_avg:40.91ms
step:925/2035 train_time:37865ms step_avg:40.93ms
step:926/2035 train_time:37925ms step_avg:40.96ms
step:927/2035 train_time:37986ms step_avg:40.98ms
step:928/2035 train_time:38047ms step_avg:41.00ms
step:929/2035 train_time:38108ms step_avg:41.02ms
step:930/2035 train_time:38168ms step_avg:41.04ms
step:931/2035 train_time:38229ms step_avg:41.06ms
step:932/2035 train_time:38290ms step_avg:41.08ms
step:933/2035 train_time:38351ms step_avg:41.10ms
step:934/2035 train_time:38411ms step_avg:41.13ms
step:935/2035 train_time:38471ms step_avg:41.15ms
step:936/2035 train_time:38532ms step_avg:41.17ms
step:937/2035 train_time:38593ms step_avg:41.19ms
step:938/2035 train_time:38652ms step_avg:41.21ms
step:939/2035 train_time:38713ms step_avg:41.23ms
step:940/2035 train_time:38773ms step_avg:41.25ms
step:941/2035 train_time:38834ms step_avg:41.27ms
step:942/2035 train_time:38895ms step_avg:41.29ms
step:943/2035 train_time:38955ms step_avg:41.31ms
step:944/2035 train_time:39015ms step_avg:41.33ms
step:945/2035 train_time:39076ms step_avg:41.35ms
step:946/2035 train_time:39136ms step_avg:41.37ms
step:947/2035 train_time:39197ms step_avg:41.39ms
step:948/2035 train_time:39257ms step_avg:41.41ms
step:949/2035 train_time:39318ms step_avg:41.43ms
step:950/2035 train_time:39377ms step_avg:41.45ms
step:951/2035 train_time:39438ms step_avg:41.47ms
step:952/2035 train_time:39498ms step_avg:41.49ms
step:953/2035 train_time:39559ms step_avg:41.51ms
step:954/2035 train_time:39619ms step_avg:41.53ms
step:955/2035 train_time:39680ms step_avg:41.55ms
step:956/2035 train_time:39739ms step_avg:41.57ms
step:957/2035 train_time:39799ms step_avg:41.59ms
step:958/2035 train_time:39859ms step_avg:41.61ms
step:959/2035 train_time:39919ms step_avg:41.63ms
step:960/2035 train_time:39979ms step_avg:41.64ms
step:961/2035 train_time:40040ms step_avg:41.67ms
step:962/2035 train_time:40101ms step_avg:41.68ms
step:963/2035 train_time:40161ms step_avg:41.70ms
step:964/2035 train_time:40220ms step_avg:41.72ms
step:965/2035 train_time:40281ms step_avg:41.74ms
step:966/2035 train_time:40341ms step_avg:41.76ms
step:967/2035 train_time:40402ms step_avg:41.78ms
step:968/2035 train_time:40461ms step_avg:41.80ms
step:969/2035 train_time:40522ms step_avg:41.82ms
step:970/2035 train_time:40582ms step_avg:41.84ms
step:971/2035 train_time:40642ms step_avg:41.86ms
step:972/2035 train_time:40702ms step_avg:41.87ms
step:973/2035 train_time:40763ms step_avg:41.89ms
step:974/2035 train_time:40823ms step_avg:41.91ms
step:975/2035 train_time:40884ms step_avg:41.93ms
step:976/2035 train_time:40944ms step_avg:41.95ms
step:977/2035 train_time:41005ms step_avg:41.97ms
step:978/2035 train_time:41064ms step_avg:41.99ms
step:979/2035 train_time:41126ms step_avg:42.01ms
step:980/2035 train_time:41186ms step_avg:42.03ms
step:981/2035 train_time:41247ms step_avg:42.05ms
step:982/2035 train_time:41307ms step_avg:42.06ms
step:983/2035 train_time:41368ms step_avg:42.08ms
step:984/2035 train_time:41428ms step_avg:42.10ms
step:985/2035 train_time:41490ms step_avg:42.12ms
step:986/2035 train_time:41551ms step_avg:42.14ms
step:987/2035 train_time:41612ms step_avg:42.16ms
step:988/2035 train_time:41672ms step_avg:42.18ms
step:989/2035 train_time:41733ms step_avg:42.20ms
step:990/2035 train_time:41794ms step_avg:42.22ms
step:991/2035 train_time:41854ms step_avg:42.23ms
step:992/2035 train_time:41915ms step_avg:42.25ms
step:993/2035 train_time:41975ms step_avg:42.27ms
step:994/2035 train_time:42035ms step_avg:42.29ms
step:995/2035 train_time:42095ms step_avg:42.31ms
step:996/2035 train_time:42155ms step_avg:42.32ms
step:997/2035 train_time:42216ms step_avg:42.34ms
step:998/2035 train_time:42276ms step_avg:42.36ms
step:999/2035 train_time:42338ms step_avg:42.38ms
step:1000/2035 train_time:42398ms step_avg:42.40ms
step:1000/2035 val_loss:3.6887 train_time:42460ms step_avg:42.46ms
step:1001/2035 train_time:42481ms step_avg:42.44ms
step:1002/2035 train_time:42520ms step_avg:42.44ms
step:1003/2035 train_time:42586ms step_avg:42.46ms
step:1004/2035 train_time:42651ms step_avg:42.48ms
step:1005/2035 train_time:42713ms step_avg:42.50ms
step:1006/2035 train_time:42772ms step_avg:42.52ms
step:1007/2035 train_time:42832ms step_avg:42.53ms
step:1008/2035 train_time:42890ms step_avg:42.55ms
step:1009/2035 train_time:42950ms step_avg:42.57ms
step:1010/2035 train_time:43009ms step_avg:42.58ms
step:1011/2035 train_time:43069ms step_avg:42.60ms
step:1012/2035 train_time:43129ms step_avg:42.62ms
step:1013/2035 train_time:43188ms step_avg:42.63ms
step:1014/2035 train_time:43248ms step_avg:42.65ms
step:1015/2035 train_time:43308ms step_avg:42.67ms
step:1016/2035 train_time:43367ms step_avg:42.68ms
step:1017/2035 train_time:43429ms step_avg:42.70ms
step:1018/2035 train_time:43490ms step_avg:42.72ms
step:1019/2035 train_time:43553ms step_avg:42.74ms
step:1020/2035 train_time:43615ms step_avg:42.76ms
step:1021/2035 train_time:43676ms step_avg:42.78ms
step:1022/2035 train_time:43736ms step_avg:42.79ms
step:1023/2035 train_time:43796ms step_avg:42.81ms
step:1024/2035 train_time:43856ms step_avg:42.83ms
step:1025/2035 train_time:43916ms step_avg:42.84ms
step:1026/2035 train_time:43975ms step_avg:42.86ms
step:1027/2035 train_time:44035ms step_avg:42.88ms
step:1028/2035 train_time:44094ms step_avg:42.89ms
step:1029/2035 train_time:44155ms step_avg:42.91ms
step:1030/2035 train_time:44214ms step_avg:42.93ms
step:1031/2035 train_time:44274ms step_avg:42.94ms
step:1032/2035 train_time:44334ms step_avg:42.96ms
step:1033/2035 train_time:44396ms step_avg:42.98ms
step:1034/2035 train_time:44457ms step_avg:43.00ms
step:1035/2035 train_time:44519ms step_avg:43.01ms
step:1036/2035 train_time:44579ms step_avg:43.03ms
step:1037/2035 train_time:44641ms step_avg:43.05ms
step:1038/2035 train_time:44701ms step_avg:43.06ms
step:1039/2035 train_time:44761ms step_avg:43.08ms
step:1040/2035 train_time:44822ms step_avg:43.10ms
step:1041/2035 train_time:44882ms step_avg:43.11ms
step:1042/2035 train_time:44942ms step_avg:43.13ms
step:1043/2035 train_time:45002ms step_avg:43.15ms
step:1044/2035 train_time:45063ms step_avg:43.16ms
step:1045/2035 train_time:45124ms step_avg:43.18ms
step:1046/2035 train_time:45183ms step_avg:43.20ms
step:1047/2035 train_time:45244ms step_avg:43.21ms
step:1048/2035 train_time:45303ms step_avg:43.23ms
step:1049/2035 train_time:45364ms step_avg:43.25ms
step:1050/2035 train_time:45424ms step_avg:43.26ms
step:1051/2035 train_time:45486ms step_avg:43.28ms
step:1052/2035 train_time:45547ms step_avg:43.30ms
step:1053/2035 train_time:45608ms step_avg:43.31ms
step:1054/2035 train_time:45668ms step_avg:43.33ms
step:1055/2035 train_time:45729ms step_avg:43.35ms
step:1056/2035 train_time:45789ms step_avg:43.36ms
step:1057/2035 train_time:45850ms step_avg:43.38ms
step:1058/2035 train_time:45911ms step_avg:43.39ms
step:1059/2035 train_time:45970ms step_avg:43.41ms
step:1060/2035 train_time:46030ms step_avg:43.42ms
step:1061/2035 train_time:46091ms step_avg:43.44ms
step:1062/2035 train_time:46150ms step_avg:43.46ms
step:1063/2035 train_time:46211ms step_avg:43.47ms
step:1064/2035 train_time:46271ms step_avg:43.49ms
step:1065/2035 train_time:46331ms step_avg:43.50ms
step:1066/2035 train_time:46391ms step_avg:43.52ms
step:1067/2035 train_time:46452ms step_avg:43.54ms
step:1068/2035 train_time:46512ms step_avg:43.55ms
step:1069/2035 train_time:46573ms step_avg:43.57ms
step:1070/2035 train_time:46633ms step_avg:43.58ms
step:1071/2035 train_time:46695ms step_avg:43.60ms
step:1072/2035 train_time:46754ms step_avg:43.61ms
step:1073/2035 train_time:46814ms step_avg:43.63ms
step:1074/2035 train_time:46874ms step_avg:43.64ms
step:1075/2035 train_time:46935ms step_avg:43.66ms
step:1076/2035 train_time:46994ms step_avg:43.68ms
step:1077/2035 train_time:47055ms step_avg:43.69ms
step:1078/2035 train_time:47114ms step_avg:43.71ms
step:1079/2035 train_time:47174ms step_avg:43.72ms
step:1080/2035 train_time:47234ms step_avg:43.74ms
step:1081/2035 train_time:47294ms step_avg:43.75ms
step:1082/2035 train_time:47354ms step_avg:43.76ms
step:1083/2035 train_time:47415ms step_avg:43.78ms
step:1084/2035 train_time:47475ms step_avg:43.80ms
step:1085/2035 train_time:47536ms step_avg:43.81ms
step:1086/2035 train_time:47596ms step_avg:43.83ms
step:1087/2035 train_time:47657ms step_avg:43.84ms
step:1088/2035 train_time:47716ms step_avg:43.86ms
step:1089/2035 train_time:47777ms step_avg:43.87ms
step:1090/2035 train_time:47837ms step_avg:43.89ms
step:1091/2035 train_time:47898ms step_avg:43.90ms
step:1092/2035 train_time:47958ms step_avg:43.92ms
step:1093/2035 train_time:48019ms step_avg:43.93ms
step:1094/2035 train_time:48078ms step_avg:43.95ms
step:1095/2035 train_time:48139ms step_avg:43.96ms
step:1096/2035 train_time:48198ms step_avg:43.98ms
step:1097/2035 train_time:48259ms step_avg:43.99ms
step:1098/2035 train_time:48318ms step_avg:44.01ms
step:1099/2035 train_time:48379ms step_avg:44.02ms
step:1100/2035 train_time:48440ms step_avg:44.04ms
step:1101/2035 train_time:48500ms step_avg:44.05ms
step:1102/2035 train_time:48560ms step_avg:44.07ms
step:1103/2035 train_time:48621ms step_avg:44.08ms
step:1104/2035 train_time:48681ms step_avg:44.10ms
step:1105/2035 train_time:48742ms step_avg:44.11ms
step:1106/2035 train_time:48802ms step_avg:44.12ms
step:1107/2035 train_time:48862ms step_avg:44.14ms
step:1108/2035 train_time:48923ms step_avg:44.15ms
step:1109/2035 train_time:48984ms step_avg:44.17ms
step:1110/2035 train_time:49045ms step_avg:44.19ms
step:1111/2035 train_time:49106ms step_avg:44.20ms
step:1112/2035 train_time:49166ms step_avg:44.21ms
step:1113/2035 train_time:49226ms step_avg:44.23ms
step:1114/2035 train_time:49286ms step_avg:44.24ms
step:1115/2035 train_time:49348ms step_avg:44.26ms
step:1116/2035 train_time:49408ms step_avg:44.27ms
step:1117/2035 train_time:49469ms step_avg:44.29ms
step:1118/2035 train_time:49530ms step_avg:44.30ms
step:1119/2035 train_time:49591ms step_avg:44.32ms
step:1120/2035 train_time:49651ms step_avg:44.33ms
step:1121/2035 train_time:49711ms step_avg:44.35ms
step:1122/2035 train_time:49771ms step_avg:44.36ms
step:1123/2035 train_time:49833ms step_avg:44.37ms
step:1124/2035 train_time:49892ms step_avg:44.39ms
step:1125/2035 train_time:49953ms step_avg:44.40ms
step:1126/2035 train_time:50014ms step_avg:44.42ms
step:1127/2035 train_time:50075ms step_avg:44.43ms
step:1128/2035 train_time:50134ms step_avg:44.45ms
step:1129/2035 train_time:50194ms step_avg:44.46ms
step:1130/2035 train_time:50254ms step_avg:44.47ms
step:1131/2035 train_time:50315ms step_avg:44.49ms
step:1132/2035 train_time:50374ms step_avg:44.50ms
step:1133/2035 train_time:50435ms step_avg:44.51ms
step:1134/2035 train_time:50495ms step_avg:44.53ms
step:1135/2035 train_time:50556ms step_avg:44.54ms
step:1136/2035 train_time:50615ms step_avg:44.56ms
step:1137/2035 train_time:50676ms step_avg:44.57ms
step:1138/2035 train_time:50735ms step_avg:44.58ms
step:1139/2035 train_time:50796ms step_avg:44.60ms
step:1140/2035 train_time:50856ms step_avg:44.61ms
step:1141/2035 train_time:50917ms step_avg:44.62ms
step:1142/2035 train_time:50977ms step_avg:44.64ms
step:1143/2035 train_time:51038ms step_avg:44.65ms
step:1144/2035 train_time:51097ms step_avg:44.67ms
step:1145/2035 train_time:51159ms step_avg:44.68ms
step:1146/2035 train_time:51218ms step_avg:44.69ms
step:1147/2035 train_time:51279ms step_avg:44.71ms
step:1148/2035 train_time:51338ms step_avg:44.72ms
step:1149/2035 train_time:51399ms step_avg:44.73ms
step:1150/2035 train_time:51459ms step_avg:44.75ms
step:1151/2035 train_time:51521ms step_avg:44.76ms
step:1152/2035 train_time:51581ms step_avg:44.77ms
step:1153/2035 train_time:51641ms step_avg:44.79ms
step:1154/2035 train_time:51702ms step_avg:44.80ms
step:1155/2035 train_time:51763ms step_avg:44.82ms
step:1156/2035 train_time:51823ms step_avg:44.83ms
step:1157/2035 train_time:51884ms step_avg:44.84ms
step:1158/2035 train_time:51944ms step_avg:44.86ms
step:1159/2035 train_time:52005ms step_avg:44.87ms
step:1160/2035 train_time:52065ms step_avg:44.88ms
step:1161/2035 train_time:52126ms step_avg:44.90ms
step:1162/2035 train_time:52187ms step_avg:44.91ms
step:1163/2035 train_time:52247ms step_avg:44.92ms
step:1164/2035 train_time:52307ms step_avg:44.94ms
step:1165/2035 train_time:52368ms step_avg:44.95ms
step:1166/2035 train_time:52429ms step_avg:44.96ms
step:1167/2035 train_time:52490ms step_avg:44.98ms
step:1168/2035 train_time:52549ms step_avg:44.99ms
step:1169/2035 train_time:52609ms step_avg:45.00ms
step:1170/2035 train_time:52669ms step_avg:45.02ms
step:1171/2035 train_time:52731ms step_avg:45.03ms
step:1172/2035 train_time:52791ms step_avg:45.04ms
step:1173/2035 train_time:52852ms step_avg:45.06ms
step:1174/2035 train_time:52912ms step_avg:45.07ms
step:1175/2035 train_time:52973ms step_avg:45.08ms
step:1176/2035 train_time:53033ms step_avg:45.10ms
step:1177/2035 train_time:53094ms step_avg:45.11ms
step:1178/2035 train_time:53154ms step_avg:45.12ms
step:1179/2035 train_time:53215ms step_avg:45.14ms
step:1180/2035 train_time:53274ms step_avg:45.15ms
step:1181/2035 train_time:53335ms step_avg:45.16ms
step:1182/2035 train_time:53394ms step_avg:45.17ms
step:1183/2035 train_time:53456ms step_avg:45.19ms
step:1184/2035 train_time:53515ms step_avg:45.20ms
step:1185/2035 train_time:53576ms step_avg:45.21ms
step:1186/2035 train_time:53636ms step_avg:45.22ms
step:1187/2035 train_time:53696ms step_avg:45.24ms
step:1188/2035 train_time:53757ms step_avg:45.25ms
step:1189/2035 train_time:53817ms step_avg:45.26ms
step:1190/2035 train_time:53877ms step_avg:45.27ms
step:1191/2035 train_time:53938ms step_avg:45.29ms
step:1192/2035 train_time:53998ms step_avg:45.30ms
step:1193/2035 train_time:54059ms step_avg:45.31ms
step:1194/2035 train_time:54119ms step_avg:45.33ms
step:1195/2035 train_time:54179ms step_avg:45.34ms
step:1196/2035 train_time:54238ms step_avg:45.35ms
step:1197/2035 train_time:54299ms step_avg:45.36ms
step:1198/2035 train_time:54358ms step_avg:45.37ms
step:1199/2035 train_time:54419ms step_avg:45.39ms
step:1200/2035 train_time:54479ms step_avg:45.40ms
step:1201/2035 train_time:54539ms step_avg:45.41ms
step:1202/2035 train_time:54599ms step_avg:45.42ms
step:1203/2035 train_time:54660ms step_avg:45.44ms
step:1204/2035 train_time:54719ms step_avg:45.45ms
step:1205/2035 train_time:54779ms step_avg:45.46ms
step:1206/2035 train_time:54840ms step_avg:45.47ms
step:1207/2035 train_time:54900ms step_avg:45.48ms
step:1208/2035 train_time:54960ms step_avg:45.50ms
step:1209/2035 train_time:55021ms step_avg:45.51ms
step:1210/2035 train_time:55081ms step_avg:45.52ms
step:1211/2035 train_time:55142ms step_avg:45.53ms
step:1212/2035 train_time:55202ms step_avg:45.55ms
step:1213/2035 train_time:55263ms step_avg:45.56ms
step:1214/2035 train_time:55323ms step_avg:45.57ms
step:1215/2035 train_time:55384ms step_avg:45.58ms
step:1216/2035 train_time:55444ms step_avg:45.60ms
step:1217/2035 train_time:55506ms step_avg:45.61ms
step:1218/2035 train_time:55565ms step_avg:45.62ms
step:1219/2035 train_time:55626ms step_avg:45.63ms
step:1220/2035 train_time:55686ms step_avg:45.64ms
step:1221/2035 train_time:55746ms step_avg:45.66ms
step:1222/2035 train_time:55807ms step_avg:45.67ms
step:1223/2035 train_time:55868ms step_avg:45.68ms
step:1224/2035 train_time:55929ms step_avg:45.69ms
step:1225/2035 train_time:55990ms step_avg:45.71ms
step:1226/2035 train_time:56050ms step_avg:45.72ms
step:1227/2035 train_time:56110ms step_avg:45.73ms
step:1228/2035 train_time:56170ms step_avg:45.74ms
step:1229/2035 train_time:56231ms step_avg:45.75ms
step:1230/2035 train_time:56291ms step_avg:45.77ms
step:1231/2035 train_time:56352ms step_avg:45.78ms
step:1232/2035 train_time:56411ms step_avg:45.79ms
step:1233/2035 train_time:56473ms step_avg:45.80ms
step:1234/2035 train_time:56533ms step_avg:45.81ms
step:1235/2035 train_time:56593ms step_avg:45.82ms
step:1236/2035 train_time:56653ms step_avg:45.84ms
step:1237/2035 train_time:56714ms step_avg:45.85ms
step:1238/2035 train_time:56774ms step_avg:45.86ms
step:1239/2035 train_time:56834ms step_avg:45.87ms
step:1240/2035 train_time:56894ms step_avg:45.88ms
step:1241/2035 train_time:56955ms step_avg:45.89ms
step:1242/2035 train_time:57014ms step_avg:45.91ms
step:1243/2035 train_time:57074ms step_avg:45.92ms
step:1244/2035 train_time:57134ms step_avg:45.93ms
step:1245/2035 train_time:57194ms step_avg:45.94ms
step:1246/2035 train_time:57255ms step_avg:45.95ms
step:1247/2035 train_time:57316ms step_avg:45.96ms
step:1248/2035 train_time:57376ms step_avg:45.97ms
step:1249/2035 train_time:57436ms step_avg:45.99ms
step:1250/2035 train_time:57496ms step_avg:46.00ms
step:1250/2035 val_loss:3.5729 train_time:57560ms step_avg:46.05ms
step:1251/2035 train_time:57581ms step_avg:46.03ms
step:1252/2035 train_time:57620ms step_avg:46.02ms
step:1253/2035 train_time:57683ms step_avg:46.04ms
step:1254/2035 train_time:57747ms step_avg:46.05ms
step:1255/2035 train_time:57808ms step_avg:46.06ms
step:1256/2035 train_time:57868ms step_avg:46.07ms
step:1257/2035 train_time:57928ms step_avg:46.08ms
step:1258/2035 train_time:57988ms step_avg:46.10ms
step:1259/2035 train_time:58048ms step_avg:46.11ms
step:1260/2035 train_time:58107ms step_avg:46.12ms
step:1261/2035 train_time:58167ms step_avg:46.13ms
step:1262/2035 train_time:58227ms step_avg:46.14ms
step:1263/2035 train_time:58286ms step_avg:46.15ms
step:1264/2035 train_time:58346ms step_avg:46.16ms
step:1265/2035 train_time:58406ms step_avg:46.17ms
step:1266/2035 train_time:58467ms step_avg:46.18ms
step:1267/2035 train_time:58530ms step_avg:46.20ms
step:1268/2035 train_time:58591ms step_avg:46.21ms
step:1269/2035 train_time:58652ms step_avg:46.22ms
step:1270/2035 train_time:58713ms step_avg:46.23ms
step:1271/2035 train_time:58775ms step_avg:46.24ms
step:1272/2035 train_time:58835ms step_avg:46.25ms
step:1273/2035 train_time:58896ms step_avg:46.27ms
step:1274/2035 train_time:58956ms step_avg:46.28ms
step:1275/2035 train_time:59016ms step_avg:46.29ms
step:1276/2035 train_time:59075ms step_avg:46.30ms
step:1277/2035 train_time:59137ms step_avg:46.31ms
step:1278/2035 train_time:59197ms step_avg:46.32ms
step:1279/2035 train_time:59258ms step_avg:46.33ms
step:1280/2035 train_time:59317ms step_avg:46.34ms
step:1281/2035 train_time:59377ms step_avg:46.35ms
step:1282/2035 train_time:59436ms step_avg:46.36ms
step:1283/2035 train_time:59497ms step_avg:46.37ms
step:1284/2035 train_time:59557ms step_avg:46.38ms
step:1285/2035 train_time:59619ms step_avg:46.40ms
step:1286/2035 train_time:59680ms step_avg:46.41ms
step:1287/2035 train_time:59740ms step_avg:46.42ms
step:1288/2035 train_time:59800ms step_avg:46.43ms
step:1289/2035 train_time:59861ms step_avg:46.44ms
step:1290/2035 train_time:59921ms step_avg:46.45ms
step:1291/2035 train_time:59982ms step_avg:46.46ms
step:1292/2035 train_time:60042ms step_avg:46.47ms
step:1293/2035 train_time:60103ms step_avg:46.48ms
step:1294/2035 train_time:60162ms step_avg:46.49ms
step:1295/2035 train_time:60223ms step_avg:46.50ms
step:1296/2035 train_time:60283ms step_avg:46.51ms
step:1297/2035 train_time:60343ms step_avg:46.53ms
step:1298/2035 train_time:60403ms step_avg:46.54ms
step:1299/2035 train_time:60464ms step_avg:46.55ms
step:1300/2035 train_time:60525ms step_avg:46.56ms
step:1301/2035 train_time:60586ms step_avg:46.57ms
step:1302/2035 train_time:60647ms step_avg:46.58ms
step:1303/2035 train_time:60708ms step_avg:46.59ms
step:1304/2035 train_time:60769ms step_avg:46.60ms
step:1305/2035 train_time:60830ms step_avg:46.61ms
step:1306/2035 train_time:60890ms step_avg:46.62ms
step:1307/2035 train_time:60951ms step_avg:46.63ms
step:1308/2035 train_time:61011ms step_avg:46.64ms
step:1309/2035 train_time:61072ms step_avg:46.66ms
step:1310/2035 train_time:61132ms step_avg:46.67ms
step:1311/2035 train_time:61192ms step_avg:46.68ms
step:1312/2035 train_time:61252ms step_avg:46.69ms
step:1313/2035 train_time:61312ms step_avg:46.70ms
step:1314/2035 train_time:61372ms step_avg:46.71ms
step:1315/2035 train_time:61432ms step_avg:46.72ms
step:1316/2035 train_time:61492ms step_avg:46.73ms
step:1317/2035 train_time:61553ms step_avg:46.74ms
step:1318/2035 train_time:61613ms step_avg:46.75ms
step:1319/2035 train_time:61673ms step_avg:46.76ms
step:1320/2035 train_time:61733ms step_avg:46.77ms
step:1321/2035 train_time:61794ms step_avg:46.78ms
step:1322/2035 train_time:61853ms step_avg:46.79ms
step:1323/2035 train_time:61914ms step_avg:46.80ms
step:1324/2035 train_time:61974ms step_avg:46.81ms
step:1325/2035 train_time:62034ms step_avg:46.82ms
step:1326/2035 train_time:62093ms step_avg:46.83ms
step:1327/2035 train_time:62153ms step_avg:46.84ms
step:1328/2035 train_time:62213ms step_avg:46.85ms
step:1329/2035 train_time:62273ms step_avg:46.86ms
step:1330/2035 train_time:62332ms step_avg:46.87ms
step:1331/2035 train_time:62394ms step_avg:46.88ms
step:1332/2035 train_time:62482ms step_avg:46.91ms
step:1333/2035 train_time:62571ms step_avg:46.94ms
step:1334/2035 train_time:62659ms step_avg:46.97ms
step:1335/2035 train_time:62748ms step_avg:47.00ms
step:1336/2035 train_time:62838ms step_avg:47.03ms
step:1337/2035 train_time:62927ms step_avg:47.07ms
step:1338/2035 train_time:63015ms step_avg:47.10ms
step:1339/2035 train_time:63104ms step_avg:47.13ms
step:1340/2035 train_time:63192ms step_avg:47.16ms
step:1341/2035 train_time:63279ms step_avg:47.19ms
step:1342/2035 train_time:63366ms step_avg:47.22ms
step:1343/2035 train_time:63454ms step_avg:47.25ms
step:1344/2035 train_time:63543ms step_avg:47.28ms
step:1345/2035 train_time:63632ms step_avg:47.31ms
step:1346/2035 train_time:63719ms step_avg:47.34ms
step:1347/2035 train_time:63807ms step_avg:47.37ms
step:1348/2035 train_time:63895ms step_avg:47.40ms
step:1349/2035 train_time:63985ms step_avg:47.43ms
step:1350/2035 train_time:64073ms step_avg:47.46ms
step:1351/2035 train_time:64162ms step_avg:47.49ms
step:1352/2035 train_time:64249ms step_avg:47.52ms
step:1353/2035 train_time:64336ms step_avg:47.55ms
step:1354/2035 train_time:64425ms step_avg:47.58ms
step:1355/2035 train_time:64513ms step_avg:47.61ms
step:1356/2035 train_time:64602ms step_avg:47.64ms
step:1357/2035 train_time:64690ms step_avg:47.67ms
step:1358/2035 train_time:64778ms step_avg:47.70ms
step:1359/2035 train_time:64865ms step_avg:47.73ms
step:1360/2035 train_time:64954ms step_avg:47.76ms
step:1361/2035 train_time:65043ms step_avg:47.79ms
step:1362/2035 train_time:65130ms step_avg:47.82ms
step:1363/2035 train_time:65219ms step_avg:47.85ms
step:1364/2035 train_time:65306ms step_avg:47.88ms
step:1365/2035 train_time:65394ms step_avg:47.91ms
step:1366/2035 train_time:65481ms step_avg:47.94ms
step:1367/2035 train_time:65571ms step_avg:47.97ms
step:1368/2035 train_time:65658ms step_avg:48.00ms
step:1369/2035 train_time:65747ms step_avg:48.03ms
step:1370/2035 train_time:65835ms step_avg:48.05ms
step:1371/2035 train_time:65922ms step_avg:48.08ms
step:1372/2035 train_time:66010ms step_avg:48.11ms
step:1373/2035 train_time:66099ms step_avg:48.14ms
step:1374/2035 train_time:66186ms step_avg:48.17ms
step:1375/2035 train_time:66274ms step_avg:48.20ms
step:1376/2035 train_time:66361ms step_avg:48.23ms
step:1377/2035 train_time:66449ms step_avg:48.26ms
step:1378/2035 train_time:66537ms step_avg:48.29ms
step:1379/2035 train_time:66625ms step_avg:48.31ms
step:1380/2035 train_time:66714ms step_avg:48.34ms
step:1381/2035 train_time:66802ms step_avg:48.37ms
step:1382/2035 train_time:66890ms step_avg:48.40ms
step:1383/2035 train_time:66979ms step_avg:48.43ms
step:1384/2035 train_time:67067ms step_avg:48.46ms
step:1385/2035 train_time:67155ms step_avg:48.49ms
step:1386/2035 train_time:67243ms step_avg:48.52ms
step:1387/2035 train_time:67331ms step_avg:48.54ms
step:1388/2035 train_time:67419ms step_avg:48.57ms
step:1389/2035 train_time:67507ms step_avg:48.60ms
step:1390/2035 train_time:67597ms step_avg:48.63ms
step:1391/2035 train_time:67685ms step_avg:48.66ms
step:1392/2035 train_time:67773ms step_avg:48.69ms
step:1393/2035 train_time:67861ms step_avg:48.72ms
step:1394/2035 train_time:67949ms step_avg:48.74ms
step:1395/2035 train_time:68038ms step_avg:48.77ms
step:1396/2035 train_time:68125ms step_avg:48.80ms
step:1397/2035 train_time:68213ms step_avg:48.83ms
step:1398/2035 train_time:68301ms step_avg:48.86ms
step:1399/2035 train_time:68389ms step_avg:48.88ms
step:1400/2035 train_time:68476ms step_avg:48.91ms
step:1401/2035 train_time:68565ms step_avg:48.94ms
step:1402/2035 train_time:68653ms step_avg:48.97ms
step:1403/2035 train_time:68742ms step_avg:49.00ms
step:1404/2035 train_time:68829ms step_avg:49.02ms
step:1405/2035 train_time:68918ms step_avg:49.05ms
step:1406/2035 train_time:69006ms step_avg:49.08ms
step:1407/2035 train_time:69095ms step_avg:49.11ms
step:1408/2035 train_time:69182ms step_avg:49.14ms
step:1409/2035 train_time:69271ms step_avg:49.16ms
step:1410/2035 train_time:69359ms step_avg:49.19ms
step:1411/2035 train_time:69447ms step_avg:49.22ms
step:1412/2035 train_time:69534ms step_avg:49.25ms
step:1413/2035 train_time:69623ms step_avg:49.27ms
step:1414/2035 train_time:69711ms step_avg:49.30ms
step:1415/2035 train_time:69800ms step_avg:49.33ms
step:1416/2035 train_time:69887ms step_avg:49.36ms
step:1417/2035 train_time:69976ms step_avg:49.38ms
step:1418/2035 train_time:70063ms step_avg:49.41ms
step:1419/2035 train_time:70151ms step_avg:49.44ms
step:1420/2035 train_time:70239ms step_avg:49.46ms
step:1421/2035 train_time:70327ms step_avg:49.49ms
step:1422/2035 train_time:70416ms step_avg:49.52ms
step:1423/2035 train_time:70505ms step_avg:49.55ms
step:1424/2035 train_time:70594ms step_avg:49.57ms
step:1425/2035 train_time:70682ms step_avg:49.60ms
step:1426/2035 train_time:70769ms step_avg:49.63ms
step:1427/2035 train_time:70857ms step_avg:49.65ms
step:1428/2035 train_time:70945ms step_avg:49.68ms
step:1429/2035 train_time:71033ms step_avg:49.71ms
step:1430/2035 train_time:71122ms step_avg:49.74ms
step:1431/2035 train_time:71210ms step_avg:49.76ms
step:1432/2035 train_time:71298ms step_avg:49.79ms
step:1433/2035 train_time:71386ms step_avg:49.82ms
step:1434/2035 train_time:71474ms step_avg:49.84ms
step:1435/2035 train_time:71563ms step_avg:49.87ms
step:1436/2035 train_time:71651ms step_avg:49.90ms
step:1437/2035 train_time:71740ms step_avg:49.92ms
step:1438/2035 train_time:71827ms step_avg:49.95ms
step:1439/2035 train_time:71915ms step_avg:49.98ms
step:1440/2035 train_time:72005ms step_avg:50.00ms
step:1441/2035 train_time:72093ms step_avg:50.03ms
step:1442/2035 train_time:72181ms step_avg:50.06ms
step:1443/2035 train_time:72268ms step_avg:50.08ms
step:1444/2035 train_time:72356ms step_avg:50.11ms
step:1445/2035 train_time:72446ms step_avg:50.14ms
step:1446/2035 train_time:72534ms step_avg:50.16ms
step:1447/2035 train_time:72623ms step_avg:50.19ms
step:1448/2035 train_time:72711ms step_avg:50.21ms
step:1449/2035 train_time:72800ms step_avg:50.24ms
step:1450/2035 train_time:72887ms step_avg:50.27ms
step:1451/2035 train_time:72976ms step_avg:50.29ms
step:1452/2035 train_time:73064ms step_avg:50.32ms
step:1453/2035 train_time:73152ms step_avg:50.35ms
step:1454/2035 train_time:73240ms step_avg:50.37ms
step:1455/2035 train_time:73329ms step_avg:50.40ms
step:1456/2035 train_time:73416ms step_avg:50.42ms
step:1457/2035 train_time:73505ms step_avg:50.45ms
step:1458/2035 train_time:73592ms step_avg:50.47ms
step:1459/2035 train_time:73680ms step_avg:50.50ms
step:1460/2035 train_time:73767ms step_avg:50.53ms
step:1461/2035 train_time:73856ms step_avg:50.55ms
step:1462/2035 train_time:73944ms step_avg:50.58ms
step:1463/2035 train_time:74033ms step_avg:50.60ms
step:1464/2035 train_time:74120ms step_avg:50.63ms
step:1465/2035 train_time:74209ms step_avg:50.65ms
step:1466/2035 train_time:74297ms step_avg:50.68ms
step:1467/2035 train_time:74385ms step_avg:50.71ms
step:1468/2035 train_time:74474ms step_avg:50.73ms
step:1469/2035 train_time:74562ms step_avg:50.76ms
step:1470/2035 train_time:74649ms step_avg:50.78ms
step:1471/2035 train_time:74737ms step_avg:50.81ms
step:1472/2035 train_time:74826ms step_avg:50.83ms
step:1473/2035 train_time:74914ms step_avg:50.86ms
step:1474/2035 train_time:75004ms step_avg:50.88ms
step:1475/2035 train_time:75092ms step_avg:50.91ms
step:1476/2035 train_time:75181ms step_avg:50.94ms
step:1477/2035 train_time:75269ms step_avg:50.96ms
step:1478/2035 train_time:75357ms step_avg:50.99ms
step:1479/2035 train_time:75446ms step_avg:51.01ms
step:1480/2035 train_time:75534ms step_avg:51.04ms
step:1481/2035 train_time:75622ms step_avg:51.06ms
step:1482/2035 train_time:75709ms step_avg:51.09ms
step:1483/2035 train_time:75797ms step_avg:51.11ms
step:1484/2035 train_time:75884ms step_avg:51.13ms
step:1485/2035 train_time:75973ms step_avg:51.16ms
step:1486/2035 train_time:76060ms step_avg:51.18ms
step:1487/2035 train_time:76148ms step_avg:51.21ms
step:1488/2035 train_time:76236ms step_avg:51.23ms
step:1489/2035 train_time:76324ms step_avg:51.26ms
step:1490/2035 train_time:76412ms step_avg:51.28ms
step:1491/2035 train_time:76501ms step_avg:51.31ms
step:1492/2035 train_time:76589ms step_avg:51.33ms
step:1493/2035 train_time:76677ms step_avg:51.36ms
step:1494/2035 train_time:76764ms step_avg:51.38ms
step:1495/2035 train_time:76853ms step_avg:51.41ms
step:1496/2035 train_time:76941ms step_avg:51.43ms
step:1497/2035 train_time:77029ms step_avg:51.46ms
step:1498/2035 train_time:77117ms step_avg:51.48ms
step:1499/2035 train_time:77206ms step_avg:51.50ms
step:1500/2035 train_time:77293ms step_avg:51.53ms
step:1500/2035 val_loss:3.4572 train_time:77384ms step_avg:51.59ms
step:1501/2035 train_time:77406ms step_avg:51.57ms
step:1502/2035 train_time:77475ms step_avg:51.58ms
step:1503/2035 train_time:77571ms step_avg:51.61ms
step:1504/2035 train_time:77659ms step_avg:51.64ms
step:1505/2035 train_time:77748ms step_avg:51.66ms
step:1506/2035 train_time:77834ms step_avg:51.68ms
step:1507/2035 train_time:77921ms step_avg:51.71ms
step:1508/2035 train_time:78008ms step_avg:51.73ms
step:1509/2035 train_time:78096ms step_avg:51.75ms
step:1510/2035 train_time:78182ms step_avg:51.78ms
step:1511/2035 train_time:78270ms step_avg:51.80ms
step:1512/2035 train_time:78359ms step_avg:51.83ms
step:1513/2035 train_time:78449ms step_avg:51.85ms
step:1514/2035 train_time:78541ms step_avg:51.88ms
step:1515/2035 train_time:78631ms step_avg:51.90ms
step:1516/2035 train_time:78719ms step_avg:51.93ms
step:1517/2035 train_time:78808ms step_avg:51.95ms
step:1518/2035 train_time:78895ms step_avg:51.97ms
step:1519/2035 train_time:78982ms step_avg:52.00ms
step:1520/2035 train_time:79069ms step_avg:52.02ms
step:1521/2035 train_time:79156ms step_avg:52.04ms
step:1522/2035 train_time:79244ms step_avg:52.07ms
step:1523/2035 train_time:79334ms step_avg:52.09ms
step:1524/2035 train_time:79421ms step_avg:52.11ms
step:1525/2035 train_time:79511ms step_avg:52.14ms
step:1526/2035 train_time:79600ms step_avg:52.16ms
step:1527/2035 train_time:79689ms step_avg:52.19ms
step:1528/2035 train_time:79777ms step_avg:52.21ms
step:1529/2035 train_time:79865ms step_avg:52.23ms
step:1530/2035 train_time:79953ms step_avg:52.26ms
step:1531/2035 train_time:80040ms step_avg:52.28ms
step:1532/2035 train_time:80127ms step_avg:52.30ms
step:1533/2035 train_time:80215ms step_avg:52.33ms
step:1534/2035 train_time:80303ms step_avg:52.35ms
step:1535/2035 train_time:80392ms step_avg:52.37ms
step:1536/2035 train_time:80480ms step_avg:52.40ms
step:1537/2035 train_time:80570ms step_avg:52.42ms
step:1538/2035 train_time:80659ms step_avg:52.44ms
step:1539/2035 train_time:80748ms step_avg:52.47ms
step:1540/2035 train_time:80837ms step_avg:52.49ms
step:1541/2035 train_time:80925ms step_avg:52.51ms
step:1542/2035 train_time:81013ms step_avg:52.54ms
step:1543/2035 train_time:81100ms step_avg:52.56ms
step:1544/2035 train_time:81187ms step_avg:52.58ms
step:1545/2035 train_time:81276ms step_avg:52.61ms
step:1546/2035 train_time:81364ms step_avg:52.63ms
step:1547/2035 train_time:81454ms step_avg:52.65ms
step:1548/2035 train_time:81542ms step_avg:52.68ms
step:1549/2035 train_time:81631ms step_avg:52.70ms
step:1550/2035 train_time:81719ms step_avg:52.72ms
step:1551/2035 train_time:81808ms step_avg:52.75ms
step:1552/2035 train_time:81896ms step_avg:52.77ms
step:1553/2035 train_time:81985ms step_avg:52.79ms
step:1554/2035 train_time:82072ms step_avg:52.81ms
step:1555/2035 train_time:82160ms step_avg:52.84ms
step:1556/2035 train_time:82249ms step_avg:52.86ms
step:1557/2035 train_time:82339ms step_avg:52.88ms
step:1558/2035 train_time:82427ms step_avg:52.91ms
step:1559/2035 train_time:82516ms step_avg:52.93ms
step:1560/2035 train_time:82604ms step_avg:52.95ms
step:1561/2035 train_time:82692ms step_avg:52.97ms
step:1562/2035 train_time:82780ms step_avg:53.00ms
step:1563/2035 train_time:82868ms step_avg:53.02ms
step:1564/2035 train_time:82957ms step_avg:53.04ms
step:1565/2035 train_time:83046ms step_avg:53.06ms
step:1566/2035 train_time:83134ms step_avg:53.09ms
step:1567/2035 train_time:83222ms step_avg:53.11ms
step:1568/2035 train_time:83310ms step_avg:53.13ms
step:1569/2035 train_time:83399ms step_avg:53.15ms
step:1570/2035 train_time:83488ms step_avg:53.18ms
step:1571/2035 train_time:83577ms step_avg:53.20ms
step:1572/2035 train_time:83665ms step_avg:53.22ms
step:1573/2035 train_time:83753ms step_avg:53.24ms
step:1574/2035 train_time:83840ms step_avg:53.27ms
step:1575/2035 train_time:83929ms step_avg:53.29ms
step:1576/2035 train_time:84017ms step_avg:53.31ms
step:1577/2035 train_time:84105ms step_avg:53.33ms
step:1578/2035 train_time:84193ms step_avg:53.35ms
step:1579/2035 train_time:84281ms step_avg:53.38ms
step:1580/2035 train_time:84370ms step_avg:53.40ms
step:1581/2035 train_time:84460ms step_avg:53.42ms
step:1582/2035 train_time:84549ms step_avg:53.44ms
step:1583/2035 train_time:84639ms step_avg:53.47ms
step:1584/2035 train_time:84727ms step_avg:53.49ms
step:1585/2035 train_time:84816ms step_avg:53.51ms
step:1586/2035 train_time:84903ms step_avg:53.53ms
step:1587/2035 train_time:84991ms step_avg:53.55ms
step:1588/2035 train_time:85079ms step_avg:53.58ms
step:1589/2035 train_time:85168ms step_avg:53.60ms
step:1590/2035 train_time:85257ms step_avg:53.62ms
step:1591/2035 train_time:85345ms step_avg:53.64ms
step:1592/2035 train_time:85434ms step_avg:53.66ms
step:1593/2035 train_time:85522ms step_avg:53.69ms
step:1594/2035 train_time:85611ms step_avg:53.71ms
step:1595/2035 train_time:85699ms step_avg:53.73ms
step:1596/2035 train_time:85787ms step_avg:53.75ms
step:1597/2035 train_time:85875ms step_avg:53.77ms
step:1598/2035 train_time:85962ms step_avg:53.79ms
step:1599/2035 train_time:86051ms step_avg:53.82ms
step:1600/2035 train_time:86138ms step_avg:53.84ms
step:1601/2035 train_time:86226ms step_avg:53.86ms
step:1602/2035 train_time:86315ms step_avg:53.88ms
step:1603/2035 train_time:86404ms step_avg:53.90ms
step:1604/2035 train_time:86493ms step_avg:53.92ms
step:1605/2035 train_time:86581ms step_avg:53.94ms
step:1606/2035 train_time:86669ms step_avg:53.97ms
step:1607/2035 train_time:86758ms step_avg:53.99ms
step:1608/2035 train_time:86846ms step_avg:54.01ms
step:1609/2035 train_time:86935ms step_avg:54.03ms
step:1610/2035 train_time:87022ms step_avg:54.05ms
step:1611/2035 train_time:87110ms step_avg:54.07ms
step:1612/2035 train_time:87197ms step_avg:54.09ms
step:1613/2035 train_time:87285ms step_avg:54.11ms
step:1614/2035 train_time:87373ms step_avg:54.13ms
step:1615/2035 train_time:87462ms step_avg:54.16ms
step:1616/2035 train_time:87550ms step_avg:54.18ms
step:1617/2035 train_time:87640ms step_avg:54.20ms
step:1618/2035 train_time:87728ms step_avg:54.22ms
step:1619/2035 train_time:87816ms step_avg:54.24ms
step:1620/2035 train_time:87903ms step_avg:54.26ms
step:1621/2035 train_time:87991ms step_avg:54.28ms
step:1622/2035 train_time:88078ms step_avg:54.30ms
step:1623/2035 train_time:88166ms step_avg:54.32ms
step:1624/2035 train_time:88255ms step_avg:54.34ms
step:1625/2035 train_time:88343ms step_avg:54.37ms
step:1626/2035 train_time:88431ms step_avg:54.39ms
step:1627/2035 train_time:88519ms step_avg:54.41ms
step:1628/2035 train_time:88607ms step_avg:54.43ms
step:1629/2035 train_time:88696ms step_avg:54.45ms
step:1630/2035 train_time:88783ms step_avg:54.47ms
step:1631/2035 train_time:88872ms step_avg:54.49ms
step:1632/2035 train_time:88959ms step_avg:54.51ms
step:1633/2035 train_time:89047ms step_avg:54.53ms
step:1634/2035 train_time:89134ms step_avg:54.55ms
step:1635/2035 train_time:89222ms step_avg:54.57ms
step:1636/2035 train_time:89310ms step_avg:54.59ms
step:1637/2035 train_time:89398ms step_avg:54.61ms
step:1638/2035 train_time:89486ms step_avg:54.63ms
step:1639/2035 train_time:89576ms step_avg:54.65ms
step:1640/2035 train_time:89664ms step_avg:54.67ms
step:1641/2035 train_time:89753ms step_avg:54.69ms
step:1642/2035 train_time:89841ms step_avg:54.71ms
step:1643/2035 train_time:89930ms step_avg:54.74ms
step:1644/2035 train_time:90017ms step_avg:54.75ms
step:1645/2035 train_time:90105ms step_avg:54.78ms
step:1646/2035 train_time:90192ms step_avg:54.79ms
step:1647/2035 train_time:90280ms step_avg:54.81ms
step:1648/2035 train_time:90368ms step_avg:54.84ms
step:1649/2035 train_time:90456ms step_avg:54.86ms
step:1650/2035 train_time:90544ms step_avg:54.87ms
step:1651/2035 train_time:90632ms step_avg:54.90ms
step:1652/2035 train_time:90720ms step_avg:54.92ms
step:1653/2035 train_time:90809ms step_avg:54.94ms
step:1654/2035 train_time:90898ms step_avg:54.96ms
step:1655/2035 train_time:90985ms step_avg:54.98ms
step:1656/2035 train_time:91073ms step_avg:55.00ms
step:1657/2035 train_time:91161ms step_avg:55.02ms
step:1658/2035 train_time:91249ms step_avg:55.04ms
step:1659/2035 train_time:91338ms step_avg:55.06ms
step:1660/2035 train_time:91426ms step_avg:55.08ms
step:1661/2035 train_time:91514ms step_avg:55.10ms
step:1662/2035 train_time:91602ms step_avg:55.12ms
step:1663/2035 train_time:91691ms step_avg:55.14ms
step:1664/2035 train_time:91779ms step_avg:55.16ms
step:1665/2035 train_time:91868ms step_avg:55.18ms
step:1666/2035 train_time:91956ms step_avg:55.20ms
step:1667/2035 train_time:92044ms step_avg:55.22ms
step:1668/2035 train_time:92132ms step_avg:55.23ms
step:1669/2035 train_time:92219ms step_avg:55.25ms
step:1670/2035 train_time:92307ms step_avg:55.27ms
step:1671/2035 train_time:92396ms step_avg:55.29ms
step:1672/2035 train_time:92483ms step_avg:55.31ms
step:1673/2035 train_time:92572ms step_avg:55.33ms
step:1674/2035 train_time:92660ms step_avg:55.35ms
step:1675/2035 train_time:92749ms step_avg:55.37ms
step:1676/2035 train_time:92837ms step_avg:55.39ms
step:1677/2035 train_time:92926ms step_avg:55.41ms
step:1678/2035 train_time:93014ms step_avg:55.43ms
step:1679/2035 train_time:93102ms step_avg:55.45ms
step:1680/2035 train_time:93190ms step_avg:55.47ms
step:1681/2035 train_time:93279ms step_avg:55.49ms
step:1682/2035 train_time:93367ms step_avg:55.51ms
step:1683/2035 train_time:93455ms step_avg:55.53ms
step:1684/2035 train_time:93542ms step_avg:55.55ms
step:1685/2035 train_time:93631ms step_avg:55.57ms
step:1686/2035 train_time:93718ms step_avg:55.59ms
step:1687/2035 train_time:93807ms step_avg:55.61ms
step:1688/2035 train_time:93895ms step_avg:55.62ms
step:1689/2035 train_time:93982ms step_avg:55.64ms
step:1690/2035 train_time:94070ms step_avg:55.66ms
step:1691/2035 train_time:94159ms step_avg:55.68ms
step:1692/2035 train_time:94247ms step_avg:55.70ms
step:1693/2035 train_time:94335ms step_avg:55.72ms
step:1694/2035 train_time:94422ms step_avg:55.74ms
step:1695/2035 train_time:94511ms step_avg:55.76ms
step:1696/2035 train_time:94599ms step_avg:55.78ms
step:1697/2035 train_time:94687ms step_avg:55.80ms
step:1698/2035 train_time:94775ms step_avg:55.82ms
step:1699/2035 train_time:94864ms step_avg:55.84ms
step:1700/2035 train_time:94952ms step_avg:55.85ms
step:1701/2035 train_time:95040ms step_avg:55.87ms
step:1702/2035 train_time:95128ms step_avg:55.89ms
step:1703/2035 train_time:95216ms step_avg:55.91ms
step:1704/2035 train_time:95304ms step_avg:55.93ms
step:1705/2035 train_time:95392ms step_avg:55.95ms
step:1706/2035 train_time:95479ms step_avg:55.97ms
step:1707/2035 train_time:95568ms step_avg:55.99ms
step:1708/2035 train_time:95656ms step_avg:56.00ms
step:1709/2035 train_time:95745ms step_avg:56.02ms
step:1710/2035 train_time:95833ms step_avg:56.04ms
step:1711/2035 train_time:95921ms step_avg:56.06ms
step:1712/2035 train_time:96008ms step_avg:56.08ms
step:1713/2035 train_time:96097ms step_avg:56.10ms
step:1714/2035 train_time:96186ms step_avg:56.12ms
step:1715/2035 train_time:96274ms step_avg:56.14ms
step:1716/2035 train_time:96362ms step_avg:56.16ms
step:1717/2035 train_time:96451ms step_avg:56.17ms
step:1718/2035 train_time:96539ms step_avg:56.19ms
step:1719/2035 train_time:96627ms step_avg:56.21ms
step:1720/2035 train_time:96715ms step_avg:56.23ms
step:1721/2035 train_time:96803ms step_avg:56.25ms
step:1722/2035 train_time:96891ms step_avg:56.27ms
step:1723/2035 train_time:96980ms step_avg:56.29ms
step:1724/2035 train_time:97069ms step_avg:56.30ms
step:1725/2035 train_time:97157ms step_avg:56.32ms
step:1726/2035 train_time:97245ms step_avg:56.34ms
step:1727/2035 train_time:97335ms step_avg:56.36ms
step:1728/2035 train_time:97422ms step_avg:56.38ms
step:1729/2035 train_time:97511ms step_avg:56.40ms
step:1730/2035 train_time:97599ms step_avg:56.42ms
step:1731/2035 train_time:97687ms step_avg:56.43ms
step:1732/2035 train_time:97775ms step_avg:56.45ms
step:1733/2035 train_time:97864ms step_avg:56.47ms
step:1734/2035 train_time:97951ms step_avg:56.49ms
step:1735/2035 train_time:98040ms step_avg:56.51ms
step:1736/2035 train_time:98127ms step_avg:56.52ms
step:1737/2035 train_time:98216ms step_avg:56.54ms
step:1738/2035 train_time:98305ms step_avg:56.56ms
step:1739/2035 train_time:98394ms step_avg:56.58ms
step:1740/2035 train_time:98481ms step_avg:56.60ms
step:1741/2035 train_time:98571ms step_avg:56.62ms
step:1742/2035 train_time:98659ms step_avg:56.64ms
step:1743/2035 train_time:98748ms step_avg:56.65ms
step:1744/2035 train_time:98836ms step_avg:56.67ms
step:1745/2035 train_time:98923ms step_avg:56.69ms
step:1746/2035 train_time:99011ms step_avg:56.71ms
step:1747/2035 train_time:99099ms step_avg:56.73ms
step:1748/2035 train_time:99187ms step_avg:56.74ms
step:1749/2035 train_time:99275ms step_avg:56.76ms
step:1750/2035 train_time:99363ms step_avg:56.78ms
step:1750/2035 val_loss:3.3620 train_time:99453ms step_avg:56.83ms
step:1751/2035 train_time:99474ms step_avg:56.81ms
step:1752/2035 train_time:99544ms step_avg:56.82ms
step:1753/2035 train_time:99636ms step_avg:56.84ms
step:1754/2035 train_time:99723ms step_avg:56.85ms
step:1755/2035 train_time:99811ms step_avg:56.87ms
step:1756/2035 train_time:99898ms step_avg:56.89ms
step:1757/2035 train_time:99985ms step_avg:56.91ms
step:1758/2035 train_time:100073ms step_avg:56.92ms
step:1759/2035 train_time:100160ms step_avg:56.94ms
step:1760/2035 train_time:100248ms step_avg:56.96ms
step:1761/2035 train_time:100335ms step_avg:56.98ms
step:1762/2035 train_time:100424ms step_avg:56.99ms
step:1763/2035 train_time:100515ms step_avg:57.01ms
step:1764/2035 train_time:100607ms step_avg:57.03ms
step:1765/2035 train_time:100695ms step_avg:57.05ms
step:1766/2035 train_time:100783ms step_avg:57.07ms
step:1767/2035 train_time:100871ms step_avg:57.09ms
step:1768/2035 train_time:100958ms step_avg:57.10ms
step:1769/2035 train_time:101045ms step_avg:57.12ms
step:1770/2035 train_time:101132ms step_avg:57.14ms
step:1771/2035 train_time:101219ms step_avg:57.15ms
step:1772/2035 train_time:101307ms step_avg:57.17ms
step:1773/2035 train_time:101395ms step_avg:57.19ms
step:1774/2035 train_time:101484ms step_avg:57.21ms
step:1775/2035 train_time:101574ms step_avg:57.22ms
step:1776/2035 train_time:101663ms step_avg:57.24ms
step:1777/2035 train_time:101751ms step_avg:57.26ms
step:1778/2035 train_time:101839ms step_avg:57.28ms
step:1779/2035 train_time:101927ms step_avg:57.29ms
step:1780/2035 train_time:102015ms step_avg:57.31ms
step:1781/2035 train_time:102101ms step_avg:57.33ms
step:1782/2035 train_time:102188ms step_avg:57.34ms
step:1783/2035 train_time:102276ms step_avg:57.36ms
step:1784/2035 train_time:102364ms step_avg:57.38ms
step:1785/2035 train_time:102453ms step_avg:57.40ms
step:1786/2035 train_time:102543ms step_avg:57.41ms
step:1787/2035 train_time:102633ms step_avg:57.43ms
step:1788/2035 train_time:102722ms step_avg:57.45ms
step:1789/2035 train_time:102810ms step_avg:57.47ms
step:1790/2035 train_time:102898ms step_avg:57.48ms
step:1791/2035 train_time:102985ms step_avg:57.50ms
step:1792/2035 train_time:103072ms step_avg:57.52ms
step:1793/2035 train_time:103160ms step_avg:57.53ms
step:1794/2035 train_time:103248ms step_avg:57.55ms
step:1795/2035 train_time:103336ms step_avg:57.57ms
step:1796/2035 train_time:103424ms step_avg:57.59ms
step:1797/2035 train_time:103512ms step_avg:57.60ms
step:1798/2035 train_time:103601ms step_avg:57.62ms
step:1799/2035 train_time:103692ms step_avg:57.64ms
step:1800/2035 train_time:103782ms step_avg:57.66ms
step:1801/2035 train_time:103870ms step_avg:57.67ms
step:1802/2035 train_time:103958ms step_avg:57.69ms
step:1803/2035 train_time:104045ms step_avg:57.71ms
step:1804/2035 train_time:104132ms step_avg:57.72ms
step:1805/2035 train_time:104219ms step_avg:57.74ms
step:1806/2035 train_time:104308ms step_avg:57.76ms
step:1807/2035 train_time:104396ms step_avg:57.77ms
step:1808/2035 train_time:104485ms step_avg:57.79ms
step:1809/2035 train_time:104573ms step_avg:57.81ms
step:1810/2035 train_time:104663ms step_avg:57.82ms
step:1811/2035 train_time:104751ms step_avg:57.84ms
step:1812/2035 train_time:104839ms step_avg:57.86ms
step:1813/2035 train_time:104926ms step_avg:57.87ms
step:1814/2035 train_time:105014ms step_avg:57.89ms
step:1815/2035 train_time:105102ms step_avg:57.91ms
step:1816/2035 train_time:105189ms step_avg:57.92ms
step:1817/2035 train_time:105278ms step_avg:57.94ms
step:1818/2035 train_time:105367ms step_avg:57.96ms
step:1819/2035 train_time:105455ms step_avg:57.97ms
step:1820/2035 train_time:105543ms step_avg:57.99ms
step:1821/2035 train_time:105631ms step_avg:58.01ms
step:1822/2035 train_time:105719ms step_avg:58.02ms
step:1823/2035 train_time:105809ms step_avg:58.04ms
step:1824/2035 train_time:105897ms step_avg:58.06ms
step:1825/2035 train_time:105984ms step_avg:58.07ms
step:1826/2035 train_time:106072ms step_avg:58.09ms
step:1827/2035 train_time:106160ms step_avg:58.11ms
step:1828/2035 train_time:106248ms step_avg:58.12ms
step:1829/2035 train_time:106336ms step_avg:58.14ms
step:1830/2035 train_time:106424ms step_avg:58.16ms
step:1831/2035 train_time:106513ms step_avg:58.17ms
step:1832/2035 train_time:106601ms step_avg:58.19ms
step:1833/2035 train_time:106690ms step_avg:58.20ms
step:1834/2035 train_time:106777ms step_avg:58.22ms
step:1835/2035 train_time:106866ms step_avg:58.24ms
step:1836/2035 train_time:106953ms step_avg:58.25ms
step:1837/2035 train_time:107041ms step_avg:58.27ms
step:1838/2035 train_time:107130ms step_avg:58.29ms
step:1839/2035 train_time:107217ms step_avg:58.30ms
step:1840/2035 train_time:107306ms step_avg:58.32ms
step:1841/2035 train_time:107394ms step_avg:58.33ms
step:1842/2035 train_time:107482ms step_avg:58.35ms
step:1843/2035 train_time:107571ms step_avg:58.37ms
step:1844/2035 train_time:107660ms step_avg:58.38ms
step:1845/2035 train_time:107748ms step_avg:58.40ms
step:1846/2035 train_time:107836ms step_avg:58.42ms
step:1847/2035 train_time:107924ms step_avg:58.43ms
step:1848/2035 train_time:108012ms step_avg:58.45ms
step:1849/2035 train_time:108100ms step_avg:58.46ms
step:1850/2035 train_time:108188ms step_avg:58.48ms
step:1851/2035 train_time:108276ms step_avg:58.50ms
step:1852/2035 train_time:108364ms step_avg:58.51ms
step:1853/2035 train_time:108452ms step_avg:58.53ms
step:1854/2035 train_time:108541ms step_avg:58.54ms
step:1855/2035 train_time:108630ms step_avg:58.56ms
step:1856/2035 train_time:108718ms step_avg:58.58ms
step:1857/2035 train_time:108806ms step_avg:58.59ms
step:1858/2035 train_time:108894ms step_avg:58.61ms
step:1859/2035 train_time:108982ms step_avg:58.62ms
step:1860/2035 train_time:109070ms step_avg:58.64ms
step:1861/2035 train_time:109158ms step_avg:58.66ms
step:1862/2035 train_time:109246ms step_avg:58.67ms
step:1863/2035 train_time:109333ms step_avg:58.69ms
step:1864/2035 train_time:109421ms step_avg:58.70ms
step:1865/2035 train_time:109511ms step_avg:58.72ms
step:1866/2035 train_time:109600ms step_avg:58.74ms
step:1867/2035 train_time:109689ms step_avg:58.75ms
step:1868/2035 train_time:109777ms step_avg:58.77ms
step:1869/2035 train_time:109866ms step_avg:58.78ms
step:1870/2035 train_time:109953ms step_avg:58.80ms
step:1871/2035 train_time:110043ms step_avg:58.81ms
step:1872/2035 train_time:110130ms step_avg:58.83ms
step:1873/2035 train_time:110218ms step_avg:58.85ms
step:1874/2035 train_time:110305ms step_avg:58.86ms
step:1875/2035 train_time:110393ms step_avg:58.88ms
step:1876/2035 train_time:110482ms step_avg:58.89ms
step:1877/2035 train_time:110572ms step_avg:58.91ms
step:1878/2035 train_time:110660ms step_avg:58.92ms
step:1879/2035 train_time:110749ms step_avg:58.94ms
step:1880/2035 train_time:110837ms step_avg:58.96ms
step:1881/2035 train_time:110924ms step_avg:58.97ms
step:1882/2035 train_time:111012ms step_avg:58.99ms
step:1883/2035 train_time:111100ms step_avg:59.00ms
step:1884/2035 train_time:111187ms step_avg:59.02ms
step:1885/2035 train_time:111274ms step_avg:59.03ms
step:1886/2035 train_time:111362ms step_avg:59.05ms
step:1887/2035 train_time:111450ms step_avg:59.06ms
step:1888/2035 train_time:111538ms step_avg:59.08ms
step:1889/2035 train_time:111626ms step_avg:59.09ms
step:1890/2035 train_time:111713ms step_avg:59.11ms
step:1891/2035 train_time:111802ms step_avg:59.12ms
step:1892/2035 train_time:111890ms step_avg:59.14ms
step:1893/2035 train_time:111978ms step_avg:59.15ms
step:1894/2035 train_time:112068ms step_avg:59.17ms
step:1895/2035 train_time:112155ms step_avg:59.18ms
step:1896/2035 train_time:112243ms step_avg:59.20ms
step:1897/2035 train_time:112331ms step_avg:59.22ms
step:1898/2035 train_time:112419ms step_avg:59.23ms
step:1899/2035 train_time:112509ms step_avg:59.25ms
step:1900/2035 train_time:112597ms step_avg:59.26ms
step:1901/2035 train_time:112685ms step_avg:59.28ms
step:1902/2035 train_time:112772ms step_avg:59.29ms
step:1903/2035 train_time:112860ms step_avg:59.31ms
step:1904/2035 train_time:112948ms step_avg:59.32ms
step:1905/2035 train_time:113036ms step_avg:59.34ms
step:1906/2035 train_time:113124ms step_avg:59.35ms
step:1907/2035 train_time:113212ms step_avg:59.37ms
step:1908/2035 train_time:113299ms step_avg:59.38ms
step:1909/2035 train_time:113388ms step_avg:59.40ms
step:1910/2035 train_time:113476ms step_avg:59.41ms
step:1911/2035 train_time:113565ms step_avg:59.43ms
step:1912/2035 train_time:113652ms step_avg:59.44ms
step:1913/2035 train_time:113741ms step_avg:59.46ms
step:1914/2035 train_time:113828ms step_avg:59.47ms
step:1915/2035 train_time:113915ms step_avg:59.49ms
step:1916/2035 train_time:114004ms step_avg:59.50ms
step:1917/2035 train_time:114092ms step_avg:59.52ms
step:1918/2035 train_time:114180ms step_avg:59.53ms
step:1919/2035 train_time:114269ms step_avg:59.55ms
step:1920/2035 train_time:114357ms step_avg:59.56ms
step:1921/2035 train_time:114445ms step_avg:59.58ms
step:1922/2035 train_time:114533ms step_avg:59.59ms
step:1923/2035 train_time:114621ms step_avg:59.61ms
step:1924/2035 train_time:114709ms step_avg:59.62ms
step:1925/2035 train_time:114797ms step_avg:59.63ms
step:1926/2035 train_time:114885ms step_avg:59.65ms
step:1927/2035 train_time:114973ms step_avg:59.66ms
step:1928/2035 train_time:115062ms step_avg:59.68ms
step:1929/2035 train_time:115151ms step_avg:59.69ms
step:1930/2035 train_time:115240ms step_avg:59.71ms
step:1931/2035 train_time:115328ms step_avg:59.72ms
step:1932/2035 train_time:115415ms step_avg:59.74ms
step:1933/2035 train_time:115505ms step_avg:59.75ms
step:1934/2035 train_time:115592ms step_avg:59.77ms
step:1935/2035 train_time:115681ms step_avg:59.78ms
step:1936/2035 train_time:115770ms step_avg:59.80ms
step:1937/2035 train_time:115858ms step_avg:59.81ms
step:1938/2035 train_time:115946ms step_avg:59.83ms
step:1939/2035 train_time:116034ms step_avg:59.84ms
step:1940/2035 train_time:116122ms step_avg:59.86ms
step:1941/2035 train_time:116211ms step_avg:59.87ms
step:1942/2035 train_time:116299ms step_avg:59.89ms
step:1943/2035 train_time:116388ms step_avg:59.90ms
step:1944/2035 train_time:116475ms step_avg:59.92ms
step:1945/2035 train_time:116564ms step_avg:59.93ms
step:1946/2035 train_time:116651ms step_avg:59.94ms
step:1947/2035 train_time:116740ms step_avg:59.96ms
step:1948/2035 train_time:116829ms step_avg:59.97ms
step:1949/2035 train_time:116917ms step_avg:59.99ms
step:1950/2035 train_time:117005ms step_avg:60.00ms
step:1951/2035 train_time:117094ms step_avg:60.02ms
step:1952/2035 train_time:117182ms step_avg:60.03ms
step:1953/2035 train_time:117271ms step_avg:60.05ms
step:1954/2035 train_time:117359ms step_avg:60.06ms
step:1955/2035 train_time:117447ms step_avg:60.08ms
step:1956/2035 train_time:117535ms step_avg:60.09ms
step:1957/2035 train_time:117623ms step_avg:60.10ms
step:1958/2035 train_time:117711ms step_avg:60.12ms
step:1959/2035 train_time:117800ms step_avg:60.13ms
step:1960/2035 train_time:117888ms step_avg:60.15ms
step:1961/2035 train_time:117976ms step_avg:60.16ms
step:1962/2035 train_time:118064ms step_avg:60.18ms
step:1963/2035 train_time:118152ms step_avg:60.19ms
step:1964/2035 train_time:118239ms step_avg:60.20ms
step:1965/2035 train_time:118327ms step_avg:60.22ms
step:1966/2035 train_time:118415ms step_avg:60.23ms
step:1967/2035 train_time:118505ms step_avg:60.25ms
step:1968/2035 train_time:118592ms step_avg:60.26ms
step:1969/2035 train_time:118681ms step_avg:60.27ms
step:1970/2035 train_time:118770ms step_avg:60.29ms
step:1971/2035 train_time:118858ms step_avg:60.30ms
step:1972/2035 train_time:118947ms step_avg:60.32ms
step:1973/2035 train_time:119035ms step_avg:60.33ms
step:1974/2035 train_time:119123ms step_avg:60.35ms
step:1975/2035 train_time:119211ms step_avg:60.36ms
step:1976/2035 train_time:119300ms step_avg:60.37ms
step:1977/2035 train_time:119388ms step_avg:60.39ms
step:1978/2035 train_time:119475ms step_avg:60.40ms
step:1979/2035 train_time:119564ms step_avg:60.42ms
step:1980/2035 train_time:119652ms step_avg:60.43ms
step:1981/2035 train_time:119741ms step_avg:60.44ms
step:1982/2035 train_time:119829ms step_avg:60.46ms
step:1983/2035 train_time:119917ms step_avg:60.47ms
step:1984/2035 train_time:120006ms step_avg:60.49ms
step:1985/2035 train_time:120094ms step_avg:60.50ms
step:1986/2035 train_time:120182ms step_avg:60.51ms
step:1987/2035 train_time:120272ms step_avg:60.53ms
step:1988/2035 train_time:120360ms step_avg:60.54ms
step:1989/2035 train_time:120448ms step_avg:60.56ms
step:1990/2035 train_time:120535ms step_avg:60.57ms
step:1991/2035 train_time:120624ms step_avg:60.58ms
step:1992/2035 train_time:120711ms step_avg:60.60ms
step:1993/2035 train_time:120799ms step_avg:60.61ms
step:1994/2035 train_time:120887ms step_avg:60.63ms
step:1995/2035 train_time:120975ms step_avg:60.64ms
step:1996/2035 train_time:121063ms step_avg:60.65ms
step:1997/2035 train_time:121152ms step_avg:60.67ms
step:1998/2035 train_time:121240ms step_avg:60.68ms
step:1999/2035 train_time:121329ms step_avg:60.69ms
step:2000/2035 train_time:121417ms step_avg:60.71ms
step:2000/2035 val_loss:3.2890 train_time:121507ms step_avg:60.75ms
step:2001/2035 train_time:121527ms step_avg:60.73ms
step:2002/2035 train_time:121601ms step_avg:60.74ms
step:2003/2035 train_time:121693ms step_avg:60.76ms
step:2004/2035 train_time:121781ms step_avg:60.77ms
step:2005/2035 train_time:121869ms step_avg:60.78ms
step:2006/2035 train_time:121957ms step_avg:60.80ms
step:2007/2035 train_time:122044ms step_avg:60.81ms
step:2008/2035 train_time:122132ms step_avg:60.82ms
step:2009/2035 train_time:122219ms step_avg:60.84ms
step:2010/2035 train_time:122307ms step_avg:60.85ms
step:2011/2035 train_time:122395ms step_avg:60.86ms
step:2012/2035 train_time:122486ms step_avg:60.88ms
step:2013/2035 train_time:122578ms step_avg:60.89ms
step:2014/2035 train_time:122669ms step_avg:60.91ms
step:2015/2035 train_time:122759ms step_avg:60.92ms
step:2016/2035 train_time:122847ms step_avg:60.94ms
step:2017/2035 train_time:122935ms step_avg:60.95ms
step:2018/2035 train_time:123022ms step_avg:60.96ms
step:2019/2035 train_time:123109ms step_avg:60.98ms
step:2020/2035 train_time:123196ms step_avg:60.99ms
step:2021/2035 train_time:123284ms step_avg:61.00ms
step:2022/2035 train_time:123372ms step_avg:61.01ms
step:2023/2035 train_time:123460ms step_avg:61.03ms
step:2024/2035 train_time:123550ms step_avg:61.04ms
step:2025/2035 train_time:123641ms step_avg:61.06ms
step:2026/2035 train_time:123730ms step_avg:61.07ms
step:2027/2035 train_time:123819ms step_avg:61.08ms
step:2028/2035 train_time:123907ms step_avg:61.10ms
step:2029/2035 train_time:123996ms step_avg:61.11ms
step:2030/2035 train_time:124084ms step_avg:61.13ms
step:2031/2035 train_time:124172ms step_avg:61.14ms
step:2032/2035 train_time:124259ms step_avg:61.15ms
step:2033/2035 train_time:124347ms step_avg:61.16ms
step:2034/2035 train_time:124437ms step_avg:61.18ms
step:2035/2035 train_time:124525ms step_avg:61.19ms
step:2035/2035 val_loss:3.2817 train_time:124617ms step_avg:61.24ms
peak memory allocated: 29634 MiB reserved: 39616 MiB

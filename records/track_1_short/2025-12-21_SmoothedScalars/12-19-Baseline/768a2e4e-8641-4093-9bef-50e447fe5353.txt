import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    logs_dir: str = f"logs/12-19-Baseline"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 14:46:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   26C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     30173      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A     30174      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     30175      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     30176      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     30177      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     30178      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     30179      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     30180      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A     30174      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A     30175      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A     30176      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A     30177      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A     30178      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A     30179      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A     30180      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8344 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:76ms step_avg:75.63ms
step:2/2035 train_time:101ms step_avg:50.51ms
step:3/2035 train_time:128ms step_avg:42.71ms
step:4/2035 train_time:161ms step_avg:40.22ms
step:5/2035 train_time:194ms step_avg:38.72ms
step:6/2035 train_time:271ms step_avg:45.15ms
step:7/2035 train_time:304ms step_avg:43.43ms
step:8/2035 train_time:337ms step_avg:42.15ms
step:9/2035 train_time:370ms step_avg:41.10ms
step:10/2035 train_time:403ms step_avg:40.29ms
step:11/2035 train_time:436ms step_avg:39.62ms
step:12/2035 train_time:469ms step_avg:39.10ms
step:13/2035 train_time:502ms step_avg:38.64ms
step:14/2035 train_time:536ms step_avg:38.27ms
step:15/2035 train_time:569ms step_avg:37.93ms
step:16/2035 train_time:602ms step_avg:37.64ms
step:17/2035 train_time:635ms step_avg:37.37ms
step:18/2035 train_time:669ms step_avg:37.15ms
step:19/2035 train_time:702ms step_avg:36.93ms
step:20/2035 train_time:735ms step_avg:36.75ms
step:21/2035 train_time:768ms step_avg:36.58ms
step:22/2035 train_time:801ms step_avg:36.43ms
step:23/2035 train_time:834ms step_avg:36.27ms
step:24/2035 train_time:868ms step_avg:36.16ms
step:25/2035 train_time:901ms step_avg:36.03ms
step:26/2035 train_time:934ms step_avg:35.92ms
step:27/2035 train_time:967ms step_avg:35.83ms
step:28/2035 train_time:1001ms step_avg:35.74ms
step:29/2035 train_time:1034ms step_avg:35.65ms
step:30/2035 train_time:1067ms step_avg:35.57ms
step:31/2035 train_time:1100ms step_avg:35.48ms
step:32/2035 train_time:1133ms step_avg:35.41ms
step:33/2035 train_time:1167ms step_avg:35.35ms
step:34/2035 train_time:1200ms step_avg:35.30ms
step:35/2035 train_time:1234ms step_avg:35.26ms
step:36/2035 train_time:1268ms step_avg:35.23ms
step:37/2035 train_time:1302ms step_avg:35.18ms
step:38/2035 train_time:1335ms step_avg:35.14ms
step:39/2035 train_time:1369ms step_avg:35.10ms
step:40/2035 train_time:1402ms step_avg:35.06ms
step:41/2035 train_time:1436ms step_avg:35.02ms
step:42/2035 train_time:1469ms step_avg:34.99ms
step:43/2035 train_time:1502ms step_avg:34.94ms
step:44/2035 train_time:1536ms step_avg:34.90ms
step:45/2035 train_time:1569ms step_avg:34.87ms
step:46/2035 train_time:1603ms step_avg:34.84ms
step:47/2035 train_time:1636ms step_avg:34.80ms
step:48/2035 train_time:1669ms step_avg:34.77ms
step:49/2035 train_time:1702ms step_avg:34.74ms
step:50/2035 train_time:1735ms step_avg:34.71ms
step:51/2035 train_time:1768ms step_avg:34.67ms
step:52/2035 train_time:1801ms step_avg:34.64ms
step:53/2035 train_time:1834ms step_avg:34.61ms
step:54/2035 train_time:1868ms step_avg:34.59ms
step:55/2035 train_time:1901ms step_avg:34.56ms
step:56/2035 train_time:1934ms step_avg:34.54ms
step:57/2035 train_time:1967ms step_avg:34.51ms
step:58/2035 train_time:2001ms step_avg:34.49ms
step:59/2035 train_time:2034ms step_avg:34.47ms
step:60/2035 train_time:2067ms step_avg:34.45ms
step:61/2035 train_time:2100ms step_avg:34.43ms
step:62/2035 train_time:2133ms step_avg:34.41ms
step:63/2035 train_time:2167ms step_avg:34.39ms
step:64/2035 train_time:2200ms step_avg:34.38ms
step:65/2035 train_time:2233ms step_avg:34.36ms
step:66/2035 train_time:2267ms step_avg:34.35ms
step:67/2035 train_time:2301ms step_avg:34.34ms
step:68/2035 train_time:2335ms step_avg:34.33ms
step:69/2035 train_time:2368ms step_avg:34.32ms
step:70/2035 train_time:2402ms step_avg:34.31ms
step:71/2035 train_time:2435ms step_avg:34.29ms
step:72/2035 train_time:2468ms step_avg:34.28ms
step:73/2035 train_time:2502ms step_avg:34.27ms
step:74/2035 train_time:2535ms step_avg:34.26ms
step:75/2035 train_time:2568ms step_avg:34.24ms
step:76/2035 train_time:2602ms step_avg:34.23ms
step:77/2035 train_time:2635ms step_avg:34.22ms
step:78/2035 train_time:2668ms step_avg:34.21ms
step:79/2035 train_time:2702ms step_avg:34.20ms
step:80/2035 train_time:2735ms step_avg:34.19ms
step:81/2035 train_time:2768ms step_avg:34.17ms
step:82/2035 train_time:2801ms step_avg:34.16ms
step:83/2035 train_time:2834ms step_avg:34.15ms
step:84/2035 train_time:2867ms step_avg:34.13ms
step:85/2035 train_time:2901ms step_avg:34.12ms
step:86/2035 train_time:2934ms step_avg:34.11ms
step:87/2035 train_time:2967ms step_avg:34.10ms
step:88/2035 train_time:3000ms step_avg:34.09ms
step:89/2035 train_time:3033ms step_avg:34.08ms
step:90/2035 train_time:3067ms step_avg:34.08ms
step:91/2035 train_time:3100ms step_avg:34.06ms
step:92/2035 train_time:3133ms step_avg:34.05ms
step:93/2035 train_time:3166ms step_avg:34.04ms
step:94/2035 train_time:3199ms step_avg:34.04ms
step:95/2035 train_time:3232ms step_avg:34.03ms
step:96/2035 train_time:3266ms step_avg:34.02ms
step:97/2035 train_time:3299ms step_avg:34.01ms
step:98/2035 train_time:3332ms step_avg:34.00ms
step:99/2035 train_time:3366ms step_avg:34.00ms
step:100/2035 train_time:3399ms step_avg:33.99ms
step:101/2035 train_time:3433ms step_avg:33.99ms
step:102/2035 train_time:3466ms step_avg:33.98ms
step:103/2035 train_time:3499ms step_avg:33.97ms
step:104/2035 train_time:3533ms step_avg:33.97ms
step:105/2035 train_time:3567ms step_avg:33.97ms
step:106/2035 train_time:3600ms step_avg:33.96ms
step:107/2035 train_time:3633ms step_avg:33.96ms
step:108/2035 train_time:3667ms step_avg:33.95ms
step:109/2035 train_time:3700ms step_avg:33.94ms
step:110/2035 train_time:3733ms step_avg:33.93ms
step:111/2035 train_time:3766ms step_avg:33.93ms
step:112/2035 train_time:3800ms step_avg:33.92ms
step:113/2035 train_time:3832ms step_avg:33.92ms
step:114/2035 train_time:3865ms step_avg:33.91ms
step:115/2035 train_time:3899ms step_avg:33.90ms
step:116/2035 train_time:3932ms step_avg:33.89ms
step:117/2035 train_time:3965ms step_avg:33.89ms
step:118/2035 train_time:3998ms step_avg:33.89ms
step:119/2035 train_time:4031ms step_avg:33.88ms
step:120/2035 train_time:4064ms step_avg:33.87ms
step:121/2035 train_time:4097ms step_avg:33.86ms
step:122/2035 train_time:4131ms step_avg:33.86ms
step:123/2035 train_time:4164ms step_avg:33.85ms
step:124/2035 train_time:4197ms step_avg:33.85ms
step:125/2035 train_time:4230ms step_avg:33.84ms
step:126/2035 train_time:4263ms step_avg:33.84ms
step:127/2035 train_time:4296ms step_avg:33.83ms
step:128/2035 train_time:4330ms step_avg:33.83ms
step:129/2035 train_time:4363ms step_avg:33.82ms
step:130/2035 train_time:4396ms step_avg:33.82ms
step:131/2035 train_time:4429ms step_avg:33.81ms
step:132/2035 train_time:4463ms step_avg:33.81ms
step:133/2035 train_time:4496ms step_avg:33.81ms
step:134/2035 train_time:4530ms step_avg:33.80ms
step:135/2035 train_time:4563ms step_avg:33.80ms
step:136/2035 train_time:4596ms step_avg:33.80ms
step:137/2035 train_time:4630ms step_avg:33.79ms
step:138/2035 train_time:4663ms step_avg:33.79ms
step:139/2035 train_time:4696ms step_avg:33.78ms
step:140/2035 train_time:4729ms step_avg:33.78ms
step:141/2035 train_time:4762ms step_avg:33.77ms
step:142/2035 train_time:4795ms step_avg:33.77ms
step:143/2035 train_time:4828ms step_avg:33.77ms
step:144/2035 train_time:4862ms step_avg:33.76ms
step:145/2035 train_time:4895ms step_avg:33.76ms
step:146/2035 train_time:4928ms step_avg:33.76ms
step:147/2035 train_time:4961ms step_avg:33.75ms
step:148/2035 train_time:4995ms step_avg:33.75ms
step:149/2035 train_time:5027ms step_avg:33.74ms
step:150/2035 train_time:5061ms step_avg:33.74ms
step:151/2035 train_time:5094ms step_avg:33.73ms
step:152/2035 train_time:5128ms step_avg:33.73ms
step:153/2035 train_time:5160ms step_avg:33.73ms
step:154/2035 train_time:5194ms step_avg:33.73ms
step:155/2035 train_time:5227ms step_avg:33.72ms
step:156/2035 train_time:5260ms step_avg:33.72ms
step:157/2035 train_time:5293ms step_avg:33.71ms
step:158/2035 train_time:5327ms step_avg:33.71ms
step:159/2035 train_time:5359ms step_avg:33.71ms
step:160/2035 train_time:5393ms step_avg:33.70ms
step:161/2035 train_time:5426ms step_avg:33.70ms
step:162/2035 train_time:5459ms step_avg:33.70ms
step:163/2035 train_time:5492ms step_avg:33.69ms
step:164/2035 train_time:5526ms step_avg:33.69ms
step:165/2035 train_time:5558ms step_avg:33.69ms
step:166/2035 train_time:5592ms step_avg:33.69ms
step:167/2035 train_time:5625ms step_avg:33.68ms
step:168/2035 train_time:5658ms step_avg:33.68ms
step:169/2035 train_time:5691ms step_avg:33.68ms
step:170/2035 train_time:5725ms step_avg:33.67ms
step:171/2035 train_time:5757ms step_avg:33.67ms
step:172/2035 train_time:5791ms step_avg:33.67ms
step:173/2035 train_time:5824ms step_avg:33.66ms
step:174/2035 train_time:5857ms step_avg:33.66ms
step:175/2035 train_time:5890ms step_avg:33.66ms
step:176/2035 train_time:5924ms step_avg:33.66ms
step:177/2035 train_time:5956ms step_avg:33.65ms
step:178/2035 train_time:5990ms step_avg:33.65ms
step:179/2035 train_time:6023ms step_avg:33.65ms
step:180/2035 train_time:6056ms step_avg:33.65ms
step:181/2035 train_time:6090ms step_avg:33.64ms
step:182/2035 train_time:6123ms step_avg:33.64ms
step:183/2035 train_time:6156ms step_avg:33.64ms
step:184/2035 train_time:6189ms step_avg:33.64ms
step:185/2035 train_time:6222ms step_avg:33.63ms
step:186/2035 train_time:6256ms step_avg:33.63ms
step:187/2035 train_time:6288ms step_avg:33.63ms
step:188/2035 train_time:6321ms step_avg:33.62ms
step:189/2035 train_time:6354ms step_avg:33.62ms
step:190/2035 train_time:6387ms step_avg:33.62ms
step:191/2035 train_time:6420ms step_avg:33.61ms
step:192/2035 train_time:6454ms step_avg:33.61ms
step:193/2035 train_time:6487ms step_avg:33.61ms
step:194/2035 train_time:6520ms step_avg:33.61ms
step:195/2035 train_time:6553ms step_avg:33.60ms
step:196/2035 train_time:6586ms step_avg:33.60ms
step:197/2035 train_time:6619ms step_avg:33.60ms
step:198/2035 train_time:6652ms step_avg:33.60ms
step:199/2035 train_time:6685ms step_avg:33.59ms
step:200/2035 train_time:6718ms step_avg:33.59ms
step:201/2035 train_time:6751ms step_avg:33.59ms
step:202/2035 train_time:6784ms step_avg:33.58ms
step:203/2035 train_time:6817ms step_avg:33.58ms
step:204/2035 train_time:6850ms step_avg:33.58ms
step:205/2035 train_time:6883ms step_avg:33.58ms
step:206/2035 train_time:6916ms step_avg:33.58ms
step:207/2035 train_time:6950ms step_avg:33.58ms
step:208/2035 train_time:6984ms step_avg:33.57ms
step:209/2035 train_time:7017ms step_avg:33.57ms
step:210/2035 train_time:7050ms step_avg:33.57ms
step:211/2035 train_time:7083ms step_avg:33.57ms
step:212/2035 train_time:7117ms step_avg:33.57ms
step:213/2035 train_time:7150ms step_avg:33.57ms
step:214/2035 train_time:7183ms step_avg:33.56ms
step:215/2035 train_time:7216ms step_avg:33.56ms
step:216/2035 train_time:7249ms step_avg:33.56ms
step:217/2035 train_time:7282ms step_avg:33.56ms
step:218/2035 train_time:7316ms step_avg:33.56ms
step:219/2035 train_time:7349ms step_avg:33.56ms
step:220/2035 train_time:7382ms step_avg:33.56ms
step:221/2035 train_time:7415ms step_avg:33.55ms
step:222/2035 train_time:7448ms step_avg:33.55ms
step:223/2035 train_time:7481ms step_avg:33.55ms
step:224/2035 train_time:7515ms step_avg:33.55ms
step:225/2035 train_time:7548ms step_avg:33.55ms
step:226/2035 train_time:7581ms step_avg:33.55ms
step:227/2035 train_time:7614ms step_avg:33.54ms
step:228/2035 train_time:7647ms step_avg:33.54ms
step:229/2035 train_time:7680ms step_avg:33.54ms
step:230/2035 train_time:7714ms step_avg:33.54ms
step:231/2035 train_time:7747ms step_avg:33.54ms
step:232/2035 train_time:7780ms step_avg:33.54ms
step:233/2035 train_time:7813ms step_avg:33.53ms
step:234/2035 train_time:7847ms step_avg:33.53ms
step:235/2035 train_time:7880ms step_avg:33.53ms
step:236/2035 train_time:7913ms step_avg:33.53ms
step:237/2035 train_time:7946ms step_avg:33.53ms
step:238/2035 train_time:7979ms step_avg:33.53ms
step:239/2035 train_time:8012ms step_avg:33.52ms
step:240/2035 train_time:8045ms step_avg:33.52ms
step:241/2035 train_time:8078ms step_avg:33.52ms
step:242/2035 train_time:8111ms step_avg:33.52ms
step:243/2035 train_time:8144ms step_avg:33.52ms
step:244/2035 train_time:8178ms step_avg:33.51ms
step:245/2035 train_time:8211ms step_avg:33.51ms
step:246/2035 train_time:8244ms step_avg:33.51ms
step:247/2035 train_time:8277ms step_avg:33.51ms
step:248/2035 train_time:8310ms step_avg:33.51ms
step:249/2035 train_time:8343ms step_avg:33.51ms
step:250/2035 train_time:8376ms step_avg:33.50ms
step:250/2035 val_loss:4.2578 train_time:8411ms step_avg:33.65ms
step:251/2035 train_time:8432ms step_avg:33.59ms
step:252/2035 train_time:8451ms step_avg:33.54ms
step:253/2035 train_time:8480ms step_avg:33.52ms
step:254/2035 train_time:8514ms step_avg:33.52ms
step:255/2035 train_time:8549ms step_avg:33.53ms
step:256/2035 train_time:8584ms step_avg:33.53ms
step:257/2035 train_time:8618ms step_avg:33.53ms
step:258/2035 train_time:8652ms step_avg:33.54ms
step:259/2035 train_time:8686ms step_avg:33.54ms
step:260/2035 train_time:8719ms step_avg:33.54ms
step:261/2035 train_time:8753ms step_avg:33.54ms
step:262/2035 train_time:8786ms step_avg:33.53ms
step:263/2035 train_time:8819ms step_avg:33.53ms
step:264/2035 train_time:8852ms step_avg:33.53ms
step:265/2035 train_time:8885ms step_avg:33.53ms
step:266/2035 train_time:8918ms step_avg:33.53ms
step:267/2035 train_time:8951ms step_avg:33.52ms
step:268/2035 train_time:8984ms step_avg:33.52ms
step:269/2035 train_time:9017ms step_avg:33.52ms
step:270/2035 train_time:9050ms step_avg:33.52ms
step:271/2035 train_time:9083ms step_avg:33.52ms
step:272/2035 train_time:9116ms step_avg:33.51ms
step:273/2035 train_time:9149ms step_avg:33.51ms
step:274/2035 train_time:9182ms step_avg:33.51ms
step:275/2035 train_time:9215ms step_avg:33.51ms
step:276/2035 train_time:9248ms step_avg:33.51ms
step:277/2035 train_time:9281ms step_avg:33.50ms
step:278/2035 train_time:9314ms step_avg:33.50ms
step:279/2035 train_time:9347ms step_avg:33.50ms
step:280/2035 train_time:9380ms step_avg:33.50ms
step:281/2035 train_time:9413ms step_avg:33.50ms
step:282/2035 train_time:9447ms step_avg:33.50ms
step:283/2035 train_time:9480ms step_avg:33.50ms
step:284/2035 train_time:9514ms step_avg:33.50ms
step:285/2035 train_time:9547ms step_avg:33.50ms
step:286/2035 train_time:9580ms step_avg:33.50ms
step:287/2035 train_time:9614ms step_avg:33.50ms
step:288/2035 train_time:9648ms step_avg:33.50ms
step:289/2035 train_time:9681ms step_avg:33.50ms
step:290/2035 train_time:9715ms step_avg:33.50ms
step:291/2035 train_time:9748ms step_avg:33.50ms
step:292/2035 train_time:9781ms step_avg:33.50ms
step:293/2035 train_time:9814ms step_avg:33.50ms
step:294/2035 train_time:9847ms step_avg:33.49ms
step:295/2035 train_time:9880ms step_avg:33.49ms
step:296/2035 train_time:9913ms step_avg:33.49ms
step:297/2035 train_time:9946ms step_avg:33.49ms
step:298/2035 train_time:9979ms step_avg:33.49ms
step:299/2035 train_time:10012ms step_avg:33.48ms
step:300/2035 train_time:10045ms step_avg:33.48ms
step:301/2035 train_time:10078ms step_avg:33.48ms
step:302/2035 train_time:10111ms step_avg:33.48ms
step:303/2035 train_time:10144ms step_avg:33.48ms
step:304/2035 train_time:10177ms step_avg:33.48ms
step:305/2035 train_time:10210ms step_avg:33.47ms
step:306/2035 train_time:10243ms step_avg:33.47ms
step:307/2035 train_time:10276ms step_avg:33.47ms
step:308/2035 train_time:10309ms step_avg:33.47ms
step:309/2035 train_time:10342ms step_avg:33.47ms
step:310/2035 train_time:10375ms step_avg:33.47ms
step:311/2035 train_time:10408ms step_avg:33.47ms
step:312/2035 train_time:10441ms step_avg:33.47ms
step:313/2035 train_time:10475ms step_avg:33.47ms
step:314/2035 train_time:10508ms step_avg:33.46ms
step:315/2035 train_time:10541ms step_avg:33.46ms
step:316/2035 train_time:10574ms step_avg:33.46ms
step:317/2035 train_time:10607ms step_avg:33.46ms
step:318/2035 train_time:10641ms step_avg:33.46ms
step:319/2035 train_time:10674ms step_avg:33.46ms
step:320/2035 train_time:10707ms step_avg:33.46ms
step:321/2035 train_time:10740ms step_avg:33.46ms
step:322/2035 train_time:10774ms step_avg:33.46ms
step:323/2035 train_time:10807ms step_avg:33.46ms
step:324/2035 train_time:10840ms step_avg:33.46ms
step:325/2035 train_time:10874ms step_avg:33.46ms
step:326/2035 train_time:10907ms step_avg:33.46ms
step:327/2035 train_time:10940ms step_avg:33.46ms
step:328/2035 train_time:10973ms step_avg:33.46ms
step:329/2035 train_time:11006ms step_avg:33.45ms
step:330/2035 train_time:11040ms step_avg:33.45ms
step:331/2035 train_time:11073ms step_avg:33.45ms
step:332/2035 train_time:11106ms step_avg:33.45ms
step:333/2035 train_time:11139ms step_avg:33.45ms
step:334/2035 train_time:11172ms step_avg:33.45ms
step:335/2035 train_time:11205ms step_avg:33.45ms
step:336/2035 train_time:11238ms step_avg:33.45ms
step:337/2035 train_time:11271ms step_avg:33.44ms
step:338/2035 train_time:11304ms step_avg:33.44ms
step:339/2035 train_time:11337ms step_avg:33.44ms
step:340/2035 train_time:11370ms step_avg:33.44ms
step:341/2035 train_time:11403ms step_avg:33.44ms
step:342/2035 train_time:11436ms step_avg:33.44ms
step:343/2035 train_time:11469ms step_avg:33.44ms
step:344/2035 train_time:11503ms step_avg:33.44ms
step:345/2035 train_time:11536ms step_avg:33.44ms
step:346/2035 train_time:11569ms step_avg:33.44ms
step:347/2035 train_time:11602ms step_avg:33.43ms
step:348/2035 train_time:11636ms step_avg:33.44ms
step:349/2035 train_time:11669ms step_avg:33.43ms
step:350/2035 train_time:11702ms step_avg:33.43ms
step:351/2035 train_time:11735ms step_avg:33.43ms
step:352/2035 train_time:11768ms step_avg:33.43ms
step:353/2035 train_time:11801ms step_avg:33.43ms
step:354/2035 train_time:11835ms step_avg:33.43ms
step:355/2035 train_time:11868ms step_avg:33.43ms
step:356/2035 train_time:11901ms step_avg:33.43ms
step:357/2035 train_time:11934ms step_avg:33.43ms
step:358/2035 train_time:11967ms step_avg:33.43ms
step:359/2035 train_time:12000ms step_avg:33.43ms
step:360/2035 train_time:12034ms step_avg:33.43ms
step:361/2035 train_time:12067ms step_avg:33.43ms
step:362/2035 train_time:12100ms step_avg:33.43ms
step:363/2035 train_time:12133ms step_avg:33.42ms
step:364/2035 train_time:12166ms step_avg:33.42ms
step:365/2035 train_time:12199ms step_avg:33.42ms
step:366/2035 train_time:12232ms step_avg:33.42ms
step:367/2035 train_time:12265ms step_avg:33.42ms
step:368/2035 train_time:12298ms step_avg:33.42ms
step:369/2035 train_time:12331ms step_avg:33.42ms
step:370/2035 train_time:12364ms step_avg:33.42ms
step:371/2035 train_time:12397ms step_avg:33.42ms
step:372/2035 train_time:12430ms step_avg:33.41ms
step:373/2035 train_time:12463ms step_avg:33.41ms
step:374/2035 train_time:12496ms step_avg:33.41ms
step:375/2035 train_time:12529ms step_avg:33.41ms
step:376/2035 train_time:12562ms step_avg:33.41ms
step:377/2035 train_time:12595ms step_avg:33.41ms
step:378/2035 train_time:12628ms step_avg:33.41ms
step:379/2035 train_time:12661ms step_avg:33.41ms
step:380/2035 train_time:12695ms step_avg:33.41ms
step:381/2035 train_time:12728ms step_avg:33.41ms
step:382/2035 train_time:12762ms step_avg:33.41ms
step:383/2035 train_time:12795ms step_avg:33.41ms
step:384/2035 train_time:12828ms step_avg:33.41ms
step:385/2035 train_time:12861ms step_avg:33.40ms
step:386/2035 train_time:12894ms step_avg:33.40ms
step:387/2035 train_time:12927ms step_avg:33.40ms
step:388/2035 train_time:12960ms step_avg:33.40ms
step:389/2035 train_time:12994ms step_avg:33.40ms
step:390/2035 train_time:13027ms step_avg:33.40ms
step:391/2035 train_time:13060ms step_avg:33.40ms
step:392/2035 train_time:13093ms step_avg:33.40ms
step:393/2035 train_time:13126ms step_avg:33.40ms
step:394/2035 train_time:13160ms step_avg:33.40ms
step:395/2035 train_time:13193ms step_avg:33.40ms
step:396/2035 train_time:13226ms step_avg:33.40ms
step:397/2035 train_time:13259ms step_avg:33.40ms
step:398/2035 train_time:13292ms step_avg:33.40ms
step:399/2035 train_time:13325ms step_avg:33.40ms
step:400/2035 train_time:13358ms step_avg:33.40ms
step:401/2035 train_time:13391ms step_avg:33.39ms
step:402/2035 train_time:13424ms step_avg:33.39ms
step:403/2035 train_time:13457ms step_avg:33.39ms
step:404/2035 train_time:13491ms step_avg:33.39ms
step:405/2035 train_time:13524ms step_avg:33.39ms
step:406/2035 train_time:13557ms step_avg:33.39ms
step:407/2035 train_time:13590ms step_avg:33.39ms
step:408/2035 train_time:13623ms step_avg:33.39ms
step:409/2035 train_time:13656ms step_avg:33.39ms
step:410/2035 train_time:13689ms step_avg:33.39ms
step:411/2035 train_time:13722ms step_avg:33.39ms
step:412/2035 train_time:13756ms step_avg:33.39ms
step:413/2035 train_time:13789ms step_avg:33.39ms
step:414/2035 train_time:13822ms step_avg:33.39ms
step:415/2035 train_time:13855ms step_avg:33.38ms
step:416/2035 train_time:13888ms step_avg:33.38ms
step:417/2035 train_time:13921ms step_avg:33.38ms
step:418/2035 train_time:13954ms step_avg:33.38ms
step:419/2035 train_time:13987ms step_avg:33.38ms
step:420/2035 train_time:14020ms step_avg:33.38ms
step:421/2035 train_time:14053ms step_avg:33.38ms
step:422/2035 train_time:14086ms step_avg:33.38ms
step:423/2035 train_time:14119ms step_avg:33.38ms
step:424/2035 train_time:14152ms step_avg:33.38ms
step:425/2035 train_time:14185ms step_avg:33.38ms
step:426/2035 train_time:14218ms step_avg:33.38ms
step:427/2035 train_time:14251ms step_avg:33.38ms
step:428/2035 train_time:14284ms step_avg:33.37ms
step:429/2035 train_time:14318ms step_avg:33.37ms
step:430/2035 train_time:14351ms step_avg:33.38ms
step:431/2035 train_time:14384ms step_avg:33.37ms
step:432/2035 train_time:14417ms step_avg:33.37ms
step:433/2035 train_time:14450ms step_avg:33.37ms
step:434/2035 train_time:14483ms step_avg:33.37ms
step:435/2035 train_time:14516ms step_avg:33.37ms
step:436/2035 train_time:14550ms step_avg:33.37ms
step:437/2035 train_time:14582ms step_avg:33.37ms
step:438/2035 train_time:14615ms step_avg:33.37ms
step:439/2035 train_time:14648ms step_avg:33.37ms
step:440/2035 train_time:14682ms step_avg:33.37ms
step:441/2035 train_time:14715ms step_avg:33.37ms
step:442/2035 train_time:14748ms step_avg:33.37ms
step:443/2035 train_time:14781ms step_avg:33.36ms
step:444/2035 train_time:14814ms step_avg:33.36ms
step:445/2035 train_time:14847ms step_avg:33.36ms
step:446/2035 train_time:14880ms step_avg:33.36ms
step:447/2035 train_time:14913ms step_avg:33.36ms
step:448/2035 train_time:14947ms step_avg:33.36ms
step:449/2035 train_time:14980ms step_avg:33.36ms
step:450/2035 train_time:15013ms step_avg:33.36ms
step:451/2035 train_time:15046ms step_avg:33.36ms
step:452/2035 train_time:15080ms step_avg:33.36ms
step:453/2035 train_time:15113ms step_avg:33.36ms
step:454/2035 train_time:15146ms step_avg:33.36ms
step:455/2035 train_time:15179ms step_avg:33.36ms
step:456/2035 train_time:15212ms step_avg:33.36ms
step:457/2035 train_time:15245ms step_avg:33.36ms
step:458/2035 train_time:15278ms step_avg:33.36ms
step:459/2035 train_time:15311ms step_avg:33.36ms
step:460/2035 train_time:15344ms step_avg:33.36ms
step:461/2035 train_time:15377ms step_avg:33.36ms
step:462/2035 train_time:15411ms step_avg:33.36ms
step:463/2035 train_time:15444ms step_avg:33.36ms
step:464/2035 train_time:15477ms step_avg:33.36ms
step:465/2035 train_time:15510ms step_avg:33.35ms
step:466/2035 train_time:15543ms step_avg:33.35ms
step:467/2035 train_time:15576ms step_avg:33.35ms
step:468/2035 train_time:15609ms step_avg:33.35ms
step:469/2035 train_time:15642ms step_avg:33.35ms
step:470/2035 train_time:15675ms step_avg:33.35ms
step:471/2035 train_time:15708ms step_avg:33.35ms
step:472/2035 train_time:15742ms step_avg:33.35ms
step:473/2035 train_time:15775ms step_avg:33.35ms
step:474/2035 train_time:15808ms step_avg:33.35ms
step:475/2035 train_time:15841ms step_avg:33.35ms
step:476/2035 train_time:15874ms step_avg:33.35ms
step:477/2035 train_time:15907ms step_avg:33.35ms
step:478/2035 train_time:15941ms step_avg:33.35ms
step:479/2035 train_time:15974ms step_avg:33.35ms
step:480/2035 train_time:16007ms step_avg:33.35ms
step:481/2035 train_time:16040ms step_avg:33.35ms
step:482/2035 train_time:16074ms step_avg:33.35ms
step:483/2035 train_time:16107ms step_avg:33.35ms
step:484/2035 train_time:16140ms step_avg:33.35ms
step:485/2035 train_time:16173ms step_avg:33.35ms
step:486/2035 train_time:16206ms step_avg:33.35ms
step:487/2035 train_time:16239ms step_avg:33.35ms
step:488/2035 train_time:16273ms step_avg:33.35ms
step:489/2035 train_time:16306ms step_avg:33.35ms
step:490/2035 train_time:16339ms step_avg:33.35ms
step:491/2035 train_time:16372ms step_avg:33.34ms
step:492/2035 train_time:16405ms step_avg:33.34ms
step:493/2035 train_time:16438ms step_avg:33.34ms
step:494/2035 train_time:16471ms step_avg:33.34ms
step:495/2035 train_time:16504ms step_avg:33.34ms
step:496/2035 train_time:16537ms step_avg:33.34ms
step:497/2035 train_time:16570ms step_avg:33.34ms
step:498/2035 train_time:16604ms step_avg:33.34ms
step:499/2035 train_time:16637ms step_avg:33.34ms
step:500/2035 train_time:16670ms step_avg:33.34ms
step:500/2035 val_loss:3.9963 train_time:16706ms step_avg:33.41ms
step:501/2035 train_time:16727ms step_avg:33.39ms
step:502/2035 train_time:16746ms step_avg:33.36ms
step:503/2035 train_time:16772ms step_avg:33.34ms
step:504/2035 train_time:16807ms step_avg:33.35ms
step:505/2035 train_time:16844ms step_avg:33.35ms
step:506/2035 train_time:16880ms step_avg:33.36ms
step:507/2035 train_time:16915ms step_avg:33.36ms
step:508/2035 train_time:16949ms step_avg:33.36ms
step:509/2035 train_time:16983ms step_avg:33.36ms
step:510/2035 train_time:17016ms step_avg:33.37ms
step:511/2035 train_time:17049ms step_avg:33.36ms
step:512/2035 train_time:17083ms step_avg:33.36ms
step:513/2035 train_time:17116ms step_avg:33.36ms
step:514/2035 train_time:17149ms step_avg:33.36ms
step:515/2035 train_time:17181ms step_avg:33.36ms
step:516/2035 train_time:17214ms step_avg:33.36ms
step:517/2035 train_time:17247ms step_avg:33.36ms
step:518/2035 train_time:17280ms step_avg:33.36ms
step:519/2035 train_time:17313ms step_avg:33.36ms
step:520/2035 train_time:17346ms step_avg:33.36ms
step:521/2035 train_time:17379ms step_avg:33.36ms
step:522/2035 train_time:17412ms step_avg:33.36ms
step:523/2035 train_time:17445ms step_avg:33.36ms
step:524/2035 train_time:17478ms step_avg:33.36ms
step:525/2035 train_time:17511ms step_avg:33.35ms
step:526/2035 train_time:17544ms step_avg:33.35ms
step:527/2035 train_time:17577ms step_avg:33.35ms
step:528/2035 train_time:17610ms step_avg:33.35ms
step:529/2035 train_time:17643ms step_avg:33.35ms
step:530/2035 train_time:17675ms step_avg:33.35ms
step:531/2035 train_time:17708ms step_avg:33.35ms
step:532/2035 train_time:17741ms step_avg:33.35ms
step:533/2035 train_time:17775ms step_avg:33.35ms
step:534/2035 train_time:17808ms step_avg:33.35ms
step:535/2035 train_time:17841ms step_avg:33.35ms
step:536/2035 train_time:17876ms step_avg:33.35ms
step:537/2035 train_time:17909ms step_avg:33.35ms
step:538/2035 train_time:17943ms step_avg:33.35ms
step:539/2035 train_time:17976ms step_avg:33.35ms
step:540/2035 train_time:18009ms step_avg:33.35ms
step:541/2035 train_time:18042ms step_avg:33.35ms
step:542/2035 train_time:18075ms step_avg:33.35ms
step:543/2035 train_time:18108ms step_avg:33.35ms
step:544/2035 train_time:18141ms step_avg:33.35ms
step:545/2035 train_time:18175ms step_avg:33.35ms
step:546/2035 train_time:18208ms step_avg:33.35ms
step:547/2035 train_time:18241ms step_avg:33.35ms
step:548/2035 train_time:18274ms step_avg:33.35ms
step:549/2035 train_time:18307ms step_avg:33.35ms
step:550/2035 train_time:18340ms step_avg:33.34ms
step:551/2035 train_time:18373ms step_avg:33.34ms
step:552/2035 train_time:18406ms step_avg:33.34ms
step:553/2035 train_time:18439ms step_avg:33.34ms
step:554/2035 train_time:18472ms step_avg:33.34ms
step:555/2035 train_time:18504ms step_avg:33.34ms
step:556/2035 train_time:18538ms step_avg:33.34ms
step:557/2035 train_time:18571ms step_avg:33.34ms
step:558/2035 train_time:18604ms step_avg:33.34ms
step:559/2035 train_time:18637ms step_avg:33.34ms
step:560/2035 train_time:18670ms step_avg:33.34ms
step:561/2035 train_time:18703ms step_avg:33.34ms
step:562/2035 train_time:18736ms step_avg:33.34ms
step:563/2035 train_time:18768ms step_avg:33.34ms
step:564/2035 train_time:18802ms step_avg:33.34ms
step:565/2035 train_time:18835ms step_avg:33.34ms
step:566/2035 train_time:18869ms step_avg:33.34ms
step:567/2035 train_time:18902ms step_avg:33.34ms
step:568/2035 train_time:18935ms step_avg:33.34ms
step:569/2035 train_time:18969ms step_avg:33.34ms
step:570/2035 train_time:19002ms step_avg:33.34ms
step:571/2035 train_time:19035ms step_avg:33.34ms
step:572/2035 train_time:19069ms step_avg:33.34ms
step:573/2035 train_time:19101ms step_avg:33.34ms
step:574/2035 train_time:19135ms step_avg:33.34ms
step:575/2035 train_time:19168ms step_avg:33.34ms
step:576/2035 train_time:19201ms step_avg:33.34ms
step:577/2035 train_time:19234ms step_avg:33.33ms
step:578/2035 train_time:19267ms step_avg:33.33ms
step:579/2035 train_time:19300ms step_avg:33.33ms
step:580/2035 train_time:19333ms step_avg:33.33ms
step:581/2035 train_time:19366ms step_avg:33.33ms
step:582/2035 train_time:19399ms step_avg:33.33ms
step:583/2035 train_time:19433ms step_avg:33.33ms
step:584/2035 train_time:19466ms step_avg:33.33ms
step:585/2035 train_time:19499ms step_avg:33.33ms
step:586/2035 train_time:19532ms step_avg:33.33ms
step:587/2035 train_time:19565ms step_avg:33.33ms
step:588/2035 train_time:19598ms step_avg:33.33ms
step:589/2035 train_time:19631ms step_avg:33.33ms
step:590/2035 train_time:19665ms step_avg:33.33ms
step:591/2035 train_time:19697ms step_avg:33.33ms
step:592/2035 train_time:19731ms step_avg:33.33ms
step:593/2035 train_time:19764ms step_avg:33.33ms
step:594/2035 train_time:19797ms step_avg:33.33ms
step:595/2035 train_time:19830ms step_avg:33.33ms
step:596/2035 train_time:19863ms step_avg:33.33ms
step:597/2035 train_time:19896ms step_avg:33.33ms
step:598/2035 train_time:19929ms step_avg:33.33ms
step:599/2035 train_time:19962ms step_avg:33.33ms
step:600/2035 train_time:19995ms step_avg:33.33ms
step:601/2035 train_time:20028ms step_avg:33.32ms
step:602/2035 train_time:20062ms step_avg:33.33ms
step:603/2035 train_time:20095ms step_avg:33.33ms
step:604/2035 train_time:20128ms step_avg:33.33ms
step:605/2035 train_time:20161ms step_avg:33.32ms
step:606/2035 train_time:20194ms step_avg:33.32ms
step:607/2035 train_time:20227ms step_avg:33.32ms
step:608/2035 train_time:20260ms step_avg:33.32ms
step:609/2035 train_time:20293ms step_avg:33.32ms
step:610/2035 train_time:20327ms step_avg:33.32ms
step:611/2035 train_time:20359ms step_avg:33.32ms
step:612/2035 train_time:20392ms step_avg:33.32ms
step:613/2035 train_time:20426ms step_avg:33.32ms
step:614/2035 train_time:20459ms step_avg:33.32ms
step:615/2035 train_time:20492ms step_avg:33.32ms
step:616/2035 train_time:20525ms step_avg:33.32ms
step:617/2035 train_time:20558ms step_avg:33.32ms
step:618/2035 train_time:20592ms step_avg:33.32ms
step:619/2035 train_time:20624ms step_avg:33.32ms
step:620/2035 train_time:20658ms step_avg:33.32ms
step:621/2035 train_time:20691ms step_avg:33.32ms
step:622/2035 train_time:20724ms step_avg:33.32ms
step:623/2035 train_time:20757ms step_avg:33.32ms
step:624/2035 train_time:20791ms step_avg:33.32ms
step:625/2035 train_time:20823ms step_avg:33.32ms
step:626/2035 train_time:20857ms step_avg:33.32ms
step:627/2035 train_time:20890ms step_avg:33.32ms
step:628/2035 train_time:20923ms step_avg:33.32ms
step:629/2035 train_time:20956ms step_avg:33.32ms
step:630/2035 train_time:20990ms step_avg:33.32ms
step:631/2035 train_time:21023ms step_avg:33.32ms
step:632/2035 train_time:21056ms step_avg:33.32ms
step:633/2035 train_time:21089ms step_avg:33.32ms
step:634/2035 train_time:21122ms step_avg:33.32ms
step:635/2035 train_time:21155ms step_avg:33.31ms
step:636/2035 train_time:21188ms step_avg:33.31ms
step:637/2035 train_time:21221ms step_avg:33.31ms
step:638/2035 train_time:21255ms step_avg:33.31ms
step:639/2035 train_time:21287ms step_avg:33.31ms
step:640/2035 train_time:21321ms step_avg:33.31ms
step:641/2035 train_time:21354ms step_avg:33.31ms
step:642/2035 train_time:21387ms step_avg:33.31ms
step:643/2035 train_time:21420ms step_avg:33.31ms
step:644/2035 train_time:21453ms step_avg:33.31ms
step:645/2035 train_time:21486ms step_avg:33.31ms
step:646/2035 train_time:21519ms step_avg:33.31ms
step:647/2035 train_time:21552ms step_avg:33.31ms
step:648/2035 train_time:21585ms step_avg:33.31ms
step:649/2035 train_time:21618ms step_avg:33.31ms
step:650/2035 train_time:21651ms step_avg:33.31ms
step:651/2035 train_time:21683ms step_avg:33.31ms
step:652/2035 train_time:21717ms step_avg:33.31ms
step:653/2035 train_time:21750ms step_avg:33.31ms
step:654/2035 train_time:21784ms step_avg:33.31ms
step:655/2035 train_time:21817ms step_avg:33.31ms
step:656/2035 train_time:21850ms step_avg:33.31ms
step:657/2035 train_time:21883ms step_avg:33.31ms
step:658/2035 train_time:21916ms step_avg:33.31ms
step:659/2035 train_time:21950ms step_avg:33.31ms
step:660/2035 train_time:21983ms step_avg:33.31ms
step:661/2035 train_time:22016ms step_avg:33.31ms
step:662/2035 train_time:22049ms step_avg:33.31ms
step:663/2035 train_time:22082ms step_avg:33.31ms
step:664/2035 train_time:22116ms step_avg:33.31ms
step:665/2035 train_time:22149ms step_avg:33.31ms
step:666/2035 train_time:22183ms step_avg:33.31ms
step:667/2035 train_time:22242ms step_avg:33.35ms
step:668/2035 train_time:22301ms step_avg:33.39ms
step:669/2035 train_time:22362ms step_avg:33.43ms
step:670/2035 train_time:22422ms step_avg:33.47ms
step:671/2035 train_time:22482ms step_avg:33.51ms
step:672/2035 train_time:22542ms step_avg:33.55ms
step:673/2035 train_time:22603ms step_avg:33.58ms
step:674/2035 train_time:22663ms step_avg:33.62ms
step:675/2035 train_time:22724ms step_avg:33.67ms
step:676/2035 train_time:22784ms step_avg:33.70ms
step:677/2035 train_time:22844ms step_avg:33.74ms
step:678/2035 train_time:22906ms step_avg:33.79ms
step:679/2035 train_time:22967ms step_avg:33.83ms
step:680/2035 train_time:23027ms step_avg:33.86ms
step:681/2035 train_time:23087ms step_avg:33.90ms
step:682/2035 train_time:23147ms step_avg:33.94ms
step:683/2035 train_time:23208ms step_avg:33.98ms
step:684/2035 train_time:23268ms step_avg:34.02ms
step:685/2035 train_time:23328ms step_avg:34.06ms
step:686/2035 train_time:23388ms step_avg:34.09ms
step:687/2035 train_time:23448ms step_avg:34.13ms
step:688/2035 train_time:23508ms step_avg:34.17ms
step:689/2035 train_time:23568ms step_avg:34.21ms
step:690/2035 train_time:23628ms step_avg:34.24ms
step:691/2035 train_time:23688ms step_avg:34.28ms
step:692/2035 train_time:23748ms step_avg:34.32ms
step:693/2035 train_time:23808ms step_avg:34.36ms
step:694/2035 train_time:23868ms step_avg:34.39ms
step:695/2035 train_time:23928ms step_avg:34.43ms
step:696/2035 train_time:23988ms step_avg:34.46ms
step:697/2035 train_time:24048ms step_avg:34.50ms
step:698/2035 train_time:24108ms step_avg:34.54ms
step:699/2035 train_time:24169ms step_avg:34.58ms
step:700/2035 train_time:24228ms step_avg:34.61ms
step:701/2035 train_time:24288ms step_avg:34.65ms
step:702/2035 train_time:24347ms step_avg:34.68ms
step:703/2035 train_time:24408ms step_avg:34.72ms
step:704/2035 train_time:24467ms step_avg:34.75ms
step:705/2035 train_time:24528ms step_avg:34.79ms
step:706/2035 train_time:24587ms step_avg:34.83ms
step:707/2035 train_time:24647ms step_avg:34.86ms
step:708/2035 train_time:24707ms step_avg:34.90ms
step:709/2035 train_time:24768ms step_avg:34.93ms
step:710/2035 train_time:24828ms step_avg:34.97ms
step:711/2035 train_time:24887ms step_avg:35.00ms
step:712/2035 train_time:24948ms step_avg:35.04ms
step:713/2035 train_time:25008ms step_avg:35.07ms
step:714/2035 train_time:25069ms step_avg:35.11ms
step:715/2035 train_time:25128ms step_avg:35.14ms
step:716/2035 train_time:25188ms step_avg:35.18ms
step:717/2035 train_time:25249ms step_avg:35.21ms
step:718/2035 train_time:25308ms step_avg:35.25ms
step:719/2035 train_time:25368ms step_avg:35.28ms
step:720/2035 train_time:25428ms step_avg:35.32ms
step:721/2035 train_time:25488ms step_avg:35.35ms
step:722/2035 train_time:25548ms step_avg:35.38ms
step:723/2035 train_time:25608ms step_avg:35.42ms
step:724/2035 train_time:25668ms step_avg:35.45ms
step:725/2035 train_time:25729ms step_avg:35.49ms
step:726/2035 train_time:25788ms step_avg:35.52ms
step:727/2035 train_time:25849ms step_avg:35.56ms
step:728/2035 train_time:25908ms step_avg:35.59ms
step:729/2035 train_time:25969ms step_avg:35.62ms
step:730/2035 train_time:26029ms step_avg:35.66ms
step:731/2035 train_time:26089ms step_avg:35.69ms
step:732/2035 train_time:26148ms step_avg:35.72ms
step:733/2035 train_time:26209ms step_avg:35.76ms
step:734/2035 train_time:26269ms step_avg:35.79ms
step:735/2035 train_time:26329ms step_avg:35.82ms
step:736/2035 train_time:26388ms step_avg:35.85ms
step:737/2035 train_time:26448ms step_avg:35.89ms
step:738/2035 train_time:26508ms step_avg:35.92ms
step:739/2035 train_time:26568ms step_avg:35.95ms
step:740/2035 train_time:26627ms step_avg:35.98ms
step:741/2035 train_time:26687ms step_avg:36.02ms
step:742/2035 train_time:26747ms step_avg:36.05ms
step:743/2035 train_time:26808ms step_avg:36.08ms
step:744/2035 train_time:26867ms step_avg:36.11ms
step:745/2035 train_time:26928ms step_avg:36.14ms
step:746/2035 train_time:26989ms step_avg:36.18ms
step:747/2035 train_time:27049ms step_avg:36.21ms
step:748/2035 train_time:27108ms step_avg:36.24ms
step:749/2035 train_time:27168ms step_avg:36.27ms
step:750/2035 train_time:27227ms step_avg:36.30ms
step:750/2035 val_loss:3.8256 train_time:27290ms step_avg:36.39ms
step:751/2035 train_time:27310ms step_avg:36.37ms
step:752/2035 train_time:27350ms step_avg:36.37ms
step:753/2035 train_time:27416ms step_avg:36.41ms
step:754/2035 train_time:27480ms step_avg:36.45ms
step:755/2035 train_time:27540ms step_avg:36.48ms
step:756/2035 train_time:27600ms step_avg:36.51ms
step:757/2035 train_time:27659ms step_avg:36.54ms
step:758/2035 train_time:27718ms step_avg:36.57ms
step:759/2035 train_time:27777ms step_avg:36.60ms
step:760/2035 train_time:27837ms step_avg:36.63ms
step:761/2035 train_time:27896ms step_avg:36.66ms
step:762/2035 train_time:27954ms step_avg:36.69ms
step:763/2035 train_time:28014ms step_avg:36.72ms
step:764/2035 train_time:28073ms step_avg:36.74ms
step:765/2035 train_time:28133ms step_avg:36.77ms
step:766/2035 train_time:28192ms step_avg:36.80ms
step:767/2035 train_time:28254ms step_avg:36.84ms
step:768/2035 train_time:28315ms step_avg:36.87ms
step:769/2035 train_time:28378ms step_avg:36.90ms
step:770/2035 train_time:28439ms step_avg:36.93ms
step:771/2035 train_time:28499ms step_avg:36.96ms
step:772/2035 train_time:28559ms step_avg:36.99ms
step:773/2035 train_time:28619ms step_avg:37.02ms
step:774/2035 train_time:28678ms step_avg:37.05ms
step:775/2035 train_time:28738ms step_avg:37.08ms
step:776/2035 train_time:28797ms step_avg:37.11ms
step:777/2035 train_time:28856ms step_avg:37.14ms
step:778/2035 train_time:28915ms step_avg:37.17ms
step:779/2035 train_time:28975ms step_avg:37.20ms
step:780/2035 train_time:29034ms step_avg:37.22ms
step:781/2035 train_time:29093ms step_avg:37.25ms
step:782/2035 train_time:29153ms step_avg:37.28ms
step:783/2035 train_time:29213ms step_avg:37.31ms
step:784/2035 train_time:29273ms step_avg:37.34ms
step:785/2035 train_time:29335ms step_avg:37.37ms
step:786/2035 train_time:29396ms step_avg:37.40ms
step:787/2035 train_time:29456ms step_avg:37.43ms
step:788/2035 train_time:29517ms step_avg:37.46ms
step:789/2035 train_time:29577ms step_avg:37.49ms
step:790/2035 train_time:29638ms step_avg:37.52ms
step:791/2035 train_time:29697ms step_avg:37.54ms
step:792/2035 train_time:29756ms step_avg:37.57ms
step:793/2035 train_time:29816ms step_avg:37.60ms
step:794/2035 train_time:29875ms step_avg:37.63ms
step:795/2035 train_time:29935ms step_avg:37.65ms
step:796/2035 train_time:29994ms step_avg:37.68ms
step:797/2035 train_time:30054ms step_avg:37.71ms
step:798/2035 train_time:30113ms step_avg:37.74ms
step:799/2035 train_time:30173ms step_avg:37.76ms
step:800/2035 train_time:30234ms step_avg:37.79ms
step:801/2035 train_time:30295ms step_avg:37.82ms
step:802/2035 train_time:30355ms step_avg:37.85ms
step:803/2035 train_time:30416ms step_avg:37.88ms
step:804/2035 train_time:30476ms step_avg:37.91ms
step:805/2035 train_time:30537ms step_avg:37.93ms
step:806/2035 train_time:30597ms step_avg:37.96ms
step:807/2035 train_time:30657ms step_avg:37.99ms
step:808/2035 train_time:30717ms step_avg:38.02ms
step:809/2035 train_time:30776ms step_avg:38.04ms
step:810/2035 train_time:30836ms step_avg:38.07ms
step:811/2035 train_time:30895ms step_avg:38.10ms
step:812/2035 train_time:30955ms step_avg:38.12ms
step:813/2035 train_time:31015ms step_avg:38.15ms
step:814/2035 train_time:31074ms step_avg:38.17ms
step:815/2035 train_time:31134ms step_avg:38.20ms
step:816/2035 train_time:31194ms step_avg:38.23ms
step:817/2035 train_time:31255ms step_avg:38.26ms
step:818/2035 train_time:31316ms step_avg:38.28ms
step:819/2035 train_time:31376ms step_avg:38.31ms
step:820/2035 train_time:31436ms step_avg:38.34ms
step:821/2035 train_time:31497ms step_avg:38.36ms
step:822/2035 train_time:31556ms step_avg:38.39ms
step:823/2035 train_time:31617ms step_avg:38.42ms
step:824/2035 train_time:31676ms step_avg:38.44ms
step:825/2035 train_time:31737ms step_avg:38.47ms
step:826/2035 train_time:31796ms step_avg:38.49ms
step:827/2035 train_time:31855ms step_avg:38.52ms
step:828/2035 train_time:31915ms step_avg:38.54ms
step:829/2035 train_time:31974ms step_avg:38.57ms
step:830/2035 train_time:32034ms step_avg:38.60ms
step:831/2035 train_time:32094ms step_avg:38.62ms
step:832/2035 train_time:32154ms step_avg:38.65ms
step:833/2035 train_time:32215ms step_avg:38.67ms
step:834/2035 train_time:32275ms step_avg:38.70ms
step:835/2035 train_time:32335ms step_avg:38.73ms
step:836/2035 train_time:32395ms step_avg:38.75ms
step:837/2035 train_time:32456ms step_avg:38.78ms
step:838/2035 train_time:32516ms step_avg:38.80ms
step:839/2035 train_time:32577ms step_avg:38.83ms
step:840/2035 train_time:32637ms step_avg:38.85ms
step:841/2035 train_time:32697ms step_avg:38.88ms
step:842/2035 train_time:32757ms step_avg:38.90ms
step:843/2035 train_time:32817ms step_avg:38.93ms
step:844/2035 train_time:32876ms step_avg:38.95ms
step:845/2035 train_time:32936ms step_avg:38.98ms
step:846/2035 train_time:32995ms step_avg:39.00ms
step:847/2035 train_time:33055ms step_avg:39.03ms
step:848/2035 train_time:33115ms step_avg:39.05ms
step:849/2035 train_time:33175ms step_avg:39.07ms
step:850/2035 train_time:33234ms step_avg:39.10ms
step:851/2035 train_time:33295ms step_avg:39.12ms
step:852/2035 train_time:33355ms step_avg:39.15ms
step:853/2035 train_time:33416ms step_avg:39.17ms
step:854/2035 train_time:33475ms step_avg:39.20ms
step:855/2035 train_time:33536ms step_avg:39.22ms
step:856/2035 train_time:33596ms step_avg:39.25ms
step:857/2035 train_time:33656ms step_avg:39.27ms
step:858/2035 train_time:33716ms step_avg:39.30ms
step:859/2035 train_time:33777ms step_avg:39.32ms
step:860/2035 train_time:33836ms step_avg:39.34ms
step:861/2035 train_time:33896ms step_avg:39.37ms
step:862/2035 train_time:33956ms step_avg:39.39ms
step:863/2035 train_time:34016ms step_avg:39.42ms
step:864/2035 train_time:34075ms step_avg:39.44ms
step:865/2035 train_time:34135ms step_avg:39.46ms
step:866/2035 train_time:34195ms step_avg:39.49ms
step:867/2035 train_time:34255ms step_avg:39.51ms
step:868/2035 train_time:34315ms step_avg:39.53ms
step:869/2035 train_time:34376ms step_avg:39.56ms
step:870/2035 train_time:34436ms step_avg:39.58ms
step:871/2035 train_time:34496ms step_avg:39.61ms
step:872/2035 train_time:34556ms step_avg:39.63ms
step:873/2035 train_time:34616ms step_avg:39.65ms
step:874/2035 train_time:34676ms step_avg:39.67ms
step:875/2035 train_time:34736ms step_avg:39.70ms
step:876/2035 train_time:34796ms step_avg:39.72ms
step:877/2035 train_time:34856ms step_avg:39.74ms
step:878/2035 train_time:34916ms step_avg:39.77ms
step:879/2035 train_time:34976ms step_avg:39.79ms
step:880/2035 train_time:35035ms step_avg:39.81ms
step:881/2035 train_time:35095ms step_avg:39.84ms
step:882/2035 train_time:35155ms step_avg:39.86ms
step:883/2035 train_time:35215ms step_avg:39.88ms
step:884/2035 train_time:35274ms step_avg:39.90ms
step:885/2035 train_time:35335ms step_avg:39.93ms
step:886/2035 train_time:35395ms step_avg:39.95ms
step:887/2035 train_time:35455ms step_avg:39.97ms
step:888/2035 train_time:35515ms step_avg:39.99ms
step:889/2035 train_time:35576ms step_avg:40.02ms
step:890/2035 train_time:35636ms step_avg:40.04ms
step:891/2035 train_time:35696ms step_avg:40.06ms
step:892/2035 train_time:35756ms step_avg:40.09ms
step:893/2035 train_time:35816ms step_avg:40.11ms
step:894/2035 train_time:35875ms step_avg:40.13ms
step:895/2035 train_time:35936ms step_avg:40.15ms
step:896/2035 train_time:35995ms step_avg:40.17ms
step:897/2035 train_time:36056ms step_avg:40.20ms
step:898/2035 train_time:36115ms step_avg:40.22ms
step:899/2035 train_time:36176ms step_avg:40.24ms
step:900/2035 train_time:36236ms step_avg:40.26ms
step:901/2035 train_time:36296ms step_avg:40.28ms
step:902/2035 train_time:36356ms step_avg:40.31ms
step:903/2035 train_time:36416ms step_avg:40.33ms
step:904/2035 train_time:36476ms step_avg:40.35ms
step:905/2035 train_time:36537ms step_avg:40.37ms
step:906/2035 train_time:36596ms step_avg:40.39ms
step:907/2035 train_time:36656ms step_avg:40.41ms
step:908/2035 train_time:36715ms step_avg:40.44ms
step:909/2035 train_time:36776ms step_avg:40.46ms
step:910/2035 train_time:36836ms step_avg:40.48ms
step:911/2035 train_time:36896ms step_avg:40.50ms
step:912/2035 train_time:36956ms step_avg:40.52ms
step:913/2035 train_time:37015ms step_avg:40.54ms
step:914/2035 train_time:37075ms step_avg:40.56ms
step:915/2035 train_time:37136ms step_avg:40.59ms
step:916/2035 train_time:37196ms step_avg:40.61ms
step:917/2035 train_time:37256ms step_avg:40.63ms
step:918/2035 train_time:37316ms step_avg:40.65ms
step:919/2035 train_time:37376ms step_avg:40.67ms
step:920/2035 train_time:37436ms step_avg:40.69ms
step:921/2035 train_time:37496ms step_avg:40.71ms
step:922/2035 train_time:37556ms step_avg:40.73ms
step:923/2035 train_time:37616ms step_avg:40.75ms
step:924/2035 train_time:37676ms step_avg:40.77ms
step:925/2035 train_time:37736ms step_avg:40.80ms
step:926/2035 train_time:37796ms step_avg:40.82ms
step:927/2035 train_time:37856ms step_avg:40.84ms
step:928/2035 train_time:37915ms step_avg:40.86ms
step:929/2035 train_time:37975ms step_avg:40.88ms
step:930/2035 train_time:38035ms step_avg:40.90ms
step:931/2035 train_time:38096ms step_avg:40.92ms
step:932/2035 train_time:38155ms step_avg:40.94ms
step:933/2035 train_time:38215ms step_avg:40.96ms
step:934/2035 train_time:38275ms step_avg:40.98ms
step:935/2035 train_time:38336ms step_avg:41.00ms
step:936/2035 train_time:38396ms step_avg:41.02ms
step:937/2035 train_time:38456ms step_avg:41.04ms
step:938/2035 train_time:38516ms step_avg:41.06ms
step:939/2035 train_time:38577ms step_avg:41.08ms
step:940/2035 train_time:38637ms step_avg:41.10ms
step:941/2035 train_time:38696ms step_avg:41.12ms
step:942/2035 train_time:38756ms step_avg:41.14ms
step:943/2035 train_time:38816ms step_avg:41.16ms
step:944/2035 train_time:38875ms step_avg:41.18ms
step:945/2035 train_time:38936ms step_avg:41.20ms
step:946/2035 train_time:38995ms step_avg:41.22ms
step:947/2035 train_time:39055ms step_avg:41.24ms
step:948/2035 train_time:39115ms step_avg:41.26ms
step:949/2035 train_time:39175ms step_avg:41.28ms
step:950/2035 train_time:39236ms step_avg:41.30ms
step:951/2035 train_time:39296ms step_avg:41.32ms
step:952/2035 train_time:39355ms step_avg:41.34ms
step:953/2035 train_time:39415ms step_avg:41.36ms
step:954/2035 train_time:39475ms step_avg:41.38ms
step:955/2035 train_time:39536ms step_avg:41.40ms
step:956/2035 train_time:39596ms step_avg:41.42ms
step:957/2035 train_time:39656ms step_avg:41.44ms
step:958/2035 train_time:39715ms step_avg:41.46ms
step:959/2035 train_time:39776ms step_avg:41.48ms
step:960/2035 train_time:39836ms step_avg:41.50ms
step:961/2035 train_time:39896ms step_avg:41.52ms
step:962/2035 train_time:39956ms step_avg:41.53ms
step:963/2035 train_time:40016ms step_avg:41.55ms
step:964/2035 train_time:40076ms step_avg:41.57ms
step:965/2035 train_time:40136ms step_avg:41.59ms
step:966/2035 train_time:40196ms step_avg:41.61ms
step:967/2035 train_time:40256ms step_avg:41.63ms
step:968/2035 train_time:40316ms step_avg:41.65ms
step:969/2035 train_time:40376ms step_avg:41.67ms
step:970/2035 train_time:40436ms step_avg:41.69ms
step:971/2035 train_time:40496ms step_avg:41.71ms
step:972/2035 train_time:40556ms step_avg:41.72ms
step:973/2035 train_time:40616ms step_avg:41.74ms
step:974/2035 train_time:40675ms step_avg:41.76ms
step:975/2035 train_time:40736ms step_avg:41.78ms
step:976/2035 train_time:40795ms step_avg:41.80ms
step:977/2035 train_time:40855ms step_avg:41.82ms
step:978/2035 train_time:40915ms step_avg:41.84ms
step:979/2035 train_time:40975ms step_avg:41.85ms
step:980/2035 train_time:41036ms step_avg:41.87ms
step:981/2035 train_time:41096ms step_avg:41.89ms
step:982/2035 train_time:41156ms step_avg:41.91ms
step:983/2035 train_time:41216ms step_avg:41.93ms
step:984/2035 train_time:41275ms step_avg:41.95ms
step:985/2035 train_time:41336ms step_avg:41.97ms
step:986/2035 train_time:41396ms step_avg:41.98ms
step:987/2035 train_time:41456ms step_avg:42.00ms
step:988/2035 train_time:41515ms step_avg:42.02ms
step:989/2035 train_time:41576ms step_avg:42.04ms
step:990/2035 train_time:41637ms step_avg:42.06ms
step:991/2035 train_time:41696ms step_avg:42.07ms
step:992/2035 train_time:41756ms step_avg:42.09ms
step:993/2035 train_time:41816ms step_avg:42.11ms
step:994/2035 train_time:41875ms step_avg:42.13ms
step:995/2035 train_time:41936ms step_avg:42.15ms
step:996/2035 train_time:41995ms step_avg:42.16ms
step:997/2035 train_time:42056ms step_avg:42.18ms
step:998/2035 train_time:42117ms step_avg:42.20ms
step:999/2035 train_time:42177ms step_avg:42.22ms
step:1000/2035 train_time:42237ms step_avg:42.24ms
step:1000/2035 val_loss:3.6868 train_time:42299ms step_avg:42.30ms
step:1001/2035 train_time:42319ms step_avg:42.28ms
step:1002/2035 train_time:42361ms step_avg:42.28ms
step:1003/2035 train_time:42427ms step_avg:42.30ms
step:1004/2035 train_time:42489ms step_avg:42.32ms
step:1005/2035 train_time:42549ms step_avg:42.34ms
step:1006/2035 train_time:42609ms step_avg:42.35ms
step:1007/2035 train_time:42669ms step_avg:42.37ms
step:1008/2035 train_time:42729ms step_avg:42.39ms
step:1009/2035 train_time:42789ms step_avg:42.41ms
step:1010/2035 train_time:42847ms step_avg:42.42ms
step:1011/2035 train_time:42907ms step_avg:42.44ms
step:1012/2035 train_time:42966ms step_avg:42.46ms
step:1013/2035 train_time:43025ms step_avg:42.47ms
step:1014/2035 train_time:43084ms step_avg:42.49ms
step:1015/2035 train_time:43143ms step_avg:42.51ms
step:1016/2035 train_time:43204ms step_avg:42.52ms
step:1017/2035 train_time:43269ms step_avg:42.55ms
step:1018/2035 train_time:43330ms step_avg:42.56ms
step:1019/2035 train_time:43392ms step_avg:42.58ms
step:1020/2035 train_time:43453ms step_avg:42.60ms
step:1021/2035 train_time:43514ms step_avg:42.62ms
step:1022/2035 train_time:43574ms step_avg:42.64ms
step:1023/2035 train_time:43635ms step_avg:42.65ms
step:1024/2035 train_time:43695ms step_avg:42.67ms
step:1025/2035 train_time:43755ms step_avg:42.69ms
step:1026/2035 train_time:43815ms step_avg:42.71ms
step:1027/2035 train_time:43876ms step_avg:42.72ms
step:1028/2035 train_time:43935ms step_avg:42.74ms
step:1029/2035 train_time:43995ms step_avg:42.76ms
step:1030/2035 train_time:44056ms step_avg:42.77ms
step:1031/2035 train_time:44116ms step_avg:42.79ms
step:1032/2035 train_time:44177ms step_avg:42.81ms
step:1033/2035 train_time:44237ms step_avg:42.82ms
step:1034/2035 train_time:44298ms step_avg:42.84ms
step:1035/2035 train_time:44361ms step_avg:42.86ms
step:1036/2035 train_time:44421ms step_avg:42.88ms
step:1037/2035 train_time:44481ms step_avg:42.89ms
step:1038/2035 train_time:44541ms step_avg:42.91ms
step:1039/2035 train_time:44601ms step_avg:42.93ms
step:1040/2035 train_time:44661ms step_avg:42.94ms
step:1041/2035 train_time:44722ms step_avg:42.96ms
step:1042/2035 train_time:44781ms step_avg:42.98ms
step:1043/2035 train_time:44842ms step_avg:42.99ms
step:1044/2035 train_time:44901ms step_avg:43.01ms
step:1045/2035 train_time:44961ms step_avg:43.03ms
step:1046/2035 train_time:45021ms step_avg:43.04ms
step:1047/2035 train_time:45081ms step_avg:43.06ms
step:1048/2035 train_time:45140ms step_avg:43.07ms
step:1049/2035 train_time:45201ms step_avg:43.09ms
step:1050/2035 train_time:45261ms step_avg:43.11ms
step:1051/2035 train_time:45322ms step_avg:43.12ms
step:1052/2035 train_time:45382ms step_avg:43.14ms
step:1053/2035 train_time:45443ms step_avg:43.16ms
step:1054/2035 train_time:45502ms step_avg:43.17ms
step:1055/2035 train_time:45564ms step_avg:43.19ms
step:1056/2035 train_time:45623ms step_avg:43.20ms
step:1057/2035 train_time:45683ms step_avg:43.22ms
step:1058/2035 train_time:45742ms step_avg:43.23ms
step:1059/2035 train_time:45802ms step_avg:43.25ms
step:1060/2035 train_time:45861ms step_avg:43.27ms
step:1061/2035 train_time:45921ms step_avg:43.28ms
step:1062/2035 train_time:45980ms step_avg:43.30ms
step:1063/2035 train_time:46040ms step_avg:43.31ms
step:1064/2035 train_time:46100ms step_avg:43.33ms
step:1065/2035 train_time:46161ms step_avg:43.34ms
step:1066/2035 train_time:46221ms step_avg:43.36ms
step:1067/2035 train_time:46281ms step_avg:43.38ms
step:1068/2035 train_time:46342ms step_avg:43.39ms
step:1069/2035 train_time:46402ms step_avg:43.41ms
step:1070/2035 train_time:46462ms step_avg:43.42ms
step:1071/2035 train_time:46523ms step_avg:43.44ms
step:1072/2035 train_time:46582ms step_avg:43.45ms
step:1073/2035 train_time:46643ms step_avg:43.47ms
step:1074/2035 train_time:46702ms step_avg:43.48ms
step:1075/2035 train_time:46763ms step_avg:43.50ms
step:1076/2035 train_time:46822ms step_avg:43.51ms
step:1077/2035 train_time:46882ms step_avg:43.53ms
step:1078/2035 train_time:46941ms step_avg:43.54ms
step:1079/2035 train_time:47001ms step_avg:43.56ms
step:1080/2035 train_time:47061ms step_avg:43.57ms
step:1081/2035 train_time:47121ms step_avg:43.59ms
step:1082/2035 train_time:47181ms step_avg:43.60ms
step:1083/2035 train_time:47242ms step_avg:43.62ms
step:1084/2035 train_time:47302ms step_avg:43.64ms
step:1085/2035 train_time:47363ms step_avg:43.65ms
step:1086/2035 train_time:47422ms step_avg:43.67ms
step:1087/2035 train_time:47483ms step_avg:43.68ms
step:1088/2035 train_time:47543ms step_avg:43.70ms
step:1089/2035 train_time:47603ms step_avg:43.71ms
step:1090/2035 train_time:47664ms step_avg:43.73ms
step:1091/2035 train_time:47724ms step_avg:43.74ms
step:1092/2035 train_time:47782ms step_avg:43.76ms
step:1093/2035 train_time:47843ms step_avg:43.77ms
step:1094/2035 train_time:47902ms step_avg:43.79ms
step:1095/2035 train_time:47961ms step_avg:43.80ms
step:1096/2035 train_time:48020ms step_avg:43.81ms
step:1097/2035 train_time:48081ms step_avg:43.83ms
step:1098/2035 train_time:48140ms step_avg:43.84ms
step:1099/2035 train_time:48202ms step_avg:43.86ms
step:1100/2035 train_time:48262ms step_avg:43.87ms
step:1101/2035 train_time:48322ms step_avg:43.89ms
step:1102/2035 train_time:48382ms step_avg:43.90ms
step:1103/2035 train_time:48443ms step_avg:43.92ms
step:1104/2035 train_time:48503ms step_avg:43.93ms
step:1105/2035 train_time:48563ms step_avg:43.95ms
step:1106/2035 train_time:48623ms step_avg:43.96ms
step:1107/2035 train_time:48683ms step_avg:43.98ms
step:1108/2035 train_time:48743ms step_avg:43.99ms
step:1109/2035 train_time:48803ms step_avg:44.01ms
step:1110/2035 train_time:48863ms step_avg:44.02ms
step:1111/2035 train_time:48922ms step_avg:44.03ms
step:1112/2035 train_time:48982ms step_avg:44.05ms
step:1113/2035 train_time:49042ms step_avg:44.06ms
step:1114/2035 train_time:49101ms step_avg:44.08ms
step:1115/2035 train_time:49161ms step_avg:44.09ms
step:1116/2035 train_time:49220ms step_avg:44.10ms
step:1117/2035 train_time:49281ms step_avg:44.12ms
step:1118/2035 train_time:49341ms step_avg:44.13ms
step:1119/2035 train_time:49402ms step_avg:44.15ms
step:1120/2035 train_time:49463ms step_avg:44.16ms
step:1121/2035 train_time:49523ms step_avg:44.18ms
step:1122/2035 train_time:49583ms step_avg:44.19ms
step:1123/2035 train_time:49644ms step_avg:44.21ms
step:1124/2035 train_time:49704ms step_avg:44.22ms
step:1125/2035 train_time:49765ms step_avg:44.24ms
step:1126/2035 train_time:49824ms step_avg:44.25ms
step:1127/2035 train_time:49884ms step_avg:44.26ms
step:1128/2035 train_time:49943ms step_avg:44.28ms
step:1129/2035 train_time:50004ms step_avg:44.29ms
step:1130/2035 train_time:50064ms step_avg:44.30ms
step:1131/2035 train_time:50124ms step_avg:44.32ms
step:1132/2035 train_time:50183ms step_avg:44.33ms
step:1133/2035 train_time:50244ms step_avg:44.35ms
step:1134/2035 train_time:50304ms step_avg:44.36ms
step:1135/2035 train_time:50365ms step_avg:44.37ms
step:1136/2035 train_time:50424ms step_avg:44.39ms
step:1137/2035 train_time:50484ms step_avg:44.40ms
step:1138/2035 train_time:50544ms step_avg:44.41ms
step:1139/2035 train_time:50605ms step_avg:44.43ms
step:1140/2035 train_time:50665ms step_avg:44.44ms
step:1141/2035 train_time:50725ms step_avg:44.46ms
step:1142/2035 train_time:50785ms step_avg:44.47ms
step:1143/2035 train_time:50844ms step_avg:44.48ms
step:1144/2035 train_time:50904ms step_avg:44.50ms
step:1145/2035 train_time:50964ms step_avg:44.51ms
step:1146/2035 train_time:51023ms step_avg:44.52ms
step:1147/2035 train_time:51083ms step_avg:44.54ms
step:1148/2035 train_time:51142ms step_avg:44.55ms
step:1149/2035 train_time:51202ms step_avg:44.56ms
step:1150/2035 train_time:51262ms step_avg:44.58ms
step:1151/2035 train_time:51323ms step_avg:44.59ms
step:1152/2035 train_time:51382ms step_avg:44.60ms
step:1153/2035 train_time:51442ms step_avg:44.62ms
step:1154/2035 train_time:51502ms step_avg:44.63ms
step:1155/2035 train_time:51563ms step_avg:44.64ms
step:1156/2035 train_time:51623ms step_avg:44.66ms
step:1157/2035 train_time:51683ms step_avg:44.67ms
step:1158/2035 train_time:51742ms step_avg:44.68ms
step:1159/2035 train_time:51803ms step_avg:44.70ms
step:1160/2035 train_time:51863ms step_avg:44.71ms
step:1161/2035 train_time:51923ms step_avg:44.72ms
step:1162/2035 train_time:51982ms step_avg:44.73ms
step:1163/2035 train_time:52042ms step_avg:44.75ms
step:1164/2035 train_time:52101ms step_avg:44.76ms
step:1165/2035 train_time:52162ms step_avg:44.77ms
step:1166/2035 train_time:52222ms step_avg:44.79ms
step:1167/2035 train_time:52283ms step_avg:44.80ms
step:1168/2035 train_time:52342ms step_avg:44.81ms
step:1169/2035 train_time:52402ms step_avg:44.83ms
step:1170/2035 train_time:52463ms step_avg:44.84ms
step:1171/2035 train_time:52522ms step_avg:44.85ms
step:1172/2035 train_time:52582ms step_avg:44.86ms
step:1173/2035 train_time:52642ms step_avg:44.88ms
step:1174/2035 train_time:52701ms step_avg:44.89ms
step:1175/2035 train_time:52762ms step_avg:44.90ms
step:1176/2035 train_time:52821ms step_avg:44.92ms
step:1177/2035 train_time:52882ms step_avg:44.93ms
step:1178/2035 train_time:52941ms step_avg:44.94ms
step:1179/2035 train_time:53001ms step_avg:44.95ms
step:1180/2035 train_time:53061ms step_avg:44.97ms
step:1181/2035 train_time:53121ms step_avg:44.98ms
step:1182/2035 train_time:53181ms step_avg:44.99ms
step:1183/2035 train_time:53241ms step_avg:45.01ms
step:1184/2035 train_time:53301ms step_avg:45.02ms
step:1185/2035 train_time:53362ms step_avg:45.03ms
step:1186/2035 train_time:53421ms step_avg:45.04ms
step:1187/2035 train_time:53481ms step_avg:45.06ms
step:1188/2035 train_time:53541ms step_avg:45.07ms
step:1189/2035 train_time:53602ms step_avg:45.08ms
step:1190/2035 train_time:53662ms step_avg:45.09ms
step:1191/2035 train_time:53722ms step_avg:45.11ms
step:1192/2035 train_time:53782ms step_avg:45.12ms
step:1193/2035 train_time:53842ms step_avg:45.13ms
step:1194/2035 train_time:53902ms step_avg:45.14ms
step:1195/2035 train_time:53962ms step_avg:45.16ms
step:1196/2035 train_time:54021ms step_avg:45.17ms
step:1197/2035 train_time:54082ms step_avg:45.18ms
step:1198/2035 train_time:54142ms step_avg:45.19ms
step:1199/2035 train_time:54202ms step_avg:45.21ms
step:1200/2035 train_time:54263ms step_avg:45.22ms
step:1201/2035 train_time:54323ms step_avg:45.23ms
step:1202/2035 train_time:54382ms step_avg:45.24ms
step:1203/2035 train_time:54442ms step_avg:45.26ms
step:1204/2035 train_time:54502ms step_avg:45.27ms
step:1205/2035 train_time:54562ms step_avg:45.28ms
step:1206/2035 train_time:54622ms step_avg:45.29ms
step:1207/2035 train_time:54682ms step_avg:45.30ms
step:1208/2035 train_time:54742ms step_avg:45.32ms
step:1209/2035 train_time:54802ms step_avg:45.33ms
step:1210/2035 train_time:54862ms step_avg:45.34ms
step:1211/2035 train_time:54921ms step_avg:45.35ms
step:1212/2035 train_time:54981ms step_avg:45.36ms
step:1213/2035 train_time:55042ms step_avg:45.38ms
step:1214/2035 train_time:55101ms step_avg:45.39ms
step:1215/2035 train_time:55162ms step_avg:45.40ms
step:1216/2035 train_time:55222ms step_avg:45.41ms
step:1217/2035 train_time:55282ms step_avg:45.42ms
step:1218/2035 train_time:55341ms step_avg:45.44ms
step:1219/2035 train_time:55402ms step_avg:45.45ms
step:1220/2035 train_time:55462ms step_avg:45.46ms
step:1221/2035 train_time:55522ms step_avg:45.47ms
step:1222/2035 train_time:55581ms step_avg:45.48ms
step:1223/2035 train_time:55641ms step_avg:45.50ms
step:1224/2035 train_time:55701ms step_avg:45.51ms
step:1225/2035 train_time:55762ms step_avg:45.52ms
step:1226/2035 train_time:55822ms step_avg:45.53ms
step:1227/2035 train_time:55883ms step_avg:45.54ms
step:1228/2035 train_time:55942ms step_avg:45.56ms
step:1229/2035 train_time:56003ms step_avg:45.57ms
step:1230/2035 train_time:56062ms step_avg:45.58ms
step:1231/2035 train_time:56122ms step_avg:45.59ms
step:1232/2035 train_time:56182ms step_avg:45.60ms
step:1233/2035 train_time:56242ms step_avg:45.61ms
step:1234/2035 train_time:56303ms step_avg:45.63ms
step:1235/2035 train_time:56363ms step_avg:45.64ms
step:1236/2035 train_time:56423ms step_avg:45.65ms
step:1237/2035 train_time:56483ms step_avg:45.66ms
step:1238/2035 train_time:56543ms step_avg:45.67ms
step:1239/2035 train_time:56602ms step_avg:45.68ms
step:1240/2035 train_time:56662ms step_avg:45.70ms
step:1241/2035 train_time:56722ms step_avg:45.71ms
step:1242/2035 train_time:56782ms step_avg:45.72ms
step:1243/2035 train_time:56842ms step_avg:45.73ms
step:1244/2035 train_time:56902ms step_avg:45.74ms
step:1245/2035 train_time:56962ms step_avg:45.75ms
step:1246/2035 train_time:57021ms step_avg:45.76ms
step:1247/2035 train_time:57082ms step_avg:45.78ms
step:1248/2035 train_time:57141ms step_avg:45.79ms
step:1249/2035 train_time:57202ms step_avg:45.80ms
step:1250/2035 train_time:57262ms step_avg:45.81ms
step:1250/2035 val_loss:3.5652 train_time:57324ms step_avg:45.86ms
step:1251/2035 train_time:57345ms step_avg:45.84ms
step:1252/2035 train_time:57384ms step_avg:45.83ms
step:1253/2035 train_time:57449ms step_avg:45.85ms
step:1254/2035 train_time:57514ms step_avg:45.86ms
step:1255/2035 train_time:57574ms step_avg:45.88ms
step:1256/2035 train_time:57634ms step_avg:45.89ms
step:1257/2035 train_time:57695ms step_avg:45.90ms
step:1258/2035 train_time:57754ms step_avg:45.91ms
step:1259/2035 train_time:57814ms step_avg:45.92ms
step:1260/2035 train_time:57873ms step_avg:45.93ms
step:1261/2035 train_time:57932ms step_avg:45.94ms
step:1262/2035 train_time:57991ms step_avg:45.95ms
step:1263/2035 train_time:58050ms step_avg:45.96ms
step:1264/2035 train_time:58109ms step_avg:45.97ms
step:1265/2035 train_time:58169ms step_avg:45.98ms
step:1266/2035 train_time:58230ms step_avg:46.00ms
step:1267/2035 train_time:58294ms step_avg:46.01ms
step:1268/2035 train_time:58355ms step_avg:46.02ms
step:1269/2035 train_time:58417ms step_avg:46.03ms
step:1270/2035 train_time:58478ms step_avg:46.05ms
step:1271/2035 train_time:58540ms step_avg:46.06ms
step:1272/2035 train_time:58601ms step_avg:46.07ms
step:1273/2035 train_time:58662ms step_avg:46.08ms
step:1274/2035 train_time:58723ms step_avg:46.09ms
step:1275/2035 train_time:58783ms step_avg:46.10ms
step:1276/2035 train_time:58843ms step_avg:46.11ms
step:1277/2035 train_time:58903ms step_avg:46.13ms
step:1278/2035 train_time:58962ms step_avg:46.14ms
step:1279/2035 train_time:59022ms step_avg:46.15ms
step:1280/2035 train_time:59082ms step_avg:46.16ms
step:1281/2035 train_time:59142ms step_avg:46.17ms
step:1282/2035 train_time:59201ms step_avg:46.18ms
step:1283/2035 train_time:59262ms step_avg:46.19ms
step:1284/2035 train_time:59322ms step_avg:46.20ms
step:1285/2035 train_time:59384ms step_avg:46.21ms
step:1286/2035 train_time:59445ms step_avg:46.22ms
step:1287/2035 train_time:59506ms step_avg:46.24ms
step:1288/2035 train_time:59568ms step_avg:46.25ms
step:1289/2035 train_time:59627ms step_avg:46.26ms
step:1290/2035 train_time:59688ms step_avg:46.27ms
step:1291/2035 train_time:59748ms step_avg:46.28ms
step:1292/2035 train_time:59807ms step_avg:46.29ms
step:1293/2035 train_time:59867ms step_avg:46.30ms
step:1294/2035 train_time:59927ms step_avg:46.31ms
step:1295/2035 train_time:59987ms step_avg:46.32ms
step:1296/2035 train_time:60046ms step_avg:46.33ms
step:1297/2035 train_time:60106ms step_avg:46.34ms
step:1298/2035 train_time:60165ms step_avg:46.35ms
step:1299/2035 train_time:60225ms step_avg:46.36ms
step:1300/2035 train_time:60285ms step_avg:46.37ms
step:1301/2035 train_time:60345ms step_avg:46.38ms
step:1302/2035 train_time:60406ms step_avg:46.39ms
step:1303/2035 train_time:60467ms step_avg:46.41ms
step:1304/2035 train_time:60528ms step_avg:46.42ms
step:1305/2035 train_time:60589ms step_avg:46.43ms
step:1306/2035 train_time:60649ms step_avg:46.44ms
step:1307/2035 train_time:60709ms step_avg:46.45ms
step:1308/2035 train_time:60768ms step_avg:46.46ms
step:1309/2035 train_time:60828ms step_avg:46.47ms
step:1310/2035 train_time:60888ms step_avg:46.48ms
step:1311/2035 train_time:60947ms step_avg:46.49ms
step:1312/2035 train_time:61007ms step_avg:46.50ms
step:1313/2035 train_time:61066ms step_avg:46.51ms
step:1314/2035 train_time:61126ms step_avg:46.52ms
step:1315/2035 train_time:61186ms step_avg:46.53ms
step:1316/2035 train_time:61246ms step_avg:46.54ms
step:1317/2035 train_time:61306ms step_avg:46.55ms
step:1318/2035 train_time:61366ms step_avg:46.56ms
step:1319/2035 train_time:61426ms step_avg:46.57ms
step:1320/2035 train_time:61487ms step_avg:46.58ms
step:1321/2035 train_time:61547ms step_avg:46.59ms
step:1322/2035 train_time:61607ms step_avg:46.60ms
step:1323/2035 train_time:61667ms step_avg:46.61ms
step:1324/2035 train_time:61727ms step_avg:46.62ms
step:1325/2035 train_time:61788ms step_avg:46.63ms
step:1326/2035 train_time:61847ms step_avg:46.64ms
step:1327/2035 train_time:61907ms step_avg:46.65ms
step:1328/2035 train_time:61966ms step_avg:46.66ms
step:1329/2035 train_time:62026ms step_avg:46.67ms
step:1330/2035 train_time:62086ms step_avg:46.68ms
step:1331/2035 train_time:62146ms step_avg:46.69ms
step:1332/2035 train_time:62234ms step_avg:46.72ms
step:1333/2035 train_time:62322ms step_avg:46.75ms
step:1334/2035 train_time:62409ms step_avg:46.78ms
step:1335/2035 train_time:62498ms step_avg:46.82ms
step:1336/2035 train_time:62586ms step_avg:46.85ms
step:1337/2035 train_time:62674ms step_avg:46.88ms
step:1338/2035 train_time:62761ms step_avg:46.91ms
step:1339/2035 train_time:62849ms step_avg:46.94ms
step:1340/2035 train_time:62937ms step_avg:46.97ms
step:1341/2035 train_time:63025ms step_avg:47.00ms
step:1342/2035 train_time:63112ms step_avg:47.03ms
step:1343/2035 train_time:63199ms step_avg:47.06ms
step:1344/2035 train_time:63287ms step_avg:47.09ms
step:1345/2035 train_time:63375ms step_avg:47.12ms
step:1346/2035 train_time:63462ms step_avg:47.15ms
step:1347/2035 train_time:63551ms step_avg:47.18ms
step:1348/2035 train_time:63640ms step_avg:47.21ms
step:1349/2035 train_time:63729ms step_avg:47.24ms
step:1350/2035 train_time:63818ms step_avg:47.27ms
step:1351/2035 train_time:63905ms step_avg:47.30ms
step:1352/2035 train_time:63994ms step_avg:47.33ms
step:1353/2035 train_time:64081ms step_avg:47.36ms
step:1354/2035 train_time:64168ms step_avg:47.39ms
step:1355/2035 train_time:64257ms step_avg:47.42ms
step:1356/2035 train_time:64344ms step_avg:47.45ms
step:1357/2035 train_time:64432ms step_avg:47.48ms
step:1358/2035 train_time:64520ms step_avg:47.51ms
step:1359/2035 train_time:64608ms step_avg:47.54ms
step:1360/2035 train_time:64696ms step_avg:47.57ms
step:1361/2035 train_time:64783ms step_avg:47.60ms
step:1362/2035 train_time:64870ms step_avg:47.63ms
step:1363/2035 train_time:64959ms step_avg:47.66ms
step:1364/2035 train_time:65047ms step_avg:47.69ms
step:1365/2035 train_time:65135ms step_avg:47.72ms
step:1366/2035 train_time:65222ms step_avg:47.75ms
step:1367/2035 train_time:65311ms step_avg:47.78ms
step:1368/2035 train_time:65399ms step_avg:47.81ms
step:1369/2035 train_time:65487ms step_avg:47.84ms
step:1370/2035 train_time:65575ms step_avg:47.86ms
step:1371/2035 train_time:65662ms step_avg:47.89ms
step:1372/2035 train_time:65749ms step_avg:47.92ms
step:1373/2035 train_time:65838ms step_avg:47.95ms
step:1374/2035 train_time:65926ms step_avg:47.98ms
step:1375/2035 train_time:66014ms step_avg:48.01ms
step:1376/2035 train_time:66101ms step_avg:48.04ms
step:1377/2035 train_time:66189ms step_avg:48.07ms
step:1378/2035 train_time:66277ms step_avg:48.10ms
step:1379/2035 train_time:66365ms step_avg:48.13ms
step:1380/2035 train_time:66453ms step_avg:48.15ms
step:1381/2035 train_time:66541ms step_avg:48.18ms
step:1382/2035 train_time:66629ms step_avg:48.21ms
step:1383/2035 train_time:66717ms step_avg:48.24ms
step:1384/2035 train_time:66804ms step_avg:48.27ms
step:1385/2035 train_time:66893ms step_avg:48.30ms
step:1386/2035 train_time:66981ms step_avg:48.33ms
step:1387/2035 train_time:67068ms step_avg:48.35ms
step:1388/2035 train_time:67157ms step_avg:48.38ms
step:1389/2035 train_time:67244ms step_avg:48.41ms
step:1390/2035 train_time:67333ms step_avg:48.44ms
step:1391/2035 train_time:67421ms step_avg:48.47ms
step:1392/2035 train_time:67509ms step_avg:48.50ms
step:1393/2035 train_time:67597ms step_avg:48.53ms
step:1394/2035 train_time:67685ms step_avg:48.55ms
step:1395/2035 train_time:67773ms step_avg:48.58ms
step:1396/2035 train_time:67861ms step_avg:48.61ms
step:1397/2035 train_time:67948ms step_avg:48.64ms
step:1398/2035 train_time:68036ms step_avg:48.67ms
step:1399/2035 train_time:68124ms step_avg:48.69ms
step:1400/2035 train_time:68211ms step_avg:48.72ms
step:1401/2035 train_time:68299ms step_avg:48.75ms
step:1402/2035 train_time:68388ms step_avg:48.78ms
step:1403/2035 train_time:68476ms step_avg:48.81ms
step:1404/2035 train_time:68563ms step_avg:48.83ms
step:1405/2035 train_time:68652ms step_avg:48.86ms
step:1406/2035 train_time:68739ms step_avg:48.89ms
step:1407/2035 train_time:68827ms step_avg:48.92ms
step:1408/2035 train_time:68916ms step_avg:48.95ms
step:1409/2035 train_time:69003ms step_avg:48.97ms
step:1410/2035 train_time:69091ms step_avg:49.00ms
step:1411/2035 train_time:69179ms step_avg:49.03ms
step:1412/2035 train_time:69266ms step_avg:49.06ms
step:1413/2035 train_time:69356ms step_avg:49.08ms
step:1414/2035 train_time:69443ms step_avg:49.11ms
step:1415/2035 train_time:69531ms step_avg:49.14ms
step:1416/2035 train_time:69619ms step_avg:49.17ms
step:1417/2035 train_time:69707ms step_avg:49.19ms
step:1418/2035 train_time:69795ms step_avg:49.22ms
step:1419/2035 train_time:69883ms step_avg:49.25ms
step:1420/2035 train_time:69971ms step_avg:49.28ms
step:1421/2035 train_time:70060ms step_avg:49.30ms
step:1422/2035 train_time:70147ms step_avg:49.33ms
step:1423/2035 train_time:70235ms step_avg:49.36ms
step:1424/2035 train_time:70323ms step_avg:49.38ms
step:1425/2035 train_time:70411ms step_avg:49.41ms
step:1426/2035 train_time:70498ms step_avg:49.44ms
step:1427/2035 train_time:70586ms step_avg:49.46ms
step:1428/2035 train_time:70674ms step_avg:49.49ms
step:1429/2035 train_time:70762ms step_avg:49.52ms
step:1430/2035 train_time:70851ms step_avg:49.55ms
step:1431/2035 train_time:70940ms step_avg:49.57ms
step:1432/2035 train_time:71028ms step_avg:49.60ms
step:1433/2035 train_time:71117ms step_avg:49.63ms
step:1434/2035 train_time:71204ms step_avg:49.65ms
step:1435/2035 train_time:71293ms step_avg:49.68ms
step:1436/2035 train_time:71380ms step_avg:49.71ms
step:1437/2035 train_time:71468ms step_avg:49.73ms
step:1438/2035 train_time:71556ms step_avg:49.76ms
step:1439/2035 train_time:71643ms step_avg:49.79ms
step:1440/2035 train_time:71731ms step_avg:49.81ms
step:1441/2035 train_time:71820ms step_avg:49.84ms
step:1442/2035 train_time:71908ms step_avg:49.87ms
step:1443/2035 train_time:71996ms step_avg:49.89ms
step:1444/2035 train_time:72083ms step_avg:49.92ms
step:1445/2035 train_time:72171ms step_avg:49.95ms
step:1446/2035 train_time:72260ms step_avg:49.97ms
step:1447/2035 train_time:72348ms step_avg:50.00ms
step:1448/2035 train_time:72436ms step_avg:50.03ms
step:1449/2035 train_time:72524ms step_avg:50.05ms
step:1450/2035 train_time:72612ms step_avg:50.08ms
step:1451/2035 train_time:72700ms step_avg:50.10ms
step:1452/2035 train_time:72788ms step_avg:50.13ms
step:1453/2035 train_time:72876ms step_avg:50.16ms
step:1454/2035 train_time:72963ms step_avg:50.18ms
step:1455/2035 train_time:73052ms step_avg:50.21ms
step:1456/2035 train_time:73141ms step_avg:50.23ms
step:1457/2035 train_time:73228ms step_avg:50.26ms
step:1458/2035 train_time:73316ms step_avg:50.29ms
step:1459/2035 train_time:73403ms step_avg:50.31ms
step:1460/2035 train_time:73491ms step_avg:50.34ms
step:1461/2035 train_time:73580ms step_avg:50.36ms
step:1462/2035 train_time:73668ms step_avg:50.39ms
step:1463/2035 train_time:73757ms step_avg:50.41ms
step:1464/2035 train_time:73844ms step_avg:50.44ms
step:1465/2035 train_time:73933ms step_avg:50.47ms
step:1466/2035 train_time:74020ms step_avg:50.49ms
step:1467/2035 train_time:74107ms step_avg:50.52ms
step:1468/2035 train_time:74195ms step_avg:50.54ms
step:1469/2035 train_time:74283ms step_avg:50.57ms
step:1470/2035 train_time:74370ms step_avg:50.59ms
step:1471/2035 train_time:74458ms step_avg:50.62ms
step:1472/2035 train_time:74546ms step_avg:50.64ms
step:1473/2035 train_time:74635ms step_avg:50.67ms
step:1474/2035 train_time:74722ms step_avg:50.69ms
step:1475/2035 train_time:74811ms step_avg:50.72ms
step:1476/2035 train_time:74899ms step_avg:50.74ms
step:1477/2035 train_time:74987ms step_avg:50.77ms
step:1478/2035 train_time:75075ms step_avg:50.79ms
step:1479/2035 train_time:75162ms step_avg:50.82ms
step:1480/2035 train_time:75250ms step_avg:50.84ms
step:1481/2035 train_time:75339ms step_avg:50.87ms
step:1482/2035 train_time:75426ms step_avg:50.89ms
step:1483/2035 train_time:75514ms step_avg:50.92ms
step:1484/2035 train_time:75601ms step_avg:50.94ms
step:1485/2035 train_time:75689ms step_avg:50.97ms
step:1486/2035 train_time:75777ms step_avg:50.99ms
step:1487/2035 train_time:75864ms step_avg:51.02ms
step:1488/2035 train_time:75952ms step_avg:51.04ms
step:1489/2035 train_time:76040ms step_avg:51.07ms
step:1490/2035 train_time:76128ms step_avg:51.09ms
step:1491/2035 train_time:76216ms step_avg:51.12ms
step:1492/2035 train_time:76303ms step_avg:51.14ms
step:1493/2035 train_time:76391ms step_avg:51.17ms
step:1494/2035 train_time:76478ms step_avg:51.19ms
step:1495/2035 train_time:76566ms step_avg:51.21ms
step:1496/2035 train_time:76654ms step_avg:51.24ms
step:1497/2035 train_time:76742ms step_avg:51.26ms
step:1498/2035 train_time:76830ms step_avg:51.29ms
step:1499/2035 train_time:76920ms step_avg:51.31ms
step:1500/2035 train_time:77008ms step_avg:51.34ms
step:1500/2035 val_loss:3.4535 train_time:77097ms step_avg:51.40ms
step:1501/2035 train_time:77118ms step_avg:51.38ms
step:1502/2035 train_time:77186ms step_avg:51.39ms
step:1503/2035 train_time:77285ms step_avg:51.42ms
step:1504/2035 train_time:77375ms step_avg:51.45ms
step:1505/2035 train_time:77463ms step_avg:51.47ms
step:1506/2035 train_time:77549ms step_avg:51.49ms
step:1507/2035 train_time:77636ms step_avg:51.52ms
step:1508/2035 train_time:77722ms step_avg:51.54ms
step:1509/2035 train_time:77810ms step_avg:51.56ms
step:1510/2035 train_time:77897ms step_avg:51.59ms
step:1511/2035 train_time:77984ms step_avg:51.61ms
step:1512/2035 train_time:78072ms step_avg:51.64ms
step:1513/2035 train_time:78163ms step_avg:51.66ms
step:1514/2035 train_time:78255ms step_avg:51.69ms
step:1515/2035 train_time:78344ms step_avg:51.71ms
step:1516/2035 train_time:78433ms step_avg:51.74ms
step:1517/2035 train_time:78521ms step_avg:51.76ms
step:1518/2035 train_time:78608ms step_avg:51.78ms
step:1519/2035 train_time:78695ms step_avg:51.81ms
step:1520/2035 train_time:78781ms step_avg:51.83ms
step:1521/2035 train_time:78868ms step_avg:51.85ms
step:1522/2035 train_time:78955ms step_avg:51.88ms
step:1523/2035 train_time:79043ms step_avg:51.90ms
step:1524/2035 train_time:79132ms step_avg:51.92ms
step:1525/2035 train_time:79223ms step_avg:51.95ms
step:1526/2035 train_time:79313ms step_avg:51.97ms
step:1527/2035 train_time:79402ms step_avg:52.00ms
step:1528/2035 train_time:79490ms step_avg:52.02ms
step:1529/2035 train_time:79579ms step_avg:52.05ms
step:1530/2035 train_time:79666ms step_avg:52.07ms
step:1531/2035 train_time:79754ms step_avg:52.09ms
step:1532/2035 train_time:79841ms step_avg:52.12ms
step:1533/2035 train_time:79928ms step_avg:52.14ms
step:1534/2035 train_time:80016ms step_avg:52.16ms
step:1535/2035 train_time:80104ms step_avg:52.18ms
step:1536/2035 train_time:80193ms step_avg:52.21ms
step:1537/2035 train_time:80282ms step_avg:52.23ms
step:1538/2035 train_time:80371ms step_avg:52.26ms
step:1539/2035 train_time:80460ms step_avg:52.28ms
step:1540/2035 train_time:80549ms step_avg:52.30ms
step:1541/2035 train_time:80637ms step_avg:52.33ms
step:1542/2035 train_time:80723ms step_avg:52.35ms
step:1543/2035 train_time:80811ms step_avg:52.37ms
step:1544/2035 train_time:80898ms step_avg:52.39ms
step:1545/2035 train_time:80985ms step_avg:52.42ms
step:1546/2035 train_time:81073ms step_avg:52.44ms
step:1547/2035 train_time:81161ms step_avg:52.46ms
step:1548/2035 train_time:81250ms step_avg:52.49ms
step:1549/2035 train_time:81340ms step_avg:52.51ms
step:1550/2035 train_time:81428ms step_avg:52.53ms
step:1551/2035 train_time:81517ms step_avg:52.56ms
step:1552/2035 train_time:81604ms step_avg:52.58ms
step:1553/2035 train_time:81692ms step_avg:52.60ms
step:1554/2035 train_time:81779ms step_avg:52.63ms
step:1555/2035 train_time:81866ms step_avg:52.65ms
step:1556/2035 train_time:81954ms step_avg:52.67ms
step:1557/2035 train_time:82042ms step_avg:52.69ms
step:1558/2035 train_time:82130ms step_avg:52.72ms
step:1559/2035 train_time:82219ms step_avg:52.74ms
step:1560/2035 train_time:82308ms step_avg:52.76ms
step:1561/2035 train_time:82396ms step_avg:52.78ms
step:1562/2035 train_time:82483ms step_avg:52.81ms
step:1563/2035 train_time:82572ms step_avg:52.83ms
step:1564/2035 train_time:82660ms step_avg:52.85ms
step:1565/2035 train_time:82747ms step_avg:52.87ms
step:1566/2035 train_time:82835ms step_avg:52.90ms
step:1567/2035 train_time:82923ms step_avg:52.92ms
step:1568/2035 train_time:83010ms step_avg:52.94ms
step:1569/2035 train_time:83098ms step_avg:52.96ms
step:1570/2035 train_time:83185ms step_avg:52.98ms
step:1571/2035 train_time:83275ms step_avg:53.01ms
step:1572/2035 train_time:83363ms step_avg:53.03ms
step:1573/2035 train_time:83451ms step_avg:53.05ms
step:1574/2035 train_time:83540ms step_avg:53.08ms
step:1575/2035 train_time:83628ms step_avg:53.10ms
step:1576/2035 train_time:83715ms step_avg:53.12ms
step:1577/2035 train_time:83803ms step_avg:53.14ms
step:1578/2035 train_time:83890ms step_avg:53.16ms
step:1579/2035 train_time:83979ms step_avg:53.19ms
step:1580/2035 train_time:84068ms step_avg:53.21ms
step:1581/2035 train_time:84157ms step_avg:53.23ms
step:1582/2035 train_time:84244ms step_avg:53.25ms
step:1583/2035 train_time:84333ms step_avg:53.27ms
step:1584/2035 train_time:84421ms step_avg:53.30ms
step:1585/2035 train_time:84510ms step_avg:53.32ms
step:1586/2035 train_time:84598ms step_avg:53.34ms
step:1587/2035 train_time:84686ms step_avg:53.36ms
step:1588/2035 train_time:84773ms step_avg:53.38ms
step:1589/2035 train_time:84862ms step_avg:53.41ms
step:1590/2035 train_time:84949ms step_avg:53.43ms
step:1591/2035 train_time:85038ms step_avg:53.45ms
step:1592/2035 train_time:85125ms step_avg:53.47ms
step:1593/2035 train_time:85213ms step_avg:53.49ms
step:1594/2035 train_time:85300ms step_avg:53.51ms
step:1595/2035 train_time:85388ms step_avg:53.53ms
step:1596/2035 train_time:85476ms step_avg:53.56ms
step:1597/2035 train_time:85564ms step_avg:53.58ms
step:1598/2035 train_time:85652ms step_avg:53.60ms
step:1599/2035 train_time:85740ms step_avg:53.62ms
step:1600/2035 train_time:85828ms step_avg:53.64ms
step:1601/2035 train_time:85916ms step_avg:53.66ms
step:1602/2035 train_time:86002ms step_avg:53.68ms
step:1603/2035 train_time:86091ms step_avg:53.71ms
step:1604/2035 train_time:86180ms step_avg:53.73ms
step:1605/2035 train_time:86268ms step_avg:53.75ms
step:1606/2035 train_time:86356ms step_avg:53.77ms
step:1607/2035 train_time:86443ms step_avg:53.79ms
step:1608/2035 train_time:86531ms step_avg:53.81ms
step:1609/2035 train_time:86619ms step_avg:53.83ms
step:1610/2035 train_time:86707ms step_avg:53.86ms
step:1611/2035 train_time:86794ms step_avg:53.88ms
step:1612/2035 train_time:86882ms step_avg:53.90ms
step:1613/2035 train_time:86971ms step_avg:53.92ms
step:1614/2035 train_time:87058ms step_avg:53.94ms
step:1615/2035 train_time:87146ms step_avg:53.96ms
step:1616/2035 train_time:87235ms step_avg:53.98ms
step:1617/2035 train_time:87322ms step_avg:54.00ms
step:1618/2035 train_time:87410ms step_avg:54.02ms
step:1619/2035 train_time:87498ms step_avg:54.04ms
step:1620/2035 train_time:87585ms step_avg:54.06ms
step:1621/2035 train_time:87674ms step_avg:54.09ms
step:1622/2035 train_time:87762ms step_avg:54.11ms
step:1623/2035 train_time:87850ms step_avg:54.13ms
step:1624/2035 train_time:87939ms step_avg:54.15ms
step:1625/2035 train_time:88026ms step_avg:54.17ms
step:1626/2035 train_time:88114ms step_avg:54.19ms
step:1627/2035 train_time:88201ms step_avg:54.21ms
step:1628/2035 train_time:88289ms step_avg:54.23ms
step:1629/2035 train_time:88378ms step_avg:54.25ms
step:1630/2035 train_time:88466ms step_avg:54.27ms
step:1631/2035 train_time:88555ms step_avg:54.29ms
step:1632/2035 train_time:88642ms step_avg:54.32ms
step:1633/2035 train_time:88730ms step_avg:54.34ms
step:1634/2035 train_time:88818ms step_avg:54.36ms
step:1635/2035 train_time:88906ms step_avg:54.38ms
step:1636/2035 train_time:88994ms step_avg:54.40ms
step:1637/2035 train_time:89082ms step_avg:54.42ms
step:1638/2035 train_time:89169ms step_avg:54.44ms
step:1639/2035 train_time:89258ms step_avg:54.46ms
step:1640/2035 train_time:89347ms step_avg:54.48ms
step:1641/2035 train_time:89436ms step_avg:54.50ms
step:1642/2035 train_time:89523ms step_avg:54.52ms
step:1643/2035 train_time:89611ms step_avg:54.54ms
step:1644/2035 train_time:89699ms step_avg:54.56ms
step:1645/2035 train_time:89787ms step_avg:54.58ms
step:1646/2035 train_time:89875ms step_avg:54.60ms
step:1647/2035 train_time:89963ms step_avg:54.62ms
step:1648/2035 train_time:90050ms step_avg:54.64ms
step:1649/2035 train_time:90139ms step_avg:54.66ms
step:1650/2035 train_time:90227ms step_avg:54.68ms
step:1651/2035 train_time:90316ms step_avg:54.70ms
step:1652/2035 train_time:90404ms step_avg:54.72ms
step:1653/2035 train_time:90492ms step_avg:54.74ms
step:1654/2035 train_time:90580ms step_avg:54.76ms
step:1655/2035 train_time:90669ms step_avg:54.78ms
step:1656/2035 train_time:90756ms step_avg:54.80ms
step:1657/2035 train_time:90844ms step_avg:54.82ms
step:1658/2035 train_time:90931ms step_avg:54.84ms
step:1659/2035 train_time:91019ms step_avg:54.86ms
step:1660/2035 train_time:91107ms step_avg:54.88ms
step:1661/2035 train_time:91195ms step_avg:54.90ms
step:1662/2035 train_time:91282ms step_avg:54.92ms
step:1663/2035 train_time:91371ms step_avg:54.94ms
step:1664/2035 train_time:91460ms step_avg:54.96ms
step:1665/2035 train_time:91548ms step_avg:54.98ms
step:1666/2035 train_time:91636ms step_avg:55.00ms
step:1667/2035 train_time:91723ms step_avg:55.02ms
step:1668/2035 train_time:91812ms step_avg:55.04ms
step:1669/2035 train_time:91900ms step_avg:55.06ms
step:1670/2035 train_time:91987ms step_avg:55.08ms
step:1671/2035 train_time:92076ms step_avg:55.10ms
step:1672/2035 train_time:92163ms step_avg:55.12ms
step:1673/2035 train_time:92252ms step_avg:55.14ms
step:1674/2035 train_time:92341ms step_avg:55.16ms
step:1675/2035 train_time:92429ms step_avg:55.18ms
step:1676/2035 train_time:92517ms step_avg:55.20ms
step:1677/2035 train_time:92605ms step_avg:55.22ms
step:1678/2035 train_time:92692ms step_avg:55.24ms
step:1679/2035 train_time:92780ms step_avg:55.26ms
step:1680/2035 train_time:92868ms step_avg:55.28ms
step:1681/2035 train_time:92957ms step_avg:55.30ms
step:1682/2035 train_time:93045ms step_avg:55.32ms
step:1683/2035 train_time:93133ms step_avg:55.34ms
step:1684/2035 train_time:93221ms step_avg:55.36ms
step:1685/2035 train_time:93310ms step_avg:55.38ms
step:1686/2035 train_time:93398ms step_avg:55.40ms
step:1687/2035 train_time:93486ms step_avg:55.42ms
step:1688/2035 train_time:93574ms step_avg:55.43ms
step:1689/2035 train_time:93662ms step_avg:55.45ms
step:1690/2035 train_time:93750ms step_avg:55.47ms
step:1691/2035 train_time:93838ms step_avg:55.49ms
step:1692/2035 train_time:93926ms step_avg:55.51ms
step:1693/2035 train_time:94014ms step_avg:55.53ms
step:1694/2035 train_time:94101ms step_avg:55.55ms
step:1695/2035 train_time:94190ms step_avg:55.57ms
step:1696/2035 train_time:94279ms step_avg:55.59ms
step:1697/2035 train_time:94368ms step_avg:55.61ms
step:1698/2035 train_time:94456ms step_avg:55.63ms
step:1699/2035 train_time:94543ms step_avg:55.65ms
step:1700/2035 train_time:94631ms step_avg:55.67ms
step:1701/2035 train_time:94719ms step_avg:55.68ms
step:1702/2035 train_time:94807ms step_avg:55.70ms
step:1703/2035 train_time:94895ms step_avg:55.72ms
step:1704/2035 train_time:94983ms step_avg:55.74ms
step:1705/2035 train_time:95071ms step_avg:55.76ms
step:1706/2035 train_time:95159ms step_avg:55.78ms
step:1707/2035 train_time:95247ms step_avg:55.80ms
step:1708/2035 train_time:95336ms step_avg:55.82ms
step:1709/2035 train_time:95423ms step_avg:55.84ms
step:1710/2035 train_time:95511ms step_avg:55.85ms
step:1711/2035 train_time:95599ms step_avg:55.87ms
step:1712/2035 train_time:95687ms step_avg:55.89ms
step:1713/2035 train_time:95776ms step_avg:55.91ms
step:1714/2035 train_time:95863ms step_avg:55.93ms
step:1715/2035 train_time:95951ms step_avg:55.95ms
step:1716/2035 train_time:96039ms step_avg:55.97ms
step:1717/2035 train_time:96128ms step_avg:55.99ms
step:1718/2035 train_time:96216ms step_avg:56.00ms
step:1719/2035 train_time:96304ms step_avg:56.02ms
step:1720/2035 train_time:96391ms step_avg:56.04ms
step:1721/2035 train_time:96481ms step_avg:56.06ms
step:1722/2035 train_time:96570ms step_avg:56.08ms
step:1723/2035 train_time:96658ms step_avg:56.10ms
step:1724/2035 train_time:96745ms step_avg:56.12ms
step:1725/2035 train_time:96833ms step_avg:56.14ms
step:1726/2035 train_time:96921ms step_avg:56.15ms
step:1727/2035 train_time:97009ms step_avg:56.17ms
step:1728/2035 train_time:97097ms step_avg:56.19ms
step:1729/2035 train_time:97184ms step_avg:56.21ms
step:1730/2035 train_time:97273ms step_avg:56.23ms
step:1731/2035 train_time:97361ms step_avg:56.25ms
step:1732/2035 train_time:97449ms step_avg:56.26ms
step:1733/2035 train_time:97538ms step_avg:56.28ms
step:1734/2035 train_time:97626ms step_avg:56.30ms
step:1735/2035 train_time:97715ms step_avg:56.32ms
step:1736/2035 train_time:97802ms step_avg:56.34ms
step:1737/2035 train_time:97890ms step_avg:56.36ms
step:1738/2035 train_time:97978ms step_avg:56.37ms
step:1739/2035 train_time:98067ms step_avg:56.39ms
step:1740/2035 train_time:98155ms step_avg:56.41ms
step:1741/2035 train_time:98243ms step_avg:56.43ms
step:1742/2035 train_time:98330ms step_avg:56.45ms
step:1743/2035 train_time:98419ms step_avg:56.47ms
step:1744/2035 train_time:98507ms step_avg:56.48ms
step:1745/2035 train_time:98595ms step_avg:56.50ms
step:1746/2035 train_time:98682ms step_avg:56.52ms
step:1747/2035 train_time:98772ms step_avg:56.54ms
step:1748/2035 train_time:98860ms step_avg:56.56ms
step:1749/2035 train_time:98948ms step_avg:56.57ms
step:1750/2035 train_time:99037ms step_avg:56.59ms
step:1750/2035 val_loss:3.3568 train_time:99126ms step_avg:56.64ms
step:1751/2035 train_time:99146ms step_avg:56.62ms
step:1752/2035 train_time:99216ms step_avg:56.63ms
step:1753/2035 train_time:99311ms step_avg:56.65ms
step:1754/2035 train_time:99399ms step_avg:56.67ms
step:1755/2035 train_time:99487ms step_avg:56.69ms
step:1756/2035 train_time:99576ms step_avg:56.71ms
step:1757/2035 train_time:99663ms step_avg:56.72ms
step:1758/2035 train_time:99749ms step_avg:56.74ms
step:1759/2035 train_time:99836ms step_avg:56.76ms
step:1760/2035 train_time:99922ms step_avg:56.77ms
step:1761/2035 train_time:100009ms step_avg:56.79ms
step:1762/2035 train_time:100096ms step_avg:56.81ms
step:1763/2035 train_time:100187ms step_avg:56.83ms
step:1764/2035 train_time:100280ms step_avg:56.85ms
step:1765/2035 train_time:100370ms step_avg:56.87ms
step:1766/2035 train_time:100457ms step_avg:56.88ms
step:1767/2035 train_time:100545ms step_avg:56.90ms
step:1768/2035 train_time:100633ms step_avg:56.92ms
step:1769/2035 train_time:100720ms step_avg:56.94ms
step:1770/2035 train_time:100806ms step_avg:56.95ms
step:1771/2035 train_time:100893ms step_avg:56.97ms
step:1772/2035 train_time:100980ms step_avg:56.99ms
step:1773/2035 train_time:101068ms step_avg:57.00ms
step:1774/2035 train_time:101158ms step_avg:57.02ms
step:1775/2035 train_time:101248ms step_avg:57.04ms
step:1776/2035 train_time:101337ms step_avg:57.06ms
step:1777/2035 train_time:101425ms step_avg:57.08ms
step:1778/2035 train_time:101513ms step_avg:57.09ms
step:1779/2035 train_time:101601ms step_avg:57.11ms
step:1780/2035 train_time:101689ms step_avg:57.13ms
step:1781/2035 train_time:101776ms step_avg:57.15ms
step:1782/2035 train_time:101863ms step_avg:57.16ms
step:1783/2035 train_time:101951ms step_avg:57.18ms
step:1784/2035 train_time:102037ms step_avg:57.20ms
step:1785/2035 train_time:102126ms step_avg:57.21ms
step:1786/2035 train_time:102215ms step_avg:57.23ms
step:1787/2035 train_time:102305ms step_avg:57.25ms
step:1788/2035 train_time:102393ms step_avg:57.27ms
step:1789/2035 train_time:102482ms step_avg:57.28ms
step:1790/2035 train_time:102571ms step_avg:57.30ms
step:1791/2035 train_time:102659ms step_avg:57.32ms
step:1792/2035 train_time:102745ms step_avg:57.34ms
step:1793/2035 train_time:102832ms step_avg:57.35ms
step:1794/2035 train_time:102919ms step_avg:57.37ms
step:1795/2035 train_time:103007ms step_avg:57.39ms
step:1796/2035 train_time:103094ms step_avg:57.40ms
step:1797/2035 train_time:103183ms step_avg:57.42ms
step:1798/2035 train_time:103272ms step_avg:57.44ms
step:1799/2035 train_time:103360ms step_avg:57.45ms
step:1800/2035 train_time:103448ms step_avg:57.47ms
step:1801/2035 train_time:103537ms step_avg:57.49ms
step:1802/2035 train_time:103624ms step_avg:57.51ms
step:1803/2035 train_time:103712ms step_avg:57.52ms
step:1804/2035 train_time:103799ms step_avg:57.54ms
step:1805/2035 train_time:103886ms step_avg:57.55ms
step:1806/2035 train_time:103974ms step_avg:57.57ms
step:1807/2035 train_time:104062ms step_avg:57.59ms
step:1808/2035 train_time:104151ms step_avg:57.61ms
step:1809/2035 train_time:104239ms step_avg:57.62ms
step:1810/2035 train_time:104328ms step_avg:57.64ms
step:1811/2035 train_time:104416ms step_avg:57.66ms
step:1812/2035 train_time:104504ms step_avg:57.67ms
step:1813/2035 train_time:104592ms step_avg:57.69ms
step:1814/2035 train_time:104679ms step_avg:57.71ms
step:1815/2035 train_time:104767ms step_avg:57.72ms
step:1816/2035 train_time:104854ms step_avg:57.74ms
step:1817/2035 train_time:104941ms step_avg:57.76ms
step:1818/2035 train_time:105029ms step_avg:57.77ms
step:1819/2035 train_time:105117ms step_avg:57.79ms
step:1820/2035 train_time:105205ms step_avg:57.80ms
step:1821/2035 train_time:105293ms step_avg:57.82ms
step:1822/2035 train_time:105381ms step_avg:57.84ms
step:1823/2035 train_time:105470ms step_avg:57.86ms
step:1824/2035 train_time:105558ms step_avg:57.87ms
step:1825/2035 train_time:105645ms step_avg:57.89ms
step:1826/2035 train_time:105733ms step_avg:57.90ms
step:1827/2035 train_time:105821ms step_avg:57.92ms
step:1828/2035 train_time:105909ms step_avg:57.94ms
step:1829/2035 train_time:105997ms step_avg:57.95ms
step:1830/2035 train_time:106084ms step_avg:57.97ms
step:1831/2035 train_time:106172ms step_avg:57.99ms
step:1832/2035 train_time:106260ms step_avg:58.00ms
step:1833/2035 train_time:106349ms step_avg:58.02ms
step:1834/2035 train_time:106437ms step_avg:58.04ms
step:1835/2035 train_time:106525ms step_avg:58.05ms
step:1836/2035 train_time:106613ms step_avg:58.07ms
step:1837/2035 train_time:106700ms step_avg:58.08ms
step:1838/2035 train_time:106788ms step_avg:58.10ms
step:1839/2035 train_time:106877ms step_avg:58.12ms
step:1840/2035 train_time:106965ms step_avg:58.13ms
step:1841/2035 train_time:107053ms step_avg:58.15ms
step:1842/2035 train_time:107140ms step_avg:58.17ms
step:1843/2035 train_time:107229ms step_avg:58.18ms
step:1844/2035 train_time:107317ms step_avg:58.20ms
step:1845/2035 train_time:107405ms step_avg:58.21ms
step:1846/2035 train_time:107494ms step_avg:58.23ms
step:1847/2035 train_time:107581ms step_avg:58.25ms
step:1848/2035 train_time:107668ms step_avg:58.26ms
step:1849/2035 train_time:107757ms step_avg:58.28ms
step:1850/2035 train_time:107846ms step_avg:58.29ms
step:1851/2035 train_time:107933ms step_avg:58.31ms
step:1852/2035 train_time:108021ms step_avg:58.33ms
step:1853/2035 train_time:108109ms step_avg:58.34ms
step:1854/2035 train_time:108195ms step_avg:58.36ms
step:1855/2035 train_time:108284ms step_avg:58.37ms
step:1856/2035 train_time:108372ms step_avg:58.39ms
step:1857/2035 train_time:108460ms step_avg:58.41ms
step:1858/2035 train_time:108547ms step_avg:58.42ms
step:1859/2035 train_time:108636ms step_avg:58.44ms
step:1860/2035 train_time:108723ms step_avg:58.45ms
step:1861/2035 train_time:108812ms step_avg:58.47ms
step:1862/2035 train_time:108899ms step_avg:58.49ms
step:1863/2035 train_time:108988ms step_avg:58.50ms
step:1864/2035 train_time:109076ms step_avg:58.52ms
step:1865/2035 train_time:109164ms step_avg:58.53ms
step:1866/2035 train_time:109252ms step_avg:58.55ms
step:1867/2035 train_time:109340ms step_avg:58.56ms
step:1868/2035 train_time:109428ms step_avg:58.58ms
step:1869/2035 train_time:109517ms step_avg:58.60ms
step:1870/2035 train_time:109605ms step_avg:58.61ms
step:1871/2035 train_time:109694ms step_avg:58.63ms
step:1872/2035 train_time:109781ms step_avg:58.64ms
step:1873/2035 train_time:109869ms step_avg:58.66ms
step:1874/2035 train_time:109957ms step_avg:58.68ms
step:1875/2035 train_time:110045ms step_avg:58.69ms
step:1876/2035 train_time:110133ms step_avg:58.71ms
step:1877/2035 train_time:110221ms step_avg:58.72ms
step:1878/2035 train_time:110309ms step_avg:58.74ms
step:1879/2035 train_time:110398ms step_avg:58.75ms
step:1880/2035 train_time:110486ms step_avg:58.77ms
step:1881/2035 train_time:110574ms step_avg:58.78ms
step:1882/2035 train_time:110662ms step_avg:58.80ms
step:1883/2035 train_time:110751ms step_avg:58.82ms
step:1884/2035 train_time:110838ms step_avg:58.83ms
step:1885/2035 train_time:110927ms step_avg:58.85ms
step:1886/2035 train_time:111014ms step_avg:58.86ms
step:1887/2035 train_time:111101ms step_avg:58.88ms
step:1888/2035 train_time:111189ms step_avg:58.89ms
step:1889/2035 train_time:111277ms step_avg:58.91ms
step:1890/2035 train_time:111365ms step_avg:58.92ms
step:1891/2035 train_time:111454ms step_avg:58.94ms
step:1892/2035 train_time:111541ms step_avg:58.95ms
step:1893/2035 train_time:111630ms step_avg:58.97ms
step:1894/2035 train_time:111717ms step_avg:58.98ms
step:1895/2035 train_time:111805ms step_avg:59.00ms
step:1896/2035 train_time:111894ms step_avg:59.02ms
step:1897/2035 train_time:111981ms step_avg:59.03ms
step:1898/2035 train_time:112069ms step_avg:59.05ms
step:1899/2035 train_time:112157ms step_avg:59.06ms
step:1900/2035 train_time:112245ms step_avg:59.08ms
step:1901/2035 train_time:112333ms step_avg:59.09ms
step:1902/2035 train_time:112420ms step_avg:59.11ms
step:1903/2035 train_time:112509ms step_avg:59.12ms
step:1904/2035 train_time:112596ms step_avg:59.14ms
step:1905/2035 train_time:112684ms step_avg:59.15ms
step:1906/2035 train_time:112772ms step_avg:59.17ms
step:1907/2035 train_time:112860ms step_avg:59.18ms
step:1908/2035 train_time:112948ms step_avg:59.20ms
step:1909/2035 train_time:113037ms step_avg:59.21ms
step:1910/2035 train_time:113125ms step_avg:59.23ms
step:1911/2035 train_time:113213ms step_avg:59.24ms
step:1912/2035 train_time:113300ms step_avg:59.26ms
step:1913/2035 train_time:113388ms step_avg:59.27ms
step:1914/2035 train_time:113476ms step_avg:59.29ms
step:1915/2035 train_time:113564ms step_avg:59.30ms
step:1916/2035 train_time:113653ms step_avg:59.32ms
step:1917/2035 train_time:113741ms step_avg:59.33ms
step:1918/2035 train_time:113828ms step_avg:59.35ms
step:1919/2035 train_time:113916ms step_avg:59.36ms
step:1920/2035 train_time:114004ms step_avg:59.38ms
step:1921/2035 train_time:114092ms step_avg:59.39ms
step:1922/2035 train_time:114180ms step_avg:59.41ms
step:1923/2035 train_time:114268ms step_avg:59.42ms
step:1924/2035 train_time:114356ms step_avg:59.44ms
step:1925/2035 train_time:114444ms step_avg:59.45ms
step:1926/2035 train_time:114532ms step_avg:59.47ms
step:1927/2035 train_time:114620ms step_avg:59.48ms
step:1928/2035 train_time:114707ms step_avg:59.50ms
step:1929/2035 train_time:114797ms step_avg:59.51ms
step:1930/2035 train_time:114886ms step_avg:59.53ms
step:1931/2035 train_time:114975ms step_avg:59.54ms
step:1932/2035 train_time:115063ms step_avg:59.56ms
step:1933/2035 train_time:115150ms step_avg:59.57ms
step:1934/2035 train_time:115237ms step_avg:59.59ms
step:1935/2035 train_time:115326ms step_avg:59.60ms
step:1936/2035 train_time:115414ms step_avg:59.61ms
step:1937/2035 train_time:115502ms step_avg:59.63ms
step:1938/2035 train_time:115590ms step_avg:59.64ms
step:1939/2035 train_time:115679ms step_avg:59.66ms
step:1940/2035 train_time:115768ms step_avg:59.67ms
step:1941/2035 train_time:115857ms step_avg:59.69ms
step:1942/2035 train_time:115946ms step_avg:59.70ms
step:1943/2035 train_time:116034ms step_avg:59.72ms
step:1944/2035 train_time:116121ms step_avg:59.73ms
step:1945/2035 train_time:116209ms step_avg:59.75ms
step:1946/2035 train_time:116297ms step_avg:59.76ms
step:1947/2035 train_time:116385ms step_avg:59.78ms
step:1948/2035 train_time:116472ms step_avg:59.79ms
step:1949/2035 train_time:116562ms step_avg:59.81ms
step:1950/2035 train_time:116649ms step_avg:59.82ms
step:1951/2035 train_time:116737ms step_avg:59.83ms
step:1952/2035 train_time:116825ms step_avg:59.85ms
step:1953/2035 train_time:116913ms step_avg:59.86ms
step:1954/2035 train_time:117000ms step_avg:59.88ms
step:1955/2035 train_time:117089ms step_avg:59.89ms
step:1956/2035 train_time:117176ms step_avg:59.91ms
step:1957/2035 train_time:117264ms step_avg:59.92ms
step:1958/2035 train_time:117353ms step_avg:59.94ms
step:1959/2035 train_time:117441ms step_avg:59.95ms
step:1960/2035 train_time:117529ms step_avg:59.96ms
step:1961/2035 train_time:117617ms step_avg:59.98ms
step:1962/2035 train_time:117705ms step_avg:59.99ms
step:1963/2035 train_time:117794ms step_avg:60.01ms
step:1964/2035 train_time:117881ms step_avg:60.02ms
step:1965/2035 train_time:117970ms step_avg:60.04ms
step:1966/2035 train_time:118057ms step_avg:60.05ms
step:1967/2035 train_time:118146ms step_avg:60.06ms
step:1968/2035 train_time:118234ms step_avg:60.08ms
step:1969/2035 train_time:118322ms step_avg:60.09ms
step:1970/2035 train_time:118410ms step_avg:60.11ms
step:1971/2035 train_time:118498ms step_avg:60.12ms
step:1972/2035 train_time:118586ms step_avg:60.13ms
step:1973/2035 train_time:118675ms step_avg:60.15ms
step:1974/2035 train_time:118763ms step_avg:60.16ms
step:1975/2035 train_time:118851ms step_avg:60.18ms
step:1976/2035 train_time:118938ms step_avg:60.19ms
step:1977/2035 train_time:119026ms step_avg:60.21ms
step:1978/2035 train_time:119115ms step_avg:60.22ms
step:1979/2035 train_time:119202ms step_avg:60.23ms
step:1980/2035 train_time:119291ms step_avg:60.25ms
step:1981/2035 train_time:119378ms step_avg:60.26ms
step:1982/2035 train_time:119466ms step_avg:60.28ms
step:1983/2035 train_time:119556ms step_avg:60.29ms
step:1984/2035 train_time:119644ms step_avg:60.30ms
step:1985/2035 train_time:119732ms step_avg:60.32ms
step:1986/2035 train_time:119819ms step_avg:60.33ms
step:1987/2035 train_time:119906ms step_avg:60.35ms
step:1988/2035 train_time:119995ms step_avg:60.36ms
step:1989/2035 train_time:120083ms step_avg:60.37ms
step:1990/2035 train_time:120171ms step_avg:60.39ms
step:1991/2035 train_time:120259ms step_avg:60.40ms
step:1992/2035 train_time:120348ms step_avg:60.42ms
step:1993/2035 train_time:120436ms step_avg:60.43ms
step:1994/2035 train_time:120524ms step_avg:60.44ms
step:1995/2035 train_time:120612ms step_avg:60.46ms
step:1996/2035 train_time:120699ms step_avg:60.47ms
step:1997/2035 train_time:120788ms step_avg:60.48ms
step:1998/2035 train_time:120876ms step_avg:60.50ms
step:1999/2035 train_time:120964ms step_avg:60.51ms
step:2000/2035 train_time:121053ms step_avg:60.53ms
step:2000/2035 val_loss:3.2839 train_time:121143ms step_avg:60.57ms
step:2001/2035 train_time:121163ms step_avg:60.55ms
step:2002/2035 train_time:121233ms step_avg:60.56ms
step:2003/2035 train_time:121330ms step_avg:60.57ms
step:2004/2035 train_time:121420ms step_avg:60.59ms
step:2005/2035 train_time:121511ms step_avg:60.60ms
step:2006/2035 train_time:121598ms step_avg:60.62ms
step:2007/2035 train_time:121686ms step_avg:60.63ms
step:2008/2035 train_time:121772ms step_avg:60.64ms
step:2009/2035 train_time:121860ms step_avg:60.66ms
step:2010/2035 train_time:121946ms step_avg:60.67ms
step:2011/2035 train_time:122034ms step_avg:60.68ms
step:2012/2035 train_time:122124ms step_avg:60.70ms
step:2013/2035 train_time:122215ms step_avg:60.71ms
step:2014/2035 train_time:122306ms step_avg:60.73ms
step:2015/2035 train_time:122397ms step_avg:60.74ms
step:2016/2035 train_time:122486ms step_avg:60.76ms
step:2017/2035 train_time:122574ms step_avg:60.77ms
step:2018/2035 train_time:122661ms step_avg:60.78ms
step:2019/2035 train_time:122749ms step_avg:60.80ms
step:2020/2035 train_time:122836ms step_avg:60.81ms
step:2021/2035 train_time:122924ms step_avg:60.82ms
step:2022/2035 train_time:123011ms step_avg:60.84ms
step:2023/2035 train_time:123100ms step_avg:60.85ms
step:2024/2035 train_time:123189ms step_avg:60.86ms
step:2025/2035 train_time:123279ms step_avg:60.88ms
step:2026/2035 train_time:123370ms step_avg:60.89ms
step:2027/2035 train_time:123459ms step_avg:60.91ms
step:2028/2035 train_time:123548ms step_avg:60.92ms
step:2029/2035 train_time:123636ms step_avg:60.93ms
step:2030/2035 train_time:123724ms step_avg:60.95ms
step:2031/2035 train_time:123811ms step_avg:60.96ms
step:2032/2035 train_time:123898ms step_avg:60.97ms
step:2033/2035 train_time:123986ms step_avg:60.99ms
step:2034/2035 train_time:124074ms step_avg:61.00ms
step:2035/2035 train_time:124163ms step_avg:61.01ms
step:2035/2035 val_loss:3.2762 train_time:124252ms step_avg:61.06ms
peak memory allocated: 29634 MiB reserved: 43476 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    logs_dir: str = f"logs/12-19-Baseline"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 14:54:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     61531      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A     61532      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     61533      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     61534      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     61535      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     61536      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     61537      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     61538      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A     61532      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A     61533      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A     61534      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A     61535      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A     61536      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A     61537      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A     61538      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8334 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:73ms step_avg:73.04ms
step:2/2035 train_time:98ms step_avg:48.85ms
step:3/2035 train_time:131ms step_avg:43.70ms
step:4/2035 train_time:164ms step_avg:40.98ms
step:5/2035 train_time:196ms step_avg:39.29ms
step:6/2035 train_time:262ms step_avg:43.64ms
step:7/2035 train_time:301ms step_avg:43.07ms
step:8/2035 train_time:335ms step_avg:41.85ms
step:9/2035 train_time:367ms step_avg:40.82ms
step:10/2035 train_time:400ms step_avg:40.02ms
step:11/2035 train_time:434ms step_avg:39.44ms
step:12/2035 train_time:467ms step_avg:38.91ms
step:13/2035 train_time:500ms step_avg:38.47ms
step:14/2035 train_time:533ms step_avg:38.09ms
step:15/2035 train_time:567ms step_avg:37.77ms
step:16/2035 train_time:600ms step_avg:37.47ms
step:17/2035 train_time:633ms step_avg:37.23ms
step:18/2035 train_time:666ms step_avg:37.00ms
step:19/2035 train_time:700ms step_avg:36.84ms
step:20/2035 train_time:733ms step_avg:36.65ms
step:21/2035 train_time:766ms step_avg:36.49ms
step:22/2035 train_time:799ms step_avg:36.34ms
step:23/2035 train_time:832ms step_avg:36.20ms
step:24/2035 train_time:866ms step_avg:36.07ms
step:25/2035 train_time:899ms step_avg:35.97ms
step:26/2035 train_time:932ms step_avg:35.87ms
step:27/2035 train_time:966ms step_avg:35.77ms
step:28/2035 train_time:999ms step_avg:35.68ms
step:29/2035 train_time:1032ms step_avg:35.59ms
step:30/2035 train_time:1066ms step_avg:35.52ms
step:31/2035 train_time:1099ms step_avg:35.44ms
step:32/2035 train_time:1132ms step_avg:35.38ms
step:33/2035 train_time:1166ms step_avg:35.32ms
step:34/2035 train_time:1200ms step_avg:35.30ms
step:35/2035 train_time:1235ms step_avg:35.29ms
step:36/2035 train_time:1269ms step_avg:35.26ms
step:37/2035 train_time:1303ms step_avg:35.21ms
step:38/2035 train_time:1336ms step_avg:35.17ms
step:39/2035 train_time:1370ms step_avg:35.12ms
step:40/2035 train_time:1403ms step_avg:35.08ms
step:41/2035 train_time:1437ms step_avg:35.04ms
step:42/2035 train_time:1470ms step_avg:35.01ms
step:43/2035 train_time:1504ms step_avg:34.97ms
step:44/2035 train_time:1537ms step_avg:34.93ms
step:45/2035 train_time:1570ms step_avg:34.89ms
step:46/2035 train_time:1604ms step_avg:34.87ms
step:47/2035 train_time:1637ms step_avg:34.83ms
step:48/2035 train_time:1671ms step_avg:34.81ms
step:49/2035 train_time:1704ms step_avg:34.78ms
step:50/2035 train_time:1737ms step_avg:34.74ms
step:51/2035 train_time:1770ms step_avg:34.71ms
step:52/2035 train_time:1803ms step_avg:34.68ms
step:53/2035 train_time:1836ms step_avg:34.65ms
step:54/2035 train_time:1870ms step_avg:34.63ms
step:55/2035 train_time:1903ms step_avg:34.60ms
step:56/2035 train_time:1936ms step_avg:34.57ms
step:57/2035 train_time:1969ms step_avg:34.55ms
step:58/2035 train_time:2003ms step_avg:34.53ms
step:59/2035 train_time:2036ms step_avg:34.50ms
step:60/2035 train_time:2069ms step_avg:34.48ms
step:61/2035 train_time:2102ms step_avg:34.46ms
step:62/2035 train_time:2135ms step_avg:34.44ms
step:63/2035 train_time:2169ms step_avg:34.42ms
step:64/2035 train_time:2203ms step_avg:34.42ms
step:65/2035 train_time:2236ms step_avg:34.40ms
step:66/2035 train_time:2269ms step_avg:34.39ms
step:67/2035 train_time:2303ms step_avg:34.37ms
step:68/2035 train_time:2336ms step_avg:34.36ms
step:69/2035 train_time:2369ms step_avg:34.34ms
step:70/2035 train_time:2403ms step_avg:34.33ms
step:71/2035 train_time:2436ms step_avg:34.32ms
step:72/2035 train_time:2470ms step_avg:34.30ms
step:73/2035 train_time:2503ms step_avg:34.29ms
step:74/2035 train_time:2537ms step_avg:34.28ms
step:75/2035 train_time:2570ms step_avg:34.27ms
step:76/2035 train_time:2604ms step_avg:34.26ms
step:77/2035 train_time:2637ms step_avg:34.25ms
step:78/2035 train_time:2670ms step_avg:34.24ms
step:79/2035 train_time:2704ms step_avg:34.22ms
step:80/2035 train_time:2737ms step_avg:34.21ms
step:81/2035 train_time:2770ms step_avg:34.20ms
step:82/2035 train_time:2803ms step_avg:34.19ms
step:83/2035 train_time:2836ms step_avg:34.17ms
step:84/2035 train_time:2870ms step_avg:34.16ms
step:85/2035 train_time:2903ms step_avg:34.15ms
step:86/2035 train_time:2936ms step_avg:34.14ms
step:87/2035 train_time:2969ms step_avg:34.12ms
step:88/2035 train_time:3002ms step_avg:34.11ms
step:89/2035 train_time:3036ms step_avg:34.11ms
step:90/2035 train_time:3069ms step_avg:34.10ms
step:91/2035 train_time:3102ms step_avg:34.09ms
step:92/2035 train_time:3135ms step_avg:34.08ms
step:93/2035 train_time:3168ms step_avg:34.07ms
step:94/2035 train_time:3202ms step_avg:34.06ms
step:95/2035 train_time:3235ms step_avg:34.05ms
step:96/2035 train_time:3269ms step_avg:34.05ms
step:97/2035 train_time:3302ms step_avg:34.04ms
step:98/2035 train_time:3336ms step_avg:34.04ms
step:99/2035 train_time:3369ms step_avg:34.03ms
step:100/2035 train_time:3403ms step_avg:34.03ms
step:101/2035 train_time:3436ms step_avg:34.02ms
step:102/2035 train_time:3470ms step_avg:34.02ms
step:103/2035 train_time:3503ms step_avg:34.01ms
step:104/2035 train_time:3536ms step_avg:34.00ms
step:105/2035 train_time:3569ms step_avg:33.99ms
step:106/2035 train_time:3602ms step_avg:33.98ms
step:107/2035 train_time:3635ms step_avg:33.98ms
step:108/2035 train_time:3669ms step_avg:33.97ms
step:109/2035 train_time:3702ms step_avg:33.96ms
step:110/2035 train_time:3735ms step_avg:33.96ms
step:111/2035 train_time:3768ms step_avg:33.95ms
step:112/2035 train_time:3802ms step_avg:33.94ms
step:113/2035 train_time:3835ms step_avg:33.94ms
step:114/2035 train_time:3868ms step_avg:33.93ms
step:115/2035 train_time:3901ms step_avg:33.92ms
step:116/2035 train_time:3934ms step_avg:33.92ms
step:117/2035 train_time:3967ms step_avg:33.91ms
step:118/2035 train_time:4000ms step_avg:33.90ms
step:119/2035 train_time:4034ms step_avg:33.90ms
step:120/2035 train_time:4067ms step_avg:33.89ms
step:121/2035 train_time:4100ms step_avg:33.89ms
step:122/2035 train_time:4133ms step_avg:33.88ms
step:123/2035 train_time:4166ms step_avg:33.87ms
step:124/2035 train_time:4199ms step_avg:33.87ms
step:125/2035 train_time:4233ms step_avg:33.86ms
step:126/2035 train_time:4266ms step_avg:33.86ms
step:127/2035 train_time:4299ms step_avg:33.85ms
step:128/2035 train_time:4333ms step_avg:33.85ms
step:129/2035 train_time:4365ms step_avg:33.84ms
step:130/2035 train_time:4399ms step_avg:33.84ms
step:131/2035 train_time:4432ms step_avg:33.83ms
step:132/2035 train_time:4466ms step_avg:33.83ms
step:133/2035 train_time:4499ms step_avg:33.82ms
step:134/2035 train_time:4532ms step_avg:33.82ms
step:135/2035 train_time:4565ms step_avg:33.81ms
step:136/2035 train_time:4598ms step_avg:33.81ms
step:137/2035 train_time:4631ms step_avg:33.80ms
step:138/2035 train_time:4664ms step_avg:33.80ms
step:139/2035 train_time:4698ms step_avg:33.80ms
step:140/2035 train_time:4731ms step_avg:33.79ms
step:141/2035 train_time:4764ms step_avg:33.79ms
step:142/2035 train_time:4797ms step_avg:33.78ms
step:143/2035 train_time:4830ms step_avg:33.78ms
step:144/2035 train_time:4863ms step_avg:33.77ms
step:145/2035 train_time:4896ms step_avg:33.77ms
step:146/2035 train_time:4930ms step_avg:33.77ms
step:147/2035 train_time:4963ms step_avg:33.76ms
step:148/2035 train_time:4996ms step_avg:33.76ms
step:149/2035 train_time:5029ms step_avg:33.75ms
step:150/2035 train_time:5063ms step_avg:33.75ms
step:151/2035 train_time:5096ms step_avg:33.75ms
step:152/2035 train_time:5129ms step_avg:33.74ms
step:153/2035 train_time:5162ms step_avg:33.74ms
step:154/2035 train_time:5195ms step_avg:33.74ms
step:155/2035 train_time:5229ms step_avg:33.73ms
step:156/2035 train_time:5262ms step_avg:33.73ms
step:157/2035 train_time:5295ms step_avg:33.73ms
step:158/2035 train_time:5328ms step_avg:33.72ms
step:159/2035 train_time:5361ms step_avg:33.72ms
step:160/2035 train_time:5394ms step_avg:33.71ms
step:161/2035 train_time:5427ms step_avg:33.71ms
step:162/2035 train_time:5460ms step_avg:33.70ms
step:163/2035 train_time:5493ms step_avg:33.70ms
step:164/2035 train_time:5526ms step_avg:33.70ms
step:165/2035 train_time:5559ms step_avg:33.69ms
step:166/2035 train_time:5593ms step_avg:33.69ms
step:167/2035 train_time:5626ms step_avg:33.69ms
step:168/2035 train_time:5660ms step_avg:33.69ms
step:169/2035 train_time:5693ms step_avg:33.68ms
step:170/2035 train_time:5726ms step_avg:33.68ms
step:171/2035 train_time:5759ms step_avg:33.68ms
step:172/2035 train_time:5792ms step_avg:33.68ms
step:173/2035 train_time:5825ms step_avg:33.67ms
step:174/2035 train_time:5858ms step_avg:33.67ms
step:175/2035 train_time:5891ms step_avg:33.67ms
step:176/2035 train_time:5924ms step_avg:33.66ms
step:177/2035 train_time:5958ms step_avg:33.66ms
step:178/2035 train_time:5991ms step_avg:33.66ms
step:179/2035 train_time:6024ms step_avg:33.65ms
step:180/2035 train_time:6057ms step_avg:33.65ms
step:181/2035 train_time:6090ms step_avg:33.64ms
step:182/2035 train_time:6123ms step_avg:33.64ms
step:183/2035 train_time:6156ms step_avg:33.64ms
step:184/2035 train_time:6189ms step_avg:33.64ms
step:185/2035 train_time:6223ms step_avg:33.64ms
step:186/2035 train_time:6256ms step_avg:33.63ms
step:187/2035 train_time:6289ms step_avg:33.63ms
step:188/2035 train_time:6322ms step_avg:33.63ms
step:189/2035 train_time:6355ms step_avg:33.63ms
step:190/2035 train_time:6389ms step_avg:33.62ms
step:191/2035 train_time:6422ms step_avg:33.62ms
step:192/2035 train_time:6455ms step_avg:33.62ms
step:193/2035 train_time:6488ms step_avg:33.62ms
step:194/2035 train_time:6521ms step_avg:33.62ms
step:195/2035 train_time:6555ms step_avg:33.61ms
step:196/2035 train_time:6588ms step_avg:33.61ms
step:197/2035 train_time:6621ms step_avg:33.61ms
step:198/2035 train_time:6654ms step_avg:33.61ms
step:199/2035 train_time:6688ms step_avg:33.61ms
step:200/2035 train_time:6721ms step_avg:33.60ms
step:201/2035 train_time:6754ms step_avg:33.60ms
step:202/2035 train_time:6787ms step_avg:33.60ms
step:203/2035 train_time:6820ms step_avg:33.60ms
step:204/2035 train_time:6854ms step_avg:33.60ms
step:205/2035 train_time:6887ms step_avg:33.59ms
step:206/2035 train_time:6920ms step_avg:33.59ms
step:207/2035 train_time:6953ms step_avg:33.59ms
step:208/2035 train_time:6986ms step_avg:33.59ms
step:209/2035 train_time:7019ms step_avg:33.59ms
step:210/2035 train_time:7052ms step_avg:33.58ms
step:211/2035 train_time:7086ms step_avg:33.58ms
step:212/2035 train_time:7119ms step_avg:33.58ms
step:213/2035 train_time:7151ms step_avg:33.58ms
step:214/2035 train_time:7185ms step_avg:33.57ms
step:215/2035 train_time:7218ms step_avg:33.57ms
step:216/2035 train_time:7251ms step_avg:33.57ms
step:217/2035 train_time:7284ms step_avg:33.57ms
step:218/2035 train_time:7317ms step_avg:33.56ms
step:219/2035 train_time:7350ms step_avg:33.56ms
step:220/2035 train_time:7383ms step_avg:33.56ms
step:221/2035 train_time:7416ms step_avg:33.56ms
step:222/2035 train_time:7450ms step_avg:33.56ms
step:223/2035 train_time:7483ms step_avg:33.56ms
step:224/2035 train_time:7516ms step_avg:33.55ms
step:225/2035 train_time:7550ms step_avg:33.55ms
step:226/2035 train_time:7583ms step_avg:33.55ms
step:227/2035 train_time:7616ms step_avg:33.55ms
step:228/2035 train_time:7649ms step_avg:33.55ms
step:229/2035 train_time:7682ms step_avg:33.55ms
step:230/2035 train_time:7715ms step_avg:33.54ms
step:231/2035 train_time:7748ms step_avg:33.54ms
step:232/2035 train_time:7781ms step_avg:33.54ms
step:233/2035 train_time:7814ms step_avg:33.54ms
step:234/2035 train_time:7848ms step_avg:33.54ms
step:235/2035 train_time:7881ms step_avg:33.53ms
step:236/2035 train_time:7914ms step_avg:33.53ms
step:237/2035 train_time:7947ms step_avg:33.53ms
step:238/2035 train_time:7980ms step_avg:33.53ms
step:239/2035 train_time:8013ms step_avg:33.53ms
step:240/2035 train_time:8046ms step_avg:33.52ms
step:241/2035 train_time:8079ms step_avg:33.52ms
step:242/2035 train_time:8112ms step_avg:33.52ms
step:243/2035 train_time:8145ms step_avg:33.52ms
step:244/2035 train_time:8178ms step_avg:33.52ms
step:245/2035 train_time:8211ms step_avg:33.52ms
step:246/2035 train_time:8245ms step_avg:33.51ms
step:247/2035 train_time:8278ms step_avg:33.51ms
step:248/2035 train_time:8310ms step_avg:33.51ms
step:249/2035 train_time:8343ms step_avg:33.51ms
step:250/2035 train_time:8377ms step_avg:33.51ms
step:250/2035 val_loss:4.2724 train_time:8412ms step_avg:33.65ms
step:251/2035 train_time:8430ms step_avg:33.59ms
step:252/2035 train_time:8449ms step_avg:33.53ms
step:253/2035 train_time:8480ms step_avg:33.52ms
step:254/2035 train_time:8514ms step_avg:33.52ms
step:255/2035 train_time:8548ms step_avg:33.52ms
step:256/2035 train_time:8582ms step_avg:33.52ms
step:257/2035 train_time:8616ms step_avg:33.53ms
step:258/2035 train_time:8649ms step_avg:33.52ms
step:259/2035 train_time:8682ms step_avg:33.52ms
step:260/2035 train_time:8715ms step_avg:33.52ms
step:261/2035 train_time:8748ms step_avg:33.52ms
step:262/2035 train_time:8781ms step_avg:33.52ms
step:263/2035 train_time:8814ms step_avg:33.51ms
step:264/2035 train_time:8848ms step_avg:33.51ms
step:265/2035 train_time:8880ms step_avg:33.51ms
step:266/2035 train_time:8913ms step_avg:33.51ms
step:267/2035 train_time:8946ms step_avg:33.51ms
step:268/2035 train_time:8980ms step_avg:33.51ms
step:269/2035 train_time:9013ms step_avg:33.50ms
step:270/2035 train_time:9045ms step_avg:33.50ms
step:271/2035 train_time:9078ms step_avg:33.50ms
step:272/2035 train_time:9111ms step_avg:33.50ms
step:273/2035 train_time:9144ms step_avg:33.49ms
step:274/2035 train_time:9177ms step_avg:33.49ms
step:275/2035 train_time:9210ms step_avg:33.49ms
step:276/2035 train_time:9243ms step_avg:33.49ms
step:277/2035 train_time:9276ms step_avg:33.49ms
step:278/2035 train_time:9309ms step_avg:33.49ms
step:279/2035 train_time:9342ms step_avg:33.48ms
step:280/2035 train_time:9375ms step_avg:33.48ms
step:281/2035 train_time:9409ms step_avg:33.48ms
step:282/2035 train_time:9442ms step_avg:33.48ms
step:283/2035 train_time:9476ms step_avg:33.48ms
step:284/2035 train_time:9510ms step_avg:33.48ms
step:285/2035 train_time:9543ms step_avg:33.49ms
step:286/2035 train_time:9577ms step_avg:33.49ms
step:287/2035 train_time:9610ms step_avg:33.49ms
step:288/2035 train_time:9644ms step_avg:33.49ms
step:289/2035 train_time:9677ms step_avg:33.49ms
step:290/2035 train_time:9710ms step_avg:33.48ms
step:291/2035 train_time:9743ms step_avg:33.48ms
step:292/2035 train_time:9777ms step_avg:33.48ms
step:293/2035 train_time:9810ms step_avg:33.48ms
step:294/2035 train_time:9843ms step_avg:33.48ms
step:295/2035 train_time:9877ms step_avg:33.48ms
step:296/2035 train_time:9909ms step_avg:33.48ms
step:297/2035 train_time:9943ms step_avg:33.48ms
step:298/2035 train_time:9976ms step_avg:33.48ms
step:299/2035 train_time:10009ms step_avg:33.47ms
step:300/2035 train_time:10042ms step_avg:33.47ms
step:301/2035 train_time:10075ms step_avg:33.47ms
step:302/2035 train_time:10108ms step_avg:33.47ms
step:303/2035 train_time:10141ms step_avg:33.47ms
step:304/2035 train_time:10174ms step_avg:33.47ms
step:305/2035 train_time:10207ms step_avg:33.47ms
step:306/2035 train_time:10240ms step_avg:33.46ms
step:307/2035 train_time:10274ms step_avg:33.46ms
step:308/2035 train_time:10307ms step_avg:33.46ms
step:309/2035 train_time:10340ms step_avg:33.46ms
step:310/2035 train_time:10373ms step_avg:33.46ms
step:311/2035 train_time:10406ms step_avg:33.46ms
step:312/2035 train_time:10440ms step_avg:33.46ms
step:313/2035 train_time:10473ms step_avg:33.46ms
step:314/2035 train_time:10506ms step_avg:33.46ms
step:315/2035 train_time:10539ms step_avg:33.46ms
step:316/2035 train_time:10573ms step_avg:33.46ms
step:317/2035 train_time:10606ms step_avg:33.46ms
step:318/2035 train_time:10639ms step_avg:33.46ms
step:319/2035 train_time:10672ms step_avg:33.46ms
step:320/2035 train_time:10705ms step_avg:33.45ms
step:321/2035 train_time:10738ms step_avg:33.45ms
step:322/2035 train_time:10772ms step_avg:33.45ms
step:323/2035 train_time:10805ms step_avg:33.45ms
step:324/2035 train_time:10838ms step_avg:33.45ms
step:325/2035 train_time:10871ms step_avg:33.45ms
step:326/2035 train_time:10905ms step_avg:33.45ms
step:327/2035 train_time:10938ms step_avg:33.45ms
step:328/2035 train_time:10971ms step_avg:33.45ms
step:329/2035 train_time:11004ms step_avg:33.45ms
step:330/2035 train_time:11037ms step_avg:33.45ms
step:331/2035 train_time:11070ms step_avg:33.44ms
step:332/2035 train_time:11104ms step_avg:33.44ms
step:333/2035 train_time:11137ms step_avg:33.44ms
step:334/2035 train_time:11170ms step_avg:33.44ms
step:335/2035 train_time:11203ms step_avg:33.44ms
step:336/2035 train_time:11236ms step_avg:33.44ms
step:337/2035 train_time:11269ms step_avg:33.44ms
step:338/2035 train_time:11302ms step_avg:33.44ms
step:339/2035 train_time:11335ms step_avg:33.44ms
step:340/2035 train_time:11368ms step_avg:33.43ms
step:341/2035 train_time:11401ms step_avg:33.43ms
step:342/2035 train_time:11434ms step_avg:33.43ms
step:343/2035 train_time:11467ms step_avg:33.43ms
step:344/2035 train_time:11501ms step_avg:33.43ms
step:345/2035 train_time:11533ms step_avg:33.43ms
step:346/2035 train_time:11567ms step_avg:33.43ms
step:347/2035 train_time:11600ms step_avg:33.43ms
step:348/2035 train_time:11633ms step_avg:33.43ms
step:349/2035 train_time:11666ms step_avg:33.43ms
step:350/2035 train_time:11699ms step_avg:33.43ms
step:351/2035 train_time:11732ms step_avg:33.43ms
step:352/2035 train_time:11765ms step_avg:33.42ms
step:353/2035 train_time:11798ms step_avg:33.42ms
step:354/2035 train_time:11831ms step_avg:33.42ms
step:355/2035 train_time:11864ms step_avg:33.42ms
step:356/2035 train_time:11897ms step_avg:33.42ms
step:357/2035 train_time:11931ms step_avg:33.42ms
step:358/2035 train_time:11964ms step_avg:33.42ms
step:359/2035 train_time:11997ms step_avg:33.42ms
step:360/2035 train_time:12031ms step_avg:33.42ms
step:361/2035 train_time:12064ms step_avg:33.42ms
step:362/2035 train_time:12097ms step_avg:33.42ms
step:363/2035 train_time:12129ms step_avg:33.41ms
step:364/2035 train_time:12162ms step_avg:33.41ms
step:365/2035 train_time:12195ms step_avg:33.41ms
step:366/2035 train_time:12229ms step_avg:33.41ms
step:367/2035 train_time:12262ms step_avg:33.41ms
step:368/2035 train_time:12295ms step_avg:33.41ms
step:369/2035 train_time:12328ms step_avg:33.41ms
step:370/2035 train_time:12361ms step_avg:33.41ms
step:371/2035 train_time:12395ms step_avg:33.41ms
step:372/2035 train_time:12428ms step_avg:33.41ms
step:373/2035 train_time:12461ms step_avg:33.41ms
step:374/2035 train_time:12495ms step_avg:33.41ms
step:375/2035 train_time:12528ms step_avg:33.41ms
step:376/2035 train_time:12561ms step_avg:33.41ms
step:377/2035 train_time:12594ms step_avg:33.41ms
step:378/2035 train_time:12627ms step_avg:33.41ms
step:379/2035 train_time:12660ms step_avg:33.40ms
step:380/2035 train_time:12694ms step_avg:33.40ms
step:381/2035 train_time:12727ms step_avg:33.40ms
step:382/2035 train_time:12760ms step_avg:33.40ms
step:383/2035 train_time:12793ms step_avg:33.40ms
step:384/2035 train_time:12826ms step_avg:33.40ms
step:385/2035 train_time:12859ms step_avg:33.40ms
step:386/2035 train_time:12893ms step_avg:33.40ms
step:387/2035 train_time:12926ms step_avg:33.40ms
step:388/2035 train_time:12959ms step_avg:33.40ms
step:389/2035 train_time:12992ms step_avg:33.40ms
step:390/2035 train_time:13025ms step_avg:33.40ms
step:391/2035 train_time:13059ms step_avg:33.40ms
step:392/2035 train_time:13092ms step_avg:33.40ms
step:393/2035 train_time:13125ms step_avg:33.40ms
step:394/2035 train_time:13158ms step_avg:33.40ms
step:395/2035 train_time:13192ms step_avg:33.40ms
step:396/2035 train_time:13225ms step_avg:33.40ms
step:397/2035 train_time:13258ms step_avg:33.40ms
step:398/2035 train_time:13291ms step_avg:33.40ms
step:399/2035 train_time:13324ms step_avg:33.39ms
step:400/2035 train_time:13358ms step_avg:33.39ms
step:401/2035 train_time:13390ms step_avg:33.39ms
step:402/2035 train_time:13424ms step_avg:33.39ms
step:403/2035 train_time:13457ms step_avg:33.39ms
step:404/2035 train_time:13490ms step_avg:33.39ms
step:405/2035 train_time:13524ms step_avg:33.39ms
step:406/2035 train_time:13557ms step_avg:33.39ms
step:407/2035 train_time:13590ms step_avg:33.39ms
step:408/2035 train_time:13623ms step_avg:33.39ms
step:409/2035 train_time:13657ms step_avg:33.39ms
step:410/2035 train_time:13690ms step_avg:33.39ms
step:411/2035 train_time:13723ms step_avg:33.39ms
step:412/2035 train_time:13756ms step_avg:33.39ms
step:413/2035 train_time:13789ms step_avg:33.39ms
step:414/2035 train_time:13822ms step_avg:33.39ms
step:415/2035 train_time:13855ms step_avg:33.39ms
step:416/2035 train_time:13888ms step_avg:33.39ms
step:417/2035 train_time:13922ms step_avg:33.38ms
step:418/2035 train_time:13955ms step_avg:33.38ms
step:419/2035 train_time:13988ms step_avg:33.38ms
step:420/2035 train_time:14021ms step_avg:33.38ms
step:421/2035 train_time:14054ms step_avg:33.38ms
step:422/2035 train_time:14087ms step_avg:33.38ms
step:423/2035 train_time:14120ms step_avg:33.38ms
step:424/2035 train_time:14153ms step_avg:33.38ms
step:425/2035 train_time:14186ms step_avg:33.38ms
step:426/2035 train_time:14220ms step_avg:33.38ms
step:427/2035 train_time:14253ms step_avg:33.38ms
step:428/2035 train_time:14286ms step_avg:33.38ms
step:429/2035 train_time:14319ms step_avg:33.38ms
step:430/2035 train_time:14353ms step_avg:33.38ms
step:431/2035 train_time:14385ms step_avg:33.38ms
step:432/2035 train_time:14419ms step_avg:33.38ms
step:433/2035 train_time:14452ms step_avg:33.38ms
step:434/2035 train_time:14485ms step_avg:33.38ms
step:435/2035 train_time:14519ms step_avg:33.38ms
step:436/2035 train_time:14552ms step_avg:33.38ms
step:437/2035 train_time:14585ms step_avg:33.38ms
step:438/2035 train_time:14618ms step_avg:33.37ms
step:439/2035 train_time:14651ms step_avg:33.37ms
step:440/2035 train_time:14684ms step_avg:33.37ms
step:441/2035 train_time:14718ms step_avg:33.37ms
step:442/2035 train_time:14751ms step_avg:33.37ms
step:443/2035 train_time:14784ms step_avg:33.37ms
step:444/2035 train_time:14817ms step_avg:33.37ms
step:445/2035 train_time:14851ms step_avg:33.37ms
step:446/2035 train_time:14884ms step_avg:33.37ms
step:447/2035 train_time:14917ms step_avg:33.37ms
step:448/2035 train_time:14950ms step_avg:33.37ms
step:449/2035 train_time:14983ms step_avg:33.37ms
step:450/2035 train_time:15016ms step_avg:33.37ms
step:451/2035 train_time:15049ms step_avg:33.37ms
step:452/2035 train_time:15082ms step_avg:33.37ms
step:453/2035 train_time:15115ms step_avg:33.37ms
step:454/2035 train_time:15148ms step_avg:33.37ms
step:455/2035 train_time:15181ms step_avg:33.37ms
step:456/2035 train_time:15214ms step_avg:33.37ms
step:457/2035 train_time:15247ms step_avg:33.36ms
step:458/2035 train_time:15280ms step_avg:33.36ms
step:459/2035 train_time:15313ms step_avg:33.36ms
step:460/2035 train_time:15346ms step_avg:33.36ms
step:461/2035 train_time:15379ms step_avg:33.36ms
step:462/2035 train_time:15413ms step_avg:33.36ms
step:463/2035 train_time:15445ms step_avg:33.36ms
step:464/2035 train_time:15479ms step_avg:33.36ms
step:465/2035 train_time:15512ms step_avg:33.36ms
step:466/2035 train_time:15545ms step_avg:33.36ms
step:467/2035 train_time:15579ms step_avg:33.36ms
step:468/2035 train_time:15612ms step_avg:33.36ms
step:469/2035 train_time:15645ms step_avg:33.36ms
step:470/2035 train_time:15678ms step_avg:33.36ms
step:471/2035 train_time:15712ms step_avg:33.36ms
step:472/2035 train_time:15745ms step_avg:33.36ms
step:473/2035 train_time:15778ms step_avg:33.36ms
step:474/2035 train_time:15811ms step_avg:33.36ms
step:475/2035 train_time:15844ms step_avg:33.36ms
step:476/2035 train_time:15877ms step_avg:33.36ms
step:477/2035 train_time:15910ms step_avg:33.35ms
step:478/2035 train_time:15944ms step_avg:33.35ms
step:479/2035 train_time:15977ms step_avg:33.35ms
step:480/2035 train_time:16010ms step_avg:33.35ms
step:481/2035 train_time:16043ms step_avg:33.35ms
step:482/2035 train_time:16076ms step_avg:33.35ms
step:483/2035 train_time:16109ms step_avg:33.35ms
step:484/2035 train_time:16143ms step_avg:33.35ms
step:485/2035 train_time:16176ms step_avg:33.35ms
step:486/2035 train_time:16209ms step_avg:33.35ms
step:487/2035 train_time:16242ms step_avg:33.35ms
step:488/2035 train_time:16276ms step_avg:33.35ms
step:489/2035 train_time:16309ms step_avg:33.35ms
step:490/2035 train_time:16342ms step_avg:33.35ms
step:491/2035 train_time:16375ms step_avg:33.35ms
step:492/2035 train_time:16408ms step_avg:33.35ms
step:493/2035 train_time:16441ms step_avg:33.35ms
step:494/2035 train_time:16474ms step_avg:33.35ms
step:495/2035 train_time:16507ms step_avg:33.35ms
step:496/2035 train_time:16540ms step_avg:33.35ms
step:497/2035 train_time:16573ms step_avg:33.35ms
step:498/2035 train_time:16606ms step_avg:33.35ms
step:499/2035 train_time:16640ms step_avg:33.35ms
step:500/2035 train_time:16673ms step_avg:33.35ms
step:500/2035 val_loss:4.0287 train_time:16709ms step_avg:33.42ms
step:501/2035 train_time:16728ms step_avg:33.39ms
step:502/2035 train_time:16746ms step_avg:33.36ms
step:503/2035 train_time:16779ms step_avg:33.36ms
step:504/2035 train_time:16813ms step_avg:33.36ms
step:505/2035 train_time:16848ms step_avg:33.36ms
step:506/2035 train_time:16882ms step_avg:33.36ms
step:507/2035 train_time:16916ms step_avg:33.36ms
step:508/2035 train_time:16949ms step_avg:33.36ms
step:509/2035 train_time:16982ms step_avg:33.36ms
step:510/2035 train_time:17016ms step_avg:33.36ms
step:511/2035 train_time:17048ms step_avg:33.36ms
step:512/2035 train_time:17082ms step_avg:33.36ms
step:513/2035 train_time:17115ms step_avg:33.36ms
step:514/2035 train_time:17148ms step_avg:33.36ms
step:515/2035 train_time:17181ms step_avg:33.36ms
step:516/2035 train_time:17214ms step_avg:33.36ms
step:517/2035 train_time:17247ms step_avg:33.36ms
step:518/2035 train_time:17280ms step_avg:33.36ms
step:519/2035 train_time:17313ms step_avg:33.36ms
step:520/2035 train_time:17346ms step_avg:33.36ms
step:521/2035 train_time:17379ms step_avg:33.36ms
step:522/2035 train_time:17412ms step_avg:33.36ms
step:523/2035 train_time:17445ms step_avg:33.35ms
step:524/2035 train_time:17478ms step_avg:33.35ms
step:525/2035 train_time:17511ms step_avg:33.35ms
step:526/2035 train_time:17544ms step_avg:33.35ms
step:527/2035 train_time:17577ms step_avg:33.35ms
step:528/2035 train_time:17610ms step_avg:33.35ms
step:529/2035 train_time:17643ms step_avg:33.35ms
step:530/2035 train_time:17676ms step_avg:33.35ms
step:531/2035 train_time:17709ms step_avg:33.35ms
step:532/2035 train_time:17743ms step_avg:33.35ms
step:533/2035 train_time:17777ms step_avg:33.35ms
step:534/2035 train_time:17810ms step_avg:33.35ms
step:535/2035 train_time:17843ms step_avg:33.35ms
step:536/2035 train_time:17877ms step_avg:33.35ms
step:537/2035 train_time:17910ms step_avg:33.35ms
step:538/2035 train_time:17944ms step_avg:33.35ms
step:539/2035 train_time:17977ms step_avg:33.35ms
step:540/2035 train_time:18010ms step_avg:33.35ms
step:541/2035 train_time:18043ms step_avg:33.35ms
step:542/2035 train_time:18077ms step_avg:33.35ms
step:543/2035 train_time:18110ms step_avg:33.35ms
step:544/2035 train_time:18143ms step_avg:33.35ms
step:545/2035 train_time:18177ms step_avg:33.35ms
step:546/2035 train_time:18210ms step_avg:33.35ms
step:547/2035 train_time:18243ms step_avg:33.35ms
step:548/2035 train_time:18276ms step_avg:33.35ms
step:549/2035 train_time:18309ms step_avg:33.35ms
step:550/2035 train_time:18342ms step_avg:33.35ms
step:551/2035 train_time:18375ms step_avg:33.35ms
step:552/2035 train_time:18408ms step_avg:33.35ms
step:553/2035 train_time:18441ms step_avg:33.35ms
step:554/2035 train_time:18474ms step_avg:33.35ms
step:555/2035 train_time:18507ms step_avg:33.35ms
step:556/2035 train_time:18540ms step_avg:33.35ms
step:557/2035 train_time:18573ms step_avg:33.35ms
step:558/2035 train_time:18607ms step_avg:33.35ms
step:559/2035 train_time:18640ms step_avg:33.35ms
step:560/2035 train_time:18673ms step_avg:33.35ms
step:561/2035 train_time:18706ms step_avg:33.34ms
step:562/2035 train_time:18740ms step_avg:33.34ms
step:563/2035 train_time:18773ms step_avg:33.34ms
step:564/2035 train_time:18807ms step_avg:33.34ms
step:565/2035 train_time:18840ms step_avg:33.34ms
step:566/2035 train_time:18873ms step_avg:33.35ms
step:567/2035 train_time:18907ms step_avg:33.34ms
step:568/2035 train_time:18940ms step_avg:33.35ms
step:569/2035 train_time:18973ms step_avg:33.34ms
step:570/2035 train_time:19006ms step_avg:33.34ms
step:571/2035 train_time:19039ms step_avg:33.34ms
step:572/2035 train_time:19072ms step_avg:33.34ms
step:573/2035 train_time:19106ms step_avg:33.34ms
step:574/2035 train_time:19139ms step_avg:33.34ms
step:575/2035 train_time:19172ms step_avg:33.34ms
step:576/2035 train_time:19205ms step_avg:33.34ms
step:577/2035 train_time:19238ms step_avg:33.34ms
step:578/2035 train_time:19271ms step_avg:33.34ms
step:579/2035 train_time:19304ms step_avg:33.34ms
step:580/2035 train_time:19337ms step_avg:33.34ms
step:581/2035 train_time:19370ms step_avg:33.34ms
step:582/2035 train_time:19403ms step_avg:33.34ms
step:583/2035 train_time:19436ms step_avg:33.34ms
step:584/2035 train_time:19470ms step_avg:33.34ms
step:585/2035 train_time:19502ms step_avg:33.34ms
step:586/2035 train_time:19535ms step_avg:33.34ms
step:587/2035 train_time:19568ms step_avg:33.34ms
step:588/2035 train_time:19601ms step_avg:33.34ms
step:589/2035 train_time:19635ms step_avg:33.34ms
step:590/2035 train_time:19668ms step_avg:33.33ms
step:591/2035 train_time:19701ms step_avg:33.33ms
step:592/2035 train_time:19734ms step_avg:33.33ms
step:593/2035 train_time:19767ms step_avg:33.33ms
step:594/2035 train_time:19800ms step_avg:33.33ms
step:595/2035 train_time:19834ms step_avg:33.33ms
step:596/2035 train_time:19867ms step_avg:33.33ms
step:597/2035 train_time:19900ms step_avg:33.33ms
step:598/2035 train_time:19934ms step_avg:33.33ms
step:599/2035 train_time:19967ms step_avg:33.33ms
step:600/2035 train_time:20000ms step_avg:33.33ms
step:601/2035 train_time:20034ms step_avg:33.33ms
step:602/2035 train_time:20067ms step_avg:33.33ms
step:603/2035 train_time:20100ms step_avg:33.33ms
step:604/2035 train_time:20134ms step_avg:33.33ms
step:605/2035 train_time:20167ms step_avg:33.33ms
step:606/2035 train_time:20200ms step_avg:33.33ms
step:607/2035 train_time:20234ms step_avg:33.33ms
step:608/2035 train_time:20267ms step_avg:33.33ms
step:609/2035 train_time:20300ms step_avg:33.33ms
step:610/2035 train_time:20333ms step_avg:33.33ms
step:611/2035 train_time:20366ms step_avg:33.33ms
step:612/2035 train_time:20399ms step_avg:33.33ms
step:613/2035 train_time:20433ms step_avg:33.33ms
step:614/2035 train_time:20466ms step_avg:33.33ms
step:615/2035 train_time:20499ms step_avg:33.33ms
step:616/2035 train_time:20532ms step_avg:33.33ms
step:617/2035 train_time:20565ms step_avg:33.33ms
step:618/2035 train_time:20599ms step_avg:33.33ms
step:619/2035 train_time:20632ms step_avg:33.33ms
step:620/2035 train_time:20665ms step_avg:33.33ms
step:621/2035 train_time:20698ms step_avg:33.33ms
step:622/2035 train_time:20731ms step_avg:33.33ms
step:623/2035 train_time:20765ms step_avg:33.33ms
step:624/2035 train_time:20798ms step_avg:33.33ms
step:625/2035 train_time:20832ms step_avg:33.33ms
step:626/2035 train_time:20865ms step_avg:33.33ms
step:627/2035 train_time:20898ms step_avg:33.33ms
step:628/2035 train_time:20931ms step_avg:33.33ms
step:629/2035 train_time:20964ms step_avg:33.33ms
step:630/2035 train_time:20998ms step_avg:33.33ms
step:631/2035 train_time:21030ms step_avg:33.33ms
step:632/2035 train_time:21063ms step_avg:33.33ms
step:633/2035 train_time:21096ms step_avg:33.33ms
step:634/2035 train_time:21130ms step_avg:33.33ms
step:635/2035 train_time:21163ms step_avg:33.33ms
step:636/2035 train_time:21196ms step_avg:33.33ms
step:637/2035 train_time:21229ms step_avg:33.33ms
step:638/2035 train_time:21263ms step_avg:33.33ms
step:639/2035 train_time:21295ms step_avg:33.33ms
step:640/2035 train_time:21329ms step_avg:33.33ms
step:641/2035 train_time:21362ms step_avg:33.33ms
step:642/2035 train_time:21395ms step_avg:33.33ms
step:643/2035 train_time:21428ms step_avg:33.33ms
step:644/2035 train_time:21462ms step_avg:33.33ms
step:645/2035 train_time:21495ms step_avg:33.32ms
step:646/2035 train_time:21528ms step_avg:33.33ms
step:647/2035 train_time:21561ms step_avg:33.32ms
step:648/2035 train_time:21595ms step_avg:33.33ms
step:649/2035 train_time:21628ms step_avg:33.32ms
step:650/2035 train_time:21661ms step_avg:33.32ms
step:651/2035 train_time:21694ms step_avg:33.32ms
step:652/2035 train_time:21727ms step_avg:33.32ms
step:653/2035 train_time:21760ms step_avg:33.32ms
step:654/2035 train_time:21794ms step_avg:33.32ms
step:655/2035 train_time:21827ms step_avg:33.32ms
step:656/2035 train_time:21860ms step_avg:33.32ms
step:657/2035 train_time:21893ms step_avg:33.32ms
step:658/2035 train_time:21926ms step_avg:33.32ms
step:659/2035 train_time:21960ms step_avg:33.32ms
step:660/2035 train_time:21994ms step_avg:33.32ms
step:661/2035 train_time:22027ms step_avg:33.32ms
step:662/2035 train_time:22060ms step_avg:33.32ms
step:663/2035 train_time:22094ms step_avg:33.32ms
step:664/2035 train_time:22127ms step_avg:33.32ms
step:665/2035 train_time:22160ms step_avg:33.32ms
step:666/2035 train_time:22194ms step_avg:33.32ms
step:667/2035 train_time:22253ms step_avg:33.36ms
step:668/2035 train_time:22313ms step_avg:33.40ms
step:669/2035 train_time:22373ms step_avg:33.44ms
step:670/2035 train_time:22433ms step_avg:33.48ms
step:671/2035 train_time:22494ms step_avg:33.52ms
step:672/2035 train_time:22553ms step_avg:33.56ms
step:673/2035 train_time:22614ms step_avg:33.60ms
step:674/2035 train_time:22674ms step_avg:33.64ms
step:675/2035 train_time:22734ms step_avg:33.68ms
step:676/2035 train_time:22793ms step_avg:33.72ms
step:677/2035 train_time:22854ms step_avg:33.76ms
step:678/2035 train_time:22914ms step_avg:33.80ms
step:679/2035 train_time:22975ms step_avg:33.84ms
step:680/2035 train_time:23035ms step_avg:33.87ms
step:681/2035 train_time:23095ms step_avg:33.91ms
step:682/2035 train_time:23154ms step_avg:33.95ms
step:683/2035 train_time:23214ms step_avg:33.99ms
step:684/2035 train_time:23273ms step_avg:34.03ms
step:685/2035 train_time:23334ms step_avg:34.06ms
step:686/2035 train_time:23394ms step_avg:34.10ms
step:687/2035 train_time:23455ms step_avg:34.14ms
step:688/2035 train_time:23514ms step_avg:34.18ms
step:689/2035 train_time:23574ms step_avg:34.22ms
step:690/2035 train_time:23634ms step_avg:34.25ms
step:691/2035 train_time:23694ms step_avg:34.29ms
step:692/2035 train_time:23754ms step_avg:34.33ms
step:693/2035 train_time:23815ms step_avg:34.36ms
step:694/2035 train_time:23874ms step_avg:34.40ms
step:695/2035 train_time:23936ms step_avg:34.44ms
step:696/2035 train_time:23996ms step_avg:34.48ms
step:697/2035 train_time:24057ms step_avg:34.52ms
step:698/2035 train_time:24116ms step_avg:34.55ms
step:699/2035 train_time:24177ms step_avg:34.59ms
step:700/2035 train_time:24236ms step_avg:34.62ms
step:701/2035 train_time:24297ms step_avg:34.66ms
step:702/2035 train_time:24357ms step_avg:34.70ms
step:703/2035 train_time:24417ms step_avg:34.73ms
step:704/2035 train_time:24477ms step_avg:34.77ms
step:705/2035 train_time:24538ms step_avg:34.81ms
step:706/2035 train_time:24598ms step_avg:34.84ms
step:707/2035 train_time:24658ms step_avg:34.88ms
step:708/2035 train_time:24718ms step_avg:34.91ms
step:709/2035 train_time:24779ms step_avg:34.95ms
step:710/2035 train_time:24839ms step_avg:34.98ms
step:711/2035 train_time:24900ms step_avg:35.02ms
step:712/2035 train_time:24960ms step_avg:35.06ms
step:713/2035 train_time:25021ms step_avg:35.09ms
step:714/2035 train_time:25081ms step_avg:35.13ms
step:715/2035 train_time:25142ms step_avg:35.16ms
step:716/2035 train_time:25203ms step_avg:35.20ms
step:717/2035 train_time:25264ms step_avg:35.24ms
step:718/2035 train_time:25324ms step_avg:35.27ms
step:719/2035 train_time:25384ms step_avg:35.30ms
step:720/2035 train_time:25444ms step_avg:35.34ms
step:721/2035 train_time:25506ms step_avg:35.38ms
step:722/2035 train_time:25567ms step_avg:35.41ms
step:723/2035 train_time:25628ms step_avg:35.45ms
step:724/2035 train_time:25688ms step_avg:35.48ms
step:725/2035 train_time:25749ms step_avg:35.52ms
step:726/2035 train_time:25809ms step_avg:35.55ms
step:727/2035 train_time:25870ms step_avg:35.58ms
step:728/2035 train_time:25929ms step_avg:35.62ms
step:729/2035 train_time:25990ms step_avg:35.65ms
step:730/2035 train_time:26050ms step_avg:35.68ms
step:731/2035 train_time:26110ms step_avg:35.72ms
step:732/2035 train_time:26170ms step_avg:35.75ms
step:733/2035 train_time:26230ms step_avg:35.78ms
step:734/2035 train_time:26289ms step_avg:35.82ms
step:735/2035 train_time:26350ms step_avg:35.85ms
step:736/2035 train_time:26410ms step_avg:35.88ms
step:737/2035 train_time:26470ms step_avg:35.92ms
step:738/2035 train_time:26529ms step_avg:35.95ms
step:739/2035 train_time:26589ms step_avg:35.98ms
step:740/2035 train_time:26649ms step_avg:36.01ms
step:741/2035 train_time:26709ms step_avg:36.04ms
step:742/2035 train_time:26769ms step_avg:36.08ms
step:743/2035 train_time:26830ms step_avg:36.11ms
step:744/2035 train_time:26889ms step_avg:36.14ms
step:745/2035 train_time:26951ms step_avg:36.18ms
step:746/2035 train_time:27010ms step_avg:36.21ms
step:747/2035 train_time:27071ms step_avg:36.24ms
step:748/2035 train_time:27130ms step_avg:36.27ms
step:749/2035 train_time:27190ms step_avg:36.30ms
step:750/2035 train_time:27249ms step_avg:36.33ms
step:750/2035 val_loss:3.8362 train_time:27312ms step_avg:36.42ms
step:751/2035 train_time:27332ms step_avg:36.39ms
step:752/2035 train_time:27373ms step_avg:36.40ms
step:753/2035 train_time:27435ms step_avg:36.43ms
step:754/2035 train_time:27496ms step_avg:36.47ms
step:755/2035 train_time:27556ms step_avg:36.50ms
step:756/2035 train_time:27615ms step_avg:36.53ms
step:757/2035 train_time:27675ms step_avg:36.56ms
step:758/2035 train_time:27734ms step_avg:36.59ms
step:759/2035 train_time:27794ms step_avg:36.62ms
step:760/2035 train_time:27854ms step_avg:36.65ms
step:761/2035 train_time:27914ms step_avg:36.68ms
step:762/2035 train_time:27974ms step_avg:36.71ms
step:763/2035 train_time:28034ms step_avg:36.74ms
step:764/2035 train_time:28094ms step_avg:36.77ms
step:765/2035 train_time:28154ms step_avg:36.80ms
step:766/2035 train_time:28213ms step_avg:36.83ms
step:767/2035 train_time:28275ms step_avg:36.86ms
step:768/2035 train_time:28337ms step_avg:36.90ms
step:769/2035 train_time:28398ms step_avg:36.93ms
step:770/2035 train_time:28459ms step_avg:36.96ms
step:771/2035 train_time:28519ms step_avg:36.99ms
step:772/2035 train_time:28578ms step_avg:37.02ms
step:773/2035 train_time:28638ms step_avg:37.05ms
step:774/2035 train_time:28697ms step_avg:37.08ms
step:775/2035 train_time:28757ms step_avg:37.11ms
step:776/2035 train_time:28816ms step_avg:37.13ms
step:777/2035 train_time:28877ms step_avg:37.16ms
step:778/2035 train_time:28935ms step_avg:37.19ms
step:779/2035 train_time:28996ms step_avg:37.22ms
step:780/2035 train_time:29055ms step_avg:37.25ms
step:781/2035 train_time:29115ms step_avg:37.28ms
step:782/2035 train_time:29175ms step_avg:37.31ms
step:783/2035 train_time:29235ms step_avg:37.34ms
step:784/2035 train_time:29295ms step_avg:37.37ms
step:785/2035 train_time:29357ms step_avg:37.40ms
step:786/2035 train_time:29417ms step_avg:37.43ms
step:787/2035 train_time:29478ms step_avg:37.46ms
step:788/2035 train_time:29537ms step_avg:37.48ms
step:789/2035 train_time:29598ms step_avg:37.51ms
step:790/2035 train_time:29658ms step_avg:37.54ms
step:791/2035 train_time:29718ms step_avg:37.57ms
step:792/2035 train_time:29777ms step_avg:37.60ms
step:793/2035 train_time:29838ms step_avg:37.63ms
step:794/2035 train_time:29897ms step_avg:37.65ms
step:795/2035 train_time:29958ms step_avg:37.68ms
step:796/2035 train_time:30016ms step_avg:37.71ms
step:797/2035 train_time:30076ms step_avg:37.74ms
step:798/2035 train_time:30135ms step_avg:37.76ms
step:799/2035 train_time:30197ms step_avg:37.79ms
step:800/2035 train_time:30256ms step_avg:37.82ms
step:801/2035 train_time:30317ms step_avg:37.85ms
step:802/2035 train_time:30377ms step_avg:37.88ms
step:803/2035 train_time:30437ms step_avg:37.90ms
step:804/2035 train_time:30497ms step_avg:37.93ms
step:805/2035 train_time:30559ms step_avg:37.96ms
step:806/2035 train_time:30618ms step_avg:37.99ms
step:807/2035 train_time:30678ms step_avg:38.02ms
step:808/2035 train_time:30738ms step_avg:38.04ms
step:809/2035 train_time:30798ms step_avg:38.07ms
step:810/2035 train_time:30858ms step_avg:38.10ms
step:811/2035 train_time:30917ms step_avg:38.12ms
step:812/2035 train_time:30976ms step_avg:38.15ms
step:813/2035 train_time:31037ms step_avg:38.18ms
step:814/2035 train_time:31096ms step_avg:38.20ms
step:815/2035 train_time:31156ms step_avg:38.23ms
step:816/2035 train_time:31216ms step_avg:38.25ms
step:817/2035 train_time:31276ms step_avg:38.28ms
step:818/2035 train_time:31335ms step_avg:38.31ms
step:819/2035 train_time:31396ms step_avg:38.33ms
step:820/2035 train_time:31456ms step_avg:38.36ms
step:821/2035 train_time:31517ms step_avg:38.39ms
step:822/2035 train_time:31576ms step_avg:38.41ms
step:823/2035 train_time:31637ms step_avg:38.44ms
step:824/2035 train_time:31696ms step_avg:38.47ms
step:825/2035 train_time:31757ms step_avg:38.49ms
step:826/2035 train_time:31816ms step_avg:38.52ms
step:827/2035 train_time:31876ms step_avg:38.54ms
step:828/2035 train_time:31935ms step_avg:38.57ms
step:829/2035 train_time:31995ms step_avg:38.59ms
step:830/2035 train_time:32055ms step_avg:38.62ms
step:831/2035 train_time:32114ms step_avg:38.65ms
step:832/2035 train_time:32174ms step_avg:38.67ms
step:833/2035 train_time:32234ms step_avg:38.70ms
step:834/2035 train_time:32294ms step_avg:38.72ms
step:835/2035 train_time:32356ms step_avg:38.75ms
step:836/2035 train_time:32415ms step_avg:38.77ms
step:837/2035 train_time:32476ms step_avg:38.80ms
step:838/2035 train_time:32536ms step_avg:38.83ms
step:839/2035 train_time:32596ms step_avg:38.85ms
step:840/2035 train_time:32656ms step_avg:38.88ms
step:841/2035 train_time:32716ms step_avg:38.90ms
step:842/2035 train_time:32776ms step_avg:38.93ms
step:843/2035 train_time:32836ms step_avg:38.95ms
step:844/2035 train_time:32895ms step_avg:38.97ms
step:845/2035 train_time:32955ms step_avg:39.00ms
step:846/2035 train_time:33015ms step_avg:39.02ms
step:847/2035 train_time:33076ms step_avg:39.05ms
step:848/2035 train_time:33136ms step_avg:39.08ms
step:849/2035 train_time:33195ms step_avg:39.10ms
step:850/2035 train_time:33256ms step_avg:39.12ms
step:851/2035 train_time:33316ms step_avg:39.15ms
step:852/2035 train_time:33376ms step_avg:39.17ms
step:853/2035 train_time:33436ms step_avg:39.20ms
step:854/2035 train_time:33497ms step_avg:39.22ms
step:855/2035 train_time:33557ms step_avg:39.25ms
step:856/2035 train_time:33617ms step_avg:39.27ms
step:857/2035 train_time:33677ms step_avg:39.30ms
step:858/2035 train_time:33736ms step_avg:39.32ms
step:859/2035 train_time:33796ms step_avg:39.34ms
step:860/2035 train_time:33855ms step_avg:39.37ms
step:861/2035 train_time:33916ms step_avg:39.39ms
step:862/2035 train_time:33975ms step_avg:39.41ms
step:863/2035 train_time:34036ms step_avg:39.44ms
step:864/2035 train_time:34095ms step_avg:39.46ms
step:865/2035 train_time:34156ms step_avg:39.49ms
step:866/2035 train_time:34215ms step_avg:39.51ms
step:867/2035 train_time:34275ms step_avg:39.53ms
step:868/2035 train_time:34334ms step_avg:39.56ms
step:869/2035 train_time:34394ms step_avg:39.58ms
step:870/2035 train_time:34455ms step_avg:39.60ms
step:871/2035 train_time:34515ms step_avg:39.63ms
step:872/2035 train_time:34575ms step_avg:39.65ms
step:873/2035 train_time:34636ms step_avg:39.68ms
step:874/2035 train_time:34697ms step_avg:39.70ms
step:875/2035 train_time:34757ms step_avg:39.72ms
step:876/2035 train_time:34817ms step_avg:39.75ms
step:877/2035 train_time:34877ms step_avg:39.77ms
step:878/2035 train_time:34936ms step_avg:39.79ms
step:879/2035 train_time:34996ms step_avg:39.81ms
step:880/2035 train_time:35056ms step_avg:39.84ms
step:881/2035 train_time:35116ms step_avg:39.86ms
step:882/2035 train_time:35175ms step_avg:39.88ms
step:883/2035 train_time:35236ms step_avg:39.90ms
step:884/2035 train_time:35295ms step_avg:39.93ms
step:885/2035 train_time:35356ms step_avg:39.95ms
step:886/2035 train_time:35415ms step_avg:39.97ms
step:887/2035 train_time:35476ms step_avg:40.00ms
step:888/2035 train_time:35535ms step_avg:40.02ms
step:889/2035 train_time:35595ms step_avg:40.04ms
step:890/2035 train_time:35655ms step_avg:40.06ms
step:891/2035 train_time:35715ms step_avg:40.08ms
step:892/2035 train_time:35775ms step_avg:40.11ms
step:893/2035 train_time:35836ms step_avg:40.13ms
step:894/2035 train_time:35895ms step_avg:40.15ms
step:895/2035 train_time:35956ms step_avg:40.17ms
step:896/2035 train_time:36016ms step_avg:40.20ms
step:897/2035 train_time:36076ms step_avg:40.22ms
step:898/2035 train_time:36136ms step_avg:40.24ms
step:899/2035 train_time:36196ms step_avg:40.26ms
step:900/2035 train_time:36256ms step_avg:40.28ms
step:901/2035 train_time:36316ms step_avg:40.31ms
step:902/2035 train_time:36376ms step_avg:40.33ms
step:903/2035 train_time:36436ms step_avg:40.35ms
step:904/2035 train_time:36496ms step_avg:40.37ms
step:905/2035 train_time:36556ms step_avg:40.39ms
step:906/2035 train_time:36615ms step_avg:40.41ms
step:907/2035 train_time:36676ms step_avg:40.44ms
step:908/2035 train_time:36735ms step_avg:40.46ms
step:909/2035 train_time:36796ms step_avg:40.48ms
step:910/2035 train_time:36856ms step_avg:40.50ms
step:911/2035 train_time:36916ms step_avg:40.52ms
step:912/2035 train_time:36976ms step_avg:40.54ms
step:913/2035 train_time:37036ms step_avg:40.57ms
step:914/2035 train_time:37096ms step_avg:40.59ms
step:915/2035 train_time:37156ms step_avg:40.61ms
step:916/2035 train_time:37215ms step_avg:40.63ms
step:917/2035 train_time:37275ms step_avg:40.65ms
step:918/2035 train_time:37335ms step_avg:40.67ms
step:919/2035 train_time:37395ms step_avg:40.69ms
step:920/2035 train_time:37455ms step_avg:40.71ms
step:921/2035 train_time:37516ms step_avg:40.73ms
step:922/2035 train_time:37576ms step_avg:40.75ms
step:923/2035 train_time:37636ms step_avg:40.78ms
step:924/2035 train_time:37695ms step_avg:40.80ms
step:925/2035 train_time:37756ms step_avg:40.82ms
step:926/2035 train_time:37816ms step_avg:40.84ms
step:927/2035 train_time:37876ms step_avg:40.86ms
step:928/2035 train_time:37935ms step_avg:40.88ms
step:929/2035 train_time:37995ms step_avg:40.90ms
step:930/2035 train_time:38055ms step_avg:40.92ms
step:931/2035 train_time:38115ms step_avg:40.94ms
step:932/2035 train_time:38175ms step_avg:40.96ms
step:933/2035 train_time:38235ms step_avg:40.98ms
step:934/2035 train_time:38295ms step_avg:41.00ms
step:935/2035 train_time:38356ms step_avg:41.02ms
step:936/2035 train_time:38415ms step_avg:41.04ms
step:937/2035 train_time:38476ms step_avg:41.06ms
step:938/2035 train_time:38536ms step_avg:41.08ms
step:939/2035 train_time:38596ms step_avg:41.10ms
step:940/2035 train_time:38656ms step_avg:41.12ms
step:941/2035 train_time:38716ms step_avg:41.14ms
step:942/2035 train_time:38775ms step_avg:41.16ms
step:943/2035 train_time:38835ms step_avg:41.18ms
step:944/2035 train_time:38895ms step_avg:41.20ms
step:945/2035 train_time:38956ms step_avg:41.22ms
step:946/2035 train_time:39015ms step_avg:41.24ms
step:947/2035 train_time:39075ms step_avg:41.26ms
step:948/2035 train_time:39135ms step_avg:41.28ms
step:949/2035 train_time:39195ms step_avg:41.30ms
step:950/2035 train_time:39255ms step_avg:41.32ms
step:951/2035 train_time:39315ms step_avg:41.34ms
step:952/2035 train_time:39375ms step_avg:41.36ms
step:953/2035 train_time:39435ms step_avg:41.38ms
step:954/2035 train_time:39496ms step_avg:41.40ms
step:955/2035 train_time:39557ms step_avg:41.42ms
step:956/2035 train_time:39616ms step_avg:41.44ms
step:957/2035 train_time:39676ms step_avg:41.46ms
step:958/2035 train_time:39735ms step_avg:41.48ms
step:959/2035 train_time:39796ms step_avg:41.50ms
step:960/2035 train_time:39856ms step_avg:41.52ms
step:961/2035 train_time:39916ms step_avg:41.54ms
step:962/2035 train_time:39976ms step_avg:41.56ms
step:963/2035 train_time:40036ms step_avg:41.57ms
step:964/2035 train_time:40096ms step_avg:41.59ms
step:965/2035 train_time:40156ms step_avg:41.61ms
step:966/2035 train_time:40215ms step_avg:41.63ms
step:967/2035 train_time:40275ms step_avg:41.65ms
step:968/2035 train_time:40335ms step_avg:41.67ms
step:969/2035 train_time:40395ms step_avg:41.69ms
step:970/2035 train_time:40455ms step_avg:41.71ms
step:971/2035 train_time:40516ms step_avg:41.73ms
step:972/2035 train_time:40575ms step_avg:41.74ms
step:973/2035 train_time:40636ms step_avg:41.76ms
step:974/2035 train_time:40695ms step_avg:41.78ms
step:975/2035 train_time:40755ms step_avg:41.80ms
step:976/2035 train_time:40815ms step_avg:41.82ms
step:977/2035 train_time:40875ms step_avg:41.84ms
step:978/2035 train_time:40935ms step_avg:41.86ms
step:979/2035 train_time:40995ms step_avg:41.87ms
step:980/2035 train_time:41055ms step_avg:41.89ms
step:981/2035 train_time:41115ms step_avg:41.91ms
step:982/2035 train_time:41174ms step_avg:41.93ms
step:983/2035 train_time:41234ms step_avg:41.95ms
step:984/2035 train_time:41293ms step_avg:41.96ms
step:985/2035 train_time:41354ms step_avg:41.98ms
step:986/2035 train_time:41414ms step_avg:42.00ms
step:987/2035 train_time:41475ms step_avg:42.02ms
step:988/2035 train_time:41535ms step_avg:42.04ms
step:989/2035 train_time:41596ms step_avg:42.06ms
step:990/2035 train_time:41656ms step_avg:42.08ms
step:991/2035 train_time:41716ms step_avg:42.09ms
step:992/2035 train_time:41775ms step_avg:42.11ms
step:993/2035 train_time:41835ms step_avg:42.13ms
step:994/2035 train_time:41895ms step_avg:42.15ms
step:995/2035 train_time:41955ms step_avg:42.17ms
step:996/2035 train_time:42015ms step_avg:42.18ms
step:997/2035 train_time:42074ms step_avg:42.20ms
step:998/2035 train_time:42133ms step_avg:42.22ms
step:999/2035 train_time:42194ms step_avg:42.24ms
step:1000/2035 train_time:42254ms step_avg:42.25ms
step:1000/2035 val_loss:3.6842 train_time:42317ms step_avg:42.32ms
step:1001/2035 train_time:42337ms step_avg:42.29ms
step:1002/2035 train_time:42377ms step_avg:42.29ms
step:1003/2035 train_time:42440ms step_avg:42.31ms
step:1004/2035 train_time:42502ms step_avg:42.33ms
step:1005/2035 train_time:42563ms step_avg:42.35ms
step:1006/2035 train_time:42623ms step_avg:42.37ms
step:1007/2035 train_time:42683ms step_avg:42.39ms
step:1008/2035 train_time:42743ms step_avg:42.40ms
step:1009/2035 train_time:42803ms step_avg:42.42ms
step:1010/2035 train_time:42862ms step_avg:42.44ms
step:1011/2035 train_time:42922ms step_avg:42.46ms
step:1012/2035 train_time:42981ms step_avg:42.47ms
step:1013/2035 train_time:43040ms step_avg:42.49ms
step:1014/2035 train_time:43102ms step_avg:42.51ms
step:1015/2035 train_time:43162ms step_avg:42.52ms
step:1016/2035 train_time:43222ms step_avg:42.54ms
step:1017/2035 train_time:43283ms step_avg:42.56ms
step:1018/2035 train_time:43344ms step_avg:42.58ms
step:1019/2035 train_time:43406ms step_avg:42.60ms
step:1020/2035 train_time:43467ms step_avg:42.61ms
step:1021/2035 train_time:43527ms step_avg:42.63ms
step:1022/2035 train_time:43586ms step_avg:42.65ms
step:1023/2035 train_time:43646ms step_avg:42.66ms
step:1024/2035 train_time:43706ms step_avg:42.68ms
step:1025/2035 train_time:43766ms step_avg:42.70ms
step:1026/2035 train_time:43825ms step_avg:42.71ms
step:1027/2035 train_time:43885ms step_avg:42.73ms
step:1028/2035 train_time:43944ms step_avg:42.75ms
step:1029/2035 train_time:44004ms step_avg:42.76ms
step:1030/2035 train_time:44063ms step_avg:42.78ms
step:1031/2035 train_time:44124ms step_avg:42.80ms
step:1032/2035 train_time:44183ms step_avg:42.81ms
step:1033/2035 train_time:44244ms step_avg:42.83ms
step:1034/2035 train_time:44304ms step_avg:42.85ms
step:1035/2035 train_time:44366ms step_avg:42.87ms
step:1036/2035 train_time:44427ms step_avg:42.88ms
step:1037/2035 train_time:44487ms step_avg:42.90ms
step:1038/2035 train_time:44547ms step_avg:42.92ms
step:1039/2035 train_time:44608ms step_avg:42.93ms
step:1040/2035 train_time:44667ms step_avg:42.95ms
step:1041/2035 train_time:44727ms step_avg:42.97ms
step:1042/2035 train_time:44786ms step_avg:42.98ms
step:1043/2035 train_time:44846ms step_avg:43.00ms
step:1044/2035 train_time:44905ms step_avg:43.01ms
step:1045/2035 train_time:44965ms step_avg:43.03ms
step:1046/2035 train_time:45024ms step_avg:43.04ms
step:1047/2035 train_time:45084ms step_avg:43.06ms
step:1048/2035 train_time:45144ms step_avg:43.08ms
step:1049/2035 train_time:45205ms step_avg:43.09ms
step:1050/2035 train_time:45264ms step_avg:43.11ms
step:1051/2035 train_time:45326ms step_avg:43.13ms
step:1052/2035 train_time:45386ms step_avg:43.14ms
step:1053/2035 train_time:45447ms step_avg:43.16ms
step:1054/2035 train_time:45507ms step_avg:43.18ms
step:1055/2035 train_time:45567ms step_avg:43.19ms
step:1056/2035 train_time:45627ms step_avg:43.21ms
step:1057/2035 train_time:45687ms step_avg:43.22ms
step:1058/2035 train_time:45746ms step_avg:43.24ms
step:1059/2035 train_time:45806ms step_avg:43.25ms
step:1060/2035 train_time:45865ms step_avg:43.27ms
step:1061/2035 train_time:45925ms step_avg:43.28ms
step:1062/2035 train_time:45985ms step_avg:43.30ms
step:1063/2035 train_time:46046ms step_avg:43.32ms
step:1064/2035 train_time:46106ms step_avg:43.33ms
step:1065/2035 train_time:46166ms step_avg:43.35ms
step:1066/2035 train_time:46225ms step_avg:43.36ms
step:1067/2035 train_time:46285ms step_avg:43.38ms
step:1068/2035 train_time:46346ms step_avg:43.40ms
step:1069/2035 train_time:46407ms step_avg:43.41ms
step:1070/2035 train_time:46467ms step_avg:43.43ms
step:1071/2035 train_time:46528ms step_avg:43.44ms
step:1072/2035 train_time:46587ms step_avg:43.46ms
step:1073/2035 train_time:46648ms step_avg:43.47ms
step:1074/2035 train_time:46708ms step_avg:43.49ms
step:1075/2035 train_time:46767ms step_avg:43.50ms
step:1076/2035 train_time:46826ms step_avg:43.52ms
step:1077/2035 train_time:46886ms step_avg:43.53ms
step:1078/2035 train_time:46945ms step_avg:43.55ms
step:1079/2035 train_time:47007ms step_avg:43.56ms
step:1080/2035 train_time:47066ms step_avg:43.58ms
step:1081/2035 train_time:47126ms step_avg:43.59ms
step:1082/2035 train_time:47185ms step_avg:43.61ms
step:1083/2035 train_time:47246ms step_avg:43.62ms
step:1084/2035 train_time:47306ms step_avg:43.64ms
step:1085/2035 train_time:47366ms step_avg:43.66ms
step:1086/2035 train_time:47426ms step_avg:43.67ms
step:1087/2035 train_time:47486ms step_avg:43.69ms
step:1088/2035 train_time:47546ms step_avg:43.70ms
step:1089/2035 train_time:47606ms step_avg:43.72ms
step:1090/2035 train_time:47666ms step_avg:43.73ms
step:1091/2035 train_time:47726ms step_avg:43.75ms
step:1092/2035 train_time:47785ms step_avg:43.76ms
step:1093/2035 train_time:47845ms step_avg:43.77ms
step:1094/2035 train_time:47905ms step_avg:43.79ms
step:1095/2035 train_time:47965ms step_avg:43.80ms
step:1096/2035 train_time:48025ms step_avg:43.82ms
step:1097/2035 train_time:48085ms step_avg:43.83ms
step:1098/2035 train_time:48145ms step_avg:43.85ms
step:1099/2035 train_time:48206ms step_avg:43.86ms
step:1100/2035 train_time:48265ms step_avg:43.88ms
step:1101/2035 train_time:48325ms step_avg:43.89ms
step:1102/2035 train_time:48385ms step_avg:43.91ms
step:1103/2035 train_time:48446ms step_avg:43.92ms
step:1104/2035 train_time:48506ms step_avg:43.94ms
step:1105/2035 train_time:48567ms step_avg:43.95ms
step:1106/2035 train_time:48626ms step_avg:43.97ms
step:1107/2035 train_time:48686ms step_avg:43.98ms
step:1108/2035 train_time:48745ms step_avg:43.99ms
step:1109/2035 train_time:48806ms step_avg:44.01ms
step:1110/2035 train_time:48866ms step_avg:44.02ms
step:1111/2035 train_time:48926ms step_avg:44.04ms
step:1112/2035 train_time:48986ms step_avg:44.05ms
step:1113/2035 train_time:49046ms step_avg:44.07ms
step:1114/2035 train_time:49105ms step_avg:44.08ms
step:1115/2035 train_time:49165ms step_avg:44.09ms
step:1116/2035 train_time:49224ms step_avg:44.11ms
step:1117/2035 train_time:49285ms step_avg:44.12ms
step:1118/2035 train_time:49345ms step_avg:44.14ms
step:1119/2035 train_time:49406ms step_avg:44.15ms
step:1120/2035 train_time:49465ms step_avg:44.17ms
step:1121/2035 train_time:49527ms step_avg:44.18ms
step:1122/2035 train_time:49586ms step_avg:44.19ms
step:1123/2035 train_time:49647ms step_avg:44.21ms
step:1124/2035 train_time:49707ms step_avg:44.22ms
step:1125/2035 train_time:49766ms step_avg:44.24ms
step:1126/2035 train_time:49825ms step_avg:44.25ms
step:1127/2035 train_time:49886ms step_avg:44.26ms
step:1128/2035 train_time:49945ms step_avg:44.28ms
step:1129/2035 train_time:50005ms step_avg:44.29ms
step:1130/2035 train_time:50065ms step_avg:44.30ms
step:1131/2035 train_time:50125ms step_avg:44.32ms
step:1132/2035 train_time:50185ms step_avg:44.33ms
step:1133/2035 train_time:50245ms step_avg:44.35ms
step:1134/2035 train_time:50305ms step_avg:44.36ms
step:1135/2035 train_time:50366ms step_avg:44.38ms
step:1136/2035 train_time:50425ms step_avg:44.39ms
step:1137/2035 train_time:50486ms step_avg:44.40ms
step:1138/2035 train_time:50545ms step_avg:44.42ms
step:1139/2035 train_time:50605ms step_avg:44.43ms
step:1140/2035 train_time:50665ms step_avg:44.44ms
step:1141/2035 train_time:50726ms step_avg:44.46ms
step:1142/2035 train_time:50785ms step_avg:44.47ms
step:1143/2035 train_time:50845ms step_avg:44.48ms
step:1144/2035 train_time:50905ms step_avg:44.50ms
step:1145/2035 train_time:50965ms step_avg:44.51ms
step:1146/2035 train_time:51025ms step_avg:44.52ms
step:1147/2035 train_time:51085ms step_avg:44.54ms
step:1148/2035 train_time:51145ms step_avg:44.55ms
step:1149/2035 train_time:51207ms step_avg:44.57ms
step:1150/2035 train_time:51266ms step_avg:44.58ms
step:1151/2035 train_time:51326ms step_avg:44.59ms
step:1152/2035 train_time:51385ms step_avg:44.61ms
step:1153/2035 train_time:51445ms step_avg:44.62ms
step:1154/2035 train_time:51505ms step_avg:44.63ms
step:1155/2035 train_time:51566ms step_avg:44.65ms
step:1156/2035 train_time:51626ms step_avg:44.66ms
step:1157/2035 train_time:51687ms step_avg:44.67ms
step:1158/2035 train_time:51746ms step_avg:44.69ms
step:1159/2035 train_time:51807ms step_avg:44.70ms
step:1160/2035 train_time:51866ms step_avg:44.71ms
step:1161/2035 train_time:51926ms step_avg:44.73ms
step:1162/2035 train_time:51985ms step_avg:44.74ms
step:1163/2035 train_time:52046ms step_avg:44.75ms
step:1164/2035 train_time:52105ms step_avg:44.76ms
step:1165/2035 train_time:52165ms step_avg:44.78ms
step:1166/2035 train_time:52225ms step_avg:44.79ms
step:1167/2035 train_time:52285ms step_avg:44.80ms
step:1168/2035 train_time:52345ms step_avg:44.82ms
step:1169/2035 train_time:52405ms step_avg:44.83ms
step:1170/2035 train_time:52464ms step_avg:44.84ms
step:1171/2035 train_time:52525ms step_avg:44.85ms
step:1172/2035 train_time:52585ms step_avg:44.87ms
step:1173/2035 train_time:52645ms step_avg:44.88ms
step:1174/2035 train_time:52706ms step_avg:44.89ms
step:1175/2035 train_time:52766ms step_avg:44.91ms
step:1176/2035 train_time:52826ms step_avg:44.92ms
step:1177/2035 train_time:52886ms step_avg:44.93ms
step:1178/2035 train_time:52945ms step_avg:44.95ms
step:1179/2035 train_time:53006ms step_avg:44.96ms
step:1180/2035 train_time:53065ms step_avg:44.97ms
step:1181/2035 train_time:53125ms step_avg:44.98ms
step:1182/2035 train_time:53185ms step_avg:45.00ms
step:1183/2035 train_time:53245ms step_avg:45.01ms
step:1184/2035 train_time:53306ms step_avg:45.02ms
step:1185/2035 train_time:53366ms step_avg:45.03ms
step:1186/2035 train_time:53425ms step_avg:45.05ms
step:1187/2035 train_time:53486ms step_avg:45.06ms
step:1188/2035 train_time:53545ms step_avg:45.07ms
step:1189/2035 train_time:53606ms step_avg:45.09ms
step:1190/2035 train_time:53666ms step_avg:45.10ms
step:1191/2035 train_time:53727ms step_avg:45.11ms
step:1192/2035 train_time:53786ms step_avg:45.12ms
step:1193/2035 train_time:53846ms step_avg:45.14ms
step:1194/2035 train_time:53906ms step_avg:45.15ms
step:1195/2035 train_time:53966ms step_avg:45.16ms
step:1196/2035 train_time:54025ms step_avg:45.17ms
step:1197/2035 train_time:54085ms step_avg:45.18ms
step:1198/2035 train_time:54144ms step_avg:45.20ms
step:1199/2035 train_time:54205ms step_avg:45.21ms
step:1200/2035 train_time:54265ms step_avg:45.22ms
step:1201/2035 train_time:54325ms step_avg:45.23ms
step:1202/2035 train_time:54385ms step_avg:45.25ms
step:1203/2035 train_time:54445ms step_avg:45.26ms
step:1204/2035 train_time:54505ms step_avg:45.27ms
step:1205/2035 train_time:54566ms step_avg:45.28ms
step:1206/2035 train_time:54626ms step_avg:45.29ms
step:1207/2035 train_time:54686ms step_avg:45.31ms
step:1208/2035 train_time:54746ms step_avg:45.32ms
step:1209/2035 train_time:54807ms step_avg:45.33ms
step:1210/2035 train_time:54866ms step_avg:45.34ms
step:1211/2035 train_time:54926ms step_avg:45.36ms
step:1212/2035 train_time:54986ms step_avg:45.37ms
step:1213/2035 train_time:55046ms step_avg:45.38ms
step:1214/2035 train_time:55106ms step_avg:45.39ms
step:1215/2035 train_time:55166ms step_avg:45.40ms
step:1216/2035 train_time:55225ms step_avg:45.42ms
step:1217/2035 train_time:55286ms step_avg:45.43ms
step:1218/2035 train_time:55345ms step_avg:45.44ms
step:1219/2035 train_time:55406ms step_avg:45.45ms
step:1220/2035 train_time:55466ms step_avg:45.46ms
step:1221/2035 train_time:55526ms step_avg:45.48ms
step:1222/2035 train_time:55586ms step_avg:45.49ms
step:1223/2035 train_time:55646ms step_avg:45.50ms
step:1224/2035 train_time:55706ms step_avg:45.51ms
step:1225/2035 train_time:55766ms step_avg:45.52ms
step:1226/2035 train_time:55826ms step_avg:45.53ms
step:1227/2035 train_time:55886ms step_avg:45.55ms
step:1228/2035 train_time:55946ms step_avg:45.56ms
step:1229/2035 train_time:56006ms step_avg:45.57ms
step:1230/2035 train_time:56066ms step_avg:45.58ms
step:1231/2035 train_time:56126ms step_avg:45.59ms
step:1232/2035 train_time:56185ms step_avg:45.60ms
step:1233/2035 train_time:56246ms step_avg:45.62ms
step:1234/2035 train_time:56306ms step_avg:45.63ms
step:1235/2035 train_time:56367ms step_avg:45.64ms
step:1236/2035 train_time:56427ms step_avg:45.65ms
step:1237/2035 train_time:56487ms step_avg:45.66ms
step:1238/2035 train_time:56546ms step_avg:45.68ms
step:1239/2035 train_time:56607ms step_avg:45.69ms
step:1240/2035 train_time:56666ms step_avg:45.70ms
step:1241/2035 train_time:56726ms step_avg:45.71ms
step:1242/2035 train_time:56786ms step_avg:45.72ms
step:1243/2035 train_time:56847ms step_avg:45.73ms
step:1244/2035 train_time:56906ms step_avg:45.74ms
step:1245/2035 train_time:56966ms step_avg:45.76ms
step:1246/2035 train_time:57025ms step_avg:45.77ms
step:1247/2035 train_time:57085ms step_avg:45.78ms
step:1248/2035 train_time:57145ms step_avg:45.79ms
step:1249/2035 train_time:57206ms step_avg:45.80ms
step:1250/2035 train_time:57265ms step_avg:45.81ms
step:1250/2035 val_loss:3.5705 train_time:57327ms step_avg:45.86ms
step:1251/2035 train_time:57347ms step_avg:45.84ms
step:1252/2035 train_time:57387ms step_avg:45.84ms
step:1253/2035 train_time:57451ms step_avg:45.85ms
step:1254/2035 train_time:57514ms step_avg:45.86ms
step:1255/2035 train_time:57575ms step_avg:45.88ms
step:1256/2035 train_time:57634ms step_avg:45.89ms
step:1257/2035 train_time:57694ms step_avg:45.90ms
step:1258/2035 train_time:57753ms step_avg:45.91ms
step:1259/2035 train_time:57813ms step_avg:45.92ms
step:1260/2035 train_time:57872ms step_avg:45.93ms
step:1261/2035 train_time:57932ms step_avg:45.94ms
step:1262/2035 train_time:57993ms step_avg:45.95ms
step:1263/2035 train_time:58054ms step_avg:45.97ms
step:1264/2035 train_time:58114ms step_avg:45.98ms
step:1265/2035 train_time:58174ms step_avg:45.99ms
step:1266/2035 train_time:58234ms step_avg:46.00ms
step:1267/2035 train_time:58295ms step_avg:46.01ms
step:1268/2035 train_time:58357ms step_avg:46.02ms
step:1269/2035 train_time:58417ms step_avg:46.03ms
step:1270/2035 train_time:58477ms step_avg:46.05ms
step:1271/2035 train_time:58538ms step_avg:46.06ms
step:1272/2035 train_time:58598ms step_avg:46.07ms
step:1273/2035 train_time:58658ms step_avg:46.08ms
step:1274/2035 train_time:58718ms step_avg:46.09ms
step:1275/2035 train_time:58778ms step_avg:46.10ms
step:1276/2035 train_time:58837ms step_avg:46.11ms
step:1277/2035 train_time:58897ms step_avg:46.12ms
step:1278/2035 train_time:58956ms step_avg:46.13ms
step:1279/2035 train_time:59016ms step_avg:46.14ms
step:1280/2035 train_time:59075ms step_avg:46.15ms
step:1281/2035 train_time:59135ms step_avg:46.16ms
step:1282/2035 train_time:59195ms step_avg:46.17ms
step:1283/2035 train_time:59257ms step_avg:46.19ms
step:1284/2035 train_time:59317ms step_avg:46.20ms
step:1285/2035 train_time:59377ms step_avg:46.21ms
step:1286/2035 train_time:59437ms step_avg:46.22ms
step:1287/2035 train_time:59499ms step_avg:46.23ms
step:1288/2035 train_time:59559ms step_avg:46.24ms
step:1289/2035 train_time:59619ms step_avg:46.25ms
step:1290/2035 train_time:59678ms step_avg:46.26ms
step:1291/2035 train_time:59738ms step_avg:46.27ms
step:1292/2035 train_time:59798ms step_avg:46.28ms
step:1293/2035 train_time:59858ms step_avg:46.29ms
step:1294/2035 train_time:59917ms step_avg:46.30ms
step:1295/2035 train_time:59977ms step_avg:46.31ms
step:1296/2035 train_time:60036ms step_avg:46.32ms
step:1297/2035 train_time:60097ms step_avg:46.34ms
step:1298/2035 train_time:60156ms step_avg:46.35ms
step:1299/2035 train_time:60217ms step_avg:46.36ms
step:1300/2035 train_time:60277ms step_avg:46.37ms
step:1301/2035 train_time:60337ms step_avg:46.38ms
step:1302/2035 train_time:60397ms step_avg:46.39ms
step:1303/2035 train_time:60459ms step_avg:46.40ms
step:1304/2035 train_time:60519ms step_avg:46.41ms
step:1305/2035 train_time:60580ms step_avg:46.42ms
step:1306/2035 train_time:60639ms step_avg:46.43ms
step:1307/2035 train_time:60699ms step_avg:46.44ms
step:1308/2035 train_time:60759ms step_avg:46.45ms
step:1309/2035 train_time:60818ms step_avg:46.46ms
step:1310/2035 train_time:60877ms step_avg:46.47ms
step:1311/2035 train_time:60938ms step_avg:46.48ms
step:1312/2035 train_time:60997ms step_avg:46.49ms
step:1313/2035 train_time:61057ms step_avg:46.50ms
step:1314/2035 train_time:61116ms step_avg:46.51ms
step:1315/2035 train_time:61177ms step_avg:46.52ms
step:1316/2035 train_time:61236ms step_avg:46.53ms
step:1317/2035 train_time:61297ms step_avg:46.54ms
step:1318/2035 train_time:61357ms step_avg:46.55ms
step:1319/2035 train_time:61417ms step_avg:46.56ms
step:1320/2035 train_time:61478ms step_avg:46.57ms
step:1321/2035 train_time:61538ms step_avg:46.58ms
step:1322/2035 train_time:61598ms step_avg:46.59ms
step:1323/2035 train_time:61658ms step_avg:46.60ms
step:1324/2035 train_time:61717ms step_avg:46.61ms
step:1325/2035 train_time:61777ms step_avg:46.62ms
step:1326/2035 train_time:61837ms step_avg:46.63ms
step:1327/2035 train_time:61898ms step_avg:46.64ms
step:1328/2035 train_time:61957ms step_avg:46.65ms
step:1329/2035 train_time:62017ms step_avg:46.66ms
step:1330/2035 train_time:62076ms step_avg:46.67ms
step:1331/2035 train_time:62137ms step_avg:46.68ms
step:1332/2035 train_time:62226ms step_avg:46.72ms
step:1333/2035 train_time:62314ms step_avg:46.75ms
step:1334/2035 train_time:62401ms step_avg:46.78ms
step:1335/2035 train_time:62491ms step_avg:46.81ms
step:1336/2035 train_time:62580ms step_avg:46.84ms
step:1337/2035 train_time:62669ms step_avg:46.87ms
step:1338/2035 train_time:62757ms step_avg:46.90ms
step:1339/2035 train_time:62845ms step_avg:46.93ms
step:1340/2035 train_time:62932ms step_avg:46.96ms
step:1341/2035 train_time:63019ms step_avg:46.99ms
step:1342/2035 train_time:63107ms step_avg:47.02ms
step:1343/2035 train_time:63195ms step_avg:47.05ms
step:1344/2035 train_time:63282ms step_avg:47.08ms
step:1345/2035 train_time:63370ms step_avg:47.12ms
step:1346/2035 train_time:63458ms step_avg:47.15ms
step:1347/2035 train_time:63548ms step_avg:47.18ms
step:1348/2035 train_time:63635ms step_avg:47.21ms
step:1349/2035 train_time:63725ms step_avg:47.24ms
step:1350/2035 train_time:63812ms step_avg:47.27ms
step:1351/2035 train_time:63900ms step_avg:47.30ms
step:1352/2035 train_time:63989ms step_avg:47.33ms
step:1353/2035 train_time:64076ms step_avg:47.36ms
step:1354/2035 train_time:64163ms step_avg:47.39ms
step:1355/2035 train_time:64251ms step_avg:47.42ms
step:1356/2035 train_time:64340ms step_avg:47.45ms
step:1357/2035 train_time:64429ms step_avg:47.48ms
step:1358/2035 train_time:64517ms step_avg:47.51ms
step:1359/2035 train_time:64605ms step_avg:47.54ms
step:1360/2035 train_time:64693ms step_avg:47.57ms
step:1361/2035 train_time:64782ms step_avg:47.60ms
step:1362/2035 train_time:64869ms step_avg:47.63ms
step:1363/2035 train_time:64957ms step_avg:47.66ms
step:1364/2035 train_time:65045ms step_avg:47.69ms
step:1365/2035 train_time:65133ms step_avg:47.72ms
step:1366/2035 train_time:65220ms step_avg:47.75ms
step:1367/2035 train_time:65309ms step_avg:47.78ms
step:1368/2035 train_time:65397ms step_avg:47.80ms
step:1369/2035 train_time:65487ms step_avg:47.84ms
step:1370/2035 train_time:65574ms step_avg:47.86ms
step:1371/2035 train_time:65662ms step_avg:47.89ms
step:1372/2035 train_time:65749ms step_avg:47.92ms
step:1373/2035 train_time:65839ms step_avg:47.95ms
step:1374/2035 train_time:65927ms step_avg:47.98ms
step:1375/2035 train_time:66014ms step_avg:48.01ms
step:1376/2035 train_time:66102ms step_avg:48.04ms
step:1377/2035 train_time:66190ms step_avg:48.07ms
step:1378/2035 train_time:66278ms step_avg:48.10ms
step:1379/2035 train_time:66366ms step_avg:48.13ms
step:1380/2035 train_time:66453ms step_avg:48.15ms
step:1381/2035 train_time:66541ms step_avg:48.18ms
step:1382/2035 train_time:66630ms step_avg:48.21ms
step:1383/2035 train_time:66717ms step_avg:48.24ms
step:1384/2035 train_time:66806ms step_avg:48.27ms
step:1385/2035 train_time:66894ms step_avg:48.30ms
step:1386/2035 train_time:66981ms step_avg:48.33ms
step:1387/2035 train_time:67069ms step_avg:48.36ms
step:1388/2035 train_time:67157ms step_avg:48.38ms
step:1389/2035 train_time:67246ms step_avg:48.41ms
step:1390/2035 train_time:67333ms step_avg:48.44ms
step:1391/2035 train_time:67421ms step_avg:48.47ms
step:1392/2035 train_time:67510ms step_avg:48.50ms
step:1393/2035 train_time:67598ms step_avg:48.53ms
step:1394/2035 train_time:67686ms step_avg:48.56ms
step:1395/2035 train_time:67773ms step_avg:48.58ms
step:1396/2035 train_time:67861ms step_avg:48.61ms
step:1397/2035 train_time:67949ms step_avg:48.64ms
step:1398/2035 train_time:68036ms step_avg:48.67ms
step:1399/2035 train_time:68125ms step_avg:48.70ms
step:1400/2035 train_time:68212ms step_avg:48.72ms
step:1401/2035 train_time:68300ms step_avg:48.75ms
step:1402/2035 train_time:68389ms step_avg:48.78ms
step:1403/2035 train_time:68478ms step_avg:48.81ms
step:1404/2035 train_time:68565ms step_avg:48.84ms
step:1405/2035 train_time:68653ms step_avg:48.86ms
step:1406/2035 train_time:68740ms step_avg:48.89ms
step:1407/2035 train_time:68829ms step_avg:48.92ms
step:1408/2035 train_time:68917ms step_avg:48.95ms
step:1409/2035 train_time:69004ms step_avg:48.97ms
step:1410/2035 train_time:69092ms step_avg:49.00ms
step:1411/2035 train_time:69180ms step_avg:49.03ms
step:1412/2035 train_time:69267ms step_avg:49.06ms
step:1413/2035 train_time:69355ms step_avg:49.08ms
step:1414/2035 train_time:69445ms step_avg:49.11ms
step:1415/2035 train_time:69532ms step_avg:49.14ms
step:1416/2035 train_time:69620ms step_avg:49.17ms
step:1417/2035 train_time:69708ms step_avg:49.19ms
step:1418/2035 train_time:69796ms step_avg:49.22ms
step:1419/2035 train_time:69884ms step_avg:49.25ms
step:1420/2035 train_time:69971ms step_avg:49.28ms
step:1421/2035 train_time:70059ms step_avg:49.30ms
step:1422/2035 train_time:70147ms step_avg:49.33ms
step:1423/2035 train_time:70235ms step_avg:49.36ms
step:1424/2035 train_time:70323ms step_avg:49.38ms
step:1425/2035 train_time:70411ms step_avg:49.41ms
step:1426/2035 train_time:70498ms step_avg:49.44ms
step:1427/2035 train_time:70587ms step_avg:49.47ms
step:1428/2035 train_time:70674ms step_avg:49.49ms
step:1429/2035 train_time:70762ms step_avg:49.52ms
step:1430/2035 train_time:70850ms step_avg:49.55ms
step:1431/2035 train_time:70937ms step_avg:49.57ms
step:1432/2035 train_time:71026ms step_avg:49.60ms
step:1433/2035 train_time:71113ms step_avg:49.63ms
step:1434/2035 train_time:71201ms step_avg:49.65ms
step:1435/2035 train_time:71291ms step_avg:49.68ms
step:1436/2035 train_time:71379ms step_avg:49.71ms
step:1437/2035 train_time:71467ms step_avg:49.73ms
step:1438/2035 train_time:71555ms step_avg:49.76ms
step:1439/2035 train_time:71643ms step_avg:49.79ms
step:1440/2035 train_time:71731ms step_avg:49.81ms
step:1441/2035 train_time:71819ms step_avg:49.84ms
step:1442/2035 train_time:71908ms step_avg:49.87ms
step:1443/2035 train_time:71995ms step_avg:49.89ms
step:1444/2035 train_time:72083ms step_avg:49.92ms
step:1445/2035 train_time:72171ms step_avg:49.95ms
step:1446/2035 train_time:72259ms step_avg:49.97ms
step:1447/2035 train_time:72348ms step_avg:50.00ms
step:1448/2035 train_time:72435ms step_avg:50.02ms
step:1449/2035 train_time:72523ms step_avg:50.05ms
step:1450/2035 train_time:72611ms step_avg:50.08ms
step:1451/2035 train_time:72700ms step_avg:50.10ms
step:1452/2035 train_time:72789ms step_avg:50.13ms
step:1453/2035 train_time:72877ms step_avg:50.16ms
step:1454/2035 train_time:72965ms step_avg:50.18ms
step:1455/2035 train_time:73052ms step_avg:50.21ms
step:1456/2035 train_time:73140ms step_avg:50.23ms
step:1457/2035 train_time:73228ms step_avg:50.26ms
step:1458/2035 train_time:73316ms step_avg:50.29ms
step:1459/2035 train_time:73405ms step_avg:50.31ms
step:1460/2035 train_time:73492ms step_avg:50.34ms
step:1461/2035 train_time:73581ms step_avg:50.36ms
step:1462/2035 train_time:73667ms step_avg:50.39ms
step:1463/2035 train_time:73756ms step_avg:50.41ms
step:1464/2035 train_time:73844ms step_avg:50.44ms
step:1465/2035 train_time:73932ms step_avg:50.47ms
step:1466/2035 train_time:74020ms step_avg:50.49ms
step:1467/2035 train_time:74108ms step_avg:50.52ms
step:1468/2035 train_time:74196ms step_avg:50.54ms
step:1469/2035 train_time:74284ms step_avg:50.57ms
step:1470/2035 train_time:74371ms step_avg:50.59ms
step:1471/2035 train_time:74460ms step_avg:50.62ms
step:1472/2035 train_time:74548ms step_avg:50.64ms
step:1473/2035 train_time:74636ms step_avg:50.67ms
step:1474/2035 train_time:74725ms step_avg:50.70ms
step:1475/2035 train_time:74813ms step_avg:50.72ms
step:1476/2035 train_time:74901ms step_avg:50.75ms
step:1477/2035 train_time:74989ms step_avg:50.77ms
step:1478/2035 train_time:75077ms step_avg:50.80ms
step:1479/2035 train_time:75166ms step_avg:50.82ms
step:1480/2035 train_time:75253ms step_avg:50.85ms
step:1481/2035 train_time:75342ms step_avg:50.87ms
step:1482/2035 train_time:75430ms step_avg:50.90ms
step:1483/2035 train_time:75518ms step_avg:50.92ms
step:1484/2035 train_time:75606ms step_avg:50.95ms
step:1485/2035 train_time:75694ms step_avg:50.97ms
step:1486/2035 train_time:75782ms step_avg:51.00ms
step:1487/2035 train_time:75871ms step_avg:51.02ms
step:1488/2035 train_time:75958ms step_avg:51.05ms
step:1489/2035 train_time:76047ms step_avg:51.07ms
step:1490/2035 train_time:76134ms step_avg:51.10ms
step:1491/2035 train_time:76222ms step_avg:51.12ms
step:1492/2035 train_time:76310ms step_avg:51.15ms
step:1493/2035 train_time:76398ms step_avg:51.17ms
step:1494/2035 train_time:76486ms step_avg:51.20ms
step:1495/2035 train_time:76574ms step_avg:51.22ms
step:1496/2035 train_time:76662ms step_avg:51.24ms
step:1497/2035 train_time:76750ms step_avg:51.27ms
step:1498/2035 train_time:76838ms step_avg:51.29ms
step:1499/2035 train_time:76928ms step_avg:51.32ms
step:1500/2035 train_time:77014ms step_avg:51.34ms
step:1500/2035 val_loss:3.4555 train_time:77105ms step_avg:51.40ms
step:1501/2035 train_time:77125ms step_avg:51.38ms
step:1502/2035 train_time:77195ms step_avg:51.39ms
step:1503/2035 train_time:77289ms step_avg:51.42ms
step:1504/2035 train_time:77377ms step_avg:51.45ms
step:1505/2035 train_time:77466ms step_avg:51.47ms
step:1506/2035 train_time:77552ms step_avg:51.50ms
step:1507/2035 train_time:77639ms step_avg:51.52ms
step:1508/2035 train_time:77726ms step_avg:51.54ms
step:1509/2035 train_time:77813ms step_avg:51.57ms
step:1510/2035 train_time:77900ms step_avg:51.59ms
step:1511/2035 train_time:77989ms step_avg:51.61ms
step:1512/2035 train_time:78077ms step_avg:51.64ms
step:1513/2035 train_time:78168ms step_avg:51.66ms
step:1514/2035 train_time:78257ms step_avg:51.69ms
step:1515/2035 train_time:78348ms step_avg:51.71ms
step:1516/2035 train_time:78436ms step_avg:51.74ms
step:1517/2035 train_time:78525ms step_avg:51.76ms
step:1518/2035 train_time:78611ms step_avg:51.79ms
step:1519/2035 train_time:78699ms step_avg:51.81ms
step:1520/2035 train_time:78785ms step_avg:51.83ms
step:1521/2035 train_time:78873ms step_avg:51.86ms
step:1522/2035 train_time:78961ms step_avg:51.88ms
step:1523/2035 train_time:79049ms step_avg:51.90ms
step:1524/2035 train_time:79138ms step_avg:51.93ms
step:1525/2035 train_time:79229ms step_avg:51.95ms
step:1526/2035 train_time:79318ms step_avg:51.98ms
step:1527/2035 train_time:79407ms step_avg:52.00ms
step:1528/2035 train_time:79495ms step_avg:52.03ms
step:1529/2035 train_time:79583ms step_avg:52.05ms
step:1530/2035 train_time:79671ms step_avg:52.07ms
step:1531/2035 train_time:79758ms step_avg:52.10ms
step:1532/2035 train_time:79845ms step_avg:52.12ms
step:1533/2035 train_time:79933ms step_avg:52.14ms
step:1534/2035 train_time:80022ms step_avg:52.17ms
step:1535/2035 train_time:80112ms step_avg:52.19ms
step:1536/2035 train_time:80201ms step_avg:52.21ms
step:1537/2035 train_time:80291ms step_avg:52.24ms
step:1538/2035 train_time:80379ms step_avg:52.26ms
step:1539/2035 train_time:80467ms step_avg:52.29ms
step:1540/2035 train_time:80555ms step_avg:52.31ms
step:1541/2035 train_time:80643ms step_avg:52.33ms
step:1542/2035 train_time:80730ms step_avg:52.35ms
step:1543/2035 train_time:80818ms step_avg:52.38ms
step:1544/2035 train_time:80905ms step_avg:52.40ms
step:1545/2035 train_time:80994ms step_avg:52.42ms
step:1546/2035 train_time:81081ms step_avg:52.45ms
step:1547/2035 train_time:81172ms step_avg:52.47ms
step:1548/2035 train_time:81260ms step_avg:52.49ms
step:1549/2035 train_time:81350ms step_avg:52.52ms
step:1550/2035 train_time:81438ms step_avg:52.54ms
step:1551/2035 train_time:81526ms step_avg:52.56ms
step:1552/2035 train_time:81614ms step_avg:52.59ms
step:1553/2035 train_time:81701ms step_avg:52.61ms
step:1554/2035 train_time:81788ms step_avg:52.63ms
step:1555/2035 train_time:81877ms step_avg:52.65ms
step:1556/2035 train_time:81965ms step_avg:52.68ms
step:1557/2035 train_time:82054ms step_avg:52.70ms
step:1558/2035 train_time:82143ms step_avg:52.72ms
step:1559/2035 train_time:82232ms step_avg:52.75ms
step:1560/2035 train_time:82321ms step_avg:52.77ms
step:1561/2035 train_time:82410ms step_avg:52.79ms
step:1562/2035 train_time:82497ms step_avg:52.81ms
step:1563/2035 train_time:82584ms step_avg:52.84ms
step:1564/2035 train_time:82672ms step_avg:52.86ms
step:1565/2035 train_time:82760ms step_avg:52.88ms
step:1566/2035 train_time:82848ms step_avg:52.90ms
step:1567/2035 train_time:82936ms step_avg:52.93ms
step:1568/2035 train_time:83023ms step_avg:52.95ms
step:1569/2035 train_time:83112ms step_avg:52.97ms
step:1570/2035 train_time:83200ms step_avg:52.99ms
step:1571/2035 train_time:83290ms step_avg:53.02ms
step:1572/2035 train_time:83377ms step_avg:53.04ms
step:1573/2035 train_time:83466ms step_avg:53.06ms
step:1574/2035 train_time:83553ms step_avg:53.08ms
step:1575/2035 train_time:83641ms step_avg:53.11ms
step:1576/2035 train_time:83729ms step_avg:53.13ms
step:1577/2035 train_time:83817ms step_avg:53.15ms
step:1578/2035 train_time:83904ms step_avg:53.17ms
step:1579/2035 train_time:83993ms step_avg:53.19ms
step:1580/2035 train_time:84081ms step_avg:53.22ms
step:1581/2035 train_time:84170ms step_avg:53.24ms
step:1582/2035 train_time:84258ms step_avg:53.26ms
step:1583/2035 train_time:84346ms step_avg:53.28ms
step:1584/2035 train_time:84434ms step_avg:53.30ms
step:1585/2035 train_time:84522ms step_avg:53.33ms
step:1586/2035 train_time:84611ms step_avg:53.35ms
step:1587/2035 train_time:84699ms step_avg:53.37ms
step:1588/2035 train_time:84787ms step_avg:53.39ms
step:1589/2035 train_time:84874ms step_avg:53.41ms
step:1590/2035 train_time:84961ms step_avg:53.43ms
step:1591/2035 train_time:85051ms step_avg:53.46ms
step:1592/2035 train_time:85138ms step_avg:53.48ms
step:1593/2035 train_time:85226ms step_avg:53.50ms
step:1594/2035 train_time:85313ms step_avg:53.52ms
step:1595/2035 train_time:85402ms step_avg:53.54ms
step:1596/2035 train_time:85490ms step_avg:53.57ms
step:1597/2035 train_time:85578ms step_avg:53.59ms
step:1598/2035 train_time:85666ms step_avg:53.61ms
step:1599/2035 train_time:85754ms step_avg:53.63ms
step:1600/2035 train_time:85842ms step_avg:53.65ms
step:1601/2035 train_time:85930ms step_avg:53.67ms
step:1602/2035 train_time:86017ms step_avg:53.69ms
step:1603/2035 train_time:86105ms step_avg:53.72ms
step:1604/2035 train_time:86193ms step_avg:53.74ms
step:1605/2035 train_time:86281ms step_avg:53.76ms
step:1606/2035 train_time:86371ms step_avg:53.78ms
step:1607/2035 train_time:86459ms step_avg:53.80ms
step:1608/2035 train_time:86547ms step_avg:53.82ms
step:1609/2035 train_time:86636ms step_avg:53.84ms
step:1610/2035 train_time:86723ms step_avg:53.87ms
step:1611/2035 train_time:86812ms step_avg:53.89ms
step:1612/2035 train_time:86900ms step_avg:53.91ms
step:1613/2035 train_time:86989ms step_avg:53.93ms
step:1614/2035 train_time:87075ms step_avg:53.95ms
step:1615/2035 train_time:87164ms step_avg:53.97ms
step:1616/2035 train_time:87252ms step_avg:53.99ms
step:1617/2035 train_time:87340ms step_avg:54.01ms
step:1618/2035 train_time:87429ms step_avg:54.04ms
step:1619/2035 train_time:87518ms step_avg:54.06ms
step:1620/2035 train_time:87606ms step_avg:54.08ms
step:1621/2035 train_time:87694ms step_avg:54.10ms
step:1622/2035 train_time:87782ms step_avg:54.12ms
step:1623/2035 train_time:87870ms step_avg:54.14ms
step:1624/2035 train_time:87957ms step_avg:54.16ms
step:1625/2035 train_time:88046ms step_avg:54.18ms
step:1626/2035 train_time:88134ms step_avg:54.20ms
step:1627/2035 train_time:88222ms step_avg:54.22ms
step:1628/2035 train_time:88309ms step_avg:54.24ms
step:1629/2035 train_time:88398ms step_avg:54.27ms
step:1630/2035 train_time:88486ms step_avg:54.29ms
step:1631/2035 train_time:88575ms step_avg:54.31ms
step:1632/2035 train_time:88664ms step_avg:54.33ms
step:1633/2035 train_time:88752ms step_avg:54.35ms
step:1634/2035 train_time:88840ms step_avg:54.37ms
step:1635/2035 train_time:88928ms step_avg:54.39ms
step:1636/2035 train_time:89015ms step_avg:54.41ms
step:1637/2035 train_time:89104ms step_avg:54.43ms
step:1638/2035 train_time:89191ms step_avg:54.45ms
step:1639/2035 train_time:89280ms step_avg:54.47ms
step:1640/2035 train_time:89368ms step_avg:54.49ms
step:1641/2035 train_time:89456ms step_avg:54.51ms
step:1642/2035 train_time:89543ms step_avg:54.53ms
step:1643/2035 train_time:89633ms step_avg:54.55ms
step:1644/2035 train_time:89722ms step_avg:54.58ms
step:1645/2035 train_time:89810ms step_avg:54.60ms
step:1646/2035 train_time:89897ms step_avg:54.62ms
step:1647/2035 train_time:89985ms step_avg:54.64ms
step:1648/2035 train_time:90073ms step_avg:54.66ms
step:1649/2035 train_time:90162ms step_avg:54.68ms
step:1650/2035 train_time:90250ms step_avg:54.70ms
step:1651/2035 train_time:90338ms step_avg:54.72ms
step:1652/2035 train_time:90427ms step_avg:54.74ms
step:1653/2035 train_time:90514ms step_avg:54.76ms
step:1654/2035 train_time:90603ms step_avg:54.78ms
step:1655/2035 train_time:90692ms step_avg:54.80ms
step:1656/2035 train_time:90780ms step_avg:54.82ms
step:1657/2035 train_time:90869ms step_avg:54.84ms
step:1658/2035 train_time:90956ms step_avg:54.86ms
step:1659/2035 train_time:91045ms step_avg:54.88ms
step:1660/2035 train_time:91133ms step_avg:54.90ms
step:1661/2035 train_time:91221ms step_avg:54.92ms
step:1662/2035 train_time:91308ms step_avg:54.94ms
step:1663/2035 train_time:91396ms step_avg:54.96ms
step:1664/2035 train_time:91484ms step_avg:54.98ms
step:1665/2035 train_time:91573ms step_avg:55.00ms
step:1666/2035 train_time:91662ms step_avg:55.02ms
step:1667/2035 train_time:91751ms step_avg:55.04ms
step:1668/2035 train_time:91837ms step_avg:55.06ms
step:1669/2035 train_time:91926ms step_avg:55.08ms
step:1670/2035 train_time:92014ms step_avg:55.10ms
step:1671/2035 train_time:92103ms step_avg:55.12ms
step:1672/2035 train_time:92191ms step_avg:55.14ms
step:1673/2035 train_time:92278ms step_avg:55.16ms
step:1674/2035 train_time:92366ms step_avg:55.18ms
step:1675/2035 train_time:92454ms step_avg:55.20ms
step:1676/2035 train_time:92543ms step_avg:55.22ms
step:1677/2035 train_time:92632ms step_avg:55.24ms
step:1678/2035 train_time:92720ms step_avg:55.26ms
step:1679/2035 train_time:92808ms step_avg:55.28ms
step:1680/2035 train_time:92896ms step_avg:55.30ms
step:1681/2035 train_time:92985ms step_avg:55.32ms
step:1682/2035 train_time:93072ms step_avg:55.33ms
step:1683/2035 train_time:93161ms step_avg:55.35ms
step:1684/2035 train_time:93248ms step_avg:55.37ms
step:1685/2035 train_time:93337ms step_avg:55.39ms
step:1686/2035 train_time:93424ms step_avg:55.41ms
step:1687/2035 train_time:93513ms step_avg:55.43ms
step:1688/2035 train_time:93601ms step_avg:55.45ms
step:1689/2035 train_time:93691ms step_avg:55.47ms
step:1690/2035 train_time:93778ms step_avg:55.49ms
step:1691/2035 train_time:93866ms step_avg:55.51ms
step:1692/2035 train_time:93954ms step_avg:55.53ms
step:1693/2035 train_time:94043ms step_avg:55.55ms
step:1694/2035 train_time:94131ms step_avg:55.57ms
step:1695/2035 train_time:94219ms step_avg:55.59ms
step:1696/2035 train_time:94306ms step_avg:55.61ms
step:1697/2035 train_time:94394ms step_avg:55.62ms
step:1698/2035 train_time:94481ms step_avg:55.64ms
step:1699/2035 train_time:94571ms step_avg:55.66ms
step:1700/2035 train_time:94659ms step_avg:55.68ms
step:1701/2035 train_time:94747ms step_avg:55.70ms
step:1702/2035 train_time:94835ms step_avg:55.72ms
step:1703/2035 train_time:94922ms step_avg:55.74ms
step:1704/2035 train_time:95010ms step_avg:55.76ms
step:1705/2035 train_time:95099ms step_avg:55.78ms
step:1706/2035 train_time:95186ms step_avg:55.79ms
step:1707/2035 train_time:95275ms step_avg:55.81ms
step:1708/2035 train_time:95363ms step_avg:55.83ms
step:1709/2035 train_time:95453ms step_avg:55.85ms
step:1710/2035 train_time:95541ms step_avg:55.87ms
step:1711/2035 train_time:95630ms step_avg:55.89ms
step:1712/2035 train_time:95717ms step_avg:55.91ms
step:1713/2035 train_time:95806ms step_avg:55.93ms
step:1714/2035 train_time:95894ms step_avg:55.95ms
step:1715/2035 train_time:95983ms step_avg:55.97ms
step:1716/2035 train_time:96072ms step_avg:55.99ms
step:1717/2035 train_time:96161ms step_avg:56.01ms
step:1718/2035 train_time:96249ms step_avg:56.02ms
step:1719/2035 train_time:96337ms step_avg:56.04ms
step:1720/2035 train_time:96426ms step_avg:56.06ms
step:1721/2035 train_time:96514ms step_avg:56.08ms
step:1722/2035 train_time:96602ms step_avg:56.10ms
step:1723/2035 train_time:96691ms step_avg:56.12ms
step:1724/2035 train_time:96778ms step_avg:56.14ms
step:1725/2035 train_time:96867ms step_avg:56.15ms
step:1726/2035 train_time:96955ms step_avg:56.17ms
step:1727/2035 train_time:97044ms step_avg:56.19ms
step:1728/2035 train_time:97132ms step_avg:56.21ms
step:1729/2035 train_time:97221ms step_avg:56.23ms
step:1730/2035 train_time:97309ms step_avg:56.25ms
step:1731/2035 train_time:97396ms step_avg:56.27ms
step:1732/2035 train_time:97484ms step_avg:56.28ms
step:1733/2035 train_time:97573ms step_avg:56.30ms
step:1734/2035 train_time:97660ms step_avg:56.32ms
step:1735/2035 train_time:97750ms step_avg:56.34ms
step:1736/2035 train_time:97837ms step_avg:56.36ms
step:1737/2035 train_time:97925ms step_avg:56.38ms
step:1738/2035 train_time:98013ms step_avg:56.39ms
step:1739/2035 train_time:98102ms step_avg:56.41ms
step:1740/2035 train_time:98190ms step_avg:56.43ms
step:1741/2035 train_time:98278ms step_avg:56.45ms
step:1742/2035 train_time:98365ms step_avg:56.47ms
step:1743/2035 train_time:98454ms step_avg:56.49ms
step:1744/2035 train_time:98543ms step_avg:56.50ms
step:1745/2035 train_time:98632ms step_avg:56.52ms
step:1746/2035 train_time:98720ms step_avg:56.54ms
step:1747/2035 train_time:98808ms step_avg:56.56ms
step:1748/2035 train_time:98895ms step_avg:56.58ms
step:1749/2035 train_time:98984ms step_avg:56.59ms
step:1750/2035 train_time:99072ms step_avg:56.61ms
step:1750/2035 val_loss:3.3581 train_time:99162ms step_avg:56.66ms
step:1751/2035 train_time:99185ms step_avg:56.65ms
step:1752/2035 train_time:99252ms step_avg:56.65ms
step:1753/2035 train_time:99345ms step_avg:56.67ms
step:1754/2035 train_time:99433ms step_avg:56.69ms
step:1755/2035 train_time:99520ms step_avg:56.71ms
step:1756/2035 train_time:99608ms step_avg:56.72ms
step:1757/2035 train_time:99695ms step_avg:56.74ms
step:1758/2035 train_time:99782ms step_avg:56.76ms
step:1759/2035 train_time:99869ms step_avg:56.78ms
step:1760/2035 train_time:99956ms step_avg:56.79ms
step:1761/2035 train_time:100044ms step_avg:56.81ms
step:1762/2035 train_time:100132ms step_avg:56.83ms
step:1763/2035 train_time:100222ms step_avg:56.85ms
step:1764/2035 train_time:100314ms step_avg:56.87ms
step:1765/2035 train_time:100403ms step_avg:56.89ms
step:1766/2035 train_time:100491ms step_avg:56.90ms
step:1767/2035 train_time:100579ms step_avg:56.92ms
step:1768/2035 train_time:100666ms step_avg:56.94ms
step:1769/2035 train_time:100754ms step_avg:56.96ms
step:1770/2035 train_time:100841ms step_avg:56.97ms
step:1771/2035 train_time:100928ms step_avg:56.99ms
step:1772/2035 train_time:101015ms step_avg:57.01ms
step:1773/2035 train_time:101103ms step_avg:57.02ms
step:1774/2035 train_time:101194ms step_avg:57.04ms
step:1775/2035 train_time:101284ms step_avg:57.06ms
step:1776/2035 train_time:101373ms step_avg:57.08ms
step:1777/2035 train_time:101461ms step_avg:57.10ms
step:1778/2035 train_time:101549ms step_avg:57.11ms
step:1779/2035 train_time:101635ms step_avg:57.13ms
step:1780/2035 train_time:101723ms step_avg:57.15ms
step:1781/2035 train_time:101811ms step_avg:57.17ms
step:1782/2035 train_time:101898ms step_avg:57.18ms
step:1783/2035 train_time:101987ms step_avg:57.20ms
step:1784/2035 train_time:102074ms step_avg:57.22ms
step:1785/2035 train_time:102163ms step_avg:57.23ms
step:1786/2035 train_time:102252ms step_avg:57.25ms
step:1787/2035 train_time:102342ms step_avg:57.27ms
step:1788/2035 train_time:102430ms step_avg:57.29ms
step:1789/2035 train_time:102518ms step_avg:57.30ms
step:1790/2035 train_time:102607ms step_avg:57.32ms
step:1791/2035 train_time:102695ms step_avg:57.34ms
step:1792/2035 train_time:102783ms step_avg:57.36ms
step:1793/2035 train_time:102871ms step_avg:57.37ms
step:1794/2035 train_time:102957ms step_avg:57.39ms
step:1795/2035 train_time:103046ms step_avg:57.41ms
step:1796/2035 train_time:103133ms step_avg:57.42ms
step:1797/2035 train_time:103222ms step_avg:57.44ms
step:1798/2035 train_time:103312ms step_avg:57.46ms
step:1799/2035 train_time:103400ms step_avg:57.48ms
step:1800/2035 train_time:103488ms step_avg:57.49ms
step:1801/2035 train_time:103576ms step_avg:57.51ms
step:1802/2035 train_time:103664ms step_avg:57.53ms
step:1803/2035 train_time:103752ms step_avg:57.54ms
step:1804/2035 train_time:103839ms step_avg:57.56ms
step:1805/2035 train_time:103927ms step_avg:57.58ms
step:1806/2035 train_time:104015ms step_avg:57.59ms
step:1807/2035 train_time:104103ms step_avg:57.61ms
step:1808/2035 train_time:104191ms step_avg:57.63ms
step:1809/2035 train_time:104279ms step_avg:57.64ms
step:1810/2035 train_time:104368ms step_avg:57.66ms
step:1811/2035 train_time:104456ms step_avg:57.68ms
step:1812/2035 train_time:104544ms step_avg:57.70ms
step:1813/2035 train_time:104633ms step_avg:57.71ms
step:1814/2035 train_time:104720ms step_avg:57.73ms
step:1815/2035 train_time:104807ms step_avg:57.75ms
step:1816/2035 train_time:104894ms step_avg:57.76ms
step:1817/2035 train_time:104982ms step_avg:57.78ms
step:1818/2035 train_time:105070ms step_avg:57.79ms
step:1819/2035 train_time:105158ms step_avg:57.81ms
step:1820/2035 train_time:105246ms step_avg:57.83ms
step:1821/2035 train_time:105334ms step_avg:57.84ms
step:1822/2035 train_time:105423ms step_avg:57.86ms
step:1823/2035 train_time:105512ms step_avg:57.88ms
step:1824/2035 train_time:105599ms step_avg:57.89ms
step:1825/2035 train_time:105688ms step_avg:57.91ms
step:1826/2035 train_time:105776ms step_avg:57.93ms
step:1827/2035 train_time:105864ms step_avg:57.94ms
step:1828/2035 train_time:105951ms step_avg:57.96ms
step:1829/2035 train_time:106039ms step_avg:57.98ms
step:1830/2035 train_time:106127ms step_avg:57.99ms
step:1831/2035 train_time:106216ms step_avg:58.01ms
step:1832/2035 train_time:106304ms step_avg:58.03ms
step:1833/2035 train_time:106392ms step_avg:58.04ms
step:1834/2035 train_time:106480ms step_avg:58.06ms
step:1835/2035 train_time:106569ms step_avg:58.08ms
step:1836/2035 train_time:106656ms step_avg:58.09ms
step:1837/2035 train_time:106744ms step_avg:58.11ms
step:1838/2035 train_time:106833ms step_avg:58.12ms
step:1839/2035 train_time:106920ms step_avg:58.14ms
step:1840/2035 train_time:107008ms step_avg:58.16ms
step:1841/2035 train_time:107096ms step_avg:58.17ms
step:1842/2035 train_time:107184ms step_avg:58.19ms
step:1843/2035 train_time:107274ms step_avg:58.21ms
step:1844/2035 train_time:107363ms step_avg:58.22ms
step:1845/2035 train_time:107451ms step_avg:58.24ms
step:1846/2035 train_time:107539ms step_avg:58.26ms
step:1847/2035 train_time:107627ms step_avg:58.27ms
step:1848/2035 train_time:107715ms step_avg:58.29ms
step:1849/2035 train_time:107802ms step_avg:58.30ms
step:1850/2035 train_time:107890ms step_avg:58.32ms
step:1851/2035 train_time:107978ms step_avg:58.34ms
step:1852/2035 train_time:108066ms step_avg:58.35ms
step:1853/2035 train_time:108154ms step_avg:58.37ms
step:1854/2035 train_time:108242ms step_avg:58.38ms
step:1855/2035 train_time:108330ms step_avg:58.40ms
step:1856/2035 train_time:108417ms step_avg:58.41ms
step:1857/2035 train_time:108506ms step_avg:58.43ms
step:1858/2035 train_time:108594ms step_avg:58.45ms
step:1859/2035 train_time:108683ms step_avg:58.46ms
step:1860/2035 train_time:108771ms step_avg:58.48ms
step:1861/2035 train_time:108858ms step_avg:58.49ms
step:1862/2035 train_time:108946ms step_avg:58.51ms
step:1863/2035 train_time:109034ms step_avg:58.53ms
step:1864/2035 train_time:109122ms step_avg:58.54ms
step:1865/2035 train_time:109210ms step_avg:58.56ms
step:1866/2035 train_time:109298ms step_avg:58.57ms
step:1867/2035 train_time:109387ms step_avg:58.59ms
step:1868/2035 train_time:109475ms step_avg:58.61ms
step:1869/2035 train_time:109562ms step_avg:58.62ms
step:1870/2035 train_time:109651ms step_avg:58.64ms
step:1871/2035 train_time:109739ms step_avg:58.65ms
step:1872/2035 train_time:109827ms step_avg:58.67ms
step:1873/2035 train_time:109915ms step_avg:58.68ms
step:1874/2035 train_time:110002ms step_avg:58.70ms
step:1875/2035 train_time:110091ms step_avg:58.72ms
step:1876/2035 train_time:110179ms step_avg:58.73ms
step:1877/2035 train_time:110267ms step_avg:58.75ms
step:1878/2035 train_time:110355ms step_avg:58.76ms
step:1879/2035 train_time:110443ms step_avg:58.78ms
step:1880/2035 train_time:110532ms step_avg:58.79ms
step:1881/2035 train_time:110619ms step_avg:58.81ms
step:1882/2035 train_time:110707ms step_avg:58.82ms
step:1883/2035 train_time:110796ms step_avg:58.84ms
step:1884/2035 train_time:110884ms step_avg:58.86ms
step:1885/2035 train_time:110972ms step_avg:58.87ms
step:1886/2035 train_time:111059ms step_avg:58.89ms
step:1887/2035 train_time:111147ms step_avg:58.90ms
step:1888/2035 train_time:111235ms step_avg:58.92ms
step:1889/2035 train_time:111323ms step_avg:58.93ms
step:1890/2035 train_time:111412ms step_avg:58.95ms
step:1891/2035 train_time:111501ms step_avg:58.96ms
step:1892/2035 train_time:111589ms step_avg:58.98ms
step:1893/2035 train_time:111677ms step_avg:58.99ms
step:1894/2035 train_time:111765ms step_avg:59.01ms
step:1895/2035 train_time:111853ms step_avg:59.03ms
step:1896/2035 train_time:111941ms step_avg:59.04ms
step:1897/2035 train_time:112029ms step_avg:59.06ms
step:1898/2035 train_time:112117ms step_avg:59.07ms
step:1899/2035 train_time:112206ms step_avg:59.09ms
step:1900/2035 train_time:112294ms step_avg:59.10ms
step:1901/2035 train_time:112382ms step_avg:59.12ms
step:1902/2035 train_time:112470ms step_avg:59.13ms
step:1903/2035 train_time:112558ms step_avg:59.15ms
step:1904/2035 train_time:112646ms step_avg:59.16ms
step:1905/2035 train_time:112734ms step_avg:59.18ms
step:1906/2035 train_time:112822ms step_avg:59.19ms
step:1907/2035 train_time:112912ms step_avg:59.21ms
step:1908/2035 train_time:112999ms step_avg:59.22ms
step:1909/2035 train_time:113087ms step_avg:59.24ms
step:1910/2035 train_time:113175ms step_avg:59.25ms
step:1911/2035 train_time:113263ms step_avg:59.27ms
step:1912/2035 train_time:113351ms step_avg:59.28ms
step:1913/2035 train_time:113439ms step_avg:59.30ms
step:1914/2035 train_time:113527ms step_avg:59.31ms
step:1915/2035 train_time:113615ms step_avg:59.33ms
step:1916/2035 train_time:113704ms step_avg:59.34ms
step:1917/2035 train_time:113792ms step_avg:59.36ms
step:1918/2035 train_time:113880ms step_avg:59.37ms
step:1919/2035 train_time:113969ms step_avg:59.39ms
step:1920/2035 train_time:114056ms step_avg:59.40ms
step:1921/2035 train_time:114144ms step_avg:59.42ms
step:1922/2035 train_time:114234ms step_avg:59.43ms
step:1923/2035 train_time:114322ms step_avg:59.45ms
step:1924/2035 train_time:114411ms step_avg:59.46ms
step:1925/2035 train_time:114498ms step_avg:59.48ms
step:1926/2035 train_time:114587ms step_avg:59.49ms
step:1927/2035 train_time:114676ms step_avg:59.51ms
step:1928/2035 train_time:114764ms step_avg:59.52ms
step:1929/2035 train_time:114853ms step_avg:59.54ms
step:1930/2035 train_time:114941ms step_avg:59.55ms
step:1931/2035 train_time:115028ms step_avg:59.57ms
step:1932/2035 train_time:115116ms step_avg:59.58ms
step:1933/2035 train_time:115205ms step_avg:59.60ms
step:1934/2035 train_time:115293ms step_avg:59.61ms
step:1935/2035 train_time:115381ms step_avg:59.63ms
step:1936/2035 train_time:115469ms step_avg:59.64ms
step:1937/2035 train_time:115556ms step_avg:59.66ms
step:1938/2035 train_time:115645ms step_avg:59.67ms
step:1939/2035 train_time:115735ms step_avg:59.69ms
step:1940/2035 train_time:115823ms step_avg:59.70ms
step:1941/2035 train_time:115913ms step_avg:59.72ms
step:1942/2035 train_time:116000ms step_avg:59.73ms
step:1943/2035 train_time:116087ms step_avg:59.75ms
step:1944/2035 train_time:116176ms step_avg:59.76ms
step:1945/2035 train_time:116264ms step_avg:59.78ms
step:1946/2035 train_time:116353ms step_avg:59.79ms
step:1947/2035 train_time:116441ms step_avg:59.81ms
step:1948/2035 train_time:116529ms step_avg:59.82ms
step:1949/2035 train_time:116618ms step_avg:59.83ms
step:1950/2035 train_time:116706ms step_avg:59.85ms
step:1951/2035 train_time:116794ms step_avg:59.86ms
step:1952/2035 train_time:116883ms step_avg:59.88ms
step:1953/2035 train_time:116971ms step_avg:59.89ms
step:1954/2035 train_time:117059ms step_avg:59.91ms
step:1955/2035 train_time:117148ms step_avg:59.92ms
step:1956/2035 train_time:117235ms step_avg:59.94ms
step:1957/2035 train_time:117323ms step_avg:59.95ms
step:1958/2035 train_time:117412ms step_avg:59.97ms
step:1959/2035 train_time:117499ms step_avg:59.98ms
step:1960/2035 train_time:117587ms step_avg:59.99ms
step:1961/2035 train_time:117676ms step_avg:60.01ms
step:1962/2035 train_time:117763ms step_avg:60.02ms
step:1963/2035 train_time:117851ms step_avg:60.04ms
step:1964/2035 train_time:117939ms step_avg:60.05ms
step:1965/2035 train_time:118027ms step_avg:60.06ms
step:1966/2035 train_time:118116ms step_avg:60.08ms
step:1967/2035 train_time:118204ms step_avg:60.09ms
step:1968/2035 train_time:118293ms step_avg:60.11ms
step:1969/2035 train_time:118381ms step_avg:60.12ms
step:1970/2035 train_time:118471ms step_avg:60.14ms
step:1971/2035 train_time:118558ms step_avg:60.15ms
step:1972/2035 train_time:118647ms step_avg:60.17ms
step:1973/2035 train_time:118735ms step_avg:60.18ms
step:1974/2035 train_time:118822ms step_avg:60.19ms
step:1975/2035 train_time:118911ms step_avg:60.21ms
step:1976/2035 train_time:118998ms step_avg:60.22ms
step:1977/2035 train_time:119087ms step_avg:60.24ms
step:1978/2035 train_time:119174ms step_avg:60.25ms
step:1979/2035 train_time:119262ms step_avg:60.26ms
step:1980/2035 train_time:119350ms step_avg:60.28ms
step:1981/2035 train_time:119438ms step_avg:60.29ms
step:1982/2035 train_time:119525ms step_avg:60.31ms
step:1983/2035 train_time:119614ms step_avg:60.32ms
step:1984/2035 train_time:119702ms step_avg:60.33ms
step:1985/2035 train_time:119789ms step_avg:60.35ms
step:1986/2035 train_time:119877ms step_avg:60.36ms
step:1987/2035 train_time:119966ms step_avg:60.38ms
step:1988/2035 train_time:120054ms step_avg:60.39ms
step:1989/2035 train_time:120142ms step_avg:60.40ms
step:1990/2035 train_time:120230ms step_avg:60.42ms
step:1991/2035 train_time:120317ms step_avg:60.43ms
step:1992/2035 train_time:120405ms step_avg:60.44ms
step:1993/2035 train_time:120494ms step_avg:60.46ms
step:1994/2035 train_time:120582ms step_avg:60.47ms
step:1995/2035 train_time:120671ms step_avg:60.49ms
step:1996/2035 train_time:120758ms step_avg:60.50ms
step:1997/2035 train_time:120846ms step_avg:60.51ms
step:1998/2035 train_time:120934ms step_avg:60.53ms
step:1999/2035 train_time:121022ms step_avg:60.54ms
step:2000/2035 train_time:121110ms step_avg:60.55ms
step:2000/2035 val_loss:3.2855 train_time:121200ms step_avg:60.60ms
step:2001/2035 train_time:121219ms step_avg:60.58ms
step:2002/2035 train_time:121290ms step_avg:60.58ms
step:2003/2035 train_time:121382ms step_avg:60.60ms
step:2004/2035 train_time:121469ms step_avg:60.61ms
step:2005/2035 train_time:121557ms step_avg:60.63ms
step:2006/2035 train_time:121644ms step_avg:60.64ms
step:2007/2035 train_time:121732ms step_avg:60.65ms
step:2008/2035 train_time:121819ms step_avg:60.67ms
step:2009/2035 train_time:121907ms step_avg:60.68ms
step:2010/2035 train_time:121995ms step_avg:60.69ms
step:2011/2035 train_time:122083ms step_avg:60.71ms
step:2012/2035 train_time:122172ms step_avg:60.72ms
step:2013/2035 train_time:122263ms step_avg:60.74ms
step:2014/2035 train_time:122352ms step_avg:60.75ms
step:2015/2035 train_time:122443ms step_avg:60.77ms
step:2016/2035 train_time:122530ms step_avg:60.78ms
step:2017/2035 train_time:122618ms step_avg:60.79ms
step:2018/2035 train_time:122705ms step_avg:60.81ms
step:2019/2035 train_time:122792ms step_avg:60.82ms
step:2020/2035 train_time:122880ms step_avg:60.83ms
step:2021/2035 train_time:122968ms step_avg:60.85ms
step:2022/2035 train_time:123056ms step_avg:60.86ms
step:2023/2035 train_time:123145ms step_avg:60.87ms
step:2024/2035 train_time:123234ms step_avg:60.89ms
step:2025/2035 train_time:123324ms step_avg:60.90ms
step:2026/2035 train_time:123412ms step_avg:60.91ms
step:2027/2035 train_time:123501ms step_avg:60.93ms
step:2028/2035 train_time:123589ms step_avg:60.94ms
step:2029/2035 train_time:123678ms step_avg:60.96ms
step:2030/2035 train_time:123766ms step_avg:60.97ms
step:2031/2035 train_time:123853ms step_avg:60.98ms
step:2032/2035 train_time:123941ms step_avg:60.99ms
step:2033/2035 train_time:124029ms step_avg:61.01ms
step:2034/2035 train_time:124118ms step_avg:61.02ms
step:2035/2035 train_time:124208ms step_avg:61.04ms
step:2035/2035 val_loss:3.2786 train_time:124298ms step_avg:61.08ms
peak memory allocated: 29475 MiB reserved: 35536 MiB

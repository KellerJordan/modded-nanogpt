import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    logs_dir: str = f"logs/12-19-Baseline"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 14:58:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     76066      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A     76067      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     76068      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     76069      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     76070      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     76071      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     76072      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A     76073      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A     76067      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A     76068      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A     76069      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A     76070      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A     76071      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A     76072      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A     76073      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8339 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:98ms step_avg:97.73ms
step:2/2035 train_time:125ms step_avg:62.34ms
step:3/2035 train_time:146ms step_avg:48.61ms
step:4/2035 train_time:168ms step_avg:41.89ms
step:5/2035 train_time:198ms step_avg:39.56ms
step:6/2035 train_time:293ms step_avg:48.89ms
step:7/2035 train_time:311ms step_avg:44.48ms
step:8/2035 train_time:343ms step_avg:42.83ms
step:9/2035 train_time:375ms step_avg:41.71ms
step:10/2035 train_time:409ms step_avg:40.88ms
step:11/2035 train_time:442ms step_avg:40.16ms
step:12/2035 train_time:475ms step_avg:39.58ms
step:13/2035 train_time:508ms step_avg:39.08ms
step:14/2035 train_time:542ms step_avg:38.68ms
step:15/2035 train_time:575ms step_avg:38.31ms
step:16/2035 train_time:608ms step_avg:38.00ms
step:17/2035 train_time:641ms step_avg:37.71ms
step:18/2035 train_time:675ms step_avg:37.49ms
step:19/2035 train_time:708ms step_avg:37.25ms
step:20/2035 train_time:741ms step_avg:37.05ms
step:21/2035 train_time:774ms step_avg:36.86ms
step:22/2035 train_time:807ms step_avg:36.70ms
step:23/2035 train_time:840ms step_avg:36.54ms
step:24/2035 train_time:874ms step_avg:36.40ms
step:25/2035 train_time:907ms step_avg:36.27ms
step:26/2035 train_time:940ms step_avg:36.15ms
step:27/2035 train_time:973ms step_avg:36.04ms
step:28/2035 train_time:1007ms step_avg:35.95ms
step:29/2035 train_time:1040ms step_avg:35.86ms
step:30/2035 train_time:1073ms step_avg:35.77ms
step:31/2035 train_time:1106ms step_avg:35.68ms
step:32/2035 train_time:1139ms step_avg:35.60ms
step:33/2035 train_time:1173ms step_avg:35.53ms
step:34/2035 train_time:1207ms step_avg:35.49ms
step:35/2035 train_time:1241ms step_avg:35.45ms
step:36/2035 train_time:1275ms step_avg:35.41ms
step:37/2035 train_time:1309ms step_avg:35.37ms
step:38/2035 train_time:1342ms step_avg:35.32ms
step:39/2035 train_time:1376ms step_avg:35.28ms
step:40/2035 train_time:1409ms step_avg:35.23ms
step:41/2035 train_time:1443ms step_avg:35.18ms
step:42/2035 train_time:1476ms step_avg:35.14ms
step:43/2035 train_time:1509ms step_avg:35.10ms
step:44/2035 train_time:1543ms step_avg:35.06ms
step:45/2035 train_time:1576ms step_avg:35.02ms
step:46/2035 train_time:1609ms step_avg:34.99ms
step:47/2035 train_time:1643ms step_avg:34.95ms
step:48/2035 train_time:1676ms step_avg:34.93ms
step:49/2035 train_time:1709ms step_avg:34.88ms
step:50/2035 train_time:1742ms step_avg:34.85ms
step:51/2035 train_time:1775ms step_avg:34.81ms
step:52/2035 train_time:1809ms step_avg:34.79ms
step:53/2035 train_time:1842ms step_avg:34.75ms
step:54/2035 train_time:1875ms step_avg:34.72ms
step:55/2035 train_time:1908ms step_avg:34.70ms
step:56/2035 train_time:1942ms step_avg:34.67ms
step:57/2035 train_time:1975ms step_avg:34.64ms
step:58/2035 train_time:2008ms step_avg:34.62ms
step:59/2035 train_time:2041ms step_avg:34.60ms
step:60/2035 train_time:2074ms step_avg:34.57ms
step:61/2035 train_time:2107ms step_avg:34.55ms
step:62/2035 train_time:2141ms step_avg:34.53ms
step:63/2035 train_time:2174ms step_avg:34.50ms
step:64/2035 train_time:2208ms step_avg:34.50ms
step:65/2035 train_time:2241ms step_avg:34.48ms
step:66/2035 train_time:2275ms step_avg:34.47ms
step:67/2035 train_time:2308ms step_avg:34.45ms
step:68/2035 train_time:2342ms step_avg:34.44ms
step:69/2035 train_time:2375ms step_avg:34.42ms
step:70/2035 train_time:2408ms step_avg:34.41ms
step:71/2035 train_time:2442ms step_avg:34.39ms
step:72/2035 train_time:2475ms step_avg:34.38ms
step:73/2035 train_time:2509ms step_avg:34.37ms
step:74/2035 train_time:2542ms step_avg:34.35ms
step:75/2035 train_time:2575ms step_avg:34.34ms
step:76/2035 train_time:2609ms step_avg:34.33ms
step:77/2035 train_time:2642ms step_avg:34.31ms
step:78/2035 train_time:2675ms step_avg:34.30ms
step:79/2035 train_time:2708ms step_avg:34.28ms
step:80/2035 train_time:2741ms step_avg:34.27ms
step:81/2035 train_time:2775ms step_avg:34.25ms
step:82/2035 train_time:2808ms step_avg:34.24ms
step:83/2035 train_time:2841ms step_avg:34.23ms
step:84/2035 train_time:2875ms step_avg:34.22ms
step:85/2035 train_time:2908ms step_avg:34.21ms
step:86/2035 train_time:2941ms step_avg:34.20ms
step:87/2035 train_time:2974ms step_avg:34.18ms
step:88/2035 train_time:3007ms step_avg:34.17ms
step:89/2035 train_time:3040ms step_avg:34.16ms
step:90/2035 train_time:3073ms step_avg:34.15ms
step:91/2035 train_time:3106ms step_avg:34.14ms
step:92/2035 train_time:3140ms step_avg:34.13ms
step:93/2035 train_time:3173ms step_avg:34.12ms
step:94/2035 train_time:3206ms step_avg:34.11ms
step:95/2035 train_time:3239ms step_avg:34.10ms
step:96/2035 train_time:3273ms step_avg:34.09ms
step:97/2035 train_time:3306ms step_avg:34.08ms
step:98/2035 train_time:3339ms step_avg:34.07ms
step:99/2035 train_time:3372ms step_avg:34.06ms
step:100/2035 train_time:3406ms step_avg:34.06ms
step:101/2035 train_time:3439ms step_avg:34.05ms
step:102/2035 train_time:3473ms step_avg:34.05ms
step:103/2035 train_time:3506ms step_avg:34.04ms
step:104/2035 train_time:3540ms step_avg:34.04ms
step:105/2035 train_time:3573ms step_avg:34.03ms
step:106/2035 train_time:3606ms step_avg:34.02ms
step:107/2035 train_time:3639ms step_avg:34.01ms
step:108/2035 train_time:3673ms step_avg:34.01ms
step:109/2035 train_time:3706ms step_avg:34.00ms
step:110/2035 train_time:3739ms step_avg:33.99ms
step:111/2035 train_time:3772ms step_avg:33.98ms
step:112/2035 train_time:3805ms step_avg:33.98ms
step:113/2035 train_time:3838ms step_avg:33.97ms
step:114/2035 train_time:3872ms step_avg:33.96ms
step:115/2035 train_time:3905ms step_avg:33.96ms
step:116/2035 train_time:3938ms step_avg:33.95ms
step:117/2035 train_time:3972ms step_avg:33.94ms
step:118/2035 train_time:4005ms step_avg:33.94ms
step:119/2035 train_time:4038ms step_avg:33.93ms
step:120/2035 train_time:4071ms step_avg:33.93ms
step:121/2035 train_time:4104ms step_avg:33.92ms
step:122/2035 train_time:4138ms step_avg:33.91ms
step:123/2035 train_time:4171ms step_avg:33.91ms
step:124/2035 train_time:4203ms step_avg:33.90ms
step:125/2035 train_time:4237ms step_avg:33.89ms
step:126/2035 train_time:4270ms step_avg:33.89ms
step:127/2035 train_time:4303ms step_avg:33.88ms
step:128/2035 train_time:4336ms step_avg:33.88ms
step:129/2035 train_time:4370ms step_avg:33.87ms
step:130/2035 train_time:4403ms step_avg:33.87ms
step:131/2035 train_time:4436ms step_avg:33.86ms
step:132/2035 train_time:4469ms step_avg:33.86ms
step:133/2035 train_time:4502ms step_avg:33.85ms
step:134/2035 train_time:4536ms step_avg:33.85ms
step:135/2035 train_time:4569ms step_avg:33.84ms
step:136/2035 train_time:4602ms step_avg:33.84ms
step:137/2035 train_time:4635ms step_avg:33.83ms
step:138/2035 train_time:4668ms step_avg:33.83ms
step:139/2035 train_time:4701ms step_avg:33.82ms
step:140/2035 train_time:4735ms step_avg:33.82ms
step:141/2035 train_time:4768ms step_avg:33.81ms
step:142/2035 train_time:4801ms step_avg:33.81ms
step:143/2035 train_time:4834ms step_avg:33.80ms
step:144/2035 train_time:4867ms step_avg:33.80ms
step:145/2035 train_time:4900ms step_avg:33.80ms
step:146/2035 train_time:4934ms step_avg:33.79ms
step:147/2035 train_time:4967ms step_avg:33.79ms
step:148/2035 train_time:5000ms step_avg:33.79ms
step:149/2035 train_time:5033ms step_avg:33.78ms
step:150/2035 train_time:5067ms step_avg:33.78ms
step:151/2035 train_time:5100ms step_avg:33.77ms
step:152/2035 train_time:5133ms step_avg:33.77ms
step:153/2035 train_time:5166ms step_avg:33.77ms
step:154/2035 train_time:5199ms step_avg:33.76ms
step:155/2035 train_time:5233ms step_avg:33.76ms
step:156/2035 train_time:5266ms step_avg:33.75ms
step:157/2035 train_time:5299ms step_avg:33.75ms
step:158/2035 train_time:5332ms step_avg:33.75ms
step:159/2035 train_time:5365ms step_avg:33.75ms
step:160/2035 train_time:5399ms step_avg:33.74ms
step:161/2035 train_time:5432ms step_avg:33.74ms
step:162/2035 train_time:5465ms step_avg:33.73ms
step:163/2035 train_time:5498ms step_avg:33.73ms
step:164/2035 train_time:5531ms step_avg:33.73ms
step:165/2035 train_time:5565ms step_avg:33.72ms
step:166/2035 train_time:5598ms step_avg:33.72ms
step:167/2035 train_time:5631ms step_avg:33.72ms
step:168/2035 train_time:5664ms step_avg:33.71ms
step:169/2035 train_time:5697ms step_avg:33.71ms
step:170/2035 train_time:5731ms step_avg:33.71ms
step:171/2035 train_time:5764ms step_avg:33.71ms
step:172/2035 train_time:5797ms step_avg:33.70ms
step:173/2035 train_time:5830ms step_avg:33.70ms
step:174/2035 train_time:5863ms step_avg:33.70ms
step:175/2035 train_time:5896ms step_avg:33.69ms
step:176/2035 train_time:5929ms step_avg:33.69ms
step:177/2035 train_time:5962ms step_avg:33.69ms
step:178/2035 train_time:5996ms step_avg:33.68ms
step:179/2035 train_time:6029ms step_avg:33.68ms
step:180/2035 train_time:6062ms step_avg:33.68ms
step:181/2035 train_time:6095ms step_avg:33.67ms
step:182/2035 train_time:6129ms step_avg:33.67ms
step:183/2035 train_time:6162ms step_avg:33.67ms
step:184/2035 train_time:6195ms step_avg:33.67ms
step:185/2035 train_time:6228ms step_avg:33.67ms
step:186/2035 train_time:6261ms step_avg:33.66ms
step:187/2035 train_time:6294ms step_avg:33.66ms
step:188/2035 train_time:6328ms step_avg:33.66ms
step:189/2035 train_time:6361ms step_avg:33.65ms
step:190/2035 train_time:6394ms step_avg:33.65ms
step:191/2035 train_time:6427ms step_avg:33.65ms
step:192/2035 train_time:6460ms step_avg:33.65ms
step:193/2035 train_time:6493ms step_avg:33.64ms
step:194/2035 train_time:6527ms step_avg:33.64ms
step:195/2035 train_time:6560ms step_avg:33.64ms
step:196/2035 train_time:6593ms step_avg:33.64ms
step:197/2035 train_time:6626ms step_avg:33.64ms
step:198/2035 train_time:6660ms step_avg:33.64ms
step:199/2035 train_time:6693ms step_avg:33.63ms
step:200/2035 train_time:6726ms step_avg:33.63ms
step:201/2035 train_time:6759ms step_avg:33.63ms
step:202/2035 train_time:6793ms step_avg:33.63ms
step:203/2035 train_time:6826ms step_avg:33.62ms
step:204/2035 train_time:6859ms step_avg:33.62ms
step:205/2035 train_time:6892ms step_avg:33.62ms
step:206/2035 train_time:6925ms step_avg:33.62ms
step:207/2035 train_time:6958ms step_avg:33.61ms
step:208/2035 train_time:6991ms step_avg:33.61ms
step:209/2035 train_time:7024ms step_avg:33.61ms
step:210/2035 train_time:7057ms step_avg:33.60ms
step:211/2035 train_time:7090ms step_avg:33.60ms
step:212/2035 train_time:7123ms step_avg:33.60ms
step:213/2035 train_time:7156ms step_avg:33.60ms
step:214/2035 train_time:7190ms step_avg:33.60ms
step:215/2035 train_time:7224ms step_avg:33.60ms
step:216/2035 train_time:7257ms step_avg:33.60ms
step:217/2035 train_time:7290ms step_avg:33.59ms
step:218/2035 train_time:7323ms step_avg:33.59ms
step:219/2035 train_time:7357ms step_avg:33.59ms
step:220/2035 train_time:7390ms step_avg:33.59ms
step:221/2035 train_time:7423ms step_avg:33.59ms
step:222/2035 train_time:7457ms step_avg:33.59ms
step:223/2035 train_time:7490ms step_avg:33.59ms
step:224/2035 train_time:7523ms step_avg:33.58ms
step:225/2035 train_time:7555ms step_avg:33.58ms
step:226/2035 train_time:7588ms step_avg:33.58ms
step:227/2035 train_time:7622ms step_avg:33.58ms
step:228/2035 train_time:7655ms step_avg:33.57ms
step:229/2035 train_time:7688ms step_avg:33.57ms
step:230/2035 train_time:7721ms step_avg:33.57ms
step:231/2035 train_time:7754ms step_avg:33.57ms
step:232/2035 train_time:7788ms step_avg:33.57ms
step:233/2035 train_time:7821ms step_avg:33.56ms
step:234/2035 train_time:7854ms step_avg:33.56ms
step:235/2035 train_time:7887ms step_avg:33.56ms
step:236/2035 train_time:7920ms step_avg:33.56ms
step:237/2035 train_time:7954ms step_avg:33.56ms
step:238/2035 train_time:7987ms step_avg:33.56ms
step:239/2035 train_time:8020ms step_avg:33.56ms
step:240/2035 train_time:8053ms step_avg:33.55ms
step:241/2035 train_time:8086ms step_avg:33.55ms
step:242/2035 train_time:8119ms step_avg:33.55ms
step:243/2035 train_time:8153ms step_avg:33.55ms
step:244/2035 train_time:8186ms step_avg:33.55ms
step:245/2035 train_time:8219ms step_avg:33.55ms
step:246/2035 train_time:8252ms step_avg:33.55ms
step:247/2035 train_time:8285ms step_avg:33.54ms
step:248/2035 train_time:8319ms step_avg:33.54ms
step:249/2035 train_time:8352ms step_avg:33.54ms
step:250/2035 train_time:8385ms step_avg:33.54ms
step:250/2035 val_loss:4.2681 train_time:8420ms step_avg:33.68ms
step:251/2035 train_time:8439ms step_avg:33.62ms
step:252/2035 train_time:8458ms step_avg:33.56ms
step:253/2035 train_time:8487ms step_avg:33.55ms
step:254/2035 train_time:8522ms step_avg:33.55ms
step:255/2035 train_time:8556ms step_avg:33.55ms
step:256/2035 train_time:8591ms step_avg:33.56ms
step:257/2035 train_time:8625ms step_avg:33.56ms
step:258/2035 train_time:8658ms step_avg:33.56ms
step:259/2035 train_time:8691ms step_avg:33.56ms
step:260/2035 train_time:8724ms step_avg:33.56ms
step:261/2035 train_time:8757ms step_avg:33.55ms
step:262/2035 train_time:8791ms step_avg:33.55ms
step:263/2035 train_time:8824ms step_avg:33.55ms
step:264/2035 train_time:8857ms step_avg:33.55ms
step:265/2035 train_time:8889ms step_avg:33.55ms
step:266/2035 train_time:8922ms step_avg:33.54ms
step:267/2035 train_time:8955ms step_avg:33.54ms
step:268/2035 train_time:8988ms step_avg:33.54ms
step:269/2035 train_time:9021ms step_avg:33.54ms
step:270/2035 train_time:9054ms step_avg:33.53ms
step:271/2035 train_time:9087ms step_avg:33.53ms
step:272/2035 train_time:9120ms step_avg:33.53ms
step:273/2035 train_time:9153ms step_avg:33.53ms
step:274/2035 train_time:9186ms step_avg:33.52ms
step:275/2035 train_time:9219ms step_avg:33.52ms
step:276/2035 train_time:9252ms step_avg:33.52ms
step:277/2035 train_time:9285ms step_avg:33.52ms
step:278/2035 train_time:9317ms step_avg:33.52ms
step:279/2035 train_time:9351ms step_avg:33.51ms
step:280/2035 train_time:9384ms step_avg:33.51ms
step:281/2035 train_time:9417ms step_avg:33.51ms
step:282/2035 train_time:9451ms step_avg:33.51ms
step:283/2035 train_time:9484ms step_avg:33.51ms
step:284/2035 train_time:9518ms step_avg:33.51ms
step:285/2035 train_time:9551ms step_avg:33.51ms
step:286/2035 train_time:9585ms step_avg:33.51ms
step:287/2035 train_time:9619ms step_avg:33.51ms
step:288/2035 train_time:9652ms step_avg:33.51ms
step:289/2035 train_time:9685ms step_avg:33.51ms
step:290/2035 train_time:9719ms step_avg:33.51ms
step:291/2035 train_time:9752ms step_avg:33.51ms
step:292/2035 train_time:9785ms step_avg:33.51ms
step:293/2035 train_time:9818ms step_avg:33.51ms
step:294/2035 train_time:9851ms step_avg:33.51ms
step:295/2035 train_time:9884ms step_avg:33.51ms
step:296/2035 train_time:9918ms step_avg:33.51ms
step:297/2035 train_time:9951ms step_avg:33.50ms
step:298/2035 train_time:9984ms step_avg:33.50ms
step:299/2035 train_time:10017ms step_avg:33.50ms
step:300/2035 train_time:10050ms step_avg:33.50ms
step:301/2035 train_time:10082ms step_avg:33.50ms
step:302/2035 train_time:10116ms step_avg:33.50ms
step:303/2035 train_time:10148ms step_avg:33.49ms
step:304/2035 train_time:10181ms step_avg:33.49ms
step:305/2035 train_time:10214ms step_avg:33.49ms
step:306/2035 train_time:10247ms step_avg:33.49ms
step:307/2035 train_time:10280ms step_avg:33.49ms
step:308/2035 train_time:10313ms step_avg:33.48ms
step:309/2035 train_time:10346ms step_avg:33.48ms
step:310/2035 train_time:10379ms step_avg:33.48ms
step:311/2035 train_time:10412ms step_avg:33.48ms
step:312/2035 train_time:10445ms step_avg:33.48ms
step:313/2035 train_time:10479ms step_avg:33.48ms
step:314/2035 train_time:10512ms step_avg:33.48ms
step:315/2035 train_time:10545ms step_avg:33.48ms
step:316/2035 train_time:10579ms step_avg:33.48ms
step:317/2035 train_time:10612ms step_avg:33.48ms
step:318/2035 train_time:10646ms step_avg:33.48ms
step:319/2035 train_time:10679ms step_avg:33.48ms
step:320/2035 train_time:10713ms step_avg:33.48ms
step:321/2035 train_time:10746ms step_avg:33.48ms
step:322/2035 train_time:10779ms step_avg:33.48ms
step:323/2035 train_time:10812ms step_avg:33.47ms
step:324/2035 train_time:10845ms step_avg:33.47ms
step:325/2035 train_time:10878ms step_avg:33.47ms
step:326/2035 train_time:10912ms step_avg:33.47ms
step:327/2035 train_time:10945ms step_avg:33.47ms
step:328/2035 train_time:10979ms step_avg:33.47ms
step:329/2035 train_time:11012ms step_avg:33.47ms
step:330/2035 train_time:11045ms step_avg:33.47ms
step:331/2035 train_time:11078ms step_avg:33.47ms
step:332/2035 train_time:11111ms step_avg:33.47ms
step:333/2035 train_time:11144ms step_avg:33.47ms
step:334/2035 train_time:11178ms step_avg:33.47ms
step:335/2035 train_time:11211ms step_avg:33.46ms
step:336/2035 train_time:11244ms step_avg:33.46ms
step:337/2035 train_time:11277ms step_avg:33.46ms
step:338/2035 train_time:11310ms step_avg:33.46ms
step:339/2035 train_time:11342ms step_avg:33.46ms
step:340/2035 train_time:11375ms step_avg:33.46ms
step:341/2035 train_time:11408ms step_avg:33.46ms
step:342/2035 train_time:11441ms step_avg:33.45ms
step:343/2035 train_time:11475ms step_avg:33.45ms
step:344/2035 train_time:11508ms step_avg:33.45ms
step:345/2035 train_time:11541ms step_avg:33.45ms
step:346/2035 train_time:11574ms step_avg:33.45ms
step:347/2035 train_time:11607ms step_avg:33.45ms
step:348/2035 train_time:11641ms step_avg:33.45ms
step:349/2035 train_time:11674ms step_avg:33.45ms
step:350/2035 train_time:11707ms step_avg:33.45ms
step:351/2035 train_time:11740ms step_avg:33.45ms
step:352/2035 train_time:11773ms step_avg:33.45ms
step:353/2035 train_time:11806ms step_avg:33.45ms
step:354/2035 train_time:11840ms step_avg:33.45ms
step:355/2035 train_time:11873ms step_avg:33.44ms
step:356/2035 train_time:11906ms step_avg:33.44ms
step:357/2035 train_time:11939ms step_avg:33.44ms
step:358/2035 train_time:11972ms step_avg:33.44ms
step:359/2035 train_time:12005ms step_avg:33.44ms
step:360/2035 train_time:12039ms step_avg:33.44ms
step:361/2035 train_time:12072ms step_avg:33.44ms
step:362/2035 train_time:12105ms step_avg:33.44ms
step:363/2035 train_time:12138ms step_avg:33.44ms
step:364/2035 train_time:12172ms step_avg:33.44ms
step:365/2035 train_time:12205ms step_avg:33.44ms
step:366/2035 train_time:12238ms step_avg:33.44ms
step:367/2035 train_time:12271ms step_avg:33.44ms
step:368/2035 train_time:12304ms step_avg:33.44ms
step:369/2035 train_time:12337ms step_avg:33.43ms
step:370/2035 train_time:12371ms step_avg:33.43ms
step:371/2035 train_time:12404ms step_avg:33.43ms
step:372/2035 train_time:12437ms step_avg:33.43ms
step:373/2035 train_time:12470ms step_avg:33.43ms
step:374/2035 train_time:12503ms step_avg:33.43ms
step:375/2035 train_time:12536ms step_avg:33.43ms
step:376/2035 train_time:12569ms step_avg:33.43ms
step:377/2035 train_time:12602ms step_avg:33.43ms
step:378/2035 train_time:12635ms step_avg:33.43ms
step:379/2035 train_time:12668ms step_avg:33.43ms
step:380/2035 train_time:12701ms step_avg:33.42ms
step:381/2035 train_time:12735ms step_avg:33.42ms
step:382/2035 train_time:12768ms step_avg:33.42ms
step:383/2035 train_time:12801ms step_avg:33.42ms
step:384/2035 train_time:12833ms step_avg:33.42ms
step:385/2035 train_time:12867ms step_avg:33.42ms
step:386/2035 train_time:12900ms step_avg:33.42ms
step:387/2035 train_time:12933ms step_avg:33.42ms
step:388/2035 train_time:12966ms step_avg:33.42ms
step:389/2035 train_time:12999ms step_avg:33.42ms
step:390/2035 train_time:13032ms step_avg:33.41ms
step:391/2035 train_time:13065ms step_avg:33.41ms
step:392/2035 train_time:13098ms step_avg:33.41ms
step:393/2035 train_time:13131ms step_avg:33.41ms
step:394/2035 train_time:13164ms step_avg:33.41ms
step:395/2035 train_time:13198ms step_avg:33.41ms
step:396/2035 train_time:13231ms step_avg:33.41ms
step:397/2035 train_time:13264ms step_avg:33.41ms
step:398/2035 train_time:13297ms step_avg:33.41ms
step:399/2035 train_time:13330ms step_avg:33.41ms
step:400/2035 train_time:13363ms step_avg:33.41ms
step:401/2035 train_time:13396ms step_avg:33.41ms
step:402/2035 train_time:13429ms step_avg:33.41ms
step:403/2035 train_time:13462ms step_avg:33.40ms
step:404/2035 train_time:13495ms step_avg:33.40ms
step:405/2035 train_time:13528ms step_avg:33.40ms
step:406/2035 train_time:13562ms step_avg:33.40ms
step:407/2035 train_time:13595ms step_avg:33.40ms
step:408/2035 train_time:13628ms step_avg:33.40ms
step:409/2035 train_time:13661ms step_avg:33.40ms
step:410/2035 train_time:13695ms step_avg:33.40ms
step:411/2035 train_time:13728ms step_avg:33.40ms
step:412/2035 train_time:13761ms step_avg:33.40ms
step:413/2035 train_time:13794ms step_avg:33.40ms
step:414/2035 train_time:13827ms step_avg:33.40ms
step:415/2035 train_time:13860ms step_avg:33.40ms
step:416/2035 train_time:13894ms step_avg:33.40ms
step:417/2035 train_time:13927ms step_avg:33.40ms
step:418/2035 train_time:13960ms step_avg:33.40ms
step:419/2035 train_time:13993ms step_avg:33.40ms
step:420/2035 train_time:14026ms step_avg:33.40ms
step:421/2035 train_time:14060ms step_avg:33.40ms
step:422/2035 train_time:14093ms step_avg:33.40ms
step:423/2035 train_time:14126ms step_avg:33.39ms
step:424/2035 train_time:14159ms step_avg:33.39ms
step:425/2035 train_time:14192ms step_avg:33.39ms
step:426/2035 train_time:14226ms step_avg:33.39ms
step:427/2035 train_time:14259ms step_avg:33.39ms
step:428/2035 train_time:14292ms step_avg:33.39ms
step:429/2035 train_time:14325ms step_avg:33.39ms
step:430/2035 train_time:14359ms step_avg:33.39ms
step:431/2035 train_time:14392ms step_avg:33.39ms
step:432/2035 train_time:14425ms step_avg:33.39ms
step:433/2035 train_time:14458ms step_avg:33.39ms
step:434/2035 train_time:14492ms step_avg:33.39ms
step:435/2035 train_time:14525ms step_avg:33.39ms
step:436/2035 train_time:14558ms step_avg:33.39ms
step:437/2035 train_time:14591ms step_avg:33.39ms
step:438/2035 train_time:14624ms step_avg:33.39ms
step:439/2035 train_time:14657ms step_avg:33.39ms
step:440/2035 train_time:14690ms step_avg:33.39ms
step:441/2035 train_time:14724ms step_avg:33.39ms
step:442/2035 train_time:14757ms step_avg:33.39ms
step:443/2035 train_time:14790ms step_avg:33.39ms
step:444/2035 train_time:14823ms step_avg:33.39ms
step:445/2035 train_time:14856ms step_avg:33.39ms
step:446/2035 train_time:14890ms step_avg:33.38ms
step:447/2035 train_time:14923ms step_avg:33.39ms
step:448/2035 train_time:14956ms step_avg:33.38ms
step:449/2035 train_time:14989ms step_avg:33.38ms
step:450/2035 train_time:15023ms step_avg:33.38ms
step:451/2035 train_time:15056ms step_avg:33.38ms
step:452/2035 train_time:15089ms step_avg:33.38ms
step:453/2035 train_time:15122ms step_avg:33.38ms
step:454/2035 train_time:15155ms step_avg:33.38ms
step:455/2035 train_time:15188ms step_avg:33.38ms
step:456/2035 train_time:15221ms step_avg:33.38ms
step:457/2035 train_time:15254ms step_avg:33.38ms
step:458/2035 train_time:15287ms step_avg:33.38ms
step:459/2035 train_time:15320ms step_avg:33.38ms
step:460/2035 train_time:15353ms step_avg:33.38ms
step:461/2035 train_time:15386ms step_avg:33.38ms
step:462/2035 train_time:15420ms step_avg:33.38ms
step:463/2035 train_time:15453ms step_avg:33.38ms
step:464/2035 train_time:15486ms step_avg:33.38ms
step:465/2035 train_time:15519ms step_avg:33.37ms
step:466/2035 train_time:15552ms step_avg:33.37ms
step:467/2035 train_time:15586ms step_avg:33.37ms
step:468/2035 train_time:15619ms step_avg:33.37ms
step:469/2035 train_time:15652ms step_avg:33.37ms
step:470/2035 train_time:15686ms step_avg:33.37ms
step:471/2035 train_time:15719ms step_avg:33.37ms
step:472/2035 train_time:15752ms step_avg:33.37ms
step:473/2035 train_time:15785ms step_avg:33.37ms
step:474/2035 train_time:15818ms step_avg:33.37ms
step:475/2035 train_time:15852ms step_avg:33.37ms
step:476/2035 train_time:15885ms step_avg:33.37ms
step:477/2035 train_time:15918ms step_avg:33.37ms
step:478/2035 train_time:15951ms step_avg:33.37ms
step:479/2035 train_time:15984ms step_avg:33.37ms
step:480/2035 train_time:16018ms step_avg:33.37ms
step:481/2035 train_time:16051ms step_avg:33.37ms
step:482/2035 train_time:16084ms step_avg:33.37ms
step:483/2035 train_time:16117ms step_avg:33.37ms
step:484/2035 train_time:16151ms step_avg:33.37ms
step:485/2035 train_time:16184ms step_avg:33.37ms
step:486/2035 train_time:16217ms step_avg:33.37ms
step:487/2035 train_time:16250ms step_avg:33.37ms
step:488/2035 train_time:16283ms step_avg:33.37ms
step:489/2035 train_time:16316ms step_avg:33.37ms
step:490/2035 train_time:16349ms step_avg:33.37ms
step:491/2035 train_time:16383ms step_avg:33.37ms
step:492/2035 train_time:16416ms step_avg:33.37ms
step:493/2035 train_time:16449ms step_avg:33.36ms
step:494/2035 train_time:16482ms step_avg:33.36ms
step:495/2035 train_time:16515ms step_avg:33.36ms
step:496/2035 train_time:16549ms step_avg:33.36ms
step:497/2035 train_time:16582ms step_avg:33.36ms
step:498/2035 train_time:16615ms step_avg:33.36ms
step:499/2035 train_time:16648ms step_avg:33.36ms
step:500/2035 train_time:16681ms step_avg:33.36ms
step:500/2035 val_loss:3.9979 train_time:16717ms step_avg:33.43ms
step:501/2035 train_time:16737ms step_avg:33.41ms
step:502/2035 train_time:16756ms step_avg:33.38ms
step:503/2035 train_time:16785ms step_avg:33.37ms
step:504/2035 train_time:16819ms step_avg:33.37ms
step:505/2035 train_time:16854ms step_avg:33.37ms
step:506/2035 train_time:16888ms step_avg:33.38ms
step:507/2035 train_time:16921ms step_avg:33.38ms
step:508/2035 train_time:16955ms step_avg:33.38ms
step:509/2035 train_time:16988ms step_avg:33.37ms
step:510/2035 train_time:17021ms step_avg:33.37ms
step:511/2035 train_time:17054ms step_avg:33.37ms
step:512/2035 train_time:17087ms step_avg:33.37ms
step:513/2035 train_time:17120ms step_avg:33.37ms
step:514/2035 train_time:17153ms step_avg:33.37ms
step:515/2035 train_time:17186ms step_avg:33.37ms
step:516/2035 train_time:17219ms step_avg:33.37ms
step:517/2035 train_time:17252ms step_avg:33.37ms
step:518/2035 train_time:17285ms step_avg:33.37ms
step:519/2035 train_time:17318ms step_avg:33.37ms
step:520/2035 train_time:17351ms step_avg:33.37ms
step:521/2035 train_time:17384ms step_avg:33.37ms
step:522/2035 train_time:17417ms step_avg:33.37ms
step:523/2035 train_time:17450ms step_avg:33.36ms
step:524/2035 train_time:17483ms step_avg:33.36ms
step:525/2035 train_time:17515ms step_avg:33.36ms
step:526/2035 train_time:17548ms step_avg:33.36ms
step:527/2035 train_time:17581ms step_avg:33.36ms
step:528/2035 train_time:17614ms step_avg:33.36ms
step:529/2035 train_time:17648ms step_avg:33.36ms
step:530/2035 train_time:17681ms step_avg:33.36ms
step:531/2035 train_time:17714ms step_avg:33.36ms
step:532/2035 train_time:17748ms step_avg:33.36ms
step:533/2035 train_time:17781ms step_avg:33.36ms
step:534/2035 train_time:17815ms step_avg:33.36ms
step:535/2035 train_time:17848ms step_avg:33.36ms
step:536/2035 train_time:17881ms step_avg:33.36ms
step:537/2035 train_time:17915ms step_avg:33.36ms
step:538/2035 train_time:17948ms step_avg:33.36ms
step:539/2035 train_time:17982ms step_avg:33.36ms
step:540/2035 train_time:18015ms step_avg:33.36ms
step:541/2035 train_time:18048ms step_avg:33.36ms
step:542/2035 train_time:18081ms step_avg:33.36ms
step:543/2035 train_time:18115ms step_avg:33.36ms
step:544/2035 train_time:18148ms step_avg:33.36ms
step:545/2035 train_time:18181ms step_avg:33.36ms
step:546/2035 train_time:18214ms step_avg:33.36ms
step:547/2035 train_time:18247ms step_avg:33.36ms
step:548/2035 train_time:18280ms step_avg:33.36ms
step:549/2035 train_time:18313ms step_avg:33.36ms
step:550/2035 train_time:18346ms step_avg:33.36ms
step:551/2035 train_time:18379ms step_avg:33.36ms
step:552/2035 train_time:18413ms step_avg:33.36ms
step:553/2035 train_time:18446ms step_avg:33.36ms
step:554/2035 train_time:18479ms step_avg:33.36ms
step:555/2035 train_time:18512ms step_avg:33.35ms
step:556/2035 train_time:18545ms step_avg:33.35ms
step:557/2035 train_time:18578ms step_avg:33.35ms
step:558/2035 train_time:18611ms step_avg:33.35ms
step:559/2035 train_time:18644ms step_avg:33.35ms
step:560/2035 train_time:18677ms step_avg:33.35ms
step:561/2035 train_time:18710ms step_avg:33.35ms
step:562/2035 train_time:18743ms step_avg:33.35ms
step:563/2035 train_time:18777ms step_avg:33.35ms
step:564/2035 train_time:18810ms step_avg:33.35ms
step:565/2035 train_time:18843ms step_avg:33.35ms
step:566/2035 train_time:18877ms step_avg:33.35ms
step:567/2035 train_time:18910ms step_avg:33.35ms
step:568/2035 train_time:18944ms step_avg:33.35ms
step:569/2035 train_time:18977ms step_avg:33.35ms
step:570/2035 train_time:19010ms step_avg:33.35ms
step:571/2035 train_time:19043ms step_avg:33.35ms
step:572/2035 train_time:19076ms step_avg:33.35ms
step:573/2035 train_time:19110ms step_avg:33.35ms
step:574/2035 train_time:19143ms step_avg:33.35ms
step:575/2035 train_time:19176ms step_avg:33.35ms
step:576/2035 train_time:19209ms step_avg:33.35ms
step:577/2035 train_time:19242ms step_avg:33.35ms
step:578/2035 train_time:19275ms step_avg:33.35ms
step:579/2035 train_time:19308ms step_avg:33.35ms
step:580/2035 train_time:19342ms step_avg:33.35ms
step:581/2035 train_time:19375ms step_avg:33.35ms
step:582/2035 train_time:19408ms step_avg:33.35ms
step:583/2035 train_time:19441ms step_avg:33.35ms
step:584/2035 train_time:19474ms step_avg:33.35ms
step:585/2035 train_time:19507ms step_avg:33.35ms
step:586/2035 train_time:19540ms step_avg:33.34ms
step:587/2035 train_time:19573ms step_avg:33.34ms
step:588/2035 train_time:19606ms step_avg:33.34ms
step:589/2035 train_time:19640ms step_avg:33.34ms
step:590/2035 train_time:19673ms step_avg:33.34ms
step:591/2035 train_time:19706ms step_avg:33.34ms
step:592/2035 train_time:19739ms step_avg:33.34ms
step:593/2035 train_time:19772ms step_avg:33.34ms
step:594/2035 train_time:19805ms step_avg:33.34ms
step:595/2035 train_time:19839ms step_avg:33.34ms
step:596/2035 train_time:19872ms step_avg:33.34ms
step:597/2035 train_time:19906ms step_avg:33.34ms
step:598/2035 train_time:19939ms step_avg:33.34ms
step:599/2035 train_time:19972ms step_avg:33.34ms
step:600/2035 train_time:20005ms step_avg:33.34ms
step:601/2035 train_time:20038ms step_avg:33.34ms
step:602/2035 train_time:20072ms step_avg:33.34ms
step:603/2035 train_time:20106ms step_avg:33.34ms
step:604/2035 train_time:20139ms step_avg:33.34ms
step:605/2035 train_time:20172ms step_avg:33.34ms
step:606/2035 train_time:20205ms step_avg:33.34ms
step:607/2035 train_time:20238ms step_avg:33.34ms
step:608/2035 train_time:20271ms step_avg:33.34ms
step:609/2035 train_time:20304ms step_avg:33.34ms
step:610/2035 train_time:20338ms step_avg:33.34ms
step:611/2035 train_time:20371ms step_avg:33.34ms
step:612/2035 train_time:20404ms step_avg:33.34ms
step:613/2035 train_time:20437ms step_avg:33.34ms
step:614/2035 train_time:20470ms step_avg:33.34ms
step:615/2035 train_time:20503ms step_avg:33.34ms
step:616/2035 train_time:20537ms step_avg:33.34ms
step:617/2035 train_time:20570ms step_avg:33.34ms
step:618/2035 train_time:20603ms step_avg:33.34ms
step:619/2035 train_time:20636ms step_avg:33.34ms
step:620/2035 train_time:20669ms step_avg:33.34ms
step:621/2035 train_time:20702ms step_avg:33.34ms
step:622/2035 train_time:20735ms step_avg:33.34ms
step:623/2035 train_time:20768ms step_avg:33.34ms
step:624/2035 train_time:20801ms step_avg:33.34ms
step:625/2035 train_time:20834ms step_avg:33.33ms
step:626/2035 train_time:20867ms step_avg:33.33ms
step:627/2035 train_time:20900ms step_avg:33.33ms
step:628/2035 train_time:20934ms step_avg:33.33ms
step:629/2035 train_time:20967ms step_avg:33.33ms
step:630/2035 train_time:21000ms step_avg:33.33ms
step:631/2035 train_time:21033ms step_avg:33.33ms
step:632/2035 train_time:21067ms step_avg:33.33ms
step:633/2035 train_time:21100ms step_avg:33.33ms
step:634/2035 train_time:21133ms step_avg:33.33ms
step:635/2035 train_time:21166ms step_avg:33.33ms
step:636/2035 train_time:21199ms step_avg:33.33ms
step:637/2035 train_time:21232ms step_avg:33.33ms
step:638/2035 train_time:21265ms step_avg:33.33ms
step:639/2035 train_time:21299ms step_avg:33.33ms
step:640/2035 train_time:21333ms step_avg:33.33ms
step:641/2035 train_time:21366ms step_avg:33.33ms
step:642/2035 train_time:21399ms step_avg:33.33ms
step:643/2035 train_time:21433ms step_avg:33.33ms
step:644/2035 train_time:21466ms step_avg:33.33ms
step:645/2035 train_time:21499ms step_avg:33.33ms
step:646/2035 train_time:21532ms step_avg:33.33ms
step:647/2035 train_time:21565ms step_avg:33.33ms
step:648/2035 train_time:21598ms step_avg:33.33ms
step:649/2035 train_time:21631ms step_avg:33.33ms
step:650/2035 train_time:21665ms step_avg:33.33ms
step:651/2035 train_time:21698ms step_avg:33.33ms
step:652/2035 train_time:21731ms step_avg:33.33ms
step:653/2035 train_time:21764ms step_avg:33.33ms
step:654/2035 train_time:21798ms step_avg:33.33ms
step:655/2035 train_time:21831ms step_avg:33.33ms
step:656/2035 train_time:21864ms step_avg:33.33ms
step:657/2035 train_time:21897ms step_avg:33.33ms
step:658/2035 train_time:21931ms step_avg:33.33ms
step:659/2035 train_time:21964ms step_avg:33.33ms
step:660/2035 train_time:21998ms step_avg:33.33ms
step:661/2035 train_time:22031ms step_avg:33.33ms
step:662/2035 train_time:22064ms step_avg:33.33ms
step:663/2035 train_time:22097ms step_avg:33.33ms
step:664/2035 train_time:22131ms step_avg:33.33ms
step:665/2035 train_time:22164ms step_avg:33.33ms
step:666/2035 train_time:22198ms step_avg:33.33ms
step:667/2035 train_time:22257ms step_avg:33.37ms
step:668/2035 train_time:22317ms step_avg:33.41ms
step:669/2035 train_time:22378ms step_avg:33.45ms
step:670/2035 train_time:22437ms step_avg:33.49ms
step:671/2035 train_time:22498ms step_avg:33.53ms
step:672/2035 train_time:22558ms step_avg:33.57ms
step:673/2035 train_time:22619ms step_avg:33.61ms
step:674/2035 train_time:22679ms step_avg:33.65ms
step:675/2035 train_time:22739ms step_avg:33.69ms
step:676/2035 train_time:22799ms step_avg:33.73ms
step:677/2035 train_time:22860ms step_avg:33.77ms
step:678/2035 train_time:22920ms step_avg:33.81ms
step:679/2035 train_time:22982ms step_avg:33.85ms
step:680/2035 train_time:23042ms step_avg:33.89ms
step:681/2035 train_time:23104ms step_avg:33.93ms
step:682/2035 train_time:23164ms step_avg:33.96ms
step:683/2035 train_time:23224ms step_avg:34.00ms
step:684/2035 train_time:23285ms step_avg:34.04ms
step:685/2035 train_time:23346ms step_avg:34.08ms
step:686/2035 train_time:23407ms step_avg:34.12ms
step:687/2035 train_time:23467ms step_avg:34.16ms
step:688/2035 train_time:23527ms step_avg:34.20ms
step:689/2035 train_time:23587ms step_avg:34.23ms
step:690/2035 train_time:23648ms step_avg:34.27ms
step:691/2035 train_time:23708ms step_avg:34.31ms
step:692/2035 train_time:23768ms step_avg:34.35ms
step:693/2035 train_time:23828ms step_avg:34.38ms
step:694/2035 train_time:23888ms step_avg:34.42ms
step:695/2035 train_time:23949ms step_avg:34.46ms
step:696/2035 train_time:24008ms step_avg:34.49ms
step:697/2035 train_time:24069ms step_avg:34.53ms
step:698/2035 train_time:24129ms step_avg:34.57ms
step:699/2035 train_time:24190ms step_avg:34.61ms
step:700/2035 train_time:24249ms step_avg:34.64ms
step:701/2035 train_time:24309ms step_avg:34.68ms
step:702/2035 train_time:24369ms step_avg:34.71ms
step:703/2035 train_time:24430ms step_avg:34.75ms
step:704/2035 train_time:24490ms step_avg:34.79ms
step:705/2035 train_time:24550ms step_avg:34.82ms
step:706/2035 train_time:24609ms step_avg:34.86ms
step:707/2035 train_time:24669ms step_avg:34.89ms
step:708/2035 train_time:24728ms step_avg:34.93ms
step:709/2035 train_time:24789ms step_avg:34.96ms
step:710/2035 train_time:24849ms step_avg:35.00ms
step:711/2035 train_time:24910ms step_avg:35.03ms
step:712/2035 train_time:24969ms step_avg:35.07ms
step:713/2035 train_time:25029ms step_avg:35.10ms
step:714/2035 train_time:25089ms step_avg:35.14ms
step:715/2035 train_time:25150ms step_avg:35.17ms
step:716/2035 train_time:25209ms step_avg:35.21ms
step:717/2035 train_time:25270ms step_avg:35.24ms
step:718/2035 train_time:25329ms step_avg:35.28ms
step:719/2035 train_time:25390ms step_avg:35.31ms
step:720/2035 train_time:25450ms step_avg:35.35ms
step:721/2035 train_time:25511ms step_avg:35.38ms
step:722/2035 train_time:25571ms step_avg:35.42ms
step:723/2035 train_time:25631ms step_avg:35.45ms
step:724/2035 train_time:25690ms step_avg:35.48ms
step:725/2035 train_time:25750ms step_avg:35.52ms
step:726/2035 train_time:25810ms step_avg:35.55ms
step:727/2035 train_time:25870ms step_avg:35.58ms
step:728/2035 train_time:25929ms step_avg:35.62ms
step:729/2035 train_time:25990ms step_avg:35.65ms
step:730/2035 train_time:26050ms step_avg:35.68ms
step:731/2035 train_time:26110ms step_avg:35.72ms
step:732/2035 train_time:26170ms step_avg:35.75ms
step:733/2035 train_time:26231ms step_avg:35.79ms
step:734/2035 train_time:26290ms step_avg:35.82ms
step:735/2035 train_time:26351ms step_avg:35.85ms
step:736/2035 train_time:26411ms step_avg:35.88ms
step:737/2035 train_time:26471ms step_avg:35.92ms
step:738/2035 train_time:26530ms step_avg:35.95ms
step:739/2035 train_time:26591ms step_avg:35.98ms
step:740/2035 train_time:26650ms step_avg:36.01ms
step:741/2035 train_time:26711ms step_avg:36.05ms
step:742/2035 train_time:26770ms step_avg:36.08ms
step:743/2035 train_time:26831ms step_avg:36.11ms
step:744/2035 train_time:26890ms step_avg:36.14ms
step:745/2035 train_time:26951ms step_avg:36.18ms
step:746/2035 train_time:27010ms step_avg:36.21ms
step:747/2035 train_time:27070ms step_avg:36.24ms
step:748/2035 train_time:27130ms step_avg:36.27ms
step:749/2035 train_time:27191ms step_avg:36.30ms
step:750/2035 train_time:27250ms step_avg:36.33ms
step:750/2035 val_loss:3.8313 train_time:27312ms step_avg:36.42ms
step:751/2035 train_time:27333ms step_avg:36.39ms
step:752/2035 train_time:27372ms step_avg:36.40ms
step:753/2035 train_time:27436ms step_avg:36.44ms
step:754/2035 train_time:27498ms step_avg:36.47ms
step:755/2035 train_time:27558ms step_avg:36.50ms
step:756/2035 train_time:27618ms step_avg:36.53ms
step:757/2035 train_time:27680ms step_avg:36.57ms
step:758/2035 train_time:27739ms step_avg:36.60ms
step:759/2035 train_time:27800ms step_avg:36.63ms
step:760/2035 train_time:27859ms step_avg:36.66ms
step:761/2035 train_time:27920ms step_avg:36.69ms
step:762/2035 train_time:27979ms step_avg:36.72ms
step:763/2035 train_time:28039ms step_avg:36.75ms
step:764/2035 train_time:28098ms step_avg:36.78ms
step:765/2035 train_time:28158ms step_avg:36.81ms
step:766/2035 train_time:28219ms step_avg:36.84ms
step:767/2035 train_time:28283ms step_avg:36.87ms
step:768/2035 train_time:28344ms step_avg:36.91ms
step:769/2035 train_time:28406ms step_avg:36.94ms
step:770/2035 train_time:28468ms step_avg:36.97ms
step:771/2035 train_time:28530ms step_avg:37.00ms
step:772/2035 train_time:28591ms step_avg:37.03ms
step:773/2035 train_time:28651ms step_avg:37.06ms
step:774/2035 train_time:28711ms step_avg:37.09ms
step:775/2035 train_time:28771ms step_avg:37.12ms
step:776/2035 train_time:28830ms step_avg:37.15ms
step:777/2035 train_time:28889ms step_avg:37.18ms
step:778/2035 train_time:28948ms step_avg:37.21ms
step:779/2035 train_time:29008ms step_avg:37.24ms
step:780/2035 train_time:29067ms step_avg:37.27ms
step:781/2035 train_time:29127ms step_avg:37.29ms
step:782/2035 train_time:29188ms step_avg:37.32ms
step:783/2035 train_time:29248ms step_avg:37.35ms
step:784/2035 train_time:29308ms step_avg:37.38ms
step:785/2035 train_time:29369ms step_avg:37.41ms
step:786/2035 train_time:29430ms step_avg:37.44ms
step:787/2035 train_time:29492ms step_avg:37.47ms
step:788/2035 train_time:29552ms step_avg:37.50ms
step:789/2035 train_time:29612ms step_avg:37.53ms
step:790/2035 train_time:29672ms step_avg:37.56ms
step:791/2035 train_time:29732ms step_avg:37.59ms
step:792/2035 train_time:29791ms step_avg:37.62ms
step:793/2035 train_time:29851ms step_avg:37.64ms
step:794/2035 train_time:29910ms step_avg:37.67ms
step:795/2035 train_time:29970ms step_avg:37.70ms
step:796/2035 train_time:30029ms step_avg:37.72ms
step:797/2035 train_time:30089ms step_avg:37.75ms
step:798/2035 train_time:30149ms step_avg:37.78ms
step:799/2035 train_time:30210ms step_avg:37.81ms
step:800/2035 train_time:30269ms step_avg:37.84ms
step:801/2035 train_time:30330ms step_avg:37.87ms
step:802/2035 train_time:30390ms step_avg:37.89ms
step:803/2035 train_time:30451ms step_avg:37.92ms
step:804/2035 train_time:30511ms step_avg:37.95ms
step:805/2035 train_time:30571ms step_avg:37.98ms
step:806/2035 train_time:30631ms step_avg:38.00ms
step:807/2035 train_time:30692ms step_avg:38.03ms
step:808/2035 train_time:30752ms step_avg:38.06ms
step:809/2035 train_time:30811ms step_avg:38.09ms
step:810/2035 train_time:30871ms step_avg:38.11ms
step:811/2035 train_time:30931ms step_avg:38.14ms
step:812/2035 train_time:30990ms step_avg:38.17ms
step:813/2035 train_time:31050ms step_avg:38.19ms
step:814/2035 train_time:31109ms step_avg:38.22ms
step:815/2035 train_time:31170ms step_avg:38.25ms
step:816/2035 train_time:31229ms step_avg:38.27ms
step:817/2035 train_time:31289ms step_avg:38.30ms
step:818/2035 train_time:31349ms step_avg:38.32ms
step:819/2035 train_time:31410ms step_avg:38.35ms
step:820/2035 train_time:31470ms step_avg:38.38ms
step:821/2035 train_time:31531ms step_avg:38.41ms
step:822/2035 train_time:31591ms step_avg:38.43ms
step:823/2035 train_time:31651ms step_avg:38.46ms
step:824/2035 train_time:31711ms step_avg:38.48ms
step:825/2035 train_time:31771ms step_avg:38.51ms
step:826/2035 train_time:31830ms step_avg:38.54ms
step:827/2035 train_time:31891ms step_avg:38.56ms
step:828/2035 train_time:31950ms step_avg:38.59ms
step:829/2035 train_time:32010ms step_avg:38.61ms
step:830/2035 train_time:32070ms step_avg:38.64ms
step:831/2035 train_time:32130ms step_avg:38.66ms
step:832/2035 train_time:32190ms step_avg:38.69ms
step:833/2035 train_time:32250ms step_avg:38.72ms
step:834/2035 train_time:32310ms step_avg:38.74ms
step:835/2035 train_time:32370ms step_avg:38.77ms
step:836/2035 train_time:32430ms step_avg:38.79ms
step:837/2035 train_time:32491ms step_avg:38.82ms
step:838/2035 train_time:32550ms step_avg:38.84ms
step:839/2035 train_time:32611ms step_avg:38.87ms
step:840/2035 train_time:32671ms step_avg:38.89ms
step:841/2035 train_time:32731ms step_avg:38.92ms
step:842/2035 train_time:32791ms step_avg:38.94ms
step:843/2035 train_time:32851ms step_avg:38.97ms
step:844/2035 train_time:32910ms step_avg:38.99ms
step:845/2035 train_time:32970ms step_avg:39.02ms
step:846/2035 train_time:33030ms step_avg:39.04ms
step:847/2035 train_time:33091ms step_avg:39.07ms
step:848/2035 train_time:33151ms step_avg:39.09ms
step:849/2035 train_time:33212ms step_avg:39.12ms
step:850/2035 train_time:33272ms step_avg:39.14ms
step:851/2035 train_time:33333ms step_avg:39.17ms
step:852/2035 train_time:33392ms step_avg:39.19ms
step:853/2035 train_time:33452ms step_avg:39.22ms
step:854/2035 train_time:33512ms step_avg:39.24ms
step:855/2035 train_time:33573ms step_avg:39.27ms
step:856/2035 train_time:33634ms step_avg:39.29ms
step:857/2035 train_time:33694ms step_avg:39.32ms
step:858/2035 train_time:33754ms step_avg:39.34ms
step:859/2035 train_time:33814ms step_avg:39.36ms
step:860/2035 train_time:33873ms step_avg:39.39ms
step:861/2035 train_time:33934ms step_avg:39.41ms
step:862/2035 train_time:33993ms step_avg:39.43ms
step:863/2035 train_time:34054ms step_avg:39.46ms
step:864/2035 train_time:34113ms step_avg:39.48ms
step:865/2035 train_time:34174ms step_avg:39.51ms
step:866/2035 train_time:34234ms step_avg:39.53ms
step:867/2035 train_time:34295ms step_avg:39.56ms
step:868/2035 train_time:34355ms step_avg:39.58ms
step:869/2035 train_time:34416ms step_avg:39.60ms
step:870/2035 train_time:34476ms step_avg:39.63ms
step:871/2035 train_time:34536ms step_avg:39.65ms
step:872/2035 train_time:34596ms step_avg:39.67ms
step:873/2035 train_time:34656ms step_avg:39.70ms
step:874/2035 train_time:34715ms step_avg:39.72ms
step:875/2035 train_time:34776ms step_avg:39.74ms
step:876/2035 train_time:34835ms step_avg:39.77ms
step:877/2035 train_time:34896ms step_avg:39.79ms
step:878/2035 train_time:34955ms step_avg:39.81ms
step:879/2035 train_time:35015ms step_avg:39.84ms
step:880/2035 train_time:35075ms step_avg:39.86ms
step:881/2035 train_time:35135ms step_avg:39.88ms
step:882/2035 train_time:35195ms step_avg:39.90ms
step:883/2035 train_time:35255ms step_avg:39.93ms
step:884/2035 train_time:35315ms step_avg:39.95ms
step:885/2035 train_time:35376ms step_avg:39.97ms
step:886/2035 train_time:35436ms step_avg:40.00ms
step:887/2035 train_time:35496ms step_avg:40.02ms
step:888/2035 train_time:35556ms step_avg:40.04ms
step:889/2035 train_time:35616ms step_avg:40.06ms
step:890/2035 train_time:35676ms step_avg:40.09ms
step:891/2035 train_time:35736ms step_avg:40.11ms
step:892/2035 train_time:35796ms step_avg:40.13ms
step:893/2035 train_time:35856ms step_avg:40.15ms
step:894/2035 train_time:35916ms step_avg:40.17ms
step:895/2035 train_time:35976ms step_avg:40.20ms
step:896/2035 train_time:36036ms step_avg:40.22ms
step:897/2035 train_time:36097ms step_avg:40.24ms
step:898/2035 train_time:36156ms step_avg:40.26ms
step:899/2035 train_time:36217ms step_avg:40.29ms
step:900/2035 train_time:36276ms step_avg:40.31ms
step:901/2035 train_time:36337ms step_avg:40.33ms
step:902/2035 train_time:36396ms step_avg:40.35ms
step:903/2035 train_time:36457ms step_avg:40.37ms
step:904/2035 train_time:36516ms step_avg:40.39ms
step:905/2035 train_time:36577ms step_avg:40.42ms
step:906/2035 train_time:36637ms step_avg:40.44ms
step:907/2035 train_time:36697ms step_avg:40.46ms
step:908/2035 train_time:36756ms step_avg:40.48ms
step:909/2035 train_time:36817ms step_avg:40.50ms
step:910/2035 train_time:36876ms step_avg:40.52ms
step:911/2035 train_time:36937ms step_avg:40.55ms
step:912/2035 train_time:36996ms step_avg:40.57ms
step:913/2035 train_time:37057ms step_avg:40.59ms
step:914/2035 train_time:37116ms step_avg:40.61ms
step:915/2035 train_time:37177ms step_avg:40.63ms
step:916/2035 train_time:37237ms step_avg:40.65ms
step:917/2035 train_time:37298ms step_avg:40.67ms
step:918/2035 train_time:37358ms step_avg:40.70ms
step:919/2035 train_time:37419ms step_avg:40.72ms
step:920/2035 train_time:37478ms step_avg:40.74ms
step:921/2035 train_time:37539ms step_avg:40.76ms
step:922/2035 train_time:37599ms step_avg:40.78ms
step:923/2035 train_time:37660ms step_avg:40.80ms
step:924/2035 train_time:37720ms step_avg:40.82ms
step:925/2035 train_time:37781ms step_avg:40.84ms
step:926/2035 train_time:37841ms step_avg:40.86ms
step:927/2035 train_time:37901ms step_avg:40.89ms
step:928/2035 train_time:37962ms step_avg:40.91ms
step:929/2035 train_time:38022ms step_avg:40.93ms
step:930/2035 train_time:38082ms step_avg:40.95ms
step:931/2035 train_time:38144ms step_avg:40.97ms
step:932/2035 train_time:38205ms step_avg:40.99ms
step:933/2035 train_time:38266ms step_avg:41.01ms
step:934/2035 train_time:38326ms step_avg:41.03ms
step:935/2035 train_time:38386ms step_avg:41.05ms
step:936/2035 train_time:38447ms step_avg:41.08ms
step:937/2035 train_time:38507ms step_avg:41.10ms
step:938/2035 train_time:38567ms step_avg:41.12ms
step:939/2035 train_time:38627ms step_avg:41.14ms
step:940/2035 train_time:38689ms step_avg:41.16ms
step:941/2035 train_time:38749ms step_avg:41.18ms
step:942/2035 train_time:38809ms step_avg:41.20ms
step:943/2035 train_time:38869ms step_avg:41.22ms
step:944/2035 train_time:38929ms step_avg:41.24ms
step:945/2035 train_time:38989ms step_avg:41.26ms
step:946/2035 train_time:39049ms step_avg:41.28ms
step:947/2035 train_time:39109ms step_avg:41.30ms
step:948/2035 train_time:39169ms step_avg:41.32ms
step:949/2035 train_time:39230ms step_avg:41.34ms
step:950/2035 train_time:39290ms step_avg:41.36ms
step:951/2035 train_time:39350ms step_avg:41.38ms
step:952/2035 train_time:39410ms step_avg:41.40ms
step:953/2035 train_time:39470ms step_avg:41.42ms
step:954/2035 train_time:39530ms step_avg:41.44ms
step:955/2035 train_time:39590ms step_avg:41.46ms
step:956/2035 train_time:39650ms step_avg:41.47ms
step:957/2035 train_time:39711ms step_avg:41.50ms
step:958/2035 train_time:39770ms step_avg:41.51ms
step:959/2035 train_time:39831ms step_avg:41.53ms
step:960/2035 train_time:39891ms step_avg:41.55ms
step:961/2035 train_time:39952ms step_avg:41.57ms
step:962/2035 train_time:40011ms step_avg:41.59ms
step:963/2035 train_time:40071ms step_avg:41.61ms
step:964/2035 train_time:40131ms step_avg:41.63ms
step:965/2035 train_time:40191ms step_avg:41.65ms
step:966/2035 train_time:40250ms step_avg:41.67ms
step:967/2035 train_time:40310ms step_avg:41.69ms
step:968/2035 train_time:40371ms step_avg:41.71ms
step:969/2035 train_time:40431ms step_avg:41.72ms
step:970/2035 train_time:40491ms step_avg:41.74ms
step:971/2035 train_time:40551ms step_avg:41.76ms
step:972/2035 train_time:40610ms step_avg:41.78ms
step:973/2035 train_time:40670ms step_avg:41.80ms
step:974/2035 train_time:40730ms step_avg:41.82ms
step:975/2035 train_time:40791ms step_avg:41.84ms
step:976/2035 train_time:40851ms step_avg:41.86ms
step:977/2035 train_time:40911ms step_avg:41.87ms
step:978/2035 train_time:40970ms step_avg:41.89ms
step:979/2035 train_time:41031ms step_avg:41.91ms
step:980/2035 train_time:41091ms step_avg:41.93ms
step:981/2035 train_time:41151ms step_avg:41.95ms
step:982/2035 train_time:41210ms step_avg:41.97ms
step:983/2035 train_time:41270ms step_avg:41.98ms
step:984/2035 train_time:41330ms step_avg:42.00ms
step:985/2035 train_time:41391ms step_avg:42.02ms
step:986/2035 train_time:41450ms step_avg:42.04ms
step:987/2035 train_time:41511ms step_avg:42.06ms
step:988/2035 train_time:41570ms step_avg:42.07ms
step:989/2035 train_time:41631ms step_avg:42.09ms
step:990/2035 train_time:41691ms step_avg:42.11ms
step:991/2035 train_time:41752ms step_avg:42.13ms
step:992/2035 train_time:41811ms step_avg:42.15ms
step:993/2035 train_time:41871ms step_avg:42.17ms
step:994/2035 train_time:41931ms step_avg:42.18ms
step:995/2035 train_time:41991ms step_avg:42.20ms
step:996/2035 train_time:42050ms step_avg:42.22ms
step:997/2035 train_time:42111ms step_avg:42.24ms
step:998/2035 train_time:42171ms step_avg:42.26ms
step:999/2035 train_time:42231ms step_avg:42.27ms
step:1000/2035 train_time:42291ms step_avg:42.29ms
step:1000/2035 val_loss:3.6860 train_time:42353ms step_avg:42.35ms
step:1001/2035 train_time:42372ms step_avg:42.33ms
step:1002/2035 train_time:42412ms step_avg:42.33ms
step:1003/2035 train_time:42474ms step_avg:42.35ms
step:1004/2035 train_time:42537ms step_avg:42.37ms
step:1005/2035 train_time:42598ms step_avg:42.39ms
step:1006/2035 train_time:42658ms step_avg:42.40ms
step:1007/2035 train_time:42717ms step_avg:42.42ms
step:1008/2035 train_time:42776ms step_avg:42.44ms
step:1009/2035 train_time:42837ms step_avg:42.45ms
step:1010/2035 train_time:42896ms step_avg:42.47ms
step:1011/2035 train_time:42956ms step_avg:42.49ms
step:1012/2035 train_time:43015ms step_avg:42.51ms
step:1013/2035 train_time:43075ms step_avg:42.52ms
step:1014/2035 train_time:43134ms step_avg:42.54ms
step:1015/2035 train_time:43194ms step_avg:42.56ms
step:1016/2035 train_time:43254ms step_avg:42.57ms
step:1017/2035 train_time:43318ms step_avg:42.59ms
step:1018/2035 train_time:43379ms step_avg:42.61ms
step:1019/2035 train_time:43441ms step_avg:42.63ms
step:1020/2035 train_time:43501ms step_avg:42.65ms
step:1021/2035 train_time:43563ms step_avg:42.67ms
step:1022/2035 train_time:43623ms step_avg:42.68ms
step:1023/2035 train_time:43684ms step_avg:42.70ms
step:1024/2035 train_time:43745ms step_avg:42.72ms
step:1025/2035 train_time:43805ms step_avg:42.74ms
step:1026/2035 train_time:43865ms step_avg:42.75ms
step:1027/2035 train_time:43926ms step_avg:42.77ms
step:1028/2035 train_time:43986ms step_avg:42.79ms
step:1029/2035 train_time:44046ms step_avg:42.80ms
step:1030/2035 train_time:44106ms step_avg:42.82ms
step:1031/2035 train_time:44167ms step_avg:42.84ms
step:1032/2035 train_time:44227ms step_avg:42.86ms
step:1033/2035 train_time:44288ms step_avg:42.87ms
step:1034/2035 train_time:44348ms step_avg:42.89ms
step:1035/2035 train_time:44409ms step_avg:42.91ms
step:1036/2035 train_time:44469ms step_avg:42.92ms
step:1037/2035 train_time:44530ms step_avg:42.94ms
step:1038/2035 train_time:44590ms step_avg:42.96ms
step:1039/2035 train_time:44650ms step_avg:42.97ms
step:1040/2035 train_time:44709ms step_avg:42.99ms
step:1041/2035 train_time:44770ms step_avg:43.01ms
step:1042/2035 train_time:44829ms step_avg:43.02ms
step:1043/2035 train_time:44889ms step_avg:43.04ms
step:1044/2035 train_time:44949ms step_avg:43.05ms
step:1045/2035 train_time:45009ms step_avg:43.07ms
step:1046/2035 train_time:45069ms step_avg:43.09ms
step:1047/2035 train_time:45130ms step_avg:43.10ms
step:1048/2035 train_time:45190ms step_avg:43.12ms
step:1049/2035 train_time:45250ms step_avg:43.14ms
step:1050/2035 train_time:45310ms step_avg:43.15ms
step:1051/2035 train_time:45370ms step_avg:43.17ms
step:1052/2035 train_time:45430ms step_avg:43.18ms
step:1053/2035 train_time:45490ms step_avg:43.20ms
step:1054/2035 train_time:45550ms step_avg:43.22ms
step:1055/2035 train_time:45610ms step_avg:43.23ms
step:1056/2035 train_time:45670ms step_avg:43.25ms
step:1057/2035 train_time:45731ms step_avg:43.26ms
step:1058/2035 train_time:45790ms step_avg:43.28ms
step:1059/2035 train_time:45850ms step_avg:43.30ms
step:1060/2035 train_time:45910ms step_avg:43.31ms
step:1061/2035 train_time:45970ms step_avg:43.33ms
step:1062/2035 train_time:46029ms step_avg:43.34ms
step:1063/2035 train_time:46089ms step_avg:43.36ms
step:1064/2035 train_time:46149ms step_avg:43.37ms
step:1065/2035 train_time:46210ms step_avg:43.39ms
step:1066/2035 train_time:46270ms step_avg:43.41ms
step:1067/2035 train_time:46331ms step_avg:43.42ms
step:1068/2035 train_time:46390ms step_avg:43.44ms
step:1069/2035 train_time:46451ms step_avg:43.45ms
step:1070/2035 train_time:46511ms step_avg:43.47ms
step:1071/2035 train_time:46571ms step_avg:43.48ms
step:1072/2035 train_time:46631ms step_avg:43.50ms
step:1073/2035 train_time:46692ms step_avg:43.51ms
step:1074/2035 train_time:46750ms step_avg:43.53ms
step:1075/2035 train_time:46811ms step_avg:43.55ms
step:1076/2035 train_time:46871ms step_avg:43.56ms
step:1077/2035 train_time:46931ms step_avg:43.58ms
step:1078/2035 train_time:46990ms step_avg:43.59ms
step:1079/2035 train_time:47051ms step_avg:43.61ms
step:1080/2035 train_time:47110ms step_avg:43.62ms
step:1081/2035 train_time:47171ms step_avg:43.64ms
step:1082/2035 train_time:47230ms step_avg:43.65ms
step:1083/2035 train_time:47291ms step_avg:43.67ms
step:1084/2035 train_time:47351ms step_avg:43.68ms
step:1085/2035 train_time:47411ms step_avg:43.70ms
step:1086/2035 train_time:47470ms step_avg:43.71ms
step:1087/2035 train_time:47531ms step_avg:43.73ms
step:1088/2035 train_time:47590ms step_avg:43.74ms
step:1089/2035 train_time:47650ms step_avg:43.76ms
step:1090/2035 train_time:47709ms step_avg:43.77ms
step:1091/2035 train_time:47770ms step_avg:43.79ms
step:1092/2035 train_time:47830ms step_avg:43.80ms
step:1093/2035 train_time:47890ms step_avg:43.82ms
step:1094/2035 train_time:47949ms step_avg:43.83ms
step:1095/2035 train_time:48010ms step_avg:43.84ms
step:1096/2035 train_time:48070ms step_avg:43.86ms
step:1097/2035 train_time:48130ms step_avg:43.87ms
step:1098/2035 train_time:48190ms step_avg:43.89ms
step:1099/2035 train_time:48250ms step_avg:43.90ms
step:1100/2035 train_time:48310ms step_avg:43.92ms
step:1101/2035 train_time:48371ms step_avg:43.93ms
step:1102/2035 train_time:48430ms step_avg:43.95ms
step:1103/2035 train_time:48491ms step_avg:43.96ms
step:1104/2035 train_time:48551ms step_avg:43.98ms
step:1105/2035 train_time:48611ms step_avg:43.99ms
step:1106/2035 train_time:48670ms step_avg:44.01ms
step:1107/2035 train_time:48731ms step_avg:44.02ms
step:1108/2035 train_time:48791ms step_avg:44.03ms
step:1109/2035 train_time:48851ms step_avg:44.05ms
step:1110/2035 train_time:48911ms step_avg:44.06ms
step:1111/2035 train_time:48971ms step_avg:44.08ms
step:1112/2035 train_time:49031ms step_avg:44.09ms
step:1113/2035 train_time:49092ms step_avg:44.11ms
step:1114/2035 train_time:49152ms step_avg:44.12ms
step:1115/2035 train_time:49213ms step_avg:44.14ms
step:1116/2035 train_time:49272ms step_avg:44.15ms
step:1117/2035 train_time:49332ms step_avg:44.16ms
step:1118/2035 train_time:49391ms step_avg:44.18ms
step:1119/2035 train_time:49452ms step_avg:44.19ms
step:1120/2035 train_time:49511ms step_avg:44.21ms
step:1121/2035 train_time:49572ms step_avg:44.22ms
step:1122/2035 train_time:49632ms step_avg:44.24ms
step:1123/2035 train_time:49692ms step_avg:44.25ms
step:1124/2035 train_time:49752ms step_avg:44.26ms
step:1125/2035 train_time:49812ms step_avg:44.28ms
step:1126/2035 train_time:49872ms step_avg:44.29ms
step:1127/2035 train_time:49932ms step_avg:44.31ms
step:1128/2035 train_time:49992ms step_avg:44.32ms
step:1129/2035 train_time:50053ms step_avg:44.33ms
step:1130/2035 train_time:50112ms step_avg:44.35ms
step:1131/2035 train_time:50173ms step_avg:44.36ms
step:1132/2035 train_time:50232ms step_avg:44.37ms
step:1133/2035 train_time:50292ms step_avg:44.39ms
step:1134/2035 train_time:50352ms step_avg:44.40ms
step:1135/2035 train_time:50412ms step_avg:44.42ms
step:1136/2035 train_time:50471ms step_avg:44.43ms
step:1137/2035 train_time:50531ms step_avg:44.44ms
step:1138/2035 train_time:50590ms step_avg:44.46ms
step:1139/2035 train_time:50650ms step_avg:44.47ms
step:1140/2035 train_time:50710ms step_avg:44.48ms
step:1141/2035 train_time:50770ms step_avg:44.50ms
step:1142/2035 train_time:50830ms step_avg:44.51ms
step:1143/2035 train_time:50891ms step_avg:44.52ms
step:1144/2035 train_time:50951ms step_avg:44.54ms
step:1145/2035 train_time:51011ms step_avg:44.55ms
step:1146/2035 train_time:51071ms step_avg:44.56ms
step:1147/2035 train_time:51132ms step_avg:44.58ms
step:1148/2035 train_time:51191ms step_avg:44.59ms
step:1149/2035 train_time:51251ms step_avg:44.61ms
step:1150/2035 train_time:51311ms step_avg:44.62ms
step:1151/2035 train_time:51372ms step_avg:44.63ms
step:1152/2035 train_time:51431ms step_avg:44.65ms
step:1153/2035 train_time:51492ms step_avg:44.66ms
step:1154/2035 train_time:51551ms step_avg:44.67ms
step:1155/2035 train_time:51612ms step_avg:44.69ms
step:1156/2035 train_time:51671ms step_avg:44.70ms
step:1157/2035 train_time:51732ms step_avg:44.71ms
step:1158/2035 train_time:51791ms step_avg:44.72ms
step:1159/2035 train_time:51852ms step_avg:44.74ms
step:1160/2035 train_time:51911ms step_avg:44.75ms
step:1161/2035 train_time:51972ms step_avg:44.76ms
step:1162/2035 train_time:52032ms step_avg:44.78ms
step:1163/2035 train_time:52093ms step_avg:44.79ms
step:1164/2035 train_time:52152ms step_avg:44.80ms
step:1165/2035 train_time:52213ms step_avg:44.82ms
step:1166/2035 train_time:52272ms step_avg:44.83ms
step:1167/2035 train_time:52333ms step_avg:44.84ms
step:1168/2035 train_time:52392ms step_avg:44.86ms
step:1169/2035 train_time:52452ms step_avg:44.87ms
step:1170/2035 train_time:52511ms step_avg:44.88ms
step:1171/2035 train_time:52572ms step_avg:44.90ms
step:1172/2035 train_time:52632ms step_avg:44.91ms
step:1173/2035 train_time:52693ms step_avg:44.92ms
step:1174/2035 train_time:52752ms step_avg:44.93ms
step:1175/2035 train_time:52813ms step_avg:44.95ms
step:1176/2035 train_time:52873ms step_avg:44.96ms
step:1177/2035 train_time:52933ms step_avg:44.97ms
step:1178/2035 train_time:52993ms step_avg:44.99ms
step:1179/2035 train_time:53053ms step_avg:45.00ms
step:1180/2035 train_time:53112ms step_avg:45.01ms
step:1181/2035 train_time:53172ms step_avg:45.02ms
step:1182/2035 train_time:53232ms step_avg:45.04ms
step:1183/2035 train_time:53291ms step_avg:45.05ms
step:1184/2035 train_time:53351ms step_avg:45.06ms
step:1185/2035 train_time:53411ms step_avg:45.07ms
step:1186/2035 train_time:53471ms step_avg:45.09ms
step:1187/2035 train_time:53531ms step_avg:45.10ms
step:1188/2035 train_time:53590ms step_avg:45.11ms
step:1189/2035 train_time:53651ms step_avg:45.12ms
step:1190/2035 train_time:53711ms step_avg:45.14ms
step:1191/2035 train_time:53772ms step_avg:45.15ms
step:1192/2035 train_time:53832ms step_avg:45.16ms
step:1193/2035 train_time:53892ms step_avg:45.17ms
step:1194/2035 train_time:53952ms step_avg:45.19ms
step:1195/2035 train_time:54013ms step_avg:45.20ms
step:1196/2035 train_time:54073ms step_avg:45.21ms
step:1197/2035 train_time:54133ms step_avg:45.22ms
step:1198/2035 train_time:54192ms step_avg:45.24ms
step:1199/2035 train_time:54252ms step_avg:45.25ms
step:1200/2035 train_time:54311ms step_avg:45.26ms
step:1201/2035 train_time:54372ms step_avg:45.27ms
step:1202/2035 train_time:54431ms step_avg:45.28ms
step:1203/2035 train_time:54492ms step_avg:45.30ms
step:1204/2035 train_time:54552ms step_avg:45.31ms
step:1205/2035 train_time:54612ms step_avg:45.32ms
step:1206/2035 train_time:54671ms step_avg:45.33ms
step:1207/2035 train_time:54732ms step_avg:45.35ms
step:1208/2035 train_time:54792ms step_avg:45.36ms
step:1209/2035 train_time:54853ms step_avg:45.37ms
step:1210/2035 train_time:54913ms step_avg:45.38ms
step:1211/2035 train_time:54973ms step_avg:45.39ms
step:1212/2035 train_time:55033ms step_avg:45.41ms
step:1213/2035 train_time:55093ms step_avg:45.42ms
step:1214/2035 train_time:55153ms step_avg:45.43ms
step:1215/2035 train_time:55214ms step_avg:45.44ms
step:1216/2035 train_time:55274ms step_avg:45.46ms
step:1217/2035 train_time:55334ms step_avg:45.47ms
step:1218/2035 train_time:55393ms step_avg:45.48ms
step:1219/2035 train_time:55454ms step_avg:45.49ms
step:1220/2035 train_time:55514ms step_avg:45.50ms
step:1221/2035 train_time:55574ms step_avg:45.52ms
step:1222/2035 train_time:55634ms step_avg:45.53ms
step:1223/2035 train_time:55695ms step_avg:45.54ms
step:1224/2035 train_time:55754ms step_avg:45.55ms
step:1225/2035 train_time:55815ms step_avg:45.56ms
step:1226/2035 train_time:55875ms step_avg:45.57ms
step:1227/2035 train_time:55935ms step_avg:45.59ms
step:1228/2035 train_time:55995ms step_avg:45.60ms
step:1229/2035 train_time:56055ms step_avg:45.61ms
step:1230/2035 train_time:56115ms step_avg:45.62ms
step:1231/2035 train_time:56175ms step_avg:45.63ms
step:1232/2035 train_time:56235ms step_avg:45.65ms
step:1233/2035 train_time:56296ms step_avg:45.66ms
step:1234/2035 train_time:56355ms step_avg:45.67ms
step:1235/2035 train_time:56415ms step_avg:45.68ms
step:1236/2035 train_time:56475ms step_avg:45.69ms
step:1237/2035 train_time:56535ms step_avg:45.70ms
step:1238/2035 train_time:56595ms step_avg:45.71ms
step:1239/2035 train_time:56656ms step_avg:45.73ms
step:1240/2035 train_time:56716ms step_avg:45.74ms
step:1241/2035 train_time:56777ms step_avg:45.75ms
step:1242/2035 train_time:56837ms step_avg:45.76ms
step:1243/2035 train_time:56897ms step_avg:45.77ms
step:1244/2035 train_time:56957ms step_avg:45.79ms
step:1245/2035 train_time:57017ms step_avg:45.80ms
step:1246/2035 train_time:57077ms step_avg:45.81ms
step:1247/2035 train_time:57138ms step_avg:45.82ms
step:1248/2035 train_time:57197ms step_avg:45.83ms
step:1249/2035 train_time:57258ms step_avg:45.84ms
step:1250/2035 train_time:57318ms step_avg:45.85ms
step:1250/2035 val_loss:3.5709 train_time:57380ms step_avg:45.90ms
step:1251/2035 train_time:57400ms step_avg:45.88ms
step:1252/2035 train_time:57439ms step_avg:45.88ms
step:1253/2035 train_time:57503ms step_avg:45.89ms
step:1254/2035 train_time:57566ms step_avg:45.91ms
step:1255/2035 train_time:57627ms step_avg:45.92ms
step:1256/2035 train_time:57686ms step_avg:45.93ms
step:1257/2035 train_time:57746ms step_avg:45.94ms
step:1258/2035 train_time:57806ms step_avg:45.95ms
step:1259/2035 train_time:57866ms step_avg:45.96ms
step:1260/2035 train_time:57926ms step_avg:45.97ms
step:1261/2035 train_time:57985ms step_avg:45.98ms
step:1262/2035 train_time:58045ms step_avg:45.99ms
step:1263/2035 train_time:58105ms step_avg:46.01ms
step:1264/2035 train_time:58165ms step_avg:46.02ms
step:1265/2035 train_time:58225ms step_avg:46.03ms
step:1266/2035 train_time:58285ms step_avg:46.04ms
step:1267/2035 train_time:58347ms step_avg:46.05ms
step:1268/2035 train_time:58408ms step_avg:46.06ms
step:1269/2035 train_time:58471ms step_avg:46.08ms
step:1270/2035 train_time:58533ms step_avg:46.09ms
step:1271/2035 train_time:58594ms step_avg:46.10ms
step:1272/2035 train_time:58653ms step_avg:46.11ms
step:1273/2035 train_time:58714ms step_avg:46.12ms
step:1274/2035 train_time:58774ms step_avg:46.13ms
step:1275/2035 train_time:58833ms step_avg:46.14ms
step:1276/2035 train_time:58892ms step_avg:46.15ms
step:1277/2035 train_time:58952ms step_avg:46.16ms
step:1278/2035 train_time:59011ms step_avg:46.17ms
step:1279/2035 train_time:59071ms step_avg:46.19ms
step:1280/2035 train_time:59131ms step_avg:46.20ms
step:1281/2035 train_time:59191ms step_avg:46.21ms
step:1282/2035 train_time:59250ms step_avg:46.22ms
step:1283/2035 train_time:59311ms step_avg:46.23ms
step:1284/2035 train_time:59372ms step_avg:46.24ms
step:1285/2035 train_time:59433ms step_avg:46.25ms
step:1286/2035 train_time:59494ms step_avg:46.26ms
step:1287/2035 train_time:59555ms step_avg:46.27ms
step:1288/2035 train_time:59615ms step_avg:46.28ms
step:1289/2035 train_time:59675ms step_avg:46.30ms
step:1290/2035 train_time:59735ms step_avg:46.31ms
step:1291/2035 train_time:59795ms step_avg:46.32ms
step:1292/2035 train_time:59854ms step_avg:46.33ms
step:1293/2035 train_time:59914ms step_avg:46.34ms
step:1294/2035 train_time:59973ms step_avg:46.35ms
step:1295/2035 train_time:60034ms step_avg:46.36ms
step:1296/2035 train_time:60093ms step_avg:46.37ms
step:1297/2035 train_time:60153ms step_avg:46.38ms
step:1298/2035 train_time:60212ms step_avg:46.39ms
step:1299/2035 train_time:60273ms step_avg:46.40ms
step:1300/2035 train_time:60333ms step_avg:46.41ms
step:1301/2035 train_time:60394ms step_avg:46.42ms
step:1302/2035 train_time:60453ms step_avg:46.43ms
step:1303/2035 train_time:60514ms step_avg:46.44ms
step:1304/2035 train_time:60575ms step_avg:46.45ms
step:1305/2035 train_time:60635ms step_avg:46.46ms
step:1306/2035 train_time:60695ms step_avg:46.47ms
step:1307/2035 train_time:60755ms step_avg:46.48ms
step:1308/2035 train_time:60814ms step_avg:46.49ms
step:1309/2035 train_time:60875ms step_avg:46.50ms
step:1310/2035 train_time:60934ms step_avg:46.51ms
step:1311/2035 train_time:60994ms step_avg:46.53ms
step:1312/2035 train_time:61054ms step_avg:46.53ms
step:1313/2035 train_time:61114ms step_avg:46.55ms
step:1314/2035 train_time:61174ms step_avg:46.56ms
step:1315/2035 train_time:61234ms step_avg:46.57ms
step:1316/2035 train_time:61293ms step_avg:46.58ms
step:1317/2035 train_time:61354ms step_avg:46.59ms
step:1318/2035 train_time:61413ms step_avg:46.60ms
step:1319/2035 train_time:61475ms step_avg:46.61ms
step:1320/2035 train_time:61534ms step_avg:46.62ms
step:1321/2035 train_time:61595ms step_avg:46.63ms
step:1322/2035 train_time:61654ms step_avg:46.64ms
step:1323/2035 train_time:61715ms step_avg:46.65ms
step:1324/2035 train_time:61776ms step_avg:46.66ms
step:1325/2035 train_time:61835ms step_avg:46.67ms
step:1326/2035 train_time:61895ms step_avg:46.68ms
step:1327/2035 train_time:61955ms step_avg:46.69ms
step:1328/2035 train_time:62014ms step_avg:46.70ms
step:1329/2035 train_time:62075ms step_avg:46.71ms
step:1330/2035 train_time:62134ms step_avg:46.72ms
step:1331/2035 train_time:62195ms step_avg:46.73ms
step:1332/2035 train_time:62282ms step_avg:46.76ms
step:1333/2035 train_time:62370ms step_avg:46.79ms
step:1334/2035 train_time:62458ms step_avg:46.82ms
step:1335/2035 train_time:62547ms step_avg:46.85ms
step:1336/2035 train_time:62635ms step_avg:46.88ms
step:1337/2035 train_time:62723ms step_avg:46.91ms
step:1338/2035 train_time:62810ms step_avg:46.94ms
step:1339/2035 train_time:62899ms step_avg:46.97ms
step:1340/2035 train_time:62987ms step_avg:47.01ms
step:1341/2035 train_time:63075ms step_avg:47.04ms
step:1342/2035 train_time:63162ms step_avg:47.07ms
step:1343/2035 train_time:63250ms step_avg:47.10ms
step:1344/2035 train_time:63337ms step_avg:47.13ms
step:1345/2035 train_time:63426ms step_avg:47.16ms
step:1346/2035 train_time:63515ms step_avg:47.19ms
step:1347/2035 train_time:63604ms step_avg:47.22ms
step:1348/2035 train_time:63691ms step_avg:47.25ms
step:1349/2035 train_time:63780ms step_avg:47.28ms
step:1350/2035 train_time:63867ms step_avg:47.31ms
step:1351/2035 train_time:63955ms step_avg:47.34ms
step:1352/2035 train_time:64043ms step_avg:47.37ms
step:1353/2035 train_time:64131ms step_avg:47.40ms
step:1354/2035 train_time:64219ms step_avg:47.43ms
step:1355/2035 train_time:64307ms step_avg:47.46ms
step:1356/2035 train_time:64395ms step_avg:47.49ms
step:1357/2035 train_time:64483ms step_avg:47.52ms
step:1358/2035 train_time:64571ms step_avg:47.55ms
step:1359/2035 train_time:64660ms step_avg:47.58ms
step:1360/2035 train_time:64748ms step_avg:47.61ms
step:1361/2035 train_time:64836ms step_avg:47.64ms
step:1362/2035 train_time:64925ms step_avg:47.67ms
step:1363/2035 train_time:65012ms step_avg:47.70ms
step:1364/2035 train_time:65100ms step_avg:47.73ms
step:1365/2035 train_time:65188ms step_avg:47.76ms
step:1366/2035 train_time:65276ms step_avg:47.79ms
step:1367/2035 train_time:65365ms step_avg:47.82ms
step:1368/2035 train_time:65452ms step_avg:47.85ms
step:1369/2035 train_time:65541ms step_avg:47.88ms
step:1370/2035 train_time:65628ms step_avg:47.90ms
step:1371/2035 train_time:65717ms step_avg:47.93ms
step:1372/2035 train_time:65805ms step_avg:47.96ms
step:1373/2035 train_time:65893ms step_avg:47.99ms
step:1374/2035 train_time:65981ms step_avg:48.02ms
step:1375/2035 train_time:66068ms step_avg:48.05ms
step:1376/2035 train_time:66155ms step_avg:48.08ms
step:1377/2035 train_time:66244ms step_avg:48.11ms
step:1378/2035 train_time:66331ms step_avg:48.14ms
step:1379/2035 train_time:66420ms step_avg:48.17ms
step:1380/2035 train_time:66507ms step_avg:48.19ms
step:1381/2035 train_time:66596ms step_avg:48.22ms
step:1382/2035 train_time:66685ms step_avg:48.25ms
step:1383/2035 train_time:66773ms step_avg:48.28ms
step:1384/2035 train_time:66861ms step_avg:48.31ms
step:1385/2035 train_time:66948ms step_avg:48.34ms
step:1386/2035 train_time:67036ms step_avg:48.37ms
step:1387/2035 train_time:67125ms step_avg:48.40ms
step:1388/2035 train_time:67213ms step_avg:48.42ms
step:1389/2035 train_time:67302ms step_avg:48.45ms
step:1390/2035 train_time:67390ms step_avg:48.48ms
step:1391/2035 train_time:67478ms step_avg:48.51ms
step:1392/2035 train_time:67565ms step_avg:48.54ms
step:1393/2035 train_time:67654ms step_avg:48.57ms
step:1394/2035 train_time:67742ms step_avg:48.60ms
step:1395/2035 train_time:67830ms step_avg:48.62ms
step:1396/2035 train_time:67917ms step_avg:48.65ms
step:1397/2035 train_time:68006ms step_avg:48.68ms
step:1398/2035 train_time:68093ms step_avg:48.71ms
step:1399/2035 train_time:68182ms step_avg:48.74ms
step:1400/2035 train_time:68269ms step_avg:48.76ms
step:1401/2035 train_time:68357ms step_avg:48.79ms
step:1402/2035 train_time:68446ms step_avg:48.82ms
step:1403/2035 train_time:68534ms step_avg:48.85ms
step:1404/2035 train_time:68622ms step_avg:48.88ms
step:1405/2035 train_time:68710ms step_avg:48.90ms
step:1406/2035 train_time:68798ms step_avg:48.93ms
step:1407/2035 train_time:68886ms step_avg:48.96ms
step:1408/2035 train_time:68975ms step_avg:48.99ms
step:1409/2035 train_time:69063ms step_avg:49.02ms
step:1410/2035 train_time:69150ms step_avg:49.04ms
step:1411/2035 train_time:69239ms step_avg:49.07ms
step:1412/2035 train_time:69326ms step_avg:49.10ms
step:1413/2035 train_time:69414ms step_avg:49.13ms
step:1414/2035 train_time:69503ms step_avg:49.15ms
step:1415/2035 train_time:69591ms step_avg:49.18ms
step:1416/2035 train_time:69679ms step_avg:49.21ms
step:1417/2035 train_time:69767ms step_avg:49.24ms
step:1418/2035 train_time:69854ms step_avg:49.26ms
step:1419/2035 train_time:69943ms step_avg:49.29ms
step:1420/2035 train_time:70031ms step_avg:49.32ms
step:1421/2035 train_time:70118ms step_avg:49.34ms
step:1422/2035 train_time:70207ms step_avg:49.37ms
step:1423/2035 train_time:70294ms step_avg:49.40ms
step:1424/2035 train_time:70382ms step_avg:49.43ms
step:1425/2035 train_time:70470ms step_avg:49.45ms
step:1426/2035 train_time:70558ms step_avg:49.48ms
step:1427/2035 train_time:70646ms step_avg:49.51ms
step:1428/2035 train_time:70734ms step_avg:49.53ms
step:1429/2035 train_time:70823ms step_avg:49.56ms
step:1430/2035 train_time:70910ms step_avg:49.59ms
step:1431/2035 train_time:70999ms step_avg:49.62ms
step:1432/2035 train_time:71087ms step_avg:49.64ms
step:1433/2035 train_time:71175ms step_avg:49.67ms
step:1434/2035 train_time:71264ms step_avg:49.70ms
step:1435/2035 train_time:71351ms step_avg:49.72ms
step:1436/2035 train_time:71438ms step_avg:49.75ms
step:1437/2035 train_time:71527ms step_avg:49.77ms
step:1438/2035 train_time:71614ms step_avg:49.80ms
step:1439/2035 train_time:71702ms step_avg:49.83ms
step:1440/2035 train_time:71789ms step_avg:49.85ms
step:1441/2035 train_time:71878ms step_avg:49.88ms
step:1442/2035 train_time:71966ms step_avg:49.91ms
step:1443/2035 train_time:72054ms step_avg:49.93ms
step:1444/2035 train_time:72143ms step_avg:49.96ms
step:1445/2035 train_time:72230ms step_avg:49.99ms
step:1446/2035 train_time:72318ms step_avg:50.01ms
step:1447/2035 train_time:72407ms step_avg:50.04ms
step:1448/2035 train_time:72495ms step_avg:50.07ms
step:1449/2035 train_time:72582ms step_avg:50.09ms
step:1450/2035 train_time:72670ms step_avg:50.12ms
step:1451/2035 train_time:72758ms step_avg:50.14ms
step:1452/2035 train_time:72846ms step_avg:50.17ms
step:1453/2035 train_time:72934ms step_avg:50.20ms
step:1454/2035 train_time:73022ms step_avg:50.22ms
step:1455/2035 train_time:73110ms step_avg:50.25ms
step:1456/2035 train_time:73198ms step_avg:50.27ms
step:1457/2035 train_time:73288ms step_avg:50.30ms
step:1458/2035 train_time:73377ms step_avg:50.33ms
step:1459/2035 train_time:73465ms step_avg:50.35ms
step:1460/2035 train_time:73552ms step_avg:50.38ms
step:1461/2035 train_time:73641ms step_avg:50.40ms
step:1462/2035 train_time:73728ms step_avg:50.43ms
step:1463/2035 train_time:73817ms step_avg:50.46ms
step:1464/2035 train_time:73905ms step_avg:50.48ms
step:1465/2035 train_time:73994ms step_avg:50.51ms
step:1466/2035 train_time:74082ms step_avg:50.53ms
step:1467/2035 train_time:74169ms step_avg:50.56ms
step:1468/2035 train_time:74258ms step_avg:50.58ms
step:1469/2035 train_time:74346ms step_avg:50.61ms
step:1470/2035 train_time:74435ms step_avg:50.64ms
step:1471/2035 train_time:74524ms step_avg:50.66ms
step:1472/2035 train_time:74611ms step_avg:50.69ms
step:1473/2035 train_time:74700ms step_avg:50.71ms
step:1474/2035 train_time:74788ms step_avg:50.74ms
step:1475/2035 train_time:74876ms step_avg:50.76ms
step:1476/2035 train_time:74964ms step_avg:50.79ms
step:1477/2035 train_time:75052ms step_avg:50.81ms
step:1478/2035 train_time:75140ms step_avg:50.84ms
step:1479/2035 train_time:75228ms step_avg:50.86ms
step:1480/2035 train_time:75316ms step_avg:50.89ms
step:1481/2035 train_time:75406ms step_avg:50.92ms
step:1482/2035 train_time:75494ms step_avg:50.94ms
step:1483/2035 train_time:75582ms step_avg:50.97ms
step:1484/2035 train_time:75669ms step_avg:50.99ms
step:1485/2035 train_time:75757ms step_avg:51.01ms
step:1486/2035 train_time:75845ms step_avg:51.04ms
step:1487/2035 train_time:75933ms step_avg:51.06ms
step:1488/2035 train_time:76021ms step_avg:51.09ms
step:1489/2035 train_time:76109ms step_avg:51.11ms
step:1490/2035 train_time:76197ms step_avg:51.14ms
step:1491/2035 train_time:76286ms step_avg:51.16ms
step:1492/2035 train_time:76374ms step_avg:51.19ms
step:1493/2035 train_time:76463ms step_avg:51.21ms
step:1494/2035 train_time:76550ms step_avg:51.24ms
step:1495/2035 train_time:76639ms step_avg:51.26ms
step:1496/2035 train_time:76726ms step_avg:51.29ms
step:1497/2035 train_time:76815ms step_avg:51.31ms
step:1498/2035 train_time:76903ms step_avg:51.34ms
step:1499/2035 train_time:76990ms step_avg:51.36ms
step:1500/2035 train_time:77078ms step_avg:51.39ms
step:1500/2035 val_loss:3.4545 train_time:77168ms step_avg:51.45ms
step:1501/2035 train_time:77189ms step_avg:51.42ms
step:1502/2035 train_time:77257ms step_avg:51.44ms
step:1503/2035 train_time:77347ms step_avg:51.46ms
step:1504/2035 train_time:77434ms step_avg:51.49ms
step:1505/2035 train_time:77522ms step_avg:51.51ms
step:1506/2035 train_time:77608ms step_avg:51.53ms
step:1507/2035 train_time:77695ms step_avg:51.56ms
step:1508/2035 train_time:77783ms step_avg:51.58ms
step:1509/2035 train_time:77872ms step_avg:51.60ms
step:1510/2035 train_time:77960ms step_avg:51.63ms
step:1511/2035 train_time:78047ms step_avg:51.65ms
step:1512/2035 train_time:78137ms step_avg:51.68ms
step:1513/2035 train_time:78227ms step_avg:51.70ms
step:1514/2035 train_time:78316ms step_avg:51.73ms
step:1515/2035 train_time:78405ms step_avg:51.75ms
step:1516/2035 train_time:78491ms step_avg:51.78ms
step:1517/2035 train_time:78580ms step_avg:51.80ms
step:1518/2035 train_time:78667ms step_avg:51.82ms
step:1519/2035 train_time:78754ms step_avg:51.85ms
step:1520/2035 train_time:78842ms step_avg:51.87ms
step:1521/2035 train_time:78930ms step_avg:51.89ms
step:1522/2035 train_time:79019ms step_avg:51.92ms
step:1523/2035 train_time:79107ms step_avg:51.94ms
step:1524/2035 train_time:79196ms step_avg:51.97ms
step:1525/2035 train_time:79286ms step_avg:51.99ms
step:1526/2035 train_time:79373ms step_avg:52.01ms
step:1527/2035 train_time:79462ms step_avg:52.04ms
step:1528/2035 train_time:79549ms step_avg:52.06ms
step:1529/2035 train_time:79637ms step_avg:52.08ms
step:1530/2035 train_time:79724ms step_avg:52.11ms
step:1531/2035 train_time:79812ms step_avg:52.13ms
step:1532/2035 train_time:79899ms step_avg:52.15ms
step:1533/2035 train_time:79987ms step_avg:52.18ms
step:1534/2035 train_time:80075ms step_avg:52.20ms
step:1535/2035 train_time:80165ms step_avg:52.23ms
step:1536/2035 train_time:80253ms step_avg:52.25ms
step:1537/2035 train_time:80341ms step_avg:52.27ms
step:1538/2035 train_time:80429ms step_avg:52.29ms
step:1539/2035 train_time:80517ms step_avg:52.32ms
step:1540/2035 train_time:80605ms step_avg:52.34ms
step:1541/2035 train_time:80693ms step_avg:52.36ms
step:1542/2035 train_time:80781ms step_avg:52.39ms
step:1543/2035 train_time:80870ms step_avg:52.41ms
step:1544/2035 train_time:80958ms step_avg:52.43ms
step:1545/2035 train_time:81047ms step_avg:52.46ms
step:1546/2035 train_time:81134ms step_avg:52.48ms
step:1547/2035 train_time:81223ms step_avg:52.50ms
step:1548/2035 train_time:81311ms step_avg:52.53ms
step:1549/2035 train_time:81400ms step_avg:52.55ms
step:1550/2035 train_time:81487ms step_avg:52.57ms
step:1551/2035 train_time:81575ms step_avg:52.59ms
step:1552/2035 train_time:81662ms step_avg:52.62ms
step:1553/2035 train_time:81751ms step_avg:52.64ms
step:1554/2035 train_time:81839ms step_avg:52.66ms
step:1555/2035 train_time:81928ms step_avg:52.69ms
step:1556/2035 train_time:82015ms step_avg:52.71ms
step:1557/2035 train_time:82104ms step_avg:52.73ms
step:1558/2035 train_time:82192ms step_avg:52.75ms
step:1559/2035 train_time:82280ms step_avg:52.78ms
step:1560/2035 train_time:82369ms step_avg:52.80ms
step:1561/2035 train_time:82457ms step_avg:52.82ms
step:1562/2035 train_time:82545ms step_avg:52.85ms
step:1563/2035 train_time:82633ms step_avg:52.87ms
step:1564/2035 train_time:82721ms step_avg:52.89ms
step:1565/2035 train_time:82810ms step_avg:52.91ms
step:1566/2035 train_time:82898ms step_avg:52.94ms
step:1567/2035 train_time:82987ms step_avg:52.96ms
step:1568/2035 train_time:83075ms step_avg:52.98ms
step:1569/2035 train_time:83163ms step_avg:53.00ms
step:1570/2035 train_time:83251ms step_avg:53.03ms
step:1571/2035 train_time:83340ms step_avg:53.05ms
step:1572/2035 train_time:83429ms step_avg:53.07ms
step:1573/2035 train_time:83517ms step_avg:53.09ms
step:1574/2035 train_time:83605ms step_avg:53.12ms
step:1575/2035 train_time:83693ms step_avg:53.14ms
step:1576/2035 train_time:83781ms step_avg:53.16ms
step:1577/2035 train_time:83871ms step_avg:53.18ms
step:1578/2035 train_time:83959ms step_avg:53.21ms
step:1579/2035 train_time:84047ms step_avg:53.23ms
step:1580/2035 train_time:84135ms step_avg:53.25ms
step:1581/2035 train_time:84223ms step_avg:53.27ms
step:1582/2035 train_time:84311ms step_avg:53.29ms
step:1583/2035 train_time:84399ms step_avg:53.32ms
step:1584/2035 train_time:84487ms step_avg:53.34ms
step:1585/2035 train_time:84575ms step_avg:53.36ms
step:1586/2035 train_time:84663ms step_avg:53.38ms
step:1587/2035 train_time:84752ms step_avg:53.40ms
step:1588/2035 train_time:84840ms step_avg:53.43ms
step:1589/2035 train_time:84929ms step_avg:53.45ms
step:1590/2035 train_time:85016ms step_avg:53.47ms
step:1591/2035 train_time:85104ms step_avg:53.49ms
step:1592/2035 train_time:85191ms step_avg:53.51ms
step:1593/2035 train_time:85279ms step_avg:53.53ms
step:1594/2035 train_time:85368ms step_avg:53.56ms
step:1595/2035 train_time:85456ms step_avg:53.58ms
step:1596/2035 train_time:85544ms step_avg:53.60ms
step:1597/2035 train_time:85631ms step_avg:53.62ms
step:1598/2035 train_time:85719ms step_avg:53.64ms
step:1599/2035 train_time:85808ms step_avg:53.66ms
step:1600/2035 train_time:85896ms step_avg:53.68ms
step:1601/2035 train_time:85983ms step_avg:53.71ms
step:1602/2035 train_time:86071ms step_avg:53.73ms
step:1603/2035 train_time:86159ms step_avg:53.75ms
step:1604/2035 train_time:86248ms step_avg:53.77ms
step:1605/2035 train_time:86337ms step_avg:53.79ms
step:1606/2035 train_time:86425ms step_avg:53.81ms
step:1607/2035 train_time:86512ms step_avg:53.83ms
step:1608/2035 train_time:86600ms step_avg:53.86ms
step:1609/2035 train_time:86688ms step_avg:53.88ms
step:1610/2035 train_time:86776ms step_avg:53.90ms
step:1611/2035 train_time:86865ms step_avg:53.92ms
step:1612/2035 train_time:86952ms step_avg:53.94ms
step:1613/2035 train_time:87041ms step_avg:53.96ms
step:1614/2035 train_time:87129ms step_avg:53.98ms
step:1615/2035 train_time:87217ms step_avg:54.00ms
step:1616/2035 train_time:87305ms step_avg:54.03ms
step:1617/2035 train_time:87392ms step_avg:54.05ms
step:1618/2035 train_time:87480ms step_avg:54.07ms
step:1619/2035 train_time:87569ms step_avg:54.09ms
step:1620/2035 train_time:87657ms step_avg:54.11ms
step:1621/2035 train_time:87745ms step_avg:54.13ms
step:1622/2035 train_time:87833ms step_avg:54.15ms
step:1623/2035 train_time:87921ms step_avg:54.17ms
step:1624/2035 train_time:88009ms step_avg:54.19ms
step:1625/2035 train_time:88098ms step_avg:54.21ms
step:1626/2035 train_time:88186ms step_avg:54.24ms
step:1627/2035 train_time:88274ms step_avg:54.26ms
step:1628/2035 train_time:88362ms step_avg:54.28ms
step:1629/2035 train_time:88450ms step_avg:54.30ms
step:1630/2035 train_time:88538ms step_avg:54.32ms
step:1631/2035 train_time:88627ms step_avg:54.34ms
step:1632/2035 train_time:88714ms step_avg:54.36ms
step:1633/2035 train_time:88802ms step_avg:54.38ms
step:1634/2035 train_time:88889ms step_avg:54.40ms
step:1635/2035 train_time:88978ms step_avg:54.42ms
step:1636/2035 train_time:89066ms step_avg:54.44ms
step:1637/2035 train_time:89154ms step_avg:54.46ms
step:1638/2035 train_time:89242ms step_avg:54.48ms
step:1639/2035 train_time:89331ms step_avg:54.50ms
step:1640/2035 train_time:89419ms step_avg:54.52ms
step:1641/2035 train_time:89507ms step_avg:54.54ms
step:1642/2035 train_time:89594ms step_avg:54.56ms
step:1643/2035 train_time:89682ms step_avg:54.58ms
step:1644/2035 train_time:89770ms step_avg:54.60ms
step:1645/2035 train_time:89858ms step_avg:54.63ms
step:1646/2035 train_time:89946ms step_avg:54.65ms
step:1647/2035 train_time:90034ms step_avg:54.67ms
step:1648/2035 train_time:90123ms step_avg:54.69ms
step:1649/2035 train_time:90211ms step_avg:54.71ms
step:1650/2035 train_time:90300ms step_avg:54.73ms
step:1651/2035 train_time:90388ms step_avg:54.75ms
step:1652/2035 train_time:90476ms step_avg:54.77ms
step:1653/2035 train_time:90565ms step_avg:54.79ms
step:1654/2035 train_time:90653ms step_avg:54.81ms
step:1655/2035 train_time:90741ms step_avg:54.83ms
step:1656/2035 train_time:90830ms step_avg:54.85ms
step:1657/2035 train_time:90918ms step_avg:54.87ms
step:1658/2035 train_time:91007ms step_avg:54.89ms
step:1659/2035 train_time:91095ms step_avg:54.91ms
step:1660/2035 train_time:91182ms step_avg:54.93ms
step:1661/2035 train_time:91271ms step_avg:54.95ms
step:1662/2035 train_time:91359ms step_avg:54.97ms
step:1663/2035 train_time:91448ms step_avg:54.99ms
step:1664/2035 train_time:91536ms step_avg:55.01ms
step:1665/2035 train_time:91624ms step_avg:55.03ms
step:1666/2035 train_time:91711ms step_avg:55.05ms
step:1667/2035 train_time:91800ms step_avg:55.07ms
step:1668/2035 train_time:91887ms step_avg:55.09ms
step:1669/2035 train_time:91975ms step_avg:55.11ms
step:1670/2035 train_time:92063ms step_avg:55.13ms
step:1671/2035 train_time:92152ms step_avg:55.15ms
step:1672/2035 train_time:92240ms step_avg:55.17ms
step:1673/2035 train_time:92329ms step_avg:55.19ms
step:1674/2035 train_time:92417ms step_avg:55.21ms
step:1675/2035 train_time:92506ms step_avg:55.23ms
step:1676/2035 train_time:92594ms step_avg:55.25ms
step:1677/2035 train_time:92682ms step_avg:55.27ms
step:1678/2035 train_time:92770ms step_avg:55.29ms
step:1679/2035 train_time:92859ms step_avg:55.31ms
step:1680/2035 train_time:92947ms step_avg:55.33ms
step:1681/2035 train_time:93034ms step_avg:55.34ms
step:1682/2035 train_time:93122ms step_avg:55.36ms
step:1683/2035 train_time:93211ms step_avg:55.38ms
step:1684/2035 train_time:93299ms step_avg:55.40ms
step:1685/2035 train_time:93388ms step_avg:55.42ms
step:1686/2035 train_time:93475ms step_avg:55.44ms
step:1687/2035 train_time:93564ms step_avg:55.46ms
step:1688/2035 train_time:93651ms step_avg:55.48ms
step:1689/2035 train_time:93739ms step_avg:55.50ms
step:1690/2035 train_time:93828ms step_avg:55.52ms
step:1691/2035 train_time:93916ms step_avg:55.54ms
step:1692/2035 train_time:94003ms step_avg:55.56ms
step:1693/2035 train_time:94091ms step_avg:55.58ms
step:1694/2035 train_time:94178ms step_avg:55.60ms
step:1695/2035 train_time:94267ms step_avg:55.61ms
step:1696/2035 train_time:94355ms step_avg:55.63ms
step:1697/2035 train_time:94443ms step_avg:55.65ms
step:1698/2035 train_time:94532ms step_avg:55.67ms
step:1699/2035 train_time:94620ms step_avg:55.69ms
step:1700/2035 train_time:94707ms step_avg:55.71ms
step:1701/2035 train_time:94795ms step_avg:55.73ms
step:1702/2035 train_time:94883ms step_avg:55.75ms
step:1703/2035 train_time:94970ms step_avg:55.77ms
step:1704/2035 train_time:95058ms step_avg:55.79ms
step:1705/2035 train_time:95146ms step_avg:55.80ms
step:1706/2035 train_time:95234ms step_avg:55.82ms
step:1707/2035 train_time:95324ms step_avg:55.84ms
step:1708/2035 train_time:95411ms step_avg:55.86ms
step:1709/2035 train_time:95500ms step_avg:55.88ms
step:1710/2035 train_time:95588ms step_avg:55.90ms
step:1711/2035 train_time:95676ms step_avg:55.92ms
step:1712/2035 train_time:95765ms step_avg:55.94ms
step:1713/2035 train_time:95853ms step_avg:55.96ms
step:1714/2035 train_time:95941ms step_avg:55.98ms
step:1715/2035 train_time:96030ms step_avg:55.99ms
step:1716/2035 train_time:96118ms step_avg:56.01ms
step:1717/2035 train_time:96207ms step_avg:56.03ms
step:1718/2035 train_time:96294ms step_avg:56.05ms
step:1719/2035 train_time:96383ms step_avg:56.07ms
step:1720/2035 train_time:96471ms step_avg:56.09ms
step:1721/2035 train_time:96559ms step_avg:56.11ms
step:1722/2035 train_time:96647ms step_avg:56.12ms
step:1723/2035 train_time:96734ms step_avg:56.14ms
step:1724/2035 train_time:96822ms step_avg:56.16ms
step:1725/2035 train_time:96911ms step_avg:56.18ms
step:1726/2035 train_time:96999ms step_avg:56.20ms
step:1727/2035 train_time:97087ms step_avg:56.22ms
step:1728/2035 train_time:97174ms step_avg:56.23ms
step:1729/2035 train_time:97263ms step_avg:56.25ms
step:1730/2035 train_time:97351ms step_avg:56.27ms
step:1731/2035 train_time:97440ms step_avg:56.29ms
step:1732/2035 train_time:97529ms step_avg:56.31ms
step:1733/2035 train_time:97618ms step_avg:56.33ms
step:1734/2035 train_time:97706ms step_avg:56.35ms
step:1735/2035 train_time:97794ms step_avg:56.37ms
step:1736/2035 train_time:97881ms step_avg:56.38ms
step:1737/2035 train_time:97971ms step_avg:56.40ms
step:1738/2035 train_time:98059ms step_avg:56.42ms
step:1739/2035 train_time:98147ms step_avg:56.44ms
step:1740/2035 train_time:98235ms step_avg:56.46ms
step:1741/2035 train_time:98324ms step_avg:56.48ms
step:1742/2035 train_time:98412ms step_avg:56.49ms
step:1743/2035 train_time:98500ms step_avg:56.51ms
step:1744/2035 train_time:98589ms step_avg:56.53ms
step:1745/2035 train_time:98676ms step_avg:56.55ms
step:1746/2035 train_time:98764ms step_avg:56.57ms
step:1747/2035 train_time:98851ms step_avg:56.58ms
step:1748/2035 train_time:98939ms step_avg:56.60ms
step:1749/2035 train_time:99027ms step_avg:56.62ms
step:1750/2035 train_time:99114ms step_avg:56.64ms
step:1750/2035 val_loss:3.3576 train_time:99204ms step_avg:56.69ms
step:1751/2035 train_time:99224ms step_avg:56.67ms
step:1752/2035 train_time:99297ms step_avg:56.68ms
step:1753/2035 train_time:99388ms step_avg:56.70ms
step:1754/2035 train_time:99476ms step_avg:56.71ms
step:1755/2035 train_time:99563ms step_avg:56.73ms
step:1756/2035 train_time:99651ms step_avg:56.75ms
step:1757/2035 train_time:99737ms step_avg:56.77ms
step:1758/2035 train_time:99824ms step_avg:56.78ms
step:1759/2035 train_time:99911ms step_avg:56.80ms
step:1760/2035 train_time:99998ms step_avg:56.82ms
step:1761/2035 train_time:100085ms step_avg:56.83ms
step:1762/2035 train_time:100176ms step_avg:56.85ms
step:1763/2035 train_time:100267ms step_avg:56.87ms
step:1764/2035 train_time:100357ms step_avg:56.89ms
step:1765/2035 train_time:100445ms step_avg:56.91ms
step:1766/2035 train_time:100534ms step_avg:56.93ms
step:1767/2035 train_time:100621ms step_avg:56.94ms
step:1768/2035 train_time:100709ms step_avg:56.96ms
step:1769/2035 train_time:100797ms step_avg:56.98ms
step:1770/2035 train_time:100884ms step_avg:57.00ms
step:1771/2035 train_time:100971ms step_avg:57.01ms
step:1772/2035 train_time:101058ms step_avg:57.03ms
step:1773/2035 train_time:101147ms step_avg:57.05ms
step:1774/2035 train_time:101237ms step_avg:57.07ms
step:1775/2035 train_time:101326ms step_avg:57.09ms
step:1776/2035 train_time:101416ms step_avg:57.10ms
step:1777/2035 train_time:101503ms step_avg:57.12ms
step:1778/2035 train_time:101591ms step_avg:57.14ms
step:1779/2035 train_time:101679ms step_avg:57.15ms
step:1780/2035 train_time:101766ms step_avg:57.17ms
step:1781/2035 train_time:101854ms step_avg:57.19ms
step:1782/2035 train_time:101941ms step_avg:57.21ms
step:1783/2035 train_time:102029ms step_avg:57.22ms
step:1784/2035 train_time:102117ms step_avg:57.24ms
step:1785/2035 train_time:102206ms step_avg:57.26ms
step:1786/2035 train_time:102295ms step_avg:57.28ms
step:1787/2035 train_time:102384ms step_avg:57.29ms
step:1788/2035 train_time:102473ms step_avg:57.31ms
step:1789/2035 train_time:102562ms step_avg:57.33ms
step:1790/2035 train_time:102650ms step_avg:57.35ms
step:1791/2035 train_time:102738ms step_avg:57.36ms
step:1792/2035 train_time:102825ms step_avg:57.38ms
step:1793/2035 train_time:102912ms step_avg:57.40ms
step:1794/2035 train_time:103000ms step_avg:57.41ms
step:1795/2035 train_time:103088ms step_avg:57.43ms
step:1796/2035 train_time:103177ms step_avg:57.45ms
step:1797/2035 train_time:103265ms step_avg:57.47ms
step:1798/2035 train_time:103353ms step_avg:57.48ms
step:1799/2035 train_time:103442ms step_avg:57.50ms
step:1800/2035 train_time:103532ms step_avg:57.52ms
step:1801/2035 train_time:103619ms step_avg:57.53ms
step:1802/2035 train_time:103707ms step_avg:57.55ms
step:1803/2035 train_time:103795ms step_avg:57.57ms
step:1804/2035 train_time:103882ms step_avg:57.58ms
step:1805/2035 train_time:103970ms step_avg:57.60ms
step:1806/2035 train_time:104058ms step_avg:57.62ms
step:1807/2035 train_time:104147ms step_avg:57.64ms
step:1808/2035 train_time:104234ms step_avg:57.65ms
step:1809/2035 train_time:104323ms step_avg:57.67ms
step:1810/2035 train_time:104411ms step_avg:57.69ms
step:1811/2035 train_time:104500ms step_avg:57.70ms
step:1812/2035 train_time:104588ms step_avg:57.72ms
step:1813/2035 train_time:104676ms step_avg:57.74ms
step:1814/2035 train_time:104763ms step_avg:57.75ms
step:1815/2035 train_time:104850ms step_avg:57.77ms
step:1816/2035 train_time:104938ms step_avg:57.79ms
step:1817/2035 train_time:105026ms step_avg:57.80ms
step:1818/2035 train_time:105115ms step_avg:57.82ms
step:1819/2035 train_time:105203ms step_avg:57.84ms
step:1820/2035 train_time:105291ms step_avg:57.85ms
step:1821/2035 train_time:105380ms step_avg:57.87ms
step:1822/2035 train_time:105469ms step_avg:57.89ms
step:1823/2035 train_time:105557ms step_avg:57.90ms
step:1824/2035 train_time:105644ms step_avg:57.92ms
step:1825/2035 train_time:105732ms step_avg:57.94ms
step:1826/2035 train_time:105820ms step_avg:57.95ms
step:1827/2035 train_time:105907ms step_avg:57.97ms
step:1828/2035 train_time:105996ms step_avg:57.98ms
step:1829/2035 train_time:106084ms step_avg:58.00ms
step:1830/2035 train_time:106173ms step_avg:58.02ms
step:1831/2035 train_time:106261ms step_avg:58.03ms
step:1832/2035 train_time:106349ms step_avg:58.05ms
step:1833/2035 train_time:106438ms step_avg:58.07ms
step:1834/2035 train_time:106526ms step_avg:58.08ms
step:1835/2035 train_time:106614ms step_avg:58.10ms
step:1836/2035 train_time:106700ms step_avg:58.12ms
step:1837/2035 train_time:106789ms step_avg:58.13ms
step:1838/2035 train_time:106877ms step_avg:58.15ms
step:1839/2035 train_time:106965ms step_avg:58.16ms
step:1840/2035 train_time:107052ms step_avg:58.18ms
step:1841/2035 train_time:107140ms step_avg:58.20ms
step:1842/2035 train_time:107228ms step_avg:58.21ms
step:1843/2035 train_time:107317ms step_avg:58.23ms
step:1844/2035 train_time:107404ms step_avg:58.25ms
step:1845/2035 train_time:107494ms step_avg:58.26ms
step:1846/2035 train_time:107581ms step_avg:58.28ms
step:1847/2035 train_time:107669ms step_avg:58.29ms
step:1848/2035 train_time:107757ms step_avg:58.31ms
step:1849/2035 train_time:107845ms step_avg:58.33ms
step:1850/2035 train_time:107933ms step_avg:58.34ms
step:1851/2035 train_time:108020ms step_avg:58.36ms
step:1852/2035 train_time:108108ms step_avg:58.37ms
step:1853/2035 train_time:108197ms step_avg:58.39ms
step:1854/2035 train_time:108286ms step_avg:58.41ms
step:1855/2035 train_time:108373ms step_avg:58.42ms
step:1856/2035 train_time:108461ms step_avg:58.44ms
step:1857/2035 train_time:108549ms step_avg:58.45ms
step:1858/2035 train_time:108637ms step_avg:58.47ms
step:1859/2035 train_time:108725ms step_avg:58.49ms
step:1860/2035 train_time:108813ms step_avg:58.50ms
step:1861/2035 train_time:108901ms step_avg:58.52ms
step:1862/2035 train_time:108989ms step_avg:58.53ms
step:1863/2035 train_time:109078ms step_avg:58.55ms
step:1864/2035 train_time:109166ms step_avg:58.57ms
step:1865/2035 train_time:109255ms step_avg:58.58ms
step:1866/2035 train_time:109343ms step_avg:58.60ms
step:1867/2035 train_time:109431ms step_avg:58.61ms
step:1868/2035 train_time:109519ms step_avg:58.63ms
step:1869/2035 train_time:109607ms step_avg:58.64ms
step:1870/2035 train_time:109695ms step_avg:58.66ms
step:1871/2035 train_time:109784ms step_avg:58.68ms
step:1872/2035 train_time:109873ms step_avg:58.69ms
step:1873/2035 train_time:109961ms step_avg:58.71ms
step:1874/2035 train_time:110049ms step_avg:58.72ms
step:1875/2035 train_time:110137ms step_avg:58.74ms
step:1876/2035 train_time:110224ms step_avg:58.75ms
step:1877/2035 train_time:110313ms step_avg:58.77ms
step:1878/2035 train_time:110401ms step_avg:58.79ms
step:1879/2035 train_time:110489ms step_avg:58.80ms
step:1880/2035 train_time:110577ms step_avg:58.82ms
step:1881/2035 train_time:110665ms step_avg:58.83ms
step:1882/2035 train_time:110754ms step_avg:58.85ms
step:1883/2035 train_time:110841ms step_avg:58.86ms
step:1884/2035 train_time:110930ms step_avg:58.88ms
step:1885/2035 train_time:111018ms step_avg:58.90ms
step:1886/2035 train_time:111106ms step_avg:58.91ms
step:1887/2035 train_time:111194ms step_avg:58.93ms
step:1888/2035 train_time:111281ms step_avg:58.94ms
step:1889/2035 train_time:111369ms step_avg:58.96ms
step:1890/2035 train_time:111458ms step_avg:58.97ms
step:1891/2035 train_time:111546ms step_avg:58.99ms
step:1892/2035 train_time:111635ms step_avg:59.00ms
step:1893/2035 train_time:111723ms step_avg:59.02ms
step:1894/2035 train_time:111811ms step_avg:59.03ms
step:1895/2035 train_time:111901ms step_avg:59.05ms
step:1896/2035 train_time:111990ms step_avg:59.07ms
step:1897/2035 train_time:112078ms step_avg:59.08ms
step:1898/2035 train_time:112166ms step_avg:59.10ms
step:1899/2035 train_time:112255ms step_avg:59.11ms
step:1900/2035 train_time:112342ms step_avg:59.13ms
step:1901/2035 train_time:112430ms step_avg:59.14ms
step:1902/2035 train_time:112518ms step_avg:59.16ms
step:1903/2035 train_time:112607ms step_avg:59.17ms
step:1904/2035 train_time:112694ms step_avg:59.19ms
step:1905/2035 train_time:112782ms step_avg:59.20ms
step:1906/2035 train_time:112870ms step_avg:59.22ms
step:1907/2035 train_time:112960ms step_avg:59.23ms
step:1908/2035 train_time:113049ms step_avg:59.25ms
step:1909/2035 train_time:113137ms step_avg:59.27ms
step:1910/2035 train_time:113224ms step_avg:59.28ms
step:1911/2035 train_time:113313ms step_avg:59.30ms
step:1912/2035 train_time:113401ms step_avg:59.31ms
step:1913/2035 train_time:113489ms step_avg:59.33ms
step:1914/2035 train_time:113578ms step_avg:59.34ms
step:1915/2035 train_time:113666ms step_avg:59.36ms
step:1916/2035 train_time:113755ms step_avg:59.37ms
step:1917/2035 train_time:113843ms step_avg:59.39ms
step:1918/2035 train_time:113931ms step_avg:59.40ms
step:1919/2035 train_time:114020ms step_avg:59.42ms
step:1920/2035 train_time:114107ms step_avg:59.43ms
step:1921/2035 train_time:114197ms step_avg:59.45ms
step:1922/2035 train_time:114284ms step_avg:59.46ms
step:1923/2035 train_time:114372ms step_avg:59.48ms
step:1924/2035 train_time:114460ms step_avg:59.49ms
step:1925/2035 train_time:114549ms step_avg:59.51ms
step:1926/2035 train_time:114636ms step_avg:59.52ms
step:1927/2035 train_time:114724ms step_avg:59.54ms
step:1928/2035 train_time:114812ms step_avg:59.55ms
step:1929/2035 train_time:114900ms step_avg:59.56ms
step:1930/2035 train_time:114989ms step_avg:59.58ms
step:1931/2035 train_time:115078ms step_avg:59.60ms
step:1932/2035 train_time:115166ms step_avg:59.61ms
step:1933/2035 train_time:115254ms step_avg:59.62ms
step:1934/2035 train_time:115342ms step_avg:59.64ms
step:1935/2035 train_time:115430ms step_avg:59.65ms
step:1936/2035 train_time:115518ms step_avg:59.67ms
step:1937/2035 train_time:115605ms step_avg:59.68ms
step:1938/2035 train_time:115693ms step_avg:59.70ms
step:1939/2035 train_time:115780ms step_avg:59.71ms
step:1940/2035 train_time:115868ms step_avg:59.73ms
step:1941/2035 train_time:115957ms step_avg:59.74ms
step:1942/2035 train_time:116044ms step_avg:59.76ms
step:1943/2035 train_time:116132ms step_avg:59.77ms
step:1944/2035 train_time:116220ms step_avg:59.78ms
step:1945/2035 train_time:116308ms step_avg:59.80ms
step:1946/2035 train_time:116397ms step_avg:59.81ms
step:1947/2035 train_time:116485ms step_avg:59.83ms
step:1948/2035 train_time:116573ms step_avg:59.84ms
step:1949/2035 train_time:116661ms step_avg:59.86ms
step:1950/2035 train_time:116749ms step_avg:59.87ms
step:1951/2035 train_time:116838ms step_avg:59.89ms
step:1952/2035 train_time:116926ms step_avg:59.90ms
step:1953/2035 train_time:117014ms step_avg:59.92ms
step:1954/2035 train_time:117101ms step_avg:59.93ms
step:1955/2035 train_time:117190ms step_avg:59.94ms
step:1956/2035 train_time:117279ms step_avg:59.96ms
step:1957/2035 train_time:117367ms step_avg:59.97ms
step:1958/2035 train_time:117455ms step_avg:59.99ms
step:1959/2035 train_time:117543ms step_avg:60.00ms
step:1960/2035 train_time:117630ms step_avg:60.02ms
step:1961/2035 train_time:117719ms step_avg:60.03ms
step:1962/2035 train_time:117807ms step_avg:60.04ms
step:1963/2035 train_time:117896ms step_avg:60.06ms
step:1964/2035 train_time:117983ms step_avg:60.07ms
step:1965/2035 train_time:118072ms step_avg:60.09ms
step:1966/2035 train_time:118159ms step_avg:60.10ms
step:1967/2035 train_time:118247ms step_avg:60.12ms
step:1968/2035 train_time:118335ms step_avg:60.13ms
step:1969/2035 train_time:118423ms step_avg:60.14ms
step:1970/2035 train_time:118511ms step_avg:60.16ms
step:1971/2035 train_time:118599ms step_avg:60.17ms
step:1972/2035 train_time:118688ms step_avg:60.19ms
step:1973/2035 train_time:118777ms step_avg:60.20ms
step:1974/2035 train_time:118864ms step_avg:60.21ms
step:1975/2035 train_time:118953ms step_avg:60.23ms
step:1976/2035 train_time:119041ms step_avg:60.24ms
step:1977/2035 train_time:119129ms step_avg:60.26ms
step:1978/2035 train_time:119217ms step_avg:60.27ms
step:1979/2035 train_time:119305ms step_avg:60.29ms
step:1980/2035 train_time:119394ms step_avg:60.30ms
step:1981/2035 train_time:119481ms step_avg:60.31ms
step:1982/2035 train_time:119570ms step_avg:60.33ms
step:1983/2035 train_time:119659ms step_avg:60.34ms
step:1984/2035 train_time:119747ms step_avg:60.36ms
step:1985/2035 train_time:119836ms step_avg:60.37ms
step:1986/2035 train_time:119924ms step_avg:60.38ms
step:1987/2035 train_time:120012ms step_avg:60.40ms
step:1988/2035 train_time:120101ms step_avg:60.41ms
step:1989/2035 train_time:120189ms step_avg:60.43ms
step:1990/2035 train_time:120276ms step_avg:60.44ms
step:1991/2035 train_time:120365ms step_avg:60.45ms
step:1992/2035 train_time:120454ms step_avg:60.47ms
step:1993/2035 train_time:120542ms step_avg:60.48ms
step:1994/2035 train_time:120629ms step_avg:60.50ms
step:1995/2035 train_time:120718ms step_avg:60.51ms
step:1996/2035 train_time:120807ms step_avg:60.52ms
step:1997/2035 train_time:120896ms step_avg:60.54ms
step:1998/2035 train_time:120983ms step_avg:60.55ms
step:1999/2035 train_time:121073ms step_avg:60.57ms
step:2000/2035 train_time:121160ms step_avg:60.58ms
step:2000/2035 val_loss:3.2844 train_time:121249ms step_avg:60.62ms
step:2001/2035 train_time:121275ms step_avg:60.61ms
step:2002/2035 train_time:121341ms step_avg:60.61ms
step:2003/2035 train_time:121432ms step_avg:60.63ms
step:2004/2035 train_time:121521ms step_avg:60.64ms
step:2005/2035 train_time:121610ms step_avg:60.65ms
step:2006/2035 train_time:121698ms step_avg:60.67ms
step:2007/2035 train_time:121786ms step_avg:60.68ms
step:2008/2035 train_time:121873ms step_avg:60.69ms
step:2009/2035 train_time:121961ms step_avg:60.71ms
step:2010/2035 train_time:122047ms step_avg:60.72ms
step:2011/2035 train_time:122136ms step_avg:60.73ms
step:2012/2035 train_time:122225ms step_avg:60.75ms
step:2013/2035 train_time:122317ms step_avg:60.76ms
step:2014/2035 train_time:122406ms step_avg:60.78ms
step:2015/2035 train_time:122496ms step_avg:60.79ms
step:2016/2035 train_time:122584ms step_avg:60.81ms
step:2017/2035 train_time:122672ms step_avg:60.82ms
step:2018/2035 train_time:122759ms step_avg:60.83ms
step:2019/2035 train_time:122846ms step_avg:60.85ms
step:2020/2035 train_time:122934ms step_avg:60.86ms
step:2021/2035 train_time:123022ms step_avg:60.87ms
step:2022/2035 train_time:123110ms step_avg:60.89ms
step:2023/2035 train_time:123200ms step_avg:60.90ms
step:2024/2035 train_time:123290ms step_avg:60.91ms
step:2025/2035 train_time:123379ms step_avg:60.93ms
step:2026/2035 train_time:123467ms step_avg:60.94ms
step:2027/2035 train_time:123556ms step_avg:60.96ms
step:2028/2035 train_time:123644ms step_avg:60.97ms
step:2029/2035 train_time:123732ms step_avg:60.98ms
step:2030/2035 train_time:123820ms step_avg:61.00ms
step:2031/2035 train_time:123908ms step_avg:61.01ms
step:2032/2035 train_time:123995ms step_avg:61.02ms
step:2033/2035 train_time:124083ms step_avg:61.03ms
step:2034/2035 train_time:124172ms step_avg:61.05ms
step:2035/2035 train_time:124261ms step_avg:61.06ms
step:2035/2035 val_loss:3.2773 train_time:124351ms step_avg:61.11ms
peak memory allocated: 29634 MiB reserved: 38716 MiB

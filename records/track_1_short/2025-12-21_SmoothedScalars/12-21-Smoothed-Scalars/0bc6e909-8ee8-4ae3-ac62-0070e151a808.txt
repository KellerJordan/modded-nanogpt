import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            grad_slice = torch.empty_like(grad[:rank_size])
            self._reduce_scatter_futures[param] = (
                dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad_slice
            )



    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

    @torch.no_grad()
    def reset_momentum(self, params=None):
        """Reset momentum buffers for specified parameters (or all if 'None')"""
        if params is None:
            # Reset all parameters
            params_to_reset = [p for group in self.param_groups for p in group['params']]
        else:
            params_to_reset = list(params)
        
        for param in params_to_reset:
            if param in self.state:
                state = self.state[param]
                if 'exp_avg' in state:
                    state['exp_avg'].zero_()
                if 'exp_avg_sq' in state:
                    state['exp_avg_sq'].zero_()
                state['step'] = 0

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0
        
        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas  
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    # evaluation and logging
    logs_dir: str = f"logs/12-21-Smooth-Scalars-stps.1960.40"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes,lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    
    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps
    
    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0
    
    if in_transition:
        adam_optimizers[1].transition_steps -= 1
            
    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) 
        is_transition = True
            
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 18:10:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    772053      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    772054      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    772055      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    772056      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    772057      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    772058      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    772059      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    772060      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    772054      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    772055      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    772056      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    772057      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    772058      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    772059      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    772060      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2000 val_loss:10.8322 train_time:0ms step_avg:0.03ms
step:1/2000 train_time:77ms step_avg:77.29ms
step:2/2000 train_time:100ms step_avg:49.81ms
step:3/2000 train_time:122ms step_avg:40.75ms
step:4/2000 train_time:155ms step_avg:38.74ms
step:5/2000 train_time:188ms step_avg:37.54ms
step:6/2000 train_time:275ms step_avg:45.81ms
step:7/2000 train_time:292ms step_avg:41.74ms
step:8/2000 train_time:320ms step_avg:39.94ms
step:9/2000 train_time:352ms step_avg:39.13ms
step:10/2000 train_time:385ms step_avg:38.55ms
step:11/2000 train_time:418ms step_avg:38.03ms
step:12/2000 train_time:452ms step_avg:37.63ms
step:13/2000 train_time:485ms step_avg:37.30ms
step:14/2000 train_time:518ms step_avg:37.03ms
step:15/2000 train_time:551ms step_avg:36.76ms
step:16/2000 train_time:585ms step_avg:36.55ms
step:17/2000 train_time:618ms step_avg:36.34ms
step:18/2000 train_time:651ms step_avg:36.17ms
step:19/2000 train_time:684ms step_avg:36.00ms
step:20/2000 train_time:717ms step_avg:35.86ms
step:21/2000 train_time:750ms step_avg:35.74ms
step:22/2000 train_time:784ms step_avg:35.64ms
step:23/2000 train_time:817ms step_avg:35.52ms
step:24/2000 train_time:850ms step_avg:35.43ms
step:25/2000 train_time:883ms step_avg:35.34ms
step:26/2000 train_time:917ms step_avg:35.27ms
step:27/2000 train_time:950ms step_avg:35.18ms
step:28/2000 train_time:983ms step_avg:35.12ms
step:29/2000 train_time:1016ms step_avg:35.04ms
step:30/2000 train_time:1049ms step_avg:34.98ms
step:31/2000 train_time:1083ms step_avg:34.92ms
step:32/2000 train_time:1116ms step_avg:34.88ms
step:33/2000 train_time:1150ms step_avg:34.83ms
step:34/2000 train_time:1184ms step_avg:34.82ms
step:35/2000 train_time:1217ms step_avg:34.78ms
step:36/2000 train_time:1251ms step_avg:34.75ms
step:37/2000 train_time:1284ms step_avg:34.71ms
step:38/2000 train_time:1318ms step_avg:34.68ms
step:39/2000 train_time:1351ms step_avg:34.64ms
step:40/2000 train_time:1385ms step_avg:34.61ms
step:41/2000 train_time:1418ms step_avg:34.58ms
step:42/2000 train_time:1451ms step_avg:34.55ms
step:43/2000 train_time:1484ms step_avg:34.52ms
step:44/2000 train_time:1518ms step_avg:34.49ms
step:45/2000 train_time:1551ms step_avg:34.46ms
step:46/2000 train_time:1584ms step_avg:34.44ms
step:47/2000 train_time:1617ms step_avg:34.41ms
step:48/2000 train_time:1651ms step_avg:34.40ms
step:49/2000 train_time:1684ms step_avg:34.37ms
step:50/2000 train_time:1717ms step_avg:34.35ms
step:51/2000 train_time:1750ms step_avg:34.32ms
step:52/2000 train_time:1783ms step_avg:34.30ms
step:53/2000 train_time:1817ms step_avg:34.27ms
step:54/2000 train_time:1850ms step_avg:34.26ms
step:55/2000 train_time:1884ms step_avg:34.25ms
step:56/2000 train_time:1917ms step_avg:34.23ms
step:57/2000 train_time:1950ms step_avg:34.21ms
step:58/2000 train_time:1984ms step_avg:34.20ms
step:59/2000 train_time:2017ms step_avg:34.18ms
step:60/2000 train_time:2050ms step_avg:34.17ms
step:61/2000 train_time:2083ms step_avg:34.15ms
step:62/2000 train_time:2117ms step_avg:34.14ms
step:63/2000 train_time:2150ms step_avg:34.13ms
step:64/2000 train_time:2184ms step_avg:34.12ms
step:65/2000 train_time:2217ms step_avg:34.11ms
step:66/2000 train_time:2250ms step_avg:34.10ms
step:67/2000 train_time:2284ms step_avg:34.09ms
step:68/2000 train_time:2317ms step_avg:34.08ms
step:69/2000 train_time:2351ms step_avg:34.07ms
step:70/2000 train_time:2384ms step_avg:34.06ms
step:71/2000 train_time:2417ms step_avg:34.05ms
step:72/2000 train_time:2451ms step_avg:34.05ms
step:73/2000 train_time:2485ms step_avg:34.04ms
step:74/2000 train_time:2519ms step_avg:34.04ms
step:75/2000 train_time:2552ms step_avg:34.03ms
step:76/2000 train_time:2585ms step_avg:34.02ms
step:77/2000 train_time:2618ms step_avg:34.01ms
step:78/2000 train_time:2652ms step_avg:34.00ms
step:79/2000 train_time:2685ms step_avg:33.99ms
step:80/2000 train_time:2718ms step_avg:33.98ms
step:81/2000 train_time:2751ms step_avg:33.96ms
step:82/2000 train_time:2784ms step_avg:33.96ms
step:83/2000 train_time:2817ms step_avg:33.95ms
step:84/2000 train_time:2851ms step_avg:33.94ms
step:85/2000 train_time:2884ms step_avg:33.93ms
step:86/2000 train_time:2917ms step_avg:33.92ms
step:87/2000 train_time:2950ms step_avg:33.91ms
step:88/2000 train_time:2984ms step_avg:33.90ms
step:89/2000 train_time:3017ms step_avg:33.89ms
step:90/2000 train_time:3050ms step_avg:33.89ms
step:91/2000 train_time:3083ms step_avg:33.88ms
step:92/2000 train_time:3116ms step_avg:33.87ms
step:93/2000 train_time:3149ms step_avg:33.87ms
step:94/2000 train_time:3183ms step_avg:33.86ms
step:95/2000 train_time:3216ms step_avg:33.85ms
step:96/2000 train_time:3249ms step_avg:33.85ms
step:97/2000 train_time:3283ms step_avg:33.84ms
step:98/2000 train_time:3316ms step_avg:33.84ms
step:99/2000 train_time:3350ms step_avg:33.83ms
step:100/2000 train_time:3383ms step_avg:33.83ms
step:101/2000 train_time:3416ms step_avg:33.82ms
step:102/2000 train_time:3449ms step_avg:33.82ms
step:103/2000 train_time:3483ms step_avg:33.81ms
step:104/2000 train_time:3516ms step_avg:33.81ms
step:105/2000 train_time:3550ms step_avg:33.81ms
step:106/2000 train_time:3583ms step_avg:33.80ms
step:107/2000 train_time:3616ms step_avg:33.80ms
step:108/2000 train_time:3649ms step_avg:33.79ms
step:109/2000 train_time:3683ms step_avg:33.79ms
step:110/2000 train_time:3716ms step_avg:33.78ms
step:111/2000 train_time:3750ms step_avg:33.78ms
step:112/2000 train_time:3783ms step_avg:33.78ms
step:113/2000 train_time:3816ms step_avg:33.77ms
step:114/2000 train_time:3849ms step_avg:33.77ms
step:115/2000 train_time:3882ms step_avg:33.76ms
step:116/2000 train_time:3916ms step_avg:33.76ms
step:117/2000 train_time:3949ms step_avg:33.75ms
step:118/2000 train_time:3982ms step_avg:33.75ms
step:119/2000 train_time:4015ms step_avg:33.74ms
step:120/2000 train_time:4048ms step_avg:33.74ms
step:121/2000 train_time:4081ms step_avg:33.73ms
step:122/2000 train_time:4115ms step_avg:33.73ms
step:123/2000 train_time:4148ms step_avg:33.72ms
step:124/2000 train_time:4181ms step_avg:33.72ms
step:125/2000 train_time:4214ms step_avg:33.71ms
step:126/2000 train_time:4247ms step_avg:33.71ms
step:127/2000 train_time:4280ms step_avg:33.70ms
step:128/2000 train_time:4314ms step_avg:33.70ms
step:129/2000 train_time:4347ms step_avg:33.70ms
step:130/2000 train_time:4380ms step_avg:33.70ms
step:131/2000 train_time:4413ms step_avg:33.69ms
step:132/2000 train_time:4447ms step_avg:33.69ms
step:133/2000 train_time:4480ms step_avg:33.68ms
step:134/2000 train_time:4513ms step_avg:33.68ms
step:135/2000 train_time:4546ms step_avg:33.68ms
step:136/2000 train_time:4579ms step_avg:33.67ms
step:137/2000 train_time:4613ms step_avg:33.67ms
step:138/2000 train_time:4646ms step_avg:33.67ms
step:139/2000 train_time:4679ms step_avg:33.66ms
step:140/2000 train_time:4713ms step_avg:33.66ms
step:141/2000 train_time:4746ms step_avg:33.66ms
step:142/2000 train_time:4779ms step_avg:33.66ms
step:143/2000 train_time:4812ms step_avg:33.65ms
step:144/2000 train_time:4846ms step_avg:33.65ms
step:145/2000 train_time:4879ms step_avg:33.65ms
step:146/2000 train_time:4912ms step_avg:33.65ms
step:147/2000 train_time:4945ms step_avg:33.64ms
step:148/2000 train_time:4979ms step_avg:33.64ms
step:149/2000 train_time:5012ms step_avg:33.63ms
step:150/2000 train_time:5045ms step_avg:33.63ms
step:151/2000 train_time:5078ms step_avg:33.63ms
step:152/2000 train_time:5111ms step_avg:33.63ms
step:153/2000 train_time:5144ms step_avg:33.62ms
step:154/2000 train_time:5178ms step_avg:33.62ms
step:155/2000 train_time:5211ms step_avg:33.62ms
step:156/2000 train_time:5244ms step_avg:33.62ms
step:157/2000 train_time:5277ms step_avg:33.61ms
step:158/2000 train_time:5310ms step_avg:33.61ms
step:159/2000 train_time:5343ms step_avg:33.61ms
step:160/2000 train_time:5377ms step_avg:33.60ms
step:161/2000 train_time:5410ms step_avg:33.60ms
step:162/2000 train_time:5444ms step_avg:33.60ms
step:163/2000 train_time:5477ms step_avg:33.60ms
step:164/2000 train_time:5510ms step_avg:33.60ms
step:165/2000 train_time:5543ms step_avg:33.59ms
step:166/2000 train_time:5576ms step_avg:33.59ms
step:167/2000 train_time:5609ms step_avg:33.59ms
step:168/2000 train_time:5642ms step_avg:33.58ms
step:169/2000 train_time:5675ms step_avg:33.58ms
step:170/2000 train_time:5709ms step_avg:33.58ms
step:171/2000 train_time:5742ms step_avg:33.58ms
step:172/2000 train_time:5775ms step_avg:33.58ms
step:173/2000 train_time:5808ms step_avg:33.57ms
step:174/2000 train_time:5842ms step_avg:33.57ms
step:175/2000 train_time:5874ms step_avg:33.57ms
step:176/2000 train_time:5908ms step_avg:33.57ms
step:177/2000 train_time:5941ms step_avg:33.56ms
step:178/2000 train_time:5974ms step_avg:33.56ms
step:179/2000 train_time:6007ms step_avg:33.56ms
step:180/2000 train_time:6040ms step_avg:33.56ms
step:181/2000 train_time:6073ms step_avg:33.55ms
step:182/2000 train_time:6107ms step_avg:33.55ms
step:183/2000 train_time:6140ms step_avg:33.55ms
step:184/2000 train_time:6173ms step_avg:33.55ms
step:185/2000 train_time:6206ms step_avg:33.54ms
step:186/2000 train_time:6239ms step_avg:33.54ms
step:187/2000 train_time:6272ms step_avg:33.54ms
step:188/2000 train_time:6305ms step_avg:33.54ms
step:189/2000 train_time:6338ms step_avg:33.54ms
step:190/2000 train_time:6372ms step_avg:33.54ms
step:191/2000 train_time:6405ms step_avg:33.53ms
step:192/2000 train_time:6438ms step_avg:33.53ms
step:193/2000 train_time:6471ms step_avg:33.53ms
step:194/2000 train_time:6505ms step_avg:33.53ms
step:195/2000 train_time:6538ms step_avg:33.53ms
step:196/2000 train_time:6571ms step_avg:33.52ms
step:197/2000 train_time:6604ms step_avg:33.52ms
step:198/2000 train_time:6637ms step_avg:33.52ms
step:199/2000 train_time:6671ms step_avg:33.52ms
step:200/2000 train_time:6704ms step_avg:33.52ms
step:201/2000 train_time:6737ms step_avg:33.52ms
step:202/2000 train_time:6771ms step_avg:33.52ms
step:203/2000 train_time:6804ms step_avg:33.52ms
step:204/2000 train_time:6837ms step_avg:33.51ms
step:205/2000 train_time:6870ms step_avg:33.51ms
step:206/2000 train_time:6903ms step_avg:33.51ms
step:207/2000 train_time:6936ms step_avg:33.51ms
step:208/2000 train_time:6969ms step_avg:33.51ms
step:209/2000 train_time:7002ms step_avg:33.50ms
step:210/2000 train_time:7036ms step_avg:33.50ms
step:211/2000 train_time:7069ms step_avg:33.50ms
step:212/2000 train_time:7103ms step_avg:33.50ms
step:213/2000 train_time:7136ms step_avg:33.50ms
step:214/2000 train_time:7169ms step_avg:33.50ms
step:215/2000 train_time:7202ms step_avg:33.50ms
step:216/2000 train_time:7236ms step_avg:33.50ms
step:217/2000 train_time:7269ms step_avg:33.50ms
step:218/2000 train_time:7302ms step_avg:33.50ms
step:219/2000 train_time:7335ms step_avg:33.49ms
step:220/2000 train_time:7369ms step_avg:33.49ms
step:221/2000 train_time:7402ms step_avg:33.49ms
step:222/2000 train_time:7435ms step_avg:33.49ms
step:223/2000 train_time:7468ms step_avg:33.49ms
step:224/2000 train_time:7501ms step_avg:33.49ms
step:225/2000 train_time:7534ms step_avg:33.49ms
step:226/2000 train_time:7568ms step_avg:33.49ms
step:227/2000 train_time:7601ms step_avg:33.48ms
step:228/2000 train_time:7634ms step_avg:33.48ms
step:229/2000 train_time:7667ms step_avg:33.48ms
step:230/2000 train_time:7701ms step_avg:33.48ms
step:231/2000 train_time:7734ms step_avg:33.48ms
step:232/2000 train_time:7767ms step_avg:33.48ms
step:233/2000 train_time:7800ms step_avg:33.48ms
step:234/2000 train_time:7833ms step_avg:33.47ms
step:235/2000 train_time:7866ms step_avg:33.47ms
step:236/2000 train_time:7900ms step_avg:33.47ms
step:237/2000 train_time:7933ms step_avg:33.47ms
step:238/2000 train_time:7966ms step_avg:33.47ms
step:239/2000 train_time:7999ms step_avg:33.47ms
step:240/2000 train_time:8032ms step_avg:33.47ms
step:241/2000 train_time:8065ms step_avg:33.46ms
step:242/2000 train_time:8098ms step_avg:33.46ms
step:243/2000 train_time:8131ms step_avg:33.46ms
step:244/2000 train_time:8165ms step_avg:33.46ms
step:245/2000 train_time:8198ms step_avg:33.46ms
step:246/2000 train_time:8231ms step_avg:33.46ms
step:247/2000 train_time:8264ms step_avg:33.46ms
step:248/2000 train_time:8297ms step_avg:33.46ms
step:249/2000 train_time:8330ms step_avg:33.45ms
step:250/2000 train_time:8364ms step_avg:33.45ms
step:250/2000 val_loss:4.2612 train_time:8400ms step_avg:33.60ms
step:251/2000 train_time:8418ms step_avg:33.54ms
step:252/2000 train_time:8436ms step_avg:33.48ms
step:253/2000 train_time:8467ms step_avg:33.47ms
step:254/2000 train_time:8501ms step_avg:33.47ms
step:255/2000 train_time:8535ms step_avg:33.47ms
step:256/2000 train_time:8570ms step_avg:33.48ms
step:257/2000 train_time:8603ms step_avg:33.48ms
step:258/2000 train_time:8637ms step_avg:33.48ms
step:259/2000 train_time:8670ms step_avg:33.47ms
step:260/2000 train_time:8703ms step_avg:33.47ms
step:261/2000 train_time:8736ms step_avg:33.47ms
step:262/2000 train_time:8769ms step_avg:33.47ms
step:263/2000 train_time:8802ms step_avg:33.47ms
step:264/2000 train_time:8835ms step_avg:33.47ms
step:265/2000 train_time:8868ms step_avg:33.46ms
step:266/2000 train_time:8901ms step_avg:33.46ms
step:267/2000 train_time:8934ms step_avg:33.46ms
step:268/2000 train_time:8967ms step_avg:33.46ms
step:269/2000 train_time:9000ms step_avg:33.46ms
step:270/2000 train_time:9033ms step_avg:33.46ms
step:271/2000 train_time:9066ms step_avg:33.45ms
step:272/2000 train_time:9099ms step_avg:33.45ms
step:273/2000 train_time:9132ms step_avg:33.45ms
step:274/2000 train_time:9165ms step_avg:33.45ms
step:275/2000 train_time:9198ms step_avg:33.45ms
step:276/2000 train_time:9231ms step_avg:33.45ms
step:277/2000 train_time:9264ms step_avg:33.44ms
step:278/2000 train_time:9297ms step_avg:33.44ms
step:279/2000 train_time:9330ms step_avg:33.44ms
step:280/2000 train_time:9363ms step_avg:33.44ms
step:281/2000 train_time:9396ms step_avg:33.44ms
step:282/2000 train_time:9429ms step_avg:33.44ms
step:283/2000 train_time:9463ms step_avg:33.44ms
step:284/2000 train_time:9496ms step_avg:33.44ms
step:285/2000 train_time:9530ms step_avg:33.44ms
step:286/2000 train_time:9564ms step_avg:33.44ms
step:287/2000 train_time:9597ms step_avg:33.44ms
step:288/2000 train_time:9630ms step_avg:33.44ms
step:289/2000 train_time:9663ms step_avg:33.44ms
step:290/2000 train_time:9697ms step_avg:33.44ms
step:291/2000 train_time:9730ms step_avg:33.44ms
step:292/2000 train_time:9763ms step_avg:33.44ms
step:293/2000 train_time:9796ms step_avg:33.43ms
step:294/2000 train_time:9829ms step_avg:33.43ms
step:295/2000 train_time:9862ms step_avg:33.43ms
step:296/2000 train_time:9895ms step_avg:33.43ms
step:297/2000 train_time:9928ms step_avg:33.43ms
step:298/2000 train_time:9962ms step_avg:33.43ms
step:299/2000 train_time:9995ms step_avg:33.43ms
step:300/2000 train_time:10028ms step_avg:33.43ms
step:301/2000 train_time:10061ms step_avg:33.43ms
step:302/2000 train_time:10094ms step_avg:33.42ms
step:303/2000 train_time:10127ms step_avg:33.42ms
step:304/2000 train_time:10160ms step_avg:33.42ms
step:305/2000 train_time:10193ms step_avg:33.42ms
step:306/2000 train_time:10226ms step_avg:33.42ms
step:307/2000 train_time:10258ms step_avg:33.42ms
step:308/2000 train_time:10291ms step_avg:33.41ms
step:309/2000 train_time:10324ms step_avg:33.41ms
step:310/2000 train_time:10357ms step_avg:33.41ms
step:311/2000 train_time:10390ms step_avg:33.41ms
step:312/2000 train_time:10424ms step_avg:33.41ms
step:313/2000 train_time:10457ms step_avg:33.41ms
step:314/2000 train_time:10490ms step_avg:33.41ms
step:315/2000 train_time:10524ms step_avg:33.41ms
step:316/2000 train_time:10557ms step_avg:33.41ms
step:317/2000 train_time:10590ms step_avg:33.41ms
step:318/2000 train_time:10624ms step_avg:33.41ms
step:319/2000 train_time:10657ms step_avg:33.41ms
step:320/2000 train_time:10690ms step_avg:33.41ms
step:321/2000 train_time:10723ms step_avg:33.41ms
step:322/2000 train_time:10757ms step_avg:33.41ms
step:323/2000 train_time:10789ms step_avg:33.40ms
step:324/2000 train_time:10823ms step_avg:33.40ms
step:325/2000 train_time:10856ms step_avg:33.40ms
step:326/2000 train_time:10889ms step_avg:33.40ms
step:327/2000 train_time:10922ms step_avg:33.40ms
step:328/2000 train_time:10956ms step_avg:33.40ms
step:329/2000 train_time:10988ms step_avg:33.40ms
step:330/2000 train_time:11022ms step_avg:33.40ms
step:331/2000 train_time:11054ms step_avg:33.40ms
step:332/2000 train_time:11088ms step_avg:33.40ms
step:333/2000 train_time:11121ms step_avg:33.40ms
step:334/2000 train_time:11154ms step_avg:33.40ms
step:335/2000 train_time:11187ms step_avg:33.40ms
step:336/2000 train_time:11221ms step_avg:33.40ms
step:337/2000 train_time:11254ms step_avg:33.39ms
step:338/2000 train_time:11287ms step_avg:33.39ms
step:339/2000 train_time:11320ms step_avg:33.39ms
step:340/2000 train_time:11354ms step_avg:33.39ms
step:341/2000 train_time:11387ms step_avg:33.39ms
step:342/2000 train_time:11420ms step_avg:33.39ms
step:343/2000 train_time:11453ms step_avg:33.39ms
step:344/2000 train_time:11487ms step_avg:33.39ms
step:345/2000 train_time:11520ms step_avg:33.39ms
step:346/2000 train_time:11553ms step_avg:33.39ms
step:347/2000 train_time:11586ms step_avg:33.39ms
step:348/2000 train_time:11620ms step_avg:33.39ms
step:349/2000 train_time:11653ms step_avg:33.39ms
step:350/2000 train_time:11687ms step_avg:33.39ms
step:351/2000 train_time:11720ms step_avg:33.39ms
step:352/2000 train_time:11753ms step_avg:33.39ms
step:353/2000 train_time:11786ms step_avg:33.39ms
step:354/2000 train_time:11820ms step_avg:33.39ms
step:355/2000 train_time:11853ms step_avg:33.39ms
step:356/2000 train_time:11886ms step_avg:33.39ms
step:357/2000 train_time:11920ms step_avg:33.39ms
step:358/2000 train_time:11953ms step_avg:33.39ms
step:359/2000 train_time:11986ms step_avg:33.39ms
step:360/2000 train_time:12019ms step_avg:33.39ms
step:361/2000 train_time:12052ms step_avg:33.39ms
step:362/2000 train_time:12086ms step_avg:33.39ms
step:363/2000 train_time:12119ms step_avg:33.38ms
step:364/2000 train_time:12152ms step_avg:33.38ms
step:365/2000 train_time:12185ms step_avg:33.38ms
step:366/2000 train_time:12218ms step_avg:33.38ms
step:367/2000 train_time:12251ms step_avg:33.38ms
step:368/2000 train_time:12285ms step_avg:33.38ms
step:369/2000 train_time:12318ms step_avg:33.38ms
step:370/2000 train_time:12351ms step_avg:33.38ms
step:371/2000 train_time:12384ms step_avg:33.38ms
step:372/2000 train_time:12417ms step_avg:33.38ms
step:373/2000 train_time:12451ms step_avg:33.38ms
step:374/2000 train_time:12484ms step_avg:33.38ms
step:375/2000 train_time:12517ms step_avg:33.38ms
step:376/2000 train_time:12550ms step_avg:33.38ms
step:377/2000 train_time:12583ms step_avg:33.38ms
step:378/2000 train_time:12617ms step_avg:33.38ms
step:379/2000 train_time:12650ms step_avg:33.38ms
step:380/2000 train_time:12684ms step_avg:33.38ms
step:381/2000 train_time:12717ms step_avg:33.38ms
step:382/2000 train_time:12750ms step_avg:33.38ms
step:383/2000 train_time:12783ms step_avg:33.37ms
step:384/2000 train_time:12816ms step_avg:33.37ms
step:385/2000 train_time:12849ms step_avg:33.37ms
step:386/2000 train_time:12883ms step_avg:33.38ms
step:387/2000 train_time:12916ms step_avg:33.37ms
step:388/2000 train_time:12949ms step_avg:33.37ms
step:389/2000 train_time:12982ms step_avg:33.37ms
step:390/2000 train_time:13016ms step_avg:33.37ms
step:391/2000 train_time:13048ms step_avg:33.37ms
step:392/2000 train_time:13081ms step_avg:33.37ms
step:393/2000 train_time:13114ms step_avg:33.37ms
step:394/2000 train_time:13148ms step_avg:33.37ms
step:395/2000 train_time:13181ms step_avg:33.37ms
step:396/2000 train_time:13214ms step_avg:33.37ms
step:397/2000 train_time:13247ms step_avg:33.37ms
step:398/2000 train_time:13281ms step_avg:33.37ms
step:399/2000 train_time:13314ms step_avg:33.37ms
step:400/2000 train_time:13347ms step_avg:33.37ms
step:401/2000 train_time:13380ms step_avg:33.37ms
step:402/2000 train_time:13413ms step_avg:33.37ms
step:403/2000 train_time:13447ms step_avg:33.37ms
step:404/2000 train_time:13480ms step_avg:33.37ms
step:405/2000 train_time:13513ms step_avg:33.37ms
step:406/2000 train_time:13547ms step_avg:33.37ms
step:407/2000 train_time:13580ms step_avg:33.37ms
step:408/2000 train_time:13613ms step_avg:33.36ms
step:409/2000 train_time:13646ms step_avg:33.36ms
step:410/2000 train_time:13679ms step_avg:33.36ms
step:411/2000 train_time:13712ms step_avg:33.36ms
step:412/2000 train_time:13745ms step_avg:33.36ms
step:413/2000 train_time:13778ms step_avg:33.36ms
step:414/2000 train_time:13812ms step_avg:33.36ms
step:415/2000 train_time:13845ms step_avg:33.36ms
step:416/2000 train_time:13879ms step_avg:33.36ms
step:417/2000 train_time:13912ms step_avg:33.36ms
step:418/2000 train_time:13945ms step_avg:33.36ms
step:419/2000 train_time:13978ms step_avg:33.36ms
step:420/2000 train_time:14011ms step_avg:33.36ms
step:421/2000 train_time:14044ms step_avg:33.36ms
step:422/2000 train_time:14077ms step_avg:33.36ms
step:423/2000 train_time:14110ms step_avg:33.36ms
step:424/2000 train_time:14144ms step_avg:33.36ms
step:425/2000 train_time:14176ms step_avg:33.36ms
step:426/2000 train_time:14209ms step_avg:33.36ms
step:427/2000 train_time:14242ms step_avg:33.35ms
step:428/2000 train_time:14276ms step_avg:33.35ms
step:429/2000 train_time:14309ms step_avg:33.35ms
step:430/2000 train_time:14342ms step_avg:33.35ms
step:431/2000 train_time:14375ms step_avg:33.35ms
step:432/2000 train_time:14408ms step_avg:33.35ms
step:433/2000 train_time:14441ms step_avg:33.35ms
step:434/2000 train_time:14475ms step_avg:33.35ms
step:435/2000 train_time:14508ms step_avg:33.35ms
step:436/2000 train_time:14541ms step_avg:33.35ms
step:437/2000 train_time:14574ms step_avg:33.35ms
step:438/2000 train_time:14608ms step_avg:33.35ms
step:439/2000 train_time:14641ms step_avg:33.35ms
step:440/2000 train_time:14674ms step_avg:33.35ms
step:441/2000 train_time:14707ms step_avg:33.35ms
step:442/2000 train_time:14741ms step_avg:33.35ms
step:443/2000 train_time:14774ms step_avg:33.35ms
step:444/2000 train_time:14808ms step_avg:33.35ms
step:445/2000 train_time:14841ms step_avg:33.35ms
step:446/2000 train_time:14875ms step_avg:33.35ms
step:447/2000 train_time:14908ms step_avg:33.35ms
step:448/2000 train_time:14941ms step_avg:33.35ms
step:449/2000 train_time:14974ms step_avg:33.35ms
step:450/2000 train_time:15008ms step_avg:33.35ms
step:451/2000 train_time:15041ms step_avg:33.35ms
step:452/2000 train_time:15074ms step_avg:33.35ms
step:453/2000 train_time:15107ms step_avg:33.35ms
step:454/2000 train_time:15141ms step_avg:33.35ms
step:455/2000 train_time:15174ms step_avg:33.35ms
step:456/2000 train_time:15207ms step_avg:33.35ms
step:457/2000 train_time:15240ms step_avg:33.35ms
step:458/2000 train_time:15273ms step_avg:33.35ms
step:459/2000 train_time:15306ms step_avg:33.35ms
step:460/2000 train_time:15340ms step_avg:33.35ms
step:461/2000 train_time:15373ms step_avg:33.35ms
step:462/2000 train_time:15406ms step_avg:33.35ms
step:463/2000 train_time:15439ms step_avg:33.35ms
step:464/2000 train_time:15473ms step_avg:33.35ms
step:465/2000 train_time:15506ms step_avg:33.35ms
step:466/2000 train_time:15539ms step_avg:33.35ms
step:467/2000 train_time:15572ms step_avg:33.34ms
step:468/2000 train_time:15606ms step_avg:33.35ms
step:469/2000 train_time:15639ms step_avg:33.35ms
step:470/2000 train_time:15672ms step_avg:33.35ms
step:471/2000 train_time:15706ms step_avg:33.35ms
step:472/2000 train_time:15739ms step_avg:33.35ms
step:473/2000 train_time:15772ms step_avg:33.35ms
step:474/2000 train_time:15806ms step_avg:33.35ms
step:475/2000 train_time:15839ms step_avg:33.35ms
step:476/2000 train_time:15872ms step_avg:33.35ms
step:477/2000 train_time:15906ms step_avg:33.34ms
step:478/2000 train_time:15939ms step_avg:33.35ms
step:479/2000 train_time:15972ms step_avg:33.34ms
step:480/2000 train_time:16006ms step_avg:33.34ms
step:481/2000 train_time:16038ms step_avg:33.34ms
step:482/2000 train_time:16072ms step_avg:33.34ms
step:483/2000 train_time:16105ms step_avg:33.34ms
step:484/2000 train_time:16138ms step_avg:33.34ms
step:485/2000 train_time:16171ms step_avg:33.34ms
step:486/2000 train_time:16205ms step_avg:33.34ms
step:487/2000 train_time:16237ms step_avg:33.34ms
step:488/2000 train_time:16271ms step_avg:33.34ms
step:489/2000 train_time:16304ms step_avg:33.34ms
step:490/2000 train_time:16337ms step_avg:33.34ms
step:491/2000 train_time:16370ms step_avg:33.34ms
step:492/2000 train_time:16404ms step_avg:33.34ms
step:493/2000 train_time:16437ms step_avg:33.34ms
step:494/2000 train_time:16470ms step_avg:33.34ms
step:495/2000 train_time:16503ms step_avg:33.34ms
step:496/2000 train_time:16536ms step_avg:33.34ms
step:497/2000 train_time:16569ms step_avg:33.34ms
step:498/2000 train_time:16602ms step_avg:33.34ms
step:499/2000 train_time:16635ms step_avg:33.34ms
step:500/2000 train_time:16668ms step_avg:33.34ms
step:500/2000 val_loss:3.9942 train_time:16705ms step_avg:33.41ms
step:501/2000 train_time:16726ms step_avg:33.39ms
step:502/2000 train_time:16745ms step_avg:33.36ms
step:503/2000 train_time:16771ms step_avg:33.34ms
step:504/2000 train_time:16805ms step_avg:33.34ms
step:505/2000 train_time:16839ms step_avg:33.34ms
step:506/2000 train_time:16872ms step_avg:33.34ms
step:507/2000 train_time:16906ms step_avg:33.34ms
step:508/2000 train_time:16939ms step_avg:33.34ms
step:509/2000 train_time:16972ms step_avg:33.34ms
step:510/2000 train_time:17005ms step_avg:33.34ms
step:511/2000 train_time:17038ms step_avg:33.34ms
step:512/2000 train_time:17072ms step_avg:33.34ms
step:513/2000 train_time:17104ms step_avg:33.34ms
step:514/2000 train_time:17137ms step_avg:33.34ms
step:515/2000 train_time:17170ms step_avg:33.34ms
step:516/2000 train_time:17203ms step_avg:33.34ms
step:517/2000 train_time:17236ms step_avg:33.34ms
step:518/2000 train_time:17269ms step_avg:33.34ms
step:519/2000 train_time:17302ms step_avg:33.34ms
step:520/2000 train_time:17335ms step_avg:33.34ms
step:521/2000 train_time:17368ms step_avg:33.34ms
step:522/2000 train_time:17401ms step_avg:33.33ms
step:523/2000 train_time:17433ms step_avg:33.33ms
step:524/2000 train_time:17466ms step_avg:33.33ms
step:525/2000 train_time:17499ms step_avg:33.33ms
step:526/2000 train_time:17532ms step_avg:33.33ms
step:527/2000 train_time:17565ms step_avg:33.33ms
step:528/2000 train_time:17598ms step_avg:33.33ms
step:529/2000 train_time:17631ms step_avg:33.33ms
step:530/2000 train_time:17665ms step_avg:33.33ms
step:531/2000 train_time:17699ms step_avg:33.33ms
step:532/2000 train_time:17733ms step_avg:33.33ms
step:533/2000 train_time:17766ms step_avg:33.33ms
step:534/2000 train_time:17800ms step_avg:33.33ms
step:535/2000 train_time:17833ms step_avg:33.33ms
step:536/2000 train_time:17867ms step_avg:33.33ms
step:537/2000 train_time:17900ms step_avg:33.33ms
step:538/2000 train_time:17934ms step_avg:33.33ms
step:539/2000 train_time:17967ms step_avg:33.33ms
step:540/2000 train_time:18000ms step_avg:33.33ms
step:541/2000 train_time:18033ms step_avg:33.33ms
step:542/2000 train_time:18067ms step_avg:33.33ms
step:543/2000 train_time:18100ms step_avg:33.33ms
step:544/2000 train_time:18133ms step_avg:33.33ms
step:545/2000 train_time:18166ms step_avg:33.33ms
step:546/2000 train_time:18199ms step_avg:33.33ms
step:547/2000 train_time:18232ms step_avg:33.33ms
step:548/2000 train_time:18265ms step_avg:33.33ms
step:549/2000 train_time:18298ms step_avg:33.33ms
step:550/2000 train_time:18331ms step_avg:33.33ms
step:551/2000 train_time:18364ms step_avg:33.33ms
step:552/2000 train_time:18397ms step_avg:33.33ms
step:553/2000 train_time:18430ms step_avg:33.33ms
step:554/2000 train_time:18463ms step_avg:33.33ms
step:555/2000 train_time:18496ms step_avg:33.33ms
step:556/2000 train_time:18529ms step_avg:33.33ms
step:557/2000 train_time:18562ms step_avg:33.33ms
step:558/2000 train_time:18595ms step_avg:33.32ms
step:559/2000 train_time:18629ms step_avg:33.32ms
step:560/2000 train_time:18662ms step_avg:33.33ms
step:561/2000 train_time:18695ms step_avg:33.32ms
step:562/2000 train_time:18728ms step_avg:33.32ms
step:563/2000 train_time:18762ms step_avg:33.32ms
step:564/2000 train_time:18795ms step_avg:33.32ms
step:565/2000 train_time:18828ms step_avg:33.32ms
step:566/2000 train_time:18862ms step_avg:33.32ms
step:567/2000 train_time:18894ms step_avg:33.32ms
step:568/2000 train_time:18928ms step_avg:33.32ms
step:569/2000 train_time:18961ms step_avg:33.32ms
step:570/2000 train_time:18994ms step_avg:33.32ms
step:571/2000 train_time:19028ms step_avg:33.32ms
step:572/2000 train_time:19061ms step_avg:33.32ms
step:573/2000 train_time:19094ms step_avg:33.32ms
step:574/2000 train_time:19127ms step_avg:33.32ms
step:575/2000 train_time:19161ms step_avg:33.32ms
step:576/2000 train_time:19194ms step_avg:33.32ms
step:577/2000 train_time:19227ms step_avg:33.32ms
step:578/2000 train_time:19260ms step_avg:33.32ms
step:579/2000 train_time:19293ms step_avg:33.32ms
step:580/2000 train_time:19327ms step_avg:33.32ms
step:581/2000 train_time:19360ms step_avg:33.32ms
step:582/2000 train_time:19393ms step_avg:33.32ms
step:583/2000 train_time:19426ms step_avg:33.32ms
step:584/2000 train_time:19459ms step_avg:33.32ms
step:585/2000 train_time:19491ms step_avg:33.32ms
step:586/2000 train_time:19525ms step_avg:33.32ms
step:587/2000 train_time:19558ms step_avg:33.32ms
step:588/2000 train_time:19591ms step_avg:33.32ms
step:589/2000 train_time:19624ms step_avg:33.32ms
step:590/2000 train_time:19658ms step_avg:33.32ms
step:591/2000 train_time:19691ms step_avg:33.32ms
step:592/2000 train_time:19725ms step_avg:33.32ms
step:593/2000 train_time:19758ms step_avg:33.32ms
step:594/2000 train_time:19791ms step_avg:33.32ms
step:595/2000 train_time:19824ms step_avg:33.32ms
step:596/2000 train_time:19857ms step_avg:33.32ms
step:597/2000 train_time:19891ms step_avg:33.32ms
step:598/2000 train_time:19924ms step_avg:33.32ms
step:599/2000 train_time:19957ms step_avg:33.32ms
step:600/2000 train_time:19990ms step_avg:33.32ms
step:601/2000 train_time:20023ms step_avg:33.32ms
step:602/2000 train_time:20057ms step_avg:33.32ms
step:603/2000 train_time:20090ms step_avg:33.32ms
step:604/2000 train_time:20123ms step_avg:33.32ms
step:605/2000 train_time:20156ms step_avg:33.32ms
step:606/2000 train_time:20189ms step_avg:33.31ms
step:607/2000 train_time:20222ms step_avg:33.31ms
step:608/2000 train_time:20255ms step_avg:33.31ms
step:609/2000 train_time:20288ms step_avg:33.31ms
step:610/2000 train_time:20321ms step_avg:33.31ms
step:611/2000 train_time:20354ms step_avg:33.31ms
step:612/2000 train_time:20387ms step_avg:33.31ms
step:613/2000 train_time:20420ms step_avg:33.31ms
step:614/2000 train_time:20454ms step_avg:33.31ms
step:615/2000 train_time:20487ms step_avg:33.31ms
step:616/2000 train_time:20521ms step_avg:33.31ms
step:617/2000 train_time:20553ms step_avg:33.31ms
step:618/2000 train_time:20586ms step_avg:33.31ms
step:619/2000 train_time:20619ms step_avg:33.31ms
step:620/2000 train_time:20653ms step_avg:33.31ms
step:621/2000 train_time:20686ms step_avg:33.31ms
step:622/2000 train_time:20720ms step_avg:33.31ms
step:623/2000 train_time:20753ms step_avg:33.31ms
step:624/2000 train_time:20786ms step_avg:33.31ms
step:625/2000 train_time:20819ms step_avg:33.31ms
step:626/2000 train_time:20852ms step_avg:33.31ms
step:627/2000 train_time:20885ms step_avg:33.31ms
step:628/2000 train_time:20919ms step_avg:33.31ms
step:629/2000 train_time:20952ms step_avg:33.31ms
step:630/2000 train_time:20985ms step_avg:33.31ms
step:631/2000 train_time:21018ms step_avg:33.31ms
step:632/2000 train_time:21051ms step_avg:33.31ms
step:633/2000 train_time:21084ms step_avg:33.31ms
step:634/2000 train_time:21118ms step_avg:33.31ms
step:635/2000 train_time:21150ms step_avg:33.31ms
step:636/2000 train_time:21184ms step_avg:33.31ms
step:637/2000 train_time:21217ms step_avg:33.31ms
step:638/2000 train_time:21250ms step_avg:33.31ms
step:639/2000 train_time:21284ms step_avg:33.31ms
step:640/2000 train_time:21317ms step_avg:33.31ms
step:641/2000 train_time:21350ms step_avg:33.31ms
step:642/2000 train_time:21383ms step_avg:33.31ms
step:643/2000 train_time:21416ms step_avg:33.31ms
step:644/2000 train_time:21450ms step_avg:33.31ms
step:645/2000 train_time:21483ms step_avg:33.31ms
step:646/2000 train_time:21516ms step_avg:33.31ms
step:647/2000 train_time:21549ms step_avg:33.31ms
step:648/2000 train_time:21583ms step_avg:33.31ms
step:649/2000 train_time:21615ms step_avg:33.31ms
step:650/2000 train_time:21649ms step_avg:33.31ms
step:651/2000 train_time:21682ms step_avg:33.31ms
step:652/2000 train_time:21715ms step_avg:33.31ms
step:653/2000 train_time:21748ms step_avg:33.30ms
step:654/2000 train_time:21782ms step_avg:33.31ms
step:655/2000 train_time:21816ms step_avg:33.31ms
step:656/2000 train_time:21874ms step_avg:33.34ms
step:657/2000 train_time:21935ms step_avg:33.39ms
step:658/2000 train_time:21995ms step_avg:33.43ms
step:659/2000 train_time:22056ms step_avg:33.47ms
step:660/2000 train_time:22115ms step_avg:33.51ms
step:661/2000 train_time:22175ms step_avg:33.55ms
step:662/2000 train_time:22235ms step_avg:33.59ms
step:663/2000 train_time:22296ms step_avg:33.63ms
step:664/2000 train_time:22355ms step_avg:33.67ms
step:665/2000 train_time:22416ms step_avg:33.71ms
step:666/2000 train_time:22476ms step_avg:33.75ms
step:667/2000 train_time:22536ms step_avg:33.79ms
step:668/2000 train_time:22596ms step_avg:33.83ms
step:669/2000 train_time:22656ms step_avg:33.87ms
step:670/2000 train_time:22717ms step_avg:33.91ms
step:671/2000 train_time:22777ms step_avg:33.94ms
step:672/2000 train_time:22836ms step_avg:33.98ms
step:673/2000 train_time:22897ms step_avg:34.02ms
step:674/2000 train_time:22956ms step_avg:34.06ms
step:675/2000 train_time:23016ms step_avg:34.10ms
step:676/2000 train_time:23076ms step_avg:34.14ms
step:677/2000 train_time:23137ms step_avg:34.18ms
step:678/2000 train_time:23196ms step_avg:34.21ms
step:679/2000 train_time:23257ms step_avg:34.25ms
step:680/2000 train_time:23316ms step_avg:34.29ms
step:681/2000 train_time:23376ms step_avg:34.33ms
step:682/2000 train_time:23436ms step_avg:34.36ms
step:683/2000 train_time:23497ms step_avg:34.40ms
step:684/2000 train_time:23557ms step_avg:34.44ms
step:685/2000 train_time:23618ms step_avg:34.48ms
step:686/2000 train_time:23678ms step_avg:34.52ms
step:687/2000 train_time:23739ms step_avg:34.55ms
step:688/2000 train_time:23798ms step_avg:34.59ms
step:689/2000 train_time:23858ms step_avg:34.63ms
step:690/2000 train_time:23917ms step_avg:34.66ms
step:691/2000 train_time:23977ms step_avg:34.70ms
step:692/2000 train_time:24036ms step_avg:34.73ms
step:693/2000 train_time:24097ms step_avg:34.77ms
step:694/2000 train_time:24156ms step_avg:34.81ms
step:695/2000 train_time:24217ms step_avg:34.84ms
step:696/2000 train_time:24276ms step_avg:34.88ms
step:697/2000 train_time:24337ms step_avg:34.92ms
step:698/2000 train_time:24396ms step_avg:34.95ms
step:699/2000 train_time:24457ms step_avg:34.99ms
step:700/2000 train_time:24517ms step_avg:35.02ms
step:701/2000 train_time:24579ms step_avg:35.06ms
step:702/2000 train_time:24638ms step_avg:35.10ms
step:703/2000 train_time:24698ms step_avg:35.13ms
step:704/2000 train_time:24758ms step_avg:35.17ms
step:705/2000 train_time:24818ms step_avg:35.20ms
step:706/2000 train_time:24878ms step_avg:35.24ms
step:707/2000 train_time:24938ms step_avg:35.27ms
step:708/2000 train_time:24997ms step_avg:35.31ms
step:709/2000 train_time:25057ms step_avg:35.34ms
step:710/2000 train_time:25117ms step_avg:35.38ms
step:711/2000 train_time:25177ms step_avg:35.41ms
step:712/2000 train_time:25237ms step_avg:35.45ms
step:713/2000 train_time:25298ms step_avg:35.48ms
step:714/2000 train_time:25357ms step_avg:35.51ms
step:715/2000 train_time:25418ms step_avg:35.55ms
step:716/2000 train_time:25478ms step_avg:35.58ms
step:717/2000 train_time:25539ms step_avg:35.62ms
step:718/2000 train_time:25598ms step_avg:35.65ms
step:719/2000 train_time:25658ms step_avg:35.69ms
step:720/2000 train_time:25718ms step_avg:35.72ms
step:721/2000 train_time:25779ms step_avg:35.76ms
step:722/2000 train_time:25839ms step_avg:35.79ms
step:723/2000 train_time:25899ms step_avg:35.82ms
step:724/2000 train_time:25959ms step_avg:35.86ms
step:725/2000 train_time:26020ms step_avg:35.89ms
step:726/2000 train_time:26080ms step_avg:35.92ms
step:727/2000 train_time:26141ms step_avg:35.96ms
step:728/2000 train_time:26200ms step_avg:35.99ms
step:729/2000 train_time:26261ms step_avg:36.02ms
step:730/2000 train_time:26320ms step_avg:36.05ms
step:731/2000 train_time:26381ms step_avg:36.09ms
step:732/2000 train_time:26440ms step_avg:36.12ms
step:733/2000 train_time:26501ms step_avg:36.15ms
step:734/2000 train_time:26560ms step_avg:36.19ms
step:735/2000 train_time:26621ms step_avg:36.22ms
step:736/2000 train_time:26680ms step_avg:36.25ms
step:737/2000 train_time:26740ms step_avg:36.28ms
step:738/2000 train_time:26799ms step_avg:36.31ms
step:739/2000 train_time:26860ms step_avg:36.35ms
step:740/2000 train_time:26920ms step_avg:36.38ms
step:741/2000 train_time:26981ms step_avg:36.41ms
step:742/2000 train_time:27040ms step_avg:36.44ms
step:743/2000 train_time:27100ms step_avg:36.47ms
step:744/2000 train_time:27160ms step_avg:36.51ms
step:745/2000 train_time:27220ms step_avg:36.54ms
step:746/2000 train_time:27279ms step_avg:36.57ms
step:747/2000 train_time:27341ms step_avg:36.60ms
step:748/2000 train_time:27400ms step_avg:36.63ms
step:749/2000 train_time:27460ms step_avg:36.66ms
step:750/2000 train_time:27520ms step_avg:36.69ms
step:750/2000 val_loss:3.8187 train_time:27583ms step_avg:36.78ms
step:751/2000 train_time:27606ms step_avg:36.76ms
step:752/2000 train_time:27644ms step_avg:36.76ms
step:753/2000 train_time:27709ms step_avg:36.80ms
step:754/2000 train_time:27770ms step_avg:36.83ms
step:755/2000 train_time:27830ms step_avg:36.86ms
step:756/2000 train_time:27890ms step_avg:36.89ms
step:757/2000 train_time:27950ms step_avg:36.92ms
step:758/2000 train_time:28008ms step_avg:36.95ms
step:759/2000 train_time:28068ms step_avg:36.98ms
step:760/2000 train_time:28126ms step_avg:37.01ms
step:761/2000 train_time:28186ms step_avg:37.04ms
step:762/2000 train_time:28245ms step_avg:37.07ms
step:763/2000 train_time:28304ms step_avg:37.10ms
step:764/2000 train_time:28364ms step_avg:37.13ms
step:765/2000 train_time:28424ms step_avg:37.15ms
step:766/2000 train_time:28482ms step_avg:37.18ms
step:767/2000 train_time:28544ms step_avg:37.21ms
step:768/2000 train_time:28605ms step_avg:37.25ms
step:769/2000 train_time:28667ms step_avg:37.28ms
step:770/2000 train_time:28728ms step_avg:37.31ms
step:771/2000 train_time:28789ms step_avg:37.34ms
step:772/2000 train_time:28849ms step_avg:37.37ms
step:773/2000 train_time:28909ms step_avg:37.40ms
step:774/2000 train_time:28969ms step_avg:37.43ms
step:775/2000 train_time:29030ms step_avg:37.46ms
step:776/2000 train_time:29088ms step_avg:37.49ms
step:777/2000 train_time:29148ms step_avg:37.51ms
step:778/2000 train_time:29207ms step_avg:37.54ms
step:779/2000 train_time:29266ms step_avg:37.57ms
step:780/2000 train_time:29325ms step_avg:37.60ms
step:781/2000 train_time:29385ms step_avg:37.63ms
step:782/2000 train_time:29444ms step_avg:37.65ms
step:783/2000 train_time:29505ms step_avg:37.68ms
step:784/2000 train_time:29564ms step_avg:37.71ms
step:785/2000 train_time:29626ms step_avg:37.74ms
step:786/2000 train_time:29687ms step_avg:37.77ms
step:787/2000 train_time:29748ms step_avg:37.80ms
step:788/2000 train_time:29808ms step_avg:37.83ms
step:789/2000 train_time:29868ms step_avg:37.86ms
step:790/2000 train_time:29928ms step_avg:37.88ms
step:791/2000 train_time:29988ms step_avg:37.91ms
step:792/2000 train_time:30047ms step_avg:37.94ms
step:793/2000 train_time:30107ms step_avg:37.97ms
step:794/2000 train_time:30166ms step_avg:37.99ms
step:795/2000 train_time:30227ms step_avg:38.02ms
step:796/2000 train_time:30286ms step_avg:38.05ms
step:797/2000 train_time:30346ms step_avg:38.08ms
step:798/2000 train_time:30406ms step_avg:38.10ms
step:799/2000 train_time:30466ms step_avg:38.13ms
step:800/2000 train_time:30527ms step_avg:38.16ms
step:801/2000 train_time:30588ms step_avg:38.19ms
step:802/2000 train_time:30648ms step_avg:38.21ms
step:803/2000 train_time:30709ms step_avg:38.24ms
step:804/2000 train_time:30769ms step_avg:38.27ms
step:805/2000 train_time:30830ms step_avg:38.30ms
step:806/2000 train_time:30889ms step_avg:38.32ms
step:807/2000 train_time:30949ms step_avg:38.35ms
step:808/2000 train_time:31008ms step_avg:38.38ms
step:809/2000 train_time:31069ms step_avg:38.40ms
step:810/2000 train_time:31128ms step_avg:38.43ms
step:811/2000 train_time:31188ms step_avg:38.46ms
step:812/2000 train_time:31247ms step_avg:38.48ms
step:813/2000 train_time:31307ms step_avg:38.51ms
step:814/2000 train_time:31366ms step_avg:38.53ms
step:815/2000 train_time:31427ms step_avg:38.56ms
step:816/2000 train_time:31487ms step_avg:38.59ms
step:817/2000 train_time:31547ms step_avg:38.61ms
step:818/2000 train_time:31607ms step_avg:38.64ms
step:819/2000 train_time:31668ms step_avg:38.67ms
step:820/2000 train_time:31728ms step_avg:38.69ms
step:821/2000 train_time:31789ms step_avg:38.72ms
step:822/2000 train_time:31848ms step_avg:38.74ms
step:823/2000 train_time:31908ms step_avg:38.77ms
step:824/2000 train_time:31968ms step_avg:38.80ms
step:825/2000 train_time:32029ms step_avg:38.82ms
step:826/2000 train_time:32087ms step_avg:38.85ms
step:827/2000 train_time:32147ms step_avg:38.87ms
step:828/2000 train_time:32206ms step_avg:38.90ms
step:829/2000 train_time:32266ms step_avg:38.92ms
step:830/2000 train_time:32325ms step_avg:38.95ms
step:831/2000 train_time:32385ms step_avg:38.97ms
step:832/2000 train_time:32444ms step_avg:39.00ms
step:833/2000 train_time:32505ms step_avg:39.02ms
step:834/2000 train_time:32565ms step_avg:39.05ms
step:835/2000 train_time:32626ms step_avg:39.07ms
step:836/2000 train_time:32685ms step_avg:39.10ms
step:837/2000 train_time:32746ms step_avg:39.12ms
step:838/2000 train_time:32807ms step_avg:39.15ms
step:839/2000 train_time:32868ms step_avg:39.18ms
step:840/2000 train_time:32928ms step_avg:39.20ms
step:841/2000 train_time:32989ms step_avg:39.23ms
step:842/2000 train_time:33048ms step_avg:39.25ms
step:843/2000 train_time:33109ms step_avg:39.27ms
step:844/2000 train_time:33168ms step_avg:39.30ms
step:845/2000 train_time:33230ms step_avg:39.32ms
step:846/2000 train_time:33288ms step_avg:39.35ms
step:847/2000 train_time:33348ms step_avg:39.37ms
step:848/2000 train_time:33407ms step_avg:39.39ms
step:849/2000 train_time:33467ms step_avg:39.42ms
step:850/2000 train_time:33526ms step_avg:39.44ms
step:851/2000 train_time:33587ms step_avg:39.47ms
step:852/2000 train_time:33646ms step_avg:39.49ms
step:853/2000 train_time:33708ms step_avg:39.52ms
step:854/2000 train_time:33768ms step_avg:39.54ms
step:855/2000 train_time:33829ms step_avg:39.57ms
step:856/2000 train_time:33889ms step_avg:39.59ms
step:857/2000 train_time:33949ms step_avg:39.61ms
step:858/2000 train_time:34009ms step_avg:39.64ms
step:859/2000 train_time:34069ms step_avg:39.66ms
step:860/2000 train_time:34128ms step_avg:39.68ms
step:861/2000 train_time:34189ms step_avg:39.71ms
step:862/2000 train_time:34248ms step_avg:39.73ms
step:863/2000 train_time:34308ms step_avg:39.75ms
step:864/2000 train_time:34367ms step_avg:39.78ms
step:865/2000 train_time:34429ms step_avg:39.80ms
step:866/2000 train_time:34488ms step_avg:39.82ms
step:867/2000 train_time:34549ms step_avg:39.85ms
step:868/2000 train_time:34608ms step_avg:39.87ms
step:869/2000 train_time:34669ms step_avg:39.89ms
step:870/2000 train_time:34728ms step_avg:39.92ms
step:871/2000 train_time:34789ms step_avg:39.94ms
step:872/2000 train_time:34848ms step_avg:39.96ms
step:873/2000 train_time:34909ms step_avg:39.99ms
step:874/2000 train_time:34968ms step_avg:40.01ms
step:875/2000 train_time:35028ms step_avg:40.03ms
step:876/2000 train_time:35086ms step_avg:40.05ms
step:877/2000 train_time:35147ms step_avg:40.08ms
step:878/2000 train_time:35206ms step_avg:40.10ms
step:879/2000 train_time:35267ms step_avg:40.12ms
step:880/2000 train_time:35326ms step_avg:40.14ms
step:881/2000 train_time:35388ms step_avg:40.17ms
step:882/2000 train_time:35447ms step_avg:40.19ms
step:883/2000 train_time:35507ms step_avg:40.21ms
step:884/2000 train_time:35567ms step_avg:40.23ms
step:885/2000 train_time:35628ms step_avg:40.26ms
step:886/2000 train_time:35687ms step_avg:40.28ms
step:887/2000 train_time:35748ms step_avg:40.30ms
step:888/2000 train_time:35808ms step_avg:40.32ms
step:889/2000 train_time:35869ms step_avg:40.35ms
step:890/2000 train_time:35929ms step_avg:40.37ms
step:891/2000 train_time:35989ms step_avg:40.39ms
step:892/2000 train_time:36048ms step_avg:40.41ms
step:893/2000 train_time:36109ms step_avg:40.44ms
step:894/2000 train_time:36168ms step_avg:40.46ms
step:895/2000 train_time:36227ms step_avg:40.48ms
step:896/2000 train_time:36287ms step_avg:40.50ms
step:897/2000 train_time:36347ms step_avg:40.52ms
step:898/2000 train_time:36407ms step_avg:40.54ms
step:899/2000 train_time:36467ms step_avg:40.56ms
step:900/2000 train_time:36526ms step_avg:40.58ms
step:901/2000 train_time:36587ms step_avg:40.61ms
step:902/2000 train_time:36646ms step_avg:40.63ms
step:903/2000 train_time:36707ms step_avg:40.65ms
step:904/2000 train_time:36767ms step_avg:40.67ms
step:905/2000 train_time:36828ms step_avg:40.69ms
step:906/2000 train_time:36888ms step_avg:40.71ms
step:907/2000 train_time:36949ms step_avg:40.74ms
step:908/2000 train_time:37009ms step_avg:40.76ms
step:909/2000 train_time:37069ms step_avg:40.78ms
step:910/2000 train_time:37128ms step_avg:40.80ms
step:911/2000 train_time:37188ms step_avg:40.82ms
step:912/2000 train_time:37248ms step_avg:40.84ms
step:913/2000 train_time:37308ms step_avg:40.86ms
step:914/2000 train_time:37367ms step_avg:40.88ms
step:915/2000 train_time:37427ms step_avg:40.90ms
step:916/2000 train_time:37487ms step_avg:40.92ms
step:917/2000 train_time:37547ms step_avg:40.95ms
step:918/2000 train_time:37607ms step_avg:40.97ms
step:919/2000 train_time:37668ms step_avg:40.99ms
step:920/2000 train_time:37728ms step_avg:41.01ms
step:921/2000 train_time:37789ms step_avg:41.03ms
step:922/2000 train_time:37848ms step_avg:41.05ms
step:923/2000 train_time:37909ms step_avg:41.07ms
step:924/2000 train_time:37968ms step_avg:41.09ms
step:925/2000 train_time:38029ms step_avg:41.11ms
step:926/2000 train_time:38088ms step_avg:41.13ms
step:927/2000 train_time:38149ms step_avg:41.15ms
step:928/2000 train_time:38208ms step_avg:41.17ms
step:929/2000 train_time:38269ms step_avg:41.19ms
step:930/2000 train_time:38328ms step_avg:41.21ms
step:931/2000 train_time:38389ms step_avg:41.23ms
step:932/2000 train_time:38448ms step_avg:41.25ms
step:933/2000 train_time:38508ms step_avg:41.27ms
step:934/2000 train_time:38568ms step_avg:41.29ms
step:935/2000 train_time:38630ms step_avg:41.32ms
step:936/2000 train_time:38689ms step_avg:41.33ms
step:937/2000 train_time:38750ms step_avg:41.36ms
step:938/2000 train_time:38810ms step_avg:41.37ms
step:939/2000 train_time:38870ms step_avg:41.39ms
step:940/2000 train_time:38929ms step_avg:41.41ms
step:941/2000 train_time:38990ms step_avg:41.43ms
step:942/2000 train_time:39049ms step_avg:41.45ms
step:943/2000 train_time:39109ms step_avg:41.47ms
step:944/2000 train_time:39168ms step_avg:41.49ms
step:945/2000 train_time:39228ms step_avg:41.51ms
step:946/2000 train_time:39288ms step_avg:41.53ms
step:947/2000 train_time:39348ms step_avg:41.55ms
step:948/2000 train_time:39408ms step_avg:41.57ms
step:949/2000 train_time:39469ms step_avg:41.59ms
step:950/2000 train_time:39528ms step_avg:41.61ms
step:951/2000 train_time:39589ms step_avg:41.63ms
step:952/2000 train_time:39648ms step_avg:41.65ms
step:953/2000 train_time:39709ms step_avg:41.67ms
step:954/2000 train_time:39768ms step_avg:41.69ms
step:955/2000 train_time:39828ms step_avg:41.70ms
step:956/2000 train_time:39887ms step_avg:41.72ms
step:957/2000 train_time:39948ms step_avg:41.74ms
step:958/2000 train_time:40007ms step_avg:41.76ms
step:959/2000 train_time:40069ms step_avg:41.78ms
step:960/2000 train_time:40128ms step_avg:41.80ms
step:961/2000 train_time:40189ms step_avg:41.82ms
step:962/2000 train_time:40248ms step_avg:41.84ms
step:963/2000 train_time:40307ms step_avg:41.86ms
step:964/2000 train_time:40367ms step_avg:41.87ms
step:965/2000 train_time:40427ms step_avg:41.89ms
step:966/2000 train_time:40486ms step_avg:41.91ms
step:967/2000 train_time:40546ms step_avg:41.93ms
step:968/2000 train_time:40606ms step_avg:41.95ms
step:969/2000 train_time:40668ms step_avg:41.97ms
step:970/2000 train_time:40727ms step_avg:41.99ms
step:971/2000 train_time:40788ms step_avg:42.01ms
step:972/2000 train_time:40848ms step_avg:42.02ms
step:973/2000 train_time:40908ms step_avg:42.04ms
step:974/2000 train_time:40967ms step_avg:42.06ms
step:975/2000 train_time:41028ms step_avg:42.08ms
step:976/2000 train_time:41088ms step_avg:42.10ms
step:977/2000 train_time:41147ms step_avg:42.12ms
step:978/2000 train_time:41206ms step_avg:42.13ms
step:979/2000 train_time:41267ms step_avg:42.15ms
step:980/2000 train_time:41326ms step_avg:42.17ms
step:981/2000 train_time:41386ms step_avg:42.19ms
step:982/2000 train_time:41445ms step_avg:42.20ms
step:983/2000 train_time:41505ms step_avg:42.22ms
step:984/2000 train_time:41565ms step_avg:42.24ms
step:985/2000 train_time:41625ms step_avg:42.26ms
step:986/2000 train_time:41685ms step_avg:42.28ms
step:987/2000 train_time:41746ms step_avg:42.30ms
step:988/2000 train_time:41806ms step_avg:42.31ms
step:989/2000 train_time:41867ms step_avg:42.33ms
step:990/2000 train_time:41927ms step_avg:42.35ms
step:991/2000 train_time:41987ms step_avg:42.37ms
step:992/2000 train_time:42047ms step_avg:42.39ms
step:993/2000 train_time:42107ms step_avg:42.40ms
step:994/2000 train_time:42167ms step_avg:42.42ms
step:995/2000 train_time:42228ms step_avg:42.44ms
step:996/2000 train_time:42287ms step_avg:42.46ms
step:997/2000 train_time:42347ms step_avg:42.47ms
step:998/2000 train_time:42406ms step_avg:42.49ms
step:999/2000 train_time:42466ms step_avg:42.51ms
step:1000/2000 train_time:42525ms step_avg:42.53ms
step:1000/2000 val_loss:3.6803 train_time:42588ms step_avg:42.59ms
step:1001/2000 train_time:42610ms step_avg:42.57ms
step:1002/2000 train_time:42648ms step_avg:42.56ms
step:1003/2000 train_time:42709ms step_avg:42.58ms
step:1004/2000 train_time:42771ms step_avg:42.60ms
step:1005/2000 train_time:42832ms step_avg:42.62ms
step:1006/2000 train_time:42890ms step_avg:42.63ms
step:1007/2000 train_time:42951ms step_avg:42.65ms
step:1008/2000 train_time:43010ms step_avg:42.67ms
step:1009/2000 train_time:43071ms step_avg:42.69ms
step:1010/2000 train_time:43129ms step_avg:42.70ms
step:1011/2000 train_time:43191ms step_avg:42.72ms
step:1012/2000 train_time:43250ms step_avg:42.74ms
step:1013/2000 train_time:43311ms step_avg:42.75ms
step:1014/2000 train_time:43370ms step_avg:42.77ms
step:1015/2000 train_time:43430ms step_avg:42.79ms
step:1016/2000 train_time:43490ms step_avg:42.81ms
step:1017/2000 train_time:43552ms step_avg:42.82ms
step:1018/2000 train_time:43613ms step_avg:42.84ms
step:1019/2000 train_time:43674ms step_avg:42.86ms
step:1020/2000 train_time:43734ms step_avg:42.88ms
step:1021/2000 train_time:43795ms step_avg:42.89ms
step:1022/2000 train_time:43854ms step_avg:42.91ms
step:1023/2000 train_time:43915ms step_avg:42.93ms
step:1024/2000 train_time:43974ms step_avg:42.94ms
step:1025/2000 train_time:44034ms step_avg:42.96ms
step:1026/2000 train_time:44092ms step_avg:42.98ms
step:1027/2000 train_time:44153ms step_avg:42.99ms
step:1028/2000 train_time:44212ms step_avg:43.01ms
step:1029/2000 train_time:44273ms step_avg:43.03ms
step:1030/2000 train_time:44333ms step_avg:43.04ms
step:1031/2000 train_time:44394ms step_avg:43.06ms
step:1032/2000 train_time:44454ms step_avg:43.08ms
step:1033/2000 train_time:44515ms step_avg:43.09ms
step:1034/2000 train_time:44574ms step_avg:43.11ms
step:1035/2000 train_time:44635ms step_avg:43.13ms
step:1036/2000 train_time:44695ms step_avg:43.14ms
step:1037/2000 train_time:44756ms step_avg:43.16ms
step:1038/2000 train_time:44815ms step_avg:43.17ms
step:1039/2000 train_time:44876ms step_avg:43.19ms
step:1040/2000 train_time:44935ms step_avg:43.21ms
step:1041/2000 train_time:44994ms step_avg:43.22ms
step:1042/2000 train_time:45054ms step_avg:43.24ms
step:1043/2000 train_time:45114ms step_avg:43.25ms
step:1044/2000 train_time:45173ms step_avg:43.27ms
step:1045/2000 train_time:45235ms step_avg:43.29ms
step:1046/2000 train_time:45294ms step_avg:43.30ms
step:1047/2000 train_time:45354ms step_avg:43.32ms
step:1048/2000 train_time:45414ms step_avg:43.33ms
step:1049/2000 train_time:45475ms step_avg:43.35ms
step:1050/2000 train_time:45535ms step_avg:43.37ms
step:1051/2000 train_time:45595ms step_avg:43.38ms
step:1052/2000 train_time:45655ms step_avg:43.40ms
step:1053/2000 train_time:45716ms step_avg:43.42ms
step:1054/2000 train_time:45775ms step_avg:43.43ms
step:1055/2000 train_time:45836ms step_avg:43.45ms
step:1056/2000 train_time:45895ms step_avg:43.46ms
step:1057/2000 train_time:45955ms step_avg:43.48ms
step:1058/2000 train_time:46015ms step_avg:43.49ms
step:1059/2000 train_time:46075ms step_avg:43.51ms
step:1060/2000 train_time:46134ms step_avg:43.52ms
step:1061/2000 train_time:46195ms step_avg:43.54ms
step:1062/2000 train_time:46254ms step_avg:43.55ms
step:1063/2000 train_time:46315ms step_avg:43.57ms
step:1064/2000 train_time:46374ms step_avg:43.58ms
step:1065/2000 train_time:46434ms step_avg:43.60ms
step:1066/2000 train_time:46493ms step_avg:43.61ms
step:1067/2000 train_time:46555ms step_avg:43.63ms
step:1068/2000 train_time:46615ms step_avg:43.65ms
step:1069/2000 train_time:46676ms step_avg:43.66ms
step:1070/2000 train_time:46735ms step_avg:43.68ms
step:1071/2000 train_time:46796ms step_avg:43.69ms
step:1072/2000 train_time:46855ms step_avg:43.71ms
step:1073/2000 train_time:46916ms step_avg:43.72ms
step:1074/2000 train_time:46976ms step_avg:43.74ms
step:1075/2000 train_time:47035ms step_avg:43.75ms
step:1076/2000 train_time:47094ms step_avg:43.77ms
step:1077/2000 train_time:47155ms step_avg:43.78ms
step:1078/2000 train_time:47214ms step_avg:43.80ms
step:1079/2000 train_time:47275ms step_avg:43.81ms
step:1080/2000 train_time:47334ms step_avg:43.83ms
step:1081/2000 train_time:47394ms step_avg:43.84ms
step:1082/2000 train_time:47453ms step_avg:43.86ms
step:1083/2000 train_time:47514ms step_avg:43.87ms
step:1084/2000 train_time:47573ms step_avg:43.89ms
step:1085/2000 train_time:47635ms step_avg:43.90ms
step:1086/2000 train_time:47694ms step_avg:43.92ms
step:1087/2000 train_time:47755ms step_avg:43.93ms
step:1088/2000 train_time:47814ms step_avg:43.95ms
step:1089/2000 train_time:47875ms step_avg:43.96ms
step:1090/2000 train_time:47934ms step_avg:43.98ms
step:1091/2000 train_time:47995ms step_avg:43.99ms
step:1092/2000 train_time:48054ms step_avg:44.01ms
step:1093/2000 train_time:48114ms step_avg:44.02ms
step:1094/2000 train_time:48173ms step_avg:44.03ms
step:1095/2000 train_time:48233ms step_avg:44.05ms
step:1096/2000 train_time:48292ms step_avg:44.06ms
step:1097/2000 train_time:48353ms step_avg:44.08ms
step:1098/2000 train_time:48413ms step_avg:44.09ms
step:1099/2000 train_time:48473ms step_avg:44.11ms
step:1100/2000 train_time:48532ms step_avg:44.12ms
step:1101/2000 train_time:48593ms step_avg:44.14ms
step:1102/2000 train_time:48653ms step_avg:44.15ms
step:1103/2000 train_time:48714ms step_avg:44.16ms
step:1104/2000 train_time:48773ms step_avg:44.18ms
step:1105/2000 train_time:48833ms step_avg:44.19ms
step:1106/2000 train_time:48893ms step_avg:44.21ms
step:1107/2000 train_time:48953ms step_avg:44.22ms
step:1108/2000 train_time:49013ms step_avg:44.24ms
step:1109/2000 train_time:49074ms step_avg:44.25ms
step:1110/2000 train_time:49134ms step_avg:44.26ms
step:1111/2000 train_time:49195ms step_avg:44.28ms
step:1112/2000 train_time:49254ms step_avg:44.29ms
step:1113/2000 train_time:49314ms step_avg:44.31ms
step:1114/2000 train_time:49373ms step_avg:44.32ms
step:1115/2000 train_time:49433ms step_avg:44.33ms
step:1116/2000 train_time:49492ms step_avg:44.35ms
step:1117/2000 train_time:49553ms step_avg:44.36ms
step:1118/2000 train_time:49613ms step_avg:44.38ms
step:1119/2000 train_time:49673ms step_avg:44.39ms
step:1120/2000 train_time:49733ms step_avg:44.40ms
step:1121/2000 train_time:49793ms step_avg:44.42ms
step:1122/2000 train_time:49853ms step_avg:44.43ms
step:1123/2000 train_time:49914ms step_avg:44.45ms
step:1124/2000 train_time:49974ms step_avg:44.46ms
step:1125/2000 train_time:50034ms step_avg:44.47ms
step:1126/2000 train_time:50093ms step_avg:44.49ms
step:1127/2000 train_time:50154ms step_avg:44.50ms
step:1128/2000 train_time:50213ms step_avg:44.52ms
step:1129/2000 train_time:50274ms step_avg:44.53ms
step:1130/2000 train_time:50334ms step_avg:44.54ms
step:1131/2000 train_time:50394ms step_avg:44.56ms
step:1132/2000 train_time:50454ms step_avg:44.57ms
step:1133/2000 train_time:50515ms step_avg:44.59ms
step:1134/2000 train_time:50574ms step_avg:44.60ms
step:1135/2000 train_time:50636ms step_avg:44.61ms
step:1136/2000 train_time:50695ms step_avg:44.63ms
step:1137/2000 train_time:50755ms step_avg:44.64ms
step:1138/2000 train_time:50814ms step_avg:44.65ms
step:1139/2000 train_time:50874ms step_avg:44.67ms
step:1140/2000 train_time:50933ms step_avg:44.68ms
step:1141/2000 train_time:50994ms step_avg:44.69ms
step:1142/2000 train_time:51054ms step_avg:44.71ms
step:1143/2000 train_time:51114ms step_avg:44.72ms
step:1144/2000 train_time:51173ms step_avg:44.73ms
step:1145/2000 train_time:51235ms step_avg:44.75ms
step:1146/2000 train_time:51294ms step_avg:44.76ms
step:1147/2000 train_time:51354ms step_avg:44.77ms
step:1148/2000 train_time:51414ms step_avg:44.79ms
step:1149/2000 train_time:51475ms step_avg:44.80ms
step:1150/2000 train_time:51534ms step_avg:44.81ms
step:1151/2000 train_time:51594ms step_avg:44.83ms
step:1152/2000 train_time:51654ms step_avg:44.84ms
step:1153/2000 train_time:51714ms step_avg:44.85ms
step:1154/2000 train_time:51774ms step_avg:44.86ms
step:1155/2000 train_time:51835ms step_avg:44.88ms
step:1156/2000 train_time:51894ms step_avg:44.89ms
step:1157/2000 train_time:51955ms step_avg:44.91ms
step:1158/2000 train_time:52014ms step_avg:44.92ms
step:1159/2000 train_time:52075ms step_avg:44.93ms
step:1160/2000 train_time:52134ms step_avg:44.94ms
step:1161/2000 train_time:52194ms step_avg:44.96ms
step:1162/2000 train_time:52253ms step_avg:44.97ms
step:1163/2000 train_time:52314ms step_avg:44.98ms
step:1164/2000 train_time:52374ms step_avg:44.99ms
step:1165/2000 train_time:52435ms step_avg:45.01ms
step:1166/2000 train_time:52495ms step_avg:45.02ms
step:1167/2000 train_time:52556ms step_avg:45.03ms
step:1168/2000 train_time:52615ms step_avg:45.05ms
step:1169/2000 train_time:52676ms step_avg:45.06ms
step:1170/2000 train_time:52735ms step_avg:45.07ms
step:1171/2000 train_time:52796ms step_avg:45.09ms
step:1172/2000 train_time:52855ms step_avg:45.10ms
step:1173/2000 train_time:52916ms step_avg:45.11ms
step:1174/2000 train_time:52975ms step_avg:45.12ms
step:1175/2000 train_time:53035ms step_avg:45.14ms
step:1176/2000 train_time:53094ms step_avg:45.15ms
step:1177/2000 train_time:53154ms step_avg:45.16ms
step:1178/2000 train_time:53213ms step_avg:45.17ms
step:1179/2000 train_time:53273ms step_avg:45.19ms
step:1180/2000 train_time:53333ms step_avg:45.20ms
step:1181/2000 train_time:53394ms step_avg:45.21ms
step:1182/2000 train_time:53453ms step_avg:45.22ms
step:1183/2000 train_time:53513ms step_avg:45.24ms
step:1184/2000 train_time:53573ms step_avg:45.25ms
step:1185/2000 train_time:53633ms step_avg:45.26ms
step:1186/2000 train_time:53692ms step_avg:45.27ms
step:1187/2000 train_time:53753ms step_avg:45.28ms
step:1188/2000 train_time:53813ms step_avg:45.30ms
step:1189/2000 train_time:53874ms step_avg:45.31ms
step:1190/2000 train_time:53934ms step_avg:45.32ms
step:1191/2000 train_time:53994ms step_avg:45.34ms
step:1192/2000 train_time:54054ms step_avg:45.35ms
step:1193/2000 train_time:54114ms step_avg:45.36ms
step:1194/2000 train_time:54174ms step_avg:45.37ms
step:1195/2000 train_time:54234ms step_avg:45.38ms
step:1196/2000 train_time:54293ms step_avg:45.40ms
step:1197/2000 train_time:54354ms step_avg:45.41ms
step:1198/2000 train_time:54414ms step_avg:45.42ms
step:1199/2000 train_time:54475ms step_avg:45.43ms
step:1200/2000 train_time:54535ms step_avg:45.45ms
step:1201/2000 train_time:54595ms step_avg:45.46ms
step:1202/2000 train_time:54655ms step_avg:45.47ms
step:1203/2000 train_time:54716ms step_avg:45.48ms
step:1204/2000 train_time:54775ms step_avg:45.49ms
step:1205/2000 train_time:54836ms step_avg:45.51ms
step:1206/2000 train_time:54895ms step_avg:45.52ms
step:1207/2000 train_time:54956ms step_avg:45.53ms
step:1208/2000 train_time:55015ms step_avg:45.54ms
step:1209/2000 train_time:55076ms step_avg:45.55ms
step:1210/2000 train_time:55134ms step_avg:45.57ms
step:1211/2000 train_time:55195ms step_avg:45.58ms
step:1212/2000 train_time:55254ms step_avg:45.59ms
step:1213/2000 train_time:55314ms step_avg:45.60ms
step:1214/2000 train_time:55374ms step_avg:45.61ms
step:1215/2000 train_time:55434ms step_avg:45.63ms
step:1216/2000 train_time:55494ms step_avg:45.64ms
step:1217/2000 train_time:55556ms step_avg:45.65ms
step:1218/2000 train_time:55614ms step_avg:45.66ms
step:1219/2000 train_time:55675ms step_avg:45.67ms
step:1220/2000 train_time:55734ms step_avg:45.68ms
step:1221/2000 train_time:55795ms step_avg:45.70ms
step:1222/2000 train_time:55854ms step_avg:45.71ms
step:1223/2000 train_time:55915ms step_avg:45.72ms
step:1224/2000 train_time:55974ms step_avg:45.73ms
step:1225/2000 train_time:56035ms step_avg:45.74ms
step:1226/2000 train_time:56094ms step_avg:45.75ms
step:1227/2000 train_time:56155ms step_avg:45.77ms
step:1228/2000 train_time:56214ms step_avg:45.78ms
step:1229/2000 train_time:56275ms step_avg:45.79ms
step:1230/2000 train_time:56334ms step_avg:45.80ms
step:1231/2000 train_time:56394ms step_avg:45.81ms
step:1232/2000 train_time:56454ms step_avg:45.82ms
step:1233/2000 train_time:56514ms step_avg:45.83ms
step:1234/2000 train_time:56574ms step_avg:45.85ms
step:1235/2000 train_time:56634ms step_avg:45.86ms
step:1236/2000 train_time:56693ms step_avg:45.87ms
step:1237/2000 train_time:56754ms step_avg:45.88ms
step:1238/2000 train_time:56813ms step_avg:45.89ms
step:1239/2000 train_time:56874ms step_avg:45.90ms
step:1240/2000 train_time:56933ms step_avg:45.91ms
step:1241/2000 train_time:56994ms step_avg:45.93ms
step:1242/2000 train_time:57054ms step_avg:45.94ms
step:1243/2000 train_time:57114ms step_avg:45.95ms
step:1244/2000 train_time:57173ms step_avg:45.96ms
step:1245/2000 train_time:57234ms step_avg:45.97ms
step:1246/2000 train_time:57293ms step_avg:45.98ms
step:1247/2000 train_time:57354ms step_avg:45.99ms
step:1248/2000 train_time:57414ms step_avg:46.00ms
step:1249/2000 train_time:57476ms step_avg:46.02ms
step:1250/2000 train_time:57535ms step_avg:46.03ms
step:1250/2000 val_loss:3.5607 train_time:57597ms step_avg:46.08ms
step:1251/2000 train_time:57617ms step_avg:46.06ms
step:1252/2000 train_time:57660ms step_avg:46.05ms
step:1253/2000 train_time:57722ms step_avg:46.07ms
step:1254/2000 train_time:57786ms step_avg:46.08ms
step:1255/2000 train_time:57847ms step_avg:46.09ms
step:1256/2000 train_time:57906ms step_avg:46.10ms
step:1257/2000 train_time:57967ms step_avg:46.12ms
step:1258/2000 train_time:58026ms step_avg:46.13ms
step:1259/2000 train_time:58085ms step_avg:46.14ms
step:1260/2000 train_time:58144ms step_avg:46.15ms
step:1261/2000 train_time:58205ms step_avg:46.16ms
step:1262/2000 train_time:58265ms step_avg:46.17ms
step:1263/2000 train_time:58324ms step_avg:46.18ms
step:1264/2000 train_time:58384ms step_avg:46.19ms
step:1265/2000 train_time:58443ms step_avg:46.20ms
step:1266/2000 train_time:58502ms step_avg:46.21ms
step:1267/2000 train_time:58564ms step_avg:46.22ms
step:1268/2000 train_time:58626ms step_avg:46.23ms
step:1269/2000 train_time:58688ms step_avg:46.25ms
step:1270/2000 train_time:58749ms step_avg:46.26ms
step:1271/2000 train_time:58810ms step_avg:46.27ms
step:1272/2000 train_time:58870ms step_avg:46.28ms
step:1273/2000 train_time:58930ms step_avg:46.29ms
step:1274/2000 train_time:58990ms step_avg:46.30ms
step:1275/2000 train_time:59049ms step_avg:46.31ms
step:1276/2000 train_time:59109ms step_avg:46.32ms
step:1277/2000 train_time:59169ms step_avg:46.33ms
step:1278/2000 train_time:59228ms step_avg:46.34ms
step:1279/2000 train_time:59289ms step_avg:46.36ms
step:1280/2000 train_time:59349ms step_avg:46.37ms
step:1281/2000 train_time:59409ms step_avg:46.38ms
step:1282/2000 train_time:59469ms step_avg:46.39ms
step:1283/2000 train_time:59530ms step_avg:46.40ms
step:1284/2000 train_time:59591ms step_avg:46.41ms
step:1285/2000 train_time:59654ms step_avg:46.42ms
step:1286/2000 train_time:59714ms step_avg:46.43ms
step:1287/2000 train_time:59775ms step_avg:46.45ms
step:1288/2000 train_time:59835ms step_avg:46.46ms
step:1289/2000 train_time:59895ms step_avg:46.47ms
step:1290/2000 train_time:59955ms step_avg:46.48ms
step:1291/2000 train_time:60015ms step_avg:46.49ms
step:1292/2000 train_time:60075ms step_avg:46.50ms
step:1293/2000 train_time:60135ms step_avg:46.51ms
step:1294/2000 train_time:60195ms step_avg:46.52ms
step:1295/2000 train_time:60256ms step_avg:46.53ms
step:1296/2000 train_time:60316ms step_avg:46.54ms
step:1297/2000 train_time:60376ms step_avg:46.55ms
step:1298/2000 train_time:60436ms step_avg:46.56ms
step:1299/2000 train_time:60496ms step_avg:46.57ms
step:1300/2000 train_time:60556ms step_avg:46.58ms
step:1301/2000 train_time:60617ms step_avg:46.59ms
step:1302/2000 train_time:60677ms step_avg:46.60ms
step:1303/2000 train_time:60738ms step_avg:46.61ms
step:1304/2000 train_time:60797ms step_avg:46.62ms
step:1305/2000 train_time:60858ms step_avg:46.63ms
step:1306/2000 train_time:60917ms step_avg:46.64ms
step:1307/2000 train_time:60978ms step_avg:46.65ms
step:1308/2000 train_time:61037ms step_avg:46.66ms
step:1309/2000 train_time:61126ms step_avg:46.70ms
step:1310/2000 train_time:61212ms step_avg:46.73ms
step:1311/2000 train_time:61300ms step_avg:46.76ms
step:1312/2000 train_time:61387ms step_avg:46.79ms
step:1313/2000 train_time:61475ms step_avg:46.82ms
step:1314/2000 train_time:61563ms step_avg:46.85ms
step:1315/2000 train_time:61651ms step_avg:46.88ms
step:1316/2000 train_time:61740ms step_avg:46.91ms
step:1317/2000 train_time:61829ms step_avg:46.95ms
step:1318/2000 train_time:61917ms step_avg:46.98ms
step:1319/2000 train_time:62005ms step_avg:47.01ms
step:1320/2000 train_time:62093ms step_avg:47.04ms
step:1321/2000 train_time:62180ms step_avg:47.07ms
step:1322/2000 train_time:62267ms step_avg:47.10ms
step:1323/2000 train_time:62355ms step_avg:47.13ms
step:1324/2000 train_time:62443ms step_avg:47.16ms
step:1325/2000 train_time:62531ms step_avg:47.19ms
step:1326/2000 train_time:62618ms step_avg:47.22ms
step:1327/2000 train_time:62707ms step_avg:47.25ms
step:1328/2000 train_time:62795ms step_avg:47.29ms
step:1329/2000 train_time:62885ms step_avg:47.32ms
step:1330/2000 train_time:62973ms step_avg:47.35ms
step:1331/2000 train_time:63060ms step_avg:47.38ms
step:1332/2000 train_time:63148ms step_avg:47.41ms
step:1333/2000 train_time:63235ms step_avg:47.44ms
step:1334/2000 train_time:63323ms step_avg:47.47ms
step:1335/2000 train_time:63412ms step_avg:47.50ms
step:1336/2000 train_time:63499ms step_avg:47.53ms
step:1337/2000 train_time:63587ms step_avg:47.56ms
step:1338/2000 train_time:63675ms step_avg:47.59ms
step:1339/2000 train_time:63764ms step_avg:47.62ms
step:1340/2000 train_time:63852ms step_avg:47.65ms
step:1341/2000 train_time:63940ms step_avg:47.68ms
step:1342/2000 train_time:64028ms step_avg:47.71ms
step:1343/2000 train_time:64116ms step_avg:47.74ms
step:1344/2000 train_time:64203ms step_avg:47.77ms
step:1345/2000 train_time:64291ms step_avg:47.80ms
step:1346/2000 train_time:64380ms step_avg:47.83ms
step:1347/2000 train_time:64468ms step_avg:47.86ms
step:1348/2000 train_time:64556ms step_avg:47.89ms
step:1349/2000 train_time:64644ms step_avg:47.92ms
step:1350/2000 train_time:64731ms step_avg:47.95ms
step:1351/2000 train_time:64819ms step_avg:47.98ms
step:1352/2000 train_time:64907ms step_avg:48.01ms
step:1353/2000 train_time:64995ms step_avg:48.04ms
step:1354/2000 train_time:65083ms step_avg:48.07ms
step:1355/2000 train_time:65171ms step_avg:48.10ms
step:1356/2000 train_time:65259ms step_avg:48.13ms
step:1357/2000 train_time:65348ms step_avg:48.16ms
step:1358/2000 train_time:65436ms step_avg:48.19ms
step:1359/2000 train_time:65524ms step_avg:48.21ms
step:1360/2000 train_time:65611ms step_avg:48.24ms
step:1361/2000 train_time:65699ms step_avg:48.27ms
step:1362/2000 train_time:65787ms step_avg:48.30ms
step:1363/2000 train_time:65875ms step_avg:48.33ms
step:1364/2000 train_time:65963ms step_avg:48.36ms
step:1365/2000 train_time:66051ms step_avg:48.39ms
step:1366/2000 train_time:66137ms step_avg:48.42ms
step:1367/2000 train_time:66225ms step_avg:48.45ms
step:1368/2000 train_time:66314ms step_avg:48.48ms
step:1369/2000 train_time:66403ms step_avg:48.50ms
step:1370/2000 train_time:66490ms step_avg:48.53ms
step:1371/2000 train_time:66577ms step_avg:48.56ms
step:1372/2000 train_time:66665ms step_avg:48.59ms
step:1373/2000 train_time:66753ms step_avg:48.62ms
step:1374/2000 train_time:66841ms step_avg:48.65ms
step:1375/2000 train_time:66929ms step_avg:48.68ms
step:1376/2000 train_time:67017ms step_avg:48.70ms
step:1377/2000 train_time:67106ms step_avg:48.73ms
step:1378/2000 train_time:67193ms step_avg:48.76ms
step:1379/2000 train_time:67281ms step_avg:48.79ms
step:1380/2000 train_time:67370ms step_avg:48.82ms
step:1381/2000 train_time:67457ms step_avg:48.85ms
step:1382/2000 train_time:67545ms step_avg:48.87ms
step:1383/2000 train_time:67633ms step_avg:48.90ms
step:1384/2000 train_time:67721ms step_avg:48.93ms
step:1385/2000 train_time:67809ms step_avg:48.96ms
step:1386/2000 train_time:67897ms step_avg:48.99ms
step:1387/2000 train_time:67986ms step_avg:49.02ms
step:1388/2000 train_time:68073ms step_avg:49.04ms
step:1389/2000 train_time:68161ms step_avg:49.07ms
step:1390/2000 train_time:68249ms step_avg:49.10ms
step:1391/2000 train_time:68336ms step_avg:49.13ms
step:1392/2000 train_time:68424ms step_avg:49.15ms
step:1393/2000 train_time:68511ms step_avg:49.18ms
step:1394/2000 train_time:68599ms step_avg:49.21ms
step:1395/2000 train_time:68687ms step_avg:49.24ms
step:1396/2000 train_time:68775ms step_avg:49.27ms
step:1397/2000 train_time:68863ms step_avg:49.29ms
step:1398/2000 train_time:68951ms step_avg:49.32ms
step:1399/2000 train_time:69039ms step_avg:49.35ms
step:1400/2000 train_time:69128ms step_avg:49.38ms
step:1401/2000 train_time:69215ms step_avg:49.40ms
step:1402/2000 train_time:69303ms step_avg:49.43ms
step:1403/2000 train_time:69390ms step_avg:49.46ms
step:1404/2000 train_time:69478ms step_avg:49.49ms
step:1405/2000 train_time:69567ms step_avg:49.51ms
step:1406/2000 train_time:69654ms step_avg:49.54ms
step:1407/2000 train_time:69743ms step_avg:49.57ms
step:1408/2000 train_time:69830ms step_avg:49.60ms
step:1409/2000 train_time:69918ms step_avg:49.62ms
step:1410/2000 train_time:70005ms step_avg:49.65ms
step:1411/2000 train_time:70094ms step_avg:49.68ms
step:1412/2000 train_time:70182ms step_avg:49.70ms
step:1413/2000 train_time:70271ms step_avg:49.73ms
step:1414/2000 train_time:70359ms step_avg:49.76ms
step:1415/2000 train_time:70447ms step_avg:49.79ms
step:1416/2000 train_time:70534ms step_avg:49.81ms
step:1417/2000 train_time:70623ms step_avg:49.84ms
step:1418/2000 train_time:70710ms step_avg:49.87ms
step:1419/2000 train_time:70798ms step_avg:49.89ms
step:1420/2000 train_time:70887ms step_avg:49.92ms
step:1421/2000 train_time:70974ms step_avg:49.95ms
step:1422/2000 train_time:71062ms step_avg:49.97ms
step:1423/2000 train_time:71150ms step_avg:50.00ms
step:1424/2000 train_time:71238ms step_avg:50.03ms
step:1425/2000 train_time:71327ms step_avg:50.05ms
step:1426/2000 train_time:71415ms step_avg:50.08ms
step:1427/2000 train_time:71503ms step_avg:50.11ms
step:1428/2000 train_time:71590ms step_avg:50.13ms
step:1429/2000 train_time:71679ms step_avg:50.16ms
step:1430/2000 train_time:71767ms step_avg:50.19ms
step:1431/2000 train_time:71855ms step_avg:50.21ms
step:1432/2000 train_time:71943ms step_avg:50.24ms
step:1433/2000 train_time:72031ms step_avg:50.27ms
step:1434/2000 train_time:72120ms step_avg:50.29ms
step:1435/2000 train_time:72209ms step_avg:50.32ms
step:1436/2000 train_time:72297ms step_avg:50.35ms
step:1437/2000 train_time:72385ms step_avg:50.37ms
step:1438/2000 train_time:72472ms step_avg:50.40ms
step:1439/2000 train_time:72560ms step_avg:50.42ms
step:1440/2000 train_time:72648ms step_avg:50.45ms
step:1441/2000 train_time:72735ms step_avg:50.48ms
step:1442/2000 train_time:72824ms step_avg:50.50ms
step:1443/2000 train_time:72912ms step_avg:50.53ms
step:1444/2000 train_time:73000ms step_avg:50.55ms
step:1445/2000 train_time:73089ms step_avg:50.58ms
step:1446/2000 train_time:73177ms step_avg:50.61ms
step:1447/2000 train_time:73266ms step_avg:50.63ms
step:1448/2000 train_time:73352ms step_avg:50.66ms
step:1449/2000 train_time:73440ms step_avg:50.68ms
step:1450/2000 train_time:73528ms step_avg:50.71ms
step:1451/2000 train_time:73616ms step_avg:50.73ms
step:1452/2000 train_time:73704ms step_avg:50.76ms
step:1453/2000 train_time:73793ms step_avg:50.79ms
step:1454/2000 train_time:73880ms step_avg:50.81ms
step:1455/2000 train_time:73969ms step_avg:50.84ms
step:1456/2000 train_time:74057ms step_avg:50.86ms
step:1457/2000 train_time:74146ms step_avg:50.89ms
step:1458/2000 train_time:74233ms step_avg:50.91ms
step:1459/2000 train_time:74322ms step_avg:50.94ms
step:1460/2000 train_time:74410ms step_avg:50.97ms
step:1461/2000 train_time:74498ms step_avg:50.99ms
step:1462/2000 train_time:74586ms step_avg:51.02ms
step:1463/2000 train_time:74673ms step_avg:51.04ms
step:1464/2000 train_time:74761ms step_avg:51.07ms
step:1465/2000 train_time:74849ms step_avg:51.09ms
step:1466/2000 train_time:74937ms step_avg:51.12ms
step:1467/2000 train_time:75025ms step_avg:51.14ms
step:1468/2000 train_time:75112ms step_avg:51.17ms
step:1469/2000 train_time:75201ms step_avg:51.19ms
step:1470/2000 train_time:75289ms step_avg:51.22ms
step:1471/2000 train_time:75376ms step_avg:51.24ms
step:1472/2000 train_time:75465ms step_avg:51.27ms
step:1473/2000 train_time:75552ms step_avg:51.29ms
step:1474/2000 train_time:75640ms step_avg:51.32ms
step:1475/2000 train_time:75728ms step_avg:51.34ms
step:1476/2000 train_time:75816ms step_avg:51.37ms
step:1477/2000 train_time:75904ms step_avg:51.39ms
step:1478/2000 train_time:75993ms step_avg:51.42ms
step:1479/2000 train_time:76081ms step_avg:51.44ms
step:1480/2000 train_time:76169ms step_avg:51.47ms
step:1481/2000 train_time:76257ms step_avg:51.49ms
step:1482/2000 train_time:76344ms step_avg:51.51ms
step:1483/2000 train_time:76431ms step_avg:51.54ms
step:1484/2000 train_time:76520ms step_avg:51.56ms
step:1485/2000 train_time:76607ms step_avg:51.59ms
step:1486/2000 train_time:76695ms step_avg:51.61ms
step:1487/2000 train_time:76783ms step_avg:51.64ms
step:1488/2000 train_time:76870ms step_avg:51.66ms
step:1489/2000 train_time:76958ms step_avg:51.68ms
step:1490/2000 train_time:77047ms step_avg:51.71ms
step:1491/2000 train_time:77135ms step_avg:51.73ms
step:1492/2000 train_time:77223ms step_avg:51.76ms
step:1493/2000 train_time:77311ms step_avg:51.78ms
step:1494/2000 train_time:77399ms step_avg:51.81ms
step:1495/2000 train_time:77488ms step_avg:51.83ms
step:1496/2000 train_time:77575ms step_avg:51.85ms
step:1497/2000 train_time:77663ms step_avg:51.88ms
step:1498/2000 train_time:77751ms step_avg:51.90ms
step:1499/2000 train_time:77840ms step_avg:51.93ms
step:1500/2000 train_time:77929ms step_avg:51.95ms
step:1500/2000 val_loss:3.4446 train_time:78019ms step_avg:52.01ms
step:1501/2000 train_time:78039ms step_avg:51.99ms
step:1502/2000 train_time:78108ms step_avg:52.00ms
step:1503/2000 train_time:78203ms step_avg:52.03ms
step:1504/2000 train_time:78290ms step_avg:52.05ms
step:1505/2000 train_time:78377ms step_avg:52.08ms
step:1506/2000 train_time:78464ms step_avg:52.10ms
step:1507/2000 train_time:78551ms step_avg:52.12ms
step:1508/2000 train_time:78638ms step_avg:52.15ms
step:1509/2000 train_time:78725ms step_avg:52.17ms
step:1510/2000 train_time:78812ms step_avg:52.19ms
step:1511/2000 train_time:78899ms step_avg:52.22ms
step:1512/2000 train_time:78988ms step_avg:52.24ms
step:1513/2000 train_time:79079ms step_avg:52.27ms
step:1514/2000 train_time:79171ms step_avg:52.29ms
step:1515/2000 train_time:79261ms step_avg:52.32ms
step:1516/2000 train_time:79348ms step_avg:52.34ms
step:1517/2000 train_time:79435ms step_avg:52.36ms
step:1518/2000 train_time:79522ms step_avg:52.39ms
step:1519/2000 train_time:79609ms step_avg:52.41ms
step:1520/2000 train_time:79696ms step_avg:52.43ms
step:1521/2000 train_time:79784ms step_avg:52.45ms
step:1522/2000 train_time:79871ms step_avg:52.48ms
step:1523/2000 train_time:79959ms step_avg:52.50ms
step:1524/2000 train_time:80049ms step_avg:52.53ms
step:1525/2000 train_time:80140ms step_avg:52.55ms
step:1526/2000 train_time:80229ms step_avg:52.57ms
step:1527/2000 train_time:80317ms step_avg:52.60ms
step:1528/2000 train_time:80406ms step_avg:52.62ms
step:1529/2000 train_time:80493ms step_avg:52.64ms
step:1530/2000 train_time:80581ms step_avg:52.67ms
step:1531/2000 train_time:80669ms step_avg:52.69ms
step:1532/2000 train_time:80757ms step_avg:52.71ms
step:1533/2000 train_time:80844ms step_avg:52.74ms
step:1534/2000 train_time:80931ms step_avg:52.76ms
step:1535/2000 train_time:81020ms step_avg:52.78ms
step:1536/2000 train_time:81110ms step_avg:52.81ms
step:1537/2000 train_time:81199ms step_avg:52.83ms
step:1538/2000 train_time:81288ms step_avg:52.85ms
step:1539/2000 train_time:81377ms step_avg:52.88ms
step:1540/2000 train_time:81465ms step_avg:52.90ms
step:1541/2000 train_time:81552ms step_avg:52.92ms
step:1542/2000 train_time:81639ms step_avg:52.94ms
step:1543/2000 train_time:81727ms step_avg:52.97ms
step:1544/2000 train_time:81814ms step_avg:52.99ms
step:1545/2000 train_time:81901ms step_avg:53.01ms
step:1546/2000 train_time:81990ms step_avg:53.03ms
step:1547/2000 train_time:82079ms step_avg:53.06ms
step:1548/2000 train_time:82168ms step_avg:53.08ms
step:1549/2000 train_time:82256ms step_avg:53.10ms
step:1550/2000 train_time:82345ms step_avg:53.13ms
step:1551/2000 train_time:82433ms step_avg:53.15ms
step:1552/2000 train_time:82520ms step_avg:53.17ms
step:1553/2000 train_time:82608ms step_avg:53.19ms
step:1554/2000 train_time:82695ms step_avg:53.21ms
step:1555/2000 train_time:82782ms step_avg:53.24ms
step:1556/2000 train_time:82870ms step_avg:53.26ms
step:1557/2000 train_time:82958ms step_avg:53.28ms
step:1558/2000 train_time:83046ms step_avg:53.30ms
step:1559/2000 train_time:83135ms step_avg:53.33ms
step:1560/2000 train_time:83224ms step_avg:53.35ms
step:1561/2000 train_time:83312ms step_avg:53.37ms
step:1562/2000 train_time:83399ms step_avg:53.39ms
step:1563/2000 train_time:83489ms step_avg:53.42ms
step:1564/2000 train_time:83577ms step_avg:53.44ms
step:1565/2000 train_time:83665ms step_avg:53.46ms
step:1566/2000 train_time:83753ms step_avg:53.48ms
step:1567/2000 train_time:83840ms step_avg:53.50ms
step:1568/2000 train_time:83928ms step_avg:53.53ms
step:1569/2000 train_time:84015ms step_avg:53.55ms
step:1570/2000 train_time:84104ms step_avg:53.57ms
step:1571/2000 train_time:84192ms step_avg:53.59ms
step:1572/2000 train_time:84280ms step_avg:53.61ms
step:1573/2000 train_time:84368ms step_avg:53.64ms
step:1574/2000 train_time:84457ms step_avg:53.66ms
step:1575/2000 train_time:84546ms step_avg:53.68ms
step:1576/2000 train_time:84634ms step_avg:53.70ms
step:1577/2000 train_time:84721ms step_avg:53.72ms
step:1578/2000 train_time:84809ms step_avg:53.74ms
step:1579/2000 train_time:84897ms step_avg:53.77ms
step:1580/2000 train_time:84985ms step_avg:53.79ms
step:1581/2000 train_time:85074ms step_avg:53.81ms
step:1582/2000 train_time:85161ms step_avg:53.83ms
step:1583/2000 train_time:85250ms step_avg:53.85ms
step:1584/2000 train_time:85338ms step_avg:53.88ms
step:1585/2000 train_time:85427ms step_avg:53.90ms
step:1586/2000 train_time:85514ms step_avg:53.92ms
step:1587/2000 train_time:85602ms step_avg:53.94ms
step:1588/2000 train_time:85688ms step_avg:53.96ms
step:1589/2000 train_time:85776ms step_avg:53.98ms
step:1590/2000 train_time:85864ms step_avg:54.00ms
step:1591/2000 train_time:85953ms step_avg:54.02ms
step:1592/2000 train_time:86040ms step_avg:54.05ms
step:1593/2000 train_time:86130ms step_avg:54.07ms
step:1594/2000 train_time:86219ms step_avg:54.09ms
step:1595/2000 train_time:86308ms step_avg:54.11ms
step:1596/2000 train_time:86395ms step_avg:54.13ms
step:1597/2000 train_time:86484ms step_avg:54.15ms
step:1598/2000 train_time:86572ms step_avg:54.18ms
step:1599/2000 train_time:86661ms step_avg:54.20ms
step:1600/2000 train_time:86748ms step_avg:54.22ms
step:1601/2000 train_time:86836ms step_avg:54.24ms
step:1602/2000 train_time:86924ms step_avg:54.26ms
step:1603/2000 train_time:87012ms step_avg:54.28ms
step:1604/2000 train_time:87101ms step_avg:54.30ms
step:1605/2000 train_time:87189ms step_avg:54.32ms
step:1606/2000 train_time:87277ms step_avg:54.34ms
step:1607/2000 train_time:87367ms step_avg:54.37ms
step:1608/2000 train_time:87454ms step_avg:54.39ms
step:1609/2000 train_time:87542ms step_avg:54.41ms
step:1610/2000 train_time:87629ms step_avg:54.43ms
step:1611/2000 train_time:87717ms step_avg:54.45ms
step:1612/2000 train_time:87806ms step_avg:54.47ms
step:1613/2000 train_time:87893ms step_avg:54.49ms
step:1614/2000 train_time:87981ms step_avg:54.51ms
step:1615/2000 train_time:88071ms step_avg:54.53ms
step:1616/2000 train_time:88158ms step_avg:54.55ms
step:1617/2000 train_time:88247ms step_avg:54.57ms
step:1618/2000 train_time:88334ms step_avg:54.59ms
step:1619/2000 train_time:88422ms step_avg:54.62ms
step:1620/2000 train_time:88511ms step_avg:54.64ms
step:1621/2000 train_time:88600ms step_avg:54.66ms
step:1622/2000 train_time:88688ms step_avg:54.68ms
step:1623/2000 train_time:88775ms step_avg:54.70ms
step:1624/2000 train_time:88863ms step_avg:54.72ms
step:1625/2000 train_time:88950ms step_avg:54.74ms
step:1626/2000 train_time:89038ms step_avg:54.76ms
step:1627/2000 train_time:89126ms step_avg:54.78ms
step:1628/2000 train_time:89213ms step_avg:54.80ms
step:1629/2000 train_time:89302ms step_avg:54.82ms
step:1630/2000 train_time:89389ms step_avg:54.84ms
step:1631/2000 train_time:89477ms step_avg:54.86ms
step:1632/2000 train_time:89565ms step_avg:54.88ms
step:1633/2000 train_time:89653ms step_avg:54.90ms
step:1634/2000 train_time:89741ms step_avg:54.92ms
step:1635/2000 train_time:89830ms step_avg:54.94ms
step:1636/2000 train_time:89918ms step_avg:54.96ms
step:1637/2000 train_time:90006ms step_avg:54.98ms
step:1638/2000 train_time:90093ms step_avg:55.00ms
step:1639/2000 train_time:90182ms step_avg:55.02ms
step:1640/2000 train_time:90270ms step_avg:55.04ms
step:1641/2000 train_time:90358ms step_avg:55.06ms
step:1642/2000 train_time:90446ms step_avg:55.08ms
step:1643/2000 train_time:90534ms step_avg:55.10ms
step:1644/2000 train_time:90622ms step_avg:55.12ms
step:1645/2000 train_time:90711ms step_avg:55.14ms
step:1646/2000 train_time:90798ms step_avg:55.16ms
step:1647/2000 train_time:90887ms step_avg:55.18ms
step:1648/2000 train_time:90974ms step_avg:55.20ms
step:1649/2000 train_time:91062ms step_avg:55.22ms
step:1650/2000 train_time:91150ms step_avg:55.24ms
step:1651/2000 train_time:91238ms step_avg:55.26ms
step:1652/2000 train_time:91326ms step_avg:55.28ms
step:1653/2000 train_time:91415ms step_avg:55.30ms
step:1654/2000 train_time:91502ms step_avg:55.32ms
step:1655/2000 train_time:91590ms step_avg:55.34ms
step:1656/2000 train_time:91679ms step_avg:55.36ms
step:1657/2000 train_time:91767ms step_avg:55.38ms
step:1658/2000 train_time:91855ms step_avg:55.40ms
step:1659/2000 train_time:91943ms step_avg:55.42ms
step:1660/2000 train_time:92031ms step_avg:55.44ms
step:1661/2000 train_time:92119ms step_avg:55.46ms
step:1662/2000 train_time:92208ms step_avg:55.48ms
step:1663/2000 train_time:92296ms step_avg:55.50ms
step:1664/2000 train_time:92384ms step_avg:55.52ms
step:1665/2000 train_time:92472ms step_avg:55.54ms
step:1666/2000 train_time:92560ms step_avg:55.56ms
step:1667/2000 train_time:92649ms step_avg:55.58ms
step:1668/2000 train_time:92737ms step_avg:55.60ms
step:1669/2000 train_time:92825ms step_avg:55.62ms
step:1670/2000 train_time:92913ms step_avg:55.64ms
step:1671/2000 train_time:93001ms step_avg:55.66ms
step:1672/2000 train_time:93089ms step_avg:55.68ms
step:1673/2000 train_time:93178ms step_avg:55.70ms
step:1674/2000 train_time:93267ms step_avg:55.71ms
step:1675/2000 train_time:93354ms step_avg:55.73ms
step:1676/2000 train_time:93442ms step_avg:55.75ms
step:1677/2000 train_time:93530ms step_avg:55.77ms
step:1678/2000 train_time:93618ms step_avg:55.79ms
step:1679/2000 train_time:93707ms step_avg:55.81ms
step:1680/2000 train_time:93794ms step_avg:55.83ms
step:1681/2000 train_time:93882ms step_avg:55.85ms
step:1682/2000 train_time:93970ms step_avg:55.87ms
step:1683/2000 train_time:94057ms step_avg:55.89ms
step:1684/2000 train_time:94145ms step_avg:55.91ms
step:1685/2000 train_time:94234ms step_avg:55.93ms
step:1686/2000 train_time:94322ms step_avg:55.94ms
step:1687/2000 train_time:94410ms step_avg:55.96ms
step:1688/2000 train_time:94498ms step_avg:55.98ms
step:1689/2000 train_time:94586ms step_avg:56.00ms
step:1690/2000 train_time:94675ms step_avg:56.02ms
step:1691/2000 train_time:94763ms step_avg:56.04ms
step:1692/2000 train_time:94850ms step_avg:56.06ms
step:1693/2000 train_time:94938ms step_avg:56.08ms
step:1694/2000 train_time:95026ms step_avg:56.10ms
step:1695/2000 train_time:95114ms step_avg:56.11ms
step:1696/2000 train_time:95202ms step_avg:56.13ms
step:1697/2000 train_time:95290ms step_avg:56.15ms
step:1698/2000 train_time:95377ms step_avg:56.17ms
step:1699/2000 train_time:95465ms step_avg:56.19ms
step:1700/2000 train_time:95552ms step_avg:56.21ms
step:1701/2000 train_time:95640ms step_avg:56.23ms
step:1702/2000 train_time:95728ms step_avg:56.24ms
step:1703/2000 train_time:95816ms step_avg:56.26ms
step:1704/2000 train_time:95905ms step_avg:56.28ms
step:1705/2000 train_time:95992ms step_avg:56.30ms
step:1706/2000 train_time:96080ms step_avg:56.32ms
step:1707/2000 train_time:96170ms step_avg:56.34ms
step:1708/2000 train_time:96258ms step_avg:56.36ms
step:1709/2000 train_time:96346ms step_avg:56.38ms
step:1710/2000 train_time:96433ms step_avg:56.39ms
step:1711/2000 train_time:96521ms step_avg:56.41ms
step:1712/2000 train_time:96609ms step_avg:56.43ms
step:1713/2000 train_time:96697ms step_avg:56.45ms
step:1714/2000 train_time:96785ms step_avg:56.47ms
step:1715/2000 train_time:96873ms step_avg:56.49ms
step:1716/2000 train_time:96961ms step_avg:56.50ms
step:1717/2000 train_time:97049ms step_avg:56.52ms
step:1718/2000 train_time:97136ms step_avg:56.54ms
step:1719/2000 train_time:97225ms step_avg:56.56ms
step:1720/2000 train_time:97312ms step_avg:56.58ms
step:1721/2000 train_time:97401ms step_avg:56.60ms
step:1722/2000 train_time:97489ms step_avg:56.61ms
step:1723/2000 train_time:97577ms step_avg:56.63ms
step:1724/2000 train_time:97666ms step_avg:56.65ms
step:1725/2000 train_time:97754ms step_avg:56.67ms
step:1726/2000 train_time:97841ms step_avg:56.69ms
step:1727/2000 train_time:97930ms step_avg:56.71ms
step:1728/2000 train_time:98018ms step_avg:56.72ms
step:1729/2000 train_time:98107ms step_avg:56.74ms
step:1730/2000 train_time:98194ms step_avg:56.76ms
step:1731/2000 train_time:98282ms step_avg:56.78ms
step:1732/2000 train_time:98370ms step_avg:56.80ms
step:1733/2000 train_time:98458ms step_avg:56.81ms
step:1734/2000 train_time:98546ms step_avg:56.83ms
step:1735/2000 train_time:98634ms step_avg:56.85ms
step:1736/2000 train_time:98721ms step_avg:56.87ms
step:1737/2000 train_time:98810ms step_avg:56.89ms
step:1738/2000 train_time:98897ms step_avg:56.90ms
step:1739/2000 train_time:98985ms step_avg:56.92ms
step:1740/2000 train_time:99072ms step_avg:56.94ms
step:1741/2000 train_time:99160ms step_avg:56.96ms
step:1742/2000 train_time:99248ms step_avg:56.97ms
step:1743/2000 train_time:99336ms step_avg:56.99ms
step:1744/2000 train_time:99423ms step_avg:57.01ms
step:1745/2000 train_time:99511ms step_avg:57.03ms
step:1746/2000 train_time:99599ms step_avg:57.04ms
step:1747/2000 train_time:99687ms step_avg:57.06ms
step:1748/2000 train_time:99775ms step_avg:57.08ms
step:1749/2000 train_time:99863ms step_avg:57.10ms
step:1750/2000 train_time:99950ms step_avg:57.11ms
step:1750/2000 val_loss:3.3485 train_time:100041ms step_avg:57.17ms
step:1751/2000 train_time:100059ms step_avg:57.14ms
step:1752/2000 train_time:100130ms step_avg:57.15ms
step:1753/2000 train_time:100221ms step_avg:57.17ms
step:1754/2000 train_time:100309ms step_avg:57.19ms
step:1755/2000 train_time:100397ms step_avg:57.21ms
step:1756/2000 train_time:100483ms step_avg:57.22ms
step:1757/2000 train_time:100570ms step_avg:57.24ms
step:1758/2000 train_time:100657ms step_avg:57.26ms
step:1759/2000 train_time:100744ms step_avg:57.27ms
step:1760/2000 train_time:100831ms step_avg:57.29ms
step:1761/2000 train_time:100918ms step_avg:57.31ms
step:1762/2000 train_time:101007ms step_avg:57.33ms
step:1763/2000 train_time:101097ms step_avg:57.34ms
step:1764/2000 train_time:101186ms step_avg:57.36ms
step:1765/2000 train_time:101275ms step_avg:57.38ms
step:1766/2000 train_time:101362ms step_avg:57.40ms
step:1767/2000 train_time:101451ms step_avg:57.41ms
step:1768/2000 train_time:101538ms step_avg:57.43ms
step:1769/2000 train_time:101625ms step_avg:57.45ms
step:1770/2000 train_time:101711ms step_avg:57.46ms
step:1771/2000 train_time:101799ms step_avg:57.48ms
step:1772/2000 train_time:101886ms step_avg:57.50ms
step:1773/2000 train_time:101974ms step_avg:57.51ms
step:1774/2000 train_time:102063ms step_avg:57.53ms
step:1775/2000 train_time:102153ms step_avg:57.55ms
step:1776/2000 train_time:102242ms step_avg:57.57ms
step:1777/2000 train_time:102331ms step_avg:57.59ms
step:1778/2000 train_time:102419ms step_avg:57.60ms
step:1779/2000 train_time:102507ms step_avg:57.62ms
step:1780/2000 train_time:102594ms step_avg:57.64ms
step:1781/2000 train_time:102682ms step_avg:57.65ms
step:1782/2000 train_time:102769ms step_avg:57.67ms
step:1783/2000 train_time:102856ms step_avg:57.69ms
step:1784/2000 train_time:102944ms step_avg:57.70ms
step:1785/2000 train_time:103032ms step_avg:57.72ms
step:1786/2000 train_time:103121ms step_avg:57.74ms
step:1787/2000 train_time:103210ms step_avg:57.76ms
step:1788/2000 train_time:103298ms step_avg:57.77ms
step:1789/2000 train_time:103387ms step_avg:57.79ms
step:1790/2000 train_time:103475ms step_avg:57.81ms
step:1791/2000 train_time:103563ms step_avg:57.82ms
step:1792/2000 train_time:103650ms step_avg:57.84ms
step:1793/2000 train_time:103738ms step_avg:57.86ms
step:1794/2000 train_time:103825ms step_avg:57.87ms
step:1795/2000 train_time:103913ms step_avg:57.89ms
step:1796/2000 train_time:104002ms step_avg:57.91ms
step:1797/2000 train_time:104089ms step_avg:57.92ms
step:1798/2000 train_time:104178ms step_avg:57.94ms
step:1799/2000 train_time:104267ms step_avg:57.96ms
step:1800/2000 train_time:104355ms step_avg:57.98ms
step:1801/2000 train_time:104444ms step_avg:57.99ms
step:1802/2000 train_time:104530ms step_avg:58.01ms
step:1803/2000 train_time:104618ms step_avg:58.02ms
step:1804/2000 train_time:104706ms step_avg:58.04ms
step:1805/2000 train_time:104793ms step_avg:58.06ms
step:1806/2000 train_time:104881ms step_avg:58.07ms
step:1807/2000 train_time:104969ms step_avg:58.09ms
step:1808/2000 train_time:105057ms step_avg:58.11ms
step:1809/2000 train_time:105146ms step_avg:58.12ms
step:1810/2000 train_time:105234ms step_avg:58.14ms
step:1811/2000 train_time:105323ms step_avg:58.16ms
step:1812/2000 train_time:105411ms step_avg:58.17ms
step:1813/2000 train_time:105499ms step_avg:58.19ms
step:1814/2000 train_time:105586ms step_avg:58.21ms
step:1815/2000 train_time:105674ms step_avg:58.22ms
step:1816/2000 train_time:105761ms step_avg:58.24ms
step:1817/2000 train_time:105849ms step_avg:58.25ms
step:1818/2000 train_time:105936ms step_avg:58.27ms
step:1819/2000 train_time:106024ms step_avg:58.29ms
step:1820/2000 train_time:106112ms step_avg:58.30ms
step:1821/2000 train_time:106200ms step_avg:58.32ms
step:1822/2000 train_time:106288ms step_avg:58.34ms
step:1823/2000 train_time:106377ms step_avg:58.35ms
step:1824/2000 train_time:106465ms step_avg:58.37ms
step:1825/2000 train_time:106552ms step_avg:58.38ms
step:1826/2000 train_time:106640ms step_avg:58.40ms
step:1827/2000 train_time:106728ms step_avg:58.42ms
step:1828/2000 train_time:106816ms step_avg:58.43ms
step:1829/2000 train_time:106904ms step_avg:58.45ms
step:1830/2000 train_time:106991ms step_avg:58.47ms
step:1831/2000 train_time:107079ms step_avg:58.48ms
step:1832/2000 train_time:107166ms step_avg:58.50ms
step:1833/2000 train_time:107255ms step_avg:58.51ms
step:1834/2000 train_time:107344ms step_avg:58.53ms
step:1835/2000 train_time:107432ms step_avg:58.55ms
step:1836/2000 train_time:107520ms step_avg:58.56ms
step:1837/2000 train_time:107609ms step_avg:58.58ms
step:1838/2000 train_time:107696ms step_avg:58.59ms
step:1839/2000 train_time:107785ms step_avg:58.61ms
step:1840/2000 train_time:107873ms step_avg:58.63ms
step:1841/2000 train_time:107961ms step_avg:58.64ms
step:1842/2000 train_time:108048ms step_avg:58.66ms
step:1843/2000 train_time:108135ms step_avg:58.67ms
step:1844/2000 train_time:108222ms step_avg:58.69ms
step:1845/2000 train_time:108312ms step_avg:58.71ms
step:1846/2000 train_time:108400ms step_avg:58.72ms
step:1847/2000 train_time:108489ms step_avg:58.74ms
step:1848/2000 train_time:108577ms step_avg:58.75ms
step:1849/2000 train_time:108665ms step_avg:58.77ms
step:1850/2000 train_time:108752ms step_avg:58.78ms
step:1851/2000 train_time:108840ms step_avg:58.80ms
step:1852/2000 train_time:108928ms step_avg:58.82ms
step:1853/2000 train_time:109016ms step_avg:58.83ms
step:1854/2000 train_time:109104ms step_avg:58.85ms
step:1855/2000 train_time:109192ms step_avg:58.86ms
step:1856/2000 train_time:109281ms step_avg:58.88ms
step:1857/2000 train_time:109369ms step_avg:58.90ms
step:1858/2000 train_time:109457ms step_avg:58.91ms
step:1859/2000 train_time:109547ms step_avg:58.93ms
step:1860/2000 train_time:109636ms step_avg:58.94ms
step:1861/2000 train_time:109724ms step_avg:58.96ms
step:1862/2000 train_time:109812ms step_avg:58.98ms
step:1863/2000 train_time:109899ms step_avg:58.99ms
step:1864/2000 train_time:109987ms step_avg:59.01ms
step:1865/2000 train_time:110074ms step_avg:59.02ms
step:1866/2000 train_time:110162ms step_avg:59.04ms
step:1867/2000 train_time:110250ms step_avg:59.05ms
step:1868/2000 train_time:110338ms step_avg:59.07ms
step:1869/2000 train_time:110426ms step_avg:59.08ms
step:1870/2000 train_time:110514ms step_avg:59.10ms
step:1871/2000 train_time:110604ms step_avg:59.11ms
step:1872/2000 train_time:110691ms step_avg:59.13ms
step:1873/2000 train_time:110778ms step_avg:59.14ms
step:1874/2000 train_time:110866ms step_avg:59.16ms
step:1875/2000 train_time:110953ms step_avg:59.18ms
step:1876/2000 train_time:111041ms step_avg:59.19ms
step:1877/2000 train_time:111129ms step_avg:59.21ms
step:1878/2000 train_time:111217ms step_avg:59.22ms
step:1879/2000 train_time:111307ms step_avg:59.24ms
step:1880/2000 train_time:111395ms step_avg:59.25ms
step:1881/2000 train_time:111484ms step_avg:59.27ms
step:1882/2000 train_time:111571ms step_avg:59.28ms
step:1883/2000 train_time:111660ms step_avg:59.30ms
step:1884/2000 train_time:111747ms step_avg:59.31ms
step:1885/2000 train_time:111835ms step_avg:59.33ms
step:1886/2000 train_time:111922ms step_avg:59.34ms
step:1887/2000 train_time:112012ms step_avg:59.36ms
step:1888/2000 train_time:112101ms step_avg:59.38ms
step:1889/2000 train_time:112188ms step_avg:59.39ms
step:1890/2000 train_time:112276ms step_avg:59.41ms
step:1891/2000 train_time:112364ms step_avg:59.42ms
step:1892/2000 train_time:112452ms step_avg:59.44ms
step:1893/2000 train_time:112540ms step_avg:59.45ms
step:1894/2000 train_time:112628ms step_avg:59.47ms
step:1895/2000 train_time:112715ms step_avg:59.48ms
step:1896/2000 train_time:112804ms step_avg:59.50ms
step:1897/2000 train_time:112892ms step_avg:59.51ms
step:1898/2000 train_time:112979ms step_avg:59.53ms
step:1899/2000 train_time:113067ms step_avg:59.54ms
step:1900/2000 train_time:113155ms step_avg:59.56ms
step:1901/2000 train_time:113244ms step_avg:59.57ms
step:1902/2000 train_time:113331ms step_avg:59.59ms
step:1903/2000 train_time:113419ms step_avg:59.60ms
step:1904/2000 train_time:113507ms step_avg:59.62ms
step:1905/2000 train_time:113595ms step_avg:59.63ms
step:1906/2000 train_time:113684ms step_avg:59.65ms
step:1907/2000 train_time:113773ms step_avg:59.66ms
step:1908/2000 train_time:113861ms step_avg:59.68ms
step:1909/2000 train_time:113948ms step_avg:59.69ms
step:1910/2000 train_time:114036ms step_avg:59.70ms
step:1911/2000 train_time:114125ms step_avg:59.72ms
step:1912/2000 train_time:114212ms step_avg:59.73ms
step:1913/2000 train_time:114300ms step_avg:59.75ms
step:1914/2000 train_time:114388ms step_avg:59.76ms
step:1915/2000 train_time:114476ms step_avg:59.78ms
step:1916/2000 train_time:114564ms step_avg:59.79ms
step:1917/2000 train_time:114652ms step_avg:59.81ms
step:1918/2000 train_time:114741ms step_avg:59.82ms
step:1919/2000 train_time:114829ms step_avg:59.84ms
step:1920/2000 train_time:114917ms step_avg:59.85ms
step:1921/2000 train_time:115005ms step_avg:59.87ms
step:1922/2000 train_time:115092ms step_avg:59.88ms
step:1923/2000 train_time:115181ms step_avg:59.90ms
step:1924/2000 train_time:115267ms step_avg:59.91ms
step:1925/2000 train_time:115355ms step_avg:59.92ms
step:1926/2000 train_time:115444ms step_avg:59.94ms
step:1927/2000 train_time:115532ms step_avg:59.95ms
step:1928/2000 train_time:115620ms step_avg:59.97ms
step:1929/2000 train_time:115708ms step_avg:59.98ms
step:1930/2000 train_time:115797ms step_avg:60.00ms
step:1931/2000 train_time:115885ms step_avg:60.01ms
step:1932/2000 train_time:115973ms step_avg:60.03ms
step:1933/2000 train_time:116061ms step_avg:60.04ms
step:1934/2000 train_time:116149ms step_avg:60.06ms
step:1935/2000 train_time:116237ms step_avg:60.07ms
step:1936/2000 train_time:116325ms step_avg:60.09ms
step:1937/2000 train_time:116413ms step_avg:60.10ms
step:1938/2000 train_time:116501ms step_avg:60.11ms
step:1939/2000 train_time:116589ms step_avg:60.13ms
step:1940/2000 train_time:116677ms step_avg:60.14ms
step:1941/2000 train_time:116765ms step_avg:60.16ms
step:1942/2000 train_time:116853ms step_avg:60.17ms
step:1943/2000 train_time:116941ms step_avg:60.19ms
step:1944/2000 train_time:117028ms step_avg:60.20ms
step:1945/2000 train_time:117116ms step_avg:60.21ms
step:1946/2000 train_time:117205ms step_avg:60.23ms
step:1947/2000 train_time:117292ms step_avg:60.24ms
step:1948/2000 train_time:117381ms step_avg:60.26ms
step:1949/2000 train_time:117469ms step_avg:60.27ms
step:1950/2000 train_time:117557ms step_avg:60.29ms
step:1951/2000 train_time:117646ms step_avg:60.30ms
step:1952/2000 train_time:117734ms step_avg:60.31ms
step:1953/2000 train_time:117822ms step_avg:60.33ms
step:1954/2000 train_time:117909ms step_avg:60.34ms
step:1955/2000 train_time:117998ms step_avg:60.36ms
step:1956/2000 train_time:118086ms step_avg:60.37ms
step:1957/2000 train_time:118174ms step_avg:60.39ms
step:1958/2000 train_time:118262ms step_avg:60.40ms
step:1959/2000 train_time:118350ms step_avg:60.41ms
step:1960/2000 train_time:118439ms step_avg:60.43ms
step:1961/2000 train_time:118527ms step_avg:60.44ms
step:1962/2000 train_time:118615ms step_avg:60.46ms
step:1963/2000 train_time:118704ms step_avg:60.47ms
step:1964/2000 train_time:118792ms step_avg:60.48ms
step:1965/2000 train_time:118880ms step_avg:60.50ms
step:1966/2000 train_time:118967ms step_avg:60.51ms
step:1967/2000 train_time:119055ms step_avg:60.53ms
step:1968/2000 train_time:119144ms step_avg:60.54ms
step:1969/2000 train_time:119232ms step_avg:60.55ms
step:1970/2000 train_time:119320ms step_avg:60.57ms
step:1971/2000 train_time:119408ms step_avg:60.58ms
step:1972/2000 train_time:119497ms step_avg:60.60ms
step:1973/2000 train_time:119587ms step_avg:60.61ms
step:1974/2000 train_time:119675ms step_avg:60.63ms
step:1975/2000 train_time:119764ms step_avg:60.64ms
step:1976/2000 train_time:119852ms step_avg:60.65ms
step:1977/2000 train_time:119940ms step_avg:60.67ms
step:1978/2000 train_time:120027ms step_avg:60.68ms
step:1979/2000 train_time:120115ms step_avg:60.69ms
step:1980/2000 train_time:120203ms step_avg:60.71ms
step:1981/2000 train_time:120292ms step_avg:60.72ms
step:1982/2000 train_time:120380ms step_avg:60.74ms
step:1983/2000 train_time:120468ms step_avg:60.75ms
step:1984/2000 train_time:120556ms step_avg:60.76ms
step:1985/2000 train_time:120647ms step_avg:60.78ms
step:1986/2000 train_time:120736ms step_avg:60.79ms
step:1987/2000 train_time:120825ms step_avg:60.81ms
step:1988/2000 train_time:120914ms step_avg:60.82ms
step:1989/2000 train_time:121003ms step_avg:60.84ms
step:1990/2000 train_time:121091ms step_avg:60.85ms
step:1991/2000 train_time:121179ms step_avg:60.86ms
step:1992/2000 train_time:121266ms step_avg:60.88ms
step:1993/2000 train_time:121354ms step_avg:60.89ms
step:1994/2000 train_time:121443ms step_avg:60.90ms
step:1995/2000 train_time:121532ms step_avg:60.92ms
step:1996/2000 train_time:121621ms step_avg:60.93ms
step:1997/2000 train_time:121710ms step_avg:60.95ms
step:1998/2000 train_time:121798ms step_avg:60.96ms
step:1999/2000 train_time:121886ms step_avg:60.97ms
step:2000/2000 train_time:121975ms step_avg:60.99ms
step:2000/2000 val_loss:3.2793 train_time:122066ms step_avg:61.03ms
peak memory allocated: 29634 MiB reserved: 35856 MiB

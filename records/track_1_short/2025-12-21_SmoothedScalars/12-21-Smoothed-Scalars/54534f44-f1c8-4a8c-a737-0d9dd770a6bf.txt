import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            grad_slice = torch.empty_like(grad[:rank_size])
            self._reduce_scatter_futures[param] = (
                dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad_slice
            )



    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

    @torch.no_grad()
    def reset_momentum(self, params=None):
        """Reset momentum buffers for specified parameters (or all if 'None')"""
        if params is None:
            # Reset all parameters
            params_to_reset = [p for group in self.param_groups for p in group['params']]
        else:
            params_to_reset = list(params)
        
        for param in params_to_reset:
            if param in self.state:
                state = self.state[param]
                if 'exp_avg' in state:
                    state['exp_avg'].zero_()
                if 'exp_avg_sq' in state:
                    state['exp_avg_sq'].zero_()
                state['step'] = 0

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0
        
        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas  
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    # evaluation and logging
    logs_dir: str = f"logs/12-21-Smooth-Scalars-stps.1960.40"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes,lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    
    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps
    
    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0
    
    if in_transition:
        adam_optimizers[1].transition_steps -= 1
            
    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) 
        is_transition = True
            
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 18:03:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   26C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    740743      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    740744      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    740745      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    740746      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    740747      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    740748      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    740749      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    740750      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    740744      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    740745      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    740746      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    740747      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    740748      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    740749      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    740750      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2000 val_loss:10.8360 train_time:0ms step_avg:0.03ms
step:1/2000 train_time:76ms step_avg:75.98ms
step:2/2000 train_time:100ms step_avg:49.89ms
step:3/2000 train_time:121ms step_avg:40.27ms
step:4/2000 train_time:154ms step_avg:38.41ms
step:5/2000 train_time:186ms step_avg:37.21ms
step:6/2000 train_time:260ms step_avg:43.39ms
step:7/2000 train_time:287ms step_avg:40.99ms
step:8/2000 train_time:320ms step_avg:40.00ms
step:9/2000 train_time:353ms step_avg:39.19ms
step:10/2000 train_time:387ms step_avg:38.66ms
step:11/2000 train_time:419ms step_avg:38.13ms
step:12/2000 train_time:453ms step_avg:37.73ms
step:13/2000 train_time:486ms step_avg:37.36ms
step:14/2000 train_time:519ms step_avg:37.08ms
step:15/2000 train_time:552ms step_avg:36.81ms
step:16/2000 train_time:586ms step_avg:36.62ms
step:17/2000 train_time:619ms step_avg:36.43ms
step:18/2000 train_time:653ms step_avg:36.26ms
step:19/2000 train_time:686ms step_avg:36.09ms
step:20/2000 train_time:719ms step_avg:35.96ms
step:21/2000 train_time:752ms step_avg:35.81ms
step:22/2000 train_time:786ms step_avg:35.71ms
step:23/2000 train_time:819ms step_avg:35.59ms
step:24/2000 train_time:852ms step_avg:35.50ms
step:25/2000 train_time:885ms step_avg:35.40ms
step:26/2000 train_time:918ms step_avg:35.32ms
step:27/2000 train_time:951ms step_avg:35.23ms
step:28/2000 train_time:985ms step_avg:35.16ms
step:29/2000 train_time:1017ms step_avg:35.08ms
step:30/2000 train_time:1051ms step_avg:35.03ms
step:31/2000 train_time:1084ms step_avg:34.97ms
step:32/2000 train_time:1118ms step_avg:34.93ms
step:33/2000 train_time:1151ms step_avg:34.88ms
step:34/2000 train_time:1186ms step_avg:34.89ms
step:35/2000 train_time:1221ms step_avg:34.87ms
step:36/2000 train_time:1254ms step_avg:34.85ms
step:37/2000 train_time:1288ms step_avg:34.81ms
step:38/2000 train_time:1322ms step_avg:34.78ms
step:39/2000 train_time:1355ms step_avg:34.75ms
step:40/2000 train_time:1389ms step_avg:34.73ms
step:41/2000 train_time:1422ms step_avg:34.69ms
step:42/2000 train_time:1456ms step_avg:34.66ms
step:43/2000 train_time:1489ms step_avg:34.62ms
step:44/2000 train_time:1522ms step_avg:34.60ms
step:45/2000 train_time:1555ms step_avg:34.56ms
step:46/2000 train_time:1589ms step_avg:34.54ms
step:47/2000 train_time:1622ms step_avg:34.51ms
step:48/2000 train_time:1655ms step_avg:34.48ms
step:49/2000 train_time:1688ms step_avg:34.44ms
step:50/2000 train_time:1721ms step_avg:34.42ms
step:51/2000 train_time:1754ms step_avg:34.39ms
step:52/2000 train_time:1787ms step_avg:34.37ms
step:53/2000 train_time:1820ms step_avg:34.35ms
step:54/2000 train_time:1854ms step_avg:34.33ms
step:55/2000 train_time:1887ms step_avg:34.30ms
step:56/2000 train_time:1920ms step_avg:34.29ms
step:57/2000 train_time:1953ms step_avg:34.26ms
step:58/2000 train_time:1987ms step_avg:34.25ms
step:59/2000 train_time:2020ms step_avg:34.23ms
step:60/2000 train_time:2053ms step_avg:34.21ms
step:61/2000 train_time:2086ms step_avg:34.20ms
step:62/2000 train_time:2120ms step_avg:34.19ms
step:63/2000 train_time:2153ms step_avg:34.17ms
step:64/2000 train_time:2186ms step_avg:34.16ms
step:65/2000 train_time:2219ms step_avg:34.14ms
step:66/2000 train_time:2253ms step_avg:34.14ms
step:67/2000 train_time:2286ms step_avg:34.12ms
step:68/2000 train_time:2320ms step_avg:34.12ms
step:69/2000 train_time:2353ms step_avg:34.10ms
step:70/2000 train_time:2387ms step_avg:34.09ms
step:71/2000 train_time:2420ms step_avg:34.08ms
step:72/2000 train_time:2453ms step_avg:34.07ms
step:73/2000 train_time:2487ms step_avg:34.06ms
step:74/2000 train_time:2521ms step_avg:34.06ms
step:75/2000 train_time:2553ms step_avg:34.05ms
step:76/2000 train_time:2587ms step_avg:34.04ms
step:77/2000 train_time:2620ms step_avg:34.03ms
step:78/2000 train_time:2653ms step_avg:34.02ms
step:79/2000 train_time:2686ms step_avg:34.00ms
step:80/2000 train_time:2720ms step_avg:33.99ms
step:81/2000 train_time:2753ms step_avg:33.98ms
step:82/2000 train_time:2786ms step_avg:33.98ms
step:83/2000 train_time:2819ms step_avg:33.97ms
step:84/2000 train_time:2853ms step_avg:33.96ms
step:85/2000 train_time:2886ms step_avg:33.95ms
step:86/2000 train_time:2919ms step_avg:33.94ms
step:87/2000 train_time:2952ms step_avg:33.93ms
step:88/2000 train_time:2985ms step_avg:33.92ms
step:89/2000 train_time:3018ms step_avg:33.91ms
step:90/2000 train_time:3052ms step_avg:33.91ms
step:91/2000 train_time:3085ms step_avg:33.90ms
step:92/2000 train_time:3118ms step_avg:33.89ms
step:93/2000 train_time:3151ms step_avg:33.88ms
step:94/2000 train_time:3185ms step_avg:33.88ms
step:95/2000 train_time:3218ms step_avg:33.87ms
step:96/2000 train_time:3251ms step_avg:33.87ms
step:97/2000 train_time:3284ms step_avg:33.85ms
step:98/2000 train_time:3317ms step_avg:33.85ms
step:99/2000 train_time:3350ms step_avg:33.84ms
step:100/2000 train_time:3384ms step_avg:33.84ms
step:101/2000 train_time:3417ms step_avg:33.83ms
step:102/2000 train_time:3450ms step_avg:33.83ms
step:103/2000 train_time:3483ms step_avg:33.82ms
step:104/2000 train_time:3517ms step_avg:33.81ms
step:105/2000 train_time:3550ms step_avg:33.81ms
step:106/2000 train_time:3584ms step_avg:33.81ms
step:107/2000 train_time:3617ms step_avg:33.80ms
step:108/2000 train_time:3650ms step_avg:33.79ms
step:109/2000 train_time:3683ms step_avg:33.79ms
step:110/2000 train_time:3716ms step_avg:33.78ms
step:111/2000 train_time:3749ms step_avg:33.77ms
step:112/2000 train_time:3782ms step_avg:33.77ms
step:113/2000 train_time:3815ms step_avg:33.76ms
step:114/2000 train_time:3849ms step_avg:33.76ms
step:115/2000 train_time:3882ms step_avg:33.76ms
step:116/2000 train_time:3915ms step_avg:33.75ms
step:117/2000 train_time:3948ms step_avg:33.75ms
step:118/2000 train_time:3982ms step_avg:33.74ms
step:119/2000 train_time:4015ms step_avg:33.74ms
step:120/2000 train_time:4048ms step_avg:33.73ms
step:121/2000 train_time:4081ms step_avg:33.73ms
step:122/2000 train_time:4114ms step_avg:33.72ms
step:123/2000 train_time:4147ms step_avg:33.72ms
step:124/2000 train_time:4181ms step_avg:33.72ms
step:125/2000 train_time:4214ms step_avg:33.71ms
step:126/2000 train_time:4247ms step_avg:33.71ms
step:127/2000 train_time:4280ms step_avg:33.70ms
step:128/2000 train_time:4313ms step_avg:33.70ms
step:129/2000 train_time:4346ms step_avg:33.69ms
step:130/2000 train_time:4380ms step_avg:33.69ms
step:131/2000 train_time:4413ms step_avg:33.69ms
step:132/2000 train_time:4446ms step_avg:33.69ms
step:133/2000 train_time:4480ms step_avg:33.68ms
step:134/2000 train_time:4513ms step_avg:33.68ms
step:135/2000 train_time:4546ms step_avg:33.67ms
step:136/2000 train_time:4579ms step_avg:33.67ms
step:137/2000 train_time:4612ms step_avg:33.67ms
step:138/2000 train_time:4646ms step_avg:33.66ms
step:139/2000 train_time:4679ms step_avg:33.66ms
step:140/2000 train_time:4712ms step_avg:33.66ms
step:141/2000 train_time:4745ms step_avg:33.65ms
step:142/2000 train_time:4779ms step_avg:33.65ms
step:143/2000 train_time:4812ms step_avg:33.65ms
step:144/2000 train_time:4845ms step_avg:33.65ms
step:145/2000 train_time:4878ms step_avg:33.64ms
step:146/2000 train_time:4912ms step_avg:33.64ms
step:147/2000 train_time:4945ms step_avg:33.64ms
step:148/2000 train_time:4978ms step_avg:33.63ms
step:149/2000 train_time:5011ms step_avg:33.63ms
step:150/2000 train_time:5044ms step_avg:33.63ms
step:151/2000 train_time:5077ms step_avg:33.62ms
step:152/2000 train_time:5110ms step_avg:33.62ms
step:153/2000 train_time:5144ms step_avg:33.62ms
step:154/2000 train_time:5177ms step_avg:33.62ms
step:155/2000 train_time:5210ms step_avg:33.61ms
step:156/2000 train_time:5243ms step_avg:33.61ms
step:157/2000 train_time:5276ms step_avg:33.60ms
step:158/2000 train_time:5309ms step_avg:33.60ms
step:159/2000 train_time:5343ms step_avg:33.60ms
step:160/2000 train_time:5377ms step_avg:33.60ms
step:161/2000 train_time:5409ms step_avg:33.60ms
step:162/2000 train_time:5443ms step_avg:33.60ms
step:163/2000 train_time:5476ms step_avg:33.60ms
step:164/2000 train_time:5510ms step_avg:33.60ms
step:165/2000 train_time:5543ms step_avg:33.59ms
step:166/2000 train_time:5576ms step_avg:33.59ms
step:167/2000 train_time:5609ms step_avg:33.59ms
step:168/2000 train_time:5642ms step_avg:33.58ms
step:169/2000 train_time:5675ms step_avg:33.58ms
step:170/2000 train_time:5709ms step_avg:33.58ms
step:171/2000 train_time:5742ms step_avg:33.58ms
step:172/2000 train_time:5775ms step_avg:33.58ms
step:173/2000 train_time:5808ms step_avg:33.57ms
step:174/2000 train_time:5841ms step_avg:33.57ms
step:175/2000 train_time:5874ms step_avg:33.57ms
step:176/2000 train_time:5907ms step_avg:33.57ms
step:177/2000 train_time:5941ms step_avg:33.56ms
step:178/2000 train_time:5974ms step_avg:33.56ms
step:179/2000 train_time:6007ms step_avg:33.56ms
step:180/2000 train_time:6041ms step_avg:33.56ms
step:181/2000 train_time:6073ms step_avg:33.56ms
step:182/2000 train_time:6107ms step_avg:33.55ms
step:183/2000 train_time:6140ms step_avg:33.55ms
step:184/2000 train_time:6173ms step_avg:33.55ms
step:185/2000 train_time:6206ms step_avg:33.55ms
step:186/2000 train_time:6239ms step_avg:33.55ms
step:187/2000 train_time:6272ms step_avg:33.54ms
step:188/2000 train_time:6306ms step_avg:33.54ms
step:189/2000 train_time:6339ms step_avg:33.54ms
step:190/2000 train_time:6372ms step_avg:33.54ms
step:191/2000 train_time:6405ms step_avg:33.54ms
step:192/2000 train_time:6439ms step_avg:33.54ms
step:193/2000 train_time:6472ms step_avg:33.53ms
step:194/2000 train_time:6505ms step_avg:33.53ms
step:195/2000 train_time:6538ms step_avg:33.53ms
step:196/2000 train_time:6571ms step_avg:33.53ms
step:197/2000 train_time:6604ms step_avg:33.53ms
step:198/2000 train_time:6638ms step_avg:33.52ms
step:199/2000 train_time:6670ms step_avg:33.52ms
step:200/2000 train_time:6704ms step_avg:33.52ms
step:201/2000 train_time:6737ms step_avg:33.52ms
step:202/2000 train_time:6771ms step_avg:33.52ms
step:203/2000 train_time:6804ms step_avg:33.51ms
step:204/2000 train_time:6837ms step_avg:33.51ms
step:205/2000 train_time:6869ms step_avg:33.51ms
step:206/2000 train_time:6903ms step_avg:33.51ms
step:207/2000 train_time:6936ms step_avg:33.51ms
step:208/2000 train_time:6969ms step_avg:33.50ms
step:209/2000 train_time:7002ms step_avg:33.50ms
step:210/2000 train_time:7035ms step_avg:33.50ms
step:211/2000 train_time:7067ms step_avg:33.49ms
step:212/2000 train_time:7101ms step_avg:33.49ms
step:213/2000 train_time:7134ms step_avg:33.49ms
step:214/2000 train_time:7167ms step_avg:33.49ms
step:215/2000 train_time:7200ms step_avg:33.49ms
step:216/2000 train_time:7233ms step_avg:33.49ms
step:217/2000 train_time:7266ms step_avg:33.48ms
step:218/2000 train_time:7299ms step_avg:33.48ms
step:219/2000 train_time:7332ms step_avg:33.48ms
step:220/2000 train_time:7366ms step_avg:33.48ms
step:221/2000 train_time:7398ms step_avg:33.48ms
step:222/2000 train_time:7432ms step_avg:33.48ms
step:223/2000 train_time:7464ms step_avg:33.47ms
step:224/2000 train_time:7498ms step_avg:33.47ms
step:225/2000 train_time:7530ms step_avg:33.47ms
step:226/2000 train_time:7564ms step_avg:33.47ms
step:227/2000 train_time:7597ms step_avg:33.47ms
step:228/2000 train_time:7630ms step_avg:33.47ms
step:229/2000 train_time:7663ms step_avg:33.46ms
step:230/2000 train_time:7697ms step_avg:33.46ms
step:231/2000 train_time:7729ms step_avg:33.46ms
step:232/2000 train_time:7763ms step_avg:33.46ms
step:233/2000 train_time:7795ms step_avg:33.46ms
step:234/2000 train_time:7829ms step_avg:33.46ms
step:235/2000 train_time:7862ms step_avg:33.45ms
step:236/2000 train_time:7895ms step_avg:33.45ms
step:237/2000 train_time:7928ms step_avg:33.45ms
step:238/2000 train_time:7961ms step_avg:33.45ms
step:239/2000 train_time:7994ms step_avg:33.45ms
step:240/2000 train_time:8027ms step_avg:33.45ms
step:241/2000 train_time:8061ms step_avg:33.45ms
step:242/2000 train_time:8094ms step_avg:33.45ms
step:243/2000 train_time:8127ms step_avg:33.44ms
step:244/2000 train_time:8160ms step_avg:33.44ms
step:245/2000 train_time:8193ms step_avg:33.44ms
step:246/2000 train_time:8227ms step_avg:33.44ms
step:247/2000 train_time:8259ms step_avg:33.44ms
step:248/2000 train_time:8293ms step_avg:33.44ms
step:249/2000 train_time:8326ms step_avg:33.44ms
step:250/2000 train_time:8360ms step_avg:33.44ms
step:250/2000 val_loss:4.2641 train_time:8395ms step_avg:33.58ms
step:251/2000 train_time:8414ms step_avg:33.52ms
step:252/2000 train_time:8433ms step_avg:33.46ms
step:253/2000 train_time:8463ms step_avg:33.45ms
step:254/2000 train_time:8496ms step_avg:33.45ms
step:255/2000 train_time:8530ms step_avg:33.45ms
step:256/2000 train_time:8565ms step_avg:33.46ms
step:257/2000 train_time:8598ms step_avg:33.46ms
step:258/2000 train_time:8632ms step_avg:33.46ms
step:259/2000 train_time:8665ms step_avg:33.45ms
step:260/2000 train_time:8698ms step_avg:33.45ms
step:261/2000 train_time:8731ms step_avg:33.45ms
step:262/2000 train_time:8764ms step_avg:33.45ms
step:263/2000 train_time:8797ms step_avg:33.45ms
step:264/2000 train_time:8830ms step_avg:33.45ms
step:265/2000 train_time:8863ms step_avg:33.44ms
step:266/2000 train_time:8896ms step_avg:33.44ms
step:267/2000 train_time:8929ms step_avg:33.44ms
step:268/2000 train_time:8962ms step_avg:33.44ms
step:269/2000 train_time:8994ms step_avg:33.44ms
step:270/2000 train_time:9027ms step_avg:33.43ms
step:271/2000 train_time:9060ms step_avg:33.43ms
step:272/2000 train_time:9093ms step_avg:33.43ms
step:273/2000 train_time:9126ms step_avg:33.43ms
step:274/2000 train_time:9159ms step_avg:33.43ms
step:275/2000 train_time:9192ms step_avg:33.42ms
step:276/2000 train_time:9225ms step_avg:33.42ms
step:277/2000 train_time:9258ms step_avg:33.42ms
step:278/2000 train_time:9291ms step_avg:33.42ms
step:279/2000 train_time:9324ms step_avg:33.42ms
step:280/2000 train_time:9357ms step_avg:33.42ms
step:281/2000 train_time:9390ms step_avg:33.42ms
step:282/2000 train_time:9424ms step_avg:33.42ms
step:283/2000 train_time:9457ms step_avg:33.42ms
step:284/2000 train_time:9490ms step_avg:33.42ms
step:285/2000 train_time:9523ms step_avg:33.42ms
step:286/2000 train_time:9557ms step_avg:33.42ms
step:287/2000 train_time:9590ms step_avg:33.42ms
step:288/2000 train_time:9624ms step_avg:33.42ms
step:289/2000 train_time:9657ms step_avg:33.42ms
step:290/2000 train_time:9691ms step_avg:33.42ms
step:291/2000 train_time:9724ms step_avg:33.41ms
step:292/2000 train_time:9757ms step_avg:33.41ms
step:293/2000 train_time:9790ms step_avg:33.41ms
step:294/2000 train_time:9823ms step_avg:33.41ms
step:295/2000 train_time:9856ms step_avg:33.41ms
step:296/2000 train_time:9889ms step_avg:33.41ms
step:297/2000 train_time:9923ms step_avg:33.41ms
step:298/2000 train_time:9956ms step_avg:33.41ms
step:299/2000 train_time:9989ms step_avg:33.41ms
step:300/2000 train_time:10022ms step_avg:33.41ms
step:301/2000 train_time:10055ms step_avg:33.40ms
step:302/2000 train_time:10088ms step_avg:33.40ms
step:303/2000 train_time:10121ms step_avg:33.40ms
step:304/2000 train_time:10154ms step_avg:33.40ms
step:305/2000 train_time:10186ms step_avg:33.40ms
step:306/2000 train_time:10219ms step_avg:33.40ms
step:307/2000 train_time:10252ms step_avg:33.39ms
step:308/2000 train_time:10285ms step_avg:33.39ms
step:309/2000 train_time:10318ms step_avg:33.39ms
step:310/2000 train_time:10352ms step_avg:33.39ms
step:311/2000 train_time:10385ms step_avg:33.39ms
step:312/2000 train_time:10418ms step_avg:33.39ms
step:313/2000 train_time:10451ms step_avg:33.39ms
step:314/2000 train_time:10485ms step_avg:33.39ms
step:315/2000 train_time:10518ms step_avg:33.39ms
step:316/2000 train_time:10551ms step_avg:33.39ms
step:317/2000 train_time:10584ms step_avg:33.39ms
step:318/2000 train_time:10618ms step_avg:33.39ms
step:319/2000 train_time:10650ms step_avg:33.39ms
step:320/2000 train_time:10684ms step_avg:33.39ms
step:321/2000 train_time:10717ms step_avg:33.39ms
step:322/2000 train_time:10750ms step_avg:33.39ms
step:323/2000 train_time:10783ms step_avg:33.38ms
step:324/2000 train_time:10816ms step_avg:33.38ms
step:325/2000 train_time:10849ms step_avg:33.38ms
step:326/2000 train_time:10883ms step_avg:33.38ms
step:327/2000 train_time:10915ms step_avg:33.38ms
step:328/2000 train_time:10949ms step_avg:33.38ms
step:329/2000 train_time:10982ms step_avg:33.38ms
step:330/2000 train_time:11015ms step_avg:33.38ms
step:331/2000 train_time:11048ms step_avg:33.38ms
step:332/2000 train_time:11081ms step_avg:33.38ms
step:333/2000 train_time:11114ms step_avg:33.37ms
step:334/2000 train_time:11147ms step_avg:33.37ms
step:335/2000 train_time:11180ms step_avg:33.37ms
step:336/2000 train_time:11213ms step_avg:33.37ms
step:337/2000 train_time:11246ms step_avg:33.37ms
step:338/2000 train_time:11280ms step_avg:33.37ms
step:339/2000 train_time:11313ms step_avg:33.37ms
step:340/2000 train_time:11346ms step_avg:33.37ms
step:341/2000 train_time:11378ms step_avg:33.37ms
step:342/2000 train_time:11411ms step_avg:33.37ms
step:343/2000 train_time:11444ms step_avg:33.36ms
step:344/2000 train_time:11477ms step_avg:33.36ms
step:345/2000 train_time:11510ms step_avg:33.36ms
step:346/2000 train_time:11544ms step_avg:33.36ms
step:347/2000 train_time:11577ms step_avg:33.36ms
step:348/2000 train_time:11610ms step_avg:33.36ms
step:349/2000 train_time:11643ms step_avg:33.36ms
step:350/2000 train_time:11676ms step_avg:33.36ms
step:351/2000 train_time:11709ms step_avg:33.36ms
step:352/2000 train_time:11742ms step_avg:33.36ms
step:353/2000 train_time:11775ms step_avg:33.36ms
step:354/2000 train_time:11809ms step_avg:33.36ms
step:355/2000 train_time:11842ms step_avg:33.36ms
step:356/2000 train_time:11875ms step_avg:33.36ms
step:357/2000 train_time:11908ms step_avg:33.36ms
step:358/2000 train_time:11942ms step_avg:33.36ms
step:359/2000 train_time:11974ms step_avg:33.35ms
step:360/2000 train_time:12008ms step_avg:33.35ms
step:361/2000 train_time:12040ms step_avg:33.35ms
step:362/2000 train_time:12074ms step_avg:33.35ms
step:363/2000 train_time:12107ms step_avg:33.35ms
step:364/2000 train_time:12140ms step_avg:33.35ms
step:365/2000 train_time:12173ms step_avg:33.35ms
step:366/2000 train_time:12206ms step_avg:33.35ms
step:367/2000 train_time:12239ms step_avg:33.35ms
step:368/2000 train_time:12272ms step_avg:33.35ms
step:369/2000 train_time:12306ms step_avg:33.35ms
step:370/2000 train_time:12339ms step_avg:33.35ms
step:371/2000 train_time:12372ms step_avg:33.35ms
step:372/2000 train_time:12405ms step_avg:33.35ms
step:373/2000 train_time:12438ms step_avg:33.35ms
step:374/2000 train_time:12471ms step_avg:33.34ms
step:375/2000 train_time:12504ms step_avg:33.34ms
step:376/2000 train_time:12537ms step_avg:33.34ms
step:377/2000 train_time:12570ms step_avg:33.34ms
step:378/2000 train_time:12603ms step_avg:33.34ms
step:379/2000 train_time:12636ms step_avg:33.34ms
step:380/2000 train_time:12669ms step_avg:33.34ms
step:381/2000 train_time:12702ms step_avg:33.34ms
step:382/2000 train_time:12735ms step_avg:33.34ms
step:383/2000 train_time:12768ms step_avg:33.34ms
step:384/2000 train_time:12801ms step_avg:33.34ms
step:385/2000 train_time:12834ms step_avg:33.34ms
step:386/2000 train_time:12867ms step_avg:33.34ms
step:387/2000 train_time:12900ms step_avg:33.33ms
step:388/2000 train_time:12933ms step_avg:33.33ms
step:389/2000 train_time:12967ms step_avg:33.33ms
step:390/2000 train_time:13000ms step_avg:33.33ms
step:391/2000 train_time:13033ms step_avg:33.33ms
step:392/2000 train_time:13067ms step_avg:33.33ms
step:393/2000 train_time:13099ms step_avg:33.33ms
step:394/2000 train_time:13133ms step_avg:33.33ms
step:395/2000 train_time:13165ms step_avg:33.33ms
step:396/2000 train_time:13198ms step_avg:33.33ms
step:397/2000 train_time:13231ms step_avg:33.33ms
step:398/2000 train_time:13264ms step_avg:33.33ms
step:399/2000 train_time:13297ms step_avg:33.33ms
step:400/2000 train_time:13330ms step_avg:33.33ms
step:401/2000 train_time:13363ms step_avg:33.32ms
step:402/2000 train_time:13396ms step_avg:33.32ms
step:403/2000 train_time:13429ms step_avg:33.32ms
step:404/2000 train_time:13462ms step_avg:33.32ms
step:405/2000 train_time:13495ms step_avg:33.32ms
step:406/2000 train_time:13529ms step_avg:33.32ms
step:407/2000 train_time:13562ms step_avg:33.32ms
step:408/2000 train_time:13595ms step_avg:33.32ms
step:409/2000 train_time:13628ms step_avg:33.32ms
step:410/2000 train_time:13661ms step_avg:33.32ms
step:411/2000 train_time:13694ms step_avg:33.32ms
step:412/2000 train_time:13727ms step_avg:33.32ms
step:413/2000 train_time:13760ms step_avg:33.32ms
step:414/2000 train_time:13793ms step_avg:33.32ms
step:415/2000 train_time:13826ms step_avg:33.32ms
step:416/2000 train_time:13859ms step_avg:33.32ms
step:417/2000 train_time:13893ms step_avg:33.32ms
step:418/2000 train_time:13926ms step_avg:33.32ms
step:419/2000 train_time:13959ms step_avg:33.32ms
step:420/2000 train_time:13992ms step_avg:33.32ms
step:421/2000 train_time:14026ms step_avg:33.32ms
step:422/2000 train_time:14059ms step_avg:33.32ms
step:423/2000 train_time:14092ms step_avg:33.31ms
step:424/2000 train_time:14125ms step_avg:33.31ms
step:425/2000 train_time:14158ms step_avg:33.31ms
step:426/2000 train_time:14192ms step_avg:33.31ms
step:427/2000 train_time:14225ms step_avg:33.31ms
step:428/2000 train_time:14258ms step_avg:33.31ms
step:429/2000 train_time:14291ms step_avg:33.31ms
step:430/2000 train_time:14324ms step_avg:33.31ms
step:431/2000 train_time:14357ms step_avg:33.31ms
step:432/2000 train_time:14391ms step_avg:33.31ms
step:433/2000 train_time:14424ms step_avg:33.31ms
step:434/2000 train_time:14457ms step_avg:33.31ms
step:435/2000 train_time:14490ms step_avg:33.31ms
step:436/2000 train_time:14524ms step_avg:33.31ms
step:437/2000 train_time:14556ms step_avg:33.31ms
step:438/2000 train_time:14590ms step_avg:33.31ms
step:439/2000 train_time:14623ms step_avg:33.31ms
step:440/2000 train_time:14656ms step_avg:33.31ms
step:441/2000 train_time:14689ms step_avg:33.31ms
step:442/2000 train_time:14723ms step_avg:33.31ms
step:443/2000 train_time:14755ms step_avg:33.31ms
step:444/2000 train_time:14789ms step_avg:33.31ms
step:445/2000 train_time:14822ms step_avg:33.31ms
step:446/2000 train_time:14855ms step_avg:33.31ms
step:447/2000 train_time:14888ms step_avg:33.31ms
step:448/2000 train_time:14922ms step_avg:33.31ms
step:449/2000 train_time:14954ms step_avg:33.31ms
step:450/2000 train_time:14988ms step_avg:33.31ms
step:451/2000 train_time:15021ms step_avg:33.31ms
step:452/2000 train_time:15054ms step_avg:33.31ms
step:453/2000 train_time:15087ms step_avg:33.31ms
step:454/2000 train_time:15120ms step_avg:33.31ms
step:455/2000 train_time:15153ms step_avg:33.30ms
step:456/2000 train_time:15186ms step_avg:33.30ms
step:457/2000 train_time:15219ms step_avg:33.30ms
step:458/2000 train_time:15253ms step_avg:33.30ms
step:459/2000 train_time:15286ms step_avg:33.30ms
step:460/2000 train_time:15319ms step_avg:33.30ms
step:461/2000 train_time:15352ms step_avg:33.30ms
step:462/2000 train_time:15385ms step_avg:33.30ms
step:463/2000 train_time:15418ms step_avg:33.30ms
step:464/2000 train_time:15452ms step_avg:33.30ms
step:465/2000 train_time:15485ms step_avg:33.30ms
step:466/2000 train_time:15518ms step_avg:33.30ms
step:467/2000 train_time:15551ms step_avg:33.30ms
step:468/2000 train_time:15585ms step_avg:33.30ms
step:469/2000 train_time:15618ms step_avg:33.30ms
step:470/2000 train_time:15651ms step_avg:33.30ms
step:471/2000 train_time:15684ms step_avg:33.30ms
step:472/2000 train_time:15718ms step_avg:33.30ms
step:473/2000 train_time:15750ms step_avg:33.30ms
step:474/2000 train_time:15784ms step_avg:33.30ms
step:475/2000 train_time:15817ms step_avg:33.30ms
step:476/2000 train_time:15850ms step_avg:33.30ms
step:477/2000 train_time:15883ms step_avg:33.30ms
step:478/2000 train_time:15917ms step_avg:33.30ms
step:479/2000 train_time:15950ms step_avg:33.30ms
step:480/2000 train_time:15983ms step_avg:33.30ms
step:481/2000 train_time:16016ms step_avg:33.30ms
step:482/2000 train_time:16050ms step_avg:33.30ms
step:483/2000 train_time:16082ms step_avg:33.30ms
step:484/2000 train_time:16116ms step_avg:33.30ms
step:485/2000 train_time:16148ms step_avg:33.30ms
step:486/2000 train_time:16181ms step_avg:33.30ms
step:487/2000 train_time:16214ms step_avg:33.29ms
step:488/2000 train_time:16247ms step_avg:33.29ms
step:489/2000 train_time:16281ms step_avg:33.29ms
step:490/2000 train_time:16314ms step_avg:33.29ms
step:491/2000 train_time:16347ms step_avg:33.29ms
step:492/2000 train_time:16380ms step_avg:33.29ms
step:493/2000 train_time:16413ms step_avg:33.29ms
step:494/2000 train_time:16446ms step_avg:33.29ms
step:495/2000 train_time:16479ms step_avg:33.29ms
step:496/2000 train_time:16512ms step_avg:33.29ms
step:497/2000 train_time:16545ms step_avg:33.29ms
step:498/2000 train_time:16578ms step_avg:33.29ms
step:499/2000 train_time:16611ms step_avg:33.29ms
step:500/2000 train_time:16644ms step_avg:33.29ms
step:500/2000 val_loss:3.9937 train_time:16680ms step_avg:33.36ms
step:501/2000 train_time:16700ms step_avg:33.33ms
step:502/2000 train_time:16720ms step_avg:33.31ms
step:503/2000 train_time:16747ms step_avg:33.30ms
step:504/2000 train_time:16782ms step_avg:33.30ms
step:505/2000 train_time:16816ms step_avg:33.30ms
step:506/2000 train_time:16850ms step_avg:33.30ms
step:507/2000 train_time:16884ms step_avg:33.30ms
step:508/2000 train_time:16917ms step_avg:33.30ms
step:509/2000 train_time:16951ms step_avg:33.30ms
step:510/2000 train_time:16984ms step_avg:33.30ms
step:511/2000 train_time:17017ms step_avg:33.30ms
step:512/2000 train_time:17050ms step_avg:33.30ms
step:513/2000 train_time:17083ms step_avg:33.30ms
step:514/2000 train_time:17116ms step_avg:33.30ms
step:515/2000 train_time:17149ms step_avg:33.30ms
step:516/2000 train_time:17182ms step_avg:33.30ms
step:517/2000 train_time:17215ms step_avg:33.30ms
step:518/2000 train_time:17248ms step_avg:33.30ms
step:519/2000 train_time:17280ms step_avg:33.30ms
step:520/2000 train_time:17314ms step_avg:33.30ms
step:521/2000 train_time:17346ms step_avg:33.29ms
step:522/2000 train_time:17379ms step_avg:33.29ms
step:523/2000 train_time:17412ms step_avg:33.29ms
step:524/2000 train_time:17445ms step_avg:33.29ms
step:525/2000 train_time:17478ms step_avg:33.29ms
step:526/2000 train_time:17511ms step_avg:33.29ms
step:527/2000 train_time:17543ms step_avg:33.29ms
step:528/2000 train_time:17577ms step_avg:33.29ms
step:529/2000 train_time:17609ms step_avg:33.29ms
step:530/2000 train_time:17643ms step_avg:33.29ms
step:531/2000 train_time:17676ms step_avg:33.29ms
step:532/2000 train_time:17710ms step_avg:33.29ms
step:533/2000 train_time:17743ms step_avg:33.29ms
step:534/2000 train_time:17777ms step_avg:33.29ms
step:535/2000 train_time:17810ms step_avg:33.29ms
step:536/2000 train_time:17844ms step_avg:33.29ms
step:537/2000 train_time:17877ms step_avg:33.29ms
step:538/2000 train_time:17911ms step_avg:33.29ms
step:539/2000 train_time:17944ms step_avg:33.29ms
step:540/2000 train_time:17977ms step_avg:33.29ms
step:541/2000 train_time:18010ms step_avg:33.29ms
step:542/2000 train_time:18043ms step_avg:33.29ms
step:543/2000 train_time:18076ms step_avg:33.29ms
step:544/2000 train_time:18109ms step_avg:33.29ms
step:545/2000 train_time:18142ms step_avg:33.29ms
step:546/2000 train_time:18175ms step_avg:33.29ms
step:547/2000 train_time:18208ms step_avg:33.29ms
step:548/2000 train_time:18241ms step_avg:33.29ms
step:549/2000 train_time:18273ms step_avg:33.28ms
step:550/2000 train_time:18307ms step_avg:33.28ms
step:551/2000 train_time:18339ms step_avg:33.28ms
step:552/2000 train_time:18372ms step_avg:33.28ms
step:553/2000 train_time:18405ms step_avg:33.28ms
step:554/2000 train_time:18438ms step_avg:33.28ms
step:555/2000 train_time:18471ms step_avg:33.28ms
step:556/2000 train_time:18505ms step_avg:33.28ms
step:557/2000 train_time:18537ms step_avg:33.28ms
step:558/2000 train_time:18571ms step_avg:33.28ms
step:559/2000 train_time:18603ms step_avg:33.28ms
step:560/2000 train_time:18636ms step_avg:33.28ms
step:561/2000 train_time:18669ms step_avg:33.28ms
step:562/2000 train_time:18702ms step_avg:33.28ms
step:563/2000 train_time:18735ms step_avg:33.28ms
step:564/2000 train_time:18769ms step_avg:33.28ms
step:565/2000 train_time:18802ms step_avg:33.28ms
step:566/2000 train_time:18835ms step_avg:33.28ms
step:567/2000 train_time:18868ms step_avg:33.28ms
step:568/2000 train_time:18901ms step_avg:33.28ms
step:569/2000 train_time:18934ms step_avg:33.28ms
step:570/2000 train_time:18968ms step_avg:33.28ms
step:571/2000 train_time:19001ms step_avg:33.28ms
step:572/2000 train_time:19034ms step_avg:33.28ms
step:573/2000 train_time:19067ms step_avg:33.28ms
step:574/2000 train_time:19101ms step_avg:33.28ms
step:575/2000 train_time:19134ms step_avg:33.28ms
step:576/2000 train_time:19167ms step_avg:33.28ms
step:577/2000 train_time:19200ms step_avg:33.28ms
step:578/2000 train_time:19233ms step_avg:33.28ms
step:579/2000 train_time:19266ms step_avg:33.28ms
step:580/2000 train_time:19300ms step_avg:33.28ms
step:581/2000 train_time:19333ms step_avg:33.27ms
step:582/2000 train_time:19366ms step_avg:33.27ms
step:583/2000 train_time:19399ms step_avg:33.27ms
step:584/2000 train_time:19432ms step_avg:33.27ms
step:585/2000 train_time:19464ms step_avg:33.27ms
step:586/2000 train_time:19498ms step_avg:33.27ms
step:587/2000 train_time:19530ms step_avg:33.27ms
step:588/2000 train_time:19564ms step_avg:33.27ms
step:589/2000 train_time:19597ms step_avg:33.27ms
step:590/2000 train_time:19630ms step_avg:33.27ms
step:591/2000 train_time:19663ms step_avg:33.27ms
step:592/2000 train_time:19696ms step_avg:33.27ms
step:593/2000 train_time:19729ms step_avg:33.27ms
step:594/2000 train_time:19763ms step_avg:33.27ms
step:595/2000 train_time:19796ms step_avg:33.27ms
step:596/2000 train_time:19829ms step_avg:33.27ms
step:597/2000 train_time:19862ms step_avg:33.27ms
step:598/2000 train_time:19895ms step_avg:33.27ms
step:599/2000 train_time:19928ms step_avg:33.27ms
step:600/2000 train_time:19962ms step_avg:33.27ms
step:601/2000 train_time:19995ms step_avg:33.27ms
step:602/2000 train_time:20028ms step_avg:33.27ms
step:603/2000 train_time:20061ms step_avg:33.27ms
step:604/2000 train_time:20094ms step_avg:33.27ms
step:605/2000 train_time:20127ms step_avg:33.27ms
step:606/2000 train_time:20161ms step_avg:33.27ms
step:607/2000 train_time:20194ms step_avg:33.27ms
step:608/2000 train_time:20227ms step_avg:33.27ms
step:609/2000 train_time:20260ms step_avg:33.27ms
step:610/2000 train_time:20294ms step_avg:33.27ms
step:611/2000 train_time:20326ms step_avg:33.27ms
step:612/2000 train_time:20360ms step_avg:33.27ms
step:613/2000 train_time:20392ms step_avg:33.27ms
step:614/2000 train_time:20426ms step_avg:33.27ms
step:615/2000 train_time:20458ms step_avg:33.27ms
step:616/2000 train_time:20492ms step_avg:33.27ms
step:617/2000 train_time:20525ms step_avg:33.27ms
step:618/2000 train_time:20558ms step_avg:33.27ms
step:619/2000 train_time:20591ms step_avg:33.26ms
step:620/2000 train_time:20624ms step_avg:33.26ms
step:621/2000 train_time:20657ms step_avg:33.26ms
step:622/2000 train_time:20690ms step_avg:33.26ms
step:623/2000 train_time:20723ms step_avg:33.26ms
step:624/2000 train_time:20757ms step_avg:33.26ms
step:625/2000 train_time:20790ms step_avg:33.26ms
step:626/2000 train_time:20823ms step_avg:33.26ms
step:627/2000 train_time:20856ms step_avg:33.26ms
step:628/2000 train_time:20890ms step_avg:33.26ms
step:629/2000 train_time:20923ms step_avg:33.26ms
step:630/2000 train_time:20956ms step_avg:33.26ms
step:631/2000 train_time:20989ms step_avg:33.26ms
step:632/2000 train_time:21023ms step_avg:33.26ms
step:633/2000 train_time:21055ms step_avg:33.26ms
step:634/2000 train_time:21089ms step_avg:33.26ms
step:635/2000 train_time:21122ms step_avg:33.26ms
step:636/2000 train_time:21155ms step_avg:33.26ms
step:637/2000 train_time:21188ms step_avg:33.26ms
step:638/2000 train_time:21221ms step_avg:33.26ms
step:639/2000 train_time:21254ms step_avg:33.26ms
step:640/2000 train_time:21287ms step_avg:33.26ms
step:641/2000 train_time:21321ms step_avg:33.26ms
step:642/2000 train_time:21354ms step_avg:33.26ms
step:643/2000 train_time:21387ms step_avg:33.26ms
step:644/2000 train_time:21420ms step_avg:33.26ms
step:645/2000 train_time:21453ms step_avg:33.26ms
step:646/2000 train_time:21487ms step_avg:33.26ms
step:647/2000 train_time:21520ms step_avg:33.26ms
step:648/2000 train_time:21553ms step_avg:33.26ms
step:649/2000 train_time:21586ms step_avg:33.26ms
step:650/2000 train_time:21620ms step_avg:33.26ms
step:651/2000 train_time:21652ms step_avg:33.26ms
step:652/2000 train_time:21686ms step_avg:33.26ms
step:653/2000 train_time:21719ms step_avg:33.26ms
step:654/2000 train_time:21752ms step_avg:33.26ms
step:655/2000 train_time:21786ms step_avg:33.26ms
step:656/2000 train_time:21845ms step_avg:33.30ms
step:657/2000 train_time:21906ms step_avg:33.34ms
step:658/2000 train_time:21966ms step_avg:33.38ms
step:659/2000 train_time:22027ms step_avg:33.43ms
step:660/2000 train_time:22087ms step_avg:33.47ms
step:661/2000 train_time:22148ms step_avg:33.51ms
step:662/2000 train_time:22208ms step_avg:33.55ms
step:663/2000 train_time:22268ms step_avg:33.59ms
step:664/2000 train_time:22327ms step_avg:33.63ms
step:665/2000 train_time:22388ms step_avg:33.67ms
step:666/2000 train_time:22448ms step_avg:33.71ms
step:667/2000 train_time:22508ms step_avg:33.74ms
step:668/2000 train_time:22568ms step_avg:33.78ms
step:669/2000 train_time:22628ms step_avg:33.82ms
step:670/2000 train_time:22688ms step_avg:33.86ms
step:671/2000 train_time:22749ms step_avg:33.90ms
step:672/2000 train_time:22808ms step_avg:33.94ms
step:673/2000 train_time:22868ms step_avg:33.98ms
step:674/2000 train_time:22929ms step_avg:34.02ms
step:675/2000 train_time:22989ms step_avg:34.06ms
step:676/2000 train_time:23050ms step_avg:34.10ms
step:677/2000 train_time:23110ms step_avg:34.14ms
step:678/2000 train_time:23171ms step_avg:34.18ms
step:679/2000 train_time:23231ms step_avg:34.21ms
step:680/2000 train_time:23291ms step_avg:34.25ms
step:681/2000 train_time:23352ms step_avg:34.29ms
step:682/2000 train_time:23411ms step_avg:34.33ms
step:683/2000 train_time:23472ms step_avg:34.37ms
step:684/2000 train_time:23532ms step_avg:34.40ms
step:685/2000 train_time:23593ms step_avg:34.44ms
step:686/2000 train_time:23652ms step_avg:34.48ms
step:687/2000 train_time:23712ms step_avg:34.52ms
step:688/2000 train_time:23772ms step_avg:34.55ms
step:689/2000 train_time:23833ms step_avg:34.59ms
step:690/2000 train_time:23893ms step_avg:34.63ms
step:691/2000 train_time:23952ms step_avg:34.66ms
step:692/2000 train_time:24013ms step_avg:34.70ms
step:693/2000 train_time:24073ms step_avg:34.74ms
step:694/2000 train_time:24134ms step_avg:34.77ms
step:695/2000 train_time:24193ms step_avg:34.81ms
step:696/2000 train_time:24254ms step_avg:34.85ms
step:697/2000 train_time:24314ms step_avg:34.88ms
step:698/2000 train_time:24374ms step_avg:34.92ms
step:699/2000 train_time:24434ms step_avg:34.96ms
step:700/2000 train_time:24494ms step_avg:34.99ms
step:701/2000 train_time:24554ms step_avg:35.03ms
step:702/2000 train_time:24614ms step_avg:35.06ms
step:703/2000 train_time:24674ms step_avg:35.10ms
step:704/2000 train_time:24734ms step_avg:35.13ms
step:705/2000 train_time:24795ms step_avg:35.17ms
step:706/2000 train_time:24854ms step_avg:35.20ms
step:707/2000 train_time:24915ms step_avg:35.24ms
step:708/2000 train_time:24975ms step_avg:35.27ms
step:709/2000 train_time:25036ms step_avg:35.31ms
step:710/2000 train_time:25095ms step_avg:35.35ms
step:711/2000 train_time:25156ms step_avg:35.38ms
step:712/2000 train_time:25216ms step_avg:35.42ms
step:713/2000 train_time:25277ms step_avg:35.45ms
step:714/2000 train_time:25336ms step_avg:35.48ms
step:715/2000 train_time:25396ms step_avg:35.52ms
step:716/2000 train_time:25455ms step_avg:35.55ms
step:717/2000 train_time:25515ms step_avg:35.59ms
step:718/2000 train_time:25574ms step_avg:35.62ms
step:719/2000 train_time:25636ms step_avg:35.65ms
step:720/2000 train_time:25696ms step_avg:35.69ms
step:721/2000 train_time:25757ms step_avg:35.72ms
step:722/2000 train_time:25816ms step_avg:35.76ms
step:723/2000 train_time:25876ms step_avg:35.79ms
step:724/2000 train_time:25936ms step_avg:35.82ms
step:725/2000 train_time:25997ms step_avg:35.86ms
step:726/2000 train_time:26057ms step_avg:35.89ms
step:727/2000 train_time:26118ms step_avg:35.93ms
step:728/2000 train_time:26178ms step_avg:35.96ms
step:729/2000 train_time:26239ms step_avg:35.99ms
step:730/2000 train_time:26299ms step_avg:36.03ms
step:731/2000 train_time:26359ms step_avg:36.06ms
step:732/2000 train_time:26419ms step_avg:36.09ms
step:733/2000 train_time:26479ms step_avg:36.12ms
step:734/2000 train_time:26538ms step_avg:36.16ms
step:735/2000 train_time:26599ms step_avg:36.19ms
step:736/2000 train_time:26658ms step_avg:36.22ms
step:737/2000 train_time:26719ms step_avg:36.25ms
step:738/2000 train_time:26778ms step_avg:36.29ms
step:739/2000 train_time:26839ms step_avg:36.32ms
step:740/2000 train_time:26898ms step_avg:36.35ms
step:741/2000 train_time:26959ms step_avg:36.38ms
step:742/2000 train_time:27018ms step_avg:36.41ms
step:743/2000 train_time:27077ms step_avg:36.44ms
step:744/2000 train_time:27138ms step_avg:36.48ms
step:745/2000 train_time:27198ms step_avg:36.51ms
step:746/2000 train_time:27257ms step_avg:36.54ms
step:747/2000 train_time:27317ms step_avg:36.57ms
step:748/2000 train_time:27377ms step_avg:36.60ms
step:749/2000 train_time:27438ms step_avg:36.63ms
step:750/2000 train_time:27498ms step_avg:36.66ms
step:750/2000 val_loss:3.8221 train_time:27561ms step_avg:36.75ms
step:751/2000 train_time:27580ms step_avg:36.72ms
step:752/2000 train_time:27621ms step_avg:36.73ms
step:753/2000 train_time:27685ms step_avg:36.77ms
step:754/2000 train_time:27749ms step_avg:36.80ms
step:755/2000 train_time:27810ms step_avg:36.83ms
step:756/2000 train_time:27871ms step_avg:36.87ms
step:757/2000 train_time:27931ms step_avg:36.90ms
step:758/2000 train_time:27990ms step_avg:36.93ms
step:759/2000 train_time:28050ms step_avg:36.96ms
step:760/2000 train_time:28109ms step_avg:36.99ms
step:761/2000 train_time:28168ms step_avg:37.01ms
step:762/2000 train_time:28228ms step_avg:37.04ms
step:763/2000 train_time:28287ms step_avg:37.07ms
step:764/2000 train_time:28347ms step_avg:37.10ms
step:765/2000 train_time:28406ms step_avg:37.13ms
step:766/2000 train_time:28466ms step_avg:37.16ms
step:767/2000 train_time:28527ms step_avg:37.19ms
step:768/2000 train_time:28587ms step_avg:37.22ms
step:769/2000 train_time:28649ms step_avg:37.26ms
step:770/2000 train_time:28711ms step_avg:37.29ms
step:771/2000 train_time:28772ms step_avg:37.32ms
step:772/2000 train_time:28832ms step_avg:37.35ms
step:773/2000 train_time:28892ms step_avg:37.38ms
step:774/2000 train_time:28951ms step_avg:37.40ms
step:775/2000 train_time:29011ms step_avg:37.43ms
step:776/2000 train_time:29070ms step_avg:37.46ms
step:777/2000 train_time:29130ms step_avg:37.49ms
step:778/2000 train_time:29189ms step_avg:37.52ms
step:779/2000 train_time:29249ms step_avg:37.55ms
step:780/2000 train_time:29308ms step_avg:37.57ms
step:781/2000 train_time:29369ms step_avg:37.60ms
step:782/2000 train_time:29428ms step_avg:37.63ms
step:783/2000 train_time:29489ms step_avg:37.66ms
step:784/2000 train_time:29549ms step_avg:37.69ms
step:785/2000 train_time:29610ms step_avg:37.72ms
step:786/2000 train_time:29670ms step_avg:37.75ms
step:787/2000 train_time:29731ms step_avg:37.78ms
step:788/2000 train_time:29792ms step_avg:37.81ms
step:789/2000 train_time:29852ms step_avg:37.84ms
step:790/2000 train_time:29911ms step_avg:37.86ms
step:791/2000 train_time:29971ms step_avg:37.89ms
step:792/2000 train_time:30030ms step_avg:37.92ms
step:793/2000 train_time:30090ms step_avg:37.94ms
step:794/2000 train_time:30150ms step_avg:37.97ms
step:795/2000 train_time:30210ms step_avg:38.00ms
step:796/2000 train_time:30269ms step_avg:38.03ms
step:797/2000 train_time:30329ms step_avg:38.05ms
step:798/2000 train_time:30389ms step_avg:38.08ms
step:799/2000 train_time:30449ms step_avg:38.11ms
step:800/2000 train_time:30509ms step_avg:38.14ms
step:801/2000 train_time:30569ms step_avg:38.16ms
step:802/2000 train_time:30629ms step_avg:38.19ms
step:803/2000 train_time:30690ms step_avg:38.22ms
step:804/2000 train_time:30751ms step_avg:38.25ms
step:805/2000 train_time:30812ms step_avg:38.28ms
step:806/2000 train_time:30872ms step_avg:38.30ms
step:807/2000 train_time:30933ms step_avg:38.33ms
step:808/2000 train_time:30992ms step_avg:38.36ms
step:809/2000 train_time:31053ms step_avg:38.38ms
step:810/2000 train_time:31112ms step_avg:38.41ms
step:811/2000 train_time:31172ms step_avg:38.44ms
step:812/2000 train_time:31231ms step_avg:38.46ms
step:813/2000 train_time:31291ms step_avg:38.49ms
step:814/2000 train_time:31351ms step_avg:38.51ms
step:815/2000 train_time:31411ms step_avg:38.54ms
step:816/2000 train_time:31470ms step_avg:38.57ms
step:817/2000 train_time:31530ms step_avg:38.59ms
step:818/2000 train_time:31590ms step_avg:38.62ms
step:819/2000 train_time:31651ms step_avg:38.65ms
step:820/2000 train_time:31711ms step_avg:38.67ms
step:821/2000 train_time:31772ms step_avg:38.70ms
step:822/2000 train_time:31832ms step_avg:38.72ms
step:823/2000 train_time:31892ms step_avg:38.75ms
step:824/2000 train_time:31952ms step_avg:38.78ms
step:825/2000 train_time:32012ms step_avg:38.80ms
step:826/2000 train_time:32071ms step_avg:38.83ms
step:827/2000 train_time:32131ms step_avg:38.85ms
step:828/2000 train_time:32190ms step_avg:38.88ms
step:829/2000 train_time:32250ms step_avg:38.90ms
step:830/2000 train_time:32309ms step_avg:38.93ms
step:831/2000 train_time:32370ms step_avg:38.95ms
step:832/2000 train_time:32429ms step_avg:38.98ms
step:833/2000 train_time:32489ms step_avg:39.00ms
step:834/2000 train_time:32549ms step_avg:39.03ms
step:835/2000 train_time:32610ms step_avg:39.05ms
step:836/2000 train_time:32670ms step_avg:39.08ms
step:837/2000 train_time:32731ms step_avg:39.10ms
step:838/2000 train_time:32790ms step_avg:39.13ms
step:839/2000 train_time:32851ms step_avg:39.16ms
step:840/2000 train_time:32911ms step_avg:39.18ms
step:841/2000 train_time:32972ms step_avg:39.21ms
step:842/2000 train_time:33032ms step_avg:39.23ms
step:843/2000 train_time:33093ms step_avg:39.26ms
step:844/2000 train_time:33152ms step_avg:39.28ms
step:845/2000 train_time:33212ms step_avg:39.30ms
step:846/2000 train_time:33272ms step_avg:39.33ms
step:847/2000 train_time:33331ms step_avg:39.35ms
step:848/2000 train_time:33391ms step_avg:39.38ms
step:849/2000 train_time:33451ms step_avg:39.40ms
step:850/2000 train_time:33510ms step_avg:39.42ms
step:851/2000 train_time:33571ms step_avg:39.45ms
step:852/2000 train_time:33630ms step_avg:39.47ms
step:853/2000 train_time:33692ms step_avg:39.50ms
step:854/2000 train_time:33751ms step_avg:39.52ms
step:855/2000 train_time:33811ms step_avg:39.55ms
step:856/2000 train_time:33872ms step_avg:39.57ms
step:857/2000 train_time:33932ms step_avg:39.59ms
step:858/2000 train_time:33992ms step_avg:39.62ms
step:859/2000 train_time:34052ms step_avg:39.64ms
step:860/2000 train_time:34112ms step_avg:39.66ms
step:861/2000 train_time:34172ms step_avg:39.69ms
step:862/2000 train_time:34232ms step_avg:39.71ms
step:863/2000 train_time:34292ms step_avg:39.74ms
step:864/2000 train_time:34352ms step_avg:39.76ms
step:865/2000 train_time:34413ms step_avg:39.78ms
step:866/2000 train_time:34472ms step_avg:39.81ms
step:867/2000 train_time:34532ms step_avg:39.83ms
step:868/2000 train_time:34592ms step_avg:39.85ms
step:869/2000 train_time:34653ms step_avg:39.88ms
step:870/2000 train_time:34713ms step_avg:39.90ms
step:871/2000 train_time:34774ms step_avg:39.92ms
step:872/2000 train_time:34833ms step_avg:39.95ms
step:873/2000 train_time:34894ms step_avg:39.97ms
step:874/2000 train_time:34953ms step_avg:39.99ms
step:875/2000 train_time:35014ms step_avg:40.02ms
step:876/2000 train_time:35074ms step_avg:40.04ms
step:877/2000 train_time:35134ms step_avg:40.06ms
step:878/2000 train_time:35194ms step_avg:40.08ms
step:879/2000 train_time:35254ms step_avg:40.11ms
step:880/2000 train_time:35313ms step_avg:40.13ms
step:881/2000 train_time:35373ms step_avg:40.15ms
step:882/2000 train_time:35432ms step_avg:40.17ms
step:883/2000 train_time:35492ms step_avg:40.19ms
step:884/2000 train_time:35552ms step_avg:40.22ms
step:885/2000 train_time:35613ms step_avg:40.24ms
step:886/2000 train_time:35673ms step_avg:40.26ms
step:887/2000 train_time:35733ms step_avg:40.29ms
step:888/2000 train_time:35793ms step_avg:40.31ms
step:889/2000 train_time:35853ms step_avg:40.33ms
step:890/2000 train_time:35912ms step_avg:40.35ms
step:891/2000 train_time:35972ms step_avg:40.37ms
step:892/2000 train_time:36032ms step_avg:40.39ms
step:893/2000 train_time:36092ms step_avg:40.42ms
step:894/2000 train_time:36152ms step_avg:40.44ms
step:895/2000 train_time:36213ms step_avg:40.46ms
step:896/2000 train_time:36272ms step_avg:40.48ms
step:897/2000 train_time:36333ms step_avg:40.50ms
step:898/2000 train_time:36392ms step_avg:40.53ms
step:899/2000 train_time:36453ms step_avg:40.55ms
step:900/2000 train_time:36513ms step_avg:40.57ms
step:901/2000 train_time:36574ms step_avg:40.59ms
step:902/2000 train_time:36633ms step_avg:40.61ms
step:903/2000 train_time:36693ms step_avg:40.64ms
step:904/2000 train_time:36753ms step_avg:40.66ms
step:905/2000 train_time:36814ms step_avg:40.68ms
step:906/2000 train_time:36874ms step_avg:40.70ms
step:907/2000 train_time:36934ms step_avg:40.72ms
step:908/2000 train_time:36994ms step_avg:40.74ms
step:909/2000 train_time:37055ms step_avg:40.76ms
step:910/2000 train_time:37114ms step_avg:40.78ms
step:911/2000 train_time:37174ms step_avg:40.81ms
step:912/2000 train_time:37233ms step_avg:40.83ms
step:913/2000 train_time:37293ms step_avg:40.85ms
step:914/2000 train_time:37353ms step_avg:40.87ms
step:915/2000 train_time:37414ms step_avg:40.89ms
step:916/2000 train_time:37475ms step_avg:40.91ms
step:917/2000 train_time:37536ms step_avg:40.93ms
step:918/2000 train_time:37595ms step_avg:40.95ms
step:919/2000 train_time:37656ms step_avg:40.97ms
step:920/2000 train_time:37716ms step_avg:41.00ms
step:921/2000 train_time:37777ms step_avg:41.02ms
step:922/2000 train_time:37836ms step_avg:41.04ms
step:923/2000 train_time:37896ms step_avg:41.06ms
step:924/2000 train_time:37956ms step_avg:41.08ms
step:925/2000 train_time:38018ms step_avg:41.10ms
step:926/2000 train_time:38077ms step_avg:41.12ms
step:927/2000 train_time:38137ms step_avg:41.14ms
step:928/2000 train_time:38196ms step_avg:41.16ms
step:929/2000 train_time:38257ms step_avg:41.18ms
step:930/2000 train_time:38317ms step_avg:41.20ms
step:931/2000 train_time:38377ms step_avg:41.22ms
step:932/2000 train_time:38437ms step_avg:41.24ms
step:933/2000 train_time:38497ms step_avg:41.26ms
step:934/2000 train_time:38557ms step_avg:41.28ms
step:935/2000 train_time:38617ms step_avg:41.30ms
step:936/2000 train_time:38677ms step_avg:41.32ms
step:937/2000 train_time:38738ms step_avg:41.34ms
step:938/2000 train_time:38798ms step_avg:41.36ms
step:939/2000 train_time:38858ms step_avg:41.38ms
step:940/2000 train_time:38918ms step_avg:41.40ms
step:941/2000 train_time:38978ms step_avg:41.42ms
step:942/2000 train_time:39038ms step_avg:41.44ms
step:943/2000 train_time:39098ms step_avg:41.46ms
step:944/2000 train_time:39157ms step_avg:41.48ms
step:945/2000 train_time:39218ms step_avg:41.50ms
step:946/2000 train_time:39278ms step_avg:41.52ms
step:947/2000 train_time:39338ms step_avg:41.54ms
step:948/2000 train_time:39398ms step_avg:41.56ms
step:949/2000 train_time:39459ms step_avg:41.58ms
step:950/2000 train_time:39519ms step_avg:41.60ms
step:951/2000 train_time:39579ms step_avg:41.62ms
step:952/2000 train_time:39639ms step_avg:41.64ms
step:953/2000 train_time:39699ms step_avg:41.66ms
step:954/2000 train_time:39759ms step_avg:41.68ms
step:955/2000 train_time:39820ms step_avg:41.70ms
step:956/2000 train_time:39881ms step_avg:41.72ms
step:957/2000 train_time:39942ms step_avg:41.74ms
step:958/2000 train_time:40003ms step_avg:41.76ms
step:959/2000 train_time:40063ms step_avg:41.78ms
step:960/2000 train_time:40123ms step_avg:41.80ms
step:961/2000 train_time:40183ms step_avg:41.81ms
step:962/2000 train_time:40243ms step_avg:41.83ms
step:963/2000 train_time:40303ms step_avg:41.85ms
step:964/2000 train_time:40364ms step_avg:41.87ms
step:965/2000 train_time:40424ms step_avg:41.89ms
step:966/2000 train_time:40484ms step_avg:41.91ms
step:967/2000 train_time:40545ms step_avg:41.93ms
step:968/2000 train_time:40605ms step_avg:41.95ms
step:969/2000 train_time:40665ms step_avg:41.97ms
step:970/2000 train_time:40725ms step_avg:41.98ms
step:971/2000 train_time:40787ms step_avg:42.00ms
step:972/2000 train_time:40847ms step_avg:42.02ms
step:973/2000 train_time:40908ms step_avg:42.04ms
step:974/2000 train_time:40967ms step_avg:42.06ms
step:975/2000 train_time:41028ms step_avg:42.08ms
step:976/2000 train_time:41088ms step_avg:42.10ms
step:977/2000 train_time:41149ms step_avg:42.12ms
step:978/2000 train_time:41209ms step_avg:42.14ms
step:979/2000 train_time:41268ms step_avg:42.15ms
step:980/2000 train_time:41328ms step_avg:42.17ms
step:981/2000 train_time:41389ms step_avg:42.19ms
step:982/2000 train_time:41449ms step_avg:42.21ms
step:983/2000 train_time:41509ms step_avg:42.23ms
step:984/2000 train_time:41569ms step_avg:42.24ms
step:985/2000 train_time:41629ms step_avg:42.26ms
step:986/2000 train_time:41689ms step_avg:42.28ms
step:987/2000 train_time:41749ms step_avg:42.30ms
step:988/2000 train_time:41810ms step_avg:42.32ms
step:989/2000 train_time:41870ms step_avg:42.34ms
step:990/2000 train_time:41930ms step_avg:42.35ms
step:991/2000 train_time:41991ms step_avg:42.37ms
step:992/2000 train_time:42051ms step_avg:42.39ms
step:993/2000 train_time:42112ms step_avg:42.41ms
step:994/2000 train_time:42171ms step_avg:42.43ms
step:995/2000 train_time:42230ms step_avg:42.44ms
step:996/2000 train_time:42289ms step_avg:42.46ms
step:997/2000 train_time:42349ms step_avg:42.48ms
step:998/2000 train_time:42409ms step_avg:42.49ms
step:999/2000 train_time:42470ms step_avg:42.51ms
step:1000/2000 train_time:42529ms step_avg:42.53ms
step:1000/2000 val_loss:3.6764 train_time:42593ms step_avg:42.59ms
step:1001/2000 train_time:42613ms step_avg:42.57ms
step:1002/2000 train_time:42652ms step_avg:42.57ms
step:1003/2000 train_time:42714ms step_avg:42.59ms
step:1004/2000 train_time:42776ms step_avg:42.61ms
step:1005/2000 train_time:42837ms step_avg:42.62ms
step:1006/2000 train_time:42897ms step_avg:42.64ms
step:1007/2000 train_time:42957ms step_avg:42.66ms
step:1008/2000 train_time:43016ms step_avg:42.67ms
step:1009/2000 train_time:43076ms step_avg:42.69ms
step:1010/2000 train_time:43136ms step_avg:42.71ms
step:1011/2000 train_time:43195ms step_avg:42.72ms
step:1012/2000 train_time:43254ms step_avg:42.74ms
step:1013/2000 train_time:43313ms step_avg:42.76ms
step:1014/2000 train_time:43373ms step_avg:42.77ms
step:1015/2000 train_time:43432ms step_avg:42.79ms
step:1016/2000 train_time:43491ms step_avg:42.81ms
step:1017/2000 train_time:43553ms step_avg:42.83ms
step:1018/2000 train_time:43614ms step_avg:42.84ms
step:1019/2000 train_time:43676ms step_avg:42.86ms
step:1020/2000 train_time:43738ms step_avg:42.88ms
step:1021/2000 train_time:43799ms step_avg:42.90ms
step:1022/2000 train_time:43860ms step_avg:42.92ms
step:1023/2000 train_time:43920ms step_avg:42.93ms
step:1024/2000 train_time:43979ms step_avg:42.95ms
step:1025/2000 train_time:44039ms step_avg:42.96ms
step:1026/2000 train_time:44098ms step_avg:42.98ms
step:1027/2000 train_time:44158ms step_avg:43.00ms
step:1028/2000 train_time:44217ms step_avg:43.01ms
step:1029/2000 train_time:44277ms step_avg:43.03ms
step:1030/2000 train_time:44336ms step_avg:43.04ms
step:1031/2000 train_time:44396ms step_avg:43.06ms
step:1032/2000 train_time:44455ms step_avg:43.08ms
step:1033/2000 train_time:44515ms step_avg:43.09ms
step:1034/2000 train_time:44576ms step_avg:43.11ms
step:1035/2000 train_time:44637ms step_avg:43.13ms
step:1036/2000 train_time:44697ms step_avg:43.14ms
step:1037/2000 train_time:44759ms step_avg:43.16ms
step:1038/2000 train_time:44819ms step_avg:43.18ms
step:1039/2000 train_time:44880ms step_avg:43.20ms
step:1040/2000 train_time:44940ms step_avg:43.21ms
step:1041/2000 train_time:45000ms step_avg:43.23ms
step:1042/2000 train_time:45059ms step_avg:43.24ms
step:1043/2000 train_time:45119ms step_avg:43.26ms
step:1044/2000 train_time:45178ms step_avg:43.27ms
step:1045/2000 train_time:45238ms step_avg:43.29ms
step:1046/2000 train_time:45297ms step_avg:43.30ms
step:1047/2000 train_time:45356ms step_avg:43.32ms
step:1048/2000 train_time:45417ms step_avg:43.34ms
step:1049/2000 train_time:45477ms step_avg:43.35ms
step:1050/2000 train_time:45537ms step_avg:43.37ms
step:1051/2000 train_time:45598ms step_avg:43.39ms
step:1052/2000 train_time:45659ms step_avg:43.40ms
step:1053/2000 train_time:45720ms step_avg:43.42ms
step:1054/2000 train_time:45780ms step_avg:43.43ms
step:1055/2000 train_time:45841ms step_avg:43.45ms
step:1056/2000 train_time:45900ms step_avg:43.47ms
step:1057/2000 train_time:45960ms step_avg:43.48ms
step:1058/2000 train_time:46019ms step_avg:43.50ms
step:1059/2000 train_time:46079ms step_avg:43.51ms
step:1060/2000 train_time:46139ms step_avg:43.53ms
step:1061/2000 train_time:46198ms step_avg:43.54ms
step:1062/2000 train_time:46257ms step_avg:43.56ms
step:1063/2000 train_time:46318ms step_avg:43.57ms
step:1064/2000 train_time:46378ms step_avg:43.59ms
step:1065/2000 train_time:46438ms step_avg:43.60ms
step:1066/2000 train_time:46498ms step_avg:43.62ms
step:1067/2000 train_time:46558ms step_avg:43.63ms
step:1068/2000 train_time:46618ms step_avg:43.65ms
step:1069/2000 train_time:46679ms step_avg:43.67ms
step:1070/2000 train_time:46740ms step_avg:43.68ms
step:1071/2000 train_time:46800ms step_avg:43.70ms
step:1072/2000 train_time:46860ms step_avg:43.71ms
step:1073/2000 train_time:46920ms step_avg:43.73ms
step:1074/2000 train_time:46979ms step_avg:43.74ms
step:1075/2000 train_time:47040ms step_avg:43.76ms
step:1076/2000 train_time:47099ms step_avg:43.77ms
step:1077/2000 train_time:47159ms step_avg:43.79ms
step:1078/2000 train_time:47218ms step_avg:43.80ms
step:1079/2000 train_time:47279ms step_avg:43.82ms
step:1080/2000 train_time:47338ms step_avg:43.83ms
step:1081/2000 train_time:47399ms step_avg:43.85ms
step:1082/2000 train_time:47459ms step_avg:43.86ms
step:1083/2000 train_time:47520ms step_avg:43.88ms
step:1084/2000 train_time:47579ms step_avg:43.89ms
step:1085/2000 train_time:47640ms step_avg:43.91ms
step:1086/2000 train_time:47701ms step_avg:43.92ms
step:1087/2000 train_time:47762ms step_avg:43.94ms
step:1088/2000 train_time:47821ms step_avg:43.95ms
step:1089/2000 train_time:47881ms step_avg:43.97ms
step:1090/2000 train_time:47941ms step_avg:43.98ms
step:1091/2000 train_time:48001ms step_avg:44.00ms
step:1092/2000 train_time:48060ms step_avg:44.01ms
step:1093/2000 train_time:48120ms step_avg:44.03ms
step:1094/2000 train_time:48179ms step_avg:44.04ms
step:1095/2000 train_time:48240ms step_avg:44.05ms
step:1096/2000 train_time:48300ms step_avg:44.07ms
step:1097/2000 train_time:48361ms step_avg:44.08ms
step:1098/2000 train_time:48420ms step_avg:44.10ms
step:1099/2000 train_time:48481ms step_avg:44.11ms
step:1100/2000 train_time:48541ms step_avg:44.13ms
step:1101/2000 train_time:48602ms step_avg:44.14ms
step:1102/2000 train_time:48662ms step_avg:44.16ms
step:1103/2000 train_time:48723ms step_avg:44.17ms
step:1104/2000 train_time:48783ms step_avg:44.19ms
step:1105/2000 train_time:48843ms step_avg:44.20ms
step:1106/2000 train_time:48903ms step_avg:44.22ms
step:1107/2000 train_time:48963ms step_avg:44.23ms
step:1108/2000 train_time:49022ms step_avg:44.24ms
step:1109/2000 train_time:49082ms step_avg:44.26ms
step:1110/2000 train_time:49141ms step_avg:44.27ms
step:1111/2000 train_time:49202ms step_avg:44.29ms
step:1112/2000 train_time:49262ms step_avg:44.30ms
step:1113/2000 train_time:49322ms step_avg:44.31ms
step:1114/2000 train_time:49381ms step_avg:44.33ms
step:1115/2000 train_time:49442ms step_avg:44.34ms
step:1116/2000 train_time:49501ms step_avg:44.36ms
step:1117/2000 train_time:49561ms step_avg:44.37ms
step:1118/2000 train_time:49621ms step_avg:44.38ms
step:1119/2000 train_time:49682ms step_avg:44.40ms
step:1120/2000 train_time:49742ms step_avg:44.41ms
step:1121/2000 train_time:49802ms step_avg:44.43ms
step:1122/2000 train_time:49862ms step_avg:44.44ms
step:1123/2000 train_time:49922ms step_avg:44.45ms
step:1124/2000 train_time:49981ms step_avg:44.47ms
step:1125/2000 train_time:50041ms step_avg:44.48ms
step:1126/2000 train_time:50100ms step_avg:44.49ms
step:1127/2000 train_time:50160ms step_avg:44.51ms
step:1128/2000 train_time:50219ms step_avg:44.52ms
step:1129/2000 train_time:50280ms step_avg:44.53ms
step:1130/2000 train_time:50340ms step_avg:44.55ms
step:1131/2000 train_time:50400ms step_avg:44.56ms
step:1132/2000 train_time:50459ms step_avg:44.57ms
step:1133/2000 train_time:50520ms step_avg:44.59ms
step:1134/2000 train_time:50579ms step_avg:44.60ms
step:1135/2000 train_time:50640ms step_avg:44.62ms
step:1136/2000 train_time:50700ms step_avg:44.63ms
step:1137/2000 train_time:50761ms step_avg:44.65ms
step:1138/2000 train_time:50821ms step_avg:44.66ms
step:1139/2000 train_time:50882ms step_avg:44.67ms
step:1140/2000 train_time:50942ms step_avg:44.69ms
step:1141/2000 train_time:51002ms step_avg:44.70ms
step:1142/2000 train_time:51061ms step_avg:44.71ms
step:1143/2000 train_time:51122ms step_avg:44.73ms
step:1144/2000 train_time:51181ms step_avg:44.74ms
step:1145/2000 train_time:51241ms step_avg:44.75ms
step:1146/2000 train_time:51301ms step_avg:44.77ms
step:1147/2000 train_time:51362ms step_avg:44.78ms
step:1148/2000 train_time:51421ms step_avg:44.79ms
step:1149/2000 train_time:51481ms step_avg:44.81ms
step:1150/2000 train_time:51541ms step_avg:44.82ms
step:1151/2000 train_time:51601ms step_avg:44.83ms
step:1152/2000 train_time:51661ms step_avg:44.84ms
step:1153/2000 train_time:51722ms step_avg:44.86ms
step:1154/2000 train_time:51782ms step_avg:44.87ms
step:1155/2000 train_time:51842ms step_avg:44.89ms
step:1156/2000 train_time:51902ms step_avg:44.90ms
step:1157/2000 train_time:51963ms step_avg:44.91ms
step:1158/2000 train_time:52023ms step_avg:44.92ms
step:1159/2000 train_time:52083ms step_avg:44.94ms
step:1160/2000 train_time:52142ms step_avg:44.95ms
step:1161/2000 train_time:52201ms step_avg:44.96ms
step:1162/2000 train_time:52261ms step_avg:44.98ms
step:1163/2000 train_time:52322ms step_avg:44.99ms
step:1164/2000 train_time:52381ms step_avg:45.00ms
step:1165/2000 train_time:52442ms step_avg:45.01ms
step:1166/2000 train_time:52502ms step_avg:45.03ms
step:1167/2000 train_time:52563ms step_avg:45.04ms
step:1168/2000 train_time:52622ms step_avg:45.05ms
step:1169/2000 train_time:52682ms step_avg:45.07ms
step:1170/2000 train_time:52742ms step_avg:45.08ms
step:1171/2000 train_time:52802ms step_avg:45.09ms
step:1172/2000 train_time:52862ms step_avg:45.10ms
step:1173/2000 train_time:52923ms step_avg:45.12ms
step:1174/2000 train_time:52982ms step_avg:45.13ms
step:1175/2000 train_time:53042ms step_avg:45.14ms
step:1176/2000 train_time:53102ms step_avg:45.15ms
step:1177/2000 train_time:53162ms step_avg:45.17ms
step:1178/2000 train_time:53221ms step_avg:45.18ms
step:1179/2000 train_time:53281ms step_avg:45.19ms
step:1180/2000 train_time:53340ms step_avg:45.20ms
step:1181/2000 train_time:53401ms step_avg:45.22ms
step:1182/2000 train_time:53460ms step_avg:45.23ms
step:1183/2000 train_time:53521ms step_avg:45.24ms
step:1184/2000 train_time:53581ms step_avg:45.25ms
step:1185/2000 train_time:53641ms step_avg:45.27ms
step:1186/2000 train_time:53700ms step_avg:45.28ms
step:1187/2000 train_time:53761ms step_avg:45.29ms
step:1188/2000 train_time:53821ms step_avg:45.30ms
step:1189/2000 train_time:53881ms step_avg:45.32ms
step:1190/2000 train_time:53941ms step_avg:45.33ms
step:1191/2000 train_time:54001ms step_avg:45.34ms
step:1192/2000 train_time:54061ms step_avg:45.35ms
step:1193/2000 train_time:54121ms step_avg:45.37ms
step:1194/2000 train_time:54180ms step_avg:45.38ms
step:1195/2000 train_time:54241ms step_avg:45.39ms
step:1196/2000 train_time:54301ms step_avg:45.40ms
step:1197/2000 train_time:54361ms step_avg:45.41ms
step:1198/2000 train_time:54420ms step_avg:45.43ms
step:1199/2000 train_time:54481ms step_avg:45.44ms
step:1200/2000 train_time:54540ms step_avg:45.45ms
step:1201/2000 train_time:54601ms step_avg:45.46ms
step:1202/2000 train_time:54661ms step_avg:45.47ms
step:1203/2000 train_time:54721ms step_avg:45.49ms
step:1204/2000 train_time:54781ms step_avg:45.50ms
step:1205/2000 train_time:54840ms step_avg:45.51ms
step:1206/2000 train_time:54900ms step_avg:45.52ms
step:1207/2000 train_time:54961ms step_avg:45.54ms
step:1208/2000 train_time:55020ms step_avg:45.55ms
step:1209/2000 train_time:55081ms step_avg:45.56ms
step:1210/2000 train_time:55140ms step_avg:45.57ms
step:1211/2000 train_time:55201ms step_avg:45.58ms
step:1212/2000 train_time:55260ms step_avg:45.59ms
step:1213/2000 train_time:55322ms step_avg:45.61ms
step:1214/2000 train_time:55381ms step_avg:45.62ms
step:1215/2000 train_time:55441ms step_avg:45.63ms
step:1216/2000 train_time:55501ms step_avg:45.64ms
step:1217/2000 train_time:55562ms step_avg:45.66ms
step:1218/2000 train_time:55622ms step_avg:45.67ms
step:1219/2000 train_time:55682ms step_avg:45.68ms
step:1220/2000 train_time:55741ms step_avg:45.69ms
step:1221/2000 train_time:55801ms step_avg:45.70ms
step:1222/2000 train_time:55861ms step_avg:45.71ms
step:1223/2000 train_time:55922ms step_avg:45.72ms
step:1224/2000 train_time:55981ms step_avg:45.74ms
step:1225/2000 train_time:56041ms step_avg:45.75ms
step:1226/2000 train_time:56100ms step_avg:45.76ms
step:1227/2000 train_time:56160ms step_avg:45.77ms
step:1228/2000 train_time:56220ms step_avg:45.78ms
step:1229/2000 train_time:56280ms step_avg:45.79ms
step:1230/2000 train_time:56340ms step_avg:45.80ms
step:1231/2000 train_time:56400ms step_avg:45.82ms
step:1232/2000 train_time:56460ms step_avg:45.83ms
step:1233/2000 train_time:56520ms step_avg:45.84ms
step:1234/2000 train_time:56580ms step_avg:45.85ms
step:1235/2000 train_time:56640ms step_avg:45.86ms
step:1236/2000 train_time:56700ms step_avg:45.87ms
step:1237/2000 train_time:56761ms step_avg:45.89ms
step:1238/2000 train_time:56820ms step_avg:45.90ms
step:1239/2000 train_time:56881ms step_avg:45.91ms
step:1240/2000 train_time:56941ms step_avg:45.92ms
step:1241/2000 train_time:57002ms step_avg:45.93ms
step:1242/2000 train_time:57061ms step_avg:45.94ms
step:1243/2000 train_time:57122ms step_avg:45.95ms
step:1244/2000 train_time:57181ms step_avg:45.97ms
step:1245/2000 train_time:57241ms step_avg:45.98ms
step:1246/2000 train_time:57301ms step_avg:45.99ms
step:1247/2000 train_time:57362ms step_avg:46.00ms
step:1248/2000 train_time:57421ms step_avg:46.01ms
step:1249/2000 train_time:57482ms step_avg:46.02ms
step:1250/2000 train_time:57541ms step_avg:46.03ms
step:1250/2000 val_loss:3.5594 train_time:57604ms step_avg:46.08ms
step:1251/2000 train_time:57624ms step_avg:46.06ms
step:1252/2000 train_time:57663ms step_avg:46.06ms
step:1253/2000 train_time:57727ms step_avg:46.07ms
step:1254/2000 train_time:57789ms step_avg:46.08ms
step:1255/2000 train_time:57848ms step_avg:46.09ms
step:1256/2000 train_time:57908ms step_avg:46.10ms
step:1257/2000 train_time:57967ms step_avg:46.12ms
step:1258/2000 train_time:58026ms step_avg:46.13ms
step:1259/2000 train_time:58085ms step_avg:46.14ms
step:1260/2000 train_time:58144ms step_avg:46.15ms
step:1261/2000 train_time:58204ms step_avg:46.16ms
step:1262/2000 train_time:58263ms step_avg:46.17ms
step:1263/2000 train_time:58323ms step_avg:46.18ms
step:1264/2000 train_time:58382ms step_avg:46.19ms
step:1265/2000 train_time:58442ms step_avg:46.20ms
step:1266/2000 train_time:58502ms step_avg:46.21ms
step:1267/2000 train_time:58565ms step_avg:46.22ms
step:1268/2000 train_time:58625ms step_avg:46.23ms
step:1269/2000 train_time:58687ms step_avg:46.25ms
step:1270/2000 train_time:58747ms step_avg:46.26ms
step:1271/2000 train_time:58808ms step_avg:46.27ms
step:1272/2000 train_time:58867ms step_avg:46.28ms
step:1273/2000 train_time:58928ms step_avg:46.29ms
step:1274/2000 train_time:58988ms step_avg:46.30ms
step:1275/2000 train_time:59047ms step_avg:46.31ms
step:1276/2000 train_time:59106ms step_avg:46.32ms
step:1277/2000 train_time:59165ms step_avg:46.33ms
step:1278/2000 train_time:59224ms step_avg:46.34ms
step:1279/2000 train_time:59284ms step_avg:46.35ms
step:1280/2000 train_time:59343ms step_avg:46.36ms
step:1281/2000 train_time:59403ms step_avg:46.37ms
step:1282/2000 train_time:59463ms step_avg:46.38ms
step:1283/2000 train_time:59523ms step_avg:46.39ms
step:1284/2000 train_time:59584ms step_avg:46.40ms
step:1285/2000 train_time:59645ms step_avg:46.42ms
step:1286/2000 train_time:59706ms step_avg:46.43ms
step:1287/2000 train_time:59766ms step_avg:46.44ms
step:1288/2000 train_time:59827ms step_avg:46.45ms
step:1289/2000 train_time:59887ms step_avg:46.46ms
step:1290/2000 train_time:59946ms step_avg:46.47ms
step:1291/2000 train_time:60007ms step_avg:46.48ms
step:1292/2000 train_time:60066ms step_avg:46.49ms
step:1293/2000 train_time:60126ms step_avg:46.50ms
step:1294/2000 train_time:60186ms step_avg:46.51ms
step:1295/2000 train_time:60246ms step_avg:46.52ms
step:1296/2000 train_time:60305ms step_avg:46.53ms
step:1297/2000 train_time:60365ms step_avg:46.54ms
step:1298/2000 train_time:60424ms step_avg:46.55ms
step:1299/2000 train_time:60485ms step_avg:46.56ms
step:1300/2000 train_time:60545ms step_avg:46.57ms
step:1301/2000 train_time:60607ms step_avg:46.58ms
step:1302/2000 train_time:60666ms step_avg:46.59ms
step:1303/2000 train_time:60728ms step_avg:46.61ms
step:1304/2000 train_time:60787ms step_avg:46.62ms
step:1305/2000 train_time:60848ms step_avg:46.63ms
step:1306/2000 train_time:60908ms step_avg:46.64ms
step:1307/2000 train_time:60968ms step_avg:46.65ms
step:1308/2000 train_time:61027ms step_avg:46.66ms
step:1309/2000 train_time:61115ms step_avg:46.69ms
step:1310/2000 train_time:61202ms step_avg:46.72ms
step:1311/2000 train_time:61290ms step_avg:46.75ms
step:1312/2000 train_time:61377ms step_avg:46.78ms
step:1313/2000 train_time:61465ms step_avg:46.81ms
step:1314/2000 train_time:61554ms step_avg:46.84ms
step:1315/2000 train_time:61642ms step_avg:46.88ms
step:1316/2000 train_time:61731ms step_avg:46.91ms
step:1317/2000 train_time:61819ms step_avg:46.94ms
step:1318/2000 train_time:61908ms step_avg:46.97ms
step:1319/2000 train_time:61996ms step_avg:47.00ms
step:1320/2000 train_time:62083ms step_avg:47.03ms
step:1321/2000 train_time:62170ms step_avg:47.06ms
step:1322/2000 train_time:62257ms step_avg:47.09ms
step:1323/2000 train_time:62345ms step_avg:47.12ms
step:1324/2000 train_time:62432ms step_avg:47.15ms
step:1325/2000 train_time:62520ms step_avg:47.18ms
step:1326/2000 train_time:62608ms step_avg:47.22ms
step:1327/2000 train_time:62697ms step_avg:47.25ms
step:1328/2000 train_time:62786ms step_avg:47.28ms
step:1329/2000 train_time:62874ms step_avg:47.31ms
step:1330/2000 train_time:62961ms step_avg:47.34ms
step:1331/2000 train_time:63049ms step_avg:47.37ms
step:1332/2000 train_time:63137ms step_avg:47.40ms
step:1333/2000 train_time:63225ms step_avg:47.43ms
step:1334/2000 train_time:63312ms step_avg:47.46ms
step:1335/2000 train_time:63400ms step_avg:47.49ms
step:1336/2000 train_time:63487ms step_avg:47.52ms
step:1337/2000 train_time:63576ms step_avg:47.55ms
step:1338/2000 train_time:63663ms step_avg:47.58ms
step:1339/2000 train_time:63752ms step_avg:47.61ms
step:1340/2000 train_time:63839ms step_avg:47.64ms
step:1341/2000 train_time:63927ms step_avg:47.67ms
step:1342/2000 train_time:64015ms step_avg:47.70ms
step:1343/2000 train_time:64104ms step_avg:47.73ms
step:1344/2000 train_time:64191ms step_avg:47.76ms
step:1345/2000 train_time:64279ms step_avg:47.79ms
step:1346/2000 train_time:64366ms step_avg:47.82ms
step:1347/2000 train_time:64455ms step_avg:47.85ms
step:1348/2000 train_time:64542ms step_avg:47.88ms
step:1349/2000 train_time:64631ms step_avg:47.91ms
step:1350/2000 train_time:64719ms step_avg:47.94ms
step:1351/2000 train_time:64807ms step_avg:47.97ms
step:1352/2000 train_time:64895ms step_avg:48.00ms
step:1353/2000 train_time:64984ms step_avg:48.03ms
step:1354/2000 train_time:65073ms step_avg:48.06ms
step:1355/2000 train_time:65160ms step_avg:48.09ms
step:1356/2000 train_time:65247ms step_avg:48.12ms
step:1357/2000 train_time:65335ms step_avg:48.15ms
step:1358/2000 train_time:65422ms step_avg:48.18ms
step:1359/2000 train_time:65510ms step_avg:48.20ms
step:1360/2000 train_time:65597ms step_avg:48.23ms
step:1361/2000 train_time:65686ms step_avg:48.26ms
step:1362/2000 train_time:65773ms step_avg:48.29ms
step:1363/2000 train_time:65861ms step_avg:48.32ms
step:1364/2000 train_time:65949ms step_avg:48.35ms
step:1365/2000 train_time:66037ms step_avg:48.38ms
step:1366/2000 train_time:66125ms step_avg:48.41ms
step:1367/2000 train_time:66213ms step_avg:48.44ms
step:1368/2000 train_time:66300ms step_avg:48.46ms
step:1369/2000 train_time:66387ms step_avg:48.49ms
step:1370/2000 train_time:66476ms step_avg:48.52ms
step:1371/2000 train_time:66562ms step_avg:48.55ms
step:1372/2000 train_time:66650ms step_avg:48.58ms
step:1373/2000 train_time:66739ms step_avg:48.61ms
step:1374/2000 train_time:66827ms step_avg:48.64ms
step:1375/2000 train_time:66916ms step_avg:48.67ms
step:1376/2000 train_time:67004ms step_avg:48.69ms
step:1377/2000 train_time:67092ms step_avg:48.72ms
step:1378/2000 train_time:67180ms step_avg:48.75ms
step:1379/2000 train_time:67268ms step_avg:48.78ms
step:1380/2000 train_time:67355ms step_avg:48.81ms
step:1381/2000 train_time:67443ms step_avg:48.84ms
step:1382/2000 train_time:67531ms step_avg:48.86ms
step:1383/2000 train_time:67619ms step_avg:48.89ms
step:1384/2000 train_time:67706ms step_avg:48.92ms
step:1385/2000 train_time:67794ms step_avg:48.95ms
step:1386/2000 train_time:67881ms step_avg:48.98ms
step:1387/2000 train_time:67970ms step_avg:49.00ms
step:1388/2000 train_time:68058ms step_avg:49.03ms
step:1389/2000 train_time:68146ms step_avg:49.06ms
step:1390/2000 train_time:68233ms step_avg:49.09ms
step:1391/2000 train_time:68320ms step_avg:49.12ms
step:1392/2000 train_time:68408ms step_avg:49.14ms
step:1393/2000 train_time:68496ms step_avg:49.17ms
step:1394/2000 train_time:68584ms step_avg:49.20ms
step:1395/2000 train_time:68672ms step_avg:49.23ms
step:1396/2000 train_time:68759ms step_avg:49.25ms
step:1397/2000 train_time:68847ms step_avg:49.28ms
step:1398/2000 train_time:68935ms step_avg:49.31ms
step:1399/2000 train_time:69023ms step_avg:49.34ms
step:1400/2000 train_time:69112ms step_avg:49.37ms
step:1401/2000 train_time:69199ms step_avg:49.39ms
step:1402/2000 train_time:69286ms step_avg:49.42ms
step:1403/2000 train_time:69374ms step_avg:49.45ms
step:1404/2000 train_time:69461ms step_avg:49.47ms
step:1405/2000 train_time:69549ms step_avg:49.50ms
step:1406/2000 train_time:69637ms step_avg:49.53ms
step:1407/2000 train_time:69726ms step_avg:49.56ms
step:1408/2000 train_time:69814ms step_avg:49.58ms
step:1409/2000 train_time:69901ms step_avg:49.61ms
step:1410/2000 train_time:69989ms step_avg:49.64ms
step:1411/2000 train_time:70078ms step_avg:49.67ms
step:1412/2000 train_time:70166ms step_avg:49.69ms
step:1413/2000 train_time:70254ms step_avg:49.72ms
step:1414/2000 train_time:70340ms step_avg:49.75ms
step:1415/2000 train_time:70428ms step_avg:49.77ms
step:1416/2000 train_time:70517ms step_avg:49.80ms
step:1417/2000 train_time:70604ms step_avg:49.83ms
step:1418/2000 train_time:70692ms step_avg:49.85ms
step:1419/2000 train_time:70780ms step_avg:49.88ms
step:1420/2000 train_time:70867ms step_avg:49.91ms
step:1421/2000 train_time:70955ms step_avg:49.93ms
step:1422/2000 train_time:71043ms step_avg:49.96ms
step:1423/2000 train_time:71131ms step_avg:49.99ms
step:1424/2000 train_time:71219ms step_avg:50.01ms
step:1425/2000 train_time:71307ms step_avg:50.04ms
step:1426/2000 train_time:71395ms step_avg:50.07ms
step:1427/2000 train_time:71483ms step_avg:50.09ms
step:1428/2000 train_time:71571ms step_avg:50.12ms
step:1429/2000 train_time:71660ms step_avg:50.15ms
step:1430/2000 train_time:71748ms step_avg:50.17ms
step:1431/2000 train_time:71835ms step_avg:50.20ms
step:1432/2000 train_time:71922ms step_avg:50.22ms
step:1433/2000 train_time:72011ms step_avg:50.25ms
step:1434/2000 train_time:72099ms step_avg:50.28ms
step:1435/2000 train_time:72187ms step_avg:50.30ms
step:1436/2000 train_time:72275ms step_avg:50.33ms
step:1437/2000 train_time:72362ms step_avg:50.36ms
step:1438/2000 train_time:72450ms step_avg:50.38ms
step:1439/2000 train_time:72537ms step_avg:50.41ms
step:1440/2000 train_time:72626ms step_avg:50.43ms
step:1441/2000 train_time:72714ms step_avg:50.46ms
step:1442/2000 train_time:72802ms step_avg:50.49ms
step:1443/2000 train_time:72889ms step_avg:50.51ms
step:1444/2000 train_time:72979ms step_avg:50.54ms
step:1445/2000 train_time:73066ms step_avg:50.56ms
step:1446/2000 train_time:73155ms step_avg:50.59ms
step:1447/2000 train_time:73242ms step_avg:50.62ms
step:1448/2000 train_time:73330ms step_avg:50.64ms
step:1449/2000 train_time:73418ms step_avg:50.67ms
step:1450/2000 train_time:73505ms step_avg:50.69ms
step:1451/2000 train_time:73593ms step_avg:50.72ms
step:1452/2000 train_time:73681ms step_avg:50.74ms
step:1453/2000 train_time:73768ms step_avg:50.77ms
step:1454/2000 train_time:73857ms step_avg:50.80ms
step:1455/2000 train_time:73944ms step_avg:50.82ms
step:1456/2000 train_time:74032ms step_avg:50.85ms
step:1457/2000 train_time:74120ms step_avg:50.87ms
step:1458/2000 train_time:74207ms step_avg:50.90ms
step:1459/2000 train_time:74295ms step_avg:50.92ms
step:1460/2000 train_time:74383ms step_avg:50.95ms
step:1461/2000 train_time:74471ms step_avg:50.97ms
step:1462/2000 train_time:74558ms step_avg:51.00ms
step:1463/2000 train_time:74645ms step_avg:51.02ms
step:1464/2000 train_time:74733ms step_avg:51.05ms
step:1465/2000 train_time:74821ms step_avg:51.07ms
step:1466/2000 train_time:74910ms step_avg:51.10ms
step:1467/2000 train_time:74999ms step_avg:51.12ms
step:1468/2000 train_time:75088ms step_avg:51.15ms
step:1469/2000 train_time:75175ms step_avg:51.17ms
step:1470/2000 train_time:75262ms step_avg:51.20ms
step:1471/2000 train_time:75350ms step_avg:51.22ms
step:1472/2000 train_time:75438ms step_avg:51.25ms
step:1473/2000 train_time:75527ms step_avg:51.27ms
step:1474/2000 train_time:75615ms step_avg:51.30ms
step:1475/2000 train_time:75703ms step_avg:51.32ms
step:1476/2000 train_time:75791ms step_avg:51.35ms
step:1477/2000 train_time:75879ms step_avg:51.37ms
step:1478/2000 train_time:75967ms step_avg:51.40ms
step:1479/2000 train_time:76056ms step_avg:51.42ms
step:1480/2000 train_time:76143ms step_avg:51.45ms
step:1481/2000 train_time:76231ms step_avg:51.47ms
step:1482/2000 train_time:76318ms step_avg:51.50ms
step:1483/2000 train_time:76406ms step_avg:51.52ms
step:1484/2000 train_time:76494ms step_avg:51.55ms
step:1485/2000 train_time:76582ms step_avg:51.57ms
step:1486/2000 train_time:76670ms step_avg:51.59ms
step:1487/2000 train_time:76758ms step_avg:51.62ms
step:1488/2000 train_time:76846ms step_avg:51.64ms
step:1489/2000 train_time:76935ms step_avg:51.67ms
step:1490/2000 train_time:77022ms step_avg:51.69ms
step:1491/2000 train_time:77110ms step_avg:51.72ms
step:1492/2000 train_time:77198ms step_avg:51.74ms
step:1493/2000 train_time:77286ms step_avg:51.77ms
step:1494/2000 train_time:77374ms step_avg:51.79ms
step:1495/2000 train_time:77462ms step_avg:51.81ms
step:1496/2000 train_time:77550ms step_avg:51.84ms
step:1497/2000 train_time:77637ms step_avg:51.86ms
step:1498/2000 train_time:77725ms step_avg:51.89ms
step:1499/2000 train_time:77813ms step_avg:51.91ms
step:1500/2000 train_time:77900ms step_avg:51.93ms
step:1500/2000 val_loss:3.4438 train_time:77990ms step_avg:51.99ms
step:1501/2000 train_time:78011ms step_avg:51.97ms
step:1502/2000 train_time:78079ms step_avg:51.98ms
step:1503/2000 train_time:78173ms step_avg:52.01ms
step:1504/2000 train_time:78261ms step_avg:52.04ms
step:1505/2000 train_time:78349ms step_avg:52.06ms
step:1506/2000 train_time:78437ms step_avg:52.08ms
step:1507/2000 train_time:78524ms step_avg:52.11ms
step:1508/2000 train_time:78610ms step_avg:52.13ms
step:1509/2000 train_time:78698ms step_avg:52.15ms
step:1510/2000 train_time:78785ms step_avg:52.18ms
step:1511/2000 train_time:78872ms step_avg:52.20ms
step:1512/2000 train_time:78960ms step_avg:52.22ms
step:1513/2000 train_time:79050ms step_avg:52.25ms
step:1514/2000 train_time:79140ms step_avg:52.27ms
step:1515/2000 train_time:79229ms step_avg:52.30ms
step:1516/2000 train_time:79317ms step_avg:52.32ms
step:1517/2000 train_time:79404ms step_avg:52.34ms
step:1518/2000 train_time:79492ms step_avg:52.37ms
step:1519/2000 train_time:79580ms step_avg:52.39ms
step:1520/2000 train_time:79667ms step_avg:52.41ms
step:1521/2000 train_time:79754ms step_avg:52.44ms
step:1522/2000 train_time:79841ms step_avg:52.46ms
step:1523/2000 train_time:79928ms step_avg:52.48ms
step:1524/2000 train_time:80017ms step_avg:52.50ms
step:1525/2000 train_time:80105ms step_avg:52.53ms
step:1526/2000 train_time:80194ms step_avg:52.55ms
step:1527/2000 train_time:80283ms step_avg:52.58ms
step:1528/2000 train_time:80370ms step_avg:52.60ms
step:1529/2000 train_time:80458ms step_avg:52.62ms
step:1530/2000 train_time:80545ms step_avg:52.64ms
step:1531/2000 train_time:80633ms step_avg:52.67ms
step:1532/2000 train_time:80719ms step_avg:52.69ms
step:1533/2000 train_time:80806ms step_avg:52.71ms
step:1534/2000 train_time:80895ms step_avg:52.73ms
step:1535/2000 train_time:80983ms step_avg:52.76ms
step:1536/2000 train_time:81071ms step_avg:52.78ms
step:1537/2000 train_time:81160ms step_avg:52.80ms
step:1538/2000 train_time:81249ms step_avg:52.83ms
step:1539/2000 train_time:81337ms step_avg:52.85ms
step:1540/2000 train_time:81424ms step_avg:52.87ms
step:1541/2000 train_time:81512ms step_avg:52.90ms
step:1542/2000 train_time:81600ms step_avg:52.92ms
step:1543/2000 train_time:81686ms step_avg:52.94ms
step:1544/2000 train_time:81773ms step_avg:52.96ms
step:1545/2000 train_time:81861ms step_avg:52.98ms
step:1546/2000 train_time:81949ms step_avg:53.01ms
step:1547/2000 train_time:82037ms step_avg:53.03ms
step:1548/2000 train_time:82125ms step_avg:53.05ms
step:1549/2000 train_time:82214ms step_avg:53.08ms
step:1550/2000 train_time:82302ms step_avg:53.10ms
step:1551/2000 train_time:82391ms step_avg:53.12ms
step:1552/2000 train_time:82479ms step_avg:53.14ms
step:1553/2000 train_time:82566ms step_avg:53.17ms
step:1554/2000 train_time:82654ms step_avg:53.19ms
step:1555/2000 train_time:82741ms step_avg:53.21ms
step:1556/2000 train_time:82829ms step_avg:53.23ms
step:1557/2000 train_time:82916ms step_avg:53.25ms
step:1558/2000 train_time:83004ms step_avg:53.28ms
step:1559/2000 train_time:83092ms step_avg:53.30ms
step:1560/2000 train_time:83181ms step_avg:53.32ms
step:1561/2000 train_time:83269ms step_avg:53.34ms
step:1562/2000 train_time:83357ms step_avg:53.37ms
step:1563/2000 train_time:83445ms step_avg:53.39ms
step:1564/2000 train_time:83532ms step_avg:53.41ms
step:1565/2000 train_time:83620ms step_avg:53.43ms
step:1566/2000 train_time:83708ms step_avg:53.45ms
step:1567/2000 train_time:83796ms step_avg:53.48ms
step:1568/2000 train_time:83884ms step_avg:53.50ms
step:1569/2000 train_time:83971ms step_avg:53.52ms
step:1570/2000 train_time:84060ms step_avg:53.54ms
step:1571/2000 train_time:84148ms step_avg:53.56ms
step:1572/2000 train_time:84236ms step_avg:53.59ms
step:1573/2000 train_time:84324ms step_avg:53.61ms
step:1574/2000 train_time:84412ms step_avg:53.63ms
step:1575/2000 train_time:84501ms step_avg:53.65ms
step:1576/2000 train_time:84589ms step_avg:53.67ms
step:1577/2000 train_time:84677ms step_avg:53.70ms
step:1578/2000 train_time:84765ms step_avg:53.72ms
step:1579/2000 train_time:84852ms step_avg:53.74ms
step:1580/2000 train_time:84939ms step_avg:53.76ms
step:1581/2000 train_time:85027ms step_avg:53.78ms
step:1582/2000 train_time:85116ms step_avg:53.80ms
step:1583/2000 train_time:85203ms step_avg:53.82ms
step:1584/2000 train_time:85291ms step_avg:53.85ms
step:1585/2000 train_time:85379ms step_avg:53.87ms
step:1586/2000 train_time:85466ms step_avg:53.89ms
step:1587/2000 train_time:85554ms step_avg:53.91ms
step:1588/2000 train_time:85642ms step_avg:53.93ms
step:1589/2000 train_time:85730ms step_avg:53.95ms
step:1590/2000 train_time:85819ms step_avg:53.97ms
step:1591/2000 train_time:85907ms step_avg:54.00ms
step:1592/2000 train_time:85994ms step_avg:54.02ms
step:1593/2000 train_time:86082ms step_avg:54.04ms
step:1594/2000 train_time:86171ms step_avg:54.06ms
step:1595/2000 train_time:86259ms step_avg:54.08ms
step:1596/2000 train_time:86346ms step_avg:54.10ms
step:1597/2000 train_time:86434ms step_avg:54.12ms
step:1598/2000 train_time:86522ms step_avg:54.14ms
step:1599/2000 train_time:86610ms step_avg:54.17ms
step:1600/2000 train_time:86698ms step_avg:54.19ms
step:1601/2000 train_time:86786ms step_avg:54.21ms
step:1602/2000 train_time:86874ms step_avg:54.23ms
step:1603/2000 train_time:86961ms step_avg:54.25ms
step:1604/2000 train_time:87050ms step_avg:54.27ms
step:1605/2000 train_time:87138ms step_avg:54.29ms
step:1606/2000 train_time:87225ms step_avg:54.31ms
step:1607/2000 train_time:87313ms step_avg:54.33ms
step:1608/2000 train_time:87400ms step_avg:54.35ms
step:1609/2000 train_time:87488ms step_avg:54.37ms
step:1610/2000 train_time:87576ms step_avg:54.40ms
step:1611/2000 train_time:87664ms step_avg:54.42ms
step:1612/2000 train_time:87751ms step_avg:54.44ms
step:1613/2000 train_time:87839ms step_avg:54.46ms
step:1614/2000 train_time:87927ms step_avg:54.48ms
step:1615/2000 train_time:88015ms step_avg:54.50ms
step:1616/2000 train_time:88102ms step_avg:54.52ms
step:1617/2000 train_time:88190ms step_avg:54.54ms
step:1618/2000 train_time:88278ms step_avg:54.56ms
step:1619/2000 train_time:88365ms step_avg:54.58ms
step:1620/2000 train_time:88452ms step_avg:54.60ms
step:1621/2000 train_time:88541ms step_avg:54.62ms
step:1622/2000 train_time:88629ms step_avg:54.64ms
step:1623/2000 train_time:88717ms step_avg:54.66ms
step:1624/2000 train_time:88804ms step_avg:54.68ms
step:1625/2000 train_time:88892ms step_avg:54.70ms
step:1626/2000 train_time:88980ms step_avg:54.72ms
step:1627/2000 train_time:89068ms step_avg:54.74ms
step:1628/2000 train_time:89157ms step_avg:54.76ms
step:1629/2000 train_time:89244ms step_avg:54.78ms
step:1630/2000 train_time:89332ms step_avg:54.80ms
step:1631/2000 train_time:89421ms step_avg:54.83ms
step:1632/2000 train_time:89508ms step_avg:54.85ms
step:1633/2000 train_time:89596ms step_avg:54.87ms
step:1634/2000 train_time:89683ms step_avg:54.89ms
step:1635/2000 train_time:89771ms step_avg:54.91ms
step:1636/2000 train_time:89860ms step_avg:54.93ms
step:1637/2000 train_time:89947ms step_avg:54.95ms
step:1638/2000 train_time:90035ms step_avg:54.97ms
step:1639/2000 train_time:90123ms step_avg:54.99ms
step:1640/2000 train_time:90210ms step_avg:55.01ms
step:1641/2000 train_time:90300ms step_avg:55.03ms
step:1642/2000 train_time:90389ms step_avg:55.05ms
step:1643/2000 train_time:90478ms step_avg:55.07ms
step:1644/2000 train_time:90567ms step_avg:55.09ms
step:1645/2000 train_time:90655ms step_avg:55.11ms
step:1646/2000 train_time:90742ms step_avg:55.13ms
step:1647/2000 train_time:90830ms step_avg:55.15ms
step:1648/2000 train_time:90918ms step_avg:55.17ms
step:1649/2000 train_time:91006ms step_avg:55.19ms
step:1650/2000 train_time:91094ms step_avg:55.21ms
step:1651/2000 train_time:91181ms step_avg:55.23ms
step:1652/2000 train_time:91269ms step_avg:55.25ms
step:1653/2000 train_time:91358ms step_avg:55.27ms
step:1654/2000 train_time:91445ms step_avg:55.29ms
step:1655/2000 train_time:91533ms step_avg:55.31ms
step:1656/2000 train_time:91620ms step_avg:55.33ms
step:1657/2000 train_time:91708ms step_avg:55.35ms
step:1658/2000 train_time:91796ms step_avg:55.37ms
step:1659/2000 train_time:91884ms step_avg:55.39ms
step:1660/2000 train_time:91972ms step_avg:55.40ms
step:1661/2000 train_time:92061ms step_avg:55.42ms
step:1662/2000 train_time:92149ms step_avg:55.44ms
step:1663/2000 train_time:92237ms step_avg:55.46ms
step:1664/2000 train_time:92325ms step_avg:55.48ms
step:1665/2000 train_time:92413ms step_avg:55.50ms
step:1666/2000 train_time:92501ms step_avg:55.52ms
step:1667/2000 train_time:92589ms step_avg:55.54ms
step:1668/2000 train_time:92677ms step_avg:55.56ms
step:1669/2000 train_time:92764ms step_avg:55.58ms
step:1670/2000 train_time:92853ms step_avg:55.60ms
step:1671/2000 train_time:92941ms step_avg:55.62ms
step:1672/2000 train_time:93029ms step_avg:55.64ms
step:1673/2000 train_time:93119ms step_avg:55.66ms
step:1674/2000 train_time:93206ms step_avg:55.68ms
step:1675/2000 train_time:93295ms step_avg:55.70ms
step:1676/2000 train_time:93382ms step_avg:55.72ms
step:1677/2000 train_time:93469ms step_avg:55.74ms
step:1678/2000 train_time:93559ms step_avg:55.76ms
step:1679/2000 train_time:93646ms step_avg:55.78ms
step:1680/2000 train_time:93733ms step_avg:55.79ms
step:1681/2000 train_time:93821ms step_avg:55.81ms
step:1682/2000 train_time:93908ms step_avg:55.83ms
step:1683/2000 train_time:93997ms step_avg:55.85ms
step:1684/2000 train_time:94084ms step_avg:55.87ms
step:1685/2000 train_time:94172ms step_avg:55.89ms
step:1686/2000 train_time:94260ms step_avg:55.91ms
step:1687/2000 train_time:94349ms step_avg:55.93ms
step:1688/2000 train_time:94436ms step_avg:55.95ms
step:1689/2000 train_time:94524ms step_avg:55.96ms
step:1690/2000 train_time:94612ms step_avg:55.98ms
step:1691/2000 train_time:94699ms step_avg:56.00ms
step:1692/2000 train_time:94787ms step_avg:56.02ms
step:1693/2000 train_time:94875ms step_avg:56.04ms
step:1694/2000 train_time:94962ms step_avg:56.06ms
step:1695/2000 train_time:95050ms step_avg:56.08ms
step:1696/2000 train_time:95139ms step_avg:56.10ms
step:1697/2000 train_time:95227ms step_avg:56.12ms
step:1698/2000 train_time:95315ms step_avg:56.13ms
step:1699/2000 train_time:95403ms step_avg:56.15ms
step:1700/2000 train_time:95490ms step_avg:56.17ms
step:1701/2000 train_time:95579ms step_avg:56.19ms
step:1702/2000 train_time:95667ms step_avg:56.21ms
step:1703/2000 train_time:95755ms step_avg:56.23ms
step:1704/2000 train_time:95842ms step_avg:56.25ms
step:1705/2000 train_time:95931ms step_avg:56.26ms
step:1706/2000 train_time:96019ms step_avg:56.28ms
step:1707/2000 train_time:96107ms step_avg:56.30ms
step:1708/2000 train_time:96196ms step_avg:56.32ms
step:1709/2000 train_time:96283ms step_avg:56.34ms
step:1710/2000 train_time:96371ms step_avg:56.36ms
step:1711/2000 train_time:96460ms step_avg:56.38ms
step:1712/2000 train_time:96548ms step_avg:56.39ms
step:1713/2000 train_time:96636ms step_avg:56.41ms
step:1714/2000 train_time:96723ms step_avg:56.43ms
step:1715/2000 train_time:96812ms step_avg:56.45ms
step:1716/2000 train_time:96900ms step_avg:56.47ms
step:1717/2000 train_time:96987ms step_avg:56.49ms
step:1718/2000 train_time:97075ms step_avg:56.50ms
step:1719/2000 train_time:97163ms step_avg:56.52ms
step:1720/2000 train_time:97250ms step_avg:56.54ms
step:1721/2000 train_time:97338ms step_avg:56.56ms
step:1722/2000 train_time:97426ms step_avg:56.58ms
step:1723/2000 train_time:97514ms step_avg:56.60ms
step:1724/2000 train_time:97602ms step_avg:56.61ms
step:1725/2000 train_time:97690ms step_avg:56.63ms
step:1726/2000 train_time:97778ms step_avg:56.65ms
step:1727/2000 train_time:97865ms step_avg:56.67ms
step:1728/2000 train_time:97954ms step_avg:56.69ms
step:1729/2000 train_time:98042ms step_avg:56.70ms
step:1730/2000 train_time:98131ms step_avg:56.72ms
step:1731/2000 train_time:98220ms step_avg:56.74ms
step:1732/2000 train_time:98307ms step_avg:56.76ms
step:1733/2000 train_time:98396ms step_avg:56.78ms
step:1734/2000 train_time:98483ms step_avg:56.80ms
step:1735/2000 train_time:98572ms step_avg:56.81ms
step:1736/2000 train_time:98660ms step_avg:56.83ms
step:1737/2000 train_time:98748ms step_avg:56.85ms
step:1738/2000 train_time:98835ms step_avg:56.87ms
step:1739/2000 train_time:98922ms step_avg:56.88ms
step:1740/2000 train_time:99009ms step_avg:56.90ms
step:1741/2000 train_time:99098ms step_avg:56.92ms
step:1742/2000 train_time:99186ms step_avg:56.94ms
step:1743/2000 train_time:99273ms step_avg:56.96ms
step:1744/2000 train_time:99361ms step_avg:56.97ms
step:1745/2000 train_time:99449ms step_avg:56.99ms
step:1746/2000 train_time:99538ms step_avg:57.01ms
step:1747/2000 train_time:99625ms step_avg:57.03ms
step:1748/2000 train_time:99714ms step_avg:57.04ms
step:1749/2000 train_time:99801ms step_avg:57.06ms
step:1750/2000 train_time:99889ms step_avg:57.08ms
step:1750/2000 val_loss:3.3479 train_time:99979ms step_avg:57.13ms
step:1751/2000 train_time:99999ms step_avg:57.11ms
step:1752/2000 train_time:100066ms step_avg:57.12ms
step:1753/2000 train_time:100162ms step_avg:57.14ms
step:1754/2000 train_time:100252ms step_avg:57.16ms
step:1755/2000 train_time:100342ms step_avg:57.17ms
step:1756/2000 train_time:100429ms step_avg:57.19ms
step:1757/2000 train_time:100516ms step_avg:57.21ms
step:1758/2000 train_time:100603ms step_avg:57.23ms
step:1759/2000 train_time:100690ms step_avg:57.24ms
step:1760/2000 train_time:100777ms step_avg:57.26ms
step:1761/2000 train_time:100863ms step_avg:57.28ms
step:1762/2000 train_time:100952ms step_avg:57.29ms
step:1763/2000 train_time:101044ms step_avg:57.31ms
step:1764/2000 train_time:101134ms step_avg:57.33ms
step:1765/2000 train_time:101224ms step_avg:57.35ms
step:1766/2000 train_time:101313ms step_avg:57.37ms
step:1767/2000 train_time:101401ms step_avg:57.39ms
step:1768/2000 train_time:101489ms step_avg:57.40ms
step:1769/2000 train_time:101576ms step_avg:57.42ms
step:1770/2000 train_time:101663ms step_avg:57.44ms
step:1771/2000 train_time:101749ms step_avg:57.45ms
step:1772/2000 train_time:101837ms step_avg:57.47ms
step:1773/2000 train_time:101925ms step_avg:57.49ms
step:1774/2000 train_time:102013ms step_avg:57.50ms
step:1775/2000 train_time:102104ms step_avg:57.52ms
step:1776/2000 train_time:102193ms step_avg:57.54ms
step:1777/2000 train_time:102282ms step_avg:57.56ms
step:1778/2000 train_time:102371ms step_avg:57.58ms
step:1779/2000 train_time:102459ms step_avg:57.59ms
step:1780/2000 train_time:102546ms step_avg:57.61ms
step:1781/2000 train_time:102632ms step_avg:57.63ms
step:1782/2000 train_time:102719ms step_avg:57.64ms
step:1783/2000 train_time:102807ms step_avg:57.66ms
step:1784/2000 train_time:102894ms step_avg:57.68ms
step:1785/2000 train_time:102983ms step_avg:57.69ms
step:1786/2000 train_time:103071ms step_avg:57.71ms
step:1787/2000 train_time:103161ms step_avg:57.73ms
step:1788/2000 train_time:103249ms step_avg:57.75ms
step:1789/2000 train_time:103338ms step_avg:57.76ms
step:1790/2000 train_time:103425ms step_avg:57.78ms
step:1791/2000 train_time:103514ms step_avg:57.80ms
step:1792/2000 train_time:103602ms step_avg:57.81ms
step:1793/2000 train_time:103688ms step_avg:57.83ms
step:1794/2000 train_time:103775ms step_avg:57.85ms
step:1795/2000 train_time:103863ms step_avg:57.86ms
step:1796/2000 train_time:103951ms step_avg:57.88ms
step:1797/2000 train_time:104040ms step_avg:57.90ms
step:1798/2000 train_time:104129ms step_avg:57.91ms
step:1799/2000 train_time:104218ms step_avg:57.93ms
step:1800/2000 train_time:104306ms step_avg:57.95ms
step:1801/2000 train_time:104394ms step_avg:57.96ms
step:1802/2000 train_time:104482ms step_avg:57.98ms
step:1803/2000 train_time:104570ms step_avg:58.00ms
step:1804/2000 train_time:104658ms step_avg:58.01ms
step:1805/2000 train_time:104744ms step_avg:58.03ms
step:1806/2000 train_time:104832ms step_avg:58.05ms
step:1807/2000 train_time:104921ms step_avg:58.06ms
step:1808/2000 train_time:105010ms step_avg:58.08ms
step:1809/2000 train_time:105098ms step_avg:58.10ms
step:1810/2000 train_time:105187ms step_avg:58.11ms
step:1811/2000 train_time:105276ms step_avg:58.13ms
step:1812/2000 train_time:105364ms step_avg:58.15ms
step:1813/2000 train_time:105451ms step_avg:58.16ms
step:1814/2000 train_time:105540ms step_avg:58.18ms
step:1815/2000 train_time:105627ms step_avg:58.20ms
step:1816/2000 train_time:105715ms step_avg:58.21ms
step:1817/2000 train_time:105803ms step_avg:58.23ms
step:1818/2000 train_time:105891ms step_avg:58.25ms
step:1819/2000 train_time:105980ms step_avg:58.26ms
step:1820/2000 train_time:106067ms step_avg:58.28ms
step:1821/2000 train_time:106156ms step_avg:58.30ms
step:1822/2000 train_time:106244ms step_avg:58.31ms
step:1823/2000 train_time:106332ms step_avg:58.33ms
step:1824/2000 train_time:106419ms step_avg:58.34ms
step:1825/2000 train_time:106507ms step_avg:58.36ms
step:1826/2000 train_time:106595ms step_avg:58.38ms
step:1827/2000 train_time:106682ms step_avg:58.39ms
step:1828/2000 train_time:106771ms step_avg:58.41ms
step:1829/2000 train_time:106859ms step_avg:58.42ms
step:1830/2000 train_time:106946ms step_avg:58.44ms
step:1831/2000 train_time:107035ms step_avg:58.46ms
step:1832/2000 train_time:107122ms step_avg:58.47ms
step:1833/2000 train_time:107211ms step_avg:58.49ms
step:1834/2000 train_time:107300ms step_avg:58.51ms
step:1835/2000 train_time:107388ms step_avg:58.52ms
step:1836/2000 train_time:107475ms step_avg:58.54ms
step:1837/2000 train_time:107564ms step_avg:58.55ms
step:1838/2000 train_time:107651ms step_avg:58.57ms
step:1839/2000 train_time:107739ms step_avg:58.59ms
step:1840/2000 train_time:107826ms step_avg:58.60ms
step:1841/2000 train_time:107913ms step_avg:58.62ms
step:1842/2000 train_time:108001ms step_avg:58.63ms
step:1843/2000 train_time:108089ms step_avg:58.65ms
step:1844/2000 train_time:108177ms step_avg:58.66ms
step:1845/2000 train_time:108266ms step_avg:58.68ms
step:1846/2000 train_time:108354ms step_avg:58.70ms
step:1847/2000 train_time:108442ms step_avg:58.71ms
step:1848/2000 train_time:108530ms step_avg:58.73ms
step:1849/2000 train_time:108617ms step_avg:58.74ms
step:1850/2000 train_time:108705ms step_avg:58.76ms
step:1851/2000 train_time:108792ms step_avg:58.77ms
step:1852/2000 train_time:108880ms step_avg:58.79ms
step:1853/2000 train_time:108968ms step_avg:58.81ms
step:1854/2000 train_time:109056ms step_avg:58.82ms
step:1855/2000 train_time:109145ms step_avg:58.84ms
step:1856/2000 train_time:109234ms step_avg:58.85ms
step:1857/2000 train_time:109322ms step_avg:58.87ms
step:1858/2000 train_time:109411ms step_avg:58.89ms
step:1859/2000 train_time:109500ms step_avg:58.90ms
step:1860/2000 train_time:109588ms step_avg:58.92ms
step:1861/2000 train_time:109675ms step_avg:58.93ms
step:1862/2000 train_time:109763ms step_avg:58.95ms
step:1863/2000 train_time:109851ms step_avg:58.96ms
step:1864/2000 train_time:109939ms step_avg:58.98ms
step:1865/2000 train_time:110026ms step_avg:59.00ms
step:1866/2000 train_time:110114ms step_avg:59.01ms
step:1867/2000 train_time:110203ms step_avg:59.03ms
step:1868/2000 train_time:110290ms step_avg:59.04ms
step:1869/2000 train_time:110378ms step_avg:59.06ms
step:1870/2000 train_time:110466ms step_avg:59.07ms
step:1871/2000 train_time:110554ms step_avg:59.09ms
step:1872/2000 train_time:110642ms step_avg:59.10ms
step:1873/2000 train_time:110729ms step_avg:59.12ms
step:1874/2000 train_time:110817ms step_avg:59.13ms
step:1875/2000 train_time:110904ms step_avg:59.15ms
step:1876/2000 train_time:110992ms step_avg:59.16ms
step:1877/2000 train_time:111081ms step_avg:59.18ms
step:1878/2000 train_time:111168ms step_avg:59.19ms
step:1879/2000 train_time:111256ms step_avg:59.21ms
step:1880/2000 train_time:111344ms step_avg:59.23ms
step:1881/2000 train_time:111432ms step_avg:59.24ms
step:1882/2000 train_time:111521ms step_avg:59.26ms
step:1883/2000 train_time:111609ms step_avg:59.27ms
step:1884/2000 train_time:111697ms step_avg:59.29ms
step:1885/2000 train_time:111785ms step_avg:59.30ms
step:1886/2000 train_time:111873ms step_avg:59.32ms
step:1887/2000 train_time:111961ms step_avg:59.33ms
step:1888/2000 train_time:112049ms step_avg:59.35ms
step:1889/2000 train_time:112137ms step_avg:59.36ms
step:1890/2000 train_time:112224ms step_avg:59.38ms
step:1891/2000 train_time:112313ms step_avg:59.39ms
step:1892/2000 train_time:112401ms step_avg:59.41ms
step:1893/2000 train_time:112489ms step_avg:59.42ms
step:1894/2000 train_time:112577ms step_avg:59.44ms
step:1895/2000 train_time:112664ms step_avg:59.45ms
step:1896/2000 train_time:112751ms step_avg:59.47ms
step:1897/2000 train_time:112840ms step_avg:59.48ms
step:1898/2000 train_time:112928ms step_avg:59.50ms
step:1899/2000 train_time:113016ms step_avg:59.51ms
step:1900/2000 train_time:113104ms step_avg:59.53ms
step:1901/2000 train_time:113192ms step_avg:59.54ms
step:1902/2000 train_time:113280ms step_avg:59.56ms
step:1903/2000 train_time:113368ms step_avg:59.57ms
step:1904/2000 train_time:113457ms step_avg:59.59ms
step:1905/2000 train_time:113545ms step_avg:59.60ms
step:1906/2000 train_time:113633ms step_avg:59.62ms
step:1907/2000 train_time:113720ms step_avg:59.63ms
step:1908/2000 train_time:113807ms step_avg:59.65ms
step:1909/2000 train_time:113896ms step_avg:59.66ms
step:1910/2000 train_time:113984ms step_avg:59.68ms
step:1911/2000 train_time:114072ms step_avg:59.69ms
step:1912/2000 train_time:114159ms step_avg:59.71ms
step:1913/2000 train_time:114248ms step_avg:59.72ms
step:1914/2000 train_time:114335ms step_avg:59.74ms
step:1915/2000 train_time:114423ms step_avg:59.75ms
step:1916/2000 train_time:114511ms step_avg:59.77ms
step:1917/2000 train_time:114599ms step_avg:59.78ms
step:1918/2000 train_time:114687ms step_avg:59.80ms
step:1919/2000 train_time:114775ms step_avg:59.81ms
step:1920/2000 train_time:114863ms step_avg:59.82ms
step:1921/2000 train_time:114951ms step_avg:59.84ms
step:1922/2000 train_time:115039ms step_avg:59.85ms
step:1923/2000 train_time:115126ms step_avg:59.87ms
step:1924/2000 train_time:115214ms step_avg:59.88ms
step:1925/2000 train_time:115303ms step_avg:59.90ms
step:1926/2000 train_time:115391ms step_avg:59.91ms
step:1927/2000 train_time:115480ms step_avg:59.93ms
step:1928/2000 train_time:115569ms step_avg:59.94ms
step:1929/2000 train_time:115657ms step_avg:59.96ms
step:1930/2000 train_time:115744ms step_avg:59.97ms
step:1931/2000 train_time:115832ms step_avg:59.99ms
step:1932/2000 train_time:115920ms step_avg:60.00ms
step:1933/2000 train_time:116008ms step_avg:60.01ms
step:1934/2000 train_time:116096ms step_avg:60.03ms
step:1935/2000 train_time:116185ms step_avg:60.04ms
step:1936/2000 train_time:116274ms step_avg:60.06ms
step:1937/2000 train_time:116362ms step_avg:60.07ms
step:1938/2000 train_time:116451ms step_avg:60.09ms
step:1939/2000 train_time:116539ms step_avg:60.10ms
step:1940/2000 train_time:116626ms step_avg:60.12ms
step:1941/2000 train_time:116714ms step_avg:60.13ms
step:1942/2000 train_time:116803ms step_avg:60.15ms
step:1943/2000 train_time:116890ms step_avg:60.16ms
step:1944/2000 train_time:116978ms step_avg:60.17ms
step:1945/2000 train_time:117066ms step_avg:60.19ms
step:1946/2000 train_time:117154ms step_avg:60.20ms
step:1947/2000 train_time:117242ms step_avg:60.22ms
step:1948/2000 train_time:117330ms step_avg:60.23ms
step:1949/2000 train_time:117417ms step_avg:60.24ms
step:1950/2000 train_time:117506ms step_avg:60.26ms
step:1951/2000 train_time:117593ms step_avg:60.27ms
step:1952/2000 train_time:117681ms step_avg:60.29ms
step:1953/2000 train_time:117769ms step_avg:60.30ms
step:1954/2000 train_time:117857ms step_avg:60.32ms
step:1955/2000 train_time:117944ms step_avg:60.33ms
step:1956/2000 train_time:118033ms step_avg:60.34ms
step:1957/2000 train_time:118121ms step_avg:60.36ms
step:1958/2000 train_time:118210ms step_avg:60.37ms
step:1959/2000 train_time:118297ms step_avg:60.39ms
step:1960/2000 train_time:118385ms step_avg:60.40ms
step:1961/2000 train_time:118474ms step_avg:60.42ms
step:1962/2000 train_time:118561ms step_avg:60.43ms
step:1963/2000 train_time:118649ms step_avg:60.44ms
step:1964/2000 train_time:118738ms step_avg:60.46ms
step:1965/2000 train_time:118826ms step_avg:60.47ms
step:1966/2000 train_time:118914ms step_avg:60.49ms
step:1967/2000 train_time:119003ms step_avg:60.50ms
step:1968/2000 train_time:119091ms step_avg:60.51ms
step:1969/2000 train_time:119180ms step_avg:60.53ms
step:1970/2000 train_time:119267ms step_avg:60.54ms
step:1971/2000 train_time:119355ms step_avg:60.56ms
step:1972/2000 train_time:119443ms step_avg:60.57ms
step:1973/2000 train_time:119532ms step_avg:60.58ms
step:1974/2000 train_time:119621ms step_avg:60.60ms
step:1975/2000 train_time:119709ms step_avg:60.61ms
step:1976/2000 train_time:119797ms step_avg:60.63ms
step:1977/2000 train_time:119884ms step_avg:60.64ms
step:1978/2000 train_time:119972ms step_avg:60.65ms
step:1979/2000 train_time:120061ms step_avg:60.67ms
step:1980/2000 train_time:120149ms step_avg:60.68ms
step:1981/2000 train_time:120238ms step_avg:60.70ms
step:1982/2000 train_time:120325ms step_avg:60.71ms
step:1983/2000 train_time:120414ms step_avg:60.72ms
step:1984/2000 train_time:120503ms step_avg:60.74ms
step:1985/2000 train_time:120591ms step_avg:60.75ms
step:1986/2000 train_time:120680ms step_avg:60.77ms
step:1987/2000 train_time:120768ms step_avg:60.78ms
step:1988/2000 train_time:120856ms step_avg:60.79ms
step:1989/2000 train_time:120945ms step_avg:60.81ms
step:1990/2000 train_time:121032ms step_avg:60.82ms
step:1991/2000 train_time:121121ms step_avg:60.83ms
step:1992/2000 train_time:121208ms step_avg:60.85ms
step:1993/2000 train_time:121297ms step_avg:60.86ms
step:1994/2000 train_time:121384ms step_avg:60.87ms
step:1995/2000 train_time:121473ms step_avg:60.89ms
step:1996/2000 train_time:121562ms step_avg:60.90ms
step:1997/2000 train_time:121650ms step_avg:60.92ms
step:1998/2000 train_time:121739ms step_avg:60.93ms
step:1999/2000 train_time:121827ms step_avg:60.94ms
step:2000/2000 train_time:121915ms step_avg:60.96ms
step:2000/2000 val_loss:3.2784 train_time:122007ms step_avg:61.00ms
peak memory allocated: 29512 MiB reserved: 44396 MiB

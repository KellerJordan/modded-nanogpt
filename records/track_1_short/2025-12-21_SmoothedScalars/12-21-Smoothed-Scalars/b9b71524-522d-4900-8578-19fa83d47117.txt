import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            grad_slice = torch.empty_like(grad[:rank_size])
            self._reduce_scatter_futures[param] = (
                dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad_slice
            )



    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

    @torch.no_grad()
    def reset_momentum(self, params=None):
        """Reset momentum buffers for specified parameters (or all if 'None')"""
        if params is None:
            # Reset all parameters
            params_to_reset = [p for group in self.param_groups for p in group['params']]
        else:
            params_to_reset = list(params)
        
        for param in params_to_reset:
            if param in self.state:
                state = self.state[param]
                if 'exp_avg' in state:
                    state['exp_avg'].zero_()
                if 'exp_avg_sq' in state:
                    state['exp_avg_sq'].zero_()
                state['step'] = 0

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0
        
        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas  
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    # evaluation and logging
    logs_dir: str = f"logs/12-21-Smooth-Scalars-stps.1960.40"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes,lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    
    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps
    
    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0
    
    if in_transition:
        adam_optimizers[1].transition_steps -= 1
            
    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) 
        is_transition = True
            
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 18:07:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    756316      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    756317      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    756318      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    756319      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    756320      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    756321      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    756322      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    756323      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    756317      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    756318      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    756319      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    756320      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    756321      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    756322      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    756323      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2000 val_loss:10.8354 train_time:0ms step_avg:0.03ms
step:1/2000 train_time:79ms step_avg:78.68ms
step:2/2000 train_time:102ms step_avg:51.04ms
step:3/2000 train_time:125ms step_avg:41.83ms
step:4/2000 train_time:158ms step_avg:39.53ms
step:5/2000 train_time:191ms step_avg:38.24ms
step:6/2000 train_time:279ms step_avg:46.51ms
step:7/2000 train_time:296ms step_avg:42.34ms
step:8/2000 train_time:328ms step_avg:40.96ms
step:9/2000 train_time:360ms step_avg:40.05ms
step:10/2000 train_time:394ms step_avg:39.35ms
step:11/2000 train_time:427ms step_avg:38.80ms
step:12/2000 train_time:460ms step_avg:38.32ms
step:13/2000 train_time:493ms step_avg:37.95ms
step:14/2000 train_time:527ms step_avg:37.62ms
step:15/2000 train_time:560ms step_avg:37.31ms
step:16/2000 train_time:593ms step_avg:37.05ms
step:17/2000 train_time:626ms step_avg:36.83ms
step:18/2000 train_time:659ms step_avg:36.62ms
step:19/2000 train_time:692ms step_avg:36.44ms
step:20/2000 train_time:726ms step_avg:36.29ms
step:21/2000 train_time:759ms step_avg:36.13ms
step:22/2000 train_time:792ms step_avg:36.00ms
step:23/2000 train_time:825ms step_avg:35.87ms
step:24/2000 train_time:858ms step_avg:35.77ms
step:25/2000 train_time:892ms step_avg:35.67ms
step:26/2000 train_time:925ms step_avg:35.58ms
step:27/2000 train_time:958ms step_avg:35.48ms
step:28/2000 train_time:991ms step_avg:35.39ms
step:29/2000 train_time:1024ms step_avg:35.32ms
step:30/2000 train_time:1058ms step_avg:35.25ms
step:31/2000 train_time:1091ms step_avg:35.19ms
step:32/2000 train_time:1124ms step_avg:35.13ms
step:33/2000 train_time:1157ms step_avg:35.07ms
step:34/2000 train_time:1192ms step_avg:35.06ms
step:35/2000 train_time:1226ms step_avg:35.03ms
step:36/2000 train_time:1260ms step_avg:34.99ms
step:37/2000 train_time:1294ms step_avg:34.97ms
step:38/2000 train_time:1328ms step_avg:34.94ms
step:39/2000 train_time:1361ms step_avg:34.90ms
step:40/2000 train_time:1395ms step_avg:34.86ms
step:41/2000 train_time:1428ms step_avg:34.83ms
step:42/2000 train_time:1461ms step_avg:34.79ms
step:43/2000 train_time:1494ms step_avg:34.75ms
step:44/2000 train_time:1528ms step_avg:34.73ms
step:45/2000 train_time:1561ms step_avg:34.69ms
step:46/2000 train_time:1595ms step_avg:34.66ms
step:47/2000 train_time:1628ms step_avg:34.63ms
step:48/2000 train_time:1661ms step_avg:34.60ms
step:49/2000 train_time:1694ms step_avg:34.58ms
step:50/2000 train_time:1727ms step_avg:34.55ms
step:51/2000 train_time:1760ms step_avg:34.52ms
step:52/2000 train_time:1794ms step_avg:34.49ms
step:53/2000 train_time:1827ms step_avg:34.47ms
step:54/2000 train_time:1860ms step_avg:34.45ms
step:55/2000 train_time:1894ms step_avg:34.43ms
step:56/2000 train_time:1927ms step_avg:34.41ms
step:57/2000 train_time:1960ms step_avg:34.38ms
step:58/2000 train_time:1993ms step_avg:34.37ms
step:59/2000 train_time:2026ms step_avg:34.34ms
step:60/2000 train_time:2059ms step_avg:34.32ms
step:61/2000 train_time:2093ms step_avg:34.31ms
step:62/2000 train_time:2127ms step_avg:34.30ms
step:63/2000 train_time:2160ms step_avg:34.28ms
step:64/2000 train_time:2193ms step_avg:34.27ms
step:65/2000 train_time:2227ms step_avg:34.26ms
step:66/2000 train_time:2260ms step_avg:34.25ms
step:67/2000 train_time:2294ms step_avg:34.24ms
step:68/2000 train_time:2328ms step_avg:34.23ms
step:69/2000 train_time:2361ms step_avg:34.21ms
step:70/2000 train_time:2394ms step_avg:34.20ms
step:71/2000 train_time:2427ms step_avg:34.19ms
step:72/2000 train_time:2461ms step_avg:34.17ms
step:73/2000 train_time:2493ms step_avg:34.16ms
step:74/2000 train_time:2527ms step_avg:34.15ms
step:75/2000 train_time:2560ms step_avg:34.13ms
step:76/2000 train_time:2593ms step_avg:34.12ms
step:77/2000 train_time:2627ms step_avg:34.11ms
step:78/2000 train_time:2660ms step_avg:34.10ms
step:79/2000 train_time:2693ms step_avg:34.09ms
step:80/2000 train_time:2726ms step_avg:34.08ms
step:81/2000 train_time:2759ms step_avg:34.06ms
step:82/2000 train_time:2792ms step_avg:34.05ms
step:83/2000 train_time:2825ms step_avg:34.04ms
step:84/2000 train_time:2859ms step_avg:34.03ms
step:85/2000 train_time:2892ms step_avg:34.02ms
step:86/2000 train_time:2925ms step_avg:34.02ms
step:87/2000 train_time:2958ms step_avg:34.00ms
step:88/2000 train_time:2991ms step_avg:33.99ms
step:89/2000 train_time:3025ms step_avg:33.98ms
step:90/2000 train_time:3058ms step_avg:33.98ms
step:91/2000 train_time:3091ms step_avg:33.97ms
step:92/2000 train_time:3125ms step_avg:33.96ms
step:93/2000 train_time:3158ms step_avg:33.95ms
step:94/2000 train_time:3191ms step_avg:33.95ms
step:95/2000 train_time:3224ms step_avg:33.94ms
step:96/2000 train_time:3258ms step_avg:33.94ms
step:97/2000 train_time:3292ms step_avg:33.94ms
step:98/2000 train_time:3326ms step_avg:33.94ms
step:99/2000 train_time:3359ms step_avg:33.93ms
step:100/2000 train_time:3393ms step_avg:33.93ms
step:101/2000 train_time:3426ms step_avg:33.93ms
step:102/2000 train_time:3460ms step_avg:33.92ms
step:103/2000 train_time:3493ms step_avg:33.91ms
step:104/2000 train_time:3527ms step_avg:33.91ms
step:105/2000 train_time:3560ms step_avg:33.90ms
step:106/2000 train_time:3593ms step_avg:33.90ms
step:107/2000 train_time:3626ms step_avg:33.89ms
step:108/2000 train_time:3660ms step_avg:33.88ms
step:109/2000 train_time:3693ms step_avg:33.88ms
step:110/2000 train_time:3726ms step_avg:33.87ms
step:111/2000 train_time:3759ms step_avg:33.86ms
step:112/2000 train_time:3792ms step_avg:33.86ms
step:113/2000 train_time:3825ms step_avg:33.85ms
step:114/2000 train_time:3858ms step_avg:33.85ms
step:115/2000 train_time:3892ms step_avg:33.84ms
step:116/2000 train_time:3925ms step_avg:33.84ms
step:117/2000 train_time:3958ms step_avg:33.83ms
step:118/2000 train_time:3991ms step_avg:33.83ms
step:119/2000 train_time:4024ms step_avg:33.82ms
step:120/2000 train_time:4057ms step_avg:33.81ms
step:121/2000 train_time:4090ms step_avg:33.80ms
step:122/2000 train_time:4123ms step_avg:33.80ms
step:123/2000 train_time:4156ms step_avg:33.79ms
step:124/2000 train_time:4190ms step_avg:33.79ms
step:125/2000 train_time:4223ms step_avg:33.79ms
step:126/2000 train_time:4257ms step_avg:33.78ms
step:127/2000 train_time:4290ms step_avg:33.78ms
step:128/2000 train_time:4323ms step_avg:33.78ms
step:129/2000 train_time:4356ms step_avg:33.77ms
step:130/2000 train_time:4390ms step_avg:33.77ms
step:131/2000 train_time:4423ms step_avg:33.76ms
step:132/2000 train_time:4456ms step_avg:33.76ms
step:133/2000 train_time:4489ms step_avg:33.75ms
step:134/2000 train_time:4522ms step_avg:33.75ms
step:135/2000 train_time:4555ms step_avg:33.74ms
step:136/2000 train_time:4589ms step_avg:33.74ms
step:137/2000 train_time:4622ms step_avg:33.74ms
step:138/2000 train_time:4655ms step_avg:33.73ms
step:139/2000 train_time:4688ms step_avg:33.73ms
step:140/2000 train_time:4721ms step_avg:33.72ms
step:141/2000 train_time:4754ms step_avg:33.72ms
step:142/2000 train_time:4788ms step_avg:33.72ms
step:143/2000 train_time:4820ms step_avg:33.71ms
step:144/2000 train_time:4853ms step_avg:33.70ms
step:145/2000 train_time:4887ms step_avg:33.70ms
step:146/2000 train_time:4920ms step_avg:33.70ms
step:147/2000 train_time:4953ms step_avg:33.69ms
step:148/2000 train_time:4986ms step_avg:33.69ms
step:149/2000 train_time:5019ms step_avg:33.68ms
step:150/2000 train_time:5052ms step_avg:33.68ms
step:151/2000 train_time:5085ms step_avg:33.68ms
step:152/2000 train_time:5119ms step_avg:33.67ms
step:153/2000 train_time:5152ms step_avg:33.67ms
step:154/2000 train_time:5185ms step_avg:33.67ms
step:155/2000 train_time:5218ms step_avg:33.66ms
step:156/2000 train_time:5251ms step_avg:33.66ms
step:157/2000 train_time:5284ms step_avg:33.66ms
step:158/2000 train_time:5317ms step_avg:33.65ms
step:159/2000 train_time:5351ms step_avg:33.65ms
step:160/2000 train_time:5384ms step_avg:33.65ms
step:161/2000 train_time:5417ms step_avg:33.64ms
step:162/2000 train_time:5450ms step_avg:33.65ms
step:163/2000 train_time:5484ms step_avg:33.64ms
step:164/2000 train_time:5517ms step_avg:33.64ms
step:165/2000 train_time:5550ms step_avg:33.64ms
step:166/2000 train_time:5583ms step_avg:33.63ms
step:167/2000 train_time:5616ms step_avg:33.63ms
step:168/2000 train_time:5650ms step_avg:33.63ms
step:169/2000 train_time:5683ms step_avg:33.63ms
step:170/2000 train_time:5716ms step_avg:33.63ms
step:171/2000 train_time:5750ms step_avg:33.62ms
step:172/2000 train_time:5783ms step_avg:33.62ms
step:173/2000 train_time:5816ms step_avg:33.62ms
step:174/2000 train_time:5849ms step_avg:33.62ms
step:175/2000 train_time:5882ms step_avg:33.61ms
step:176/2000 train_time:5916ms step_avg:33.61ms
step:177/2000 train_time:5949ms step_avg:33.61ms
step:178/2000 train_time:5982ms step_avg:33.61ms
step:179/2000 train_time:6015ms step_avg:33.60ms
step:180/2000 train_time:6048ms step_avg:33.60ms
step:181/2000 train_time:6081ms step_avg:33.60ms
step:182/2000 train_time:6114ms step_avg:33.60ms
step:183/2000 train_time:6147ms step_avg:33.59ms
step:184/2000 train_time:6180ms step_avg:33.59ms
step:185/2000 train_time:6213ms step_avg:33.58ms
step:186/2000 train_time:6247ms step_avg:33.58ms
step:187/2000 train_time:6279ms step_avg:33.58ms
step:188/2000 train_time:6313ms step_avg:33.58ms
step:189/2000 train_time:6346ms step_avg:33.58ms
step:190/2000 train_time:6379ms step_avg:33.57ms
step:191/2000 train_time:6412ms step_avg:33.57ms
step:192/2000 train_time:6445ms step_avg:33.57ms
step:193/2000 train_time:6478ms step_avg:33.57ms
step:194/2000 train_time:6512ms step_avg:33.56ms
step:195/2000 train_time:6545ms step_avg:33.56ms
step:196/2000 train_time:6578ms step_avg:33.56ms
step:197/2000 train_time:6611ms step_avg:33.56ms
step:198/2000 train_time:6644ms step_avg:33.56ms
step:199/2000 train_time:6678ms step_avg:33.56ms
step:200/2000 train_time:6711ms step_avg:33.55ms
step:201/2000 train_time:6744ms step_avg:33.55ms
step:202/2000 train_time:6777ms step_avg:33.55ms
step:203/2000 train_time:6810ms step_avg:33.55ms
step:204/2000 train_time:6843ms step_avg:33.55ms
step:205/2000 train_time:6877ms step_avg:33.54ms
step:206/2000 train_time:6910ms step_avg:33.54ms
step:207/2000 train_time:6943ms step_avg:33.54ms
step:208/2000 train_time:6977ms step_avg:33.54ms
step:209/2000 train_time:7010ms step_avg:33.54ms
step:210/2000 train_time:7043ms step_avg:33.54ms
step:211/2000 train_time:7076ms step_avg:33.53ms
step:212/2000 train_time:7109ms step_avg:33.53ms
step:213/2000 train_time:7142ms step_avg:33.53ms
step:214/2000 train_time:7176ms step_avg:33.53ms
step:215/2000 train_time:7208ms step_avg:33.53ms
step:216/2000 train_time:7241ms step_avg:33.53ms
step:217/2000 train_time:7274ms step_avg:33.52ms
step:218/2000 train_time:7307ms step_avg:33.52ms
step:219/2000 train_time:7340ms step_avg:33.52ms
step:220/2000 train_time:7374ms step_avg:33.52ms
step:221/2000 train_time:7407ms step_avg:33.52ms
step:222/2000 train_time:7440ms step_avg:33.52ms
step:223/2000 train_time:7473ms step_avg:33.51ms
step:224/2000 train_time:7506ms step_avg:33.51ms
step:225/2000 train_time:7539ms step_avg:33.51ms
step:226/2000 train_time:7572ms step_avg:33.50ms
step:227/2000 train_time:7605ms step_avg:33.50ms
step:228/2000 train_time:7638ms step_avg:33.50ms
step:229/2000 train_time:7671ms step_avg:33.50ms
step:230/2000 train_time:7704ms step_avg:33.50ms
step:231/2000 train_time:7737ms step_avg:33.49ms
step:232/2000 train_time:7771ms step_avg:33.49ms
step:233/2000 train_time:7804ms step_avg:33.49ms
step:234/2000 train_time:7837ms step_avg:33.49ms
step:235/2000 train_time:7870ms step_avg:33.49ms
step:236/2000 train_time:7904ms step_avg:33.49ms
step:237/2000 train_time:7936ms step_avg:33.49ms
step:238/2000 train_time:7969ms step_avg:33.48ms
step:239/2000 train_time:8002ms step_avg:33.48ms
step:240/2000 train_time:8035ms step_avg:33.48ms
step:241/2000 train_time:8068ms step_avg:33.48ms
step:242/2000 train_time:8102ms step_avg:33.48ms
step:243/2000 train_time:8135ms step_avg:33.48ms
step:244/2000 train_time:8168ms step_avg:33.48ms
step:245/2000 train_time:8201ms step_avg:33.47ms
step:246/2000 train_time:8234ms step_avg:33.47ms
step:247/2000 train_time:8268ms step_avg:33.47ms
step:248/2000 train_time:8301ms step_avg:33.47ms
step:249/2000 train_time:8334ms step_avg:33.47ms
step:250/2000 train_time:8367ms step_avg:33.47ms
step:250/2000 val_loss:4.2693 train_time:8403ms step_avg:33.61ms
step:251/2000 train_time:8425ms step_avg:33.56ms
step:252/2000 train_time:8444ms step_avg:33.51ms
step:253/2000 train_time:8469ms step_avg:33.47ms
step:254/2000 train_time:8502ms step_avg:33.47ms
step:255/2000 train_time:8536ms step_avg:33.48ms
step:256/2000 train_time:8571ms step_avg:33.48ms
step:257/2000 train_time:8605ms step_avg:33.48ms
step:258/2000 train_time:8639ms step_avg:33.48ms
step:259/2000 train_time:8672ms step_avg:33.48ms
step:260/2000 train_time:8706ms step_avg:33.48ms
step:261/2000 train_time:8739ms step_avg:33.48ms
step:262/2000 train_time:8772ms step_avg:33.48ms
step:263/2000 train_time:8804ms step_avg:33.48ms
step:264/2000 train_time:8838ms step_avg:33.48ms
step:265/2000 train_time:8870ms step_avg:33.47ms
step:266/2000 train_time:8903ms step_avg:33.47ms
step:267/2000 train_time:8936ms step_avg:33.47ms
step:268/2000 train_time:8969ms step_avg:33.47ms
step:269/2000 train_time:9002ms step_avg:33.46ms
step:270/2000 train_time:9035ms step_avg:33.46ms
step:271/2000 train_time:9068ms step_avg:33.46ms
step:272/2000 train_time:9101ms step_avg:33.46ms
step:273/2000 train_time:9134ms step_avg:33.46ms
step:274/2000 train_time:9167ms step_avg:33.45ms
step:275/2000 train_time:9199ms step_avg:33.45ms
step:276/2000 train_time:9232ms step_avg:33.45ms
step:277/2000 train_time:9265ms step_avg:33.45ms
step:278/2000 train_time:9298ms step_avg:33.45ms
step:279/2000 train_time:9331ms step_avg:33.44ms
step:280/2000 train_time:9364ms step_avg:33.44ms
step:281/2000 train_time:9398ms step_avg:33.44ms
step:282/2000 train_time:9431ms step_avg:33.44ms
step:283/2000 train_time:9465ms step_avg:33.44ms
step:284/2000 train_time:9498ms step_avg:33.44ms
step:285/2000 train_time:9531ms step_avg:33.44ms
step:286/2000 train_time:9565ms step_avg:33.44ms
step:287/2000 train_time:9598ms step_avg:33.44ms
step:288/2000 train_time:9631ms step_avg:33.44ms
step:289/2000 train_time:9664ms step_avg:33.44ms
step:290/2000 train_time:9697ms step_avg:33.44ms
step:291/2000 train_time:9730ms step_avg:33.44ms
step:292/2000 train_time:9764ms step_avg:33.44ms
step:293/2000 train_time:9797ms step_avg:33.44ms
step:294/2000 train_time:9830ms step_avg:33.43ms
step:295/2000 train_time:9863ms step_avg:33.43ms
step:296/2000 train_time:9896ms step_avg:33.43ms
step:297/2000 train_time:9929ms step_avg:33.43ms
step:298/2000 train_time:9962ms step_avg:33.43ms
step:299/2000 train_time:9995ms step_avg:33.43ms
step:300/2000 train_time:10028ms step_avg:33.43ms
step:301/2000 train_time:10061ms step_avg:33.42ms
step:302/2000 train_time:10094ms step_avg:33.42ms
step:303/2000 train_time:10127ms step_avg:33.42ms
step:304/2000 train_time:10160ms step_avg:33.42ms
step:305/2000 train_time:10193ms step_avg:33.42ms
step:306/2000 train_time:10226ms step_avg:33.42ms
step:307/2000 train_time:10259ms step_avg:33.42ms
step:308/2000 train_time:10292ms step_avg:33.42ms
step:309/2000 train_time:10325ms step_avg:33.42ms
step:310/2000 train_time:10359ms step_avg:33.42ms
step:311/2000 train_time:10392ms step_avg:33.41ms
step:312/2000 train_time:10425ms step_avg:33.41ms
step:313/2000 train_time:10458ms step_avg:33.41ms
step:314/2000 train_time:10492ms step_avg:33.41ms
step:315/2000 train_time:10525ms step_avg:33.41ms
step:316/2000 train_time:10558ms step_avg:33.41ms
step:317/2000 train_time:10591ms step_avg:33.41ms
step:318/2000 train_time:10625ms step_avg:33.41ms
step:319/2000 train_time:10658ms step_avg:33.41ms
step:320/2000 train_time:10691ms step_avg:33.41ms
step:321/2000 train_time:10724ms step_avg:33.41ms
step:322/2000 train_time:10757ms step_avg:33.41ms
step:323/2000 train_time:10791ms step_avg:33.41ms
step:324/2000 train_time:10824ms step_avg:33.41ms
step:325/2000 train_time:10857ms step_avg:33.41ms
step:326/2000 train_time:10890ms step_avg:33.40ms
step:327/2000 train_time:10923ms step_avg:33.40ms
step:328/2000 train_time:10956ms step_avg:33.40ms
step:329/2000 train_time:10989ms step_avg:33.40ms
step:330/2000 train_time:11022ms step_avg:33.40ms
step:331/2000 train_time:11055ms step_avg:33.40ms
step:332/2000 train_time:11088ms step_avg:33.40ms
step:333/2000 train_time:11121ms step_avg:33.40ms
step:334/2000 train_time:11155ms step_avg:33.40ms
step:335/2000 train_time:11187ms step_avg:33.40ms
step:336/2000 train_time:11221ms step_avg:33.39ms
step:337/2000 train_time:11253ms step_avg:33.39ms
step:338/2000 train_time:11286ms step_avg:33.39ms
step:339/2000 train_time:11319ms step_avg:33.39ms
step:340/2000 train_time:11353ms step_avg:33.39ms
step:341/2000 train_time:11386ms step_avg:33.39ms
step:342/2000 train_time:11419ms step_avg:33.39ms
step:343/2000 train_time:11452ms step_avg:33.39ms
step:344/2000 train_time:11485ms step_avg:33.39ms
step:345/2000 train_time:11518ms step_avg:33.38ms
step:346/2000 train_time:11551ms step_avg:33.38ms
step:347/2000 train_time:11584ms step_avg:33.38ms
step:348/2000 train_time:11617ms step_avg:33.38ms
step:349/2000 train_time:11650ms step_avg:33.38ms
step:350/2000 train_time:11683ms step_avg:33.38ms
step:351/2000 train_time:11717ms step_avg:33.38ms
step:352/2000 train_time:11750ms step_avg:33.38ms
step:353/2000 train_time:11783ms step_avg:33.38ms
step:354/2000 train_time:11817ms step_avg:33.38ms
step:355/2000 train_time:11850ms step_avg:33.38ms
step:356/2000 train_time:11883ms step_avg:33.38ms
step:357/2000 train_time:11916ms step_avg:33.38ms
step:358/2000 train_time:11949ms step_avg:33.38ms
step:359/2000 train_time:11982ms step_avg:33.38ms
step:360/2000 train_time:12015ms step_avg:33.38ms
step:361/2000 train_time:12048ms step_avg:33.37ms
step:362/2000 train_time:12082ms step_avg:33.37ms
step:363/2000 train_time:12114ms step_avg:33.37ms
step:364/2000 train_time:12148ms step_avg:33.37ms
step:365/2000 train_time:12181ms step_avg:33.37ms
step:366/2000 train_time:12214ms step_avg:33.37ms
step:367/2000 train_time:12247ms step_avg:33.37ms
step:368/2000 train_time:12280ms step_avg:33.37ms
step:369/2000 train_time:12313ms step_avg:33.37ms
step:370/2000 train_time:12346ms step_avg:33.37ms
step:371/2000 train_time:12379ms step_avg:33.37ms
step:372/2000 train_time:12413ms step_avg:33.37ms
step:373/2000 train_time:12446ms step_avg:33.37ms
step:374/2000 train_time:12479ms step_avg:33.37ms
step:375/2000 train_time:12513ms step_avg:33.37ms
step:376/2000 train_time:12546ms step_avg:33.37ms
step:377/2000 train_time:12578ms step_avg:33.36ms
step:378/2000 train_time:12612ms step_avg:33.36ms
step:379/2000 train_time:12645ms step_avg:33.36ms
step:380/2000 train_time:12678ms step_avg:33.36ms
step:381/2000 train_time:12712ms step_avg:33.36ms
step:382/2000 train_time:12745ms step_avg:33.36ms
step:383/2000 train_time:12778ms step_avg:33.36ms
step:384/2000 train_time:12811ms step_avg:33.36ms
step:385/2000 train_time:12844ms step_avg:33.36ms
step:386/2000 train_time:12878ms step_avg:33.36ms
step:387/2000 train_time:12911ms step_avg:33.36ms
step:388/2000 train_time:12944ms step_avg:33.36ms
step:389/2000 train_time:12976ms step_avg:33.36ms
step:390/2000 train_time:13010ms step_avg:33.36ms
step:391/2000 train_time:13043ms step_avg:33.36ms
step:392/2000 train_time:13076ms step_avg:33.36ms
step:393/2000 train_time:13109ms step_avg:33.36ms
step:394/2000 train_time:13142ms step_avg:33.36ms
step:395/2000 train_time:13175ms step_avg:33.35ms
step:396/2000 train_time:13208ms step_avg:33.35ms
step:397/2000 train_time:13241ms step_avg:33.35ms
step:398/2000 train_time:13274ms step_avg:33.35ms
step:399/2000 train_time:13307ms step_avg:33.35ms
step:400/2000 train_time:13340ms step_avg:33.35ms
step:401/2000 train_time:13373ms step_avg:33.35ms
step:402/2000 train_time:13407ms step_avg:33.35ms
step:403/2000 train_time:13439ms step_avg:33.35ms
step:404/2000 train_time:13473ms step_avg:33.35ms
step:405/2000 train_time:13505ms step_avg:33.35ms
step:406/2000 train_time:13539ms step_avg:33.35ms
step:407/2000 train_time:13572ms step_avg:33.35ms
step:408/2000 train_time:13605ms step_avg:33.35ms
step:409/2000 train_time:13638ms step_avg:33.35ms
step:410/2000 train_time:13672ms step_avg:33.35ms
step:411/2000 train_time:13705ms step_avg:33.34ms
step:412/2000 train_time:13738ms step_avg:33.34ms
step:413/2000 train_time:13771ms step_avg:33.34ms
step:414/2000 train_time:13805ms step_avg:33.34ms
step:415/2000 train_time:13838ms step_avg:33.34ms
step:416/2000 train_time:13871ms step_avg:33.34ms
step:417/2000 train_time:13904ms step_avg:33.34ms
step:418/2000 train_time:13938ms step_avg:33.34ms
step:419/2000 train_time:13971ms step_avg:33.34ms
step:420/2000 train_time:14004ms step_avg:33.34ms
step:421/2000 train_time:14037ms step_avg:33.34ms
step:422/2000 train_time:14070ms step_avg:33.34ms
step:423/2000 train_time:14103ms step_avg:33.34ms
step:424/2000 train_time:14137ms step_avg:33.34ms
step:425/2000 train_time:14170ms step_avg:33.34ms
step:426/2000 train_time:14203ms step_avg:33.34ms
step:427/2000 train_time:14236ms step_avg:33.34ms
step:428/2000 train_time:14269ms step_avg:33.34ms
step:429/2000 train_time:14302ms step_avg:33.34ms
step:430/2000 train_time:14336ms step_avg:33.34ms
step:431/2000 train_time:14369ms step_avg:33.34ms
step:432/2000 train_time:14402ms step_avg:33.34ms
step:433/2000 train_time:14435ms step_avg:33.34ms
step:434/2000 train_time:14468ms step_avg:33.34ms
step:435/2000 train_time:14501ms step_avg:33.34ms
step:436/2000 train_time:14535ms step_avg:33.34ms
step:437/2000 train_time:14567ms step_avg:33.34ms
step:438/2000 train_time:14601ms step_avg:33.33ms
step:439/2000 train_time:14634ms step_avg:33.33ms
step:440/2000 train_time:14667ms step_avg:33.33ms
step:441/2000 train_time:14700ms step_avg:33.33ms
step:442/2000 train_time:14733ms step_avg:33.33ms
step:443/2000 train_time:14767ms step_avg:33.33ms
step:444/2000 train_time:14800ms step_avg:33.33ms
step:445/2000 train_time:14833ms step_avg:33.33ms
step:446/2000 train_time:14866ms step_avg:33.33ms
step:447/2000 train_time:14899ms step_avg:33.33ms
step:448/2000 train_time:14933ms step_avg:33.33ms
step:449/2000 train_time:14966ms step_avg:33.33ms
step:450/2000 train_time:14999ms step_avg:33.33ms
step:451/2000 train_time:15032ms step_avg:33.33ms
step:452/2000 train_time:15066ms step_avg:33.33ms
step:453/2000 train_time:15099ms step_avg:33.33ms
step:454/2000 train_time:15132ms step_avg:33.33ms
step:455/2000 train_time:15166ms step_avg:33.33ms
step:456/2000 train_time:15199ms step_avg:33.33ms
step:457/2000 train_time:15232ms step_avg:33.33ms
step:458/2000 train_time:15265ms step_avg:33.33ms
step:459/2000 train_time:15299ms step_avg:33.33ms
step:460/2000 train_time:15332ms step_avg:33.33ms
step:461/2000 train_time:15365ms step_avg:33.33ms
step:462/2000 train_time:15399ms step_avg:33.33ms
step:463/2000 train_time:15432ms step_avg:33.33ms
step:464/2000 train_time:15465ms step_avg:33.33ms
step:465/2000 train_time:15498ms step_avg:33.33ms
step:466/2000 train_time:15532ms step_avg:33.33ms
step:467/2000 train_time:15565ms step_avg:33.33ms
step:468/2000 train_time:15598ms step_avg:33.33ms
step:469/2000 train_time:15631ms step_avg:33.33ms
step:470/2000 train_time:15664ms step_avg:33.33ms
step:471/2000 train_time:15697ms step_avg:33.33ms
step:472/2000 train_time:15731ms step_avg:33.33ms
step:473/2000 train_time:15764ms step_avg:33.33ms
step:474/2000 train_time:15798ms step_avg:33.33ms
step:475/2000 train_time:15830ms step_avg:33.33ms
step:476/2000 train_time:15863ms step_avg:33.33ms
step:477/2000 train_time:15896ms step_avg:33.33ms
step:478/2000 train_time:15930ms step_avg:33.33ms
step:479/2000 train_time:15963ms step_avg:33.33ms
step:480/2000 train_time:15996ms step_avg:33.33ms
step:481/2000 train_time:16029ms step_avg:33.33ms
step:482/2000 train_time:16063ms step_avg:33.33ms
step:483/2000 train_time:16096ms step_avg:33.33ms
step:484/2000 train_time:16129ms step_avg:33.33ms
step:485/2000 train_time:16162ms step_avg:33.32ms
step:486/2000 train_time:16195ms step_avg:33.32ms
step:487/2000 train_time:16229ms step_avg:33.32ms
step:488/2000 train_time:16262ms step_avg:33.32ms
step:489/2000 train_time:16295ms step_avg:33.32ms
step:490/2000 train_time:16328ms step_avg:33.32ms
step:491/2000 train_time:16361ms step_avg:33.32ms
step:492/2000 train_time:16395ms step_avg:33.32ms
step:493/2000 train_time:16428ms step_avg:33.32ms
step:494/2000 train_time:16461ms step_avg:33.32ms
step:495/2000 train_time:16494ms step_avg:33.32ms
step:496/2000 train_time:16527ms step_avg:33.32ms
step:497/2000 train_time:16559ms step_avg:33.32ms
step:498/2000 train_time:16593ms step_avg:33.32ms
step:499/2000 train_time:16626ms step_avg:33.32ms
step:500/2000 train_time:16660ms step_avg:33.32ms
step:500/2000 val_loss:3.9964 train_time:16696ms step_avg:33.39ms
step:501/2000 train_time:16714ms step_avg:33.36ms
step:502/2000 train_time:16733ms step_avg:33.33ms
step:503/2000 train_time:16762ms step_avg:33.32ms
step:504/2000 train_time:16796ms step_avg:33.32ms
step:505/2000 train_time:16831ms step_avg:33.33ms
step:506/2000 train_time:16866ms step_avg:33.33ms
step:507/2000 train_time:16900ms step_avg:33.33ms
step:508/2000 train_time:16934ms step_avg:33.34ms
step:509/2000 train_time:16967ms step_avg:33.33ms
step:510/2000 train_time:17001ms step_avg:33.33ms
step:511/2000 train_time:17034ms step_avg:33.33ms
step:512/2000 train_time:17067ms step_avg:33.33ms
step:513/2000 train_time:17100ms step_avg:33.33ms
step:514/2000 train_time:17133ms step_avg:33.33ms
step:515/2000 train_time:17166ms step_avg:33.33ms
step:516/2000 train_time:17199ms step_avg:33.33ms
step:517/2000 train_time:17232ms step_avg:33.33ms
step:518/2000 train_time:17265ms step_avg:33.33ms
step:519/2000 train_time:17297ms step_avg:33.33ms
step:520/2000 train_time:17330ms step_avg:33.33ms
step:521/2000 train_time:17363ms step_avg:33.33ms
step:522/2000 train_time:17396ms step_avg:33.33ms
step:523/2000 train_time:17429ms step_avg:33.33ms
step:524/2000 train_time:17462ms step_avg:33.32ms
step:525/2000 train_time:17495ms step_avg:33.32ms
step:526/2000 train_time:17528ms step_avg:33.32ms
step:527/2000 train_time:17561ms step_avg:33.32ms
step:528/2000 train_time:17594ms step_avg:33.32ms
step:529/2000 train_time:17627ms step_avg:33.32ms
step:530/2000 train_time:17660ms step_avg:33.32ms
step:531/2000 train_time:17693ms step_avg:33.32ms
step:532/2000 train_time:17727ms step_avg:33.32ms
step:533/2000 train_time:17760ms step_avg:33.32ms
step:534/2000 train_time:17794ms step_avg:33.32ms
step:535/2000 train_time:17828ms step_avg:33.32ms
step:536/2000 train_time:17861ms step_avg:33.32ms
step:537/2000 train_time:17895ms step_avg:33.32ms
step:538/2000 train_time:17929ms step_avg:33.32ms
step:539/2000 train_time:17962ms step_avg:33.32ms
step:540/2000 train_time:17995ms step_avg:33.32ms
step:541/2000 train_time:18028ms step_avg:33.32ms
step:542/2000 train_time:18062ms step_avg:33.32ms
step:543/2000 train_time:18095ms step_avg:33.32ms
step:544/2000 train_time:18128ms step_avg:33.32ms
step:545/2000 train_time:18161ms step_avg:33.32ms
step:546/2000 train_time:18194ms step_avg:33.32ms
step:547/2000 train_time:18228ms step_avg:33.32ms
step:548/2000 train_time:18261ms step_avg:33.32ms
step:549/2000 train_time:18293ms step_avg:33.32ms
step:550/2000 train_time:18327ms step_avg:33.32ms
step:551/2000 train_time:18359ms step_avg:33.32ms
step:552/2000 train_time:18392ms step_avg:33.32ms
step:553/2000 train_time:18425ms step_avg:33.32ms
step:554/2000 train_time:18458ms step_avg:33.32ms
step:555/2000 train_time:18491ms step_avg:33.32ms
step:556/2000 train_time:18524ms step_avg:33.32ms
step:557/2000 train_time:18557ms step_avg:33.32ms
step:558/2000 train_time:18591ms step_avg:33.32ms
step:559/2000 train_time:18624ms step_avg:33.32ms
step:560/2000 train_time:18657ms step_avg:33.32ms
step:561/2000 train_time:18690ms step_avg:33.32ms
step:562/2000 train_time:18724ms step_avg:33.32ms
step:563/2000 train_time:18757ms step_avg:33.32ms
step:564/2000 train_time:18791ms step_avg:33.32ms
step:565/2000 train_time:18824ms step_avg:33.32ms
step:566/2000 train_time:18858ms step_avg:33.32ms
step:567/2000 train_time:18891ms step_avg:33.32ms
step:568/2000 train_time:18924ms step_avg:33.32ms
step:569/2000 train_time:18958ms step_avg:33.32ms
step:570/2000 train_time:18991ms step_avg:33.32ms
step:571/2000 train_time:19024ms step_avg:33.32ms
step:572/2000 train_time:19057ms step_avg:33.32ms
step:573/2000 train_time:19090ms step_avg:33.32ms
step:574/2000 train_time:19124ms step_avg:33.32ms
step:575/2000 train_time:19157ms step_avg:33.32ms
step:576/2000 train_time:19190ms step_avg:33.32ms
step:577/2000 train_time:19224ms step_avg:33.32ms
step:578/2000 train_time:19257ms step_avg:33.32ms
step:579/2000 train_time:19290ms step_avg:33.32ms
step:580/2000 train_time:19323ms step_avg:33.32ms
step:581/2000 train_time:19356ms step_avg:33.31ms
step:582/2000 train_time:19389ms step_avg:33.31ms
step:583/2000 train_time:19422ms step_avg:33.31ms
step:584/2000 train_time:19455ms step_avg:33.31ms
step:585/2000 train_time:19488ms step_avg:33.31ms
step:586/2000 train_time:19521ms step_avg:33.31ms
step:587/2000 train_time:19554ms step_avg:33.31ms
step:588/2000 train_time:19588ms step_avg:33.31ms
step:589/2000 train_time:19620ms step_avg:33.31ms
step:590/2000 train_time:19655ms step_avg:33.31ms
step:591/2000 train_time:19688ms step_avg:33.31ms
step:592/2000 train_time:19721ms step_avg:33.31ms
step:593/2000 train_time:19754ms step_avg:33.31ms
step:594/2000 train_time:19788ms step_avg:33.31ms
step:595/2000 train_time:19821ms step_avg:33.31ms
step:596/2000 train_time:19854ms step_avg:33.31ms
step:597/2000 train_time:19888ms step_avg:33.31ms
step:598/2000 train_time:19921ms step_avg:33.31ms
step:599/2000 train_time:19954ms step_avg:33.31ms
step:600/2000 train_time:19988ms step_avg:33.31ms
step:601/2000 train_time:20021ms step_avg:33.31ms
step:602/2000 train_time:20054ms step_avg:33.31ms
step:603/2000 train_time:20087ms step_avg:33.31ms
step:604/2000 train_time:20121ms step_avg:33.31ms
step:605/2000 train_time:20154ms step_avg:33.31ms
step:606/2000 train_time:20188ms step_avg:33.31ms
step:607/2000 train_time:20221ms step_avg:33.31ms
step:608/2000 train_time:20254ms step_avg:33.31ms
step:609/2000 train_time:20287ms step_avg:33.31ms
step:610/2000 train_time:20320ms step_avg:33.31ms
step:611/2000 train_time:20353ms step_avg:33.31ms
step:612/2000 train_time:20386ms step_avg:33.31ms
step:613/2000 train_time:20419ms step_avg:33.31ms
step:614/2000 train_time:20453ms step_avg:33.31ms
step:615/2000 train_time:20486ms step_avg:33.31ms
step:616/2000 train_time:20519ms step_avg:33.31ms
step:617/2000 train_time:20552ms step_avg:33.31ms
step:618/2000 train_time:20585ms step_avg:33.31ms
step:619/2000 train_time:20618ms step_avg:33.31ms
step:620/2000 train_time:20651ms step_avg:33.31ms
step:621/2000 train_time:20684ms step_avg:33.31ms
step:622/2000 train_time:20717ms step_avg:33.31ms
step:623/2000 train_time:20750ms step_avg:33.31ms
step:624/2000 train_time:20784ms step_avg:33.31ms
step:625/2000 train_time:20817ms step_avg:33.31ms
step:626/2000 train_time:20851ms step_avg:33.31ms
step:627/2000 train_time:20884ms step_avg:33.31ms
step:628/2000 train_time:20918ms step_avg:33.31ms
step:629/2000 train_time:20951ms step_avg:33.31ms
step:630/2000 train_time:20984ms step_avg:33.31ms
step:631/2000 train_time:21017ms step_avg:33.31ms
step:632/2000 train_time:21050ms step_avg:33.31ms
step:633/2000 train_time:21083ms step_avg:33.31ms
step:634/2000 train_time:21117ms step_avg:33.31ms
step:635/2000 train_time:21150ms step_avg:33.31ms
step:636/2000 train_time:21183ms step_avg:33.31ms
step:637/2000 train_time:21216ms step_avg:33.31ms
step:638/2000 train_time:21250ms step_avg:33.31ms
step:639/2000 train_time:21282ms step_avg:33.31ms
step:640/2000 train_time:21315ms step_avg:33.31ms
step:641/2000 train_time:21349ms step_avg:33.31ms
step:642/2000 train_time:21382ms step_avg:33.31ms
step:643/2000 train_time:21415ms step_avg:33.31ms
step:644/2000 train_time:21449ms step_avg:33.31ms
step:645/2000 train_time:21482ms step_avg:33.31ms
step:646/2000 train_time:21515ms step_avg:33.31ms
step:647/2000 train_time:21548ms step_avg:33.30ms
step:648/2000 train_time:21581ms step_avg:33.30ms
step:649/2000 train_time:21614ms step_avg:33.30ms
step:650/2000 train_time:21647ms step_avg:33.30ms
step:651/2000 train_time:21680ms step_avg:33.30ms
step:652/2000 train_time:21713ms step_avg:33.30ms
step:653/2000 train_time:21746ms step_avg:33.30ms
step:654/2000 train_time:21779ms step_avg:33.30ms
step:655/2000 train_time:21814ms step_avg:33.30ms
step:656/2000 train_time:21873ms step_avg:33.34ms
step:657/2000 train_time:21933ms step_avg:33.38ms
step:658/2000 train_time:21992ms step_avg:33.42ms
step:659/2000 train_time:22052ms step_avg:33.46ms
step:660/2000 train_time:22112ms step_avg:33.50ms
step:661/2000 train_time:22173ms step_avg:33.54ms
step:662/2000 train_time:22232ms step_avg:33.58ms
step:663/2000 train_time:22292ms step_avg:33.62ms
step:664/2000 train_time:22352ms step_avg:33.66ms
step:665/2000 train_time:22413ms step_avg:33.70ms
step:666/2000 train_time:22472ms step_avg:33.74ms
step:667/2000 train_time:22532ms step_avg:33.78ms
step:668/2000 train_time:22592ms step_avg:33.82ms
step:669/2000 train_time:22652ms step_avg:33.86ms
step:670/2000 train_time:22712ms step_avg:33.90ms
step:671/2000 train_time:22772ms step_avg:33.94ms
step:672/2000 train_time:22832ms step_avg:33.98ms
step:673/2000 train_time:22892ms step_avg:34.01ms
step:674/2000 train_time:22951ms step_avg:34.05ms
step:675/2000 train_time:23012ms step_avg:34.09ms
step:676/2000 train_time:23071ms step_avg:34.13ms
step:677/2000 train_time:23132ms step_avg:34.17ms
step:678/2000 train_time:23191ms step_avg:34.20ms
step:679/2000 train_time:23252ms step_avg:34.25ms
step:680/2000 train_time:23313ms step_avg:34.28ms
step:681/2000 train_time:23373ms step_avg:34.32ms
step:682/2000 train_time:23433ms step_avg:34.36ms
step:683/2000 train_time:23493ms step_avg:34.40ms
step:684/2000 train_time:23552ms step_avg:34.43ms
step:685/2000 train_time:23614ms step_avg:34.47ms
step:686/2000 train_time:23673ms step_avg:34.51ms
step:687/2000 train_time:23733ms step_avg:34.55ms
step:688/2000 train_time:23791ms step_avg:34.58ms
step:689/2000 train_time:23851ms step_avg:34.62ms
step:690/2000 train_time:23911ms step_avg:34.65ms
step:691/2000 train_time:23971ms step_avg:34.69ms
step:692/2000 train_time:24030ms step_avg:34.73ms
step:693/2000 train_time:24091ms step_avg:34.76ms
step:694/2000 train_time:24150ms step_avg:34.80ms
step:695/2000 train_time:24212ms step_avg:34.84ms
step:696/2000 train_time:24272ms step_avg:34.87ms
step:697/2000 train_time:24332ms step_avg:34.91ms
step:698/2000 train_time:24391ms step_avg:34.94ms
step:699/2000 train_time:24451ms step_avg:34.98ms
step:700/2000 train_time:24511ms step_avg:35.02ms
step:701/2000 train_time:24573ms step_avg:35.05ms
step:702/2000 train_time:24632ms step_avg:35.09ms
step:703/2000 train_time:24692ms step_avg:35.12ms
step:704/2000 train_time:24751ms step_avg:35.16ms
step:705/2000 train_time:24812ms step_avg:35.19ms
step:706/2000 train_time:24871ms step_avg:35.23ms
step:707/2000 train_time:24932ms step_avg:35.26ms
step:708/2000 train_time:24991ms step_avg:35.30ms
step:709/2000 train_time:25051ms step_avg:35.33ms
step:710/2000 train_time:25111ms step_avg:35.37ms
step:711/2000 train_time:25172ms step_avg:35.40ms
step:712/2000 train_time:25232ms step_avg:35.44ms
step:713/2000 train_time:25293ms step_avg:35.47ms
step:714/2000 train_time:25353ms step_avg:35.51ms
step:715/2000 train_time:25413ms step_avg:35.54ms
step:716/2000 train_time:25472ms step_avg:35.58ms
step:717/2000 train_time:25533ms step_avg:35.61ms
step:718/2000 train_time:25592ms step_avg:35.64ms
step:719/2000 train_time:25653ms step_avg:35.68ms
step:720/2000 train_time:25714ms step_avg:35.71ms
step:721/2000 train_time:25773ms step_avg:35.75ms
step:722/2000 train_time:25832ms step_avg:35.78ms
step:723/2000 train_time:25893ms step_avg:35.81ms
step:724/2000 train_time:25952ms step_avg:35.85ms
step:725/2000 train_time:26012ms step_avg:35.88ms
step:726/2000 train_time:26072ms step_avg:35.91ms
step:727/2000 train_time:26132ms step_avg:35.95ms
step:728/2000 train_time:26192ms step_avg:35.98ms
step:729/2000 train_time:26252ms step_avg:36.01ms
step:730/2000 train_time:26312ms step_avg:36.04ms
step:731/2000 train_time:26373ms step_avg:36.08ms
step:732/2000 train_time:26433ms step_avg:36.11ms
step:733/2000 train_time:26493ms step_avg:36.14ms
step:734/2000 train_time:26554ms step_avg:36.18ms
step:735/2000 train_time:26614ms step_avg:36.21ms
step:736/2000 train_time:26673ms step_avg:36.24ms
step:737/2000 train_time:26734ms step_avg:36.27ms
step:738/2000 train_time:26793ms step_avg:36.31ms
step:739/2000 train_time:26854ms step_avg:36.34ms
step:740/2000 train_time:26913ms step_avg:36.37ms
step:741/2000 train_time:26974ms step_avg:36.40ms
step:742/2000 train_time:27033ms step_avg:36.43ms
step:743/2000 train_time:27093ms step_avg:36.46ms
step:744/2000 train_time:27153ms step_avg:36.50ms
step:745/2000 train_time:27213ms step_avg:36.53ms
step:746/2000 train_time:27273ms step_avg:36.56ms
step:747/2000 train_time:27334ms step_avg:36.59ms
step:748/2000 train_time:27393ms step_avg:36.62ms
step:749/2000 train_time:27454ms step_avg:36.65ms
step:750/2000 train_time:27514ms step_avg:36.68ms
step:750/2000 val_loss:3.8171 train_time:27577ms step_avg:36.77ms
step:751/2000 train_time:27597ms step_avg:36.75ms
step:752/2000 train_time:27636ms step_avg:36.75ms
step:753/2000 train_time:27699ms step_avg:36.78ms
step:754/2000 train_time:27760ms step_avg:36.82ms
step:755/2000 train_time:27821ms step_avg:36.85ms
step:756/2000 train_time:27882ms step_avg:36.88ms
step:757/2000 train_time:27942ms step_avg:36.91ms
step:758/2000 train_time:28001ms step_avg:36.94ms
step:759/2000 train_time:28061ms step_avg:36.97ms
step:760/2000 train_time:28120ms step_avg:37.00ms
step:761/2000 train_time:28180ms step_avg:37.03ms
step:762/2000 train_time:28239ms step_avg:37.06ms
step:763/2000 train_time:28299ms step_avg:37.09ms
step:764/2000 train_time:28359ms step_avg:37.12ms
step:765/2000 train_time:28419ms step_avg:37.15ms
step:766/2000 train_time:28479ms step_avg:37.18ms
step:767/2000 train_time:28542ms step_avg:37.21ms
step:768/2000 train_time:28604ms step_avg:37.24ms
step:769/2000 train_time:28667ms step_avg:37.28ms
step:770/2000 train_time:28728ms step_avg:37.31ms
step:771/2000 train_time:28789ms step_avg:37.34ms
step:772/2000 train_time:28849ms step_avg:37.37ms
step:773/2000 train_time:28909ms step_avg:37.40ms
step:774/2000 train_time:28969ms step_avg:37.43ms
step:775/2000 train_time:29029ms step_avg:37.46ms
step:776/2000 train_time:29088ms step_avg:37.48ms
step:777/2000 train_time:29149ms step_avg:37.51ms
step:778/2000 train_time:29208ms step_avg:37.54ms
step:779/2000 train_time:29268ms step_avg:37.57ms
step:780/2000 train_time:29327ms step_avg:37.60ms
step:781/2000 train_time:29388ms step_avg:37.63ms
step:782/2000 train_time:29447ms step_avg:37.66ms
step:783/2000 train_time:29509ms step_avg:37.69ms
step:784/2000 train_time:29569ms step_avg:37.72ms
step:785/2000 train_time:29631ms step_avg:37.75ms
step:786/2000 train_time:29691ms step_avg:37.77ms
step:787/2000 train_time:29751ms step_avg:37.80ms
step:788/2000 train_time:29812ms step_avg:37.83ms
step:789/2000 train_time:29872ms step_avg:37.86ms
step:790/2000 train_time:29932ms step_avg:37.89ms
step:791/2000 train_time:29993ms step_avg:37.92ms
step:792/2000 train_time:30053ms step_avg:37.95ms
step:793/2000 train_time:30113ms step_avg:37.97ms
step:794/2000 train_time:30172ms step_avg:38.00ms
step:795/2000 train_time:30232ms step_avg:38.03ms
step:796/2000 train_time:30291ms step_avg:38.05ms
step:797/2000 train_time:30352ms step_avg:38.08ms
step:798/2000 train_time:30411ms step_avg:38.11ms
step:799/2000 train_time:30472ms step_avg:38.14ms
step:800/2000 train_time:30532ms step_avg:38.16ms
step:801/2000 train_time:30592ms step_avg:38.19ms
step:802/2000 train_time:30652ms step_avg:38.22ms
step:803/2000 train_time:30713ms step_avg:38.25ms
step:804/2000 train_time:30773ms step_avg:38.27ms
step:805/2000 train_time:30833ms step_avg:38.30ms
step:806/2000 train_time:30894ms step_avg:38.33ms
step:807/2000 train_time:30955ms step_avg:38.36ms
step:808/2000 train_time:31014ms step_avg:38.38ms
step:809/2000 train_time:31075ms step_avg:38.41ms
step:810/2000 train_time:31135ms step_avg:38.44ms
step:811/2000 train_time:31195ms step_avg:38.46ms
step:812/2000 train_time:31254ms step_avg:38.49ms
step:813/2000 train_time:31315ms step_avg:38.52ms
step:814/2000 train_time:31374ms step_avg:38.54ms
step:815/2000 train_time:31434ms step_avg:38.57ms
step:816/2000 train_time:31494ms step_avg:38.60ms
step:817/2000 train_time:31554ms step_avg:38.62ms
step:818/2000 train_time:31614ms step_avg:38.65ms
step:819/2000 train_time:31676ms step_avg:38.68ms
step:820/2000 train_time:31735ms step_avg:38.70ms
step:821/2000 train_time:31796ms step_avg:38.73ms
step:822/2000 train_time:31855ms step_avg:38.75ms
step:823/2000 train_time:31916ms step_avg:38.78ms
step:824/2000 train_time:31975ms step_avg:38.81ms
step:825/2000 train_time:32036ms step_avg:38.83ms
step:826/2000 train_time:32095ms step_avg:38.86ms
step:827/2000 train_time:32156ms step_avg:38.88ms
step:828/2000 train_time:32215ms step_avg:38.91ms
step:829/2000 train_time:32276ms step_avg:38.93ms
step:830/2000 train_time:32335ms step_avg:38.96ms
step:831/2000 train_time:32396ms step_avg:38.98ms
step:832/2000 train_time:32456ms step_avg:39.01ms
step:833/2000 train_time:32516ms step_avg:39.03ms
step:834/2000 train_time:32575ms step_avg:39.06ms
step:835/2000 train_time:32636ms step_avg:39.08ms
step:836/2000 train_time:32696ms step_avg:39.11ms
step:837/2000 train_time:32756ms step_avg:39.14ms
step:838/2000 train_time:32816ms step_avg:39.16ms
step:839/2000 train_time:32876ms step_avg:39.19ms
step:840/2000 train_time:32936ms step_avg:39.21ms
step:841/2000 train_time:32996ms step_avg:39.23ms
step:842/2000 train_time:33056ms step_avg:39.26ms
step:843/2000 train_time:33117ms step_avg:39.28ms
step:844/2000 train_time:33176ms step_avg:39.31ms
step:845/2000 train_time:33237ms step_avg:39.33ms
step:846/2000 train_time:33296ms step_avg:39.36ms
step:847/2000 train_time:33357ms step_avg:39.38ms
step:848/2000 train_time:33416ms step_avg:39.41ms
step:849/2000 train_time:33476ms step_avg:39.43ms
step:850/2000 train_time:33536ms step_avg:39.45ms
step:851/2000 train_time:33596ms step_avg:39.48ms
step:852/2000 train_time:33656ms step_avg:39.50ms
step:853/2000 train_time:33717ms step_avg:39.53ms
step:854/2000 train_time:33776ms step_avg:39.55ms
step:855/2000 train_time:33836ms step_avg:39.57ms
step:856/2000 train_time:33897ms step_avg:39.60ms
step:857/2000 train_time:33957ms step_avg:39.62ms
step:858/2000 train_time:34016ms step_avg:39.65ms
step:859/2000 train_time:34076ms step_avg:39.67ms
step:860/2000 train_time:34135ms step_avg:39.69ms
step:861/2000 train_time:34196ms step_avg:39.72ms
step:862/2000 train_time:34256ms step_avg:39.74ms
step:863/2000 train_time:34317ms step_avg:39.76ms
step:864/2000 train_time:34376ms step_avg:39.79ms
step:865/2000 train_time:34437ms step_avg:39.81ms
step:866/2000 train_time:34496ms step_avg:39.83ms
step:867/2000 train_time:34557ms step_avg:39.86ms
step:868/2000 train_time:34616ms step_avg:39.88ms
step:869/2000 train_time:34677ms step_avg:39.90ms
step:870/2000 train_time:34737ms step_avg:39.93ms
step:871/2000 train_time:34797ms step_avg:39.95ms
step:872/2000 train_time:34857ms step_avg:39.97ms
step:873/2000 train_time:34917ms step_avg:40.00ms
step:874/2000 train_time:34977ms step_avg:40.02ms
step:875/2000 train_time:35037ms step_avg:40.04ms
step:876/2000 train_time:35097ms step_avg:40.06ms
step:877/2000 train_time:35157ms step_avg:40.09ms
step:878/2000 train_time:35217ms step_avg:40.11ms
step:879/2000 train_time:35278ms step_avg:40.13ms
step:880/2000 train_time:35338ms step_avg:40.16ms
step:881/2000 train_time:35399ms step_avg:40.18ms
step:882/2000 train_time:35459ms step_avg:40.20ms
step:883/2000 train_time:35519ms step_avg:40.23ms
step:884/2000 train_time:35579ms step_avg:40.25ms
step:885/2000 train_time:35640ms step_avg:40.27ms
step:886/2000 train_time:35700ms step_avg:40.29ms
step:887/2000 train_time:35760ms step_avg:40.32ms
step:888/2000 train_time:35820ms step_avg:40.34ms
step:889/2000 train_time:35881ms step_avg:40.36ms
step:890/2000 train_time:35941ms step_avg:40.38ms
step:891/2000 train_time:36002ms step_avg:40.41ms
step:892/2000 train_time:36062ms step_avg:40.43ms
step:893/2000 train_time:36123ms step_avg:40.45ms
step:894/2000 train_time:36183ms step_avg:40.47ms
step:895/2000 train_time:36244ms step_avg:40.50ms
step:896/2000 train_time:36304ms step_avg:40.52ms
step:897/2000 train_time:36365ms step_avg:40.54ms
step:898/2000 train_time:36425ms step_avg:40.56ms
step:899/2000 train_time:36485ms step_avg:40.58ms
step:900/2000 train_time:36546ms step_avg:40.61ms
step:901/2000 train_time:36606ms step_avg:40.63ms
step:902/2000 train_time:36666ms step_avg:40.65ms
step:903/2000 train_time:36727ms step_avg:40.67ms
step:904/2000 train_time:36786ms step_avg:40.69ms
step:905/2000 train_time:36847ms step_avg:40.71ms
step:906/2000 train_time:36907ms step_avg:40.74ms
step:907/2000 train_time:36967ms step_avg:40.76ms
step:908/2000 train_time:37027ms step_avg:40.78ms
step:909/2000 train_time:37088ms step_avg:40.80ms
step:910/2000 train_time:37147ms step_avg:40.82ms
step:911/2000 train_time:37208ms step_avg:40.84ms
step:912/2000 train_time:37268ms step_avg:40.86ms
step:913/2000 train_time:37329ms step_avg:40.89ms
step:914/2000 train_time:37389ms step_avg:40.91ms
step:915/2000 train_time:37449ms step_avg:40.93ms
step:916/2000 train_time:37509ms step_avg:40.95ms
step:917/2000 train_time:37569ms step_avg:40.97ms
step:918/2000 train_time:37629ms step_avg:40.99ms
step:919/2000 train_time:37690ms step_avg:41.01ms
step:920/2000 train_time:37749ms step_avg:41.03ms
step:921/2000 train_time:37810ms step_avg:41.05ms
step:922/2000 train_time:37869ms step_avg:41.07ms
step:923/2000 train_time:37930ms step_avg:41.09ms
step:924/2000 train_time:37989ms step_avg:41.11ms
step:925/2000 train_time:38050ms step_avg:41.14ms
step:926/2000 train_time:38109ms step_avg:41.15ms
step:927/2000 train_time:38170ms step_avg:41.18ms
step:928/2000 train_time:38230ms step_avg:41.20ms
step:929/2000 train_time:38291ms step_avg:41.22ms
step:930/2000 train_time:38351ms step_avg:41.24ms
step:931/2000 train_time:38412ms step_avg:41.26ms
step:932/2000 train_time:38471ms step_avg:41.28ms
step:933/2000 train_time:38532ms step_avg:41.30ms
step:934/2000 train_time:38591ms step_avg:41.32ms
step:935/2000 train_time:38652ms step_avg:41.34ms
step:936/2000 train_time:38711ms step_avg:41.36ms
step:937/2000 train_time:38771ms step_avg:41.38ms
step:938/2000 train_time:38831ms step_avg:41.40ms
step:939/2000 train_time:38892ms step_avg:41.42ms
step:940/2000 train_time:38952ms step_avg:41.44ms
step:941/2000 train_time:39013ms step_avg:41.46ms
step:942/2000 train_time:39072ms step_avg:41.48ms
step:943/2000 train_time:39133ms step_avg:41.50ms
step:944/2000 train_time:39193ms step_avg:41.52ms
step:945/2000 train_time:39254ms step_avg:41.54ms
step:946/2000 train_time:39314ms step_avg:41.56ms
step:947/2000 train_time:39374ms step_avg:41.58ms
step:948/2000 train_time:39434ms step_avg:41.60ms
step:949/2000 train_time:39494ms step_avg:41.62ms
step:950/2000 train_time:39554ms step_avg:41.64ms
step:951/2000 train_time:39615ms step_avg:41.66ms
step:952/2000 train_time:39674ms step_avg:41.67ms
step:953/2000 train_time:39735ms step_avg:41.69ms
step:954/2000 train_time:39795ms step_avg:41.71ms
step:955/2000 train_time:39855ms step_avg:41.73ms
step:956/2000 train_time:39915ms step_avg:41.75ms
step:957/2000 train_time:39975ms step_avg:41.77ms
step:958/2000 train_time:40035ms step_avg:41.79ms
step:959/2000 train_time:40095ms step_avg:41.81ms
step:960/2000 train_time:40155ms step_avg:41.83ms
step:961/2000 train_time:40215ms step_avg:41.85ms
step:962/2000 train_time:40275ms step_avg:41.87ms
step:963/2000 train_time:40336ms step_avg:41.89ms
step:964/2000 train_time:40395ms step_avg:41.90ms
step:965/2000 train_time:40457ms step_avg:41.92ms
step:966/2000 train_time:40516ms step_avg:41.94ms
step:967/2000 train_time:40576ms step_avg:41.96ms
step:968/2000 train_time:40635ms step_avg:41.98ms
step:969/2000 train_time:40695ms step_avg:42.00ms
step:970/2000 train_time:40755ms step_avg:42.01ms
step:971/2000 train_time:40815ms step_avg:42.03ms
step:972/2000 train_time:40874ms step_avg:42.05ms
step:973/2000 train_time:40935ms step_avg:42.07ms
step:974/2000 train_time:40994ms step_avg:42.09ms
step:975/2000 train_time:41055ms step_avg:42.11ms
step:976/2000 train_time:41114ms step_avg:42.13ms
step:977/2000 train_time:41175ms step_avg:42.14ms
step:978/2000 train_time:41234ms step_avg:42.16ms
step:979/2000 train_time:41296ms step_avg:42.18ms
step:980/2000 train_time:41355ms step_avg:42.20ms
step:981/2000 train_time:41416ms step_avg:42.22ms
step:982/2000 train_time:41475ms step_avg:42.24ms
step:983/2000 train_time:41536ms step_avg:42.25ms
step:984/2000 train_time:41596ms step_avg:42.27ms
step:985/2000 train_time:41657ms step_avg:42.29ms
step:986/2000 train_time:41716ms step_avg:42.31ms
step:987/2000 train_time:41777ms step_avg:42.33ms
step:988/2000 train_time:41836ms step_avg:42.34ms
step:989/2000 train_time:41897ms step_avg:42.36ms
step:990/2000 train_time:41956ms step_avg:42.38ms
step:991/2000 train_time:42017ms step_avg:42.40ms
step:992/2000 train_time:42077ms step_avg:42.42ms
step:993/2000 train_time:42137ms step_avg:42.43ms
step:994/2000 train_time:42197ms step_avg:42.45ms
step:995/2000 train_time:42258ms step_avg:42.47ms
step:996/2000 train_time:42318ms step_avg:42.49ms
step:997/2000 train_time:42378ms step_avg:42.51ms
step:998/2000 train_time:42438ms step_avg:42.52ms
step:999/2000 train_time:42498ms step_avg:42.54ms
step:1000/2000 train_time:42558ms step_avg:42.56ms
step:1000/2000 val_loss:3.6743 train_time:42621ms step_avg:42.62ms
step:1001/2000 train_time:42640ms step_avg:42.60ms
step:1002/2000 train_time:42680ms step_avg:42.60ms
step:1003/2000 train_time:42744ms step_avg:42.62ms
step:1004/2000 train_time:42806ms step_avg:42.64ms
step:1005/2000 train_time:42867ms step_avg:42.65ms
step:1006/2000 train_time:42927ms step_avg:42.67ms
step:1007/2000 train_time:42986ms step_avg:42.69ms
step:1008/2000 train_time:43046ms step_avg:42.70ms
step:1009/2000 train_time:43106ms step_avg:42.72ms
step:1010/2000 train_time:43165ms step_avg:42.74ms
step:1011/2000 train_time:43226ms step_avg:42.76ms
step:1012/2000 train_time:43284ms step_avg:42.77ms
step:1013/2000 train_time:43344ms step_avg:42.79ms
step:1014/2000 train_time:43405ms step_avg:42.81ms
step:1015/2000 train_time:43465ms step_avg:42.82ms
step:1016/2000 train_time:43524ms step_avg:42.84ms
step:1017/2000 train_time:43586ms step_avg:42.86ms
step:1018/2000 train_time:43647ms step_avg:42.87ms
step:1019/2000 train_time:43709ms step_avg:42.89ms
step:1020/2000 train_time:43770ms step_avg:42.91ms
step:1021/2000 train_time:43831ms step_avg:42.93ms
step:1022/2000 train_time:43891ms step_avg:42.95ms
step:1023/2000 train_time:43951ms step_avg:42.96ms
step:1024/2000 train_time:44010ms step_avg:42.98ms
step:1025/2000 train_time:44071ms step_avg:43.00ms
step:1026/2000 train_time:44131ms step_avg:43.01ms
step:1027/2000 train_time:44191ms step_avg:43.03ms
step:1028/2000 train_time:44250ms step_avg:43.04ms
step:1029/2000 train_time:44310ms step_avg:43.06ms
step:1030/2000 train_time:44370ms step_avg:43.08ms
step:1031/2000 train_time:44431ms step_avg:43.09ms
step:1032/2000 train_time:44492ms step_avg:43.11ms
step:1033/2000 train_time:44553ms step_avg:43.13ms
step:1034/2000 train_time:44613ms step_avg:43.15ms
step:1035/2000 train_time:44675ms step_avg:43.16ms
step:1036/2000 train_time:44736ms step_avg:43.18ms
step:1037/2000 train_time:44797ms step_avg:43.20ms
step:1038/2000 train_time:44856ms step_avg:43.21ms
step:1039/2000 train_time:44917ms step_avg:43.23ms
step:1040/2000 train_time:44976ms step_avg:43.25ms
step:1041/2000 train_time:45037ms step_avg:43.26ms
step:1042/2000 train_time:45097ms step_avg:43.28ms
step:1043/2000 train_time:45157ms step_avg:43.29ms
step:1044/2000 train_time:45215ms step_avg:43.31ms
step:1045/2000 train_time:45276ms step_avg:43.33ms
step:1046/2000 train_time:45336ms step_avg:43.34ms
step:1047/2000 train_time:45397ms step_avg:43.36ms
step:1048/2000 train_time:45456ms step_avg:43.37ms
step:1049/2000 train_time:45517ms step_avg:43.39ms
step:1050/2000 train_time:45576ms step_avg:43.41ms
step:1051/2000 train_time:45637ms step_avg:43.42ms
step:1052/2000 train_time:45697ms step_avg:43.44ms
step:1053/2000 train_time:45758ms step_avg:43.46ms
step:1054/2000 train_time:45817ms step_avg:43.47ms
step:1055/2000 train_time:45878ms step_avg:43.49ms
step:1056/2000 train_time:45938ms step_avg:43.50ms
step:1057/2000 train_time:45999ms step_avg:43.52ms
step:1058/2000 train_time:46059ms step_avg:43.53ms
step:1059/2000 train_time:46119ms step_avg:43.55ms
step:1060/2000 train_time:46178ms step_avg:43.56ms
step:1061/2000 train_time:46240ms step_avg:43.58ms
step:1062/2000 train_time:46300ms step_avg:43.60ms
step:1063/2000 train_time:46361ms step_avg:43.61ms
step:1064/2000 train_time:46420ms step_avg:43.63ms
step:1065/2000 train_time:46481ms step_avg:43.64ms
step:1066/2000 train_time:46540ms step_avg:43.66ms
step:1067/2000 train_time:46601ms step_avg:43.67ms
step:1068/2000 train_time:46660ms step_avg:43.69ms
step:1069/2000 train_time:46721ms step_avg:43.71ms
step:1070/2000 train_time:46780ms step_avg:43.72ms
step:1071/2000 train_time:46841ms step_avg:43.74ms
step:1072/2000 train_time:46900ms step_avg:43.75ms
step:1073/2000 train_time:46961ms step_avg:43.77ms
step:1074/2000 train_time:47020ms step_avg:43.78ms
step:1075/2000 train_time:47080ms step_avg:43.80ms
step:1076/2000 train_time:47140ms step_avg:43.81ms
step:1077/2000 train_time:47201ms step_avg:43.83ms
step:1078/2000 train_time:47261ms step_avg:43.84ms
step:1079/2000 train_time:47322ms step_avg:43.86ms
step:1080/2000 train_time:47381ms step_avg:43.87ms
step:1081/2000 train_time:47442ms step_avg:43.89ms
step:1082/2000 train_time:47502ms step_avg:43.90ms
step:1083/2000 train_time:47562ms step_avg:43.92ms
step:1084/2000 train_time:47621ms step_avg:43.93ms
step:1085/2000 train_time:47682ms step_avg:43.95ms
step:1086/2000 train_time:47741ms step_avg:43.96ms
step:1087/2000 train_time:47802ms step_avg:43.98ms
step:1088/2000 train_time:47860ms step_avg:43.99ms
step:1089/2000 train_time:47921ms step_avg:44.00ms
step:1090/2000 train_time:47980ms step_avg:44.02ms
step:1091/2000 train_time:48040ms step_avg:44.03ms
step:1092/2000 train_time:48100ms step_avg:44.05ms
step:1093/2000 train_time:48161ms step_avg:44.06ms
step:1094/2000 train_time:48220ms step_avg:44.08ms
step:1095/2000 train_time:48281ms step_avg:44.09ms
step:1096/2000 train_time:48340ms step_avg:44.11ms
step:1097/2000 train_time:48401ms step_avg:44.12ms
step:1098/2000 train_time:48461ms step_avg:44.14ms
step:1099/2000 train_time:48522ms step_avg:44.15ms
step:1100/2000 train_time:48581ms step_avg:44.16ms
step:1101/2000 train_time:48642ms step_avg:44.18ms
step:1102/2000 train_time:48702ms step_avg:44.19ms
step:1103/2000 train_time:48762ms step_avg:44.21ms
step:1104/2000 train_time:48822ms step_avg:44.22ms
step:1105/2000 train_time:48882ms step_avg:44.24ms
step:1106/2000 train_time:48942ms step_avg:44.25ms
step:1107/2000 train_time:49002ms step_avg:44.27ms
step:1108/2000 train_time:49061ms step_avg:44.28ms
step:1109/2000 train_time:49122ms step_avg:44.29ms
step:1110/2000 train_time:49181ms step_avg:44.31ms
step:1111/2000 train_time:49241ms step_avg:44.32ms
step:1112/2000 train_time:49300ms step_avg:44.33ms
step:1113/2000 train_time:49361ms step_avg:44.35ms
step:1114/2000 train_time:49420ms step_avg:44.36ms
step:1115/2000 train_time:49481ms step_avg:44.38ms
step:1116/2000 train_time:49540ms step_avg:44.39ms
step:1117/2000 train_time:49601ms step_avg:44.41ms
step:1118/2000 train_time:49661ms step_avg:44.42ms
step:1119/2000 train_time:49722ms step_avg:44.43ms
step:1120/2000 train_time:49781ms step_avg:44.45ms
step:1121/2000 train_time:49842ms step_avg:44.46ms
step:1122/2000 train_time:49902ms step_avg:44.48ms
step:1123/2000 train_time:49962ms step_avg:44.49ms
step:1124/2000 train_time:50021ms step_avg:44.50ms
step:1125/2000 train_time:50081ms step_avg:44.52ms
step:1126/2000 train_time:50141ms step_avg:44.53ms
step:1127/2000 train_time:50202ms step_avg:44.54ms
step:1128/2000 train_time:50261ms step_avg:44.56ms
step:1129/2000 train_time:50322ms step_avg:44.57ms
step:1130/2000 train_time:50380ms step_avg:44.58ms
step:1131/2000 train_time:50441ms step_avg:44.60ms
step:1132/2000 train_time:50500ms step_avg:44.61ms
step:1133/2000 train_time:50561ms step_avg:44.63ms
step:1134/2000 train_time:50621ms step_avg:44.64ms
step:1135/2000 train_time:50681ms step_avg:44.65ms
step:1136/2000 train_time:50740ms step_avg:44.67ms
step:1137/2000 train_time:50801ms step_avg:44.68ms
step:1138/2000 train_time:50861ms step_avg:44.69ms
step:1139/2000 train_time:50922ms step_avg:44.71ms
step:1140/2000 train_time:50980ms step_avg:44.72ms
step:1141/2000 train_time:51042ms step_avg:44.73ms
step:1142/2000 train_time:51101ms step_avg:44.75ms
step:1143/2000 train_time:51161ms step_avg:44.76ms
step:1144/2000 train_time:51221ms step_avg:44.77ms
step:1145/2000 train_time:51281ms step_avg:44.79ms
step:1146/2000 train_time:51341ms step_avg:44.80ms
step:1147/2000 train_time:51402ms step_avg:44.81ms
step:1148/2000 train_time:51461ms step_avg:44.83ms
step:1149/2000 train_time:51522ms step_avg:44.84ms
step:1150/2000 train_time:51581ms step_avg:44.85ms
step:1151/2000 train_time:51642ms step_avg:44.87ms
step:1152/2000 train_time:51701ms step_avg:44.88ms
step:1153/2000 train_time:51761ms step_avg:44.89ms
step:1154/2000 train_time:51820ms step_avg:44.90ms
step:1155/2000 train_time:51881ms step_avg:44.92ms
step:1156/2000 train_time:51941ms step_avg:44.93ms
step:1157/2000 train_time:52002ms step_avg:44.95ms
step:1158/2000 train_time:52061ms step_avg:44.96ms
step:1159/2000 train_time:52121ms step_avg:44.97ms
step:1160/2000 train_time:52180ms step_avg:44.98ms
step:1161/2000 train_time:52241ms step_avg:45.00ms
step:1162/2000 train_time:52300ms step_avg:45.01ms
step:1163/2000 train_time:52362ms step_avg:45.02ms
step:1164/2000 train_time:52421ms step_avg:45.04ms
step:1165/2000 train_time:52481ms step_avg:45.05ms
step:1166/2000 train_time:52541ms step_avg:45.06ms
step:1167/2000 train_time:52602ms step_avg:45.07ms
step:1168/2000 train_time:52661ms step_avg:45.09ms
step:1169/2000 train_time:52721ms step_avg:45.10ms
step:1170/2000 train_time:52781ms step_avg:45.11ms
step:1171/2000 train_time:52841ms step_avg:45.12ms
step:1172/2000 train_time:52901ms step_avg:45.14ms
step:1173/2000 train_time:52962ms step_avg:45.15ms
step:1174/2000 train_time:53021ms step_avg:45.16ms
step:1175/2000 train_time:53082ms step_avg:45.18ms
step:1176/2000 train_time:53142ms step_avg:45.19ms
step:1177/2000 train_time:53202ms step_avg:45.20ms
step:1178/2000 train_time:53261ms step_avg:45.21ms
step:1179/2000 train_time:53322ms step_avg:45.23ms
step:1180/2000 train_time:53381ms step_avg:45.24ms
step:1181/2000 train_time:53442ms step_avg:45.25ms
step:1182/2000 train_time:53501ms step_avg:45.26ms
step:1183/2000 train_time:53562ms step_avg:45.28ms
step:1184/2000 train_time:53621ms step_avg:45.29ms
step:1185/2000 train_time:53681ms step_avg:45.30ms
step:1186/2000 train_time:53741ms step_avg:45.31ms
step:1187/2000 train_time:53801ms step_avg:45.33ms
step:1188/2000 train_time:53861ms step_avg:45.34ms
step:1189/2000 train_time:53922ms step_avg:45.35ms
step:1190/2000 train_time:53981ms step_avg:45.36ms
step:1191/2000 train_time:54041ms step_avg:45.37ms
step:1192/2000 train_time:54101ms step_avg:45.39ms
step:1193/2000 train_time:54163ms step_avg:45.40ms
step:1194/2000 train_time:54222ms step_avg:45.41ms
step:1195/2000 train_time:54282ms step_avg:45.42ms
step:1196/2000 train_time:54341ms step_avg:45.44ms
step:1197/2000 train_time:54401ms step_avg:45.45ms
step:1198/2000 train_time:54461ms step_avg:45.46ms
step:1199/2000 train_time:54521ms step_avg:45.47ms
step:1200/2000 train_time:54581ms step_avg:45.48ms
step:1201/2000 train_time:54642ms step_avg:45.50ms
step:1202/2000 train_time:54701ms step_avg:45.51ms
step:1203/2000 train_time:54762ms step_avg:45.52ms
step:1204/2000 train_time:54821ms step_avg:45.53ms
step:1205/2000 train_time:54882ms step_avg:45.55ms
step:1206/2000 train_time:54941ms step_avg:45.56ms
step:1207/2000 train_time:55002ms step_avg:45.57ms
step:1208/2000 train_time:55062ms step_avg:45.58ms
step:1209/2000 train_time:55122ms step_avg:45.59ms
step:1210/2000 train_time:55182ms step_avg:45.60ms
step:1211/2000 train_time:55242ms step_avg:45.62ms
step:1212/2000 train_time:55302ms step_avg:45.63ms
step:1213/2000 train_time:55362ms step_avg:45.64ms
step:1214/2000 train_time:55422ms step_avg:45.65ms
step:1215/2000 train_time:55482ms step_avg:45.66ms
step:1216/2000 train_time:55542ms step_avg:45.68ms
step:1217/2000 train_time:55602ms step_avg:45.69ms
step:1218/2000 train_time:55661ms step_avg:45.70ms
step:1219/2000 train_time:55722ms step_avg:45.71ms
step:1220/2000 train_time:55781ms step_avg:45.72ms
step:1221/2000 train_time:55841ms step_avg:45.73ms
step:1222/2000 train_time:55901ms step_avg:45.75ms
step:1223/2000 train_time:55961ms step_avg:45.76ms
step:1224/2000 train_time:56020ms step_avg:45.77ms
step:1225/2000 train_time:56080ms step_avg:45.78ms
step:1226/2000 train_time:56139ms step_avg:45.79ms
step:1227/2000 train_time:56200ms step_avg:45.80ms
step:1228/2000 train_time:56259ms step_avg:45.81ms
step:1229/2000 train_time:56320ms step_avg:45.83ms
step:1230/2000 train_time:56379ms step_avg:45.84ms
step:1231/2000 train_time:56440ms step_avg:45.85ms
step:1232/2000 train_time:56499ms step_avg:45.86ms
step:1233/2000 train_time:56560ms step_avg:45.87ms
step:1234/2000 train_time:56619ms step_avg:45.88ms
step:1235/2000 train_time:56680ms step_avg:45.89ms
step:1236/2000 train_time:56739ms step_avg:45.91ms
step:1237/2000 train_time:56800ms step_avg:45.92ms
step:1238/2000 train_time:56859ms step_avg:45.93ms
step:1239/2000 train_time:56920ms step_avg:45.94ms
step:1240/2000 train_time:56980ms step_avg:45.95ms
step:1241/2000 train_time:57040ms step_avg:45.96ms
step:1242/2000 train_time:57100ms step_avg:45.97ms
step:1243/2000 train_time:57161ms step_avg:45.99ms
step:1244/2000 train_time:57220ms step_avg:46.00ms
step:1245/2000 train_time:57281ms step_avg:46.01ms
step:1246/2000 train_time:57340ms step_avg:46.02ms
step:1247/2000 train_time:57400ms step_avg:46.03ms
step:1248/2000 train_time:57460ms step_avg:46.04ms
step:1249/2000 train_time:57520ms step_avg:46.05ms
step:1250/2000 train_time:57579ms step_avg:46.06ms
step:1250/2000 val_loss:3.5584 train_time:57641ms step_avg:46.11ms
step:1251/2000 train_time:57662ms step_avg:46.09ms
step:1252/2000 train_time:57700ms step_avg:46.09ms
step:1253/2000 train_time:57762ms step_avg:46.10ms
step:1254/2000 train_time:57825ms step_avg:46.11ms
step:1255/2000 train_time:57885ms step_avg:46.12ms
step:1256/2000 train_time:57945ms step_avg:46.13ms
step:1257/2000 train_time:58005ms step_avg:46.15ms
step:1258/2000 train_time:58065ms step_avg:46.16ms
step:1259/2000 train_time:58125ms step_avg:46.17ms
step:1260/2000 train_time:58184ms step_avg:46.18ms
step:1261/2000 train_time:58244ms step_avg:46.19ms
step:1262/2000 train_time:58302ms step_avg:46.20ms
step:1263/2000 train_time:58362ms step_avg:46.21ms
step:1264/2000 train_time:58421ms step_avg:46.22ms
step:1265/2000 train_time:58480ms step_avg:46.23ms
step:1266/2000 train_time:58540ms step_avg:46.24ms
step:1267/2000 train_time:58602ms step_avg:46.25ms
step:1268/2000 train_time:58662ms step_avg:46.26ms
step:1269/2000 train_time:58724ms step_avg:46.28ms
step:1270/2000 train_time:58784ms step_avg:46.29ms
step:1271/2000 train_time:58846ms step_avg:46.30ms
step:1272/2000 train_time:58905ms step_avg:46.31ms
step:1273/2000 train_time:58965ms step_avg:46.32ms
step:1274/2000 train_time:59025ms step_avg:46.33ms
step:1275/2000 train_time:59086ms step_avg:46.34ms
step:1276/2000 train_time:59146ms step_avg:46.35ms
step:1277/2000 train_time:59206ms step_avg:46.36ms
step:1278/2000 train_time:59264ms step_avg:46.37ms
step:1279/2000 train_time:59324ms step_avg:46.38ms
step:1280/2000 train_time:59383ms step_avg:46.39ms
step:1281/2000 train_time:59443ms step_avg:46.40ms
step:1282/2000 train_time:59503ms step_avg:46.41ms
step:1283/2000 train_time:59563ms step_avg:46.43ms
step:1284/2000 train_time:59624ms step_avg:46.44ms
step:1285/2000 train_time:59686ms step_avg:46.45ms
step:1286/2000 train_time:59746ms step_avg:46.46ms
step:1287/2000 train_time:59808ms step_avg:46.47ms
step:1288/2000 train_time:59868ms step_avg:46.48ms
step:1289/2000 train_time:59928ms step_avg:46.49ms
step:1290/2000 train_time:59988ms step_avg:46.50ms
step:1291/2000 train_time:60049ms step_avg:46.51ms
step:1292/2000 train_time:60109ms step_avg:46.52ms
step:1293/2000 train_time:60169ms step_avg:46.53ms
step:1294/2000 train_time:60229ms step_avg:46.54ms
step:1295/2000 train_time:60289ms step_avg:46.56ms
step:1296/2000 train_time:60349ms step_avg:46.57ms
step:1297/2000 train_time:60409ms step_avg:46.58ms
step:1298/2000 train_time:60468ms step_avg:46.59ms
step:1299/2000 train_time:60529ms step_avg:46.60ms
step:1300/2000 train_time:60589ms step_avg:46.61ms
step:1301/2000 train_time:60650ms step_avg:46.62ms
step:1302/2000 train_time:60710ms step_avg:46.63ms
step:1303/2000 train_time:60771ms step_avg:46.64ms
step:1304/2000 train_time:60832ms step_avg:46.65ms
step:1305/2000 train_time:60893ms step_avg:46.66ms
step:1306/2000 train_time:60954ms step_avg:46.67ms
step:1307/2000 train_time:61014ms step_avg:46.68ms
step:1308/2000 train_time:61074ms step_avg:46.69ms
step:1309/2000 train_time:61163ms step_avg:46.72ms
step:1310/2000 train_time:61250ms step_avg:46.76ms
step:1311/2000 train_time:61338ms step_avg:46.79ms
step:1312/2000 train_time:61426ms step_avg:46.82ms
step:1313/2000 train_time:61514ms step_avg:46.85ms
step:1314/2000 train_time:61602ms step_avg:46.88ms
step:1315/2000 train_time:61690ms step_avg:46.91ms
step:1316/2000 train_time:61779ms step_avg:46.94ms
step:1317/2000 train_time:61868ms step_avg:46.98ms
step:1318/2000 train_time:61957ms step_avg:47.01ms
step:1319/2000 train_time:62045ms step_avg:47.04ms
step:1320/2000 train_time:62132ms step_avg:47.07ms
step:1321/2000 train_time:62220ms step_avg:47.10ms
step:1322/2000 train_time:62307ms step_avg:47.13ms
step:1323/2000 train_time:62396ms step_avg:47.16ms
step:1324/2000 train_time:62483ms step_avg:47.19ms
step:1325/2000 train_time:62570ms step_avg:47.22ms
step:1326/2000 train_time:62658ms step_avg:47.25ms
step:1327/2000 train_time:62748ms step_avg:47.29ms
step:1328/2000 train_time:62836ms step_avg:47.32ms
step:1329/2000 train_time:62924ms step_avg:47.35ms
step:1330/2000 train_time:63011ms step_avg:47.38ms
step:1331/2000 train_time:63100ms step_avg:47.41ms
step:1332/2000 train_time:63188ms step_avg:47.44ms
step:1333/2000 train_time:63275ms step_avg:47.47ms
step:1334/2000 train_time:63362ms step_avg:47.50ms
step:1335/2000 train_time:63450ms step_avg:47.53ms
step:1336/2000 train_time:63538ms step_avg:47.56ms
step:1337/2000 train_time:63625ms step_avg:47.59ms
step:1338/2000 train_time:63712ms step_avg:47.62ms
step:1339/2000 train_time:63802ms step_avg:47.65ms
step:1340/2000 train_time:63890ms step_avg:47.68ms
step:1341/2000 train_time:63978ms step_avg:47.71ms
step:1342/2000 train_time:64066ms step_avg:47.74ms
step:1343/2000 train_time:64154ms step_avg:47.77ms
step:1344/2000 train_time:64242ms step_avg:47.80ms
step:1345/2000 train_time:64330ms step_avg:47.83ms
step:1346/2000 train_time:64419ms step_avg:47.86ms
step:1347/2000 train_time:64508ms step_avg:47.89ms
step:1348/2000 train_time:64596ms step_avg:47.92ms
step:1349/2000 train_time:64684ms step_avg:47.95ms
step:1350/2000 train_time:64772ms step_avg:47.98ms
step:1351/2000 train_time:64860ms step_avg:48.01ms
step:1352/2000 train_time:64948ms step_avg:48.04ms
step:1353/2000 train_time:65036ms step_avg:48.07ms
step:1354/2000 train_time:65125ms step_avg:48.10ms
step:1355/2000 train_time:65212ms step_avg:48.13ms
step:1356/2000 train_time:65300ms step_avg:48.16ms
step:1357/2000 train_time:65388ms step_avg:48.19ms
step:1358/2000 train_time:65477ms step_avg:48.22ms
step:1359/2000 train_time:65564ms step_avg:48.24ms
step:1360/2000 train_time:65652ms step_avg:48.27ms
step:1361/2000 train_time:65740ms step_avg:48.30ms
step:1362/2000 train_time:65828ms step_avg:48.33ms
step:1363/2000 train_time:65917ms step_avg:48.36ms
step:1364/2000 train_time:66005ms step_avg:48.39ms
step:1365/2000 train_time:66093ms step_avg:48.42ms
step:1366/2000 train_time:66180ms step_avg:48.45ms
step:1367/2000 train_time:66268ms step_avg:48.48ms
step:1368/2000 train_time:66356ms step_avg:48.51ms
step:1369/2000 train_time:66444ms step_avg:48.53ms
step:1370/2000 train_time:66531ms step_avg:48.56ms
step:1371/2000 train_time:66618ms step_avg:48.59ms
step:1372/2000 train_time:66707ms step_avg:48.62ms
step:1373/2000 train_time:66795ms step_avg:48.65ms
step:1374/2000 train_time:66884ms step_avg:48.68ms
step:1375/2000 train_time:66971ms step_avg:48.71ms
step:1376/2000 train_time:67059ms step_avg:48.73ms
step:1377/2000 train_time:67148ms step_avg:48.76ms
step:1378/2000 train_time:67236ms step_avg:48.79ms
step:1379/2000 train_time:67324ms step_avg:48.82ms
step:1380/2000 train_time:67412ms step_avg:48.85ms
step:1381/2000 train_time:67500ms step_avg:48.88ms
step:1382/2000 train_time:67589ms step_avg:48.91ms
step:1383/2000 train_time:67676ms step_avg:48.93ms
step:1384/2000 train_time:67764ms step_avg:48.96ms
step:1385/2000 train_time:67852ms step_avg:48.99ms
step:1386/2000 train_time:67940ms step_avg:49.02ms
step:1387/2000 train_time:68029ms step_avg:49.05ms
step:1388/2000 train_time:68117ms step_avg:49.08ms
step:1389/2000 train_time:68205ms step_avg:49.10ms
step:1390/2000 train_time:68293ms step_avg:49.13ms
step:1391/2000 train_time:68380ms step_avg:49.16ms
step:1392/2000 train_time:68468ms step_avg:49.19ms
step:1393/2000 train_time:68556ms step_avg:49.21ms
step:1394/2000 train_time:68644ms step_avg:49.24ms
step:1395/2000 train_time:68732ms step_avg:49.27ms
step:1396/2000 train_time:68820ms step_avg:49.30ms
step:1397/2000 train_time:68908ms step_avg:49.33ms
step:1398/2000 train_time:68996ms step_avg:49.35ms
step:1399/2000 train_time:69086ms step_avg:49.38ms
step:1400/2000 train_time:69174ms step_avg:49.41ms
step:1401/2000 train_time:69262ms step_avg:49.44ms
step:1402/2000 train_time:69349ms step_avg:49.46ms
step:1403/2000 train_time:69436ms step_avg:49.49ms
step:1404/2000 train_time:69524ms step_avg:49.52ms
step:1405/2000 train_time:69613ms step_avg:49.55ms
step:1406/2000 train_time:69701ms step_avg:49.57ms
step:1407/2000 train_time:69789ms step_avg:49.60ms
step:1408/2000 train_time:69877ms step_avg:49.63ms
step:1409/2000 train_time:69966ms step_avg:49.66ms
step:1410/2000 train_time:70052ms step_avg:49.68ms
step:1411/2000 train_time:70140ms step_avg:49.71ms
step:1412/2000 train_time:70228ms step_avg:49.74ms
step:1413/2000 train_time:70316ms step_avg:49.76ms
step:1414/2000 train_time:70405ms step_avg:49.79ms
step:1415/2000 train_time:70493ms step_avg:49.82ms
step:1416/2000 train_time:70581ms step_avg:49.85ms
step:1417/2000 train_time:70669ms step_avg:49.87ms
step:1418/2000 train_time:70757ms step_avg:49.90ms
step:1419/2000 train_time:70845ms step_avg:49.93ms
step:1420/2000 train_time:70933ms step_avg:49.95ms
step:1421/2000 train_time:71021ms step_avg:49.98ms
step:1422/2000 train_time:71109ms step_avg:50.01ms
step:1423/2000 train_time:71197ms step_avg:50.03ms
step:1424/2000 train_time:71284ms step_avg:50.06ms
step:1425/2000 train_time:71373ms step_avg:50.09ms
step:1426/2000 train_time:71461ms step_avg:50.11ms
step:1427/2000 train_time:71550ms step_avg:50.14ms
step:1428/2000 train_time:71638ms step_avg:50.17ms
step:1429/2000 train_time:71727ms step_avg:50.19ms
step:1430/2000 train_time:71815ms step_avg:50.22ms
step:1431/2000 train_time:71903ms step_avg:50.25ms
step:1432/2000 train_time:71990ms step_avg:50.27ms
step:1433/2000 train_time:72079ms step_avg:50.30ms
step:1434/2000 train_time:72168ms step_avg:50.33ms
step:1435/2000 train_time:72256ms step_avg:50.35ms
step:1436/2000 train_time:72345ms step_avg:50.38ms
step:1437/2000 train_time:72432ms step_avg:50.41ms
step:1438/2000 train_time:72521ms step_avg:50.43ms
step:1439/2000 train_time:72609ms step_avg:50.46ms
step:1440/2000 train_time:72697ms step_avg:50.48ms
step:1441/2000 train_time:72785ms step_avg:50.51ms
step:1442/2000 train_time:72874ms step_avg:50.54ms
step:1443/2000 train_time:72962ms step_avg:50.56ms
step:1444/2000 train_time:73050ms step_avg:50.59ms
step:1445/2000 train_time:73138ms step_avg:50.61ms
step:1446/2000 train_time:73226ms step_avg:50.64ms
step:1447/2000 train_time:73315ms step_avg:50.67ms
step:1448/2000 train_time:73402ms step_avg:50.69ms
step:1449/2000 train_time:73490ms step_avg:50.72ms
step:1450/2000 train_time:73578ms step_avg:50.74ms
step:1451/2000 train_time:73667ms step_avg:50.77ms
step:1452/2000 train_time:73754ms step_avg:50.79ms
step:1453/2000 train_time:73842ms step_avg:50.82ms
step:1454/2000 train_time:73930ms step_avg:50.85ms
step:1455/2000 train_time:74018ms step_avg:50.87ms
step:1456/2000 train_time:74107ms step_avg:50.90ms
step:1457/2000 train_time:74194ms step_avg:50.92ms
step:1458/2000 train_time:74282ms step_avg:50.95ms
step:1459/2000 train_time:74370ms step_avg:50.97ms
step:1460/2000 train_time:74456ms step_avg:51.00ms
step:1461/2000 train_time:74544ms step_avg:51.02ms
step:1462/2000 train_time:74632ms step_avg:51.05ms
step:1463/2000 train_time:74721ms step_avg:51.07ms
step:1464/2000 train_time:74808ms step_avg:51.10ms
step:1465/2000 train_time:74896ms step_avg:51.12ms
step:1466/2000 train_time:74984ms step_avg:51.15ms
step:1467/2000 train_time:75072ms step_avg:51.17ms
step:1468/2000 train_time:75160ms step_avg:51.20ms
step:1469/2000 train_time:75248ms step_avg:51.22ms
step:1470/2000 train_time:75335ms step_avg:51.25ms
step:1471/2000 train_time:75424ms step_avg:51.27ms
step:1472/2000 train_time:75512ms step_avg:51.30ms
step:1473/2000 train_time:75599ms step_avg:51.32ms
step:1474/2000 train_time:75687ms step_avg:51.35ms
step:1475/2000 train_time:75775ms step_avg:51.37ms
step:1476/2000 train_time:75864ms step_avg:51.40ms
step:1477/2000 train_time:75952ms step_avg:51.42ms
step:1478/2000 train_time:76040ms step_avg:51.45ms
step:1479/2000 train_time:76128ms step_avg:51.47ms
step:1480/2000 train_time:76216ms step_avg:51.50ms
step:1481/2000 train_time:76305ms step_avg:51.52ms
step:1482/2000 train_time:76392ms step_avg:51.55ms
step:1483/2000 train_time:76480ms step_avg:51.57ms
step:1484/2000 train_time:76568ms step_avg:51.60ms
step:1485/2000 train_time:76656ms step_avg:51.62ms
step:1486/2000 train_time:76745ms step_avg:51.65ms
step:1487/2000 train_time:76833ms step_avg:51.67ms
step:1488/2000 train_time:76921ms step_avg:51.69ms
step:1489/2000 train_time:77009ms step_avg:51.72ms
step:1490/2000 train_time:77096ms step_avg:51.74ms
step:1491/2000 train_time:77185ms step_avg:51.77ms
step:1492/2000 train_time:77272ms step_avg:51.79ms
step:1493/2000 train_time:77361ms step_avg:51.82ms
step:1494/2000 train_time:77447ms step_avg:51.84ms
step:1495/2000 train_time:77535ms step_avg:51.86ms
step:1496/2000 train_time:77623ms step_avg:51.89ms
step:1497/2000 train_time:77711ms step_avg:51.91ms
step:1498/2000 train_time:77800ms step_avg:51.94ms
step:1499/2000 train_time:77888ms step_avg:51.96ms
step:1500/2000 train_time:77976ms step_avg:51.98ms
step:1500/2000 val_loss:3.4437 train_time:78066ms step_avg:52.04ms
step:1501/2000 train_time:78089ms step_avg:52.02ms
step:1502/2000 train_time:78156ms step_avg:52.03ms
step:1503/2000 train_time:78248ms step_avg:52.06ms
step:1504/2000 train_time:78337ms step_avg:52.09ms
step:1505/2000 train_time:78423ms step_avg:52.11ms
step:1506/2000 train_time:78511ms step_avg:52.13ms
step:1507/2000 train_time:78598ms step_avg:52.16ms
step:1508/2000 train_time:78685ms step_avg:52.18ms
step:1509/2000 train_time:78772ms step_avg:52.20ms
step:1510/2000 train_time:78860ms step_avg:52.22ms
step:1511/2000 train_time:78946ms step_avg:52.25ms
step:1512/2000 train_time:79036ms step_avg:52.27ms
step:1513/2000 train_time:79127ms step_avg:52.30ms
step:1514/2000 train_time:79217ms step_avg:52.32ms
step:1515/2000 train_time:79306ms step_avg:52.35ms
step:1516/2000 train_time:79394ms step_avg:52.37ms
step:1517/2000 train_time:79481ms step_avg:52.39ms
step:1518/2000 train_time:79568ms step_avg:52.42ms
step:1519/2000 train_time:79656ms step_avg:52.44ms
step:1520/2000 train_time:79744ms step_avg:52.46ms
step:1521/2000 train_time:79830ms step_avg:52.49ms
step:1522/2000 train_time:79918ms step_avg:52.51ms
step:1523/2000 train_time:80006ms step_avg:52.53ms
step:1524/2000 train_time:80096ms step_avg:52.56ms
step:1525/2000 train_time:80186ms step_avg:52.58ms
step:1526/2000 train_time:80275ms step_avg:52.61ms
step:1527/2000 train_time:80365ms step_avg:52.63ms
step:1528/2000 train_time:80452ms step_avg:52.65ms
step:1529/2000 train_time:80540ms step_avg:52.68ms
step:1530/2000 train_time:80627ms step_avg:52.70ms
step:1531/2000 train_time:80715ms step_avg:52.72ms
step:1532/2000 train_time:80801ms step_avg:52.74ms
step:1533/2000 train_time:80889ms step_avg:52.77ms
step:1534/2000 train_time:80977ms step_avg:52.79ms
step:1535/2000 train_time:81066ms step_avg:52.81ms
step:1536/2000 train_time:81155ms step_avg:52.84ms
step:1537/2000 train_time:81245ms step_avg:52.86ms
step:1538/2000 train_time:81334ms step_avg:52.88ms
step:1539/2000 train_time:81422ms step_avg:52.91ms
step:1540/2000 train_time:81510ms step_avg:52.93ms
step:1541/2000 train_time:81598ms step_avg:52.95ms
step:1542/2000 train_time:81685ms step_avg:52.97ms
step:1543/2000 train_time:81772ms step_avg:53.00ms
step:1544/2000 train_time:81860ms step_avg:53.02ms
step:1545/2000 train_time:81947ms step_avg:53.04ms
step:1546/2000 train_time:82035ms step_avg:53.06ms
step:1547/2000 train_time:82124ms step_avg:53.09ms
step:1548/2000 train_time:82212ms step_avg:53.11ms
step:1549/2000 train_time:82301ms step_avg:53.13ms
step:1550/2000 train_time:82389ms step_avg:53.15ms
step:1551/2000 train_time:82477ms step_avg:53.18ms
step:1552/2000 train_time:82565ms step_avg:53.20ms
step:1553/2000 train_time:82653ms step_avg:53.22ms
step:1554/2000 train_time:82740ms step_avg:53.24ms
step:1555/2000 train_time:82828ms step_avg:53.27ms
step:1556/2000 train_time:82916ms step_avg:53.29ms
step:1557/2000 train_time:83004ms step_avg:53.31ms
step:1558/2000 train_time:83092ms step_avg:53.33ms
step:1559/2000 train_time:83181ms step_avg:53.36ms
step:1560/2000 train_time:83269ms step_avg:53.38ms
step:1561/2000 train_time:83357ms step_avg:53.40ms
step:1562/2000 train_time:83445ms step_avg:53.42ms
step:1563/2000 train_time:83533ms step_avg:53.44ms
step:1564/2000 train_time:83621ms step_avg:53.47ms
step:1565/2000 train_time:83708ms step_avg:53.49ms
step:1566/2000 train_time:83796ms step_avg:53.51ms
step:1567/2000 train_time:83883ms step_avg:53.53ms
step:1568/2000 train_time:83972ms step_avg:53.55ms
step:1569/2000 train_time:84060ms step_avg:53.58ms
step:1570/2000 train_time:84149ms step_avg:53.60ms
step:1571/2000 train_time:84237ms step_avg:53.62ms
step:1572/2000 train_time:84325ms step_avg:53.64ms
step:1573/2000 train_time:84413ms step_avg:53.66ms
step:1574/2000 train_time:84501ms step_avg:53.69ms
step:1575/2000 train_time:84589ms step_avg:53.71ms
step:1576/2000 train_time:84677ms step_avg:53.73ms
step:1577/2000 train_time:84765ms step_avg:53.75ms
step:1578/2000 train_time:84852ms step_avg:53.77ms
step:1579/2000 train_time:84940ms step_avg:53.79ms
step:1580/2000 train_time:85027ms step_avg:53.81ms
step:1581/2000 train_time:85116ms step_avg:53.84ms
step:1582/2000 train_time:85204ms step_avg:53.86ms
step:1583/2000 train_time:85292ms step_avg:53.88ms
step:1584/2000 train_time:85381ms step_avg:53.90ms
step:1585/2000 train_time:85469ms step_avg:53.92ms
step:1586/2000 train_time:85556ms step_avg:53.94ms
step:1587/2000 train_time:85645ms step_avg:53.97ms
step:1588/2000 train_time:85732ms step_avg:53.99ms
step:1589/2000 train_time:85820ms step_avg:54.01ms
step:1590/2000 train_time:85907ms step_avg:54.03ms
step:1591/2000 train_time:85994ms step_avg:54.05ms
step:1592/2000 train_time:86083ms step_avg:54.07ms
step:1593/2000 train_time:86172ms step_avg:54.09ms
step:1594/2000 train_time:86260ms step_avg:54.12ms
step:1595/2000 train_time:86349ms step_avg:54.14ms
step:1596/2000 train_time:86437ms step_avg:54.16ms
step:1597/2000 train_time:86526ms step_avg:54.18ms
step:1598/2000 train_time:86614ms step_avg:54.20ms
step:1599/2000 train_time:86703ms step_avg:54.22ms
step:1600/2000 train_time:86790ms step_avg:54.24ms
step:1601/2000 train_time:86877ms step_avg:54.26ms
step:1602/2000 train_time:86964ms step_avg:54.28ms
step:1603/2000 train_time:87052ms step_avg:54.31ms
step:1604/2000 train_time:87141ms step_avg:54.33ms
step:1605/2000 train_time:87228ms step_avg:54.35ms
step:1606/2000 train_time:87317ms step_avg:54.37ms
step:1607/2000 train_time:87406ms step_avg:54.39ms
step:1608/2000 train_time:87494ms step_avg:54.41ms
step:1609/2000 train_time:87582ms step_avg:54.43ms
step:1610/2000 train_time:87669ms step_avg:54.45ms
step:1611/2000 train_time:87757ms step_avg:54.47ms
step:1612/2000 train_time:87844ms step_avg:54.49ms
step:1613/2000 train_time:87932ms step_avg:54.51ms
step:1614/2000 train_time:88019ms step_avg:54.53ms
step:1615/2000 train_time:88108ms step_avg:54.56ms
step:1616/2000 train_time:88196ms step_avg:54.58ms
step:1617/2000 train_time:88284ms step_avg:54.60ms
step:1618/2000 train_time:88372ms step_avg:54.62ms
step:1619/2000 train_time:88460ms step_avg:54.64ms
step:1620/2000 train_time:88547ms step_avg:54.66ms
step:1621/2000 train_time:88635ms step_avg:54.68ms
step:1622/2000 train_time:88723ms step_avg:54.70ms
step:1623/2000 train_time:88811ms step_avg:54.72ms
step:1624/2000 train_time:88899ms step_avg:54.74ms
step:1625/2000 train_time:88986ms step_avg:54.76ms
step:1626/2000 train_time:89074ms step_avg:54.78ms
step:1627/2000 train_time:89163ms step_avg:54.80ms
step:1628/2000 train_time:89250ms step_avg:54.82ms
step:1629/2000 train_time:89339ms step_avg:54.84ms
step:1630/2000 train_time:89426ms step_avg:54.86ms
step:1631/2000 train_time:89514ms step_avg:54.88ms
step:1632/2000 train_time:89602ms step_avg:54.90ms
step:1633/2000 train_time:89689ms step_avg:54.92ms
step:1634/2000 train_time:89777ms step_avg:54.94ms
step:1635/2000 train_time:89865ms step_avg:54.96ms
step:1636/2000 train_time:89954ms step_avg:54.98ms
step:1637/2000 train_time:90042ms step_avg:55.00ms
step:1638/2000 train_time:90130ms step_avg:55.02ms
step:1639/2000 train_time:90219ms step_avg:55.05ms
step:1640/2000 train_time:90306ms step_avg:55.06ms
step:1641/2000 train_time:90395ms step_avg:55.09ms
step:1642/2000 train_time:90483ms step_avg:55.11ms
step:1643/2000 train_time:90572ms step_avg:55.13ms
step:1644/2000 train_time:90660ms step_avg:55.15ms
step:1645/2000 train_time:90748ms step_avg:55.17ms
step:1646/2000 train_time:90835ms step_avg:55.19ms
step:1647/2000 train_time:90925ms step_avg:55.21ms
step:1648/2000 train_time:91013ms step_avg:55.23ms
step:1649/2000 train_time:91101ms step_avg:55.25ms
step:1650/2000 train_time:91188ms step_avg:55.27ms
step:1651/2000 train_time:91276ms step_avg:55.29ms
step:1652/2000 train_time:91364ms step_avg:55.31ms
step:1653/2000 train_time:91453ms step_avg:55.33ms
step:1654/2000 train_time:91542ms step_avg:55.35ms
step:1655/2000 train_time:91630ms step_avg:55.37ms
step:1656/2000 train_time:91717ms step_avg:55.38ms
step:1657/2000 train_time:91805ms step_avg:55.40ms
step:1658/2000 train_time:91893ms step_avg:55.42ms
step:1659/2000 train_time:91982ms step_avg:55.44ms
step:1660/2000 train_time:92069ms step_avg:55.46ms
step:1661/2000 train_time:92158ms step_avg:55.48ms
step:1662/2000 train_time:92245ms step_avg:55.50ms
step:1663/2000 train_time:92333ms step_avg:55.52ms
step:1664/2000 train_time:92420ms step_avg:55.54ms
step:1665/2000 train_time:92508ms step_avg:55.56ms
step:1666/2000 train_time:92595ms step_avg:55.58ms
step:1667/2000 train_time:92684ms step_avg:55.60ms
step:1668/2000 train_time:92772ms step_avg:55.62ms
step:1669/2000 train_time:92860ms step_avg:55.64ms
step:1670/2000 train_time:92948ms step_avg:55.66ms
step:1671/2000 train_time:93036ms step_avg:55.68ms
step:1672/2000 train_time:93124ms step_avg:55.70ms
step:1673/2000 train_time:93212ms step_avg:55.72ms
step:1674/2000 train_time:93301ms step_avg:55.74ms
step:1675/2000 train_time:93388ms step_avg:55.75ms
step:1676/2000 train_time:93475ms step_avg:55.77ms
step:1677/2000 train_time:93564ms step_avg:55.79ms
step:1678/2000 train_time:93652ms step_avg:55.81ms
step:1679/2000 train_time:93739ms step_avg:55.83ms
step:1680/2000 train_time:93826ms step_avg:55.85ms
step:1681/2000 train_time:93914ms step_avg:55.87ms
step:1682/2000 train_time:94002ms step_avg:55.89ms
step:1683/2000 train_time:94090ms step_avg:55.91ms
step:1684/2000 train_time:94178ms step_avg:55.93ms
step:1685/2000 train_time:94266ms step_avg:55.94ms
step:1686/2000 train_time:94355ms step_avg:55.96ms
step:1687/2000 train_time:94443ms step_avg:55.98ms
step:1688/2000 train_time:94531ms step_avg:56.00ms
step:1689/2000 train_time:94620ms step_avg:56.02ms
step:1690/2000 train_time:94708ms step_avg:56.04ms
step:1691/2000 train_time:94796ms step_avg:56.06ms
step:1692/2000 train_time:94883ms step_avg:56.08ms
step:1693/2000 train_time:94971ms step_avg:56.10ms
step:1694/2000 train_time:95059ms step_avg:56.12ms
step:1695/2000 train_time:95147ms step_avg:56.13ms
step:1696/2000 train_time:95236ms step_avg:56.15ms
step:1697/2000 train_time:95325ms step_avg:56.17ms
step:1698/2000 train_time:95413ms step_avg:56.19ms
step:1699/2000 train_time:95500ms step_avg:56.21ms
step:1700/2000 train_time:95588ms step_avg:56.23ms
step:1701/2000 train_time:95676ms step_avg:56.25ms
step:1702/2000 train_time:95765ms step_avg:56.27ms
step:1703/2000 train_time:95853ms step_avg:56.28ms
step:1704/2000 train_time:95941ms step_avg:56.30ms
step:1705/2000 train_time:96029ms step_avg:56.32ms
step:1706/2000 train_time:96117ms step_avg:56.34ms
step:1707/2000 train_time:96205ms step_avg:56.36ms
step:1708/2000 train_time:96292ms step_avg:56.38ms
step:1709/2000 train_time:96380ms step_avg:56.40ms
step:1710/2000 train_time:96467ms step_avg:56.41ms
step:1711/2000 train_time:96556ms step_avg:56.43ms
step:1712/2000 train_time:96644ms step_avg:56.45ms
step:1713/2000 train_time:96732ms step_avg:56.47ms
step:1714/2000 train_time:96821ms step_avg:56.49ms
step:1715/2000 train_time:96909ms step_avg:56.51ms
step:1716/2000 train_time:96997ms step_avg:56.53ms
step:1717/2000 train_time:97085ms step_avg:56.54ms
step:1718/2000 train_time:97173ms step_avg:56.56ms
step:1719/2000 train_time:97261ms step_avg:56.58ms
step:1720/2000 train_time:97348ms step_avg:56.60ms
step:1721/2000 train_time:97437ms step_avg:56.62ms
step:1722/2000 train_time:97525ms step_avg:56.63ms
step:1723/2000 train_time:97613ms step_avg:56.65ms
step:1724/2000 train_time:97702ms step_avg:56.67ms
step:1725/2000 train_time:97789ms step_avg:56.69ms
step:1726/2000 train_time:97877ms step_avg:56.71ms
step:1727/2000 train_time:97965ms step_avg:56.73ms
step:1728/2000 train_time:98054ms step_avg:56.74ms
step:1729/2000 train_time:98142ms step_avg:56.76ms
step:1730/2000 train_time:98229ms step_avg:56.78ms
step:1731/2000 train_time:98317ms step_avg:56.80ms
step:1732/2000 train_time:98405ms step_avg:56.82ms
step:1733/2000 train_time:98493ms step_avg:56.83ms
step:1734/2000 train_time:98582ms step_avg:56.85ms
step:1735/2000 train_time:98670ms step_avg:56.87ms
step:1736/2000 train_time:98758ms step_avg:56.89ms
step:1737/2000 train_time:98847ms step_avg:56.91ms
step:1738/2000 train_time:98936ms step_avg:56.93ms
step:1739/2000 train_time:99024ms step_avg:56.94ms
step:1740/2000 train_time:99113ms step_avg:56.96ms
step:1741/2000 train_time:99202ms step_avg:56.98ms
step:1742/2000 train_time:99289ms step_avg:57.00ms
step:1743/2000 train_time:99377ms step_avg:57.01ms
step:1744/2000 train_time:99465ms step_avg:57.03ms
step:1745/2000 train_time:99552ms step_avg:57.05ms
step:1746/2000 train_time:99640ms step_avg:57.07ms
step:1747/2000 train_time:99728ms step_avg:57.09ms
step:1748/2000 train_time:99816ms step_avg:57.10ms
step:1749/2000 train_time:99905ms step_avg:57.12ms
step:1750/2000 train_time:99992ms step_avg:57.14ms
step:1750/2000 val_loss:3.3478 train_time:100083ms step_avg:57.19ms
step:1751/2000 train_time:100106ms step_avg:57.17ms
step:1752/2000 train_time:100172ms step_avg:57.18ms
step:1753/2000 train_time:100265ms step_avg:57.20ms
step:1754/2000 train_time:100353ms step_avg:57.21ms
step:1755/2000 train_time:100440ms step_avg:57.23ms
step:1756/2000 train_time:100527ms step_avg:57.25ms
step:1757/2000 train_time:100614ms step_avg:57.26ms
step:1758/2000 train_time:100701ms step_avg:57.28ms
step:1759/2000 train_time:100788ms step_avg:57.30ms
step:1760/2000 train_time:100877ms step_avg:57.32ms
step:1761/2000 train_time:100964ms step_avg:57.33ms
step:1762/2000 train_time:101053ms step_avg:57.35ms
step:1763/2000 train_time:101143ms step_avg:57.37ms
step:1764/2000 train_time:101234ms step_avg:57.39ms
step:1765/2000 train_time:101323ms step_avg:57.41ms
step:1766/2000 train_time:101410ms step_avg:57.42ms
step:1767/2000 train_time:101498ms step_avg:57.44ms
step:1768/2000 train_time:101585ms step_avg:57.46ms
step:1769/2000 train_time:101672ms step_avg:57.47ms
step:1770/2000 train_time:101759ms step_avg:57.49ms
step:1771/2000 train_time:101846ms step_avg:57.51ms
step:1772/2000 train_time:101933ms step_avg:57.52ms
step:1773/2000 train_time:102022ms step_avg:57.54ms
step:1774/2000 train_time:102110ms step_avg:57.56ms
step:1775/2000 train_time:102201ms step_avg:57.58ms
step:1776/2000 train_time:102290ms step_avg:57.60ms
step:1777/2000 train_time:102378ms step_avg:57.61ms
step:1778/2000 train_time:102466ms step_avg:57.63ms
step:1779/2000 train_time:102555ms step_avg:57.65ms
step:1780/2000 train_time:102642ms step_avg:57.66ms
step:1781/2000 train_time:102728ms step_avg:57.68ms
step:1782/2000 train_time:102816ms step_avg:57.70ms
step:1783/2000 train_time:102903ms step_avg:57.71ms
step:1784/2000 train_time:102990ms step_avg:57.73ms
step:1785/2000 train_time:103079ms step_avg:57.75ms
step:1786/2000 train_time:103167ms step_avg:57.76ms
step:1787/2000 train_time:103256ms step_avg:57.78ms
step:1788/2000 train_time:103344ms step_avg:57.80ms
step:1789/2000 train_time:103432ms step_avg:57.82ms
step:1790/2000 train_time:103520ms step_avg:57.83ms
step:1791/2000 train_time:103608ms step_avg:57.85ms
step:1792/2000 train_time:103696ms step_avg:57.87ms
step:1793/2000 train_time:103783ms step_avg:57.88ms
step:1794/2000 train_time:103870ms step_avg:57.90ms
step:1795/2000 train_time:103958ms step_avg:57.92ms
step:1796/2000 train_time:104047ms step_avg:57.93ms
step:1797/2000 train_time:104136ms step_avg:57.95ms
step:1798/2000 train_time:104224ms step_avg:57.97ms
step:1799/2000 train_time:104313ms step_avg:57.98ms
step:1800/2000 train_time:104401ms step_avg:58.00ms
step:1801/2000 train_time:104489ms step_avg:58.02ms
step:1802/2000 train_time:104577ms step_avg:58.03ms
step:1803/2000 train_time:104665ms step_avg:58.05ms
step:1804/2000 train_time:104752ms step_avg:58.07ms
step:1805/2000 train_time:104840ms step_avg:58.08ms
step:1806/2000 train_time:104927ms step_avg:58.10ms
step:1807/2000 train_time:105015ms step_avg:58.12ms
step:1808/2000 train_time:105103ms step_avg:58.13ms
step:1809/2000 train_time:105191ms step_avg:58.15ms
step:1810/2000 train_time:105280ms step_avg:58.17ms
step:1811/2000 train_time:105369ms step_avg:58.18ms
step:1812/2000 train_time:105457ms step_avg:58.20ms
step:1813/2000 train_time:105544ms step_avg:58.22ms
step:1814/2000 train_time:105632ms step_avg:58.23ms
step:1815/2000 train_time:105720ms step_avg:58.25ms
step:1816/2000 train_time:105807ms step_avg:58.26ms
step:1817/2000 train_time:105895ms step_avg:58.28ms
step:1818/2000 train_time:105983ms step_avg:58.30ms
step:1819/2000 train_time:106071ms step_avg:58.31ms
step:1820/2000 train_time:106159ms step_avg:58.33ms
step:1821/2000 train_time:106247ms step_avg:58.35ms
step:1822/2000 train_time:106335ms step_avg:58.36ms
step:1823/2000 train_time:106424ms step_avg:58.38ms
step:1824/2000 train_time:106511ms step_avg:58.39ms
step:1825/2000 train_time:106599ms step_avg:58.41ms
step:1826/2000 train_time:106685ms step_avg:58.43ms
step:1827/2000 train_time:106774ms step_avg:58.44ms
step:1828/2000 train_time:106862ms step_avg:58.46ms
step:1829/2000 train_time:106949ms step_avg:58.47ms
step:1830/2000 train_time:107037ms step_avg:58.49ms
step:1831/2000 train_time:107126ms step_avg:58.51ms
step:1832/2000 train_time:107214ms step_avg:58.52ms
step:1833/2000 train_time:107302ms step_avg:58.54ms
step:1834/2000 train_time:107390ms step_avg:58.56ms
step:1835/2000 train_time:107479ms step_avg:58.57ms
step:1836/2000 train_time:107566ms step_avg:58.59ms
step:1837/2000 train_time:107654ms step_avg:58.60ms
step:1838/2000 train_time:107741ms step_avg:58.62ms
step:1839/2000 train_time:107830ms step_avg:58.63ms
step:1840/2000 train_time:107919ms step_avg:58.65ms
step:1841/2000 train_time:108007ms step_avg:58.67ms
step:1842/2000 train_time:108094ms step_avg:58.68ms
step:1843/2000 train_time:108182ms step_avg:58.70ms
step:1844/2000 train_time:108270ms step_avg:58.71ms
step:1845/2000 train_time:108358ms step_avg:58.73ms
step:1846/2000 train_time:108447ms step_avg:58.75ms
step:1847/2000 train_time:108534ms step_avg:58.76ms
step:1848/2000 train_time:108622ms step_avg:58.78ms
step:1849/2000 train_time:108709ms step_avg:58.79ms
step:1850/2000 train_time:108797ms step_avg:58.81ms
step:1851/2000 train_time:108885ms step_avg:58.82ms
step:1852/2000 train_time:108973ms step_avg:58.84ms
step:1853/2000 train_time:109062ms step_avg:58.86ms
step:1854/2000 train_time:109150ms step_avg:58.87ms
step:1855/2000 train_time:109238ms step_avg:58.89ms
step:1856/2000 train_time:109325ms step_avg:58.90ms
step:1857/2000 train_time:109414ms step_avg:58.92ms
step:1858/2000 train_time:109502ms step_avg:58.94ms
step:1859/2000 train_time:109590ms step_avg:58.95ms
step:1860/2000 train_time:109679ms step_avg:58.97ms
step:1861/2000 train_time:109766ms step_avg:58.98ms
step:1862/2000 train_time:109855ms step_avg:59.00ms
step:1863/2000 train_time:109942ms step_avg:59.01ms
step:1864/2000 train_time:110030ms step_avg:59.03ms
step:1865/2000 train_time:110118ms step_avg:59.04ms
step:1866/2000 train_time:110205ms step_avg:59.06ms
step:1867/2000 train_time:110293ms step_avg:59.07ms
step:1868/2000 train_time:110381ms step_avg:59.09ms
step:1869/2000 train_time:110469ms step_avg:59.11ms
step:1870/2000 train_time:110557ms step_avg:59.12ms
step:1871/2000 train_time:110645ms step_avg:59.14ms
step:1872/2000 train_time:110733ms step_avg:59.15ms
step:1873/2000 train_time:110821ms step_avg:59.17ms
step:1874/2000 train_time:110909ms step_avg:59.18ms
step:1875/2000 train_time:110998ms step_avg:59.20ms
step:1876/2000 train_time:111085ms step_avg:59.21ms
step:1877/2000 train_time:111173ms step_avg:59.23ms
step:1878/2000 train_time:111260ms step_avg:59.24ms
step:1879/2000 train_time:111349ms step_avg:59.26ms
step:1880/2000 train_time:111437ms step_avg:59.27ms
step:1881/2000 train_time:111525ms step_avg:59.29ms
step:1882/2000 train_time:111612ms step_avg:59.31ms
step:1883/2000 train_time:111702ms step_avg:59.32ms
step:1884/2000 train_time:111789ms step_avg:59.34ms
step:1885/2000 train_time:111877ms step_avg:59.35ms
step:1886/2000 train_time:111965ms step_avg:59.37ms
step:1887/2000 train_time:112054ms step_avg:59.38ms
step:1888/2000 train_time:112141ms step_avg:59.40ms
step:1889/2000 train_time:112229ms step_avg:59.41ms
step:1890/2000 train_time:112317ms step_avg:59.43ms
step:1891/2000 train_time:112405ms step_avg:59.44ms
step:1892/2000 train_time:112493ms step_avg:59.46ms
step:1893/2000 train_time:112581ms step_avg:59.47ms
step:1894/2000 train_time:112669ms step_avg:59.49ms
step:1895/2000 train_time:112757ms step_avg:59.50ms
step:1896/2000 train_time:112844ms step_avg:59.52ms
step:1897/2000 train_time:112931ms step_avg:59.53ms
step:1898/2000 train_time:113019ms step_avg:59.55ms
step:1899/2000 train_time:113107ms step_avg:59.56ms
step:1900/2000 train_time:113195ms step_avg:59.58ms
step:1901/2000 train_time:113284ms step_avg:59.59ms
step:1902/2000 train_time:113372ms step_avg:59.61ms
step:1903/2000 train_time:113460ms step_avg:59.62ms
step:1904/2000 train_time:113548ms step_avg:59.64ms
step:1905/2000 train_time:113636ms step_avg:59.65ms
step:1906/2000 train_time:113724ms step_avg:59.67ms
step:1907/2000 train_time:113812ms step_avg:59.68ms
step:1908/2000 train_time:113901ms step_avg:59.70ms
step:1909/2000 train_time:113989ms step_avg:59.71ms
step:1910/2000 train_time:114077ms step_avg:59.73ms
step:1911/2000 train_time:114165ms step_avg:59.74ms
step:1912/2000 train_time:114254ms step_avg:59.76ms
step:1913/2000 train_time:114343ms step_avg:59.77ms
step:1914/2000 train_time:114430ms step_avg:59.79ms
step:1915/2000 train_time:114518ms step_avg:59.80ms
step:1916/2000 train_time:114605ms step_avg:59.81ms
step:1917/2000 train_time:114693ms step_avg:59.83ms
step:1918/2000 train_time:114781ms step_avg:59.84ms
step:1919/2000 train_time:114869ms step_avg:59.86ms
step:1920/2000 train_time:114957ms step_avg:59.87ms
step:1921/2000 train_time:115045ms step_avg:59.89ms
step:1922/2000 train_time:115132ms step_avg:59.90ms
step:1923/2000 train_time:115221ms step_avg:59.92ms
step:1924/2000 train_time:115308ms step_avg:59.93ms
step:1925/2000 train_time:115395ms step_avg:59.95ms
step:1926/2000 train_time:115484ms step_avg:59.96ms
step:1927/2000 train_time:115571ms step_avg:59.97ms
step:1928/2000 train_time:115660ms step_avg:59.99ms
step:1929/2000 train_time:115747ms step_avg:60.00ms
step:1930/2000 train_time:115836ms step_avg:60.02ms
step:1931/2000 train_time:115924ms step_avg:60.03ms
step:1932/2000 train_time:116012ms step_avg:60.05ms
step:1933/2000 train_time:116101ms step_avg:60.06ms
step:1934/2000 train_time:116189ms step_avg:60.08ms
step:1935/2000 train_time:116278ms step_avg:60.09ms
step:1936/2000 train_time:116365ms step_avg:60.11ms
step:1937/2000 train_time:116453ms step_avg:60.12ms
step:1938/2000 train_time:116541ms step_avg:60.13ms
step:1939/2000 train_time:116629ms step_avg:60.15ms
step:1940/2000 train_time:116717ms step_avg:60.16ms
step:1941/2000 train_time:116805ms step_avg:60.18ms
step:1942/2000 train_time:116894ms step_avg:60.19ms
step:1943/2000 train_time:116982ms step_avg:60.21ms
step:1944/2000 train_time:117070ms step_avg:60.22ms
step:1945/2000 train_time:117159ms step_avg:60.24ms
step:1946/2000 train_time:117247ms step_avg:60.25ms
step:1947/2000 train_time:117335ms step_avg:60.26ms
step:1948/2000 train_time:117422ms step_avg:60.28ms
step:1949/2000 train_time:117510ms step_avg:60.29ms
step:1950/2000 train_time:117599ms step_avg:60.31ms
step:1951/2000 train_time:117686ms step_avg:60.32ms
step:1952/2000 train_time:117774ms step_avg:60.33ms
step:1953/2000 train_time:117862ms step_avg:60.35ms
step:1954/2000 train_time:117950ms step_avg:60.36ms
step:1955/2000 train_time:118038ms step_avg:60.38ms
step:1956/2000 train_time:118126ms step_avg:60.39ms
step:1957/2000 train_time:118215ms step_avg:60.41ms
step:1958/2000 train_time:118302ms step_avg:60.42ms
step:1959/2000 train_time:118391ms step_avg:60.43ms
step:1960/2000 train_time:118480ms step_avg:60.45ms
step:1961/2000 train_time:118569ms step_avg:60.46ms
step:1962/2000 train_time:118656ms step_avg:60.48ms
step:1963/2000 train_time:118744ms step_avg:60.49ms
step:1964/2000 train_time:118832ms step_avg:60.51ms
step:1965/2000 train_time:118921ms step_avg:60.52ms
step:1966/2000 train_time:119010ms step_avg:60.53ms
step:1967/2000 train_time:119098ms step_avg:60.55ms
step:1968/2000 train_time:119186ms step_avg:60.56ms
step:1969/2000 train_time:119275ms step_avg:60.58ms
step:1970/2000 train_time:119362ms step_avg:60.59ms
step:1971/2000 train_time:119451ms step_avg:60.60ms
step:1972/2000 train_time:119539ms step_avg:60.62ms
step:1973/2000 train_time:119627ms step_avg:60.63ms
step:1974/2000 train_time:119715ms step_avg:60.65ms
step:1975/2000 train_time:119803ms step_avg:60.66ms
step:1976/2000 train_time:119891ms step_avg:60.67ms
step:1977/2000 train_time:119979ms step_avg:60.69ms
step:1978/2000 train_time:120067ms step_avg:60.70ms
step:1979/2000 train_time:120156ms step_avg:60.72ms
step:1980/2000 train_time:120244ms step_avg:60.73ms
step:1981/2000 train_time:120333ms step_avg:60.74ms
step:1982/2000 train_time:120421ms step_avg:60.76ms
step:1983/2000 train_time:120509ms step_avg:60.77ms
step:1984/2000 train_time:120598ms step_avg:60.79ms
step:1985/2000 train_time:120686ms step_avg:60.80ms
step:1986/2000 train_time:120774ms step_avg:60.81ms
step:1987/2000 train_time:120862ms step_avg:60.83ms
step:1988/2000 train_time:120951ms step_avg:60.84ms
step:1989/2000 train_time:121039ms step_avg:60.85ms
step:1990/2000 train_time:121127ms step_avg:60.87ms
step:1991/2000 train_time:121216ms step_avg:60.88ms
step:1992/2000 train_time:121303ms step_avg:60.90ms
step:1993/2000 train_time:121392ms step_avg:60.91ms
step:1994/2000 train_time:121481ms step_avg:60.92ms
step:1995/2000 train_time:121570ms step_avg:60.94ms
step:1996/2000 train_time:121659ms step_avg:60.95ms
step:1997/2000 train_time:121747ms step_avg:60.96ms
step:1998/2000 train_time:121834ms step_avg:60.98ms
step:1999/2000 train_time:121923ms step_avg:60.99ms
step:2000/2000 train_time:122012ms step_avg:61.01ms
step:2000/2000 val_loss:3.2786 train_time:122104ms step_avg:61.05ms
peak memory allocated: 29512 MiB reserved: 35316 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            grad_slice = torch.empty_like(grad[:rank_size])
            self._reduce_scatter_futures[param] = (
                dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad_slice
            )



    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

    @torch.no_grad()
    def reset_momentum(self, params=None):
        """Reset momentum buffers for specified parameters (or all if 'None')"""
        if params is None:
            # Reset all parameters
            params_to_reset = [p for group in self.param_groups for p in group['params']]
        else:
            params_to_reset = list(params)
        
        for param in params_to_reset:
            if param in self.state:
                state = self.state[param]
                if 'exp_avg' in state:
                    state['exp_avg'].zero_()
                if 'exp_avg_sq' in state:
                    state['exp_avg_sq'].zero_()
                state['step'] = 0

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0
        
        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas  
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    # evaluation and logging
    logs_dir: str = f"logs/12-21-Smooth-Scalars-stps.1960.40"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes,lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    
    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps
    
    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0
    
    if in_transition:
        adam_optimizers[1].transition_steps -= 1
            
    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) 
        is_transition = True
            
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 18:17:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    803193      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    803194      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    803195      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    803196      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    803197      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    803198      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    803199      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    803200      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    803194      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    803195      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    803196      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    803197      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    803198      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    803199      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    803200      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2000 val_loss:10.8350 train_time:0ms step_avg:0.03ms
step:1/2000 train_time:83ms step_avg:82.93ms
step:2/2000 train_time:106ms step_avg:52.98ms
step:3/2000 train_time:126ms step_avg:41.93ms
step:4/2000 train_time:150ms step_avg:37.59ms
step:5/2000 train_time:183ms step_avg:36.64ms
step:6/2000 train_time:265ms step_avg:44.25ms
step:7/2000 train_time:284ms step_avg:40.53ms
step:8/2000 train_time:313ms step_avg:39.18ms
step:9/2000 train_time:346ms step_avg:38.46ms
step:10/2000 train_time:380ms step_avg:37.95ms
step:11/2000 train_time:412ms step_avg:37.49ms
step:12/2000 train_time:446ms step_avg:37.15ms
step:13/2000 train_time:479ms step_avg:36.84ms
step:14/2000 train_time:512ms step_avg:36.60ms
step:15/2000 train_time:546ms step_avg:36.37ms
step:16/2000 train_time:579ms step_avg:36.20ms
step:17/2000 train_time:612ms step_avg:36.01ms
step:18/2000 train_time:646ms step_avg:35.86ms
step:19/2000 train_time:678ms step_avg:35.71ms
step:20/2000 train_time:712ms step_avg:35.59ms
step:21/2000 train_time:745ms step_avg:35.47ms
step:22/2000 train_time:778ms step_avg:35.38ms
step:23/2000 train_time:811ms step_avg:35.28ms
step:24/2000 train_time:845ms step_avg:35.20ms
step:25/2000 train_time:878ms step_avg:35.12ms
step:26/2000 train_time:911ms step_avg:35.04ms
step:27/2000 train_time:944ms step_avg:34.97ms
step:28/2000 train_time:977ms step_avg:34.91ms
step:29/2000 train_time:1010ms step_avg:34.84ms
step:30/2000 train_time:1044ms step_avg:34.79ms
step:31/2000 train_time:1077ms step_avg:34.73ms
step:32/2000 train_time:1110ms step_avg:34.69ms
step:33/2000 train_time:1144ms step_avg:34.66ms
step:34/2000 train_time:1178ms step_avg:34.65ms
step:35/2000 train_time:1213ms step_avg:34.65ms
step:36/2000 train_time:1247ms step_avg:34.63ms
step:37/2000 train_time:1281ms step_avg:34.61ms
step:38/2000 train_time:1314ms step_avg:34.59ms
step:39/2000 train_time:1348ms step_avg:34.56ms
step:40/2000 train_time:1382ms step_avg:34.54ms
step:41/2000 train_time:1415ms step_avg:34.51ms
step:42/2000 train_time:1448ms step_avg:34.48ms
step:43/2000 train_time:1482ms step_avg:34.45ms
step:44/2000 train_time:1515ms step_avg:34.43ms
step:45/2000 train_time:1548ms step_avg:34.40ms
step:46/2000 train_time:1581ms step_avg:34.38ms
step:47/2000 train_time:1614ms step_avg:34.35ms
step:48/2000 train_time:1648ms step_avg:34.33ms
step:49/2000 train_time:1681ms step_avg:34.30ms
step:50/2000 train_time:1714ms step_avg:34.28ms
step:51/2000 train_time:1747ms step_avg:34.26ms
step:52/2000 train_time:1780ms step_avg:34.24ms
step:53/2000 train_time:1814ms step_avg:34.22ms
step:54/2000 train_time:1847ms step_avg:34.20ms
step:55/2000 train_time:1880ms step_avg:34.18ms
step:56/2000 train_time:1913ms step_avg:34.17ms
step:57/2000 train_time:1946ms step_avg:34.15ms
step:58/2000 train_time:1980ms step_avg:34.14ms
step:59/2000 train_time:2013ms step_avg:34.11ms
step:60/2000 train_time:2046ms step_avg:34.10ms
step:61/2000 train_time:2079ms step_avg:34.09ms
step:62/2000 train_time:2113ms step_avg:34.07ms
step:63/2000 train_time:2145ms step_avg:34.06ms
step:64/2000 train_time:2179ms step_avg:34.05ms
step:65/2000 train_time:2212ms step_avg:34.04ms
step:66/2000 train_time:2246ms step_avg:34.04ms
step:67/2000 train_time:2280ms step_avg:34.03ms
step:68/2000 train_time:2314ms step_avg:34.02ms
step:69/2000 train_time:2347ms step_avg:34.01ms
step:70/2000 train_time:2381ms step_avg:34.01ms
step:71/2000 train_time:2414ms step_avg:34.00ms
step:72/2000 train_time:2447ms step_avg:33.99ms
step:73/2000 train_time:2481ms step_avg:33.99ms
step:74/2000 train_time:2515ms step_avg:33.99ms
step:75/2000 train_time:2548ms step_avg:33.97ms
step:76/2000 train_time:2581ms step_avg:33.97ms
step:77/2000 train_time:2615ms step_avg:33.96ms
step:78/2000 train_time:2648ms step_avg:33.94ms
step:79/2000 train_time:2681ms step_avg:33.94ms
step:80/2000 train_time:2714ms step_avg:33.93ms
step:81/2000 train_time:2747ms step_avg:33.92ms
step:82/2000 train_time:2781ms step_avg:33.91ms
step:83/2000 train_time:2813ms step_avg:33.90ms
step:84/2000 train_time:2847ms step_avg:33.89ms
step:85/2000 train_time:2880ms step_avg:33.88ms
step:86/2000 train_time:2913ms step_avg:33.87ms
step:87/2000 train_time:2946ms step_avg:33.86ms
step:88/2000 train_time:2979ms step_avg:33.86ms
step:89/2000 train_time:3012ms step_avg:33.85ms
step:90/2000 train_time:3046ms step_avg:33.84ms
step:91/2000 train_time:3079ms step_avg:33.84ms
step:92/2000 train_time:3112ms step_avg:33.83ms
step:93/2000 train_time:3145ms step_avg:33.82ms
step:94/2000 train_time:3179ms step_avg:33.81ms
step:95/2000 train_time:3212ms step_avg:33.81ms
step:96/2000 train_time:3245ms step_avg:33.80ms
step:97/2000 train_time:3279ms step_avg:33.80ms
step:98/2000 train_time:3313ms step_avg:33.80ms
step:99/2000 train_time:3346ms step_avg:33.80ms
step:100/2000 train_time:3380ms step_avg:33.80ms
step:101/2000 train_time:3413ms step_avg:33.79ms
step:102/2000 train_time:3446ms step_avg:33.79ms
step:103/2000 train_time:3479ms step_avg:33.78ms
step:104/2000 train_time:3513ms step_avg:33.77ms
step:105/2000 train_time:3546ms step_avg:33.77ms
step:106/2000 train_time:3579ms step_avg:33.76ms
step:107/2000 train_time:3612ms step_avg:33.76ms
step:108/2000 train_time:3646ms step_avg:33.76ms
step:109/2000 train_time:3679ms step_avg:33.75ms
step:110/2000 train_time:3713ms step_avg:33.75ms
step:111/2000 train_time:3746ms step_avg:33.74ms
step:112/2000 train_time:3779ms step_avg:33.74ms
step:113/2000 train_time:3812ms step_avg:33.73ms
step:114/2000 train_time:3845ms step_avg:33.73ms
step:115/2000 train_time:3878ms step_avg:33.72ms
step:116/2000 train_time:3912ms step_avg:33.72ms
step:117/2000 train_time:3944ms step_avg:33.71ms
step:118/2000 train_time:3978ms step_avg:33.71ms
step:119/2000 train_time:4011ms step_avg:33.70ms
step:120/2000 train_time:4044ms step_avg:33.70ms
step:121/2000 train_time:4077ms step_avg:33.70ms
step:122/2000 train_time:4111ms step_avg:33.69ms
step:123/2000 train_time:4144ms step_avg:33.69ms
step:124/2000 train_time:4177ms step_avg:33.69ms
step:125/2000 train_time:4210ms step_avg:33.68ms
step:126/2000 train_time:4244ms step_avg:33.68ms
step:127/2000 train_time:4277ms step_avg:33.67ms
step:128/2000 train_time:4310ms step_avg:33.67ms
step:129/2000 train_time:4344ms step_avg:33.67ms
step:130/2000 train_time:4377ms step_avg:33.67ms
step:131/2000 train_time:4410ms step_avg:33.67ms
step:132/2000 train_time:4444ms step_avg:33.66ms
step:133/2000 train_time:4477ms step_avg:33.66ms
step:134/2000 train_time:4510ms step_avg:33.66ms
step:135/2000 train_time:4543ms step_avg:33.65ms
step:136/2000 train_time:4576ms step_avg:33.65ms
step:137/2000 train_time:4609ms step_avg:33.64ms
step:138/2000 train_time:4643ms step_avg:33.64ms
step:139/2000 train_time:4676ms step_avg:33.64ms
step:140/2000 train_time:4709ms step_avg:33.64ms
step:141/2000 train_time:4742ms step_avg:33.63ms
step:142/2000 train_time:4775ms step_avg:33.63ms
step:143/2000 train_time:4808ms step_avg:33.62ms
step:144/2000 train_time:4842ms step_avg:33.62ms
step:145/2000 train_time:4875ms step_avg:33.62ms
step:146/2000 train_time:4908ms step_avg:33.62ms
step:147/2000 train_time:4941ms step_avg:33.61ms
step:148/2000 train_time:4974ms step_avg:33.61ms
step:149/2000 train_time:5008ms step_avg:33.61ms
step:150/2000 train_time:5041ms step_avg:33.61ms
step:151/2000 train_time:5074ms step_avg:33.60ms
step:152/2000 train_time:5107ms step_avg:33.60ms
step:153/2000 train_time:5140ms step_avg:33.60ms
step:154/2000 train_time:5173ms step_avg:33.59ms
step:155/2000 train_time:5206ms step_avg:33.59ms
step:156/2000 train_time:5240ms step_avg:33.59ms
step:157/2000 train_time:5272ms step_avg:33.58ms
step:158/2000 train_time:5306ms step_avg:33.58ms
step:159/2000 train_time:5339ms step_avg:33.58ms
step:160/2000 train_time:5372ms step_avg:33.58ms
step:161/2000 train_time:5405ms step_avg:33.57ms
step:162/2000 train_time:5438ms step_avg:33.57ms
step:163/2000 train_time:5471ms step_avg:33.57ms
step:164/2000 train_time:5505ms step_avg:33.57ms
step:165/2000 train_time:5538ms step_avg:33.56ms
step:166/2000 train_time:5571ms step_avg:33.56ms
step:167/2000 train_time:5604ms step_avg:33.56ms
step:168/2000 train_time:5638ms step_avg:33.56ms
step:169/2000 train_time:5671ms step_avg:33.55ms
step:170/2000 train_time:5704ms step_avg:33.55ms
step:171/2000 train_time:5737ms step_avg:33.55ms
step:172/2000 train_time:5770ms step_avg:33.55ms
step:173/2000 train_time:5803ms step_avg:33.55ms
step:174/2000 train_time:5837ms step_avg:33.55ms
step:175/2000 train_time:5870ms step_avg:33.54ms
step:176/2000 train_time:5903ms step_avg:33.54ms
step:177/2000 train_time:5936ms step_avg:33.54ms
step:178/2000 train_time:5969ms step_avg:33.53ms
step:179/2000 train_time:6002ms step_avg:33.53ms
step:180/2000 train_time:6036ms step_avg:33.53ms
step:181/2000 train_time:6069ms step_avg:33.53ms
step:182/2000 train_time:6102ms step_avg:33.53ms
step:183/2000 train_time:6135ms step_avg:33.53ms
step:184/2000 train_time:6169ms step_avg:33.52ms
step:185/2000 train_time:6202ms step_avg:33.52ms
step:186/2000 train_time:6235ms step_avg:33.52ms
step:187/2000 train_time:6268ms step_avg:33.52ms
step:188/2000 train_time:6302ms step_avg:33.52ms
step:189/2000 train_time:6335ms step_avg:33.52ms
step:190/2000 train_time:6368ms step_avg:33.51ms
step:191/2000 train_time:6401ms step_avg:33.51ms
step:192/2000 train_time:6434ms step_avg:33.51ms
step:193/2000 train_time:6467ms step_avg:33.51ms
step:194/2000 train_time:6501ms step_avg:33.51ms
step:195/2000 train_time:6533ms step_avg:33.50ms
step:196/2000 train_time:6567ms step_avg:33.50ms
step:197/2000 train_time:6600ms step_avg:33.50ms
step:198/2000 train_time:6633ms step_avg:33.50ms
step:199/2000 train_time:6667ms step_avg:33.50ms
step:200/2000 train_time:6700ms step_avg:33.50ms
step:201/2000 train_time:6733ms step_avg:33.50ms
step:202/2000 train_time:6767ms step_avg:33.50ms
step:203/2000 train_time:6800ms step_avg:33.50ms
step:204/2000 train_time:6833ms step_avg:33.50ms
step:205/2000 train_time:6866ms step_avg:33.49ms
step:206/2000 train_time:6900ms step_avg:33.49ms
step:207/2000 train_time:6933ms step_avg:33.49ms
step:208/2000 train_time:6966ms step_avg:33.49ms
step:209/2000 train_time:6999ms step_avg:33.49ms
step:210/2000 train_time:7033ms step_avg:33.49ms
step:211/2000 train_time:7066ms step_avg:33.49ms
step:212/2000 train_time:7099ms step_avg:33.49ms
step:213/2000 train_time:7132ms step_avg:33.48ms
step:214/2000 train_time:7165ms step_avg:33.48ms
step:215/2000 train_time:7198ms step_avg:33.48ms
step:216/2000 train_time:7232ms step_avg:33.48ms
step:217/2000 train_time:7265ms step_avg:33.48ms
step:218/2000 train_time:7299ms step_avg:33.48ms
step:219/2000 train_time:7332ms step_avg:33.48ms
step:220/2000 train_time:7365ms step_avg:33.48ms
step:221/2000 train_time:7398ms step_avg:33.48ms
step:222/2000 train_time:7432ms step_avg:33.48ms
step:223/2000 train_time:7465ms step_avg:33.47ms
step:224/2000 train_time:7498ms step_avg:33.48ms
step:225/2000 train_time:7531ms step_avg:33.47ms
step:226/2000 train_time:7564ms step_avg:33.47ms
step:227/2000 train_time:7597ms step_avg:33.47ms
step:228/2000 train_time:7631ms step_avg:33.47ms
step:229/2000 train_time:7664ms step_avg:33.47ms
step:230/2000 train_time:7698ms step_avg:33.47ms
step:231/2000 train_time:7730ms step_avg:33.47ms
step:232/2000 train_time:7764ms step_avg:33.46ms
step:233/2000 train_time:7797ms step_avg:33.46ms
step:234/2000 train_time:7830ms step_avg:33.46ms
step:235/2000 train_time:7863ms step_avg:33.46ms
step:236/2000 train_time:7897ms step_avg:33.46ms
step:237/2000 train_time:7930ms step_avg:33.46ms
step:238/2000 train_time:7963ms step_avg:33.46ms
step:239/2000 train_time:7996ms step_avg:33.46ms
step:240/2000 train_time:8029ms step_avg:33.46ms
step:241/2000 train_time:8062ms step_avg:33.45ms
step:242/2000 train_time:8096ms step_avg:33.45ms
step:243/2000 train_time:8129ms step_avg:33.45ms
step:244/2000 train_time:8162ms step_avg:33.45ms
step:245/2000 train_time:8195ms step_avg:33.45ms
step:246/2000 train_time:8229ms step_avg:33.45ms
step:247/2000 train_time:8262ms step_avg:33.45ms
step:248/2000 train_time:8295ms step_avg:33.45ms
step:249/2000 train_time:8328ms step_avg:33.45ms
step:250/2000 train_time:8361ms step_avg:33.44ms
step:250/2000 val_loss:4.2649 train_time:8397ms step_avg:33.59ms
step:251/2000 train_time:8419ms step_avg:33.54ms
step:252/2000 train_time:8438ms step_avg:33.48ms
step:253/2000 train_time:8466ms step_avg:33.46ms
step:254/2000 train_time:8500ms step_avg:33.47ms
step:255/2000 train_time:8534ms step_avg:33.47ms
step:256/2000 train_time:8568ms step_avg:33.47ms
step:257/2000 train_time:8601ms step_avg:33.47ms
step:258/2000 train_time:8635ms step_avg:33.47ms
step:259/2000 train_time:8668ms step_avg:33.47ms
step:260/2000 train_time:8701ms step_avg:33.47ms
step:261/2000 train_time:8734ms step_avg:33.46ms
step:262/2000 train_time:8767ms step_avg:33.46ms
step:263/2000 train_time:8800ms step_avg:33.46ms
step:264/2000 train_time:8834ms step_avg:33.46ms
step:265/2000 train_time:8866ms step_avg:33.46ms
step:266/2000 train_time:8900ms step_avg:33.46ms
step:267/2000 train_time:8932ms step_avg:33.45ms
step:268/2000 train_time:8966ms step_avg:33.45ms
step:269/2000 train_time:8999ms step_avg:33.45ms
step:270/2000 train_time:9032ms step_avg:33.45ms
step:271/2000 train_time:9064ms step_avg:33.45ms
step:272/2000 train_time:9097ms step_avg:33.45ms
step:273/2000 train_time:9130ms step_avg:33.44ms
step:274/2000 train_time:9163ms step_avg:33.44ms
step:275/2000 train_time:9196ms step_avg:33.44ms
step:276/2000 train_time:9229ms step_avg:33.44ms
step:277/2000 train_time:9262ms step_avg:33.44ms
step:278/2000 train_time:9295ms step_avg:33.43ms
step:279/2000 train_time:9328ms step_avg:33.43ms
step:280/2000 train_time:9361ms step_avg:33.43ms
step:281/2000 train_time:9394ms step_avg:33.43ms
step:282/2000 train_time:9428ms step_avg:33.43ms
step:283/2000 train_time:9462ms step_avg:33.43ms
step:284/2000 train_time:9495ms step_avg:33.43ms
step:285/2000 train_time:9528ms step_avg:33.43ms
step:286/2000 train_time:9563ms step_avg:33.44ms
step:287/2000 train_time:9596ms step_avg:33.44ms
step:288/2000 train_time:9630ms step_avg:33.44ms
step:289/2000 train_time:9662ms step_avg:33.43ms
step:290/2000 train_time:9696ms step_avg:33.43ms
step:291/2000 train_time:9729ms step_avg:33.43ms
step:292/2000 train_time:9762ms step_avg:33.43ms
step:293/2000 train_time:9795ms step_avg:33.43ms
step:294/2000 train_time:9828ms step_avg:33.43ms
step:295/2000 train_time:9862ms step_avg:33.43ms
step:296/2000 train_time:9895ms step_avg:33.43ms
step:297/2000 train_time:9927ms step_avg:33.43ms
step:298/2000 train_time:9961ms step_avg:33.42ms
step:299/2000 train_time:9993ms step_avg:33.42ms
step:300/2000 train_time:10026ms step_avg:33.42ms
step:301/2000 train_time:10059ms step_avg:33.42ms
step:302/2000 train_time:10093ms step_avg:33.42ms
step:303/2000 train_time:10125ms step_avg:33.42ms
step:304/2000 train_time:10159ms step_avg:33.42ms
step:305/2000 train_time:10191ms step_avg:33.41ms
step:306/2000 train_time:10225ms step_avg:33.41ms
step:307/2000 train_time:10258ms step_avg:33.41ms
step:308/2000 train_time:10291ms step_avg:33.41ms
step:309/2000 train_time:10324ms step_avg:33.41ms
step:310/2000 train_time:10357ms step_avg:33.41ms
step:311/2000 train_time:10390ms step_avg:33.41ms
step:312/2000 train_time:10423ms step_avg:33.41ms
step:313/2000 train_time:10456ms step_avg:33.41ms
step:314/2000 train_time:10490ms step_avg:33.41ms
step:315/2000 train_time:10523ms step_avg:33.41ms
step:316/2000 train_time:10557ms step_avg:33.41ms
step:317/2000 train_time:10590ms step_avg:33.41ms
step:318/2000 train_time:10624ms step_avg:33.41ms
step:319/2000 train_time:10657ms step_avg:33.41ms
step:320/2000 train_time:10690ms step_avg:33.41ms
step:321/2000 train_time:10724ms step_avg:33.41ms
step:322/2000 train_time:10757ms step_avg:33.41ms
step:323/2000 train_time:10790ms step_avg:33.40ms
step:324/2000 train_time:10823ms step_avg:33.41ms
step:325/2000 train_time:10857ms step_avg:33.40ms
step:326/2000 train_time:10890ms step_avg:33.41ms
step:327/2000 train_time:10923ms step_avg:33.40ms
step:328/2000 train_time:10956ms step_avg:33.40ms
step:329/2000 train_time:10989ms step_avg:33.40ms
step:330/2000 train_time:11022ms step_avg:33.40ms
step:331/2000 train_time:11055ms step_avg:33.40ms
step:332/2000 train_time:11088ms step_avg:33.40ms
step:333/2000 train_time:11121ms step_avg:33.40ms
step:334/2000 train_time:11154ms step_avg:33.40ms
step:335/2000 train_time:11187ms step_avg:33.39ms
step:336/2000 train_time:11220ms step_avg:33.39ms
step:337/2000 train_time:11253ms step_avg:33.39ms
step:338/2000 train_time:11287ms step_avg:33.39ms
step:339/2000 train_time:11320ms step_avg:33.39ms
step:340/2000 train_time:11353ms step_avg:33.39ms
step:341/2000 train_time:11386ms step_avg:33.39ms
step:342/2000 train_time:11419ms step_avg:33.39ms
step:343/2000 train_time:11452ms step_avg:33.39ms
step:344/2000 train_time:11486ms step_avg:33.39ms
step:345/2000 train_time:11519ms step_avg:33.39ms
step:346/2000 train_time:11552ms step_avg:33.39ms
step:347/2000 train_time:11585ms step_avg:33.39ms
step:348/2000 train_time:11619ms step_avg:33.39ms
step:349/2000 train_time:11652ms step_avg:33.39ms
step:350/2000 train_time:11685ms step_avg:33.39ms
step:351/2000 train_time:11718ms step_avg:33.38ms
step:352/2000 train_time:11751ms step_avg:33.38ms
step:353/2000 train_time:11784ms step_avg:33.38ms
step:354/2000 train_time:11818ms step_avg:33.38ms
step:355/2000 train_time:11851ms step_avg:33.38ms
step:356/2000 train_time:11884ms step_avg:33.38ms
step:357/2000 train_time:11917ms step_avg:33.38ms
step:358/2000 train_time:11950ms step_avg:33.38ms
step:359/2000 train_time:11983ms step_avg:33.38ms
step:360/2000 train_time:12016ms step_avg:33.38ms
step:361/2000 train_time:12049ms step_avg:33.38ms
step:362/2000 train_time:12082ms step_avg:33.38ms
step:363/2000 train_time:12115ms step_avg:33.38ms
step:364/2000 train_time:12148ms step_avg:33.37ms
step:365/2000 train_time:12181ms step_avg:33.37ms
step:366/2000 train_time:12215ms step_avg:33.37ms
step:367/2000 train_time:12248ms step_avg:33.37ms
step:368/2000 train_time:12281ms step_avg:33.37ms
step:369/2000 train_time:12313ms step_avg:33.37ms
step:370/2000 train_time:12347ms step_avg:33.37ms
step:371/2000 train_time:12380ms step_avg:33.37ms
step:372/2000 train_time:12413ms step_avg:33.37ms
step:373/2000 train_time:12446ms step_avg:33.37ms
step:374/2000 train_time:12480ms step_avg:33.37ms
step:375/2000 train_time:12513ms step_avg:33.37ms
step:376/2000 train_time:12546ms step_avg:33.37ms
step:377/2000 train_time:12579ms step_avg:33.37ms
step:378/2000 train_time:12613ms step_avg:33.37ms
step:379/2000 train_time:12646ms step_avg:33.37ms
step:380/2000 train_time:12679ms step_avg:33.37ms
step:381/2000 train_time:12712ms step_avg:33.37ms
step:382/2000 train_time:12746ms step_avg:33.37ms
step:383/2000 train_time:12779ms step_avg:33.36ms
step:384/2000 train_time:12812ms step_avg:33.36ms
step:385/2000 train_time:12845ms step_avg:33.36ms
step:386/2000 train_time:12878ms step_avg:33.36ms
step:387/2000 train_time:12911ms step_avg:33.36ms
step:388/2000 train_time:12944ms step_avg:33.36ms
step:389/2000 train_time:12977ms step_avg:33.36ms
step:390/2000 train_time:13010ms step_avg:33.36ms
step:391/2000 train_time:13043ms step_avg:33.36ms
step:392/2000 train_time:13076ms step_avg:33.36ms
step:393/2000 train_time:13109ms step_avg:33.36ms
step:394/2000 train_time:13143ms step_avg:33.36ms
step:395/2000 train_time:13176ms step_avg:33.36ms
step:396/2000 train_time:13210ms step_avg:33.36ms
step:397/2000 train_time:13243ms step_avg:33.36ms
step:398/2000 train_time:13276ms step_avg:33.36ms
step:399/2000 train_time:13309ms step_avg:33.36ms
step:400/2000 train_time:13342ms step_avg:33.36ms
step:401/2000 train_time:13375ms step_avg:33.35ms
step:402/2000 train_time:13408ms step_avg:33.35ms
step:403/2000 train_time:13442ms step_avg:33.35ms
step:404/2000 train_time:13475ms step_avg:33.36ms
step:405/2000 train_time:13508ms step_avg:33.35ms
step:406/2000 train_time:13542ms step_avg:33.35ms
step:407/2000 train_time:13575ms step_avg:33.35ms
step:408/2000 train_time:13608ms step_avg:33.35ms
step:409/2000 train_time:13641ms step_avg:33.35ms
step:410/2000 train_time:13674ms step_avg:33.35ms
step:411/2000 train_time:13707ms step_avg:33.35ms
step:412/2000 train_time:13740ms step_avg:33.35ms
step:413/2000 train_time:13773ms step_avg:33.35ms
step:414/2000 train_time:13806ms step_avg:33.35ms
step:415/2000 train_time:13840ms step_avg:33.35ms
step:416/2000 train_time:13873ms step_avg:33.35ms
step:417/2000 train_time:13906ms step_avg:33.35ms
step:418/2000 train_time:13940ms step_avg:33.35ms
step:419/2000 train_time:13973ms step_avg:33.35ms
step:420/2000 train_time:14006ms step_avg:33.35ms
step:421/2000 train_time:14039ms step_avg:33.35ms
step:422/2000 train_time:14072ms step_avg:33.35ms
step:423/2000 train_time:14105ms step_avg:33.35ms
step:424/2000 train_time:14139ms step_avg:33.35ms
step:425/2000 train_time:14172ms step_avg:33.35ms
step:426/2000 train_time:14205ms step_avg:33.35ms
step:427/2000 train_time:14238ms step_avg:33.35ms
step:428/2000 train_time:14272ms step_avg:33.34ms
step:429/2000 train_time:14305ms step_avg:33.34ms
step:430/2000 train_time:14338ms step_avg:33.34ms
step:431/2000 train_time:14371ms step_avg:33.34ms
step:432/2000 train_time:14405ms step_avg:33.34ms
step:433/2000 train_time:14438ms step_avg:33.34ms
step:434/2000 train_time:14471ms step_avg:33.34ms
step:435/2000 train_time:14504ms step_avg:33.34ms
step:436/2000 train_time:14537ms step_avg:33.34ms
step:437/2000 train_time:14570ms step_avg:33.34ms
step:438/2000 train_time:14604ms step_avg:33.34ms
step:439/2000 train_time:14637ms step_avg:33.34ms
step:440/2000 train_time:14670ms step_avg:33.34ms
step:441/2000 train_time:14703ms step_avg:33.34ms
step:442/2000 train_time:14737ms step_avg:33.34ms
step:443/2000 train_time:14770ms step_avg:33.34ms
step:444/2000 train_time:14803ms step_avg:33.34ms
step:445/2000 train_time:14836ms step_avg:33.34ms
step:446/2000 train_time:14869ms step_avg:33.34ms
step:447/2000 train_time:14902ms step_avg:33.34ms
step:448/2000 train_time:14936ms step_avg:33.34ms
step:449/2000 train_time:14969ms step_avg:33.34ms
step:450/2000 train_time:15003ms step_avg:33.34ms
step:451/2000 train_time:15036ms step_avg:33.34ms
step:452/2000 train_time:15070ms step_avg:33.34ms
step:453/2000 train_time:15103ms step_avg:33.34ms
step:454/2000 train_time:15136ms step_avg:33.34ms
step:455/2000 train_time:15169ms step_avg:33.34ms
step:456/2000 train_time:15202ms step_avg:33.34ms
step:457/2000 train_time:15235ms step_avg:33.34ms
step:458/2000 train_time:15269ms step_avg:33.34ms
step:459/2000 train_time:15302ms step_avg:33.34ms
step:460/2000 train_time:15335ms step_avg:33.34ms
step:461/2000 train_time:15368ms step_avg:33.34ms
step:462/2000 train_time:15401ms step_avg:33.34ms
step:463/2000 train_time:15434ms step_avg:33.34ms
step:464/2000 train_time:15468ms step_avg:33.34ms
step:465/2000 train_time:15501ms step_avg:33.33ms
step:466/2000 train_time:15534ms step_avg:33.33ms
step:467/2000 train_time:15567ms step_avg:33.33ms
step:468/2000 train_time:15601ms step_avg:33.34ms
step:469/2000 train_time:15634ms step_avg:33.33ms
step:470/2000 train_time:15667ms step_avg:33.33ms
step:471/2000 train_time:15700ms step_avg:33.33ms
step:472/2000 train_time:15733ms step_avg:33.33ms
step:473/2000 train_time:15767ms step_avg:33.33ms
step:474/2000 train_time:15800ms step_avg:33.33ms
step:475/2000 train_time:15833ms step_avg:33.33ms
step:476/2000 train_time:15866ms step_avg:33.33ms
step:477/2000 train_time:15899ms step_avg:33.33ms
step:478/2000 train_time:15933ms step_avg:33.33ms
step:479/2000 train_time:15966ms step_avg:33.33ms
step:480/2000 train_time:15999ms step_avg:33.33ms
step:481/2000 train_time:16031ms step_avg:33.33ms
step:482/2000 train_time:16065ms step_avg:33.33ms
step:483/2000 train_time:16098ms step_avg:33.33ms
step:484/2000 train_time:16131ms step_avg:33.33ms
step:485/2000 train_time:16164ms step_avg:33.33ms
step:486/2000 train_time:16197ms step_avg:33.33ms
step:487/2000 train_time:16230ms step_avg:33.33ms
step:488/2000 train_time:16264ms step_avg:33.33ms
step:489/2000 train_time:16297ms step_avg:33.33ms
step:490/2000 train_time:16330ms step_avg:33.33ms
step:491/2000 train_time:16363ms step_avg:33.33ms
step:492/2000 train_time:16396ms step_avg:33.33ms
step:493/2000 train_time:16429ms step_avg:33.33ms
step:494/2000 train_time:16463ms step_avg:33.33ms
step:495/2000 train_time:16496ms step_avg:33.32ms
step:496/2000 train_time:16529ms step_avg:33.33ms
step:497/2000 train_time:16562ms step_avg:33.32ms
step:498/2000 train_time:16595ms step_avg:33.32ms
step:499/2000 train_time:16629ms step_avg:33.32ms
step:500/2000 train_time:16662ms step_avg:33.32ms
step:500/2000 val_loss:3.9969 train_time:16698ms step_avg:33.40ms
step:501/2000 train_time:16717ms step_avg:33.37ms
step:502/2000 train_time:16737ms step_avg:33.34ms
step:503/2000 train_time:16765ms step_avg:33.33ms
step:504/2000 train_time:16799ms step_avg:33.33ms
step:505/2000 train_time:16834ms step_avg:33.33ms
step:506/2000 train_time:16868ms step_avg:33.34ms
step:507/2000 train_time:16901ms step_avg:33.33ms
step:508/2000 train_time:16934ms step_avg:33.33ms
step:509/2000 train_time:16967ms step_avg:33.33ms
step:510/2000 train_time:17000ms step_avg:33.33ms
step:511/2000 train_time:17033ms step_avg:33.33ms
step:512/2000 train_time:17066ms step_avg:33.33ms
step:513/2000 train_time:17099ms step_avg:33.33ms
step:514/2000 train_time:17133ms step_avg:33.33ms
step:515/2000 train_time:17166ms step_avg:33.33ms
step:516/2000 train_time:17199ms step_avg:33.33ms
step:517/2000 train_time:17232ms step_avg:33.33ms
step:518/2000 train_time:17265ms step_avg:33.33ms
step:519/2000 train_time:17298ms step_avg:33.33ms
step:520/2000 train_time:17331ms step_avg:33.33ms
step:521/2000 train_time:17364ms step_avg:33.33ms
step:522/2000 train_time:17397ms step_avg:33.33ms
step:523/2000 train_time:17429ms step_avg:33.33ms
step:524/2000 train_time:17463ms step_avg:33.33ms
step:525/2000 train_time:17495ms step_avg:33.32ms
step:526/2000 train_time:17529ms step_avg:33.32ms
step:527/2000 train_time:17561ms step_avg:33.32ms
step:528/2000 train_time:17594ms step_avg:33.32ms
step:529/2000 train_time:17628ms step_avg:33.32ms
step:530/2000 train_time:17661ms step_avg:33.32ms
step:531/2000 train_time:17694ms step_avg:33.32ms
step:532/2000 train_time:17728ms step_avg:33.32ms
step:533/2000 train_time:17761ms step_avg:33.32ms
step:534/2000 train_time:17795ms step_avg:33.32ms
step:535/2000 train_time:17828ms step_avg:33.32ms
step:536/2000 train_time:17862ms step_avg:33.33ms
step:537/2000 train_time:17896ms step_avg:33.32ms
step:538/2000 train_time:17929ms step_avg:33.33ms
step:539/2000 train_time:17962ms step_avg:33.32ms
step:540/2000 train_time:17995ms step_avg:33.32ms
step:541/2000 train_time:18028ms step_avg:33.32ms
step:542/2000 train_time:18062ms step_avg:33.32ms
step:543/2000 train_time:18095ms step_avg:33.32ms
step:544/2000 train_time:18128ms step_avg:33.32ms
step:545/2000 train_time:18161ms step_avg:33.32ms
step:546/2000 train_time:18194ms step_avg:33.32ms
step:547/2000 train_time:18227ms step_avg:33.32ms
step:548/2000 train_time:18260ms step_avg:33.32ms
step:549/2000 train_time:18293ms step_avg:33.32ms
step:550/2000 train_time:18327ms step_avg:33.32ms
step:551/2000 train_time:18359ms step_avg:33.32ms
step:552/2000 train_time:18393ms step_avg:33.32ms
step:553/2000 train_time:18425ms step_avg:33.32ms
step:554/2000 train_time:18459ms step_avg:33.32ms
step:555/2000 train_time:18491ms step_avg:33.32ms
step:556/2000 train_time:18525ms step_avg:33.32ms
step:557/2000 train_time:18558ms step_avg:33.32ms
step:558/2000 train_time:18591ms step_avg:33.32ms
step:559/2000 train_time:18624ms step_avg:33.32ms
step:560/2000 train_time:18658ms step_avg:33.32ms
step:561/2000 train_time:18691ms step_avg:33.32ms
step:562/2000 train_time:18724ms step_avg:33.32ms
step:563/2000 train_time:18758ms step_avg:33.32ms
step:564/2000 train_time:18791ms step_avg:33.32ms
step:565/2000 train_time:18825ms step_avg:33.32ms
step:566/2000 train_time:18859ms step_avg:33.32ms
step:567/2000 train_time:18892ms step_avg:33.32ms
step:568/2000 train_time:18925ms step_avg:33.32ms
step:569/2000 train_time:18958ms step_avg:33.32ms
step:570/2000 train_time:18992ms step_avg:33.32ms
step:571/2000 train_time:19025ms step_avg:33.32ms
step:572/2000 train_time:19058ms step_avg:33.32ms
step:573/2000 train_time:19091ms step_avg:33.32ms
step:574/2000 train_time:19125ms step_avg:33.32ms
step:575/2000 train_time:19158ms step_avg:33.32ms
step:576/2000 train_time:19192ms step_avg:33.32ms
step:577/2000 train_time:19224ms step_avg:33.32ms
step:578/2000 train_time:19257ms step_avg:33.32ms
step:579/2000 train_time:19290ms step_avg:33.32ms
step:580/2000 train_time:19324ms step_avg:33.32ms
step:581/2000 train_time:19356ms step_avg:33.32ms
step:582/2000 train_time:19390ms step_avg:33.32ms
step:583/2000 train_time:19423ms step_avg:33.32ms
step:584/2000 train_time:19456ms step_avg:33.32ms
step:585/2000 train_time:19489ms step_avg:33.31ms
step:586/2000 train_time:19522ms step_avg:33.31ms
step:587/2000 train_time:19555ms step_avg:33.31ms
step:588/2000 train_time:19589ms step_avg:33.31ms
step:589/2000 train_time:19622ms step_avg:33.31ms
step:590/2000 train_time:19655ms step_avg:33.31ms
step:591/2000 train_time:19688ms step_avg:33.31ms
step:592/2000 train_time:19722ms step_avg:33.31ms
step:593/2000 train_time:19755ms step_avg:33.31ms
step:594/2000 train_time:19788ms step_avg:33.31ms
step:595/2000 train_time:19822ms step_avg:33.31ms
step:596/2000 train_time:19855ms step_avg:33.31ms
step:597/2000 train_time:19888ms step_avg:33.31ms
step:598/2000 train_time:19922ms step_avg:33.31ms
step:599/2000 train_time:19955ms step_avg:33.31ms
step:600/2000 train_time:19988ms step_avg:33.31ms
step:601/2000 train_time:20021ms step_avg:33.31ms
step:602/2000 train_time:20055ms step_avg:33.31ms
step:603/2000 train_time:20088ms step_avg:33.31ms
step:604/2000 train_time:20121ms step_avg:33.31ms
step:605/2000 train_time:20154ms step_avg:33.31ms
step:606/2000 train_time:20187ms step_avg:33.31ms
step:607/2000 train_time:20220ms step_avg:33.31ms
step:608/2000 train_time:20254ms step_avg:33.31ms
step:609/2000 train_time:20286ms step_avg:33.31ms
step:610/2000 train_time:20320ms step_avg:33.31ms
step:611/2000 train_time:20353ms step_avg:33.31ms
step:612/2000 train_time:20386ms step_avg:33.31ms
step:613/2000 train_time:20419ms step_avg:33.31ms
step:614/2000 train_time:20452ms step_avg:33.31ms
step:615/2000 train_time:20485ms step_avg:33.31ms
step:616/2000 train_time:20518ms step_avg:33.31ms
step:617/2000 train_time:20551ms step_avg:33.31ms
step:618/2000 train_time:20584ms step_avg:33.31ms
step:619/2000 train_time:20617ms step_avg:33.31ms
step:620/2000 train_time:20651ms step_avg:33.31ms
step:621/2000 train_time:20684ms step_avg:33.31ms
step:622/2000 train_time:20717ms step_avg:33.31ms
step:623/2000 train_time:20750ms step_avg:33.31ms
step:624/2000 train_time:20784ms step_avg:33.31ms
step:625/2000 train_time:20816ms step_avg:33.31ms
step:626/2000 train_time:20850ms step_avg:33.31ms
step:627/2000 train_time:20883ms step_avg:33.31ms
step:628/2000 train_time:20916ms step_avg:33.31ms
step:629/2000 train_time:20949ms step_avg:33.31ms
step:630/2000 train_time:20982ms step_avg:33.31ms
step:631/2000 train_time:21015ms step_avg:33.30ms
step:632/2000 train_time:21049ms step_avg:33.30ms
step:633/2000 train_time:21082ms step_avg:33.31ms
step:634/2000 train_time:21116ms step_avg:33.31ms
step:635/2000 train_time:21149ms step_avg:33.30ms
step:636/2000 train_time:21182ms step_avg:33.31ms
step:637/2000 train_time:21215ms step_avg:33.30ms
step:638/2000 train_time:21248ms step_avg:33.30ms
step:639/2000 train_time:21281ms step_avg:33.30ms
step:640/2000 train_time:21314ms step_avg:33.30ms
step:641/2000 train_time:21347ms step_avg:33.30ms
step:642/2000 train_time:21381ms step_avg:33.30ms
step:643/2000 train_time:21414ms step_avg:33.30ms
step:644/2000 train_time:21447ms step_avg:33.30ms
step:645/2000 train_time:21480ms step_avg:33.30ms
step:646/2000 train_time:21513ms step_avg:33.30ms
step:647/2000 train_time:21546ms step_avg:33.30ms
step:648/2000 train_time:21580ms step_avg:33.30ms
step:649/2000 train_time:21613ms step_avg:33.30ms
step:650/2000 train_time:21646ms step_avg:33.30ms
step:651/2000 train_time:21679ms step_avg:33.30ms
step:652/2000 train_time:21713ms step_avg:33.30ms
step:653/2000 train_time:21746ms step_avg:33.30ms
step:654/2000 train_time:21779ms step_avg:33.30ms
step:655/2000 train_time:21813ms step_avg:33.30ms
step:656/2000 train_time:21872ms step_avg:33.34ms
step:657/2000 train_time:21932ms step_avg:33.38ms
step:658/2000 train_time:21993ms step_avg:33.42ms
step:659/2000 train_time:22055ms step_avg:33.47ms
step:660/2000 train_time:22114ms step_avg:33.51ms
step:661/2000 train_time:22175ms step_avg:33.55ms
step:662/2000 train_time:22235ms step_avg:33.59ms
step:663/2000 train_time:22295ms step_avg:33.63ms
step:664/2000 train_time:22355ms step_avg:33.67ms
step:665/2000 train_time:22415ms step_avg:33.71ms
step:666/2000 train_time:22474ms step_avg:33.75ms
step:667/2000 train_time:22535ms step_avg:33.79ms
step:668/2000 train_time:22595ms step_avg:33.82ms
step:669/2000 train_time:22656ms step_avg:33.86ms
step:670/2000 train_time:22715ms step_avg:33.90ms
step:671/2000 train_time:22776ms step_avg:33.94ms
step:672/2000 train_time:22835ms step_avg:33.98ms
step:673/2000 train_time:22895ms step_avg:34.02ms
step:674/2000 train_time:22954ms step_avg:34.06ms
step:675/2000 train_time:23015ms step_avg:34.10ms
step:676/2000 train_time:23075ms step_avg:34.14ms
step:677/2000 train_time:23136ms step_avg:34.17ms
step:678/2000 train_time:23195ms step_avg:34.21ms
step:679/2000 train_time:23255ms step_avg:34.25ms
step:680/2000 train_time:23315ms step_avg:34.29ms
step:681/2000 train_time:23375ms step_avg:34.32ms
step:682/2000 train_time:23435ms step_avg:34.36ms
step:683/2000 train_time:23495ms step_avg:34.40ms
step:684/2000 train_time:23555ms step_avg:34.44ms
step:685/2000 train_time:23615ms step_avg:34.48ms
step:686/2000 train_time:23675ms step_avg:34.51ms
step:687/2000 train_time:23736ms step_avg:34.55ms
step:688/2000 train_time:23795ms step_avg:34.59ms
step:689/2000 train_time:23855ms step_avg:34.62ms
step:690/2000 train_time:23914ms step_avg:34.66ms
step:691/2000 train_time:23975ms step_avg:34.70ms
step:692/2000 train_time:24034ms step_avg:34.73ms
step:693/2000 train_time:24095ms step_avg:34.77ms
step:694/2000 train_time:24155ms step_avg:34.81ms
step:695/2000 train_time:24216ms step_avg:34.84ms
step:696/2000 train_time:24275ms step_avg:34.88ms
step:697/2000 train_time:24335ms step_avg:34.91ms
step:698/2000 train_time:24395ms step_avg:34.95ms
step:699/2000 train_time:24455ms step_avg:34.99ms
step:700/2000 train_time:24515ms step_avg:35.02ms
step:701/2000 train_time:24576ms step_avg:35.06ms
step:702/2000 train_time:24635ms step_avg:35.09ms
step:703/2000 train_time:24696ms step_avg:35.13ms
step:704/2000 train_time:24756ms step_avg:35.16ms
step:705/2000 train_time:24816ms step_avg:35.20ms
step:706/2000 train_time:24875ms step_avg:35.23ms
step:707/2000 train_time:24936ms step_avg:35.27ms
step:708/2000 train_time:24995ms step_avg:35.30ms
step:709/2000 train_time:25055ms step_avg:35.34ms
step:710/2000 train_time:25115ms step_avg:35.37ms
step:711/2000 train_time:25176ms step_avg:35.41ms
step:712/2000 train_time:25235ms step_avg:35.44ms
step:713/2000 train_time:25296ms step_avg:35.48ms
step:714/2000 train_time:25355ms step_avg:35.51ms
step:715/2000 train_time:25416ms step_avg:35.55ms
step:716/2000 train_time:25475ms step_avg:35.58ms
step:717/2000 train_time:25536ms step_avg:35.62ms
step:718/2000 train_time:25595ms step_avg:35.65ms
step:719/2000 train_time:25656ms step_avg:35.68ms
step:720/2000 train_time:25716ms step_avg:35.72ms
step:721/2000 train_time:25778ms step_avg:35.75ms
step:722/2000 train_time:25837ms step_avg:35.79ms
step:723/2000 train_time:25898ms step_avg:35.82ms
step:724/2000 train_time:25957ms step_avg:35.85ms
step:725/2000 train_time:26017ms step_avg:35.89ms
step:726/2000 train_time:26076ms step_avg:35.92ms
step:727/2000 train_time:26137ms step_avg:35.95ms
step:728/2000 train_time:26196ms step_avg:35.98ms
step:729/2000 train_time:26257ms step_avg:36.02ms
step:730/2000 train_time:26317ms step_avg:36.05ms
step:731/2000 train_time:26377ms step_avg:36.08ms
step:732/2000 train_time:26436ms step_avg:36.11ms
step:733/2000 train_time:26497ms step_avg:36.15ms
step:734/2000 train_time:26556ms step_avg:36.18ms
step:735/2000 train_time:26617ms step_avg:36.21ms
step:736/2000 train_time:26676ms step_avg:36.25ms
step:737/2000 train_time:26737ms step_avg:36.28ms
step:738/2000 train_time:26797ms step_avg:36.31ms
step:739/2000 train_time:26857ms step_avg:36.34ms
step:740/2000 train_time:26916ms step_avg:36.37ms
step:741/2000 train_time:26977ms step_avg:36.41ms
step:742/2000 train_time:27037ms step_avg:36.44ms
step:743/2000 train_time:27097ms step_avg:36.47ms
step:744/2000 train_time:27157ms step_avg:36.50ms
step:745/2000 train_time:27217ms step_avg:36.53ms
step:746/2000 train_time:27276ms step_avg:36.56ms
step:747/2000 train_time:27336ms step_avg:36.59ms
step:748/2000 train_time:27396ms step_avg:36.63ms
step:749/2000 train_time:27457ms step_avg:36.66ms
step:750/2000 train_time:27516ms step_avg:36.69ms
step:750/2000 val_loss:3.8223 train_time:27579ms step_avg:36.77ms
step:751/2000 train_time:27599ms step_avg:36.75ms
step:752/2000 train_time:27638ms step_avg:36.75ms
step:753/2000 train_time:27701ms step_avg:36.79ms
step:754/2000 train_time:27763ms step_avg:36.82ms
step:755/2000 train_time:27823ms step_avg:36.85ms
step:756/2000 train_time:27882ms step_avg:36.88ms
step:757/2000 train_time:27943ms step_avg:36.91ms
step:758/2000 train_time:28002ms step_avg:36.94ms
step:759/2000 train_time:28062ms step_avg:36.97ms
step:760/2000 train_time:28121ms step_avg:37.00ms
step:761/2000 train_time:28181ms step_avg:37.03ms
step:762/2000 train_time:28241ms step_avg:37.06ms
step:763/2000 train_time:28301ms step_avg:37.09ms
step:764/2000 train_time:28360ms step_avg:37.12ms
step:765/2000 train_time:28420ms step_avg:37.15ms
step:766/2000 train_time:28480ms step_avg:37.18ms
step:767/2000 train_time:28541ms step_avg:37.21ms
step:768/2000 train_time:28602ms step_avg:37.24ms
step:769/2000 train_time:28665ms step_avg:37.28ms
step:770/2000 train_time:28725ms step_avg:37.31ms
step:771/2000 train_time:28787ms step_avg:37.34ms
step:772/2000 train_time:28846ms step_avg:37.37ms
step:773/2000 train_time:28907ms step_avg:37.40ms
step:774/2000 train_time:28967ms step_avg:37.42ms
step:775/2000 train_time:29027ms step_avg:37.45ms
step:776/2000 train_time:29086ms step_avg:37.48ms
step:777/2000 train_time:29147ms step_avg:37.51ms
step:778/2000 train_time:29206ms step_avg:37.54ms
step:779/2000 train_time:29266ms step_avg:37.57ms
step:780/2000 train_time:29324ms step_avg:37.60ms
step:781/2000 train_time:29384ms step_avg:37.62ms
step:782/2000 train_time:29444ms step_avg:37.65ms
step:783/2000 train_time:29505ms step_avg:37.68ms
step:784/2000 train_time:29565ms step_avg:37.71ms
step:785/2000 train_time:29626ms step_avg:37.74ms
step:786/2000 train_time:29686ms step_avg:37.77ms
step:787/2000 train_time:29747ms step_avg:37.80ms
step:788/2000 train_time:29808ms step_avg:37.83ms
step:789/2000 train_time:29869ms step_avg:37.86ms
step:790/2000 train_time:29927ms step_avg:37.88ms
step:791/2000 train_time:29988ms step_avg:37.91ms
step:792/2000 train_time:30047ms step_avg:37.94ms
step:793/2000 train_time:30107ms step_avg:37.97ms
step:794/2000 train_time:30166ms step_avg:37.99ms
step:795/2000 train_time:30226ms step_avg:38.02ms
step:796/2000 train_time:30285ms step_avg:38.05ms
step:797/2000 train_time:30346ms step_avg:38.08ms
step:798/2000 train_time:30406ms step_avg:38.10ms
step:799/2000 train_time:30466ms step_avg:38.13ms
step:800/2000 train_time:30525ms step_avg:38.16ms
step:801/2000 train_time:30586ms step_avg:38.19ms
step:802/2000 train_time:30646ms step_avg:38.21ms
step:803/2000 train_time:30708ms step_avg:38.24ms
step:804/2000 train_time:30768ms step_avg:38.27ms
step:805/2000 train_time:30829ms step_avg:38.30ms
step:806/2000 train_time:30888ms step_avg:38.32ms
step:807/2000 train_time:30948ms step_avg:38.35ms
step:808/2000 train_time:31007ms step_avg:38.38ms
step:809/2000 train_time:31067ms step_avg:38.40ms
step:810/2000 train_time:31127ms step_avg:38.43ms
step:811/2000 train_time:31187ms step_avg:38.45ms
step:812/2000 train_time:31245ms step_avg:38.48ms
step:813/2000 train_time:31306ms step_avg:38.51ms
step:814/2000 train_time:31365ms step_avg:38.53ms
step:815/2000 train_time:31425ms step_avg:38.56ms
step:816/2000 train_time:31484ms step_avg:38.58ms
step:817/2000 train_time:31545ms step_avg:38.61ms
step:818/2000 train_time:31604ms step_avg:38.64ms
step:819/2000 train_time:31666ms step_avg:38.66ms
step:820/2000 train_time:31726ms step_avg:38.69ms
step:821/2000 train_time:31787ms step_avg:38.72ms
step:822/2000 train_time:31846ms step_avg:38.74ms
step:823/2000 train_time:31907ms step_avg:38.77ms
step:824/2000 train_time:31966ms step_avg:38.79ms
step:825/2000 train_time:32026ms step_avg:38.82ms
step:826/2000 train_time:32086ms step_avg:38.84ms
step:827/2000 train_time:32146ms step_avg:38.87ms
step:828/2000 train_time:32205ms step_avg:38.90ms
step:829/2000 train_time:32266ms step_avg:38.92ms
step:830/2000 train_time:32325ms step_avg:38.95ms
step:831/2000 train_time:32385ms step_avg:38.97ms
step:832/2000 train_time:32444ms step_avg:39.00ms
step:833/2000 train_time:32505ms step_avg:39.02ms
step:834/2000 train_time:32565ms step_avg:39.05ms
step:835/2000 train_time:32627ms step_avg:39.07ms
step:836/2000 train_time:32686ms step_avg:39.10ms
step:837/2000 train_time:32748ms step_avg:39.12ms
step:838/2000 train_time:32807ms step_avg:39.15ms
step:839/2000 train_time:32868ms step_avg:39.17ms
step:840/2000 train_time:32927ms step_avg:39.20ms
step:841/2000 train_time:32988ms step_avg:39.22ms
step:842/2000 train_time:33047ms step_avg:39.25ms
step:843/2000 train_time:33108ms step_avg:39.27ms
step:844/2000 train_time:33167ms step_avg:39.30ms
step:845/2000 train_time:33228ms step_avg:39.32ms
step:846/2000 train_time:33287ms step_avg:39.35ms
step:847/2000 train_time:33347ms step_avg:39.37ms
step:848/2000 train_time:33406ms step_avg:39.39ms
step:849/2000 train_time:33467ms step_avg:39.42ms
step:850/2000 train_time:33527ms step_avg:39.44ms
step:851/2000 train_time:33588ms step_avg:39.47ms
step:852/2000 train_time:33647ms step_avg:39.49ms
step:853/2000 train_time:33709ms step_avg:39.52ms
step:854/2000 train_time:33768ms step_avg:39.54ms
step:855/2000 train_time:33828ms step_avg:39.57ms
step:856/2000 train_time:33888ms step_avg:39.59ms
step:857/2000 train_time:33948ms step_avg:39.61ms
step:858/2000 train_time:34007ms step_avg:39.64ms
step:859/2000 train_time:34067ms step_avg:39.66ms
step:860/2000 train_time:34126ms step_avg:39.68ms
step:861/2000 train_time:34186ms step_avg:39.71ms
step:862/2000 train_time:34246ms step_avg:39.73ms
step:863/2000 train_time:34306ms step_avg:39.75ms
step:864/2000 train_time:34365ms step_avg:39.77ms
step:865/2000 train_time:34426ms step_avg:39.80ms
step:866/2000 train_time:34486ms step_avg:39.82ms
step:867/2000 train_time:34547ms step_avg:39.85ms
step:868/2000 train_time:34607ms step_avg:39.87ms
step:869/2000 train_time:34667ms step_avg:39.89ms
step:870/2000 train_time:34726ms step_avg:39.92ms
step:871/2000 train_time:34786ms step_avg:39.94ms
step:872/2000 train_time:34845ms step_avg:39.96ms
step:873/2000 train_time:34907ms step_avg:39.99ms
step:874/2000 train_time:34966ms step_avg:40.01ms
step:875/2000 train_time:35027ms step_avg:40.03ms
step:876/2000 train_time:35086ms step_avg:40.05ms
step:877/2000 train_time:35146ms step_avg:40.08ms
step:878/2000 train_time:35206ms step_avg:40.10ms
step:879/2000 train_time:35266ms step_avg:40.12ms
step:880/2000 train_time:35326ms step_avg:40.14ms
step:881/2000 train_time:35386ms step_avg:40.17ms
step:882/2000 train_time:35444ms step_avg:40.19ms
step:883/2000 train_time:35505ms step_avg:40.21ms
step:884/2000 train_time:35565ms step_avg:40.23ms
step:885/2000 train_time:35626ms step_avg:40.26ms
step:886/2000 train_time:35686ms step_avg:40.28ms
step:887/2000 train_time:35746ms step_avg:40.30ms
step:888/2000 train_time:35806ms step_avg:40.32ms
step:889/2000 train_time:35867ms step_avg:40.35ms
step:890/2000 train_time:35926ms step_avg:40.37ms
step:891/2000 train_time:35987ms step_avg:40.39ms
step:892/2000 train_time:36046ms step_avg:40.41ms
step:893/2000 train_time:36107ms step_avg:40.43ms
step:894/2000 train_time:36167ms step_avg:40.45ms
step:895/2000 train_time:36227ms step_avg:40.48ms
step:896/2000 train_time:36286ms step_avg:40.50ms
step:897/2000 train_time:36346ms step_avg:40.52ms
step:898/2000 train_time:36405ms step_avg:40.54ms
step:899/2000 train_time:36466ms step_avg:40.56ms
step:900/2000 train_time:36525ms step_avg:40.58ms
step:901/2000 train_time:36587ms step_avg:40.61ms
step:902/2000 train_time:36645ms step_avg:40.63ms
step:903/2000 train_time:36706ms step_avg:40.65ms
step:904/2000 train_time:36766ms step_avg:40.67ms
step:905/2000 train_time:36827ms step_avg:40.69ms
step:906/2000 train_time:36887ms step_avg:40.71ms
step:907/2000 train_time:36948ms step_avg:40.74ms
step:908/2000 train_time:37007ms step_avg:40.76ms
step:909/2000 train_time:37067ms step_avg:40.78ms
step:910/2000 train_time:37127ms step_avg:40.80ms
step:911/2000 train_time:37187ms step_avg:40.82ms
step:912/2000 train_time:37246ms step_avg:40.84ms
step:913/2000 train_time:37306ms step_avg:40.86ms
step:914/2000 train_time:37366ms step_avg:40.88ms
step:915/2000 train_time:37426ms step_avg:40.90ms
step:916/2000 train_time:37486ms step_avg:40.92ms
step:917/2000 train_time:37546ms step_avg:40.94ms
step:918/2000 train_time:37606ms step_avg:40.97ms
step:919/2000 train_time:37667ms step_avg:40.99ms
step:920/2000 train_time:37726ms step_avg:41.01ms
step:921/2000 train_time:37786ms step_avg:41.03ms
step:922/2000 train_time:37846ms step_avg:41.05ms
step:923/2000 train_time:37906ms step_avg:41.07ms
step:924/2000 train_time:37966ms step_avg:41.09ms
step:925/2000 train_time:38027ms step_avg:41.11ms
step:926/2000 train_time:38086ms step_avg:41.13ms
step:927/2000 train_time:38146ms step_avg:41.15ms
step:928/2000 train_time:38206ms step_avg:41.17ms
step:929/2000 train_time:38267ms step_avg:41.19ms
step:930/2000 train_time:38325ms step_avg:41.21ms
step:931/2000 train_time:38386ms step_avg:41.23ms
step:932/2000 train_time:38445ms step_avg:41.25ms
step:933/2000 train_time:38506ms step_avg:41.27ms
step:934/2000 train_time:38566ms step_avg:41.29ms
step:935/2000 train_time:38627ms step_avg:41.31ms
step:936/2000 train_time:38686ms step_avg:41.33ms
step:937/2000 train_time:38747ms step_avg:41.35ms
step:938/2000 train_time:38806ms step_avg:41.37ms
step:939/2000 train_time:38867ms step_avg:41.39ms
step:940/2000 train_time:38927ms step_avg:41.41ms
step:941/2000 train_time:38987ms step_avg:41.43ms
step:942/2000 train_time:39047ms step_avg:41.45ms
step:943/2000 train_time:39108ms step_avg:41.47ms
step:944/2000 train_time:39167ms step_avg:41.49ms
step:945/2000 train_time:39227ms step_avg:41.51ms
step:946/2000 train_time:39286ms step_avg:41.53ms
step:947/2000 train_time:39347ms step_avg:41.55ms
step:948/2000 train_time:39406ms step_avg:41.57ms
step:949/2000 train_time:39467ms step_avg:41.59ms
step:950/2000 train_time:39526ms step_avg:41.61ms
step:951/2000 train_time:39587ms step_avg:41.63ms
step:952/2000 train_time:39646ms step_avg:41.65ms
step:953/2000 train_time:39708ms step_avg:41.67ms
step:954/2000 train_time:39767ms step_avg:41.68ms
step:955/2000 train_time:39827ms step_avg:41.70ms
step:956/2000 train_time:39887ms step_avg:41.72ms
step:957/2000 train_time:39948ms step_avg:41.74ms
step:958/2000 train_time:40007ms step_avg:41.76ms
step:959/2000 train_time:40068ms step_avg:41.78ms
step:960/2000 train_time:40127ms step_avg:41.80ms
step:961/2000 train_time:40188ms step_avg:41.82ms
step:962/2000 train_time:40247ms step_avg:41.84ms
step:963/2000 train_time:40307ms step_avg:41.86ms
step:964/2000 train_time:40366ms step_avg:41.87ms
step:965/2000 train_time:40427ms step_avg:41.89ms
step:966/2000 train_time:40486ms step_avg:41.91ms
step:967/2000 train_time:40546ms step_avg:41.93ms
step:968/2000 train_time:40606ms step_avg:41.95ms
step:969/2000 train_time:40666ms step_avg:41.97ms
step:970/2000 train_time:40726ms step_avg:41.99ms
step:971/2000 train_time:40786ms step_avg:42.00ms
step:972/2000 train_time:40846ms step_avg:42.02ms
step:973/2000 train_time:40906ms step_avg:42.04ms
step:974/2000 train_time:40966ms step_avg:42.06ms
step:975/2000 train_time:41027ms step_avg:42.08ms
step:976/2000 train_time:41087ms step_avg:42.10ms
step:977/2000 train_time:41148ms step_avg:42.12ms
step:978/2000 train_time:41207ms step_avg:42.13ms
step:979/2000 train_time:41268ms step_avg:42.15ms
step:980/2000 train_time:41326ms step_avg:42.17ms
step:981/2000 train_time:41388ms step_avg:42.19ms
step:982/2000 train_time:41447ms step_avg:42.21ms
step:983/2000 train_time:41507ms step_avg:42.23ms
step:984/2000 train_time:41567ms step_avg:42.24ms
step:985/2000 train_time:41627ms step_avg:42.26ms
step:986/2000 train_time:41686ms step_avg:42.28ms
step:987/2000 train_time:41746ms step_avg:42.30ms
step:988/2000 train_time:41806ms step_avg:42.31ms
step:989/2000 train_time:41867ms step_avg:42.33ms
step:990/2000 train_time:41926ms step_avg:42.35ms
step:991/2000 train_time:41987ms step_avg:42.37ms
step:992/2000 train_time:42046ms step_avg:42.39ms
step:993/2000 train_time:42106ms step_avg:42.40ms
step:994/2000 train_time:42166ms step_avg:42.42ms
step:995/2000 train_time:42226ms step_avg:42.44ms
step:996/2000 train_time:42285ms step_avg:42.45ms
step:997/2000 train_time:42346ms step_avg:42.47ms
step:998/2000 train_time:42406ms step_avg:42.49ms
step:999/2000 train_time:42466ms step_avg:42.51ms
step:1000/2000 train_time:42526ms step_avg:42.53ms
step:1000/2000 val_loss:3.6760 train_time:42589ms step_avg:42.59ms
step:1001/2000 train_time:42609ms step_avg:42.57ms
step:1002/2000 train_time:42651ms step_avg:42.57ms
step:1003/2000 train_time:42713ms step_avg:42.59ms
step:1004/2000 train_time:42776ms step_avg:42.61ms
step:1005/2000 train_time:42837ms step_avg:42.62ms
step:1006/2000 train_time:42897ms step_avg:42.64ms
step:1007/2000 train_time:42957ms step_avg:42.66ms
step:1008/2000 train_time:43017ms step_avg:42.68ms
step:1009/2000 train_time:43077ms step_avg:42.69ms
step:1010/2000 train_time:43136ms step_avg:42.71ms
step:1011/2000 train_time:43196ms step_avg:42.73ms
step:1012/2000 train_time:43255ms step_avg:42.74ms
step:1013/2000 train_time:43315ms step_avg:42.76ms
step:1014/2000 train_time:43375ms step_avg:42.78ms
step:1015/2000 train_time:43435ms step_avg:42.79ms
step:1016/2000 train_time:43495ms step_avg:42.81ms
step:1017/2000 train_time:43557ms step_avg:42.83ms
step:1018/2000 train_time:43618ms step_avg:42.85ms
step:1019/2000 train_time:43681ms step_avg:42.87ms
step:1020/2000 train_time:43741ms step_avg:42.88ms
step:1021/2000 train_time:43803ms step_avg:42.90ms
step:1022/2000 train_time:43862ms step_avg:42.92ms
step:1023/2000 train_time:43922ms step_avg:42.93ms
step:1024/2000 train_time:43981ms step_avg:42.95ms
step:1025/2000 train_time:44041ms step_avg:42.97ms
step:1026/2000 train_time:44100ms step_avg:42.98ms
step:1027/2000 train_time:44160ms step_avg:43.00ms
step:1028/2000 train_time:44219ms step_avg:43.01ms
step:1029/2000 train_time:44279ms step_avg:43.03ms
step:1030/2000 train_time:44338ms step_avg:43.05ms
step:1031/2000 train_time:44398ms step_avg:43.06ms
step:1032/2000 train_time:44458ms step_avg:43.08ms
step:1033/2000 train_time:44519ms step_avg:43.10ms
step:1034/2000 train_time:44580ms step_avg:43.11ms
step:1035/2000 train_time:44642ms step_avg:43.13ms
step:1036/2000 train_time:44702ms step_avg:43.15ms
step:1037/2000 train_time:44764ms step_avg:43.17ms
step:1038/2000 train_time:44823ms step_avg:43.18ms
step:1039/2000 train_time:44884ms step_avg:43.20ms
step:1040/2000 train_time:44943ms step_avg:43.21ms
step:1041/2000 train_time:45003ms step_avg:43.23ms
step:1042/2000 train_time:45063ms step_avg:43.25ms
step:1043/2000 train_time:45123ms step_avg:43.26ms
step:1044/2000 train_time:45181ms step_avg:43.28ms
step:1045/2000 train_time:45241ms step_avg:43.29ms
step:1046/2000 train_time:45300ms step_avg:43.31ms
step:1047/2000 train_time:45361ms step_avg:43.32ms
step:1048/2000 train_time:45421ms step_avg:43.34ms
step:1049/2000 train_time:45482ms step_avg:43.36ms
step:1050/2000 train_time:45541ms step_avg:43.37ms
step:1051/2000 train_time:45602ms step_avg:43.39ms
step:1052/2000 train_time:45663ms step_avg:43.41ms
step:1053/2000 train_time:45723ms step_avg:43.42ms
step:1054/2000 train_time:45783ms step_avg:43.44ms
step:1055/2000 train_time:45843ms step_avg:43.45ms
step:1056/2000 train_time:45902ms step_avg:43.47ms
step:1057/2000 train_time:45963ms step_avg:43.48ms
step:1058/2000 train_time:46022ms step_avg:43.50ms
step:1059/2000 train_time:46083ms step_avg:43.52ms
step:1060/2000 train_time:46142ms step_avg:43.53ms
step:1061/2000 train_time:46201ms step_avg:43.55ms
step:1062/2000 train_time:46261ms step_avg:43.56ms
step:1063/2000 train_time:46321ms step_avg:43.58ms
step:1064/2000 train_time:46380ms step_avg:43.59ms
step:1065/2000 train_time:46441ms step_avg:43.61ms
step:1066/2000 train_time:46500ms step_avg:43.62ms
step:1067/2000 train_time:46561ms step_avg:43.64ms
step:1068/2000 train_time:46621ms step_avg:43.65ms
step:1069/2000 train_time:46682ms step_avg:43.67ms
step:1070/2000 train_time:46742ms step_avg:43.68ms
step:1071/2000 train_time:46803ms step_avg:43.70ms
step:1072/2000 train_time:46862ms step_avg:43.71ms
step:1073/2000 train_time:46923ms step_avg:43.73ms
step:1074/2000 train_time:46982ms step_avg:43.75ms
step:1075/2000 train_time:47042ms step_avg:43.76ms
step:1076/2000 train_time:47102ms step_avg:43.78ms
step:1077/2000 train_time:47162ms step_avg:43.79ms
step:1078/2000 train_time:47221ms step_avg:43.80ms
step:1079/2000 train_time:47281ms step_avg:43.82ms
step:1080/2000 train_time:47340ms step_avg:43.83ms
step:1081/2000 train_time:47401ms step_avg:43.85ms
step:1082/2000 train_time:47460ms step_avg:43.86ms
step:1083/2000 train_time:47520ms step_avg:43.88ms
step:1084/2000 train_time:47580ms step_avg:43.89ms
step:1085/2000 train_time:47641ms step_avg:43.91ms
step:1086/2000 train_time:47701ms step_avg:43.92ms
step:1087/2000 train_time:47763ms step_avg:43.94ms
step:1088/2000 train_time:47822ms step_avg:43.95ms
step:1089/2000 train_time:47883ms step_avg:43.97ms
step:1090/2000 train_time:47943ms step_avg:43.98ms
step:1091/2000 train_time:48003ms step_avg:44.00ms
step:1092/2000 train_time:48063ms step_avg:44.01ms
step:1093/2000 train_time:48124ms step_avg:44.03ms
step:1094/2000 train_time:48183ms step_avg:44.04ms
step:1095/2000 train_time:48243ms step_avg:44.06ms
step:1096/2000 train_time:48302ms step_avg:44.07ms
step:1097/2000 train_time:48362ms step_avg:44.09ms
step:1098/2000 train_time:48422ms step_avg:44.10ms
step:1099/2000 train_time:48483ms step_avg:44.12ms
step:1100/2000 train_time:48542ms step_avg:44.13ms
step:1101/2000 train_time:48602ms step_avg:44.14ms
step:1102/2000 train_time:48662ms step_avg:44.16ms
step:1103/2000 train_time:48723ms step_avg:44.17ms
step:1104/2000 train_time:48782ms step_avg:44.19ms
step:1105/2000 train_time:48842ms step_avg:44.20ms
step:1106/2000 train_time:48902ms step_avg:44.21ms
step:1107/2000 train_time:48963ms step_avg:44.23ms
step:1108/2000 train_time:49022ms step_avg:44.24ms
step:1109/2000 train_time:49082ms step_avg:44.26ms
step:1110/2000 train_time:49141ms step_avg:44.27ms
step:1111/2000 train_time:49202ms step_avg:44.29ms
step:1112/2000 train_time:49261ms step_avg:44.30ms
step:1113/2000 train_time:49321ms step_avg:44.31ms
step:1114/2000 train_time:49380ms step_avg:44.33ms
step:1115/2000 train_time:49441ms step_avg:44.34ms
step:1116/2000 train_time:49500ms step_avg:44.36ms
step:1117/2000 train_time:49561ms step_avg:44.37ms
step:1118/2000 train_time:49621ms step_avg:44.38ms
step:1119/2000 train_time:49683ms step_avg:44.40ms
step:1120/2000 train_time:49742ms step_avg:44.41ms
step:1121/2000 train_time:49802ms step_avg:44.43ms
step:1122/2000 train_time:49862ms step_avg:44.44ms
step:1123/2000 train_time:49922ms step_avg:44.45ms
step:1124/2000 train_time:49981ms step_avg:44.47ms
step:1125/2000 train_time:50042ms step_avg:44.48ms
step:1126/2000 train_time:50101ms step_avg:44.49ms
step:1127/2000 train_time:50162ms step_avg:44.51ms
step:1128/2000 train_time:50222ms step_avg:44.52ms
step:1129/2000 train_time:50282ms step_avg:44.54ms
step:1130/2000 train_time:50341ms step_avg:44.55ms
step:1131/2000 train_time:50401ms step_avg:44.56ms
step:1132/2000 train_time:50461ms step_avg:44.58ms
step:1133/2000 train_time:50522ms step_avg:44.59ms
step:1134/2000 train_time:50581ms step_avg:44.60ms
step:1135/2000 train_time:50642ms step_avg:44.62ms
step:1136/2000 train_time:50702ms step_avg:44.63ms
step:1137/2000 train_time:50763ms step_avg:44.65ms
step:1138/2000 train_time:50822ms step_avg:44.66ms
step:1139/2000 train_time:50883ms step_avg:44.67ms
step:1140/2000 train_time:50942ms step_avg:44.69ms
step:1141/2000 train_time:51003ms step_avg:44.70ms
step:1142/2000 train_time:51062ms step_avg:44.71ms
step:1143/2000 train_time:51122ms step_avg:44.73ms
step:1144/2000 train_time:51182ms step_avg:44.74ms
step:1145/2000 train_time:51243ms step_avg:44.75ms
step:1146/2000 train_time:51302ms step_avg:44.77ms
step:1147/2000 train_time:51362ms step_avg:44.78ms
step:1148/2000 train_time:51421ms step_avg:44.79ms
step:1149/2000 train_time:51482ms step_avg:44.81ms
step:1150/2000 train_time:51541ms step_avg:44.82ms
step:1151/2000 train_time:51602ms step_avg:44.83ms
step:1152/2000 train_time:51662ms step_avg:44.85ms
step:1153/2000 train_time:51722ms step_avg:44.86ms
step:1154/2000 train_time:51782ms step_avg:44.87ms
step:1155/2000 train_time:51843ms step_avg:44.89ms
step:1156/2000 train_time:51902ms step_avg:44.90ms
step:1157/2000 train_time:51963ms step_avg:44.91ms
step:1158/2000 train_time:52022ms step_avg:44.92ms
step:1159/2000 train_time:52082ms step_avg:44.94ms
step:1160/2000 train_time:52142ms step_avg:44.95ms
step:1161/2000 train_time:52202ms step_avg:44.96ms
step:1162/2000 train_time:52262ms step_avg:44.98ms
step:1163/2000 train_time:52322ms step_avg:44.99ms
step:1164/2000 train_time:52381ms step_avg:45.00ms
step:1165/2000 train_time:52442ms step_avg:45.01ms
step:1166/2000 train_time:52501ms step_avg:45.03ms
step:1167/2000 train_time:52561ms step_avg:45.04ms
step:1168/2000 train_time:52620ms step_avg:45.05ms
step:1169/2000 train_time:52681ms step_avg:45.07ms
step:1170/2000 train_time:52741ms step_avg:45.08ms
step:1171/2000 train_time:52802ms step_avg:45.09ms
step:1172/2000 train_time:52861ms step_avg:45.10ms
step:1173/2000 train_time:52922ms step_avg:45.12ms
step:1174/2000 train_time:52981ms step_avg:45.13ms
step:1175/2000 train_time:53042ms step_avg:45.14ms
step:1176/2000 train_time:53101ms step_avg:45.15ms
step:1177/2000 train_time:53162ms step_avg:45.17ms
step:1178/2000 train_time:53221ms step_avg:45.18ms
step:1179/2000 train_time:53281ms step_avg:45.19ms
step:1180/2000 train_time:53341ms step_avg:45.20ms
step:1181/2000 train_time:53401ms step_avg:45.22ms
step:1182/2000 train_time:53461ms step_avg:45.23ms
step:1183/2000 train_time:53521ms step_avg:45.24ms
step:1184/2000 train_time:53581ms step_avg:45.25ms
step:1185/2000 train_time:53642ms step_avg:45.27ms
step:1186/2000 train_time:53701ms step_avg:45.28ms
step:1187/2000 train_time:53762ms step_avg:45.29ms
step:1188/2000 train_time:53822ms step_avg:45.30ms
step:1189/2000 train_time:53883ms step_avg:45.32ms
step:1190/2000 train_time:53942ms step_avg:45.33ms
step:1191/2000 train_time:54002ms step_avg:45.34ms
step:1192/2000 train_time:54062ms step_avg:45.35ms
step:1193/2000 train_time:54123ms step_avg:45.37ms
step:1194/2000 train_time:54182ms step_avg:45.38ms
step:1195/2000 train_time:54243ms step_avg:45.39ms
step:1196/2000 train_time:54302ms step_avg:45.40ms
step:1197/2000 train_time:54363ms step_avg:45.42ms
step:1198/2000 train_time:54422ms step_avg:45.43ms
step:1199/2000 train_time:54482ms step_avg:45.44ms
step:1200/2000 train_time:54541ms step_avg:45.45ms
step:1201/2000 train_time:54602ms step_avg:45.46ms
step:1202/2000 train_time:54661ms step_avg:45.48ms
step:1203/2000 train_time:54722ms step_avg:45.49ms
step:1204/2000 train_time:54782ms step_avg:45.50ms
step:1205/2000 train_time:54843ms step_avg:45.51ms
step:1206/2000 train_time:54902ms step_avg:45.52ms
step:1207/2000 train_time:54963ms step_avg:45.54ms
step:1208/2000 train_time:55022ms step_avg:45.55ms
step:1209/2000 train_time:55083ms step_avg:45.56ms
step:1210/2000 train_time:55143ms step_avg:45.57ms
step:1211/2000 train_time:55202ms step_avg:45.58ms
step:1212/2000 train_time:55261ms step_avg:45.60ms
step:1213/2000 train_time:55323ms step_avg:45.61ms
step:1214/2000 train_time:55382ms step_avg:45.62ms
step:1215/2000 train_time:55442ms step_avg:45.63ms
step:1216/2000 train_time:55502ms step_avg:45.64ms
step:1217/2000 train_time:55562ms step_avg:45.66ms
step:1218/2000 train_time:55621ms step_avg:45.67ms
step:1219/2000 train_time:55681ms step_avg:45.68ms
step:1220/2000 train_time:55740ms step_avg:45.69ms
step:1221/2000 train_time:55801ms step_avg:45.70ms
step:1222/2000 train_time:55861ms step_avg:45.71ms
step:1223/2000 train_time:55921ms step_avg:45.72ms
step:1224/2000 train_time:55980ms step_avg:45.74ms
step:1225/2000 train_time:56041ms step_avg:45.75ms
step:1226/2000 train_time:56100ms step_avg:45.76ms
step:1227/2000 train_time:56161ms step_avg:45.77ms
step:1228/2000 train_time:56220ms step_avg:45.78ms
step:1229/2000 train_time:56281ms step_avg:45.79ms
step:1230/2000 train_time:56340ms step_avg:45.80ms
step:1231/2000 train_time:56400ms step_avg:45.82ms
step:1232/2000 train_time:56460ms step_avg:45.83ms
step:1233/2000 train_time:56520ms step_avg:45.84ms
step:1234/2000 train_time:56579ms step_avg:45.85ms
step:1235/2000 train_time:56640ms step_avg:45.86ms
step:1236/2000 train_time:56699ms step_avg:45.87ms
step:1237/2000 train_time:56760ms step_avg:45.89ms
step:1238/2000 train_time:56820ms step_avg:45.90ms
step:1239/2000 train_time:56882ms step_avg:45.91ms
step:1240/2000 train_time:56942ms step_avg:45.92ms
step:1241/2000 train_time:57002ms step_avg:45.93ms
step:1242/2000 train_time:57061ms step_avg:45.94ms
step:1243/2000 train_time:57122ms step_avg:45.96ms
step:1244/2000 train_time:57182ms step_avg:45.97ms
step:1245/2000 train_time:57242ms step_avg:45.98ms
step:1246/2000 train_time:57301ms step_avg:45.99ms
step:1247/2000 train_time:57361ms step_avg:46.00ms
step:1248/2000 train_time:57421ms step_avg:46.01ms
step:1249/2000 train_time:57482ms step_avg:46.02ms
step:1250/2000 train_time:57541ms step_avg:46.03ms
step:1250/2000 val_loss:3.5594 train_time:57604ms step_avg:46.08ms
step:1251/2000 train_time:57624ms step_avg:46.06ms
step:1252/2000 train_time:57664ms step_avg:46.06ms
step:1253/2000 train_time:57727ms step_avg:46.07ms
step:1254/2000 train_time:57790ms step_avg:46.08ms
step:1255/2000 train_time:57851ms step_avg:46.10ms
step:1256/2000 train_time:57913ms step_avg:46.11ms
step:1257/2000 train_time:57973ms step_avg:46.12ms
step:1258/2000 train_time:58031ms step_avg:46.13ms
step:1259/2000 train_time:58091ms step_avg:46.14ms
step:1260/2000 train_time:58150ms step_avg:46.15ms
step:1261/2000 train_time:58210ms step_avg:46.16ms
step:1262/2000 train_time:58270ms step_avg:46.17ms
step:1263/2000 train_time:58330ms step_avg:46.18ms
step:1264/2000 train_time:58389ms step_avg:46.19ms
step:1265/2000 train_time:58450ms step_avg:46.21ms
step:1266/2000 train_time:58510ms step_avg:46.22ms
step:1267/2000 train_time:58572ms step_avg:46.23ms
step:1268/2000 train_time:58633ms step_avg:46.24ms
step:1269/2000 train_time:58695ms step_avg:46.25ms
step:1270/2000 train_time:58755ms step_avg:46.26ms
step:1271/2000 train_time:58816ms step_avg:46.28ms
step:1272/2000 train_time:58876ms step_avg:46.29ms
step:1273/2000 train_time:58937ms step_avg:46.30ms
step:1274/2000 train_time:58996ms step_avg:46.31ms
step:1275/2000 train_time:59055ms step_avg:46.32ms
step:1276/2000 train_time:59114ms step_avg:46.33ms
step:1277/2000 train_time:59174ms step_avg:46.34ms
step:1278/2000 train_time:59233ms step_avg:46.35ms
step:1279/2000 train_time:59293ms step_avg:46.36ms
step:1280/2000 train_time:59353ms step_avg:46.37ms
step:1281/2000 train_time:59413ms step_avg:46.38ms
step:1282/2000 train_time:59473ms step_avg:46.39ms
step:1283/2000 train_time:59534ms step_avg:46.40ms
step:1284/2000 train_time:59594ms step_avg:46.41ms
step:1285/2000 train_time:59655ms step_avg:46.42ms
step:1286/2000 train_time:59716ms step_avg:46.44ms
step:1287/2000 train_time:59778ms step_avg:46.45ms
step:1288/2000 train_time:59837ms step_avg:46.46ms
step:1289/2000 train_time:59897ms step_avg:46.47ms
step:1290/2000 train_time:59957ms step_avg:46.48ms
step:1291/2000 train_time:60017ms step_avg:46.49ms
step:1292/2000 train_time:60077ms step_avg:46.50ms
step:1293/2000 train_time:60136ms step_avg:46.51ms
step:1294/2000 train_time:60195ms step_avg:46.52ms
step:1295/2000 train_time:60255ms step_avg:46.53ms
step:1296/2000 train_time:60314ms step_avg:46.54ms
step:1297/2000 train_time:60374ms step_avg:46.55ms
step:1298/2000 train_time:60434ms step_avg:46.56ms
step:1299/2000 train_time:60494ms step_avg:46.57ms
step:1300/2000 train_time:60554ms step_avg:46.58ms
step:1301/2000 train_time:60614ms step_avg:46.59ms
step:1302/2000 train_time:60674ms step_avg:46.60ms
step:1303/2000 train_time:60735ms step_avg:46.61ms
step:1304/2000 train_time:60795ms step_avg:46.62ms
step:1305/2000 train_time:60856ms step_avg:46.63ms
step:1306/2000 train_time:60916ms step_avg:46.64ms
step:1307/2000 train_time:60977ms step_avg:46.65ms
step:1308/2000 train_time:61036ms step_avg:46.66ms
step:1309/2000 train_time:61125ms step_avg:46.70ms
step:1310/2000 train_time:61212ms step_avg:46.73ms
step:1311/2000 train_time:61301ms step_avg:46.76ms
step:1312/2000 train_time:61388ms step_avg:46.79ms
step:1313/2000 train_time:61477ms step_avg:46.82ms
step:1314/2000 train_time:61565ms step_avg:46.85ms
step:1315/2000 train_time:61653ms step_avg:46.88ms
step:1316/2000 train_time:61742ms step_avg:46.92ms
step:1317/2000 train_time:61830ms step_avg:46.95ms
step:1318/2000 train_time:61918ms step_avg:46.98ms
step:1319/2000 train_time:62007ms step_avg:47.01ms
step:1320/2000 train_time:62095ms step_avg:47.04ms
step:1321/2000 train_time:62182ms step_avg:47.07ms
step:1322/2000 train_time:62270ms step_avg:47.10ms
step:1323/2000 train_time:62358ms step_avg:47.13ms
step:1324/2000 train_time:62446ms step_avg:47.16ms
step:1325/2000 train_time:62534ms step_avg:47.20ms
step:1326/2000 train_time:62622ms step_avg:47.23ms
step:1327/2000 train_time:62710ms step_avg:47.26ms
step:1328/2000 train_time:62798ms step_avg:47.29ms
step:1329/2000 train_time:62887ms step_avg:47.32ms
step:1330/2000 train_time:62975ms step_avg:47.35ms
step:1331/2000 train_time:63063ms step_avg:47.38ms
step:1332/2000 train_time:63151ms step_avg:47.41ms
step:1333/2000 train_time:63240ms step_avg:47.44ms
step:1334/2000 train_time:63327ms step_avg:47.47ms
step:1335/2000 train_time:63416ms step_avg:47.50ms
step:1336/2000 train_time:63503ms step_avg:47.53ms
step:1337/2000 train_time:63591ms step_avg:47.56ms
step:1338/2000 train_time:63679ms step_avg:47.59ms
step:1339/2000 train_time:63767ms step_avg:47.62ms
step:1340/2000 train_time:63855ms step_avg:47.65ms
step:1341/2000 train_time:63944ms step_avg:47.68ms
step:1342/2000 train_time:64032ms step_avg:47.71ms
step:1343/2000 train_time:64121ms step_avg:47.74ms
step:1344/2000 train_time:64208ms step_avg:47.77ms
step:1345/2000 train_time:64296ms step_avg:47.80ms
step:1346/2000 train_time:64383ms step_avg:47.83ms
step:1347/2000 train_time:64471ms step_avg:47.86ms
step:1348/2000 train_time:64560ms step_avg:47.89ms
step:1349/2000 train_time:64647ms step_avg:47.92ms
step:1350/2000 train_time:64735ms step_avg:47.95ms
step:1351/2000 train_time:64823ms step_avg:47.98ms
step:1352/2000 train_time:64910ms step_avg:48.01ms
step:1353/2000 train_time:64999ms step_avg:48.04ms
step:1354/2000 train_time:65086ms step_avg:48.07ms
step:1355/2000 train_time:65175ms step_avg:48.10ms
step:1356/2000 train_time:65263ms step_avg:48.13ms
step:1357/2000 train_time:65350ms step_avg:48.16ms
step:1358/2000 train_time:65438ms step_avg:48.19ms
step:1359/2000 train_time:65528ms step_avg:48.22ms
step:1360/2000 train_time:65616ms step_avg:48.25ms
step:1361/2000 train_time:65704ms step_avg:48.28ms
step:1362/2000 train_time:65792ms step_avg:48.31ms
step:1363/2000 train_time:65882ms step_avg:48.34ms
step:1364/2000 train_time:65968ms step_avg:48.36ms
step:1365/2000 train_time:66056ms step_avg:48.39ms
step:1366/2000 train_time:66144ms step_avg:48.42ms
step:1367/2000 train_time:66232ms step_avg:48.45ms
step:1368/2000 train_time:66320ms step_avg:48.48ms
step:1369/2000 train_time:66408ms step_avg:48.51ms
step:1370/2000 train_time:66496ms step_avg:48.54ms
step:1371/2000 train_time:66584ms step_avg:48.57ms
step:1372/2000 train_time:66672ms step_avg:48.59ms
step:1373/2000 train_time:66760ms step_avg:48.62ms
step:1374/2000 train_time:66847ms step_avg:48.65ms
step:1375/2000 train_time:66935ms step_avg:48.68ms
step:1376/2000 train_time:67023ms step_avg:48.71ms
step:1377/2000 train_time:67111ms step_avg:48.74ms
step:1378/2000 train_time:67199ms step_avg:48.77ms
step:1379/2000 train_time:67287ms step_avg:48.79ms
step:1380/2000 train_time:67375ms step_avg:48.82ms
step:1381/2000 train_time:67463ms step_avg:48.85ms
step:1382/2000 train_time:67551ms step_avg:48.88ms
step:1383/2000 train_time:67639ms step_avg:48.91ms
step:1384/2000 train_time:67726ms step_avg:48.94ms
step:1385/2000 train_time:67816ms step_avg:48.96ms
step:1386/2000 train_time:67903ms step_avg:48.99ms
step:1387/2000 train_time:67990ms step_avg:49.02ms
step:1388/2000 train_time:68078ms step_avg:49.05ms
step:1389/2000 train_time:68167ms step_avg:49.08ms
step:1390/2000 train_time:68255ms step_avg:49.10ms
step:1391/2000 train_time:68343ms step_avg:49.13ms
step:1392/2000 train_time:68432ms step_avg:49.16ms
step:1393/2000 train_time:68521ms step_avg:49.19ms
step:1394/2000 train_time:68608ms step_avg:49.22ms
step:1395/2000 train_time:68697ms step_avg:49.25ms
step:1396/2000 train_time:68784ms step_avg:49.27ms
step:1397/2000 train_time:68872ms step_avg:49.30ms
step:1398/2000 train_time:68960ms step_avg:49.33ms
step:1399/2000 train_time:69049ms step_avg:49.36ms
step:1400/2000 train_time:69136ms step_avg:49.38ms
step:1401/2000 train_time:69225ms step_avg:49.41ms
step:1402/2000 train_time:69312ms step_avg:49.44ms
step:1403/2000 train_time:69401ms step_avg:49.47ms
step:1404/2000 train_time:69488ms step_avg:49.49ms
step:1405/2000 train_time:69577ms step_avg:49.52ms
step:1406/2000 train_time:69665ms step_avg:49.55ms
step:1407/2000 train_time:69753ms step_avg:49.58ms
step:1408/2000 train_time:69842ms step_avg:49.60ms
step:1409/2000 train_time:69929ms step_avg:49.63ms
step:1410/2000 train_time:70017ms step_avg:49.66ms
step:1411/2000 train_time:70106ms step_avg:49.69ms
step:1412/2000 train_time:70193ms step_avg:49.71ms
step:1413/2000 train_time:70283ms step_avg:49.74ms
step:1414/2000 train_time:70371ms step_avg:49.77ms
step:1415/2000 train_time:70459ms step_avg:49.79ms
step:1416/2000 train_time:70547ms step_avg:49.82ms
step:1417/2000 train_time:70635ms step_avg:49.85ms
step:1418/2000 train_time:70723ms step_avg:49.88ms
step:1419/2000 train_time:70810ms step_avg:49.90ms
step:1420/2000 train_time:70899ms step_avg:49.93ms
step:1421/2000 train_time:70987ms step_avg:49.96ms
step:1422/2000 train_time:71074ms step_avg:49.98ms
step:1423/2000 train_time:71162ms step_avg:50.01ms
step:1424/2000 train_time:71250ms step_avg:50.03ms
step:1425/2000 train_time:71338ms step_avg:50.06ms
step:1426/2000 train_time:71426ms step_avg:50.09ms
step:1427/2000 train_time:71513ms step_avg:50.11ms
step:1428/2000 train_time:71602ms step_avg:50.14ms
step:1429/2000 train_time:71690ms step_avg:50.17ms
step:1430/2000 train_time:71779ms step_avg:50.20ms
step:1431/2000 train_time:71867ms step_avg:50.22ms
step:1432/2000 train_time:71955ms step_avg:50.25ms
step:1433/2000 train_time:72044ms step_avg:50.27ms
step:1434/2000 train_time:72132ms step_avg:50.30ms
step:1435/2000 train_time:72220ms step_avg:50.33ms
step:1436/2000 train_time:72307ms step_avg:50.35ms
step:1437/2000 train_time:72395ms step_avg:50.38ms
step:1438/2000 train_time:72483ms step_avg:50.41ms
step:1439/2000 train_time:72571ms step_avg:50.43ms
step:1440/2000 train_time:72660ms step_avg:50.46ms
step:1441/2000 train_time:72747ms step_avg:50.48ms
step:1442/2000 train_time:72834ms step_avg:50.51ms
step:1443/2000 train_time:72923ms step_avg:50.54ms
step:1444/2000 train_time:73012ms step_avg:50.56ms
step:1445/2000 train_time:73101ms step_avg:50.59ms
step:1446/2000 train_time:73188ms step_avg:50.61ms
step:1447/2000 train_time:73277ms step_avg:50.64ms
step:1448/2000 train_time:73364ms step_avg:50.67ms
step:1449/2000 train_time:73452ms step_avg:50.69ms
step:1450/2000 train_time:73540ms step_avg:50.72ms
step:1451/2000 train_time:73628ms step_avg:50.74ms
step:1452/2000 train_time:73716ms step_avg:50.77ms
step:1453/2000 train_time:73805ms step_avg:50.79ms
step:1454/2000 train_time:73892ms step_avg:50.82ms
step:1455/2000 train_time:73980ms step_avg:50.85ms
step:1456/2000 train_time:74067ms step_avg:50.87ms
step:1457/2000 train_time:74156ms step_avg:50.90ms
step:1458/2000 train_time:74244ms step_avg:50.92ms
step:1459/2000 train_time:74332ms step_avg:50.95ms
step:1460/2000 train_time:74420ms step_avg:50.97ms
step:1461/2000 train_time:74508ms step_avg:51.00ms
step:1462/2000 train_time:74596ms step_avg:51.02ms
step:1463/2000 train_time:74684ms step_avg:51.05ms
step:1464/2000 train_time:74772ms step_avg:51.07ms
step:1465/2000 train_time:74861ms step_avg:51.10ms
step:1466/2000 train_time:74949ms step_avg:51.12ms
step:1467/2000 train_time:75036ms step_avg:51.15ms
step:1468/2000 train_time:75124ms step_avg:51.17ms
step:1469/2000 train_time:75213ms step_avg:51.20ms
step:1470/2000 train_time:75300ms step_avg:51.22ms
step:1471/2000 train_time:75388ms step_avg:51.25ms
step:1472/2000 train_time:75476ms step_avg:51.27ms
step:1473/2000 train_time:75565ms step_avg:51.30ms
step:1474/2000 train_time:75653ms step_avg:51.33ms
step:1475/2000 train_time:75742ms step_avg:51.35ms
step:1476/2000 train_time:75829ms step_avg:51.37ms
step:1477/2000 train_time:75917ms step_avg:51.40ms
step:1478/2000 train_time:76005ms step_avg:51.42ms
step:1479/2000 train_time:76093ms step_avg:51.45ms
step:1480/2000 train_time:76182ms step_avg:51.47ms
step:1481/2000 train_time:76270ms step_avg:51.50ms
step:1482/2000 train_time:76357ms step_avg:51.52ms
step:1483/2000 train_time:76445ms step_avg:51.55ms
step:1484/2000 train_time:76534ms step_avg:51.57ms
step:1485/2000 train_time:76623ms step_avg:51.60ms
step:1486/2000 train_time:76711ms step_avg:51.62ms
step:1487/2000 train_time:76799ms step_avg:51.65ms
step:1488/2000 train_time:76886ms step_avg:51.67ms
step:1489/2000 train_time:76975ms step_avg:51.70ms
step:1490/2000 train_time:77063ms step_avg:51.72ms
step:1491/2000 train_time:77150ms step_avg:51.74ms
step:1492/2000 train_time:77238ms step_avg:51.77ms
step:1493/2000 train_time:77326ms step_avg:51.79ms
step:1494/2000 train_time:77414ms step_avg:51.82ms
step:1495/2000 train_time:77504ms step_avg:51.84ms
step:1496/2000 train_time:77593ms step_avg:51.87ms
step:1497/2000 train_time:77681ms step_avg:51.89ms
step:1498/2000 train_time:77769ms step_avg:51.91ms
step:1499/2000 train_time:77857ms step_avg:51.94ms
step:1500/2000 train_time:77945ms step_avg:51.96ms
step:1500/2000 val_loss:3.4430 train_time:78035ms step_avg:52.02ms
step:1501/2000 train_time:78055ms step_avg:52.00ms
step:1502/2000 train_time:78124ms step_avg:52.01ms
step:1503/2000 train_time:78217ms step_avg:52.04ms
step:1504/2000 train_time:78306ms step_avg:52.07ms
step:1505/2000 train_time:78393ms step_avg:52.09ms
step:1506/2000 train_time:78481ms step_avg:52.11ms
step:1507/2000 train_time:78567ms step_avg:52.13ms
step:1508/2000 train_time:78654ms step_avg:52.16ms
step:1509/2000 train_time:78741ms step_avg:52.18ms
step:1510/2000 train_time:78829ms step_avg:52.20ms
step:1511/2000 train_time:78916ms step_avg:52.23ms
step:1512/2000 train_time:79006ms step_avg:52.25ms
step:1513/2000 train_time:79095ms step_avg:52.28ms
step:1514/2000 train_time:79185ms step_avg:52.30ms
step:1515/2000 train_time:79274ms step_avg:52.33ms
step:1516/2000 train_time:79363ms step_avg:52.35ms
step:1517/2000 train_time:79451ms step_avg:52.37ms
step:1518/2000 train_time:79538ms step_avg:52.40ms
step:1519/2000 train_time:79627ms step_avg:52.42ms
step:1520/2000 train_time:79713ms step_avg:52.44ms
step:1521/2000 train_time:79801ms step_avg:52.47ms
step:1522/2000 train_time:79889ms step_avg:52.49ms
step:1523/2000 train_time:79978ms step_avg:52.51ms
step:1524/2000 train_time:80066ms step_avg:52.54ms
step:1525/2000 train_time:80156ms step_avg:52.56ms
step:1526/2000 train_time:80244ms step_avg:52.58ms
step:1527/2000 train_time:80333ms step_avg:52.61ms
step:1528/2000 train_time:80421ms step_avg:52.63ms
step:1529/2000 train_time:80508ms step_avg:52.65ms
step:1530/2000 train_time:80596ms step_avg:52.68ms
step:1531/2000 train_time:80684ms step_avg:52.70ms
step:1532/2000 train_time:80771ms step_avg:52.72ms
step:1533/2000 train_time:80859ms step_avg:52.75ms
step:1534/2000 train_time:80947ms step_avg:52.77ms
step:1535/2000 train_time:81035ms step_avg:52.79ms
step:1536/2000 train_time:81124ms step_avg:52.82ms
step:1537/2000 train_time:81212ms step_avg:52.84ms
step:1538/2000 train_time:81301ms step_avg:52.86ms
step:1539/2000 train_time:81391ms step_avg:52.89ms
step:1540/2000 train_time:81478ms step_avg:52.91ms
step:1541/2000 train_time:81567ms step_avg:52.93ms
step:1542/2000 train_time:81654ms step_avg:52.95ms
step:1543/2000 train_time:81742ms step_avg:52.98ms
step:1544/2000 train_time:81829ms step_avg:53.00ms
step:1545/2000 train_time:81917ms step_avg:53.02ms
step:1546/2000 train_time:82006ms step_avg:53.04ms
step:1547/2000 train_time:82094ms step_avg:53.07ms
step:1548/2000 train_time:82183ms step_avg:53.09ms
step:1549/2000 train_time:82271ms step_avg:53.11ms
step:1550/2000 train_time:82359ms step_avg:53.14ms
step:1551/2000 train_time:82449ms step_avg:53.16ms
step:1552/2000 train_time:82536ms step_avg:53.18ms
step:1553/2000 train_time:82624ms step_avg:53.20ms
step:1554/2000 train_time:82711ms step_avg:53.22ms
step:1555/2000 train_time:82799ms step_avg:53.25ms
step:1556/2000 train_time:82888ms step_avg:53.27ms
step:1557/2000 train_time:82975ms step_avg:53.29ms
step:1558/2000 train_time:83064ms step_avg:53.31ms
step:1559/2000 train_time:83152ms step_avg:53.34ms
step:1560/2000 train_time:83241ms step_avg:53.36ms
step:1561/2000 train_time:83330ms step_avg:53.38ms
step:1562/2000 train_time:83418ms step_avg:53.40ms
step:1563/2000 train_time:83506ms step_avg:53.43ms
step:1564/2000 train_time:83593ms step_avg:53.45ms
step:1565/2000 train_time:83682ms step_avg:53.47ms
step:1566/2000 train_time:83769ms step_avg:53.49ms
step:1567/2000 train_time:83857ms step_avg:53.51ms
step:1568/2000 train_time:83945ms step_avg:53.54ms
step:1569/2000 train_time:84032ms step_avg:53.56ms
step:1570/2000 train_time:84121ms step_avg:53.58ms
step:1571/2000 train_time:84209ms step_avg:53.60ms
step:1572/2000 train_time:84296ms step_avg:53.62ms
step:1573/2000 train_time:84385ms step_avg:53.65ms
step:1574/2000 train_time:84472ms step_avg:53.67ms
step:1575/2000 train_time:84561ms step_avg:53.69ms
step:1576/2000 train_time:84648ms step_avg:53.71ms
step:1577/2000 train_time:84736ms step_avg:53.73ms
step:1578/2000 train_time:84824ms step_avg:53.75ms
step:1579/2000 train_time:84912ms step_avg:53.78ms
step:1580/2000 train_time:84999ms step_avg:53.80ms
step:1581/2000 train_time:85087ms step_avg:53.82ms
step:1582/2000 train_time:85175ms step_avg:53.84ms
step:1583/2000 train_time:85263ms step_avg:53.86ms
step:1584/2000 train_time:85350ms step_avg:53.88ms
step:1585/2000 train_time:85439ms step_avg:53.90ms
step:1586/2000 train_time:85527ms step_avg:53.93ms
step:1587/2000 train_time:85615ms step_avg:53.95ms
step:1588/2000 train_time:85703ms step_avg:53.97ms
step:1589/2000 train_time:85791ms step_avg:53.99ms
step:1590/2000 train_time:85879ms step_avg:54.01ms
step:1591/2000 train_time:85967ms step_avg:54.03ms
step:1592/2000 train_time:86055ms step_avg:54.05ms
step:1593/2000 train_time:86144ms step_avg:54.08ms
step:1594/2000 train_time:86231ms step_avg:54.10ms
step:1595/2000 train_time:86321ms step_avg:54.12ms
step:1596/2000 train_time:86408ms step_avg:54.14ms
step:1597/2000 train_time:86496ms step_avg:54.16ms
step:1598/2000 train_time:86584ms step_avg:54.18ms
step:1599/2000 train_time:86671ms step_avg:54.20ms
step:1600/2000 train_time:86759ms step_avg:54.22ms
step:1601/2000 train_time:86847ms step_avg:54.25ms
step:1602/2000 train_time:86934ms step_avg:54.27ms
step:1603/2000 train_time:87023ms step_avg:54.29ms
step:1604/2000 train_time:87110ms step_avg:54.31ms
step:1605/2000 train_time:87198ms step_avg:54.33ms
step:1606/2000 train_time:87287ms step_avg:54.35ms
step:1607/2000 train_time:87375ms step_avg:54.37ms
step:1608/2000 train_time:87464ms step_avg:54.39ms
step:1609/2000 train_time:87552ms step_avg:54.41ms
step:1610/2000 train_time:87640ms step_avg:54.44ms
step:1611/2000 train_time:87728ms step_avg:54.46ms
step:1612/2000 train_time:87816ms step_avg:54.48ms
step:1613/2000 train_time:87905ms step_avg:54.50ms
step:1614/2000 train_time:87992ms step_avg:54.52ms
step:1615/2000 train_time:88080ms step_avg:54.54ms
step:1616/2000 train_time:88167ms step_avg:54.56ms
step:1617/2000 train_time:88256ms step_avg:54.58ms
step:1618/2000 train_time:88345ms step_avg:54.60ms
step:1619/2000 train_time:88432ms step_avg:54.62ms
step:1620/2000 train_time:88520ms step_avg:54.64ms
step:1621/2000 train_time:88609ms step_avg:54.66ms
step:1622/2000 train_time:88696ms step_avg:54.68ms
step:1623/2000 train_time:88784ms step_avg:54.70ms
step:1624/2000 train_time:88872ms step_avg:54.72ms
step:1625/2000 train_time:88960ms step_avg:54.74ms
step:1626/2000 train_time:89048ms step_avg:54.77ms
step:1627/2000 train_time:89136ms step_avg:54.79ms
step:1628/2000 train_time:89225ms step_avg:54.81ms
step:1629/2000 train_time:89313ms step_avg:54.83ms
step:1630/2000 train_time:89401ms step_avg:54.85ms
step:1631/2000 train_time:89489ms step_avg:54.87ms
step:1632/2000 train_time:89577ms step_avg:54.89ms
step:1633/2000 train_time:89666ms step_avg:54.91ms
step:1634/2000 train_time:89753ms step_avg:54.93ms
step:1635/2000 train_time:89842ms step_avg:54.95ms
step:1636/2000 train_time:89930ms step_avg:54.97ms
step:1637/2000 train_time:90018ms step_avg:54.99ms
step:1638/2000 train_time:90105ms step_avg:55.01ms
step:1639/2000 train_time:90194ms step_avg:55.03ms
step:1640/2000 train_time:90281ms step_avg:55.05ms
step:1641/2000 train_time:90370ms step_avg:55.07ms
step:1642/2000 train_time:90458ms step_avg:55.09ms
step:1643/2000 train_time:90547ms step_avg:55.11ms
step:1644/2000 train_time:90634ms step_avg:55.13ms
step:1645/2000 train_time:90723ms step_avg:55.15ms
step:1646/2000 train_time:90811ms step_avg:55.17ms
step:1647/2000 train_time:90899ms step_avg:55.19ms
step:1648/2000 train_time:90987ms step_avg:55.21ms
step:1649/2000 train_time:91075ms step_avg:55.23ms
step:1650/2000 train_time:91163ms step_avg:55.25ms
step:1651/2000 train_time:91251ms step_avg:55.27ms
step:1652/2000 train_time:91339ms step_avg:55.29ms
step:1653/2000 train_time:91427ms step_avg:55.31ms
step:1654/2000 train_time:91515ms step_avg:55.33ms
step:1655/2000 train_time:91603ms step_avg:55.35ms
step:1656/2000 train_time:91692ms step_avg:55.37ms
step:1657/2000 train_time:91779ms step_avg:55.39ms
step:1658/2000 train_time:91867ms step_avg:55.41ms
step:1659/2000 train_time:91955ms step_avg:55.43ms
step:1660/2000 train_time:92043ms step_avg:55.45ms
step:1661/2000 train_time:92132ms step_avg:55.47ms
step:1662/2000 train_time:92220ms step_avg:55.49ms
step:1663/2000 train_time:92308ms step_avg:55.51ms
step:1664/2000 train_time:92396ms step_avg:55.53ms
step:1665/2000 train_time:92485ms step_avg:55.55ms
step:1666/2000 train_time:92572ms step_avg:55.57ms
step:1667/2000 train_time:92661ms step_avg:55.59ms
step:1668/2000 train_time:92748ms step_avg:55.60ms
step:1669/2000 train_time:92837ms step_avg:55.62ms
step:1670/2000 train_time:92927ms step_avg:55.65ms
step:1671/2000 train_time:93016ms step_avg:55.66ms
step:1672/2000 train_time:93104ms step_avg:55.68ms
step:1673/2000 train_time:93191ms step_avg:55.70ms
step:1674/2000 train_time:93280ms step_avg:55.72ms
step:1675/2000 train_time:93367ms step_avg:55.74ms
step:1676/2000 train_time:93455ms step_avg:55.76ms
step:1677/2000 train_time:93543ms step_avg:55.78ms
step:1678/2000 train_time:93631ms step_avg:55.80ms
step:1679/2000 train_time:93718ms step_avg:55.82ms
step:1680/2000 train_time:93806ms step_avg:55.84ms
step:1681/2000 train_time:93894ms step_avg:55.86ms
step:1682/2000 train_time:93982ms step_avg:55.88ms
step:1683/2000 train_time:94070ms step_avg:55.89ms
step:1684/2000 train_time:94158ms step_avg:55.91ms
step:1685/2000 train_time:94248ms step_avg:55.93ms
step:1686/2000 train_time:94335ms step_avg:55.95ms
step:1687/2000 train_time:94423ms step_avg:55.97ms
step:1688/2000 train_time:94511ms step_avg:55.99ms
step:1689/2000 train_time:94599ms step_avg:56.01ms
step:1690/2000 train_time:94687ms step_avg:56.03ms
step:1691/2000 train_time:94775ms step_avg:56.05ms
step:1692/2000 train_time:94863ms step_avg:56.07ms
step:1693/2000 train_time:94951ms step_avg:56.08ms
step:1694/2000 train_time:95040ms step_avg:56.10ms
step:1695/2000 train_time:95127ms step_avg:56.12ms
step:1696/2000 train_time:95215ms step_avg:56.14ms
step:1697/2000 train_time:95304ms step_avg:56.16ms
step:1698/2000 train_time:95391ms step_avg:56.18ms
step:1699/2000 train_time:95479ms step_avg:56.20ms
step:1700/2000 train_time:95566ms step_avg:56.22ms
step:1701/2000 train_time:95655ms step_avg:56.23ms
step:1702/2000 train_time:95743ms step_avg:56.25ms
step:1703/2000 train_time:95832ms step_avg:56.27ms
step:1704/2000 train_time:95920ms step_avg:56.29ms
step:1705/2000 train_time:96009ms step_avg:56.31ms
step:1706/2000 train_time:96097ms step_avg:56.33ms
step:1707/2000 train_time:96185ms step_avg:56.35ms
step:1708/2000 train_time:96272ms step_avg:56.37ms
step:1709/2000 train_time:96360ms step_avg:56.38ms
step:1710/2000 train_time:96448ms step_avg:56.40ms
step:1711/2000 train_time:96536ms step_avg:56.42ms
step:1712/2000 train_time:96625ms step_avg:56.44ms
step:1713/2000 train_time:96712ms step_avg:56.46ms
step:1714/2000 train_time:96800ms step_avg:56.48ms
step:1715/2000 train_time:96890ms step_avg:56.50ms
step:1716/2000 train_time:96979ms step_avg:56.51ms
step:1717/2000 train_time:97067ms step_avg:56.53ms
step:1718/2000 train_time:97156ms step_avg:56.55ms
step:1719/2000 train_time:97245ms step_avg:56.57ms
step:1720/2000 train_time:97332ms step_avg:56.59ms
step:1721/2000 train_time:97420ms step_avg:56.61ms
step:1722/2000 train_time:97509ms step_avg:56.63ms
step:1723/2000 train_time:97596ms step_avg:56.64ms
step:1724/2000 train_time:97685ms step_avg:56.66ms
step:1725/2000 train_time:97772ms step_avg:56.68ms
step:1726/2000 train_time:97860ms step_avg:56.70ms
step:1727/2000 train_time:97949ms step_avg:56.72ms
step:1728/2000 train_time:98037ms step_avg:56.73ms
step:1729/2000 train_time:98126ms step_avg:56.75ms
step:1730/2000 train_time:98213ms step_avg:56.77ms
step:1731/2000 train_time:98302ms step_avg:56.79ms
step:1732/2000 train_time:98389ms step_avg:56.81ms
step:1733/2000 train_time:98478ms step_avg:56.82ms
step:1734/2000 train_time:98565ms step_avg:56.84ms
step:1735/2000 train_time:98653ms step_avg:56.86ms
step:1736/2000 train_time:98740ms step_avg:56.88ms
step:1737/2000 train_time:98830ms step_avg:56.90ms
step:1738/2000 train_time:98919ms step_avg:56.92ms
step:1739/2000 train_time:99007ms step_avg:56.93ms
step:1740/2000 train_time:99095ms step_avg:56.95ms
step:1741/2000 train_time:99183ms step_avg:56.97ms
step:1742/2000 train_time:99271ms step_avg:56.99ms
step:1743/2000 train_time:99359ms step_avg:57.00ms
step:1744/2000 train_time:99447ms step_avg:57.02ms
step:1745/2000 train_time:99535ms step_avg:57.04ms
step:1746/2000 train_time:99623ms step_avg:57.06ms
step:1747/2000 train_time:99711ms step_avg:57.08ms
step:1748/2000 train_time:99799ms step_avg:57.09ms
step:1749/2000 train_time:99888ms step_avg:57.11ms
step:1750/2000 train_time:99976ms step_avg:57.13ms
step:1750/2000 val_loss:3.3477 train_time:100067ms step_avg:57.18ms
step:1751/2000 train_time:100086ms step_avg:57.16ms
step:1752/2000 train_time:100155ms step_avg:57.17ms
step:1753/2000 train_time:100247ms step_avg:57.19ms
step:1754/2000 train_time:100336ms step_avg:57.20ms
step:1755/2000 train_time:100424ms step_avg:57.22ms
step:1756/2000 train_time:100510ms step_avg:57.24ms
step:1757/2000 train_time:100597ms step_avg:57.26ms
step:1758/2000 train_time:100684ms step_avg:57.27ms
step:1759/2000 train_time:100771ms step_avg:57.29ms
step:1760/2000 train_time:100859ms step_avg:57.31ms
step:1761/2000 train_time:100947ms step_avg:57.32ms
step:1762/2000 train_time:101035ms step_avg:57.34ms
step:1763/2000 train_time:101126ms step_avg:57.36ms
step:1764/2000 train_time:101216ms step_avg:57.38ms
step:1765/2000 train_time:101305ms step_avg:57.40ms
step:1766/2000 train_time:101392ms step_avg:57.41ms
step:1767/2000 train_time:101481ms step_avg:57.43ms
step:1768/2000 train_time:101568ms step_avg:57.45ms
step:1769/2000 train_time:101656ms step_avg:57.46ms
step:1770/2000 train_time:101743ms step_avg:57.48ms
step:1771/2000 train_time:101830ms step_avg:57.50ms
step:1772/2000 train_time:101917ms step_avg:57.52ms
step:1773/2000 train_time:102006ms step_avg:57.53ms
step:1774/2000 train_time:102094ms step_avg:57.55ms
step:1775/2000 train_time:102184ms step_avg:57.57ms
step:1776/2000 train_time:102272ms step_avg:57.59ms
step:1777/2000 train_time:102360ms step_avg:57.60ms
step:1778/2000 train_time:102449ms step_avg:57.62ms
step:1779/2000 train_time:102537ms step_avg:57.64ms
step:1780/2000 train_time:102626ms step_avg:57.65ms
step:1781/2000 train_time:102713ms step_avg:57.67ms
step:1782/2000 train_time:102801ms step_avg:57.69ms
step:1783/2000 train_time:102888ms step_avg:57.71ms
step:1784/2000 train_time:102977ms step_avg:57.72ms
step:1785/2000 train_time:103067ms step_avg:57.74ms
step:1786/2000 train_time:103155ms step_avg:57.76ms
step:1787/2000 train_time:103244ms step_avg:57.78ms
step:1788/2000 train_time:103332ms step_avg:57.79ms
step:1789/2000 train_time:103421ms step_avg:57.81ms
step:1790/2000 train_time:103508ms step_avg:57.83ms
step:1791/2000 train_time:103596ms step_avg:57.84ms
step:1792/2000 train_time:103684ms step_avg:57.86ms
step:1793/2000 train_time:103771ms step_avg:57.88ms
step:1794/2000 train_time:103859ms step_avg:57.89ms
step:1795/2000 train_time:103947ms step_avg:57.91ms
step:1796/2000 train_time:104036ms step_avg:57.93ms
step:1797/2000 train_time:104125ms step_avg:57.94ms
step:1798/2000 train_time:104213ms step_avg:57.96ms
step:1799/2000 train_time:104302ms step_avg:57.98ms
step:1800/2000 train_time:104390ms step_avg:57.99ms
step:1801/2000 train_time:104477ms step_avg:58.01ms
step:1802/2000 train_time:104566ms step_avg:58.03ms
step:1803/2000 train_time:104653ms step_avg:58.04ms
step:1804/2000 train_time:104740ms step_avg:58.06ms
step:1805/2000 train_time:104828ms step_avg:58.08ms
step:1806/2000 train_time:104916ms step_avg:58.09ms
step:1807/2000 train_time:105005ms step_avg:58.11ms
step:1808/2000 train_time:105093ms step_avg:58.13ms
step:1809/2000 train_time:105180ms step_avg:58.14ms
step:1810/2000 train_time:105268ms step_avg:58.16ms
step:1811/2000 train_time:105356ms step_avg:58.18ms
step:1812/2000 train_time:105444ms step_avg:58.19ms
step:1813/2000 train_time:105532ms step_avg:58.21ms
step:1814/2000 train_time:105620ms step_avg:58.22ms
step:1815/2000 train_time:105707ms step_avg:58.24ms
step:1816/2000 train_time:105795ms step_avg:58.26ms
step:1817/2000 train_time:105884ms step_avg:58.27ms
step:1818/2000 train_time:105971ms step_avg:58.29ms
step:1819/2000 train_time:106059ms step_avg:58.31ms
step:1820/2000 train_time:106148ms step_avg:58.32ms
step:1821/2000 train_time:106236ms step_avg:58.34ms
step:1822/2000 train_time:106325ms step_avg:58.36ms
step:1823/2000 train_time:106413ms step_avg:58.37ms
step:1824/2000 train_time:106501ms step_avg:58.39ms
step:1825/2000 train_time:106589ms step_avg:58.40ms
step:1826/2000 train_time:106676ms step_avg:58.42ms
step:1827/2000 train_time:106765ms step_avg:58.44ms
step:1828/2000 train_time:106853ms step_avg:58.45ms
step:1829/2000 train_time:106941ms step_avg:58.47ms
step:1830/2000 train_time:107029ms step_avg:58.49ms
step:1831/2000 train_time:107117ms step_avg:58.50ms
step:1832/2000 train_time:107205ms step_avg:58.52ms
step:1833/2000 train_time:107293ms step_avg:58.53ms
step:1834/2000 train_time:107381ms step_avg:58.55ms
step:1835/2000 train_time:107470ms step_avg:58.57ms
step:1836/2000 train_time:107557ms step_avg:58.58ms
step:1837/2000 train_time:107645ms step_avg:58.60ms
step:1838/2000 train_time:107733ms step_avg:58.61ms
step:1839/2000 train_time:107821ms step_avg:58.63ms
step:1840/2000 train_time:107909ms step_avg:58.65ms
step:1841/2000 train_time:107996ms step_avg:58.66ms
step:1842/2000 train_time:108084ms step_avg:58.68ms
step:1843/2000 train_time:108172ms step_avg:58.69ms
step:1844/2000 train_time:108260ms step_avg:58.71ms
step:1845/2000 train_time:108349ms step_avg:58.73ms
step:1846/2000 train_time:108437ms step_avg:58.74ms
step:1847/2000 train_time:108525ms step_avg:58.76ms
step:1848/2000 train_time:108612ms step_avg:58.77ms
step:1849/2000 train_time:108700ms step_avg:58.79ms
step:1850/2000 train_time:108787ms step_avg:58.80ms
step:1851/2000 train_time:108876ms step_avg:58.82ms
step:1852/2000 train_time:108965ms step_avg:58.84ms
step:1853/2000 train_time:109053ms step_avg:58.85ms
step:1854/2000 train_time:109141ms step_avg:58.87ms
step:1855/2000 train_time:109229ms step_avg:58.88ms
step:1856/2000 train_time:109317ms step_avg:58.90ms
step:1857/2000 train_time:109405ms step_avg:58.91ms
step:1858/2000 train_time:109493ms step_avg:58.93ms
step:1859/2000 train_time:109581ms step_avg:58.95ms
step:1860/2000 train_time:109668ms step_avg:58.96ms
step:1861/2000 train_time:109757ms step_avg:58.98ms
step:1862/2000 train_time:109845ms step_avg:58.99ms
step:1863/2000 train_time:109934ms step_avg:59.01ms
step:1864/2000 train_time:110022ms step_avg:59.02ms
step:1865/2000 train_time:110110ms step_avg:59.04ms
step:1866/2000 train_time:110197ms step_avg:59.06ms
step:1867/2000 train_time:110286ms step_avg:59.07ms
step:1868/2000 train_time:110374ms step_avg:59.09ms
step:1869/2000 train_time:110463ms step_avg:59.10ms
step:1870/2000 train_time:110550ms step_avg:59.12ms
step:1871/2000 train_time:110639ms step_avg:59.13ms
step:1872/2000 train_time:110727ms step_avg:59.15ms
step:1873/2000 train_time:110815ms step_avg:59.16ms
step:1874/2000 train_time:110903ms step_avg:59.18ms
step:1875/2000 train_time:110991ms step_avg:59.20ms
step:1876/2000 train_time:111078ms step_avg:59.21ms
step:1877/2000 train_time:111167ms step_avg:59.23ms
step:1878/2000 train_time:111255ms step_avg:59.24ms
step:1879/2000 train_time:111344ms step_avg:59.26ms
step:1880/2000 train_time:111432ms step_avg:59.27ms
step:1881/2000 train_time:111520ms step_avg:59.29ms
step:1882/2000 train_time:111607ms step_avg:59.30ms
step:1883/2000 train_time:111695ms step_avg:59.32ms
step:1884/2000 train_time:111783ms step_avg:59.33ms
step:1885/2000 train_time:111871ms step_avg:59.35ms
step:1886/2000 train_time:111960ms step_avg:59.36ms
step:1887/2000 train_time:112048ms step_avg:59.38ms
step:1888/2000 train_time:112136ms step_avg:59.39ms
step:1889/2000 train_time:112226ms step_avg:59.41ms
step:1890/2000 train_time:112313ms step_avg:59.42ms
step:1891/2000 train_time:112401ms step_avg:59.44ms
step:1892/2000 train_time:112489ms step_avg:59.46ms
step:1893/2000 train_time:112577ms step_avg:59.47ms
step:1894/2000 train_time:112665ms step_avg:59.49ms
step:1895/2000 train_time:112752ms step_avg:59.50ms
step:1896/2000 train_time:112840ms step_avg:59.51ms
step:1897/2000 train_time:112930ms step_avg:59.53ms
step:1898/2000 train_time:113018ms step_avg:59.55ms
step:1899/2000 train_time:113107ms step_avg:59.56ms
step:1900/2000 train_time:113196ms step_avg:59.58ms
step:1901/2000 train_time:113284ms step_avg:59.59ms
step:1902/2000 train_time:113371ms step_avg:59.61ms
step:1903/2000 train_time:113459ms step_avg:59.62ms
step:1904/2000 train_time:113548ms step_avg:59.64ms
step:1905/2000 train_time:113636ms step_avg:59.65ms
step:1906/2000 train_time:113724ms step_avg:59.67ms
step:1907/2000 train_time:113810ms step_avg:59.68ms
step:1908/2000 train_time:113898ms step_avg:59.69ms
step:1909/2000 train_time:113986ms step_avg:59.71ms
step:1910/2000 train_time:114074ms step_avg:59.72ms
step:1911/2000 train_time:114162ms step_avg:59.74ms
step:1912/2000 train_time:114249ms step_avg:59.75ms
step:1913/2000 train_time:114338ms step_avg:59.77ms
step:1914/2000 train_time:114427ms step_avg:59.78ms
step:1915/2000 train_time:114514ms step_avg:59.80ms
step:1916/2000 train_time:114602ms step_avg:59.81ms
step:1917/2000 train_time:114690ms step_avg:59.83ms
step:1918/2000 train_time:114777ms step_avg:59.84ms
step:1919/2000 train_time:114866ms step_avg:59.86ms
step:1920/2000 train_time:114953ms step_avg:59.87ms
step:1921/2000 train_time:115041ms step_avg:59.89ms
step:1922/2000 train_time:115129ms step_avg:59.90ms
step:1923/2000 train_time:115218ms step_avg:59.92ms
step:1924/2000 train_time:115306ms step_avg:59.93ms
step:1925/2000 train_time:115393ms step_avg:59.94ms
step:1926/2000 train_time:115481ms step_avg:59.96ms
step:1927/2000 train_time:115569ms step_avg:59.97ms
step:1928/2000 train_time:115658ms step_avg:59.99ms
step:1929/2000 train_time:115748ms step_avg:60.00ms
step:1930/2000 train_time:115836ms step_avg:60.02ms
step:1931/2000 train_time:115925ms step_avg:60.03ms
step:1932/2000 train_time:116013ms step_avg:60.05ms
step:1933/2000 train_time:116101ms step_avg:60.06ms
step:1934/2000 train_time:116189ms step_avg:60.08ms
step:1935/2000 train_time:116277ms step_avg:60.09ms
step:1936/2000 train_time:116367ms step_avg:60.11ms
step:1937/2000 train_time:116454ms step_avg:60.12ms
step:1938/2000 train_time:116542ms step_avg:60.14ms
step:1939/2000 train_time:116630ms step_avg:60.15ms
step:1940/2000 train_time:116718ms step_avg:60.16ms
step:1941/2000 train_time:116806ms step_avg:60.18ms
step:1942/2000 train_time:116894ms step_avg:60.19ms
step:1943/2000 train_time:116983ms step_avg:60.21ms
step:1944/2000 train_time:117070ms step_avg:60.22ms
step:1945/2000 train_time:117159ms step_avg:60.24ms
step:1946/2000 train_time:117247ms step_avg:60.25ms
step:1947/2000 train_time:117336ms step_avg:60.26ms
step:1948/2000 train_time:117424ms step_avg:60.28ms
step:1949/2000 train_time:117512ms step_avg:60.29ms
step:1950/2000 train_time:117600ms step_avg:60.31ms
step:1951/2000 train_time:117689ms step_avg:60.32ms
step:1952/2000 train_time:117777ms step_avg:60.34ms
step:1953/2000 train_time:117865ms step_avg:60.35ms
step:1954/2000 train_time:117952ms step_avg:60.36ms
step:1955/2000 train_time:118040ms step_avg:60.38ms
step:1956/2000 train_time:118129ms step_avg:60.39ms
step:1957/2000 train_time:118217ms step_avg:60.41ms
step:1958/2000 train_time:118305ms step_avg:60.42ms
step:1959/2000 train_time:118393ms step_avg:60.44ms
step:1960/2000 train_time:118481ms step_avg:60.45ms
step:1961/2000 train_time:118570ms step_avg:60.46ms
step:1962/2000 train_time:118659ms step_avg:60.48ms
step:1963/2000 train_time:118749ms step_avg:60.49ms
step:1964/2000 train_time:118837ms step_avg:60.51ms
step:1965/2000 train_time:118926ms step_avg:60.52ms
step:1966/2000 train_time:119013ms step_avg:60.54ms
step:1967/2000 train_time:119102ms step_avg:60.55ms
step:1968/2000 train_time:119189ms step_avg:60.56ms
step:1969/2000 train_time:119279ms step_avg:60.58ms
step:1970/2000 train_time:119367ms step_avg:60.59ms
step:1971/2000 train_time:119455ms step_avg:60.61ms
step:1972/2000 train_time:119542ms step_avg:60.62ms
step:1973/2000 train_time:119631ms step_avg:60.63ms
step:1974/2000 train_time:119719ms step_avg:60.65ms
step:1975/2000 train_time:119807ms step_avg:60.66ms
step:1976/2000 train_time:119895ms step_avg:60.68ms
step:1977/2000 train_time:119984ms step_avg:60.69ms
step:1978/2000 train_time:120071ms step_avg:60.70ms
step:1979/2000 train_time:120160ms step_avg:60.72ms
step:1980/2000 train_time:120248ms step_avg:60.73ms
step:1981/2000 train_time:120337ms step_avg:60.75ms
step:1982/2000 train_time:120426ms step_avg:60.76ms
step:1983/2000 train_time:120514ms step_avg:60.77ms
step:1984/2000 train_time:120602ms step_avg:60.79ms
step:1985/2000 train_time:120691ms step_avg:60.80ms
step:1986/2000 train_time:120779ms step_avg:60.82ms
step:1987/2000 train_time:120868ms step_avg:60.83ms
step:1988/2000 train_time:120956ms step_avg:60.84ms
step:1989/2000 train_time:121045ms step_avg:60.86ms
step:1990/2000 train_time:121133ms step_avg:60.87ms
step:1991/2000 train_time:121221ms step_avg:60.88ms
step:1992/2000 train_time:121309ms step_avg:60.90ms
step:1993/2000 train_time:121397ms step_avg:60.91ms
step:1994/2000 train_time:121486ms step_avg:60.93ms
step:1995/2000 train_time:121574ms step_avg:60.94ms
step:1996/2000 train_time:121663ms step_avg:60.95ms
step:1997/2000 train_time:121751ms step_avg:60.97ms
step:1998/2000 train_time:121840ms step_avg:60.98ms
step:1999/2000 train_time:121929ms step_avg:60.99ms
step:2000/2000 train_time:122017ms step_avg:61.01ms
step:2000/2000 val_loss:3.2783 train_time:122108ms step_avg:61.05ms
peak memory allocated: 29634 MiB reserved: 43536 MiB

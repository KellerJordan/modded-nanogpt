import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            grad_slice = torch.empty_like(grad[:rank_size])
            self._reduce_scatter_futures[param] = (
                dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad_slice
            )



    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

    @torch.no_grad()
    def reset_momentum(self, params=None):
        """Reset momentum buffers for specified parameters (or all if 'None')"""
        if params is None:
            # Reset all parameters
            params_to_reset = [p for group in self.param_groups for p in group['params']]
        else:
            params_to_reset = list(params)
        
        for param in params_to_reset:
            if param in self.state:
                state = self.state[param]
                if 'exp_avg' in state:
                    state['exp_avg'].zero_()
                if 'exp_avg_sq' in state:
                    state['exp_avg_sq'].zero_()
                state['step'] = 0

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0
        
        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas  
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    # evaluation and logging
    logs_dir: str = f"logs/12-21-Smooth-Scalars-stps.1960.40"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes,lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    
    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps
    
    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0
    
    if in_transition:
        adam_optimizers[1].transition_steps -= 1
            
    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) 
        is_transition = True
            
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 18:25:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    835107      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    835108      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    835109      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    835110      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    835111      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    835112      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    835113      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    835114      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    835108      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    835109      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    835110      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    835111      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    835112      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    835113      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    835114      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2000 val_loss:10.8322 train_time:0ms step_avg:0.03ms
step:1/2000 train_time:85ms step_avg:85.31ms
step:2/2000 train_time:108ms step_avg:54.08ms
step:3/2000 train_time:129ms step_avg:42.84ms
step:4/2000 train_time:161ms step_avg:40.29ms
step:5/2000 train_time:194ms step_avg:38.84ms
step:6/2000 train_time:281ms step_avg:46.91ms
step:7/2000 train_time:300ms step_avg:42.87ms
step:8/2000 train_time:329ms step_avg:41.07ms
step:9/2000 train_time:361ms step_avg:40.13ms
step:10/2000 train_time:394ms step_avg:39.44ms
step:11/2000 train_time:428ms step_avg:38.88ms
step:12/2000 train_time:461ms step_avg:38.41ms
step:13/2000 train_time:494ms step_avg:38.00ms
step:14/2000 train_time:527ms step_avg:37.66ms
step:15/2000 train_time:560ms step_avg:37.34ms
step:16/2000 train_time:594ms step_avg:37.11ms
step:17/2000 train_time:627ms step_avg:36.87ms
step:18/2000 train_time:660ms step_avg:36.68ms
step:19/2000 train_time:693ms step_avg:36.49ms
step:20/2000 train_time:726ms step_avg:36.32ms
step:21/2000 train_time:760ms step_avg:36.17ms
step:22/2000 train_time:793ms step_avg:36.04ms
step:23/2000 train_time:826ms step_avg:35.91ms
step:24/2000 train_time:860ms step_avg:35.82ms
step:25/2000 train_time:893ms step_avg:35.71ms
step:26/2000 train_time:926ms step_avg:35.61ms
step:27/2000 train_time:959ms step_avg:35.52ms
step:28/2000 train_time:993ms step_avg:35.46ms
step:29/2000 train_time:1026ms step_avg:35.37ms
step:30/2000 train_time:1059ms step_avg:35.30ms
step:31/2000 train_time:1092ms step_avg:35.23ms
step:32/2000 train_time:1126ms step_avg:35.18ms
step:33/2000 train_time:1159ms step_avg:35.12ms
step:34/2000 train_time:1193ms step_avg:35.09ms
step:35/2000 train_time:1227ms step_avg:35.05ms
step:36/2000 train_time:1261ms step_avg:35.03ms
step:37/2000 train_time:1294ms step_avg:34.99ms
step:38/2000 train_time:1328ms step_avg:34.95ms
step:39/2000 train_time:1362ms step_avg:34.91ms
step:40/2000 train_time:1395ms step_avg:34.89ms
step:41/2000 train_time:1429ms step_avg:34.85ms
step:42/2000 train_time:1462ms step_avg:34.82ms
step:43/2000 train_time:1496ms step_avg:34.79ms
step:44/2000 train_time:1530ms step_avg:34.76ms
step:45/2000 train_time:1563ms step_avg:34.73ms
step:46/2000 train_time:1596ms step_avg:34.70ms
step:47/2000 train_time:1630ms step_avg:34.68ms
step:48/2000 train_time:1664ms step_avg:34.66ms
step:49/2000 train_time:1697ms step_avg:34.62ms
step:50/2000 train_time:1730ms step_avg:34.60ms
step:51/2000 train_time:1763ms step_avg:34.57ms
step:52/2000 train_time:1797ms step_avg:34.55ms
step:53/2000 train_time:1830ms step_avg:34.53ms
step:54/2000 train_time:1863ms step_avg:34.51ms
step:55/2000 train_time:1896ms step_avg:34.48ms
step:56/2000 train_time:1930ms step_avg:34.46ms
step:57/2000 train_time:1963ms step_avg:34.44ms
step:58/2000 train_time:1996ms step_avg:34.42ms
step:59/2000 train_time:2029ms step_avg:34.39ms
step:60/2000 train_time:2063ms step_avg:34.38ms
step:61/2000 train_time:2096ms step_avg:34.36ms
step:62/2000 train_time:2129ms step_avg:34.35ms
step:63/2000 train_time:2163ms step_avg:34.33ms
step:64/2000 train_time:2197ms step_avg:34.32ms
step:65/2000 train_time:2230ms step_avg:34.31ms
step:66/2000 train_time:2263ms step_avg:34.29ms
step:67/2000 train_time:2297ms step_avg:34.28ms
step:68/2000 train_time:2331ms step_avg:34.27ms
step:69/2000 train_time:2364ms step_avg:34.26ms
step:70/2000 train_time:2397ms step_avg:34.25ms
step:71/2000 train_time:2431ms step_avg:34.23ms
step:72/2000 train_time:2464ms step_avg:34.23ms
step:73/2000 train_time:2497ms step_avg:34.21ms
step:74/2000 train_time:2531ms step_avg:34.20ms
step:75/2000 train_time:2564ms step_avg:34.19ms
step:76/2000 train_time:2598ms step_avg:34.18ms
step:77/2000 train_time:2631ms step_avg:34.17ms
step:78/2000 train_time:2664ms step_avg:34.16ms
step:79/2000 train_time:2697ms step_avg:34.14ms
step:80/2000 train_time:2731ms step_avg:34.14ms
step:81/2000 train_time:2764ms step_avg:34.12ms
step:82/2000 train_time:2797ms step_avg:34.11ms
step:83/2000 train_time:2830ms step_avg:34.10ms
step:84/2000 train_time:2864ms step_avg:34.09ms
step:85/2000 train_time:2897ms step_avg:34.08ms
step:86/2000 train_time:2931ms step_avg:34.08ms
step:87/2000 train_time:2964ms step_avg:34.06ms
step:88/2000 train_time:2997ms step_avg:34.05ms
step:89/2000 train_time:3030ms step_avg:34.04ms
step:90/2000 train_time:3063ms step_avg:34.03ms
step:91/2000 train_time:3096ms step_avg:34.02ms
step:92/2000 train_time:3130ms step_avg:34.02ms
step:93/2000 train_time:3163ms step_avg:34.01ms
step:94/2000 train_time:3197ms step_avg:34.01ms
step:95/2000 train_time:3231ms step_avg:34.01ms
step:96/2000 train_time:3264ms step_avg:34.00ms
step:97/2000 train_time:3297ms step_avg:33.99ms
step:98/2000 train_time:3331ms step_avg:33.99ms
step:99/2000 train_time:3364ms step_avg:33.98ms
step:100/2000 train_time:3398ms step_avg:33.98ms
step:101/2000 train_time:3431ms step_avg:33.98ms
step:102/2000 train_time:3465ms step_avg:33.97ms
step:103/2000 train_time:3498ms step_avg:33.96ms
step:104/2000 train_time:3532ms step_avg:33.96ms
step:105/2000 train_time:3565ms step_avg:33.95ms
step:106/2000 train_time:3599ms step_avg:33.95ms
step:107/2000 train_time:3632ms step_avg:33.94ms
step:108/2000 train_time:3665ms step_avg:33.93ms
step:109/2000 train_time:3698ms step_avg:33.93ms
step:110/2000 train_time:3732ms step_avg:33.93ms
step:111/2000 train_time:3765ms step_avg:33.92ms
step:112/2000 train_time:3798ms step_avg:33.91ms
step:113/2000 train_time:3832ms step_avg:33.91ms
step:114/2000 train_time:3865ms step_avg:33.90ms
step:115/2000 train_time:3898ms step_avg:33.90ms
step:116/2000 train_time:3932ms step_avg:33.89ms
step:117/2000 train_time:3964ms step_avg:33.88ms
step:118/2000 train_time:3998ms step_avg:33.88ms
step:119/2000 train_time:4030ms step_avg:33.87ms
step:120/2000 train_time:4064ms step_avg:33.86ms
step:121/2000 train_time:4097ms step_avg:33.86ms
step:122/2000 train_time:4131ms step_avg:33.86ms
step:123/2000 train_time:4163ms step_avg:33.85ms
step:124/2000 train_time:4197ms step_avg:33.84ms
step:125/2000 train_time:4230ms step_avg:33.84ms
step:126/2000 train_time:4263ms step_avg:33.83ms
step:127/2000 train_time:4296ms step_avg:33.83ms
step:128/2000 train_time:4330ms step_avg:33.83ms
step:129/2000 train_time:4363ms step_avg:33.82ms
step:130/2000 train_time:4397ms step_avg:33.82ms
step:131/2000 train_time:4430ms step_avg:33.82ms
step:132/2000 train_time:4464ms step_avg:33.82ms
step:133/2000 train_time:4497ms step_avg:33.81ms
step:134/2000 train_time:4531ms step_avg:33.81ms
step:135/2000 train_time:4564ms step_avg:33.81ms
step:136/2000 train_time:4598ms step_avg:33.81ms
step:137/2000 train_time:4631ms step_avg:33.80ms
step:138/2000 train_time:4664ms step_avg:33.80ms
step:139/2000 train_time:4698ms step_avg:33.80ms
step:140/2000 train_time:4731ms step_avg:33.79ms
step:141/2000 train_time:4765ms step_avg:33.79ms
step:142/2000 train_time:4798ms step_avg:33.79ms
step:143/2000 train_time:4831ms step_avg:33.79ms
step:144/2000 train_time:4865ms step_avg:33.78ms
step:145/2000 train_time:4898ms step_avg:33.78ms
step:146/2000 train_time:4931ms step_avg:33.77ms
step:147/2000 train_time:4964ms step_avg:33.77ms
step:148/2000 train_time:4998ms step_avg:33.77ms
step:149/2000 train_time:5031ms step_avg:33.76ms
step:150/2000 train_time:5064ms step_avg:33.76ms
step:151/2000 train_time:5097ms step_avg:33.76ms
step:152/2000 train_time:5131ms step_avg:33.76ms
step:153/2000 train_time:5164ms step_avg:33.75ms
step:154/2000 train_time:5198ms step_avg:33.75ms
step:155/2000 train_time:5231ms step_avg:33.75ms
step:156/2000 train_time:5264ms step_avg:33.74ms
step:157/2000 train_time:5297ms step_avg:33.74ms
step:158/2000 train_time:5330ms step_avg:33.73ms
step:159/2000 train_time:5363ms step_avg:33.73ms
step:160/2000 train_time:5396ms step_avg:33.73ms
step:161/2000 train_time:5429ms step_avg:33.72ms
step:162/2000 train_time:5463ms step_avg:33.72ms
step:163/2000 train_time:5496ms step_avg:33.72ms
step:164/2000 train_time:5529ms step_avg:33.71ms
step:165/2000 train_time:5562ms step_avg:33.71ms
step:166/2000 train_time:5595ms step_avg:33.70ms
step:167/2000 train_time:5628ms step_avg:33.70ms
step:168/2000 train_time:5661ms step_avg:33.70ms
step:169/2000 train_time:5694ms step_avg:33.69ms
step:170/2000 train_time:5728ms step_avg:33.69ms
step:171/2000 train_time:5761ms step_avg:33.69ms
step:172/2000 train_time:5794ms step_avg:33.69ms
step:173/2000 train_time:5827ms step_avg:33.68ms
step:174/2000 train_time:5860ms step_avg:33.68ms
step:175/2000 train_time:5894ms step_avg:33.68ms
step:176/2000 train_time:5927ms step_avg:33.67ms
step:177/2000 train_time:5960ms step_avg:33.67ms
step:178/2000 train_time:5993ms step_avg:33.67ms
step:179/2000 train_time:6026ms step_avg:33.67ms
step:180/2000 train_time:6060ms step_avg:33.67ms
step:181/2000 train_time:6093ms step_avg:33.66ms
step:182/2000 train_time:6126ms step_avg:33.66ms
step:183/2000 train_time:6159ms step_avg:33.66ms
step:184/2000 train_time:6193ms step_avg:33.66ms
step:185/2000 train_time:6226ms step_avg:33.65ms
step:186/2000 train_time:6259ms step_avg:33.65ms
step:187/2000 train_time:6292ms step_avg:33.65ms
step:188/2000 train_time:6325ms step_avg:33.64ms
step:189/2000 train_time:6358ms step_avg:33.64ms
step:190/2000 train_time:6392ms step_avg:33.64ms
step:191/2000 train_time:6424ms step_avg:33.64ms
step:192/2000 train_time:6458ms step_avg:33.63ms
step:193/2000 train_time:6491ms step_avg:33.63ms
step:194/2000 train_time:6524ms step_avg:33.63ms
step:195/2000 train_time:6557ms step_avg:33.63ms
step:196/2000 train_time:6591ms step_avg:33.63ms
step:197/2000 train_time:6624ms step_avg:33.62ms
step:198/2000 train_time:6657ms step_avg:33.62ms
step:199/2000 train_time:6690ms step_avg:33.62ms
step:200/2000 train_time:6724ms step_avg:33.62ms
step:201/2000 train_time:6757ms step_avg:33.62ms
step:202/2000 train_time:6790ms step_avg:33.62ms
step:203/2000 train_time:6824ms step_avg:33.61ms
step:204/2000 train_time:6857ms step_avg:33.61ms
step:205/2000 train_time:6890ms step_avg:33.61ms
step:206/2000 train_time:6923ms step_avg:33.61ms
step:207/2000 train_time:6957ms step_avg:33.61ms
step:208/2000 train_time:6990ms step_avg:33.61ms
step:209/2000 train_time:7023ms step_avg:33.61ms
step:210/2000 train_time:7057ms step_avg:33.60ms
step:211/2000 train_time:7090ms step_avg:33.60ms
step:212/2000 train_time:7123ms step_avg:33.60ms
step:213/2000 train_time:7156ms step_avg:33.60ms
step:214/2000 train_time:7190ms step_avg:33.60ms
step:215/2000 train_time:7223ms step_avg:33.60ms
step:216/2000 train_time:7256ms step_avg:33.59ms
step:217/2000 train_time:7290ms step_avg:33.59ms
step:218/2000 train_time:7323ms step_avg:33.59ms
step:219/2000 train_time:7356ms step_avg:33.59ms
step:220/2000 train_time:7389ms step_avg:33.59ms
step:221/2000 train_time:7422ms step_avg:33.58ms
step:222/2000 train_time:7456ms step_avg:33.58ms
step:223/2000 train_time:7489ms step_avg:33.58ms
step:224/2000 train_time:7522ms step_avg:33.58ms
step:225/2000 train_time:7555ms step_avg:33.58ms
step:226/2000 train_time:7589ms step_avg:33.58ms
step:227/2000 train_time:7621ms step_avg:33.57ms
step:228/2000 train_time:7655ms step_avg:33.57ms
step:229/2000 train_time:7688ms step_avg:33.57ms
step:230/2000 train_time:7721ms step_avg:33.57ms
step:231/2000 train_time:7754ms step_avg:33.57ms
step:232/2000 train_time:7787ms step_avg:33.57ms
step:233/2000 train_time:7820ms step_avg:33.56ms
step:234/2000 train_time:7854ms step_avg:33.56ms
step:235/2000 train_time:7887ms step_avg:33.56ms
step:236/2000 train_time:7920ms step_avg:33.56ms
step:237/2000 train_time:7953ms step_avg:33.56ms
step:238/2000 train_time:7987ms step_avg:33.56ms
step:239/2000 train_time:8020ms step_avg:33.56ms
step:240/2000 train_time:8053ms step_avg:33.56ms
step:241/2000 train_time:8086ms step_avg:33.55ms
step:242/2000 train_time:8120ms step_avg:33.55ms
step:243/2000 train_time:8152ms step_avg:33.55ms
step:244/2000 train_time:8186ms step_avg:33.55ms
step:245/2000 train_time:8219ms step_avg:33.55ms
step:246/2000 train_time:8252ms step_avg:33.55ms
step:247/2000 train_time:8285ms step_avg:33.54ms
step:248/2000 train_time:8319ms step_avg:33.54ms
step:249/2000 train_time:8352ms step_avg:33.54ms
step:250/2000 train_time:8385ms step_avg:33.54ms
step:250/2000 val_loss:4.2591 train_time:8421ms step_avg:33.68ms
step:251/2000 train_time:8446ms step_avg:33.65ms
step:252/2000 train_time:8466ms step_avg:33.60ms
step:253/2000 train_time:8488ms step_avg:33.55ms
step:254/2000 train_time:8522ms step_avg:33.55ms
step:255/2000 train_time:8556ms step_avg:33.55ms
step:256/2000 train_time:8591ms step_avg:33.56ms
step:257/2000 train_time:8625ms step_avg:33.56ms
step:258/2000 train_time:8659ms step_avg:33.56ms
step:259/2000 train_time:8692ms step_avg:33.56ms
step:260/2000 train_time:8726ms step_avg:33.56ms
step:261/2000 train_time:8759ms step_avg:33.56ms
step:262/2000 train_time:8792ms step_avg:33.56ms
step:263/2000 train_time:8825ms step_avg:33.56ms
step:264/2000 train_time:8859ms step_avg:33.56ms
step:265/2000 train_time:8891ms step_avg:33.55ms
step:266/2000 train_time:8925ms step_avg:33.55ms
step:267/2000 train_time:8957ms step_avg:33.55ms
step:268/2000 train_time:8991ms step_avg:33.55ms
step:269/2000 train_time:9023ms step_avg:33.54ms
step:270/2000 train_time:9057ms step_avg:33.54ms
step:271/2000 train_time:9090ms step_avg:33.54ms
step:272/2000 train_time:9123ms step_avg:33.54ms
step:273/2000 train_time:9155ms step_avg:33.54ms
step:274/2000 train_time:9189ms step_avg:33.53ms
step:275/2000 train_time:9221ms step_avg:33.53ms
step:276/2000 train_time:9254ms step_avg:33.53ms
step:277/2000 train_time:9287ms step_avg:33.53ms
step:278/2000 train_time:9321ms step_avg:33.53ms
step:279/2000 train_time:9353ms step_avg:33.52ms
step:280/2000 train_time:9387ms step_avg:33.52ms
step:281/2000 train_time:9420ms step_avg:33.52ms
step:282/2000 train_time:9453ms step_avg:33.52ms
step:283/2000 train_time:9487ms step_avg:33.52ms
step:284/2000 train_time:9521ms step_avg:33.52ms
step:285/2000 train_time:9554ms step_avg:33.52ms
step:286/2000 train_time:9588ms step_avg:33.52ms
step:287/2000 train_time:9621ms step_avg:33.52ms
step:288/2000 train_time:9654ms step_avg:33.52ms
step:289/2000 train_time:9688ms step_avg:33.52ms
step:290/2000 train_time:9721ms step_avg:33.52ms
step:291/2000 train_time:9754ms step_avg:33.52ms
step:292/2000 train_time:9788ms step_avg:33.52ms
step:293/2000 train_time:9821ms step_avg:33.52ms
step:294/2000 train_time:9854ms step_avg:33.52ms
step:295/2000 train_time:9887ms step_avg:33.51ms
step:296/2000 train_time:9920ms step_avg:33.51ms
step:297/2000 train_time:9953ms step_avg:33.51ms
step:298/2000 train_time:9986ms step_avg:33.51ms
step:299/2000 train_time:10019ms step_avg:33.51ms
step:300/2000 train_time:10052ms step_avg:33.51ms
step:301/2000 train_time:10086ms step_avg:33.51ms
step:302/2000 train_time:10119ms step_avg:33.51ms
step:303/2000 train_time:10152ms step_avg:33.51ms
step:304/2000 train_time:10185ms step_avg:33.50ms
step:305/2000 train_time:10218ms step_avg:33.50ms
step:306/2000 train_time:10252ms step_avg:33.50ms
step:307/2000 train_time:10285ms step_avg:33.50ms
step:308/2000 train_time:10318ms step_avg:33.50ms
step:309/2000 train_time:10351ms step_avg:33.50ms
step:310/2000 train_time:10384ms step_avg:33.50ms
step:311/2000 train_time:10417ms step_avg:33.50ms
step:312/2000 train_time:10451ms step_avg:33.50ms
step:313/2000 train_time:10485ms step_avg:33.50ms
step:314/2000 train_time:10518ms step_avg:33.50ms
step:315/2000 train_time:10552ms step_avg:33.50ms
step:316/2000 train_time:10585ms step_avg:33.50ms
step:317/2000 train_time:10619ms step_avg:33.50ms
step:318/2000 train_time:10652ms step_avg:33.50ms
step:319/2000 train_time:10685ms step_avg:33.50ms
step:320/2000 train_time:10719ms step_avg:33.50ms
step:321/2000 train_time:10752ms step_avg:33.49ms
step:322/2000 train_time:10785ms step_avg:33.49ms
step:323/2000 train_time:10818ms step_avg:33.49ms
step:324/2000 train_time:10851ms step_avg:33.49ms
step:325/2000 train_time:10884ms step_avg:33.49ms
step:326/2000 train_time:10917ms step_avg:33.49ms
step:327/2000 train_time:10950ms step_avg:33.49ms
step:328/2000 train_time:10983ms step_avg:33.49ms
step:329/2000 train_time:11016ms step_avg:33.48ms
step:330/2000 train_time:11050ms step_avg:33.48ms
step:331/2000 train_time:11082ms step_avg:33.48ms
step:332/2000 train_time:11116ms step_avg:33.48ms
step:333/2000 train_time:11149ms step_avg:33.48ms
step:334/2000 train_time:11182ms step_avg:33.48ms
step:335/2000 train_time:11215ms step_avg:33.48ms
step:336/2000 train_time:11248ms step_avg:33.48ms
step:337/2000 train_time:11281ms step_avg:33.47ms
step:338/2000 train_time:11314ms step_avg:33.47ms
step:339/2000 train_time:11347ms step_avg:33.47ms
step:340/2000 train_time:11380ms step_avg:33.47ms
step:341/2000 train_time:11413ms step_avg:33.47ms
step:342/2000 train_time:11447ms step_avg:33.47ms
step:343/2000 train_time:11480ms step_avg:33.47ms
step:344/2000 train_time:11513ms step_avg:33.47ms
step:345/2000 train_time:11546ms step_avg:33.47ms
step:346/2000 train_time:11579ms step_avg:33.47ms
step:347/2000 train_time:11612ms step_avg:33.46ms
step:348/2000 train_time:11646ms step_avg:33.47ms
step:349/2000 train_time:11679ms step_avg:33.46ms
step:350/2000 train_time:11712ms step_avg:33.46ms
step:351/2000 train_time:11745ms step_avg:33.46ms
step:352/2000 train_time:11779ms step_avg:33.46ms
step:353/2000 train_time:11812ms step_avg:33.46ms
step:354/2000 train_time:11845ms step_avg:33.46ms
step:355/2000 train_time:11878ms step_avg:33.46ms
step:356/2000 train_time:11911ms step_avg:33.46ms
step:357/2000 train_time:11944ms step_avg:33.46ms
step:358/2000 train_time:11978ms step_avg:33.46ms
step:359/2000 train_time:12010ms step_avg:33.46ms
step:360/2000 train_time:12044ms step_avg:33.45ms
step:361/2000 train_time:12077ms step_avg:33.45ms
step:362/2000 train_time:12111ms step_avg:33.45ms
step:363/2000 train_time:12144ms step_avg:33.45ms
step:364/2000 train_time:12177ms step_avg:33.45ms
step:365/2000 train_time:12210ms step_avg:33.45ms
step:366/2000 train_time:12243ms step_avg:33.45ms
step:367/2000 train_time:12276ms step_avg:33.45ms
step:368/2000 train_time:12309ms step_avg:33.45ms
step:369/2000 train_time:12342ms step_avg:33.45ms
step:370/2000 train_time:12375ms step_avg:33.45ms
step:371/2000 train_time:12408ms step_avg:33.44ms
step:372/2000 train_time:12441ms step_avg:33.44ms
step:373/2000 train_time:12474ms step_avg:33.44ms
step:374/2000 train_time:12508ms step_avg:33.44ms
step:375/2000 train_time:12541ms step_avg:33.44ms
step:376/2000 train_time:12574ms step_avg:33.44ms
step:377/2000 train_time:12607ms step_avg:33.44ms
step:378/2000 train_time:12640ms step_avg:33.44ms
step:379/2000 train_time:12673ms step_avg:33.44ms
step:380/2000 train_time:12707ms step_avg:33.44ms
step:381/2000 train_time:12740ms step_avg:33.44ms
step:382/2000 train_time:12773ms step_avg:33.44ms
step:383/2000 train_time:12807ms step_avg:33.44ms
step:384/2000 train_time:12840ms step_avg:33.44ms
step:385/2000 train_time:12873ms step_avg:33.44ms
step:386/2000 train_time:12906ms step_avg:33.44ms
step:387/2000 train_time:12939ms step_avg:33.43ms
step:388/2000 train_time:12972ms step_avg:33.43ms
step:389/2000 train_time:13006ms step_avg:33.43ms
step:390/2000 train_time:13039ms step_avg:33.43ms
step:391/2000 train_time:13072ms step_avg:33.43ms
step:392/2000 train_time:13106ms step_avg:33.43ms
step:393/2000 train_time:13138ms step_avg:33.43ms
step:394/2000 train_time:13172ms step_avg:33.43ms
step:395/2000 train_time:13205ms step_avg:33.43ms
step:396/2000 train_time:13238ms step_avg:33.43ms
step:397/2000 train_time:13271ms step_avg:33.43ms
step:398/2000 train_time:13305ms step_avg:33.43ms
step:399/2000 train_time:13338ms step_avg:33.43ms
step:400/2000 train_time:13371ms step_avg:33.43ms
step:401/2000 train_time:13404ms step_avg:33.43ms
step:402/2000 train_time:13438ms step_avg:33.43ms
step:403/2000 train_time:13471ms step_avg:33.43ms
step:404/2000 train_time:13504ms step_avg:33.43ms
step:405/2000 train_time:13538ms step_avg:33.43ms
step:406/2000 train_time:13571ms step_avg:33.43ms
step:407/2000 train_time:13605ms step_avg:33.43ms
step:408/2000 train_time:13638ms step_avg:33.43ms
step:409/2000 train_time:13671ms step_avg:33.43ms
step:410/2000 train_time:13705ms step_avg:33.43ms
step:411/2000 train_time:13738ms step_avg:33.42ms
step:412/2000 train_time:13771ms step_avg:33.42ms
step:413/2000 train_time:13804ms step_avg:33.42ms
step:414/2000 train_time:13838ms step_avg:33.43ms
step:415/2000 train_time:13871ms step_avg:33.42ms
step:416/2000 train_time:13905ms step_avg:33.42ms
step:417/2000 train_time:13938ms step_avg:33.42ms
step:418/2000 train_time:13971ms step_avg:33.42ms
step:419/2000 train_time:14004ms step_avg:33.42ms
step:420/2000 train_time:14038ms step_avg:33.42ms
step:421/2000 train_time:14070ms step_avg:33.42ms
step:422/2000 train_time:14104ms step_avg:33.42ms
step:423/2000 train_time:14137ms step_avg:33.42ms
step:424/2000 train_time:14170ms step_avg:33.42ms
step:425/2000 train_time:14203ms step_avg:33.42ms
step:426/2000 train_time:14237ms step_avg:33.42ms
step:427/2000 train_time:14270ms step_avg:33.42ms
step:428/2000 train_time:14303ms step_avg:33.42ms
step:429/2000 train_time:14336ms step_avg:33.42ms
step:430/2000 train_time:14369ms step_avg:33.42ms
step:431/2000 train_time:14403ms step_avg:33.42ms
step:432/2000 train_time:14436ms step_avg:33.42ms
step:433/2000 train_time:14469ms step_avg:33.42ms
step:434/2000 train_time:14502ms step_avg:33.42ms
step:435/2000 train_time:14535ms step_avg:33.41ms
step:436/2000 train_time:14568ms step_avg:33.41ms
step:437/2000 train_time:14601ms step_avg:33.41ms
step:438/2000 train_time:14634ms step_avg:33.41ms
step:439/2000 train_time:14667ms step_avg:33.41ms
step:440/2000 train_time:14700ms step_avg:33.41ms
step:441/2000 train_time:14733ms step_avg:33.41ms
step:442/2000 train_time:14767ms step_avg:33.41ms
step:443/2000 train_time:14801ms step_avg:33.41ms
step:444/2000 train_time:14834ms step_avg:33.41ms
step:445/2000 train_time:14867ms step_avg:33.41ms
step:446/2000 train_time:14901ms step_avg:33.41ms
step:447/2000 train_time:14934ms step_avg:33.41ms
step:448/2000 train_time:14967ms step_avg:33.41ms
step:449/2000 train_time:15000ms step_avg:33.41ms
step:450/2000 train_time:15033ms step_avg:33.41ms
step:451/2000 train_time:15066ms step_avg:33.41ms
step:452/2000 train_time:15100ms step_avg:33.41ms
step:453/2000 train_time:15133ms step_avg:33.41ms
step:454/2000 train_time:15166ms step_avg:33.41ms
step:455/2000 train_time:15199ms step_avg:33.40ms
step:456/2000 train_time:15232ms step_avg:33.40ms
step:457/2000 train_time:15266ms step_avg:33.40ms
step:458/2000 train_time:15299ms step_avg:33.40ms
step:459/2000 train_time:15332ms step_avg:33.40ms
step:460/2000 train_time:15366ms step_avg:33.40ms
step:461/2000 train_time:15399ms step_avg:33.40ms
step:462/2000 train_time:15433ms step_avg:33.40ms
step:463/2000 train_time:15466ms step_avg:33.40ms
step:464/2000 train_time:15499ms step_avg:33.40ms
step:465/2000 train_time:15532ms step_avg:33.40ms
step:466/2000 train_time:15566ms step_avg:33.40ms
step:467/2000 train_time:15598ms step_avg:33.40ms
step:468/2000 train_time:15632ms step_avg:33.40ms
step:469/2000 train_time:15665ms step_avg:33.40ms
step:470/2000 train_time:15698ms step_avg:33.40ms
step:471/2000 train_time:15731ms step_avg:33.40ms
step:472/2000 train_time:15765ms step_avg:33.40ms
step:473/2000 train_time:15798ms step_avg:33.40ms
step:474/2000 train_time:15831ms step_avg:33.40ms
step:475/2000 train_time:15864ms step_avg:33.40ms
step:476/2000 train_time:15898ms step_avg:33.40ms
step:477/2000 train_time:15930ms step_avg:33.40ms
step:478/2000 train_time:15964ms step_avg:33.40ms
step:479/2000 train_time:15997ms step_avg:33.40ms
step:480/2000 train_time:16030ms step_avg:33.40ms
step:481/2000 train_time:16063ms step_avg:33.40ms
step:482/2000 train_time:16096ms step_avg:33.40ms
step:483/2000 train_time:16130ms step_avg:33.39ms
step:484/2000 train_time:16163ms step_avg:33.40ms
step:485/2000 train_time:16197ms step_avg:33.39ms
step:486/2000 train_time:16231ms step_avg:33.40ms
step:487/2000 train_time:16264ms step_avg:33.40ms
step:488/2000 train_time:16297ms step_avg:33.40ms
step:489/2000 train_time:16330ms step_avg:33.39ms
step:490/2000 train_time:16363ms step_avg:33.39ms
step:491/2000 train_time:16396ms step_avg:33.39ms
step:492/2000 train_time:16429ms step_avg:33.39ms
step:493/2000 train_time:16462ms step_avg:33.39ms
step:494/2000 train_time:16496ms step_avg:33.39ms
step:495/2000 train_time:16529ms step_avg:33.39ms
step:496/2000 train_time:16562ms step_avg:33.39ms
step:497/2000 train_time:16595ms step_avg:33.39ms
step:498/2000 train_time:16629ms step_avg:33.39ms
step:499/2000 train_time:16661ms step_avg:33.39ms
step:500/2000 train_time:16695ms step_avg:33.39ms
step:500/2000 val_loss:3.9957 train_time:16731ms step_avg:33.46ms
step:501/2000 train_time:16752ms step_avg:33.44ms
step:502/2000 train_time:16771ms step_avg:33.41ms
step:503/2000 train_time:16798ms step_avg:33.40ms
step:504/2000 train_time:16833ms step_avg:33.40ms
step:505/2000 train_time:16867ms step_avg:33.40ms
step:506/2000 train_time:16902ms step_avg:33.40ms
step:507/2000 train_time:16935ms step_avg:33.40ms
step:508/2000 train_time:16969ms step_avg:33.40ms
step:509/2000 train_time:17002ms step_avg:33.40ms
step:510/2000 train_time:17035ms step_avg:33.40ms
step:511/2000 train_time:17068ms step_avg:33.40ms
step:512/2000 train_time:17102ms step_avg:33.40ms
step:513/2000 train_time:17135ms step_avg:33.40ms
step:514/2000 train_time:17168ms step_avg:33.40ms
step:515/2000 train_time:17200ms step_avg:33.40ms
step:516/2000 train_time:17234ms step_avg:33.40ms
step:517/2000 train_time:17266ms step_avg:33.40ms
step:518/2000 train_time:17299ms step_avg:33.40ms
step:519/2000 train_time:17332ms step_avg:33.40ms
step:520/2000 train_time:17365ms step_avg:33.39ms
step:521/2000 train_time:17398ms step_avg:33.39ms
step:522/2000 train_time:17431ms step_avg:33.39ms
step:523/2000 train_time:17463ms step_avg:33.39ms
step:524/2000 train_time:17497ms step_avg:33.39ms
step:525/2000 train_time:17529ms step_avg:33.39ms
step:526/2000 train_time:17562ms step_avg:33.39ms
step:527/2000 train_time:17595ms step_avg:33.39ms
step:528/2000 train_time:17628ms step_avg:33.39ms
step:529/2000 train_time:17661ms step_avg:33.39ms
step:530/2000 train_time:17695ms step_avg:33.39ms
step:531/2000 train_time:17728ms step_avg:33.39ms
step:532/2000 train_time:17762ms step_avg:33.39ms
step:533/2000 train_time:17795ms step_avg:33.39ms
step:534/2000 train_time:17829ms step_avg:33.39ms
step:535/2000 train_time:17863ms step_avg:33.39ms
step:536/2000 train_time:17897ms step_avg:33.39ms
step:537/2000 train_time:17930ms step_avg:33.39ms
step:538/2000 train_time:17964ms step_avg:33.39ms
step:539/2000 train_time:17997ms step_avg:33.39ms
step:540/2000 train_time:18030ms step_avg:33.39ms
step:541/2000 train_time:18063ms step_avg:33.39ms
step:542/2000 train_time:18097ms step_avg:33.39ms
step:543/2000 train_time:18130ms step_avg:33.39ms
step:544/2000 train_time:18163ms step_avg:33.39ms
step:545/2000 train_time:18196ms step_avg:33.39ms
step:546/2000 train_time:18229ms step_avg:33.39ms
step:547/2000 train_time:18262ms step_avg:33.39ms
step:548/2000 train_time:18295ms step_avg:33.39ms
step:549/2000 train_time:18328ms step_avg:33.38ms
step:550/2000 train_time:18361ms step_avg:33.38ms
step:551/2000 train_time:18394ms step_avg:33.38ms
step:552/2000 train_time:18428ms step_avg:33.38ms
step:553/2000 train_time:18461ms step_avg:33.38ms
step:554/2000 train_time:18494ms step_avg:33.38ms
step:555/2000 train_time:18527ms step_avg:33.38ms
step:556/2000 train_time:18560ms step_avg:33.38ms
step:557/2000 train_time:18593ms step_avg:33.38ms
step:558/2000 train_time:18626ms step_avg:33.38ms
step:559/2000 train_time:18659ms step_avg:33.38ms
step:560/2000 train_time:18693ms step_avg:33.38ms
step:561/2000 train_time:18726ms step_avg:33.38ms
step:562/2000 train_time:18760ms step_avg:33.38ms
step:563/2000 train_time:18793ms step_avg:33.38ms
step:564/2000 train_time:18827ms step_avg:33.38ms
step:565/2000 train_time:18860ms step_avg:33.38ms
step:566/2000 train_time:18894ms step_avg:33.38ms
step:567/2000 train_time:18927ms step_avg:33.38ms
step:568/2000 train_time:18960ms step_avg:33.38ms
step:569/2000 train_time:18994ms step_avg:33.38ms
step:570/2000 train_time:19027ms step_avg:33.38ms
step:571/2000 train_time:19060ms step_avg:33.38ms
step:572/2000 train_time:19093ms step_avg:33.38ms
step:573/2000 train_time:19126ms step_avg:33.38ms
step:574/2000 train_time:19160ms step_avg:33.38ms
step:575/2000 train_time:19193ms step_avg:33.38ms
step:576/2000 train_time:19226ms step_avg:33.38ms
step:577/2000 train_time:19260ms step_avg:33.38ms
step:578/2000 train_time:19293ms step_avg:33.38ms
step:579/2000 train_time:19326ms step_avg:33.38ms
step:580/2000 train_time:19360ms step_avg:33.38ms
step:581/2000 train_time:19393ms step_avg:33.38ms
step:582/2000 train_time:19426ms step_avg:33.38ms
step:583/2000 train_time:19459ms step_avg:33.38ms
step:584/2000 train_time:19492ms step_avg:33.38ms
step:585/2000 train_time:19525ms step_avg:33.38ms
step:586/2000 train_time:19558ms step_avg:33.38ms
step:587/2000 train_time:19591ms step_avg:33.37ms
step:588/2000 train_time:19624ms step_avg:33.37ms
step:589/2000 train_time:19657ms step_avg:33.37ms
step:590/2000 train_time:19690ms step_avg:33.37ms
step:591/2000 train_time:19723ms step_avg:33.37ms
step:592/2000 train_time:19756ms step_avg:33.37ms
step:593/2000 train_time:19789ms step_avg:33.37ms
step:594/2000 train_time:19823ms step_avg:33.37ms
step:595/2000 train_time:19856ms step_avg:33.37ms
step:596/2000 train_time:19889ms step_avg:33.37ms
step:597/2000 train_time:19922ms step_avg:33.37ms
step:598/2000 train_time:19955ms step_avg:33.37ms
step:599/2000 train_time:19989ms step_avg:33.37ms
step:600/2000 train_time:20022ms step_avg:33.37ms
step:601/2000 train_time:20055ms step_avg:33.37ms
step:602/2000 train_time:20089ms step_avg:33.37ms
step:603/2000 train_time:20122ms step_avg:33.37ms
step:604/2000 train_time:20156ms step_avg:33.37ms
step:605/2000 train_time:20189ms step_avg:33.37ms
step:606/2000 train_time:20222ms step_avg:33.37ms
step:607/2000 train_time:20255ms step_avg:33.37ms
step:608/2000 train_time:20288ms step_avg:33.37ms
step:609/2000 train_time:20321ms step_avg:33.37ms
step:610/2000 train_time:20355ms step_avg:33.37ms
step:611/2000 train_time:20388ms step_avg:33.37ms
step:612/2000 train_time:20421ms step_avg:33.37ms
step:613/2000 train_time:20454ms step_avg:33.37ms
step:614/2000 train_time:20487ms step_avg:33.37ms
step:615/2000 train_time:20520ms step_avg:33.37ms
step:616/2000 train_time:20554ms step_avg:33.37ms
step:617/2000 train_time:20587ms step_avg:33.37ms
step:618/2000 train_time:20620ms step_avg:33.37ms
step:619/2000 train_time:20653ms step_avg:33.37ms
step:620/2000 train_time:20687ms step_avg:33.37ms
step:621/2000 train_time:20720ms step_avg:33.36ms
step:622/2000 train_time:20753ms step_avg:33.36ms
step:623/2000 train_time:20786ms step_avg:33.36ms
step:624/2000 train_time:20819ms step_avg:33.36ms
step:625/2000 train_time:20853ms step_avg:33.36ms
step:626/2000 train_time:20886ms step_avg:33.36ms
step:627/2000 train_time:20919ms step_avg:33.36ms
step:628/2000 train_time:20953ms step_avg:33.36ms
step:629/2000 train_time:20986ms step_avg:33.36ms
step:630/2000 train_time:21019ms step_avg:33.36ms
step:631/2000 train_time:21053ms step_avg:33.36ms
step:632/2000 train_time:21086ms step_avg:33.36ms
step:633/2000 train_time:21119ms step_avg:33.36ms
step:634/2000 train_time:21152ms step_avg:33.36ms
step:635/2000 train_time:21186ms step_avg:33.36ms
step:636/2000 train_time:21219ms step_avg:33.36ms
step:637/2000 train_time:21252ms step_avg:33.36ms
step:638/2000 train_time:21285ms step_avg:33.36ms
step:639/2000 train_time:21318ms step_avg:33.36ms
step:640/2000 train_time:21351ms step_avg:33.36ms
step:641/2000 train_time:21384ms step_avg:33.36ms
step:642/2000 train_time:21418ms step_avg:33.36ms
step:643/2000 train_time:21451ms step_avg:33.36ms
step:644/2000 train_time:21484ms step_avg:33.36ms
step:645/2000 train_time:21517ms step_avg:33.36ms
step:646/2000 train_time:21550ms step_avg:33.36ms
step:647/2000 train_time:21583ms step_avg:33.36ms
step:648/2000 train_time:21617ms step_avg:33.36ms
step:649/2000 train_time:21650ms step_avg:33.36ms
step:650/2000 train_time:21683ms step_avg:33.36ms
step:651/2000 train_time:21716ms step_avg:33.36ms
step:652/2000 train_time:21750ms step_avg:33.36ms
step:653/2000 train_time:21783ms step_avg:33.36ms
step:654/2000 train_time:21816ms step_avg:33.36ms
step:655/2000 train_time:21850ms step_avg:33.36ms
step:656/2000 train_time:21909ms step_avg:33.40ms
step:657/2000 train_time:21969ms step_avg:33.44ms
step:658/2000 train_time:22030ms step_avg:33.48ms
step:659/2000 train_time:22090ms step_avg:33.52ms
step:660/2000 train_time:22150ms step_avg:33.56ms
step:661/2000 train_time:22211ms step_avg:33.60ms
step:662/2000 train_time:22270ms step_avg:33.64ms
step:663/2000 train_time:22331ms step_avg:33.68ms
step:664/2000 train_time:22390ms step_avg:33.72ms
step:665/2000 train_time:22451ms step_avg:33.76ms
step:666/2000 train_time:22510ms step_avg:33.80ms
step:667/2000 train_time:22571ms step_avg:33.84ms
step:668/2000 train_time:22631ms step_avg:33.88ms
step:669/2000 train_time:22692ms step_avg:33.92ms
step:670/2000 train_time:22751ms step_avg:33.96ms
step:671/2000 train_time:22811ms step_avg:34.00ms
step:672/2000 train_time:22871ms step_avg:34.03ms
step:673/2000 train_time:22931ms step_avg:34.07ms
step:674/2000 train_time:22991ms step_avg:34.11ms
step:675/2000 train_time:23051ms step_avg:34.15ms
step:676/2000 train_time:23111ms step_avg:34.19ms
step:677/2000 train_time:23172ms step_avg:34.23ms
step:678/2000 train_time:23231ms step_avg:34.26ms
step:679/2000 train_time:23292ms step_avg:34.30ms
step:680/2000 train_time:23351ms step_avg:34.34ms
step:681/2000 train_time:23411ms step_avg:34.38ms
step:682/2000 train_time:23471ms step_avg:34.41ms
step:683/2000 train_time:23531ms step_avg:34.45ms
step:684/2000 train_time:23591ms step_avg:34.49ms
step:685/2000 train_time:23651ms step_avg:34.53ms
step:686/2000 train_time:23711ms step_avg:34.56ms
step:687/2000 train_time:23771ms step_avg:34.60ms
step:688/2000 train_time:23831ms step_avg:34.64ms
step:689/2000 train_time:23891ms step_avg:34.68ms
step:690/2000 train_time:23951ms step_avg:34.71ms
step:691/2000 train_time:24011ms step_avg:34.75ms
step:692/2000 train_time:24070ms step_avg:34.78ms
step:693/2000 train_time:24131ms step_avg:34.82ms
step:694/2000 train_time:24190ms step_avg:34.86ms
step:695/2000 train_time:24251ms step_avg:34.89ms
step:696/2000 train_time:24311ms step_avg:34.93ms
step:697/2000 train_time:24372ms step_avg:34.97ms
step:698/2000 train_time:24432ms step_avg:35.00ms
step:699/2000 train_time:24492ms step_avg:35.04ms
step:700/2000 train_time:24551ms step_avg:35.07ms
step:701/2000 train_time:24612ms step_avg:35.11ms
step:702/2000 train_time:24671ms step_avg:35.14ms
step:703/2000 train_time:24731ms step_avg:35.18ms
step:704/2000 train_time:24790ms step_avg:35.21ms
step:705/2000 train_time:24851ms step_avg:35.25ms
step:706/2000 train_time:24910ms step_avg:35.28ms
step:707/2000 train_time:24971ms step_avg:35.32ms
step:708/2000 train_time:25031ms step_avg:35.35ms
step:709/2000 train_time:25091ms step_avg:35.39ms
step:710/2000 train_time:25151ms step_avg:35.42ms
step:711/2000 train_time:25212ms step_avg:35.46ms
step:712/2000 train_time:25271ms step_avg:35.49ms
step:713/2000 train_time:25332ms step_avg:35.53ms
step:714/2000 train_time:25391ms step_avg:35.56ms
step:715/2000 train_time:25452ms step_avg:35.60ms
step:716/2000 train_time:25511ms step_avg:35.63ms
step:717/2000 train_time:25571ms step_avg:35.66ms
step:718/2000 train_time:25631ms step_avg:35.70ms
step:719/2000 train_time:25692ms step_avg:35.73ms
step:720/2000 train_time:25750ms step_avg:35.76ms
step:721/2000 train_time:25811ms step_avg:35.80ms
step:722/2000 train_time:25870ms step_avg:35.83ms
step:723/2000 train_time:25931ms step_avg:35.87ms
step:724/2000 train_time:25990ms step_avg:35.90ms
step:725/2000 train_time:26051ms step_avg:35.93ms
step:726/2000 train_time:26111ms step_avg:35.97ms
step:727/2000 train_time:26171ms step_avg:36.00ms
step:728/2000 train_time:26231ms step_avg:36.03ms
step:729/2000 train_time:26292ms step_avg:36.07ms
step:730/2000 train_time:26351ms step_avg:36.10ms
step:731/2000 train_time:26411ms step_avg:36.13ms
step:732/2000 train_time:26470ms step_avg:36.16ms
step:733/2000 train_time:26531ms step_avg:36.20ms
step:734/2000 train_time:26591ms step_avg:36.23ms
step:735/2000 train_time:26652ms step_avg:36.26ms
step:736/2000 train_time:26711ms step_avg:36.29ms
step:737/2000 train_time:26771ms step_avg:36.32ms
step:738/2000 train_time:26830ms step_avg:36.36ms
step:739/2000 train_time:26891ms step_avg:36.39ms
step:740/2000 train_time:26950ms step_avg:36.42ms
step:741/2000 train_time:27011ms step_avg:36.45ms
step:742/2000 train_time:27071ms step_avg:36.48ms
step:743/2000 train_time:27131ms step_avg:36.52ms
step:744/2000 train_time:27191ms step_avg:36.55ms
step:745/2000 train_time:27251ms step_avg:36.58ms
step:746/2000 train_time:27311ms step_avg:36.61ms
step:747/2000 train_time:27372ms step_avg:36.64ms
step:748/2000 train_time:27432ms step_avg:36.67ms
step:749/2000 train_time:27493ms step_avg:36.71ms
step:750/2000 train_time:27552ms step_avg:36.74ms
step:750/2000 val_loss:3.8222 train_time:27615ms step_avg:36.82ms
step:751/2000 train_time:27638ms step_avg:36.80ms
step:752/2000 train_time:27676ms step_avg:36.80ms
step:753/2000 train_time:27740ms step_avg:36.84ms
step:754/2000 train_time:27802ms step_avg:36.87ms
step:755/2000 train_time:27863ms step_avg:36.91ms
step:756/2000 train_time:27924ms step_avg:36.94ms
step:757/2000 train_time:27984ms step_avg:36.97ms
step:758/2000 train_time:28043ms step_avg:37.00ms
step:759/2000 train_time:28103ms step_avg:37.03ms
step:760/2000 train_time:28162ms step_avg:37.06ms
step:761/2000 train_time:28222ms step_avg:37.08ms
step:762/2000 train_time:28281ms step_avg:37.11ms
step:763/2000 train_time:28340ms step_avg:37.14ms
step:764/2000 train_time:28400ms step_avg:37.17ms
step:765/2000 train_time:28460ms step_avg:37.20ms
step:766/2000 train_time:28520ms step_avg:37.23ms
step:767/2000 train_time:28584ms step_avg:37.27ms
step:768/2000 train_time:28646ms step_avg:37.30ms
step:769/2000 train_time:28709ms step_avg:37.33ms
step:770/2000 train_time:28771ms step_avg:37.37ms
step:771/2000 train_time:28832ms step_avg:37.40ms
step:772/2000 train_time:28892ms step_avg:37.42ms
step:773/2000 train_time:28951ms step_avg:37.45ms
step:774/2000 train_time:29011ms step_avg:37.48ms
step:775/2000 train_time:29071ms step_avg:37.51ms
step:776/2000 train_time:29130ms step_avg:37.54ms
step:777/2000 train_time:29190ms step_avg:37.57ms
step:778/2000 train_time:29248ms step_avg:37.59ms
step:779/2000 train_time:29309ms step_avg:37.62ms
step:780/2000 train_time:29368ms step_avg:37.65ms
step:781/2000 train_time:29429ms step_avg:37.68ms
step:782/2000 train_time:29490ms step_avg:37.71ms
step:783/2000 train_time:29551ms step_avg:37.74ms
step:784/2000 train_time:29611ms step_avg:37.77ms
step:785/2000 train_time:29673ms step_avg:37.80ms
step:786/2000 train_time:29733ms step_avg:37.83ms
step:787/2000 train_time:29794ms step_avg:37.86ms
step:788/2000 train_time:29854ms step_avg:37.89ms
step:789/2000 train_time:29914ms step_avg:37.91ms
step:790/2000 train_time:29973ms step_avg:37.94ms
step:791/2000 train_time:30033ms step_avg:37.97ms
step:792/2000 train_time:30091ms step_avg:37.99ms
step:793/2000 train_time:30151ms step_avg:38.02ms
step:794/2000 train_time:30210ms step_avg:38.05ms
step:795/2000 train_time:30270ms step_avg:38.08ms
step:796/2000 train_time:30330ms step_avg:38.10ms
step:797/2000 train_time:30390ms step_avg:38.13ms
step:798/2000 train_time:30450ms step_avg:38.16ms
step:799/2000 train_time:30510ms step_avg:38.19ms
step:800/2000 train_time:30570ms step_avg:38.21ms
step:801/2000 train_time:30632ms step_avg:38.24ms
step:802/2000 train_time:30691ms step_avg:38.27ms
step:803/2000 train_time:30752ms step_avg:38.30ms
step:804/2000 train_time:30812ms step_avg:38.32ms
step:805/2000 train_time:30873ms step_avg:38.35ms
step:806/2000 train_time:30932ms step_avg:38.38ms
step:807/2000 train_time:30992ms step_avg:38.40ms
step:808/2000 train_time:31051ms step_avg:38.43ms
step:809/2000 train_time:31112ms step_avg:38.46ms
step:810/2000 train_time:31171ms step_avg:38.48ms
step:811/2000 train_time:31230ms step_avg:38.51ms
step:812/2000 train_time:31290ms step_avg:38.53ms
step:813/2000 train_time:31350ms step_avg:38.56ms
step:814/2000 train_time:31409ms step_avg:38.59ms
step:815/2000 train_time:31470ms step_avg:38.61ms
step:816/2000 train_time:31530ms step_avg:38.64ms
step:817/2000 train_time:31591ms step_avg:38.67ms
step:818/2000 train_time:31650ms step_avg:38.69ms
step:819/2000 train_time:31711ms step_avg:38.72ms
step:820/2000 train_time:31771ms step_avg:38.75ms
step:821/2000 train_time:31832ms step_avg:38.77ms
step:822/2000 train_time:31891ms step_avg:38.80ms
step:823/2000 train_time:31951ms step_avg:38.82ms
step:824/2000 train_time:32010ms step_avg:38.85ms
step:825/2000 train_time:32071ms step_avg:38.87ms
step:826/2000 train_time:32130ms step_avg:38.90ms
step:827/2000 train_time:32190ms step_avg:38.92ms
step:828/2000 train_time:32249ms step_avg:38.95ms
step:829/2000 train_time:32310ms step_avg:38.97ms
step:830/2000 train_time:32370ms step_avg:39.00ms
step:831/2000 train_time:32430ms step_avg:39.03ms
step:832/2000 train_time:32490ms step_avg:39.05ms
step:833/2000 train_time:32552ms step_avg:39.08ms
step:834/2000 train_time:32611ms step_avg:39.10ms
step:835/2000 train_time:32672ms step_avg:39.13ms
step:836/2000 train_time:32732ms step_avg:39.15ms
step:837/2000 train_time:32792ms step_avg:39.18ms
step:838/2000 train_time:32852ms step_avg:39.20ms
step:839/2000 train_time:32912ms step_avg:39.23ms
step:840/2000 train_time:32972ms step_avg:39.25ms
step:841/2000 train_time:33032ms step_avg:39.28ms
step:842/2000 train_time:33092ms step_avg:39.30ms
step:843/2000 train_time:33152ms step_avg:39.33ms
step:844/2000 train_time:33210ms step_avg:39.35ms
step:845/2000 train_time:33270ms step_avg:39.37ms
step:846/2000 train_time:33330ms step_avg:39.40ms
step:847/2000 train_time:33390ms step_avg:39.42ms
step:848/2000 train_time:33449ms step_avg:39.44ms
step:849/2000 train_time:33510ms step_avg:39.47ms
step:850/2000 train_time:33570ms step_avg:39.49ms
step:851/2000 train_time:33631ms step_avg:39.52ms
step:852/2000 train_time:33691ms step_avg:39.54ms
step:853/2000 train_time:33751ms step_avg:39.57ms
step:854/2000 train_time:33811ms step_avg:39.59ms
step:855/2000 train_time:33872ms step_avg:39.62ms
step:856/2000 train_time:33931ms step_avg:39.64ms
step:857/2000 train_time:33992ms step_avg:39.66ms
step:858/2000 train_time:34051ms step_avg:39.69ms
step:859/2000 train_time:34111ms step_avg:39.71ms
step:860/2000 train_time:34171ms step_avg:39.73ms
step:861/2000 train_time:34232ms step_avg:39.76ms
step:862/2000 train_time:34291ms step_avg:39.78ms
step:863/2000 train_time:34350ms step_avg:39.80ms
step:864/2000 train_time:34410ms step_avg:39.83ms
step:865/2000 train_time:34471ms step_avg:39.85ms
step:866/2000 train_time:34531ms step_avg:39.87ms
step:867/2000 train_time:34591ms step_avg:39.90ms
step:868/2000 train_time:34651ms step_avg:39.92ms
step:869/2000 train_time:34712ms step_avg:39.95ms
step:870/2000 train_time:34773ms step_avg:39.97ms
step:871/2000 train_time:34833ms step_avg:39.99ms
step:872/2000 train_time:34892ms step_avg:40.01ms
step:873/2000 train_time:34952ms step_avg:40.04ms
step:874/2000 train_time:35012ms step_avg:40.06ms
step:875/2000 train_time:35073ms step_avg:40.08ms
step:876/2000 train_time:35132ms step_avg:40.11ms
step:877/2000 train_time:35192ms step_avg:40.13ms
step:878/2000 train_time:35251ms step_avg:40.15ms
step:879/2000 train_time:35311ms step_avg:40.17ms
step:880/2000 train_time:35370ms step_avg:40.19ms
step:881/2000 train_time:35431ms step_avg:40.22ms
step:882/2000 train_time:35491ms step_avg:40.24ms
step:883/2000 train_time:35551ms step_avg:40.26ms
step:884/2000 train_time:35610ms step_avg:40.28ms
step:885/2000 train_time:35672ms step_avg:40.31ms
step:886/2000 train_time:35732ms step_avg:40.33ms
step:887/2000 train_time:35792ms step_avg:40.35ms
step:888/2000 train_time:35852ms step_avg:40.37ms
step:889/2000 train_time:35913ms step_avg:40.40ms
step:890/2000 train_time:35972ms step_avg:40.42ms
step:891/2000 train_time:36032ms step_avg:40.44ms
step:892/2000 train_time:36092ms step_avg:40.46ms
step:893/2000 train_time:36151ms step_avg:40.48ms
step:894/2000 train_time:36211ms step_avg:40.50ms
step:895/2000 train_time:36271ms step_avg:40.53ms
step:896/2000 train_time:36331ms step_avg:40.55ms
step:897/2000 train_time:36391ms step_avg:40.57ms
step:898/2000 train_time:36450ms step_avg:40.59ms
step:899/2000 train_time:36511ms step_avg:40.61ms
step:900/2000 train_time:36571ms step_avg:40.63ms
step:901/2000 train_time:36632ms step_avg:40.66ms
step:902/2000 train_time:36691ms step_avg:40.68ms
step:903/2000 train_time:36752ms step_avg:40.70ms
step:904/2000 train_time:36812ms step_avg:40.72ms
step:905/2000 train_time:36873ms step_avg:40.74ms
step:906/2000 train_time:36932ms step_avg:40.76ms
step:907/2000 train_time:36993ms step_avg:40.79ms
step:908/2000 train_time:37053ms step_avg:40.81ms
step:909/2000 train_time:37113ms step_avg:40.83ms
step:910/2000 train_time:37173ms step_avg:40.85ms
step:911/2000 train_time:37234ms step_avg:40.87ms
step:912/2000 train_time:37293ms step_avg:40.89ms
step:913/2000 train_time:37353ms step_avg:40.91ms
step:914/2000 train_time:37412ms step_avg:40.93ms
step:915/2000 train_time:37472ms step_avg:40.95ms
step:916/2000 train_time:37531ms step_avg:40.97ms
step:917/2000 train_time:37592ms step_avg:40.99ms
step:918/2000 train_time:37651ms step_avg:41.01ms
step:919/2000 train_time:37711ms step_avg:41.04ms
step:920/2000 train_time:37772ms step_avg:41.06ms
step:921/2000 train_time:37832ms step_avg:41.08ms
step:922/2000 train_time:37891ms step_avg:41.10ms
step:923/2000 train_time:37952ms step_avg:41.12ms
step:924/2000 train_time:38011ms step_avg:41.14ms
step:925/2000 train_time:38071ms step_avg:41.16ms
step:926/2000 train_time:38131ms step_avg:41.18ms
step:927/2000 train_time:38192ms step_avg:41.20ms
step:928/2000 train_time:38252ms step_avg:41.22ms
step:929/2000 train_time:38312ms step_avg:41.24ms
step:930/2000 train_time:38371ms step_avg:41.26ms
step:931/2000 train_time:38431ms step_avg:41.28ms
step:932/2000 train_time:38490ms step_avg:41.30ms
step:933/2000 train_time:38551ms step_avg:41.32ms
step:934/2000 train_time:38610ms step_avg:41.34ms
step:935/2000 train_time:38671ms step_avg:41.36ms
step:936/2000 train_time:38730ms step_avg:41.38ms
step:937/2000 train_time:38791ms step_avg:41.40ms
step:938/2000 train_time:38851ms step_avg:41.42ms
step:939/2000 train_time:38911ms step_avg:41.44ms
step:940/2000 train_time:38971ms step_avg:41.46ms
step:941/2000 train_time:39031ms step_avg:41.48ms
step:942/2000 train_time:39091ms step_avg:41.50ms
step:943/2000 train_time:39152ms step_avg:41.52ms
step:944/2000 train_time:39211ms step_avg:41.54ms
step:945/2000 train_time:39272ms step_avg:41.56ms
step:946/2000 train_time:39331ms step_avg:41.58ms
step:947/2000 train_time:39392ms step_avg:41.60ms
step:948/2000 train_time:39451ms step_avg:41.62ms
step:949/2000 train_time:39511ms step_avg:41.63ms
step:950/2000 train_time:39571ms step_avg:41.65ms
step:951/2000 train_time:39631ms step_avg:41.67ms
step:952/2000 train_time:39691ms step_avg:41.69ms
step:953/2000 train_time:39751ms step_avg:41.71ms
step:954/2000 train_time:39810ms step_avg:41.73ms
step:955/2000 train_time:39871ms step_avg:41.75ms
step:956/2000 train_time:39931ms step_avg:41.77ms
step:957/2000 train_time:39991ms step_avg:41.79ms
step:958/2000 train_time:40050ms step_avg:41.81ms
step:959/2000 train_time:40111ms step_avg:41.83ms
step:960/2000 train_time:40171ms step_avg:41.84ms
step:961/2000 train_time:40231ms step_avg:41.86ms
step:962/2000 train_time:40290ms step_avg:41.88ms
step:963/2000 train_time:40350ms step_avg:41.90ms
step:964/2000 train_time:40410ms step_avg:41.92ms
step:965/2000 train_time:40471ms step_avg:41.94ms
step:966/2000 train_time:40531ms step_avg:41.96ms
step:967/2000 train_time:40591ms step_avg:41.98ms
step:968/2000 train_time:40651ms step_avg:41.99ms
step:969/2000 train_time:40711ms step_avg:42.01ms
step:970/2000 train_time:40770ms step_avg:42.03ms
step:971/2000 train_time:40831ms step_avg:42.05ms
step:972/2000 train_time:40890ms step_avg:42.07ms
step:973/2000 train_time:40951ms step_avg:42.09ms
step:974/2000 train_time:41011ms step_avg:42.11ms
step:975/2000 train_time:41071ms step_avg:42.12ms
step:976/2000 train_time:41130ms step_avg:42.14ms
step:977/2000 train_time:41190ms step_avg:42.16ms
step:978/2000 train_time:41250ms step_avg:42.18ms
step:979/2000 train_time:41310ms step_avg:42.20ms
step:980/2000 train_time:41370ms step_avg:42.21ms
step:981/2000 train_time:41431ms step_avg:42.23ms
step:982/2000 train_time:41490ms step_avg:42.25ms
step:983/2000 train_time:41551ms step_avg:42.27ms
step:984/2000 train_time:41611ms step_avg:42.29ms
step:985/2000 train_time:41671ms step_avg:42.31ms
step:986/2000 train_time:41730ms step_avg:42.32ms
step:987/2000 train_time:41791ms step_avg:42.34ms
step:988/2000 train_time:41850ms step_avg:42.36ms
step:989/2000 train_time:41911ms step_avg:42.38ms
step:990/2000 train_time:41971ms step_avg:42.39ms
step:991/2000 train_time:42031ms step_avg:42.41ms
step:992/2000 train_time:42090ms step_avg:42.43ms
step:993/2000 train_time:42151ms step_avg:42.45ms
step:994/2000 train_time:42210ms step_avg:42.46ms
step:995/2000 train_time:42270ms step_avg:42.48ms
step:996/2000 train_time:42330ms step_avg:42.50ms
step:997/2000 train_time:42391ms step_avg:42.52ms
step:998/2000 train_time:42450ms step_avg:42.54ms
step:999/2000 train_time:42511ms step_avg:42.55ms
step:1000/2000 train_time:42571ms step_avg:42.57ms
step:1000/2000 val_loss:3.6784 train_time:42633ms step_avg:42.63ms
step:1001/2000 train_time:42653ms step_avg:42.61ms
step:1002/2000 train_time:42693ms step_avg:42.61ms
step:1003/2000 train_time:42757ms step_avg:42.63ms
step:1004/2000 train_time:42820ms step_avg:42.65ms
step:1005/2000 train_time:42880ms step_avg:42.67ms
step:1006/2000 train_time:42939ms step_avg:42.68ms
step:1007/2000 train_time:42999ms step_avg:42.70ms
step:1008/2000 train_time:43058ms step_avg:42.72ms
step:1009/2000 train_time:43119ms step_avg:42.73ms
step:1010/2000 train_time:43178ms step_avg:42.75ms
step:1011/2000 train_time:43237ms step_avg:42.77ms
step:1012/2000 train_time:43296ms step_avg:42.78ms
step:1013/2000 train_time:43356ms step_avg:42.80ms
step:1014/2000 train_time:43414ms step_avg:42.82ms
step:1015/2000 train_time:43473ms step_avg:42.83ms
step:1016/2000 train_time:43532ms step_avg:42.85ms
step:1017/2000 train_time:43594ms step_avg:42.87ms
step:1018/2000 train_time:43654ms step_avg:42.88ms
step:1019/2000 train_time:43716ms step_avg:42.90ms
step:1020/2000 train_time:43777ms step_avg:42.92ms
step:1021/2000 train_time:43838ms step_avg:42.94ms
step:1022/2000 train_time:43897ms step_avg:42.95ms
step:1023/2000 train_time:43958ms step_avg:42.97ms
step:1024/2000 train_time:44018ms step_avg:42.99ms
step:1025/2000 train_time:44077ms step_avg:43.00ms
step:1026/2000 train_time:44136ms step_avg:43.02ms
step:1027/2000 train_time:44196ms step_avg:43.03ms
step:1028/2000 train_time:44255ms step_avg:43.05ms
step:1029/2000 train_time:44315ms step_avg:43.07ms
step:1030/2000 train_time:44374ms step_avg:43.08ms
step:1031/2000 train_time:44433ms step_avg:43.10ms
step:1032/2000 train_time:44492ms step_avg:43.11ms
step:1033/2000 train_time:44552ms step_avg:43.13ms
step:1034/2000 train_time:44612ms step_avg:43.15ms
step:1035/2000 train_time:44673ms step_avg:43.16ms
step:1036/2000 train_time:44734ms step_avg:43.18ms
step:1037/2000 train_time:44795ms step_avg:43.20ms
step:1038/2000 train_time:44854ms step_avg:43.21ms
step:1039/2000 train_time:44915ms step_avg:43.23ms
step:1040/2000 train_time:44975ms step_avg:43.24ms
step:1041/2000 train_time:45035ms step_avg:43.26ms
step:1042/2000 train_time:45094ms step_avg:43.28ms
step:1043/2000 train_time:45154ms step_avg:43.29ms
step:1044/2000 train_time:45214ms step_avg:43.31ms
step:1045/2000 train_time:45274ms step_avg:43.32ms
step:1046/2000 train_time:45333ms step_avg:43.34ms
step:1047/2000 train_time:45392ms step_avg:43.35ms
step:1048/2000 train_time:45451ms step_avg:43.37ms
step:1049/2000 train_time:45511ms step_avg:43.39ms
step:1050/2000 train_time:45572ms step_avg:43.40ms
step:1051/2000 train_time:45633ms step_avg:43.42ms
step:1052/2000 train_time:45693ms step_avg:43.43ms
step:1053/2000 train_time:45754ms step_avg:43.45ms
step:1054/2000 train_time:45814ms step_avg:43.47ms
step:1055/2000 train_time:45875ms step_avg:43.48ms
step:1056/2000 train_time:45936ms step_avg:43.50ms
step:1057/2000 train_time:45995ms step_avg:43.51ms
step:1058/2000 train_time:46054ms step_avg:43.53ms
step:1059/2000 train_time:46114ms step_avg:43.55ms
step:1060/2000 train_time:46173ms step_avg:43.56ms
step:1061/2000 train_time:46234ms step_avg:43.58ms
step:1062/2000 train_time:46293ms step_avg:43.59ms
step:1063/2000 train_time:46353ms step_avg:43.61ms
step:1064/2000 train_time:46412ms step_avg:43.62ms
step:1065/2000 train_time:46473ms step_avg:43.64ms
step:1066/2000 train_time:46532ms step_avg:43.65ms
step:1067/2000 train_time:46593ms step_avg:43.67ms
step:1068/2000 train_time:46652ms step_avg:43.68ms
step:1069/2000 train_time:46714ms step_avg:43.70ms
step:1070/2000 train_time:46774ms step_avg:43.71ms
step:1071/2000 train_time:46835ms step_avg:43.73ms
step:1072/2000 train_time:46894ms step_avg:43.74ms
step:1073/2000 train_time:46955ms step_avg:43.76ms
step:1074/2000 train_time:47015ms step_avg:43.78ms
step:1075/2000 train_time:47075ms step_avg:43.79ms
step:1076/2000 train_time:47134ms step_avg:43.81ms
step:1077/2000 train_time:47195ms step_avg:43.82ms
step:1078/2000 train_time:47253ms step_avg:43.83ms
step:1079/2000 train_time:47314ms step_avg:43.85ms
step:1080/2000 train_time:47373ms step_avg:43.86ms
step:1081/2000 train_time:47432ms step_avg:43.88ms
step:1082/2000 train_time:47492ms step_avg:43.89ms
step:1083/2000 train_time:47552ms step_avg:43.91ms
step:1084/2000 train_time:47612ms step_avg:43.92ms
step:1085/2000 train_time:47673ms step_avg:43.94ms
step:1086/2000 train_time:47733ms step_avg:43.95ms
step:1087/2000 train_time:47794ms step_avg:43.97ms
step:1088/2000 train_time:47853ms step_avg:43.98ms
step:1089/2000 train_time:47914ms step_avg:44.00ms
step:1090/2000 train_time:47974ms step_avg:44.01ms
step:1091/2000 train_time:48034ms step_avg:44.03ms
step:1092/2000 train_time:48094ms step_avg:44.04ms
step:1093/2000 train_time:48154ms step_avg:44.06ms
step:1094/2000 train_time:48213ms step_avg:44.07ms
step:1095/2000 train_time:48274ms step_avg:44.09ms
step:1096/2000 train_time:48333ms step_avg:44.10ms
step:1097/2000 train_time:48393ms step_avg:44.11ms
step:1098/2000 train_time:48452ms step_avg:44.13ms
step:1099/2000 train_time:48513ms step_avg:44.14ms
step:1100/2000 train_time:48573ms step_avg:44.16ms
step:1101/2000 train_time:48634ms step_avg:44.17ms
step:1102/2000 train_time:48693ms step_avg:44.19ms
step:1103/2000 train_time:48754ms step_avg:44.20ms
step:1104/2000 train_time:48814ms step_avg:44.22ms
step:1105/2000 train_time:48875ms step_avg:44.23ms
step:1106/2000 train_time:48934ms step_avg:44.24ms
step:1107/2000 train_time:48995ms step_avg:44.26ms
step:1108/2000 train_time:49055ms step_avg:44.27ms
step:1109/2000 train_time:49115ms step_avg:44.29ms
step:1110/2000 train_time:49174ms step_avg:44.30ms
step:1111/2000 train_time:49234ms step_avg:44.32ms
step:1112/2000 train_time:49294ms step_avg:44.33ms
step:1113/2000 train_time:49354ms step_avg:44.34ms
step:1114/2000 train_time:49413ms step_avg:44.36ms
step:1115/2000 train_time:49474ms step_avg:44.37ms
step:1116/2000 train_time:49533ms step_avg:44.38ms
step:1117/2000 train_time:49593ms step_avg:44.40ms
step:1118/2000 train_time:49653ms step_avg:44.41ms
step:1119/2000 train_time:49714ms step_avg:44.43ms
step:1120/2000 train_time:49773ms step_avg:44.44ms
step:1121/2000 train_time:49834ms step_avg:44.45ms
step:1122/2000 train_time:49893ms step_avg:44.47ms
step:1123/2000 train_time:49954ms step_avg:44.48ms
step:1124/2000 train_time:50014ms step_avg:44.50ms
step:1125/2000 train_time:50074ms step_avg:44.51ms
step:1126/2000 train_time:50134ms step_avg:44.52ms
step:1127/2000 train_time:50194ms step_avg:44.54ms
step:1128/2000 train_time:50253ms step_avg:44.55ms
step:1129/2000 train_time:50313ms step_avg:44.56ms
step:1130/2000 train_time:50372ms step_avg:44.58ms
step:1131/2000 train_time:50433ms step_avg:44.59ms
step:1132/2000 train_time:50492ms step_avg:44.60ms
step:1133/2000 train_time:50552ms step_avg:44.62ms
step:1134/2000 train_time:50613ms step_avg:44.63ms
step:1135/2000 train_time:50673ms step_avg:44.65ms
step:1136/2000 train_time:50733ms step_avg:44.66ms
step:1137/2000 train_time:50793ms step_avg:44.67ms
step:1138/2000 train_time:50853ms step_avg:44.69ms
step:1139/2000 train_time:50914ms step_avg:44.70ms
step:1140/2000 train_time:50974ms step_avg:44.71ms
step:1141/2000 train_time:51034ms step_avg:44.73ms
step:1142/2000 train_time:51094ms step_avg:44.74ms
step:1143/2000 train_time:51154ms step_avg:44.75ms
step:1144/2000 train_time:51213ms step_avg:44.77ms
step:1145/2000 train_time:51274ms step_avg:44.78ms
step:1146/2000 train_time:51333ms step_avg:44.79ms
step:1147/2000 train_time:51393ms step_avg:44.81ms
step:1148/2000 train_time:51453ms step_avg:44.82ms
step:1149/2000 train_time:51513ms step_avg:44.83ms
step:1150/2000 train_time:51573ms step_avg:44.85ms
step:1151/2000 train_time:51634ms step_avg:44.86ms
step:1152/2000 train_time:51693ms step_avg:44.87ms
step:1153/2000 train_time:51753ms step_avg:44.89ms
step:1154/2000 train_time:51813ms step_avg:44.90ms
step:1155/2000 train_time:51873ms step_avg:44.91ms
step:1156/2000 train_time:51933ms step_avg:44.92ms
step:1157/2000 train_time:51994ms step_avg:44.94ms
step:1158/2000 train_time:52053ms step_avg:44.95ms
step:1159/2000 train_time:52114ms step_avg:44.96ms
step:1160/2000 train_time:52173ms step_avg:44.98ms
step:1161/2000 train_time:52234ms step_avg:44.99ms
step:1162/2000 train_time:52293ms step_avg:45.00ms
step:1163/2000 train_time:52353ms step_avg:45.02ms
step:1164/2000 train_time:52413ms step_avg:45.03ms
step:1165/2000 train_time:52474ms step_avg:45.04ms
step:1166/2000 train_time:52533ms step_avg:45.05ms
step:1167/2000 train_time:52593ms step_avg:45.07ms
step:1168/2000 train_time:52653ms step_avg:45.08ms
step:1169/2000 train_time:52713ms step_avg:45.09ms
step:1170/2000 train_time:52773ms step_avg:45.10ms
step:1171/2000 train_time:52833ms step_avg:45.12ms
step:1172/2000 train_time:52893ms step_avg:45.13ms
step:1173/2000 train_time:52953ms step_avg:45.14ms
step:1174/2000 train_time:53013ms step_avg:45.16ms
step:1175/2000 train_time:53073ms step_avg:45.17ms
step:1176/2000 train_time:53132ms step_avg:45.18ms
step:1177/2000 train_time:53193ms step_avg:45.19ms
step:1178/2000 train_time:53253ms step_avg:45.21ms
step:1179/2000 train_time:53314ms step_avg:45.22ms
step:1180/2000 train_time:53373ms step_avg:45.23ms
step:1181/2000 train_time:53434ms step_avg:45.24ms
step:1182/2000 train_time:53493ms step_avg:45.26ms
step:1183/2000 train_time:53554ms step_avg:45.27ms
step:1184/2000 train_time:53614ms step_avg:45.28ms
step:1185/2000 train_time:53674ms step_avg:45.29ms
step:1186/2000 train_time:53734ms step_avg:45.31ms
step:1187/2000 train_time:53794ms step_avg:45.32ms
step:1188/2000 train_time:53853ms step_avg:45.33ms
step:1189/2000 train_time:53914ms step_avg:45.34ms
step:1190/2000 train_time:53973ms step_avg:45.36ms
step:1191/2000 train_time:54034ms step_avg:45.37ms
step:1192/2000 train_time:54093ms step_avg:45.38ms
step:1193/2000 train_time:54154ms step_avg:45.39ms
step:1194/2000 train_time:54214ms step_avg:45.41ms
step:1195/2000 train_time:54274ms step_avg:45.42ms
step:1196/2000 train_time:54333ms step_avg:45.43ms
step:1197/2000 train_time:54394ms step_avg:45.44ms
step:1198/2000 train_time:54453ms step_avg:45.45ms
step:1199/2000 train_time:54514ms step_avg:45.47ms
step:1200/2000 train_time:54574ms step_avg:45.48ms
step:1201/2000 train_time:54634ms step_avg:45.49ms
step:1202/2000 train_time:54694ms step_avg:45.50ms
step:1203/2000 train_time:54755ms step_avg:45.52ms
step:1204/2000 train_time:54814ms step_avg:45.53ms
step:1205/2000 train_time:54874ms step_avg:45.54ms
step:1206/2000 train_time:54934ms step_avg:45.55ms
step:1207/2000 train_time:54994ms step_avg:45.56ms
step:1208/2000 train_time:55053ms step_avg:45.57ms
step:1209/2000 train_time:55114ms step_avg:45.59ms
step:1210/2000 train_time:55173ms step_avg:45.60ms
step:1211/2000 train_time:55234ms step_avg:45.61ms
step:1212/2000 train_time:55293ms step_avg:45.62ms
step:1213/2000 train_time:55353ms step_avg:45.63ms
step:1214/2000 train_time:55413ms step_avg:45.64ms
step:1215/2000 train_time:55473ms step_avg:45.66ms
step:1216/2000 train_time:55533ms step_avg:45.67ms
step:1217/2000 train_time:55594ms step_avg:45.68ms
step:1218/2000 train_time:55653ms step_avg:45.69ms
step:1219/2000 train_time:55714ms step_avg:45.70ms
step:1220/2000 train_time:55773ms step_avg:45.72ms
step:1221/2000 train_time:55834ms step_avg:45.73ms
step:1222/2000 train_time:55893ms step_avg:45.74ms
step:1223/2000 train_time:55953ms step_avg:45.75ms
step:1224/2000 train_time:56013ms step_avg:45.76ms
step:1225/2000 train_time:56074ms step_avg:45.77ms
step:1226/2000 train_time:56133ms step_avg:45.79ms
step:1227/2000 train_time:56193ms step_avg:45.80ms
step:1228/2000 train_time:56252ms step_avg:45.81ms
step:1229/2000 train_time:56313ms step_avg:45.82ms
step:1230/2000 train_time:56372ms step_avg:45.83ms
step:1231/2000 train_time:56433ms step_avg:45.84ms
step:1232/2000 train_time:56492ms step_avg:45.85ms
step:1233/2000 train_time:56553ms step_avg:45.87ms
step:1234/2000 train_time:56613ms step_avg:45.88ms
step:1235/2000 train_time:56674ms step_avg:45.89ms
step:1236/2000 train_time:56733ms step_avg:45.90ms
step:1237/2000 train_time:56794ms step_avg:45.91ms
step:1238/2000 train_time:56853ms step_avg:45.92ms
step:1239/2000 train_time:56914ms step_avg:45.94ms
step:1240/2000 train_time:56973ms step_avg:45.95ms
step:1241/2000 train_time:57034ms step_avg:45.96ms
step:1242/2000 train_time:57093ms step_avg:45.97ms
step:1243/2000 train_time:57154ms step_avg:45.98ms
step:1244/2000 train_time:57213ms step_avg:45.99ms
step:1245/2000 train_time:57273ms step_avg:46.00ms
step:1246/2000 train_time:57333ms step_avg:46.01ms
step:1247/2000 train_time:57393ms step_avg:46.03ms
step:1248/2000 train_time:57453ms step_avg:46.04ms
step:1249/2000 train_time:57514ms step_avg:46.05ms
step:1250/2000 train_time:57573ms step_avg:46.06ms
step:1250/2000 val_loss:3.5606 train_time:57637ms step_avg:46.11ms
step:1251/2000 train_time:57655ms step_avg:46.09ms
step:1252/2000 train_time:57695ms step_avg:46.08ms
step:1253/2000 train_time:57758ms step_avg:46.10ms
step:1254/2000 train_time:57822ms step_avg:46.11ms
step:1255/2000 train_time:57883ms step_avg:46.12ms
step:1256/2000 train_time:57943ms step_avg:46.13ms
step:1257/2000 train_time:58003ms step_avg:46.14ms
step:1258/2000 train_time:58062ms step_avg:46.15ms
step:1259/2000 train_time:58122ms step_avg:46.17ms
step:1260/2000 train_time:58181ms step_avg:46.18ms
step:1261/2000 train_time:58241ms step_avg:46.19ms
step:1262/2000 train_time:58299ms step_avg:46.20ms
step:1263/2000 train_time:58359ms step_avg:46.21ms
step:1264/2000 train_time:58417ms step_avg:46.22ms
step:1265/2000 train_time:58477ms step_avg:46.23ms
step:1266/2000 train_time:58536ms step_avg:46.24ms
step:1267/2000 train_time:58596ms step_avg:46.25ms
step:1268/2000 train_time:58656ms step_avg:46.26ms
step:1269/2000 train_time:58717ms step_avg:46.27ms
step:1270/2000 train_time:58779ms step_avg:46.28ms
step:1271/2000 train_time:58840ms step_avg:46.29ms
step:1272/2000 train_time:58900ms step_avg:46.31ms
step:1273/2000 train_time:58960ms step_avg:46.32ms
step:1274/2000 train_time:59020ms step_avg:46.33ms
step:1275/2000 train_time:59080ms step_avg:46.34ms
step:1276/2000 train_time:59138ms step_avg:46.35ms
step:1277/2000 train_time:59198ms step_avg:46.36ms
step:1278/2000 train_time:59257ms step_avg:46.37ms
step:1279/2000 train_time:59317ms step_avg:46.38ms
step:1280/2000 train_time:59376ms step_avg:46.39ms
step:1281/2000 train_time:59436ms step_avg:46.40ms
step:1282/2000 train_time:59495ms step_avg:46.41ms
step:1283/2000 train_time:59555ms step_avg:46.42ms
step:1284/2000 train_time:59616ms step_avg:46.43ms
step:1285/2000 train_time:59676ms step_avg:46.44ms
step:1286/2000 train_time:59737ms step_avg:46.45ms
step:1287/2000 train_time:59798ms step_avg:46.46ms
step:1288/2000 train_time:59858ms step_avg:46.47ms
step:1289/2000 train_time:59919ms step_avg:46.48ms
step:1290/2000 train_time:59978ms step_avg:46.49ms
step:1291/2000 train_time:60039ms step_avg:46.51ms
step:1292/2000 train_time:60098ms step_avg:46.52ms
step:1293/2000 train_time:60159ms step_avg:46.53ms
step:1294/2000 train_time:60218ms step_avg:46.54ms
step:1295/2000 train_time:60278ms step_avg:46.55ms
step:1296/2000 train_time:60336ms step_avg:46.56ms
step:1297/2000 train_time:60397ms step_avg:46.57ms
step:1298/2000 train_time:60455ms step_avg:46.58ms
step:1299/2000 train_time:60516ms step_avg:46.59ms
step:1300/2000 train_time:60575ms step_avg:46.60ms
step:1301/2000 train_time:60636ms step_avg:46.61ms
step:1302/2000 train_time:60696ms step_avg:46.62ms
step:1303/2000 train_time:60757ms step_avg:46.63ms
step:1304/2000 train_time:60817ms step_avg:46.64ms
step:1305/2000 train_time:60878ms step_avg:46.65ms
step:1306/2000 train_time:60938ms step_avg:46.66ms
step:1307/2000 train_time:60998ms step_avg:46.67ms
step:1308/2000 train_time:61059ms step_avg:46.68ms
step:1309/2000 train_time:61148ms step_avg:46.71ms
step:1310/2000 train_time:61236ms step_avg:46.74ms
step:1311/2000 train_time:61324ms step_avg:46.78ms
step:1312/2000 train_time:61411ms step_avg:46.81ms
step:1313/2000 train_time:61499ms step_avg:46.84ms
step:1314/2000 train_time:61587ms step_avg:46.87ms
step:1315/2000 train_time:61676ms step_avg:46.90ms
step:1316/2000 train_time:61763ms step_avg:46.93ms
step:1317/2000 train_time:61852ms step_avg:46.96ms
step:1318/2000 train_time:61941ms step_avg:47.00ms
step:1319/2000 train_time:62030ms step_avg:47.03ms
step:1320/2000 train_time:62118ms step_avg:47.06ms
step:1321/2000 train_time:62206ms step_avg:47.09ms
step:1322/2000 train_time:62293ms step_avg:47.12ms
step:1323/2000 train_time:62380ms step_avg:47.15ms
step:1324/2000 train_time:62468ms step_avg:47.18ms
step:1325/2000 train_time:62555ms step_avg:47.21ms
step:1326/2000 train_time:62643ms step_avg:47.24ms
step:1327/2000 train_time:62731ms step_avg:47.27ms
step:1328/2000 train_time:62819ms step_avg:47.30ms
step:1329/2000 train_time:62909ms step_avg:47.34ms
step:1330/2000 train_time:62997ms step_avg:47.37ms
step:1331/2000 train_time:63085ms step_avg:47.40ms
step:1332/2000 train_time:63172ms step_avg:47.43ms
step:1333/2000 train_time:63260ms step_avg:47.46ms
step:1334/2000 train_time:63347ms step_avg:47.49ms
step:1335/2000 train_time:63434ms step_avg:47.52ms
step:1336/2000 train_time:63523ms step_avg:47.55ms
step:1337/2000 train_time:63610ms step_avg:47.58ms
step:1338/2000 train_time:63697ms step_avg:47.61ms
step:1339/2000 train_time:63788ms step_avg:47.64ms
step:1340/2000 train_time:63876ms step_avg:47.67ms
step:1341/2000 train_time:63965ms step_avg:47.70ms
step:1342/2000 train_time:64053ms step_avg:47.73ms
step:1343/2000 train_time:64141ms step_avg:47.76ms
step:1344/2000 train_time:64229ms step_avg:47.79ms
step:1345/2000 train_time:64316ms step_avg:47.82ms
step:1346/2000 train_time:64403ms step_avg:47.85ms
step:1347/2000 train_time:64491ms step_avg:47.88ms
step:1348/2000 train_time:64579ms step_avg:47.91ms
step:1349/2000 train_time:64669ms step_avg:47.94ms
step:1350/2000 train_time:64756ms step_avg:47.97ms
step:1351/2000 train_time:64845ms step_avg:48.00ms
step:1352/2000 train_time:64933ms step_avg:48.03ms
step:1353/2000 train_time:65022ms step_avg:48.06ms
step:1354/2000 train_time:65110ms step_avg:48.09ms
step:1355/2000 train_time:65198ms step_avg:48.12ms
step:1356/2000 train_time:65286ms step_avg:48.15ms
step:1357/2000 train_time:65373ms step_avg:48.17ms
step:1358/2000 train_time:65462ms step_avg:48.20ms
step:1359/2000 train_time:65549ms step_avg:48.23ms
step:1360/2000 train_time:65637ms step_avg:48.26ms
step:1361/2000 train_time:65725ms step_avg:48.29ms
step:1362/2000 train_time:65812ms step_avg:48.32ms
step:1363/2000 train_time:65901ms step_avg:48.35ms
step:1364/2000 train_time:65989ms step_avg:48.38ms
step:1365/2000 train_time:66077ms step_avg:48.41ms
step:1366/2000 train_time:66164ms step_avg:48.44ms
step:1367/2000 train_time:66252ms step_avg:48.47ms
step:1368/2000 train_time:66339ms step_avg:48.49ms
step:1369/2000 train_time:66428ms step_avg:48.52ms
step:1370/2000 train_time:66515ms step_avg:48.55ms
step:1371/2000 train_time:66603ms step_avg:48.58ms
step:1372/2000 train_time:66691ms step_avg:48.61ms
step:1373/2000 train_time:66780ms step_avg:48.64ms
step:1374/2000 train_time:66868ms step_avg:48.67ms
step:1375/2000 train_time:66957ms step_avg:48.70ms
step:1376/2000 train_time:67045ms step_avg:48.72ms
step:1377/2000 train_time:67132ms step_avg:48.75ms
step:1378/2000 train_time:67220ms step_avg:48.78ms
step:1379/2000 train_time:67309ms step_avg:48.81ms
step:1380/2000 train_time:67397ms step_avg:48.84ms
step:1381/2000 train_time:67485ms step_avg:48.87ms
step:1382/2000 train_time:67572ms step_avg:48.89ms
step:1383/2000 train_time:67660ms step_avg:48.92ms
step:1384/2000 train_time:67748ms step_avg:48.95ms
step:1385/2000 train_time:67836ms step_avg:48.98ms
step:1386/2000 train_time:67925ms step_avg:49.01ms
step:1387/2000 train_time:68013ms step_avg:49.04ms
step:1388/2000 train_time:68101ms step_avg:49.06ms
step:1389/2000 train_time:68189ms step_avg:49.09ms
step:1390/2000 train_time:68277ms step_avg:49.12ms
step:1391/2000 train_time:68365ms step_avg:49.15ms
step:1392/2000 train_time:68452ms step_avg:49.18ms
step:1393/2000 train_time:68541ms step_avg:49.20ms
step:1394/2000 train_time:68629ms step_avg:49.23ms
step:1395/2000 train_time:68717ms step_avg:49.26ms
step:1396/2000 train_time:68805ms step_avg:49.29ms
step:1397/2000 train_time:68893ms step_avg:49.31ms
step:1398/2000 train_time:68981ms step_avg:49.34ms
step:1399/2000 train_time:69069ms step_avg:49.37ms
step:1400/2000 train_time:69157ms step_avg:49.40ms
step:1401/2000 train_time:69245ms step_avg:49.43ms
step:1402/2000 train_time:69333ms step_avg:49.45ms
step:1403/2000 train_time:69422ms step_avg:49.48ms
step:1404/2000 train_time:69509ms step_avg:49.51ms
step:1405/2000 train_time:69597ms step_avg:49.54ms
step:1406/2000 train_time:69685ms step_avg:49.56ms
step:1407/2000 train_time:69773ms step_avg:49.59ms
step:1408/2000 train_time:69862ms step_avg:49.62ms
step:1409/2000 train_time:69950ms step_avg:49.65ms
step:1410/2000 train_time:70039ms step_avg:49.67ms
step:1411/2000 train_time:70128ms step_avg:49.70ms
step:1412/2000 train_time:70218ms step_avg:49.73ms
step:1413/2000 train_time:70306ms step_avg:49.76ms
step:1414/2000 train_time:70394ms step_avg:49.78ms
step:1415/2000 train_time:70482ms step_avg:49.81ms
step:1416/2000 train_time:70569ms step_avg:49.84ms
step:1417/2000 train_time:70657ms step_avg:49.86ms
step:1418/2000 train_time:70745ms step_avg:49.89ms
step:1419/2000 train_time:70832ms step_avg:49.92ms
step:1420/2000 train_time:70921ms step_avg:49.94ms
step:1421/2000 train_time:71009ms step_avg:49.97ms
step:1422/2000 train_time:71097ms step_avg:50.00ms
step:1423/2000 train_time:71186ms step_avg:50.03ms
step:1424/2000 train_time:71274ms step_avg:50.05ms
step:1425/2000 train_time:71362ms step_avg:50.08ms
step:1426/2000 train_time:71450ms step_avg:50.11ms
step:1427/2000 train_time:71537ms step_avg:50.13ms
step:1428/2000 train_time:71626ms step_avg:50.16ms
step:1429/2000 train_time:71713ms step_avg:50.18ms
step:1430/2000 train_time:71800ms step_avg:50.21ms
step:1431/2000 train_time:71890ms step_avg:50.24ms
step:1432/2000 train_time:71978ms step_avg:50.26ms
step:1433/2000 train_time:72067ms step_avg:50.29ms
step:1434/2000 train_time:72155ms step_avg:50.32ms
step:1435/2000 train_time:72244ms step_avg:50.34ms
step:1436/2000 train_time:72331ms step_avg:50.37ms
step:1437/2000 train_time:72419ms step_avg:50.40ms
step:1438/2000 train_time:72507ms step_avg:50.42ms
step:1439/2000 train_time:72595ms step_avg:50.45ms
step:1440/2000 train_time:72685ms step_avg:50.48ms
step:1441/2000 train_time:72772ms step_avg:50.50ms
step:1442/2000 train_time:72859ms step_avg:50.53ms
step:1443/2000 train_time:72948ms step_avg:50.55ms
step:1444/2000 train_time:73036ms step_avg:50.58ms
step:1445/2000 train_time:73124ms step_avg:50.61ms
step:1446/2000 train_time:73212ms step_avg:50.63ms
step:1447/2000 train_time:73300ms step_avg:50.66ms
step:1448/2000 train_time:73388ms step_avg:50.68ms
step:1449/2000 train_time:73476ms step_avg:50.71ms
step:1450/2000 train_time:73564ms step_avg:50.73ms
step:1451/2000 train_time:73652ms step_avg:50.76ms
step:1452/2000 train_time:73740ms step_avg:50.78ms
step:1453/2000 train_time:73827ms step_avg:50.81ms
step:1454/2000 train_time:73916ms step_avg:50.84ms
step:1455/2000 train_time:74004ms step_avg:50.86ms
step:1456/2000 train_time:74092ms step_avg:50.89ms
step:1457/2000 train_time:74179ms step_avg:50.91ms
step:1458/2000 train_time:74267ms step_avg:50.94ms
step:1459/2000 train_time:74355ms step_avg:50.96ms
step:1460/2000 train_time:74443ms step_avg:50.99ms
step:1461/2000 train_time:74530ms step_avg:51.01ms
step:1462/2000 train_time:74618ms step_avg:51.04ms
step:1463/2000 train_time:74707ms step_avg:51.06ms
step:1464/2000 train_time:74793ms step_avg:51.09ms
step:1465/2000 train_time:74882ms step_avg:51.11ms
step:1466/2000 train_time:74970ms step_avg:51.14ms
step:1467/2000 train_time:75058ms step_avg:51.16ms
step:1468/2000 train_time:75146ms step_avg:51.19ms
step:1469/2000 train_time:75234ms step_avg:51.21ms
step:1470/2000 train_time:75322ms step_avg:51.24ms
step:1471/2000 train_time:75411ms step_avg:51.27ms
step:1472/2000 train_time:75500ms step_avg:51.29ms
step:1473/2000 train_time:75589ms step_avg:51.32ms
step:1474/2000 train_time:75677ms step_avg:51.34ms
step:1475/2000 train_time:75764ms step_avg:51.37ms
step:1476/2000 train_time:75852ms step_avg:51.39ms
step:1477/2000 train_time:75940ms step_avg:51.42ms
step:1478/2000 train_time:76028ms step_avg:51.44ms
step:1479/2000 train_time:76116ms step_avg:51.46ms
step:1480/2000 train_time:76203ms step_avg:51.49ms
step:1481/2000 train_time:76292ms step_avg:51.51ms
step:1482/2000 train_time:76380ms step_avg:51.54ms
step:1483/2000 train_time:76468ms step_avg:51.56ms
step:1484/2000 train_time:76556ms step_avg:51.59ms
step:1485/2000 train_time:76646ms step_avg:51.61ms
step:1486/2000 train_time:76733ms step_avg:51.64ms
step:1487/2000 train_time:76821ms step_avg:51.66ms
step:1488/2000 train_time:76908ms step_avg:51.69ms
step:1489/2000 train_time:76996ms step_avg:51.71ms
step:1490/2000 train_time:77085ms step_avg:51.73ms
step:1491/2000 train_time:77172ms step_avg:51.76ms
step:1492/2000 train_time:77260ms step_avg:51.78ms
step:1493/2000 train_time:77349ms step_avg:51.81ms
step:1494/2000 train_time:77436ms step_avg:51.83ms
step:1495/2000 train_time:77525ms step_avg:51.86ms
step:1496/2000 train_time:77613ms step_avg:51.88ms
step:1497/2000 train_time:77701ms step_avg:51.90ms
step:1498/2000 train_time:77789ms step_avg:51.93ms
step:1499/2000 train_time:77877ms step_avg:51.95ms
step:1500/2000 train_time:77966ms step_avg:51.98ms
step:1500/2000 val_loss:3.4450 train_time:78055ms step_avg:52.04ms
step:1501/2000 train_time:78074ms step_avg:52.01ms
step:1502/2000 train_time:78143ms step_avg:52.03ms
step:1503/2000 train_time:78238ms step_avg:52.05ms
step:1504/2000 train_time:78327ms step_avg:52.08ms
step:1505/2000 train_time:78414ms step_avg:52.10ms
step:1506/2000 train_time:78502ms step_avg:52.13ms
step:1507/2000 train_time:78588ms step_avg:52.15ms
step:1508/2000 train_time:78675ms step_avg:52.17ms
step:1509/2000 train_time:78762ms step_avg:52.19ms
step:1510/2000 train_time:78849ms step_avg:52.22ms
step:1511/2000 train_time:78937ms step_avg:52.24ms
step:1512/2000 train_time:79026ms step_avg:52.27ms
step:1513/2000 train_time:79115ms step_avg:52.29ms
step:1514/2000 train_time:79206ms step_avg:52.32ms
step:1515/2000 train_time:79295ms step_avg:52.34ms
step:1516/2000 train_time:79383ms step_avg:52.36ms
step:1517/2000 train_time:79470ms step_avg:52.39ms
step:1518/2000 train_time:79558ms step_avg:52.41ms
step:1519/2000 train_time:79645ms step_avg:52.43ms
step:1520/2000 train_time:79732ms step_avg:52.46ms
step:1521/2000 train_time:79819ms step_avg:52.48ms
step:1522/2000 train_time:79907ms step_avg:52.50ms
step:1523/2000 train_time:79995ms step_avg:52.52ms
step:1524/2000 train_time:80084ms step_avg:52.55ms
step:1525/2000 train_time:80174ms step_avg:52.57ms
step:1526/2000 train_time:80263ms step_avg:52.60ms
step:1527/2000 train_time:80351ms step_avg:52.62ms
step:1528/2000 train_time:80439ms step_avg:52.64ms
step:1529/2000 train_time:80527ms step_avg:52.67ms
step:1530/2000 train_time:80615ms step_avg:52.69ms
step:1531/2000 train_time:80702ms step_avg:52.71ms
step:1532/2000 train_time:80790ms step_avg:52.73ms
step:1533/2000 train_time:80878ms step_avg:52.76ms
step:1534/2000 train_time:80965ms step_avg:52.78ms
step:1535/2000 train_time:81054ms step_avg:52.80ms
step:1536/2000 train_time:81144ms step_avg:52.83ms
step:1537/2000 train_time:81232ms step_avg:52.85ms
step:1538/2000 train_time:81321ms step_avg:52.87ms
step:1539/2000 train_time:81409ms step_avg:52.90ms
step:1540/2000 train_time:81496ms step_avg:52.92ms
step:1541/2000 train_time:81584ms step_avg:52.94ms
step:1542/2000 train_time:81671ms step_avg:52.96ms
step:1543/2000 train_time:81759ms step_avg:52.99ms
step:1544/2000 train_time:81845ms step_avg:53.01ms
step:1545/2000 train_time:81934ms step_avg:53.03ms
step:1546/2000 train_time:82024ms step_avg:53.06ms
step:1547/2000 train_time:82112ms step_avg:53.08ms
step:1548/2000 train_time:82200ms step_avg:53.10ms
step:1549/2000 train_time:82288ms step_avg:53.12ms
step:1550/2000 train_time:82376ms step_avg:53.15ms
step:1551/2000 train_time:82464ms step_avg:53.17ms
step:1552/2000 train_time:82551ms step_avg:53.19ms
step:1553/2000 train_time:82639ms step_avg:53.21ms
step:1554/2000 train_time:82727ms step_avg:53.23ms
step:1555/2000 train_time:82813ms step_avg:53.26ms
step:1556/2000 train_time:82900ms step_avg:53.28ms
step:1557/2000 train_time:82989ms step_avg:53.30ms
step:1558/2000 train_time:83076ms step_avg:53.32ms
step:1559/2000 train_time:83166ms step_avg:53.35ms
step:1560/2000 train_time:83254ms step_avg:53.37ms
step:1561/2000 train_time:83344ms step_avg:53.39ms
step:1562/2000 train_time:83431ms step_avg:53.41ms
step:1563/2000 train_time:83519ms step_avg:53.44ms
step:1564/2000 train_time:83606ms step_avg:53.46ms
step:1565/2000 train_time:83694ms step_avg:53.48ms
step:1566/2000 train_time:83783ms step_avg:53.50ms
step:1567/2000 train_time:83870ms step_avg:53.52ms
step:1568/2000 train_time:83958ms step_avg:53.54ms
step:1569/2000 train_time:84047ms step_avg:53.57ms
step:1570/2000 train_time:84136ms step_avg:53.59ms
step:1571/2000 train_time:84225ms step_avg:53.61ms
step:1572/2000 train_time:84314ms step_avg:53.63ms
step:1573/2000 train_time:84402ms step_avg:53.66ms
step:1574/2000 train_time:84489ms step_avg:53.68ms
step:1575/2000 train_time:84578ms step_avg:53.70ms
step:1576/2000 train_time:84666ms step_avg:53.72ms
step:1577/2000 train_time:84753ms step_avg:53.74ms
step:1578/2000 train_time:84841ms step_avg:53.77ms
step:1579/2000 train_time:84929ms step_avg:53.79ms
step:1580/2000 train_time:85018ms step_avg:53.81ms
step:1581/2000 train_time:85106ms step_avg:53.83ms
step:1582/2000 train_time:85194ms step_avg:53.85ms
step:1583/2000 train_time:85283ms step_avg:53.87ms
step:1584/2000 train_time:85371ms step_avg:53.90ms
step:1585/2000 train_time:85460ms step_avg:53.92ms
step:1586/2000 train_time:85547ms step_avg:53.94ms
step:1587/2000 train_time:85635ms step_avg:53.96ms
step:1588/2000 train_time:85723ms step_avg:53.98ms
step:1589/2000 train_time:85810ms step_avg:54.00ms
step:1590/2000 train_time:85898ms step_avg:54.02ms
step:1591/2000 train_time:85987ms step_avg:54.05ms
step:1592/2000 train_time:86074ms step_avg:54.07ms
step:1593/2000 train_time:86164ms step_avg:54.09ms
step:1594/2000 train_time:86253ms step_avg:54.11ms
step:1595/2000 train_time:86342ms step_avg:54.13ms
step:1596/2000 train_time:86429ms step_avg:54.15ms
step:1597/2000 train_time:86517ms step_avg:54.17ms
step:1598/2000 train_time:86604ms step_avg:54.20ms
step:1599/2000 train_time:86693ms step_avg:54.22ms
step:1600/2000 train_time:86781ms step_avg:54.24ms
step:1601/2000 train_time:86869ms step_avg:54.26ms
step:1602/2000 train_time:86957ms step_avg:54.28ms
step:1603/2000 train_time:87045ms step_avg:54.30ms
step:1604/2000 train_time:87134ms step_avg:54.32ms
step:1605/2000 train_time:87222ms step_avg:54.34ms
step:1606/2000 train_time:87310ms step_avg:54.37ms
step:1607/2000 train_time:87399ms step_avg:54.39ms
step:1608/2000 train_time:87486ms step_avg:54.41ms
step:1609/2000 train_time:87575ms step_avg:54.43ms
step:1610/2000 train_time:87663ms step_avg:54.45ms
step:1611/2000 train_time:87751ms step_avg:54.47ms
step:1612/2000 train_time:87839ms step_avg:54.49ms
step:1613/2000 train_time:87926ms step_avg:54.51ms
step:1614/2000 train_time:88015ms step_avg:54.53ms
step:1615/2000 train_time:88103ms step_avg:54.55ms
step:1616/2000 train_time:88191ms step_avg:54.57ms
step:1617/2000 train_time:88279ms step_avg:54.59ms
step:1618/2000 train_time:88367ms step_avg:54.61ms
step:1619/2000 train_time:88455ms step_avg:54.64ms
step:1620/2000 train_time:88543ms step_avg:54.66ms
step:1621/2000 train_time:88631ms step_avg:54.68ms
step:1622/2000 train_time:88719ms step_avg:54.70ms
step:1623/2000 train_time:88807ms step_avg:54.72ms
step:1624/2000 train_time:88895ms step_avg:54.74ms
step:1625/2000 train_time:88984ms step_avg:54.76ms
step:1626/2000 train_time:89071ms step_avg:54.78ms
step:1627/2000 train_time:89160ms step_avg:54.80ms
step:1628/2000 train_time:89247ms step_avg:54.82ms
step:1629/2000 train_time:89336ms step_avg:54.84ms
step:1630/2000 train_time:89424ms step_avg:54.86ms
step:1631/2000 train_time:89512ms step_avg:54.88ms
step:1632/2000 train_time:89600ms step_avg:54.90ms
step:1633/2000 train_time:89688ms step_avg:54.92ms
step:1634/2000 train_time:89775ms step_avg:54.94ms
step:1635/2000 train_time:89864ms step_avg:54.96ms
step:1636/2000 train_time:89953ms step_avg:54.98ms
step:1637/2000 train_time:90041ms step_avg:55.00ms
step:1638/2000 train_time:90128ms step_avg:55.02ms
step:1639/2000 train_time:90217ms step_avg:55.04ms
step:1640/2000 train_time:90304ms step_avg:55.06ms
step:1641/2000 train_time:90392ms step_avg:55.08ms
step:1642/2000 train_time:90480ms step_avg:55.10ms
step:1643/2000 train_time:90568ms step_avg:55.12ms
step:1644/2000 train_time:90656ms step_avg:55.14ms
step:1645/2000 train_time:90745ms step_avg:55.16ms
step:1646/2000 train_time:90834ms step_avg:55.18ms
step:1647/2000 train_time:90923ms step_avg:55.21ms
step:1648/2000 train_time:91011ms step_avg:55.23ms
step:1649/2000 train_time:91100ms step_avg:55.25ms
step:1650/2000 train_time:91187ms step_avg:55.26ms
step:1651/2000 train_time:91275ms step_avg:55.28ms
step:1652/2000 train_time:91363ms step_avg:55.30ms
step:1653/2000 train_time:91451ms step_avg:55.32ms
step:1654/2000 train_time:91538ms step_avg:55.34ms
step:1655/2000 train_time:91627ms step_avg:55.36ms
step:1656/2000 train_time:91715ms step_avg:55.38ms
step:1657/2000 train_time:91804ms step_avg:55.40ms
step:1658/2000 train_time:91891ms step_avg:55.42ms
step:1659/2000 train_time:91979ms step_avg:55.44ms
step:1660/2000 train_time:92067ms step_avg:55.46ms
step:1661/2000 train_time:92155ms step_avg:55.48ms
step:1662/2000 train_time:92244ms step_avg:55.50ms
step:1663/2000 train_time:92332ms step_avg:55.52ms
step:1664/2000 train_time:92421ms step_avg:55.54ms
step:1665/2000 train_time:92508ms step_avg:55.56ms
step:1666/2000 train_time:92596ms step_avg:55.58ms
step:1667/2000 train_time:92685ms step_avg:55.60ms
step:1668/2000 train_time:92772ms step_avg:55.62ms
step:1669/2000 train_time:92862ms step_avg:55.64ms
step:1670/2000 train_time:92949ms step_avg:55.66ms
step:1671/2000 train_time:93037ms step_avg:55.68ms
step:1672/2000 train_time:93125ms step_avg:55.70ms
step:1673/2000 train_time:93213ms step_avg:55.72ms
step:1674/2000 train_time:93302ms step_avg:55.74ms
step:1675/2000 train_time:93389ms step_avg:55.75ms
step:1676/2000 train_time:93477ms step_avg:55.77ms
step:1677/2000 train_time:93564ms step_avg:55.79ms
step:1678/2000 train_time:93652ms step_avg:55.81ms
step:1679/2000 train_time:93740ms step_avg:55.83ms
step:1680/2000 train_time:93828ms step_avg:55.85ms
step:1681/2000 train_time:93916ms step_avg:55.87ms
step:1682/2000 train_time:94004ms step_avg:55.89ms
step:1683/2000 train_time:94093ms step_avg:55.91ms
step:1684/2000 train_time:94181ms step_avg:55.93ms
step:1685/2000 train_time:94268ms step_avg:55.95ms
step:1686/2000 train_time:94356ms step_avg:55.96ms
step:1687/2000 train_time:94444ms step_avg:55.98ms
step:1688/2000 train_time:94532ms step_avg:56.00ms
step:1689/2000 train_time:94620ms step_avg:56.02ms
step:1690/2000 train_time:94707ms step_avg:56.04ms
step:1691/2000 train_time:94796ms step_avg:56.06ms
step:1692/2000 train_time:94884ms step_avg:56.08ms
step:1693/2000 train_time:94972ms step_avg:56.10ms
step:1694/2000 train_time:95062ms step_avg:56.12ms
step:1695/2000 train_time:95149ms step_avg:56.13ms
step:1696/2000 train_time:95237ms step_avg:56.15ms
step:1697/2000 train_time:95325ms step_avg:56.17ms
step:1698/2000 train_time:95413ms step_avg:56.19ms
step:1699/2000 train_time:95502ms step_avg:56.21ms
step:1700/2000 train_time:95589ms step_avg:56.23ms
step:1701/2000 train_time:95678ms step_avg:56.25ms
step:1702/2000 train_time:95766ms step_avg:56.27ms
step:1703/2000 train_time:95854ms step_avg:56.29ms
step:1704/2000 train_time:95944ms step_avg:56.30ms
step:1705/2000 train_time:96032ms step_avg:56.32ms
step:1706/2000 train_time:96121ms step_avg:56.34ms
step:1707/2000 train_time:96208ms step_avg:56.36ms
step:1708/2000 train_time:96297ms step_avg:56.38ms
step:1709/2000 train_time:96385ms step_avg:56.40ms
step:1710/2000 train_time:96473ms step_avg:56.42ms
step:1711/2000 train_time:96563ms step_avg:56.44ms
step:1712/2000 train_time:96650ms step_avg:56.45ms
step:1713/2000 train_time:96738ms step_avg:56.47ms
step:1714/2000 train_time:96826ms step_avg:56.49ms
step:1715/2000 train_time:96915ms step_avg:56.51ms
step:1716/2000 train_time:97004ms step_avg:56.53ms
step:1717/2000 train_time:97092ms step_avg:56.55ms
step:1718/2000 train_time:97180ms step_avg:56.57ms
step:1719/2000 train_time:97267ms step_avg:56.58ms
step:1720/2000 train_time:97356ms step_avg:56.60ms
step:1721/2000 train_time:97445ms step_avg:56.62ms
step:1722/2000 train_time:97533ms step_avg:56.64ms
step:1723/2000 train_time:97622ms step_avg:56.66ms
step:1724/2000 train_time:97709ms step_avg:56.68ms
step:1725/2000 train_time:97798ms step_avg:56.69ms
step:1726/2000 train_time:97885ms step_avg:56.71ms
step:1727/2000 train_time:97974ms step_avg:56.73ms
step:1728/2000 train_time:98062ms step_avg:56.75ms
step:1729/2000 train_time:98150ms step_avg:56.77ms
step:1730/2000 train_time:98238ms step_avg:56.79ms
step:1731/2000 train_time:98326ms step_avg:56.80ms
step:1732/2000 train_time:98414ms step_avg:56.82ms
step:1733/2000 train_time:98503ms step_avg:56.84ms
step:1734/2000 train_time:98591ms step_avg:56.86ms
step:1735/2000 train_time:98679ms step_avg:56.88ms
step:1736/2000 train_time:98766ms step_avg:56.89ms
step:1737/2000 train_time:98855ms step_avg:56.91ms
step:1738/2000 train_time:98942ms step_avg:56.93ms
step:1739/2000 train_time:99030ms step_avg:56.95ms
step:1740/2000 train_time:99118ms step_avg:56.96ms
step:1741/2000 train_time:99206ms step_avg:56.98ms
step:1742/2000 train_time:99294ms step_avg:57.00ms
step:1743/2000 train_time:99381ms step_avg:57.02ms
step:1744/2000 train_time:99469ms step_avg:57.04ms
step:1745/2000 train_time:99558ms step_avg:57.05ms
step:1746/2000 train_time:99646ms step_avg:57.07ms
step:1747/2000 train_time:99734ms step_avg:57.09ms
step:1748/2000 train_time:99822ms step_avg:57.11ms
step:1749/2000 train_time:99910ms step_avg:57.12ms
step:1750/2000 train_time:99997ms step_avg:57.14ms
step:1750/2000 val_loss:3.3488 train_time:100089ms step_avg:57.19ms
step:1751/2000 train_time:100108ms step_avg:57.17ms
step:1752/2000 train_time:100178ms step_avg:57.18ms
step:1753/2000 train_time:100272ms step_avg:57.20ms
step:1754/2000 train_time:100360ms step_avg:57.22ms
step:1755/2000 train_time:100447ms step_avg:57.23ms
step:1756/2000 train_time:100534ms step_avg:57.25ms
step:1757/2000 train_time:100621ms step_avg:57.27ms
step:1758/2000 train_time:100708ms step_avg:57.29ms
step:1759/2000 train_time:100796ms step_avg:57.30ms
step:1760/2000 train_time:100883ms step_avg:57.32ms
step:1761/2000 train_time:100970ms step_avg:57.34ms
step:1762/2000 train_time:101060ms step_avg:57.36ms
step:1763/2000 train_time:101150ms step_avg:57.37ms
step:1764/2000 train_time:101239ms step_avg:57.39ms
step:1765/2000 train_time:101328ms step_avg:57.41ms
step:1766/2000 train_time:101415ms step_avg:57.43ms
step:1767/2000 train_time:101503ms step_avg:57.44ms
step:1768/2000 train_time:101590ms step_avg:57.46ms
step:1769/2000 train_time:101677ms step_avg:57.48ms
step:1770/2000 train_time:101764ms step_avg:57.49ms
step:1771/2000 train_time:101852ms step_avg:57.51ms
step:1772/2000 train_time:101939ms step_avg:57.53ms
step:1773/2000 train_time:102027ms step_avg:57.54ms
step:1774/2000 train_time:102116ms step_avg:57.56ms
step:1775/2000 train_time:102206ms step_avg:57.58ms
step:1776/2000 train_time:102296ms step_avg:57.60ms
step:1777/2000 train_time:102385ms step_avg:57.62ms
step:1778/2000 train_time:102474ms step_avg:57.63ms
step:1779/2000 train_time:102561ms step_avg:57.65ms
step:1780/2000 train_time:102649ms step_avg:57.67ms
step:1781/2000 train_time:102736ms step_avg:57.68ms
step:1782/2000 train_time:102824ms step_avg:57.70ms
step:1783/2000 train_time:102911ms step_avg:57.72ms
step:1784/2000 train_time:102998ms step_avg:57.73ms
step:1785/2000 train_time:103087ms step_avg:57.75ms
step:1786/2000 train_time:103175ms step_avg:57.77ms
step:1787/2000 train_time:103264ms step_avg:57.79ms
step:1788/2000 train_time:103354ms step_avg:57.80ms
step:1789/2000 train_time:103442ms step_avg:57.82ms
step:1790/2000 train_time:103529ms step_avg:57.84ms
step:1791/2000 train_time:103617ms step_avg:57.85ms
step:1792/2000 train_time:103704ms step_avg:57.87ms
step:1793/2000 train_time:103792ms step_avg:57.89ms
step:1794/2000 train_time:103879ms step_avg:57.90ms
step:1795/2000 train_time:103968ms step_avg:57.92ms
step:1796/2000 train_time:104056ms step_avg:57.94ms
step:1797/2000 train_time:104145ms step_avg:57.95ms
step:1798/2000 train_time:104233ms step_avg:57.97ms
step:1799/2000 train_time:104321ms step_avg:57.99ms
step:1800/2000 train_time:104409ms step_avg:58.01ms
step:1801/2000 train_time:104498ms step_avg:58.02ms
step:1802/2000 train_time:104586ms step_avg:58.04ms
step:1803/2000 train_time:104674ms step_avg:58.06ms
step:1804/2000 train_time:104762ms step_avg:58.07ms
step:1805/2000 train_time:104850ms step_avg:58.09ms
step:1806/2000 train_time:104937ms step_avg:58.10ms
step:1807/2000 train_time:105026ms step_avg:58.12ms
step:1808/2000 train_time:105114ms step_avg:58.14ms
step:1809/2000 train_time:105202ms step_avg:58.15ms
step:1810/2000 train_time:105290ms step_avg:58.17ms
step:1811/2000 train_time:105378ms step_avg:58.19ms
step:1812/2000 train_time:105466ms step_avg:58.20ms
step:1813/2000 train_time:105555ms step_avg:58.22ms
step:1814/2000 train_time:105642ms step_avg:58.24ms
step:1815/2000 train_time:105729ms step_avg:58.25ms
step:1816/2000 train_time:105817ms step_avg:58.27ms
step:1817/2000 train_time:105904ms step_avg:58.29ms
step:1818/2000 train_time:105992ms step_avg:58.30ms
step:1819/2000 train_time:106080ms step_avg:58.32ms
step:1820/2000 train_time:106169ms step_avg:58.33ms
step:1821/2000 train_time:106258ms step_avg:58.35ms
step:1822/2000 train_time:106347ms step_avg:58.37ms
step:1823/2000 train_time:106436ms step_avg:58.38ms
step:1824/2000 train_time:106524ms step_avg:58.40ms
step:1825/2000 train_time:106612ms step_avg:58.42ms
step:1826/2000 train_time:106699ms step_avg:58.43ms
step:1827/2000 train_time:106787ms step_avg:58.45ms
step:1828/2000 train_time:106875ms step_avg:58.47ms
step:1829/2000 train_time:106962ms step_avg:58.48ms
step:1830/2000 train_time:107051ms step_avg:58.50ms
step:1831/2000 train_time:107139ms step_avg:58.51ms
step:1832/2000 train_time:107227ms step_avg:58.53ms
step:1833/2000 train_time:107315ms step_avg:58.55ms
step:1834/2000 train_time:107404ms step_avg:58.56ms
step:1835/2000 train_time:107493ms step_avg:58.58ms
step:1836/2000 train_time:107579ms step_avg:58.59ms
step:1837/2000 train_time:107668ms step_avg:58.61ms
step:1838/2000 train_time:107756ms step_avg:58.63ms
step:1839/2000 train_time:107844ms step_avg:58.64ms
step:1840/2000 train_time:107932ms step_avg:58.66ms
step:1841/2000 train_time:108020ms step_avg:58.67ms
step:1842/2000 train_time:108107ms step_avg:58.69ms
step:1843/2000 train_time:108195ms step_avg:58.71ms
step:1844/2000 train_time:108283ms step_avg:58.72ms
step:1845/2000 train_time:108372ms step_avg:58.74ms
step:1846/2000 train_time:108459ms step_avg:58.75ms
step:1847/2000 train_time:108547ms step_avg:58.77ms
step:1848/2000 train_time:108635ms step_avg:58.79ms
step:1849/2000 train_time:108723ms step_avg:58.80ms
step:1850/2000 train_time:108811ms step_avg:58.82ms
step:1851/2000 train_time:108898ms step_avg:58.83ms
step:1852/2000 train_time:108986ms step_avg:58.85ms
step:1853/2000 train_time:109075ms step_avg:58.86ms
step:1854/2000 train_time:109164ms step_avg:58.88ms
step:1855/2000 train_time:109251ms step_avg:58.90ms
step:1856/2000 train_time:109339ms step_avg:58.91ms
step:1857/2000 train_time:109428ms step_avg:58.93ms
step:1858/2000 train_time:109516ms step_avg:58.94ms
step:1859/2000 train_time:109604ms step_avg:58.96ms
step:1860/2000 train_time:109692ms step_avg:58.97ms
step:1861/2000 train_time:109779ms step_avg:58.99ms
step:1862/2000 train_time:109868ms step_avg:59.01ms
step:1863/2000 train_time:109956ms step_avg:59.02ms
step:1864/2000 train_time:110044ms step_avg:59.04ms
step:1865/2000 train_time:110132ms step_avg:59.05ms
step:1866/2000 train_time:110220ms step_avg:59.07ms
step:1867/2000 train_time:110309ms step_avg:59.08ms
step:1868/2000 train_time:110397ms step_avg:59.10ms
step:1869/2000 train_time:110485ms step_avg:59.11ms
step:1870/2000 train_time:110572ms step_avg:59.13ms
step:1871/2000 train_time:110660ms step_avg:59.15ms
step:1872/2000 train_time:110748ms step_avg:59.16ms
step:1873/2000 train_time:110836ms step_avg:59.18ms
step:1874/2000 train_time:110923ms step_avg:59.19ms
step:1875/2000 train_time:111012ms step_avg:59.21ms
step:1876/2000 train_time:111099ms step_avg:59.22ms
step:1877/2000 train_time:111187ms step_avg:59.24ms
step:1878/2000 train_time:111275ms step_avg:59.25ms
step:1879/2000 train_time:111362ms step_avg:59.27ms
step:1880/2000 train_time:111451ms step_avg:59.28ms
step:1881/2000 train_time:111539ms step_avg:59.30ms
step:1882/2000 train_time:111626ms step_avg:59.31ms
step:1883/2000 train_time:111715ms step_avg:59.33ms
step:1884/2000 train_time:111803ms step_avg:59.34ms
step:1885/2000 train_time:111890ms step_avg:59.36ms
step:1886/2000 train_time:111978ms step_avg:59.37ms
step:1887/2000 train_time:112066ms step_avg:59.39ms
step:1888/2000 train_time:112155ms step_avg:59.40ms
step:1889/2000 train_time:112243ms step_avg:59.42ms
step:1890/2000 train_time:112331ms step_avg:59.43ms
step:1891/2000 train_time:112419ms step_avg:59.45ms
step:1892/2000 train_time:112507ms step_avg:59.46ms
step:1893/2000 train_time:112595ms step_avg:59.48ms
step:1894/2000 train_time:112683ms step_avg:59.49ms
step:1895/2000 train_time:112773ms step_avg:59.51ms
step:1896/2000 train_time:112861ms step_avg:59.53ms
step:1897/2000 train_time:112949ms step_avg:59.54ms
step:1898/2000 train_time:113037ms step_avg:59.56ms
step:1899/2000 train_time:113125ms step_avg:59.57ms
step:1900/2000 train_time:113212ms step_avg:59.59ms
step:1901/2000 train_time:113300ms step_avg:59.60ms
step:1902/2000 train_time:113389ms step_avg:59.62ms
step:1903/2000 train_time:113477ms step_avg:59.63ms
step:1904/2000 train_time:113565ms step_avg:59.65ms
step:1905/2000 train_time:113654ms step_avg:59.66ms
step:1906/2000 train_time:113742ms step_avg:59.68ms
step:1907/2000 train_time:113831ms step_avg:59.69ms
step:1908/2000 train_time:113918ms step_avg:59.71ms
step:1909/2000 train_time:114007ms step_avg:59.72ms
step:1910/2000 train_time:114096ms step_avg:59.74ms
step:1911/2000 train_time:114183ms step_avg:59.75ms
step:1912/2000 train_time:114271ms step_avg:59.77ms
step:1913/2000 train_time:114360ms step_avg:59.78ms
step:1914/2000 train_time:114448ms step_avg:59.80ms
step:1915/2000 train_time:114535ms step_avg:59.81ms
step:1916/2000 train_time:114624ms step_avg:59.82ms
step:1917/2000 train_time:114712ms step_avg:59.84ms
step:1918/2000 train_time:114800ms step_avg:59.85ms
step:1919/2000 train_time:114888ms step_avg:59.87ms
step:1920/2000 train_time:114975ms step_avg:59.88ms
step:1921/2000 train_time:115063ms step_avg:59.90ms
step:1922/2000 train_time:115152ms step_avg:59.91ms
step:1923/2000 train_time:115239ms step_avg:59.93ms
step:1924/2000 train_time:115327ms step_avg:59.94ms
step:1925/2000 train_time:115415ms step_avg:59.96ms
step:1926/2000 train_time:115502ms step_avg:59.97ms
step:1927/2000 train_time:115591ms step_avg:59.98ms
step:1928/2000 train_time:115678ms step_avg:60.00ms
step:1929/2000 train_time:115766ms step_avg:60.01ms
step:1930/2000 train_time:115855ms step_avg:60.03ms
step:1931/2000 train_time:115943ms step_avg:60.04ms
step:1932/2000 train_time:116030ms step_avg:60.06ms
step:1933/2000 train_time:116119ms step_avg:60.07ms
step:1934/2000 train_time:116207ms step_avg:60.09ms
step:1935/2000 train_time:116296ms step_avg:60.10ms
step:1936/2000 train_time:116384ms step_avg:60.12ms
step:1937/2000 train_time:116472ms step_avg:60.13ms
step:1938/2000 train_time:116559ms step_avg:60.14ms
step:1939/2000 train_time:116649ms step_avg:60.16ms
step:1940/2000 train_time:116736ms step_avg:60.17ms
step:1941/2000 train_time:116824ms step_avg:60.19ms
step:1942/2000 train_time:116912ms step_avg:60.20ms
step:1943/2000 train_time:117000ms step_avg:60.22ms
step:1944/2000 train_time:117087ms step_avg:60.23ms
step:1945/2000 train_time:117176ms step_avg:60.24ms
step:1946/2000 train_time:117264ms step_avg:60.26ms
step:1947/2000 train_time:117353ms step_avg:60.27ms
step:1948/2000 train_time:117440ms step_avg:60.29ms
step:1949/2000 train_time:117528ms step_avg:60.30ms
step:1950/2000 train_time:117616ms step_avg:60.32ms
step:1951/2000 train_time:117704ms step_avg:60.33ms
step:1952/2000 train_time:117793ms step_avg:60.34ms
step:1953/2000 train_time:117881ms step_avg:60.36ms
step:1954/2000 train_time:117969ms step_avg:60.37ms
step:1955/2000 train_time:118058ms step_avg:60.39ms
step:1956/2000 train_time:118147ms step_avg:60.40ms
step:1957/2000 train_time:118234ms step_avg:60.42ms
step:1958/2000 train_time:118322ms step_avg:60.43ms
step:1959/2000 train_time:118411ms step_avg:60.44ms
step:1960/2000 train_time:118498ms step_avg:60.46ms
step:1961/2000 train_time:118586ms step_avg:60.47ms
step:1962/2000 train_time:118673ms step_avg:60.49ms
step:1963/2000 train_time:118762ms step_avg:60.50ms
step:1964/2000 train_time:118851ms step_avg:60.51ms
step:1965/2000 train_time:118939ms step_avg:60.53ms
step:1966/2000 train_time:119028ms step_avg:60.54ms
step:1967/2000 train_time:119116ms step_avg:60.56ms
step:1968/2000 train_time:119204ms step_avg:60.57ms
step:1969/2000 train_time:119295ms step_avg:60.59ms
step:1970/2000 train_time:119384ms step_avg:60.60ms
step:1971/2000 train_time:119473ms step_avg:60.62ms
step:1972/2000 train_time:119561ms step_avg:60.63ms
step:1973/2000 train_time:119649ms step_avg:60.64ms
step:1974/2000 train_time:119737ms step_avg:60.66ms
step:1975/2000 train_time:119826ms step_avg:60.67ms
step:1976/2000 train_time:119914ms step_avg:60.69ms
step:1977/2000 train_time:120002ms step_avg:60.70ms
step:1978/2000 train_time:120090ms step_avg:60.71ms
step:1979/2000 train_time:120178ms step_avg:60.73ms
step:1980/2000 train_time:120267ms step_avg:60.74ms
step:1981/2000 train_time:120357ms step_avg:60.76ms
step:1982/2000 train_time:120446ms step_avg:60.77ms
step:1983/2000 train_time:120534ms step_avg:60.78ms
step:1984/2000 train_time:120622ms step_avg:60.80ms
step:1985/2000 train_time:120710ms step_avg:60.81ms
step:1986/2000 train_time:120798ms step_avg:60.82ms
step:1987/2000 train_time:120887ms step_avg:60.84ms
step:1988/2000 train_time:120975ms step_avg:60.85ms
step:1989/2000 train_time:121063ms step_avg:60.87ms
step:1990/2000 train_time:121152ms step_avg:60.88ms
step:1991/2000 train_time:121240ms step_avg:60.89ms
step:1992/2000 train_time:121329ms step_avg:60.91ms
step:1993/2000 train_time:121417ms step_avg:60.92ms
step:1994/2000 train_time:121505ms step_avg:60.94ms
step:1995/2000 train_time:121595ms step_avg:60.95ms
step:1996/2000 train_time:121684ms step_avg:60.96ms
step:1997/2000 train_time:121773ms step_avg:60.98ms
step:1998/2000 train_time:121861ms step_avg:60.99ms
step:1999/2000 train_time:121951ms step_avg:61.01ms
step:2000/2000 train_time:122039ms step_avg:61.02ms
step:2000/2000 val_loss:3.2790 train_time:122131ms step_avg:61.07ms
peak memory allocated: 29625 MiB reserved: 39256 MiB

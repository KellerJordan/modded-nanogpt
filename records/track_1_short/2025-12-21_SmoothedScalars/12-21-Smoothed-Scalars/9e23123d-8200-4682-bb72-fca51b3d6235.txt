import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            grad_slice = torch.empty_like(grad[:rank_size])
            self._reduce_scatter_futures[param] = (
                dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad_slice
            )



    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

    @torch.no_grad()
    def reset_momentum(self, params=None):
        """Reset momentum buffers for specified parameters (or all if 'None')"""
        if params is None:
            # Reset all parameters
            params_to_reset = [p for group in self.param_groups for p in group['params']]
        else:
            params_to_reset = list(params)
        
        for param in params_to_reset:
            if param in self.state:
                state = self.state[param]
                if 'exp_avg' in state:
                    state['exp_avg'].zero_()
                if 'exp_avg_sq' in state:
                    state['exp_avg_sq'].zero_()
                state['step'] = 0

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0
        
        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas  
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    # evaluation and logging
    logs_dir: str = f"logs/12-21-Smooth-Scalars-stps.1960.40"
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    os.makedirs(args.logs_dir, exist_ok=True)
    logfile = f"{args.logs_dir}/{args.run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes,lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    
    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps
    
    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0
    
    if in_transition:
        adam_optimizers[1].transition_steps -= 1
            
    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)               
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) 
        is_transition = True
            
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 21 18:14:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    787728      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    0   N/A  N/A    787729      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    787730      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    787731      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    787732      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    787733      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    787734      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    0   N/A  N/A    787735      C   ...omamba/envs/speedrun/bin/python3.12        614MiB |
|    1   N/A  N/A    787729      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    787730      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    787731      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    787732      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    787733      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    787734      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    787735      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2000 val_loss:10.8335 train_time:0ms step_avg:0.04ms
step:1/2000 train_time:80ms step_avg:79.93ms
step:2/2000 train_time:104ms step_avg:52.18ms
step:3/2000 train_time:124ms step_avg:41.37ms
step:4/2000 train_time:152ms step_avg:37.89ms
step:5/2000 train_time:185ms step_avg:36.92ms
step:6/2000 train_time:273ms step_avg:45.42ms
step:7/2000 train_time:290ms step_avg:41.40ms
step:8/2000 train_time:322ms step_avg:40.20ms
step:9/2000 train_time:354ms step_avg:39.37ms
step:10/2000 train_time:388ms step_avg:38.77ms
step:11/2000 train_time:421ms step_avg:38.27ms
step:12/2000 train_time:454ms step_avg:37.86ms
step:13/2000 train_time:487ms step_avg:37.50ms
step:14/2000 train_time:521ms step_avg:37.21ms
step:15/2000 train_time:554ms step_avg:36.93ms
step:16/2000 train_time:587ms step_avg:36.71ms
step:17/2000 train_time:620ms step_avg:36.50ms
step:18/2000 train_time:654ms step_avg:36.32ms
step:19/2000 train_time:687ms step_avg:36.15ms
step:20/2000 train_time:720ms step_avg:36.00ms
step:21/2000 train_time:753ms step_avg:35.87ms
step:22/2000 train_time:787ms step_avg:35.76ms
step:23/2000 train_time:820ms step_avg:35.64ms
step:24/2000 train_time:853ms step_avg:35.55ms
step:25/2000 train_time:887ms step_avg:35.46ms
step:26/2000 train_time:920ms step_avg:35.38ms
step:27/2000 train_time:953ms step_avg:35.30ms
step:28/2000 train_time:987ms step_avg:35.23ms
step:29/2000 train_time:1020ms step_avg:35.16ms
step:30/2000 train_time:1053ms step_avg:35.10ms
step:31/2000 train_time:1086ms step_avg:35.03ms
step:32/2000 train_time:1119ms step_avg:34.97ms
step:33/2000 train_time:1153ms step_avg:34.94ms
step:34/2000 train_time:1187ms step_avg:34.92ms
step:35/2000 train_time:1222ms step_avg:34.91ms
step:36/2000 train_time:1256ms step_avg:34.89ms
step:37/2000 train_time:1290ms step_avg:34.87ms
step:38/2000 train_time:1324ms step_avg:34.85ms
step:39/2000 train_time:1358ms step_avg:34.81ms
step:40/2000 train_time:1392ms step_avg:34.80ms
step:41/2000 train_time:1425ms step_avg:34.77ms
step:42/2000 train_time:1459ms step_avg:34.74ms
step:43/2000 train_time:1492ms step_avg:34.70ms
step:44/2000 train_time:1526ms step_avg:34.68ms
step:45/2000 train_time:1559ms step_avg:34.65ms
step:46/2000 train_time:1593ms step_avg:34.62ms
step:47/2000 train_time:1626ms step_avg:34.60ms
step:48/2000 train_time:1660ms step_avg:34.58ms
step:49/2000 train_time:1693ms step_avg:34.54ms
step:50/2000 train_time:1726ms step_avg:34.53ms
step:51/2000 train_time:1759ms step_avg:34.50ms
step:52/2000 train_time:1793ms step_avg:34.48ms
step:53/2000 train_time:1826ms step_avg:34.45ms
step:54/2000 train_time:1859ms step_avg:34.43ms
step:55/2000 train_time:1892ms step_avg:34.40ms
step:56/2000 train_time:1926ms step_avg:34.38ms
step:57/2000 train_time:1959ms step_avg:34.37ms
step:58/2000 train_time:1992ms step_avg:34.35ms
step:59/2000 train_time:2025ms step_avg:34.33ms
step:60/2000 train_time:2059ms step_avg:34.31ms
step:61/2000 train_time:2091ms step_avg:34.29ms
step:62/2000 train_time:2125ms step_avg:34.27ms
step:63/2000 train_time:2158ms step_avg:34.25ms
step:64/2000 train_time:2192ms step_avg:34.24ms
step:65/2000 train_time:2225ms step_avg:34.23ms
step:66/2000 train_time:2258ms step_avg:34.22ms
step:67/2000 train_time:2292ms step_avg:34.20ms
step:68/2000 train_time:2326ms step_avg:34.20ms
step:69/2000 train_time:2359ms step_avg:34.18ms
step:70/2000 train_time:2392ms step_avg:34.18ms
step:71/2000 train_time:2426ms step_avg:34.17ms
step:72/2000 train_time:2459ms step_avg:34.15ms
step:73/2000 train_time:2492ms step_avg:34.14ms
step:74/2000 train_time:2526ms step_avg:34.14ms
step:75/2000 train_time:2559ms step_avg:34.12ms
step:76/2000 train_time:2593ms step_avg:34.12ms
step:77/2000 train_time:2626ms step_avg:34.11ms
step:78/2000 train_time:2660ms step_avg:34.10ms
step:79/2000 train_time:2693ms step_avg:34.08ms
step:80/2000 train_time:2727ms step_avg:34.08ms
step:81/2000 train_time:2760ms step_avg:34.07ms
step:82/2000 train_time:2793ms step_avg:34.06ms
step:83/2000 train_time:2826ms step_avg:34.04ms
step:84/2000 train_time:2859ms step_avg:34.04ms
step:85/2000 train_time:2892ms step_avg:34.02ms
step:86/2000 train_time:2925ms step_avg:34.01ms
step:87/2000 train_time:2958ms step_avg:34.00ms
step:88/2000 train_time:2991ms step_avg:33.99ms
step:89/2000 train_time:3024ms step_avg:33.98ms
step:90/2000 train_time:3058ms step_avg:33.97ms
step:91/2000 train_time:3091ms step_avg:33.96ms
step:92/2000 train_time:3124ms step_avg:33.96ms
step:93/2000 train_time:3157ms step_avg:33.94ms
step:94/2000 train_time:3190ms step_avg:33.94ms
step:95/2000 train_time:3224ms step_avg:33.93ms
step:96/2000 train_time:3257ms step_avg:33.93ms
step:97/2000 train_time:3291ms step_avg:33.93ms
step:98/2000 train_time:3325ms step_avg:33.93ms
step:99/2000 train_time:3358ms step_avg:33.92ms
step:100/2000 train_time:3391ms step_avg:33.91ms
step:101/2000 train_time:3424ms step_avg:33.90ms
step:102/2000 train_time:3458ms step_avg:33.90ms
step:103/2000 train_time:3491ms step_avg:33.89ms
step:104/2000 train_time:3524ms step_avg:33.89ms
step:105/2000 train_time:3557ms step_avg:33.88ms
step:106/2000 train_time:3590ms step_avg:33.87ms
step:107/2000 train_time:3624ms step_avg:33.86ms
step:108/2000 train_time:3657ms step_avg:33.86ms
step:109/2000 train_time:3690ms step_avg:33.85ms
step:110/2000 train_time:3723ms step_avg:33.85ms
step:111/2000 train_time:3756ms step_avg:33.84ms
step:112/2000 train_time:3789ms step_avg:33.83ms
step:113/2000 train_time:3822ms step_avg:33.83ms
step:114/2000 train_time:3856ms step_avg:33.82ms
step:115/2000 train_time:3889ms step_avg:33.82ms
step:116/2000 train_time:3922ms step_avg:33.81ms
step:117/2000 train_time:3955ms step_avg:33.80ms
step:118/2000 train_time:3988ms step_avg:33.80ms
step:119/2000 train_time:4022ms step_avg:33.79ms
step:120/2000 train_time:4055ms step_avg:33.79ms
step:121/2000 train_time:4088ms step_avg:33.78ms
step:122/2000 train_time:4121ms step_avg:33.78ms
step:123/2000 train_time:4154ms step_avg:33.78ms
step:124/2000 train_time:4188ms step_avg:33.78ms
step:125/2000 train_time:4222ms step_avg:33.77ms
step:126/2000 train_time:4255ms step_avg:33.77ms
step:127/2000 train_time:4288ms step_avg:33.77ms
step:128/2000 train_time:4322ms step_avg:33.76ms
step:129/2000 train_time:4355ms step_avg:33.76ms
step:130/2000 train_time:4388ms step_avg:33.76ms
step:131/2000 train_time:4422ms step_avg:33.76ms
step:132/2000 train_time:4456ms step_avg:33.76ms
step:133/2000 train_time:4489ms step_avg:33.75ms
step:134/2000 train_time:4523ms step_avg:33.75ms
step:135/2000 train_time:4556ms step_avg:33.75ms
step:136/2000 train_time:4589ms step_avg:33.74ms
step:137/2000 train_time:4623ms step_avg:33.74ms
step:138/2000 train_time:4656ms step_avg:33.74ms
step:139/2000 train_time:4689ms step_avg:33.73ms
step:140/2000 train_time:4723ms step_avg:33.73ms
step:141/2000 train_time:4756ms step_avg:33.73ms
step:142/2000 train_time:4789ms step_avg:33.73ms
step:143/2000 train_time:4823ms step_avg:33.72ms
step:144/2000 train_time:4856ms step_avg:33.72ms
step:145/2000 train_time:4889ms step_avg:33.72ms
step:146/2000 train_time:4922ms step_avg:33.71ms
step:147/2000 train_time:4955ms step_avg:33.71ms
step:148/2000 train_time:4988ms step_avg:33.71ms
step:149/2000 train_time:5021ms step_avg:33.70ms
step:150/2000 train_time:5055ms step_avg:33.70ms
step:151/2000 train_time:5088ms step_avg:33.69ms
step:152/2000 train_time:5121ms step_avg:33.69ms
step:153/2000 train_time:5154ms step_avg:33.69ms
step:154/2000 train_time:5188ms step_avg:33.69ms
step:155/2000 train_time:5221ms step_avg:33.68ms
step:156/2000 train_time:5254ms step_avg:33.68ms
step:157/2000 train_time:5287ms step_avg:33.68ms
step:158/2000 train_time:5320ms step_avg:33.67ms
step:159/2000 train_time:5353ms step_avg:33.67ms
step:160/2000 train_time:5387ms step_avg:33.67ms
step:161/2000 train_time:5420ms step_avg:33.66ms
step:162/2000 train_time:5453ms step_avg:33.66ms
step:163/2000 train_time:5486ms step_avg:33.66ms
step:164/2000 train_time:5520ms step_avg:33.66ms
step:165/2000 train_time:5553ms step_avg:33.65ms
step:166/2000 train_time:5586ms step_avg:33.65ms
step:167/2000 train_time:5619ms step_avg:33.65ms
step:168/2000 train_time:5652ms step_avg:33.65ms
step:169/2000 train_time:5685ms step_avg:33.64ms
step:170/2000 train_time:5719ms step_avg:33.64ms
step:171/2000 train_time:5751ms step_avg:33.63ms
step:172/2000 train_time:5785ms step_avg:33.63ms
step:173/2000 train_time:5818ms step_avg:33.63ms
step:174/2000 train_time:5852ms step_avg:33.63ms
step:175/2000 train_time:5885ms step_avg:33.63ms
step:176/2000 train_time:5918ms step_avg:33.62ms
step:177/2000 train_time:5951ms step_avg:33.62ms
step:178/2000 train_time:5984ms step_avg:33.62ms
step:179/2000 train_time:6017ms step_avg:33.61ms
step:180/2000 train_time:6050ms step_avg:33.61ms
step:181/2000 train_time:6084ms step_avg:33.61ms
step:182/2000 train_time:6118ms step_avg:33.61ms
step:183/2000 train_time:6150ms step_avg:33.61ms
step:184/2000 train_time:6184ms step_avg:33.61ms
step:185/2000 train_time:6217ms step_avg:33.60ms
step:186/2000 train_time:6250ms step_avg:33.60ms
step:187/2000 train_time:6283ms step_avg:33.60ms
step:188/2000 train_time:6316ms step_avg:33.60ms
step:189/2000 train_time:6349ms step_avg:33.59ms
step:190/2000 train_time:6383ms step_avg:33.59ms
step:191/2000 train_time:6415ms step_avg:33.59ms
step:192/2000 train_time:6449ms step_avg:33.59ms
step:193/2000 train_time:6482ms step_avg:33.59ms
step:194/2000 train_time:6516ms step_avg:33.59ms
step:195/2000 train_time:6549ms step_avg:33.58ms
step:196/2000 train_time:6583ms step_avg:33.58ms
step:197/2000 train_time:6616ms step_avg:33.58ms
step:198/2000 train_time:6649ms step_avg:33.58ms
step:199/2000 train_time:6682ms step_avg:33.58ms
step:200/2000 train_time:6716ms step_avg:33.58ms
step:201/2000 train_time:6749ms step_avg:33.58ms
step:202/2000 train_time:6782ms step_avg:33.58ms
step:203/2000 train_time:6816ms step_avg:33.58ms
step:204/2000 train_time:6849ms step_avg:33.57ms
step:205/2000 train_time:6882ms step_avg:33.57ms
step:206/2000 train_time:6916ms step_avg:33.57ms
step:207/2000 train_time:6949ms step_avg:33.57ms
step:208/2000 train_time:6982ms step_avg:33.57ms
step:209/2000 train_time:7015ms step_avg:33.56ms
step:210/2000 train_time:7048ms step_avg:33.56ms
step:211/2000 train_time:7082ms step_avg:33.56ms
step:212/2000 train_time:7115ms step_avg:33.56ms
step:213/2000 train_time:7148ms step_avg:33.56ms
step:214/2000 train_time:7181ms step_avg:33.56ms
step:215/2000 train_time:7214ms step_avg:33.55ms
step:216/2000 train_time:7247ms step_avg:33.55ms
step:217/2000 train_time:7281ms step_avg:33.55ms
step:218/2000 train_time:7314ms step_avg:33.55ms
step:219/2000 train_time:7347ms step_avg:33.55ms
step:220/2000 train_time:7380ms step_avg:33.55ms
step:221/2000 train_time:7413ms step_avg:33.54ms
step:222/2000 train_time:7446ms step_avg:33.54ms
step:223/2000 train_time:7480ms step_avg:33.54ms
step:224/2000 train_time:7513ms step_avg:33.54ms
step:225/2000 train_time:7546ms step_avg:33.54ms
step:226/2000 train_time:7579ms step_avg:33.54ms
step:227/2000 train_time:7612ms step_avg:33.53ms
step:228/2000 train_time:7646ms step_avg:33.53ms
step:229/2000 train_time:7678ms step_avg:33.53ms
step:230/2000 train_time:7712ms step_avg:33.53ms
step:231/2000 train_time:7745ms step_avg:33.53ms
step:232/2000 train_time:7778ms step_avg:33.53ms
step:233/2000 train_time:7812ms step_avg:33.53ms
step:234/2000 train_time:7845ms step_avg:33.53ms
step:235/2000 train_time:7878ms step_avg:33.52ms
step:236/2000 train_time:7911ms step_avg:33.52ms
step:237/2000 train_time:7944ms step_avg:33.52ms
step:238/2000 train_time:7978ms step_avg:33.52ms
step:239/2000 train_time:8011ms step_avg:33.52ms
step:240/2000 train_time:8044ms step_avg:33.52ms
step:241/2000 train_time:8077ms step_avg:33.51ms
step:242/2000 train_time:8110ms step_avg:33.51ms
step:243/2000 train_time:8143ms step_avg:33.51ms
step:244/2000 train_time:8177ms step_avg:33.51ms
step:245/2000 train_time:8210ms step_avg:33.51ms
step:246/2000 train_time:8243ms step_avg:33.51ms
step:247/2000 train_time:8276ms step_avg:33.51ms
step:248/2000 train_time:8309ms step_avg:33.51ms
step:249/2000 train_time:8342ms step_avg:33.50ms
step:250/2000 train_time:8376ms step_avg:33.50ms
step:250/2000 val_loss:4.2637 train_time:8412ms step_avg:33.65ms
step:251/2000 train_time:8431ms step_avg:33.59ms
step:252/2000 train_time:8449ms step_avg:33.53ms
step:253/2000 train_time:8479ms step_avg:33.51ms
step:254/2000 train_time:8512ms step_avg:33.51ms
step:255/2000 train_time:8547ms step_avg:33.52ms
step:256/2000 train_time:8580ms step_avg:33.52ms
step:257/2000 train_time:8613ms step_avg:33.52ms
step:258/2000 train_time:8647ms step_avg:33.51ms
step:259/2000 train_time:8679ms step_avg:33.51ms
step:260/2000 train_time:8712ms step_avg:33.51ms
step:261/2000 train_time:8745ms step_avg:33.51ms
step:262/2000 train_time:8778ms step_avg:33.51ms
step:263/2000 train_time:8811ms step_avg:33.50ms
step:264/2000 train_time:8845ms step_avg:33.50ms
step:265/2000 train_time:8877ms step_avg:33.50ms
step:266/2000 train_time:8910ms step_avg:33.50ms
step:267/2000 train_time:8943ms step_avg:33.50ms
step:268/2000 train_time:8977ms step_avg:33.49ms
step:269/2000 train_time:9009ms step_avg:33.49ms
step:270/2000 train_time:9042ms step_avg:33.49ms
step:271/2000 train_time:9075ms step_avg:33.49ms
step:272/2000 train_time:9108ms step_avg:33.49ms
step:273/2000 train_time:9141ms step_avg:33.48ms
step:274/2000 train_time:9174ms step_avg:33.48ms
step:275/2000 train_time:9207ms step_avg:33.48ms
step:276/2000 train_time:9240ms step_avg:33.48ms
step:277/2000 train_time:9273ms step_avg:33.48ms
step:278/2000 train_time:9307ms step_avg:33.48ms
step:279/2000 train_time:9339ms step_avg:33.47ms
step:280/2000 train_time:9373ms step_avg:33.48ms
step:281/2000 train_time:9407ms step_avg:33.48ms
step:282/2000 train_time:9440ms step_avg:33.48ms
step:283/2000 train_time:9474ms step_avg:33.48ms
step:284/2000 train_time:9508ms step_avg:33.48ms
step:285/2000 train_time:9541ms step_avg:33.48ms
step:286/2000 train_time:9575ms step_avg:33.48ms
step:287/2000 train_time:9608ms step_avg:33.48ms
step:288/2000 train_time:9641ms step_avg:33.48ms
step:289/2000 train_time:9674ms step_avg:33.47ms
step:290/2000 train_time:9708ms step_avg:33.47ms
step:291/2000 train_time:9741ms step_avg:33.47ms
step:292/2000 train_time:9774ms step_avg:33.47ms
step:293/2000 train_time:9807ms step_avg:33.47ms
step:294/2000 train_time:9840ms step_avg:33.47ms
step:295/2000 train_time:9874ms step_avg:33.47ms
step:296/2000 train_time:9907ms step_avg:33.47ms
step:297/2000 train_time:9940ms step_avg:33.47ms
step:298/2000 train_time:9973ms step_avg:33.47ms
step:299/2000 train_time:10006ms step_avg:33.46ms
step:300/2000 train_time:10039ms step_avg:33.46ms
step:301/2000 train_time:10072ms step_avg:33.46ms
step:302/2000 train_time:10105ms step_avg:33.46ms
step:303/2000 train_time:10138ms step_avg:33.46ms
step:304/2000 train_time:10172ms step_avg:33.46ms
step:305/2000 train_time:10204ms step_avg:33.46ms
step:306/2000 train_time:10238ms step_avg:33.46ms
step:307/2000 train_time:10271ms step_avg:33.45ms
step:308/2000 train_time:10304ms step_avg:33.45ms
step:309/2000 train_time:10337ms step_avg:33.45ms
step:310/2000 train_time:10370ms step_avg:33.45ms
step:311/2000 train_time:10404ms step_avg:33.45ms
step:312/2000 train_time:10437ms step_avg:33.45ms
step:313/2000 train_time:10470ms step_avg:33.45ms
step:314/2000 train_time:10504ms step_avg:33.45ms
step:315/2000 train_time:10537ms step_avg:33.45ms
step:316/2000 train_time:10570ms step_avg:33.45ms
step:317/2000 train_time:10604ms step_avg:33.45ms
step:318/2000 train_time:10637ms step_avg:33.45ms
step:319/2000 train_time:10670ms step_avg:33.45ms
step:320/2000 train_time:10704ms step_avg:33.45ms
step:321/2000 train_time:10737ms step_avg:33.45ms
step:322/2000 train_time:10770ms step_avg:33.45ms
step:323/2000 train_time:10803ms step_avg:33.45ms
step:324/2000 train_time:10837ms step_avg:33.45ms
step:325/2000 train_time:10870ms step_avg:33.45ms
step:326/2000 train_time:10903ms step_avg:33.45ms
step:327/2000 train_time:10936ms step_avg:33.44ms
step:328/2000 train_time:10969ms step_avg:33.44ms
step:329/2000 train_time:11002ms step_avg:33.44ms
step:330/2000 train_time:11036ms step_avg:33.44ms
step:331/2000 train_time:11069ms step_avg:33.44ms
step:332/2000 train_time:11103ms step_avg:33.44ms
step:333/2000 train_time:11135ms step_avg:33.44ms
step:334/2000 train_time:11169ms step_avg:33.44ms
step:335/2000 train_time:11202ms step_avg:33.44ms
step:336/2000 train_time:11235ms step_avg:33.44ms
step:337/2000 train_time:11268ms step_avg:33.44ms
step:338/2000 train_time:11301ms step_avg:33.44ms
step:339/2000 train_time:11334ms step_avg:33.43ms
step:340/2000 train_time:11368ms step_avg:33.43ms
step:341/2000 train_time:11401ms step_avg:33.43ms
step:342/2000 train_time:11434ms step_avg:33.43ms
step:343/2000 train_time:11467ms step_avg:33.43ms
step:344/2000 train_time:11501ms step_avg:33.43ms
step:345/2000 train_time:11534ms step_avg:33.43ms
step:346/2000 train_time:11568ms step_avg:33.43ms
step:347/2000 train_time:11601ms step_avg:33.43ms
step:348/2000 train_time:11635ms step_avg:33.43ms
step:349/2000 train_time:11667ms step_avg:33.43ms
step:350/2000 train_time:11701ms step_avg:33.43ms
step:351/2000 train_time:11734ms step_avg:33.43ms
step:352/2000 train_time:11767ms step_avg:33.43ms
step:353/2000 train_time:11800ms step_avg:33.43ms
step:354/2000 train_time:11834ms step_avg:33.43ms
step:355/2000 train_time:11867ms step_avg:33.43ms
step:356/2000 train_time:11900ms step_avg:33.43ms
step:357/2000 train_time:11933ms step_avg:33.43ms
step:358/2000 train_time:11966ms step_avg:33.42ms
step:359/2000 train_time:11999ms step_avg:33.42ms
step:360/2000 train_time:12033ms step_avg:33.42ms
step:361/2000 train_time:12066ms step_avg:33.42ms
step:362/2000 train_time:12099ms step_avg:33.42ms
step:363/2000 train_time:12132ms step_avg:33.42ms
step:364/2000 train_time:12165ms step_avg:33.42ms
step:365/2000 train_time:12198ms step_avg:33.42ms
step:366/2000 train_time:12231ms step_avg:33.42ms
step:367/2000 train_time:12264ms step_avg:33.42ms
step:368/2000 train_time:12297ms step_avg:33.42ms
step:369/2000 train_time:12331ms step_avg:33.42ms
step:370/2000 train_time:12364ms step_avg:33.42ms
step:371/2000 train_time:12397ms step_avg:33.42ms
step:372/2000 train_time:12430ms step_avg:33.41ms
step:373/2000 train_time:12463ms step_avg:33.41ms
step:374/2000 train_time:12496ms step_avg:33.41ms
step:375/2000 train_time:12530ms step_avg:33.41ms
step:376/2000 train_time:12564ms step_avg:33.41ms
step:377/2000 train_time:12596ms step_avg:33.41ms
step:378/2000 train_time:12630ms step_avg:33.41ms
step:379/2000 train_time:12663ms step_avg:33.41ms
step:380/2000 train_time:12697ms step_avg:33.41ms
step:381/2000 train_time:12730ms step_avg:33.41ms
step:382/2000 train_time:12763ms step_avg:33.41ms
step:383/2000 train_time:12796ms step_avg:33.41ms
step:384/2000 train_time:12830ms step_avg:33.41ms
step:385/2000 train_time:12863ms step_avg:33.41ms
step:386/2000 train_time:12896ms step_avg:33.41ms
step:387/2000 train_time:12929ms step_avg:33.41ms
step:388/2000 train_time:12962ms step_avg:33.41ms
step:389/2000 train_time:12995ms step_avg:33.41ms
step:390/2000 train_time:13029ms step_avg:33.41ms
step:391/2000 train_time:13062ms step_avg:33.41ms
step:392/2000 train_time:13095ms step_avg:33.41ms
step:393/2000 train_time:13128ms step_avg:33.40ms
step:394/2000 train_time:13161ms step_avg:33.40ms
step:395/2000 train_time:13194ms step_avg:33.40ms
step:396/2000 train_time:13227ms step_avg:33.40ms
step:397/2000 train_time:13261ms step_avg:33.40ms
step:398/2000 train_time:13294ms step_avg:33.40ms
step:399/2000 train_time:13327ms step_avg:33.40ms
step:400/2000 train_time:13360ms step_avg:33.40ms
step:401/2000 train_time:13393ms step_avg:33.40ms
step:402/2000 train_time:13427ms step_avg:33.40ms
step:403/2000 train_time:13460ms step_avg:33.40ms
step:404/2000 train_time:13493ms step_avg:33.40ms
step:405/2000 train_time:13526ms step_avg:33.40ms
step:406/2000 train_time:13559ms step_avg:33.40ms
step:407/2000 train_time:13592ms step_avg:33.40ms
step:408/2000 train_time:13626ms step_avg:33.40ms
step:409/2000 train_time:13659ms step_avg:33.40ms
step:410/2000 train_time:13692ms step_avg:33.39ms
step:411/2000 train_time:13725ms step_avg:33.39ms
step:412/2000 train_time:13758ms step_avg:33.39ms
step:413/2000 train_time:13791ms step_avg:33.39ms
step:414/2000 train_time:13825ms step_avg:33.39ms
step:415/2000 train_time:13857ms step_avg:33.39ms
step:416/2000 train_time:13891ms step_avg:33.39ms
step:417/2000 train_time:13924ms step_avg:33.39ms
step:418/2000 train_time:13957ms step_avg:33.39ms
step:419/2000 train_time:13990ms step_avg:33.39ms
step:420/2000 train_time:14024ms step_avg:33.39ms
step:421/2000 train_time:14057ms step_avg:33.39ms
step:422/2000 train_time:14090ms step_avg:33.39ms
step:423/2000 train_time:14123ms step_avg:33.39ms
step:424/2000 train_time:14156ms step_avg:33.39ms
step:425/2000 train_time:14190ms step_avg:33.39ms
step:426/2000 train_time:14223ms step_avg:33.39ms
step:427/2000 train_time:14256ms step_avg:33.39ms
step:428/2000 train_time:14290ms step_avg:33.39ms
step:429/2000 train_time:14323ms step_avg:33.39ms
step:430/2000 train_time:14356ms step_avg:33.39ms
step:431/2000 train_time:14389ms step_avg:33.38ms
step:432/2000 train_time:14422ms step_avg:33.38ms
step:433/2000 train_time:14455ms step_avg:33.38ms
step:434/2000 train_time:14488ms step_avg:33.38ms
step:435/2000 train_time:14521ms step_avg:33.38ms
step:436/2000 train_time:14555ms step_avg:33.38ms
step:437/2000 train_time:14588ms step_avg:33.38ms
step:438/2000 train_time:14621ms step_avg:33.38ms
step:439/2000 train_time:14654ms step_avg:33.38ms
step:440/2000 train_time:14688ms step_avg:33.38ms
step:441/2000 train_time:14721ms step_avg:33.38ms
step:442/2000 train_time:14754ms step_avg:33.38ms
step:443/2000 train_time:14787ms step_avg:33.38ms
step:444/2000 train_time:14821ms step_avg:33.38ms
step:445/2000 train_time:14854ms step_avg:33.38ms
step:446/2000 train_time:14887ms step_avg:33.38ms
step:447/2000 train_time:14920ms step_avg:33.38ms
step:448/2000 train_time:14953ms step_avg:33.38ms
step:449/2000 train_time:14986ms step_avg:33.38ms
step:450/2000 train_time:15019ms step_avg:33.38ms
step:451/2000 train_time:15052ms step_avg:33.38ms
step:452/2000 train_time:15086ms step_avg:33.38ms
step:453/2000 train_time:15118ms step_avg:33.37ms
step:454/2000 train_time:15152ms step_avg:33.37ms
step:455/2000 train_time:15185ms step_avg:33.37ms
step:456/2000 train_time:15218ms step_avg:33.37ms
step:457/2000 train_time:15251ms step_avg:33.37ms
step:458/2000 train_time:15285ms step_avg:33.37ms
step:459/2000 train_time:15318ms step_avg:33.37ms
step:460/2000 train_time:15351ms step_avg:33.37ms
step:461/2000 train_time:15384ms step_avg:33.37ms
step:462/2000 train_time:15417ms step_avg:33.37ms
step:463/2000 train_time:15450ms step_avg:33.37ms
step:464/2000 train_time:15484ms step_avg:33.37ms
step:465/2000 train_time:15517ms step_avg:33.37ms
step:466/2000 train_time:15550ms step_avg:33.37ms
step:467/2000 train_time:15583ms step_avg:33.37ms
step:468/2000 train_time:15617ms step_avg:33.37ms
step:469/2000 train_time:15649ms step_avg:33.37ms
step:470/2000 train_time:15683ms step_avg:33.37ms
step:471/2000 train_time:15716ms step_avg:33.37ms
step:472/2000 train_time:15750ms step_avg:33.37ms
step:473/2000 train_time:15783ms step_avg:33.37ms
step:474/2000 train_time:15816ms step_avg:33.37ms
step:475/2000 train_time:15849ms step_avg:33.37ms
step:476/2000 train_time:15882ms step_avg:33.37ms
step:477/2000 train_time:15915ms step_avg:33.37ms
step:478/2000 train_time:15949ms step_avg:33.37ms
step:479/2000 train_time:15982ms step_avg:33.36ms
step:480/2000 train_time:16015ms step_avg:33.36ms
step:481/2000 train_time:16048ms step_avg:33.36ms
step:482/2000 train_time:16081ms step_avg:33.36ms
step:483/2000 train_time:16114ms step_avg:33.36ms
step:484/2000 train_time:16147ms step_avg:33.36ms
step:485/2000 train_time:16180ms step_avg:33.36ms
step:486/2000 train_time:16214ms step_avg:33.36ms
step:487/2000 train_time:16247ms step_avg:33.36ms
step:488/2000 train_time:16281ms step_avg:33.36ms
step:489/2000 train_time:16314ms step_avg:33.36ms
step:490/2000 train_time:16347ms step_avg:33.36ms
step:491/2000 train_time:16380ms step_avg:33.36ms
step:492/2000 train_time:16413ms step_avg:33.36ms
step:493/2000 train_time:16446ms step_avg:33.36ms
step:494/2000 train_time:16480ms step_avg:33.36ms
step:495/2000 train_time:16513ms step_avg:33.36ms
step:496/2000 train_time:16546ms step_avg:33.36ms
step:497/2000 train_time:16579ms step_avg:33.36ms
step:498/2000 train_time:16612ms step_avg:33.36ms
step:499/2000 train_time:16646ms step_avg:33.36ms
step:500/2000 train_time:16679ms step_avg:33.36ms
step:500/2000 val_loss:3.9961 train_time:16715ms step_avg:33.43ms
step:501/2000 train_time:16735ms step_avg:33.40ms
step:502/2000 train_time:16753ms step_avg:33.37ms
step:503/2000 train_time:16782ms step_avg:33.36ms
step:504/2000 train_time:16817ms step_avg:33.37ms
step:505/2000 train_time:16851ms step_avg:33.37ms
step:506/2000 train_time:16885ms step_avg:33.37ms
step:507/2000 train_time:16919ms step_avg:33.37ms
step:508/2000 train_time:16953ms step_avg:33.37ms
step:509/2000 train_time:16986ms step_avg:33.37ms
step:510/2000 train_time:17019ms step_avg:33.37ms
step:511/2000 train_time:17053ms step_avg:33.37ms
step:512/2000 train_time:17086ms step_avg:33.37ms
step:513/2000 train_time:17119ms step_avg:33.37ms
step:514/2000 train_time:17152ms step_avg:33.37ms
step:515/2000 train_time:17185ms step_avg:33.37ms
step:516/2000 train_time:17218ms step_avg:33.37ms
step:517/2000 train_time:17251ms step_avg:33.37ms
step:518/2000 train_time:17284ms step_avg:33.37ms
step:519/2000 train_time:17317ms step_avg:33.37ms
step:520/2000 train_time:17350ms step_avg:33.37ms
step:521/2000 train_time:17383ms step_avg:33.36ms
step:522/2000 train_time:17416ms step_avg:33.36ms
step:523/2000 train_time:17449ms step_avg:33.36ms
step:524/2000 train_time:17482ms step_avg:33.36ms
step:525/2000 train_time:17515ms step_avg:33.36ms
step:526/2000 train_time:17548ms step_avg:33.36ms
step:527/2000 train_time:17581ms step_avg:33.36ms
step:528/2000 train_time:17614ms step_avg:33.36ms
step:529/2000 train_time:17647ms step_avg:33.36ms
step:530/2000 train_time:17680ms step_avg:33.36ms
step:531/2000 train_time:17713ms step_avg:33.36ms
step:532/2000 train_time:17747ms step_avg:33.36ms
step:533/2000 train_time:17780ms step_avg:33.36ms
step:534/2000 train_time:17815ms step_avg:33.36ms
step:535/2000 train_time:17848ms step_avg:33.36ms
step:536/2000 train_time:17882ms step_avg:33.36ms
step:537/2000 train_time:17916ms step_avg:33.36ms
step:538/2000 train_time:17949ms step_avg:33.36ms
step:539/2000 train_time:17983ms step_avg:33.36ms
step:540/2000 train_time:18016ms step_avg:33.36ms
step:541/2000 train_time:18049ms step_avg:33.36ms
step:542/2000 train_time:18082ms step_avg:33.36ms
step:543/2000 train_time:18115ms step_avg:33.36ms
step:544/2000 train_time:18149ms step_avg:33.36ms
step:545/2000 train_time:18181ms step_avg:33.36ms
step:546/2000 train_time:18215ms step_avg:33.36ms
step:547/2000 train_time:18247ms step_avg:33.36ms
step:548/2000 train_time:18281ms step_avg:33.36ms
step:549/2000 train_time:18313ms step_avg:33.36ms
step:550/2000 train_time:18346ms step_avg:33.36ms
step:551/2000 train_time:18380ms step_avg:33.36ms
step:552/2000 train_time:18413ms step_avg:33.36ms
step:553/2000 train_time:18446ms step_avg:33.36ms
step:554/2000 train_time:18479ms step_avg:33.36ms
step:555/2000 train_time:18512ms step_avg:33.36ms
step:556/2000 train_time:18546ms step_avg:33.36ms
step:557/2000 train_time:18578ms step_avg:33.35ms
step:558/2000 train_time:18612ms step_avg:33.35ms
step:559/2000 train_time:18644ms step_avg:33.35ms
step:560/2000 train_time:18678ms step_avg:33.35ms
step:561/2000 train_time:18711ms step_avg:33.35ms
step:562/2000 train_time:18745ms step_avg:33.35ms
step:563/2000 train_time:18778ms step_avg:33.35ms
step:564/2000 train_time:18812ms step_avg:33.35ms
step:565/2000 train_time:18845ms step_avg:33.35ms
step:566/2000 train_time:18878ms step_avg:33.35ms
step:567/2000 train_time:18911ms step_avg:33.35ms
step:568/2000 train_time:18944ms step_avg:33.35ms
step:569/2000 train_time:18978ms step_avg:33.35ms
step:570/2000 train_time:19011ms step_avg:33.35ms
step:571/2000 train_time:19044ms step_avg:33.35ms
step:572/2000 train_time:19078ms step_avg:33.35ms
step:573/2000 train_time:19111ms step_avg:33.35ms
step:574/2000 train_time:19144ms step_avg:33.35ms
step:575/2000 train_time:19177ms step_avg:33.35ms
step:576/2000 train_time:19211ms step_avg:33.35ms
step:577/2000 train_time:19244ms step_avg:33.35ms
step:578/2000 train_time:19277ms step_avg:33.35ms
step:579/2000 train_time:19310ms step_avg:33.35ms
step:580/2000 train_time:19343ms step_avg:33.35ms
step:581/2000 train_time:19376ms step_avg:33.35ms
step:582/2000 train_time:19410ms step_avg:33.35ms
step:583/2000 train_time:19442ms step_avg:33.35ms
step:584/2000 train_time:19476ms step_avg:33.35ms
step:585/2000 train_time:19509ms step_avg:33.35ms
step:586/2000 train_time:19542ms step_avg:33.35ms
step:587/2000 train_time:19575ms step_avg:33.35ms
step:588/2000 train_time:19608ms step_avg:33.35ms
step:589/2000 train_time:19641ms step_avg:33.35ms
step:590/2000 train_time:19674ms step_avg:33.35ms
step:591/2000 train_time:19708ms step_avg:33.35ms
step:592/2000 train_time:19741ms step_avg:33.35ms
step:593/2000 train_time:19774ms step_avg:33.35ms
step:594/2000 train_time:19808ms step_avg:33.35ms
step:595/2000 train_time:19841ms step_avg:33.35ms
step:596/2000 train_time:19874ms step_avg:33.35ms
step:597/2000 train_time:19908ms step_avg:33.35ms
step:598/2000 train_time:19941ms step_avg:33.35ms
step:599/2000 train_time:19974ms step_avg:33.35ms
step:600/2000 train_time:20008ms step_avg:33.35ms
step:601/2000 train_time:20041ms step_avg:33.35ms
step:602/2000 train_time:20074ms step_avg:33.35ms
step:603/2000 train_time:20107ms step_avg:33.35ms
step:604/2000 train_time:20141ms step_avg:33.35ms
step:605/2000 train_time:20174ms step_avg:33.34ms
step:606/2000 train_time:20207ms step_avg:33.34ms
step:607/2000 train_time:20240ms step_avg:33.34ms
step:608/2000 train_time:20273ms step_avg:33.34ms
step:609/2000 train_time:20306ms step_avg:33.34ms
step:610/2000 train_time:20339ms step_avg:33.34ms
step:611/2000 train_time:20372ms step_avg:33.34ms
step:612/2000 train_time:20405ms step_avg:33.34ms
step:613/2000 train_time:20438ms step_avg:33.34ms
step:614/2000 train_time:20472ms step_avg:33.34ms
step:615/2000 train_time:20505ms step_avg:33.34ms
step:616/2000 train_time:20538ms step_avg:33.34ms
step:617/2000 train_time:20571ms step_avg:33.34ms
step:618/2000 train_time:20604ms step_avg:33.34ms
step:619/2000 train_time:20637ms step_avg:33.34ms
step:620/2000 train_time:20670ms step_avg:33.34ms
step:621/2000 train_time:20703ms step_avg:33.34ms
step:622/2000 train_time:20737ms step_avg:33.34ms
step:623/2000 train_time:20770ms step_avg:33.34ms
step:624/2000 train_time:20804ms step_avg:33.34ms
step:625/2000 train_time:20837ms step_avg:33.34ms
step:626/2000 train_time:20870ms step_avg:33.34ms
step:627/2000 train_time:20903ms step_avg:33.34ms
step:628/2000 train_time:20936ms step_avg:33.34ms
step:629/2000 train_time:20969ms step_avg:33.34ms
step:630/2000 train_time:21003ms step_avg:33.34ms
step:631/2000 train_time:21036ms step_avg:33.34ms
step:632/2000 train_time:21069ms step_avg:33.34ms
step:633/2000 train_time:21102ms step_avg:33.34ms
step:634/2000 train_time:21135ms step_avg:33.34ms
step:635/2000 train_time:21169ms step_avg:33.34ms
step:636/2000 train_time:21202ms step_avg:33.34ms
step:637/2000 train_time:21235ms step_avg:33.34ms
step:638/2000 train_time:21268ms step_avg:33.34ms
step:639/2000 train_time:21301ms step_avg:33.34ms
step:640/2000 train_time:21335ms step_avg:33.34ms
step:641/2000 train_time:21368ms step_avg:33.33ms
step:642/2000 train_time:21401ms step_avg:33.34ms
step:643/2000 train_time:21434ms step_avg:33.33ms
step:644/2000 train_time:21467ms step_avg:33.33ms
step:645/2000 train_time:21500ms step_avg:33.33ms
step:646/2000 train_time:21534ms step_avg:33.33ms
step:647/2000 train_time:21567ms step_avg:33.33ms
step:648/2000 train_time:21600ms step_avg:33.33ms
step:649/2000 train_time:21633ms step_avg:33.33ms
step:650/2000 train_time:21667ms step_avg:33.33ms
step:651/2000 train_time:21700ms step_avg:33.33ms
step:652/2000 train_time:21733ms step_avg:33.33ms
step:653/2000 train_time:21766ms step_avg:33.33ms
step:654/2000 train_time:21800ms step_avg:33.33ms
step:655/2000 train_time:21834ms step_avg:33.33ms
step:656/2000 train_time:21893ms step_avg:33.37ms
step:657/2000 train_time:21953ms step_avg:33.41ms
step:658/2000 train_time:22012ms step_avg:33.45ms
step:659/2000 train_time:22072ms step_avg:33.49ms
step:660/2000 train_time:22132ms step_avg:33.53ms
step:661/2000 train_time:22193ms step_avg:33.57ms
step:662/2000 train_time:22253ms step_avg:33.61ms
step:663/2000 train_time:22314ms step_avg:33.66ms
step:664/2000 train_time:22373ms step_avg:33.69ms
step:665/2000 train_time:22434ms step_avg:33.74ms
step:666/2000 train_time:22493ms step_avg:33.77ms
step:667/2000 train_time:22554ms step_avg:33.81ms
step:668/2000 train_time:22614ms step_avg:33.85ms
step:669/2000 train_time:22674ms step_avg:33.89ms
step:670/2000 train_time:22734ms step_avg:33.93ms
step:671/2000 train_time:22794ms step_avg:33.97ms
step:672/2000 train_time:22854ms step_avg:34.01ms
step:673/2000 train_time:22915ms step_avg:34.05ms
step:674/2000 train_time:22975ms step_avg:34.09ms
step:675/2000 train_time:23035ms step_avg:34.13ms
step:676/2000 train_time:23095ms step_avg:34.16ms
step:677/2000 train_time:23156ms step_avg:34.20ms
step:678/2000 train_time:23216ms step_avg:34.24ms
step:679/2000 train_time:23276ms step_avg:34.28ms
step:680/2000 train_time:23336ms step_avg:34.32ms
step:681/2000 train_time:23397ms step_avg:34.36ms
step:682/2000 train_time:23457ms step_avg:34.39ms
step:683/2000 train_time:23517ms step_avg:34.43ms
step:684/2000 train_time:23577ms step_avg:34.47ms
step:685/2000 train_time:23638ms step_avg:34.51ms
step:686/2000 train_time:23698ms step_avg:34.55ms
step:687/2000 train_time:23759ms step_avg:34.58ms
step:688/2000 train_time:23819ms step_avg:34.62ms
step:689/2000 train_time:23879ms step_avg:34.66ms
step:690/2000 train_time:23940ms step_avg:34.70ms
step:691/2000 train_time:24000ms step_avg:34.73ms
step:692/2000 train_time:24061ms step_avg:34.77ms
step:693/2000 train_time:24121ms step_avg:34.81ms
step:694/2000 train_time:24181ms step_avg:34.84ms
step:695/2000 train_time:24242ms step_avg:34.88ms
step:696/2000 train_time:24303ms step_avg:34.92ms
step:697/2000 train_time:24364ms step_avg:34.96ms
step:698/2000 train_time:24424ms step_avg:34.99ms
step:699/2000 train_time:24485ms step_avg:35.03ms
step:700/2000 train_time:24545ms step_avg:35.06ms
step:701/2000 train_time:24605ms step_avg:35.10ms
step:702/2000 train_time:24665ms step_avg:35.14ms
step:703/2000 train_time:24726ms step_avg:35.17ms
step:704/2000 train_time:24785ms step_avg:35.21ms
step:705/2000 train_time:24846ms step_avg:35.24ms
step:706/2000 train_time:24906ms step_avg:35.28ms
step:707/2000 train_time:24966ms step_avg:35.31ms
step:708/2000 train_time:25025ms step_avg:35.35ms
step:709/2000 train_time:25086ms step_avg:35.38ms
step:710/2000 train_time:25146ms step_avg:35.42ms
step:711/2000 train_time:25207ms step_avg:35.45ms
step:712/2000 train_time:25267ms step_avg:35.49ms
step:713/2000 train_time:25327ms step_avg:35.52ms
step:714/2000 train_time:25387ms step_avg:35.56ms
step:715/2000 train_time:25447ms step_avg:35.59ms
step:716/2000 train_time:25509ms step_avg:35.63ms
step:717/2000 train_time:25570ms step_avg:35.66ms
step:718/2000 train_time:25629ms step_avg:35.70ms
step:719/2000 train_time:25689ms step_avg:35.73ms
step:720/2000 train_time:25749ms step_avg:35.76ms
step:721/2000 train_time:25809ms step_avg:35.80ms
step:722/2000 train_time:25869ms step_avg:35.83ms
step:723/2000 train_time:25930ms step_avg:35.86ms
step:724/2000 train_time:25990ms step_avg:35.90ms
step:725/2000 train_time:26050ms step_avg:35.93ms
step:726/2000 train_time:26109ms step_avg:35.96ms
step:727/2000 train_time:26170ms step_avg:36.00ms
step:728/2000 train_time:26229ms step_avg:36.03ms
step:729/2000 train_time:26290ms step_avg:36.06ms
step:730/2000 train_time:26349ms step_avg:36.09ms
step:731/2000 train_time:26410ms step_avg:36.13ms
step:732/2000 train_time:26470ms step_avg:36.16ms
step:733/2000 train_time:26531ms step_avg:36.19ms
step:734/2000 train_time:26590ms step_avg:36.23ms
step:735/2000 train_time:26650ms step_avg:36.26ms
step:736/2000 train_time:26710ms step_avg:36.29ms
step:737/2000 train_time:26770ms step_avg:36.32ms
step:738/2000 train_time:26829ms step_avg:36.35ms
step:739/2000 train_time:26890ms step_avg:36.39ms
step:740/2000 train_time:26949ms step_avg:36.42ms
step:741/2000 train_time:27009ms step_avg:36.45ms
step:742/2000 train_time:27068ms step_avg:36.48ms
step:743/2000 train_time:27129ms step_avg:36.51ms
step:744/2000 train_time:27188ms step_avg:36.54ms
step:745/2000 train_time:27248ms step_avg:36.58ms
step:746/2000 train_time:27308ms step_avg:36.61ms
step:747/2000 train_time:27369ms step_avg:36.64ms
step:748/2000 train_time:27428ms step_avg:36.67ms
step:749/2000 train_time:27489ms step_avg:36.70ms
step:750/2000 train_time:27548ms step_avg:36.73ms
step:750/2000 val_loss:3.8216 train_time:27611ms step_avg:36.82ms
step:751/2000 train_time:27633ms step_avg:36.80ms
step:752/2000 train_time:27670ms step_avg:36.80ms
step:753/2000 train_time:27733ms step_avg:36.83ms
step:754/2000 train_time:27796ms step_avg:36.86ms
step:755/2000 train_time:27857ms step_avg:36.90ms
step:756/2000 train_time:27917ms step_avg:36.93ms
step:757/2000 train_time:27976ms step_avg:36.96ms
step:758/2000 train_time:28035ms step_avg:36.99ms
step:759/2000 train_time:28096ms step_avg:37.02ms
step:760/2000 train_time:28155ms step_avg:37.05ms
step:761/2000 train_time:28215ms step_avg:37.08ms
step:762/2000 train_time:28274ms step_avg:37.10ms
step:763/2000 train_time:28334ms step_avg:37.13ms
step:764/2000 train_time:28395ms step_avg:37.17ms
step:765/2000 train_time:28455ms step_avg:37.20ms
step:766/2000 train_time:28515ms step_avg:37.23ms
step:767/2000 train_time:28576ms step_avg:37.26ms
step:768/2000 train_time:28638ms step_avg:37.29ms
step:769/2000 train_time:28700ms step_avg:37.32ms
step:770/2000 train_time:28761ms step_avg:37.35ms
step:771/2000 train_time:28823ms step_avg:37.38ms
step:772/2000 train_time:28882ms step_avg:37.41ms
step:773/2000 train_time:28942ms step_avg:37.44ms
step:774/2000 train_time:29001ms step_avg:37.47ms
step:775/2000 train_time:29061ms step_avg:37.50ms
step:776/2000 train_time:29120ms step_avg:37.53ms
step:777/2000 train_time:29180ms step_avg:37.56ms
step:778/2000 train_time:29239ms step_avg:37.58ms
step:779/2000 train_time:29299ms step_avg:37.61ms
step:780/2000 train_time:29358ms step_avg:37.64ms
step:781/2000 train_time:29418ms step_avg:37.67ms
step:782/2000 train_time:29478ms step_avg:37.70ms
step:783/2000 train_time:29540ms step_avg:37.73ms
step:784/2000 train_time:29600ms step_avg:37.76ms
step:785/2000 train_time:29662ms step_avg:37.79ms
step:786/2000 train_time:29723ms step_avg:37.82ms
step:787/2000 train_time:29784ms step_avg:37.84ms
step:788/2000 train_time:29844ms step_avg:37.87ms
step:789/2000 train_time:29904ms step_avg:37.90ms
step:790/2000 train_time:29963ms step_avg:37.93ms
step:791/2000 train_time:30023ms step_avg:37.96ms
step:792/2000 train_time:30082ms step_avg:37.98ms
step:793/2000 train_time:30142ms step_avg:38.01ms
step:794/2000 train_time:30201ms step_avg:38.04ms
step:795/2000 train_time:30260ms step_avg:38.06ms
step:796/2000 train_time:30319ms step_avg:38.09ms
step:797/2000 train_time:30379ms step_avg:38.12ms
step:798/2000 train_time:30438ms step_avg:38.14ms
step:799/2000 train_time:30499ms step_avg:38.17ms
step:800/2000 train_time:30559ms step_avg:38.20ms
step:801/2000 train_time:30620ms step_avg:38.23ms
step:802/2000 train_time:30680ms step_avg:38.25ms
step:803/2000 train_time:30740ms step_avg:38.28ms
step:804/2000 train_time:30801ms step_avg:38.31ms
step:805/2000 train_time:30862ms step_avg:38.34ms
step:806/2000 train_time:30921ms step_avg:38.36ms
step:807/2000 train_time:30982ms step_avg:38.39ms
step:808/2000 train_time:31041ms step_avg:38.42ms
step:809/2000 train_time:31101ms step_avg:38.44ms
step:810/2000 train_time:31160ms step_avg:38.47ms
step:811/2000 train_time:31220ms step_avg:38.50ms
step:812/2000 train_time:31280ms step_avg:38.52ms
step:813/2000 train_time:31340ms step_avg:38.55ms
step:814/2000 train_time:31400ms step_avg:38.57ms
step:815/2000 train_time:31461ms step_avg:38.60ms
step:816/2000 train_time:31521ms step_avg:38.63ms
step:817/2000 train_time:31581ms step_avg:38.66ms
step:818/2000 train_time:31641ms step_avg:38.68ms
step:819/2000 train_time:31702ms step_avg:38.71ms
step:820/2000 train_time:31763ms step_avg:38.74ms
step:821/2000 train_time:31824ms step_avg:38.76ms
step:822/2000 train_time:31884ms step_avg:38.79ms
step:823/2000 train_time:31944ms step_avg:38.81ms
step:824/2000 train_time:32003ms step_avg:38.84ms
step:825/2000 train_time:32063ms step_avg:38.86ms
step:826/2000 train_time:32123ms step_avg:38.89ms
step:827/2000 train_time:32183ms step_avg:38.91ms
step:828/2000 train_time:32241ms step_avg:38.94ms
step:829/2000 train_time:32302ms step_avg:38.97ms
step:830/2000 train_time:32361ms step_avg:38.99ms
step:831/2000 train_time:32421ms step_avg:39.01ms
step:832/2000 train_time:32481ms step_avg:39.04ms
step:833/2000 train_time:32541ms step_avg:39.06ms
step:834/2000 train_time:32601ms step_avg:39.09ms
step:835/2000 train_time:32661ms step_avg:39.12ms
step:836/2000 train_time:32721ms step_avg:39.14ms
step:837/2000 train_time:32782ms step_avg:39.17ms
step:838/2000 train_time:32842ms step_avg:39.19ms
step:839/2000 train_time:32902ms step_avg:39.22ms
step:840/2000 train_time:32962ms step_avg:39.24ms
step:841/2000 train_time:33023ms step_avg:39.27ms
step:842/2000 train_time:33082ms step_avg:39.29ms
step:843/2000 train_time:33142ms step_avg:39.31ms
step:844/2000 train_time:33201ms step_avg:39.34ms
step:845/2000 train_time:33262ms step_avg:39.36ms
step:846/2000 train_time:33321ms step_avg:39.39ms
step:847/2000 train_time:33381ms step_avg:39.41ms
step:848/2000 train_time:33440ms step_avg:39.43ms
step:849/2000 train_time:33501ms step_avg:39.46ms
step:850/2000 train_time:33562ms step_avg:39.48ms
step:851/2000 train_time:33622ms step_avg:39.51ms
step:852/2000 train_time:33682ms step_avg:39.53ms
step:853/2000 train_time:33743ms step_avg:39.56ms
step:854/2000 train_time:33803ms step_avg:39.58ms
step:855/2000 train_time:33864ms step_avg:39.61ms
step:856/2000 train_time:33923ms step_avg:39.63ms
step:857/2000 train_time:33984ms step_avg:39.65ms
step:858/2000 train_time:34043ms step_avg:39.68ms
step:859/2000 train_time:34103ms step_avg:39.70ms
step:860/2000 train_time:34162ms step_avg:39.72ms
step:861/2000 train_time:34223ms step_avg:39.75ms
step:862/2000 train_time:34282ms step_avg:39.77ms
step:863/2000 train_time:34342ms step_avg:39.79ms
step:864/2000 train_time:34401ms step_avg:39.82ms
step:865/2000 train_time:34462ms step_avg:39.84ms
step:866/2000 train_time:34521ms step_avg:39.86ms
step:867/2000 train_time:34582ms step_avg:39.89ms
step:868/2000 train_time:34641ms step_avg:39.91ms
step:869/2000 train_time:34701ms step_avg:39.93ms
step:870/2000 train_time:34762ms step_avg:39.96ms
step:871/2000 train_time:34822ms step_avg:39.98ms
step:872/2000 train_time:34882ms step_avg:40.00ms
step:873/2000 train_time:34942ms step_avg:40.02ms
step:874/2000 train_time:35001ms step_avg:40.05ms
step:875/2000 train_time:35062ms step_avg:40.07ms
step:876/2000 train_time:35122ms step_avg:40.09ms
step:877/2000 train_time:35183ms step_avg:40.12ms
step:878/2000 train_time:35242ms step_avg:40.14ms
step:879/2000 train_time:35302ms step_avg:40.16ms
step:880/2000 train_time:35361ms step_avg:40.18ms
step:881/2000 train_time:35421ms step_avg:40.21ms
step:882/2000 train_time:35481ms step_avg:40.23ms
step:883/2000 train_time:35541ms step_avg:40.25ms
step:884/2000 train_time:35601ms step_avg:40.27ms
step:885/2000 train_time:35662ms step_avg:40.30ms
step:886/2000 train_time:35722ms step_avg:40.32ms
step:887/2000 train_time:35783ms step_avg:40.34ms
step:888/2000 train_time:35842ms step_avg:40.36ms
step:889/2000 train_time:35903ms step_avg:40.39ms
step:890/2000 train_time:35962ms step_avg:40.41ms
step:891/2000 train_time:36023ms step_avg:40.43ms
step:892/2000 train_time:36082ms step_avg:40.45ms
step:893/2000 train_time:36142ms step_avg:40.47ms
step:894/2000 train_time:36202ms step_avg:40.49ms
step:895/2000 train_time:36262ms step_avg:40.52ms
step:896/2000 train_time:36322ms step_avg:40.54ms
step:897/2000 train_time:36382ms step_avg:40.56ms
step:898/2000 train_time:36442ms step_avg:40.58ms
step:899/2000 train_time:36502ms step_avg:40.60ms
step:900/2000 train_time:36562ms step_avg:40.62ms
step:901/2000 train_time:36622ms step_avg:40.65ms
step:902/2000 train_time:36681ms step_avg:40.67ms
step:903/2000 train_time:36742ms step_avg:40.69ms
step:904/2000 train_time:36802ms step_avg:40.71ms
step:905/2000 train_time:36863ms step_avg:40.73ms
step:906/2000 train_time:36923ms step_avg:40.75ms
step:907/2000 train_time:36983ms step_avg:40.78ms
step:908/2000 train_time:37042ms step_avg:40.80ms
step:909/2000 train_time:37103ms step_avg:40.82ms
step:910/2000 train_time:37162ms step_avg:40.84ms
step:911/2000 train_time:37223ms step_avg:40.86ms
step:912/2000 train_time:37282ms step_avg:40.88ms
step:913/2000 train_time:37341ms step_avg:40.90ms
step:914/2000 train_time:37401ms step_avg:40.92ms
step:915/2000 train_time:37462ms step_avg:40.94ms
step:916/2000 train_time:37521ms step_avg:40.96ms
step:917/2000 train_time:37582ms step_avg:40.98ms
step:918/2000 train_time:37641ms step_avg:41.00ms
step:919/2000 train_time:37702ms step_avg:41.02ms
step:920/2000 train_time:37761ms step_avg:41.04ms
step:921/2000 train_time:37822ms step_avg:41.07ms
step:922/2000 train_time:37882ms step_avg:41.09ms
step:923/2000 train_time:37943ms step_avg:41.11ms
step:924/2000 train_time:38002ms step_avg:41.13ms
step:925/2000 train_time:38063ms step_avg:41.15ms
step:926/2000 train_time:38122ms step_avg:41.17ms
step:927/2000 train_time:38182ms step_avg:41.19ms
step:928/2000 train_time:38241ms step_avg:41.21ms
step:929/2000 train_time:38302ms step_avg:41.23ms
step:930/2000 train_time:38362ms step_avg:41.25ms
step:931/2000 train_time:38423ms step_avg:41.27ms
step:932/2000 train_time:38482ms step_avg:41.29ms
step:933/2000 train_time:38542ms step_avg:41.31ms
step:934/2000 train_time:38602ms step_avg:41.33ms
step:935/2000 train_time:38662ms step_avg:41.35ms
step:936/2000 train_time:38722ms step_avg:41.37ms
step:937/2000 train_time:38782ms step_avg:41.39ms
step:938/2000 train_time:38841ms step_avg:41.41ms
step:939/2000 train_time:38902ms step_avg:41.43ms
step:940/2000 train_time:38962ms step_avg:41.45ms
step:941/2000 train_time:39022ms step_avg:41.47ms
step:942/2000 train_time:39081ms step_avg:41.49ms
step:943/2000 train_time:39141ms step_avg:41.51ms
step:944/2000 train_time:39200ms step_avg:41.53ms
step:945/2000 train_time:39261ms step_avg:41.55ms
step:946/2000 train_time:39321ms step_avg:41.57ms
step:947/2000 train_time:39382ms step_avg:41.59ms
step:948/2000 train_time:39441ms step_avg:41.60ms
step:949/2000 train_time:39501ms step_avg:41.62ms
step:950/2000 train_time:39561ms step_avg:41.64ms
step:951/2000 train_time:39622ms step_avg:41.66ms
step:952/2000 train_time:39681ms step_avg:41.68ms
step:953/2000 train_time:39742ms step_avg:41.70ms
step:954/2000 train_time:39802ms step_avg:41.72ms
step:955/2000 train_time:39863ms step_avg:41.74ms
step:956/2000 train_time:39922ms step_avg:41.76ms
step:957/2000 train_time:39982ms step_avg:41.78ms
step:958/2000 train_time:40041ms step_avg:41.80ms
step:959/2000 train_time:40101ms step_avg:41.82ms
step:960/2000 train_time:40161ms step_avg:41.83ms
step:961/2000 train_time:40222ms step_avg:41.85ms
step:962/2000 train_time:40282ms step_avg:41.87ms
step:963/2000 train_time:40343ms step_avg:41.89ms
step:964/2000 train_time:40402ms step_avg:41.91ms
step:965/2000 train_time:40463ms step_avg:41.93ms
step:966/2000 train_time:40522ms step_avg:41.95ms
step:967/2000 train_time:40582ms step_avg:41.97ms
step:968/2000 train_time:40642ms step_avg:41.99ms
step:969/2000 train_time:40702ms step_avg:42.00ms
step:970/2000 train_time:40763ms step_avg:42.02ms
step:971/2000 train_time:40824ms step_avg:42.04ms
step:972/2000 train_time:40883ms step_avg:42.06ms
step:973/2000 train_time:40943ms step_avg:42.08ms
step:974/2000 train_time:41002ms step_avg:42.10ms
step:975/2000 train_time:41064ms step_avg:42.12ms
step:976/2000 train_time:41123ms step_avg:42.13ms
step:977/2000 train_time:41184ms step_avg:42.15ms
step:978/2000 train_time:41243ms step_avg:42.17ms
step:979/2000 train_time:41303ms step_avg:42.19ms
step:980/2000 train_time:41363ms step_avg:42.21ms
step:981/2000 train_time:41424ms step_avg:42.23ms
step:982/2000 train_time:41483ms step_avg:42.24ms
step:983/2000 train_time:41544ms step_avg:42.26ms
step:984/2000 train_time:41603ms step_avg:42.28ms
step:985/2000 train_time:41663ms step_avg:42.30ms
step:986/2000 train_time:41723ms step_avg:42.32ms
step:987/2000 train_time:41784ms step_avg:42.33ms
step:988/2000 train_time:41843ms step_avg:42.35ms
step:989/2000 train_time:41904ms step_avg:42.37ms
step:990/2000 train_time:41963ms step_avg:42.39ms
step:991/2000 train_time:42024ms step_avg:42.41ms
step:992/2000 train_time:42083ms step_avg:42.42ms
step:993/2000 train_time:42143ms step_avg:42.44ms
step:994/2000 train_time:42202ms step_avg:42.46ms
step:995/2000 train_time:42263ms step_avg:42.48ms
step:996/2000 train_time:42323ms step_avg:42.49ms
step:997/2000 train_time:42383ms step_avg:42.51ms
step:998/2000 train_time:42442ms step_avg:42.53ms
step:999/2000 train_time:42503ms step_avg:42.55ms
step:1000/2000 train_time:42562ms step_avg:42.56ms
step:1000/2000 val_loss:3.6757 train_time:42625ms step_avg:42.63ms
step:1001/2000 train_time:42644ms step_avg:42.60ms
step:1002/2000 train_time:42683ms step_avg:42.60ms
step:1003/2000 train_time:42747ms step_avg:42.62ms
step:1004/2000 train_time:42809ms step_avg:42.64ms
step:1005/2000 train_time:42871ms step_avg:42.66ms
step:1006/2000 train_time:42931ms step_avg:42.67ms
step:1007/2000 train_time:42990ms step_avg:42.69ms
step:1008/2000 train_time:43049ms step_avg:42.71ms
step:1009/2000 train_time:43109ms step_avg:42.72ms
step:1010/2000 train_time:43168ms step_avg:42.74ms
step:1011/2000 train_time:43227ms step_avg:42.76ms
step:1012/2000 train_time:43286ms step_avg:42.77ms
step:1013/2000 train_time:43346ms step_avg:42.79ms
step:1014/2000 train_time:43405ms step_avg:42.81ms
step:1015/2000 train_time:43465ms step_avg:42.82ms
step:1016/2000 train_time:43525ms step_avg:42.84ms
step:1017/2000 train_time:43587ms step_avg:42.86ms
step:1018/2000 train_time:43647ms step_avg:42.88ms
step:1019/2000 train_time:43709ms step_avg:42.89ms
step:1020/2000 train_time:43770ms step_avg:42.91ms
step:1021/2000 train_time:43831ms step_avg:42.93ms
step:1022/2000 train_time:43890ms step_avg:42.95ms
step:1023/2000 train_time:43952ms step_avg:42.96ms
step:1024/2000 train_time:44012ms step_avg:42.98ms
step:1025/2000 train_time:44072ms step_avg:43.00ms
step:1026/2000 train_time:44131ms step_avg:43.01ms
step:1027/2000 train_time:44190ms step_avg:43.03ms
step:1028/2000 train_time:44249ms step_avg:43.04ms
step:1029/2000 train_time:44311ms step_avg:43.06ms
step:1030/2000 train_time:44369ms step_avg:43.08ms
step:1031/2000 train_time:44429ms step_avg:43.09ms
step:1032/2000 train_time:44489ms step_avg:43.11ms
step:1033/2000 train_time:44549ms step_avg:43.13ms
step:1034/2000 train_time:44610ms step_avg:43.14ms
step:1035/2000 train_time:44672ms step_avg:43.16ms
step:1036/2000 train_time:44732ms step_avg:43.18ms
step:1037/2000 train_time:44793ms step_avg:43.19ms
step:1038/2000 train_time:44852ms step_avg:43.21ms
step:1039/2000 train_time:44913ms step_avg:43.23ms
step:1040/2000 train_time:44973ms step_avg:43.24ms
step:1041/2000 train_time:45033ms step_avg:43.26ms
step:1042/2000 train_time:45092ms step_avg:43.27ms
step:1043/2000 train_time:45152ms step_avg:43.29ms
step:1044/2000 train_time:45211ms step_avg:43.31ms
step:1045/2000 train_time:45272ms step_avg:43.32ms
step:1046/2000 train_time:45331ms step_avg:43.34ms
step:1047/2000 train_time:45390ms step_avg:43.35ms
step:1048/2000 train_time:45449ms step_avg:43.37ms
step:1049/2000 train_time:45510ms step_avg:43.38ms
step:1050/2000 train_time:45570ms step_avg:43.40ms
step:1051/2000 train_time:45631ms step_avg:43.42ms
step:1052/2000 train_time:45690ms step_avg:43.43ms
step:1053/2000 train_time:45751ms step_avg:43.45ms
step:1054/2000 train_time:45812ms step_avg:43.46ms
step:1055/2000 train_time:45873ms step_avg:43.48ms
step:1056/2000 train_time:45932ms step_avg:43.50ms
step:1057/2000 train_time:45992ms step_avg:43.51ms
step:1058/2000 train_time:46052ms step_avg:43.53ms
step:1059/2000 train_time:46112ms step_avg:43.54ms
step:1060/2000 train_time:46171ms step_avg:43.56ms
step:1061/2000 train_time:46232ms step_avg:43.57ms
step:1062/2000 train_time:46291ms step_avg:43.59ms
step:1063/2000 train_time:46351ms step_avg:43.60ms
step:1064/2000 train_time:46410ms step_avg:43.62ms
step:1065/2000 train_time:46471ms step_avg:43.63ms
step:1066/2000 train_time:46530ms step_avg:43.65ms
step:1067/2000 train_time:46590ms step_avg:43.66ms
step:1068/2000 train_time:46650ms step_avg:43.68ms
step:1069/2000 train_time:46712ms step_avg:43.70ms
step:1070/2000 train_time:46772ms step_avg:43.71ms
step:1071/2000 train_time:46833ms step_avg:43.73ms
step:1072/2000 train_time:46892ms step_avg:43.74ms
step:1073/2000 train_time:46952ms step_avg:43.76ms
step:1074/2000 train_time:47012ms step_avg:43.77ms
step:1075/2000 train_time:47072ms step_avg:43.79ms
step:1076/2000 train_time:47131ms step_avg:43.80ms
step:1077/2000 train_time:47191ms step_avg:43.82ms
step:1078/2000 train_time:47251ms step_avg:43.83ms
step:1079/2000 train_time:47311ms step_avg:43.85ms
step:1080/2000 train_time:47370ms step_avg:43.86ms
step:1081/2000 train_time:47431ms step_avg:43.88ms
step:1082/2000 train_time:47490ms step_avg:43.89ms
step:1083/2000 train_time:47551ms step_avg:43.91ms
step:1084/2000 train_time:47610ms step_avg:43.92ms
step:1085/2000 train_time:47671ms step_avg:43.94ms
step:1086/2000 train_time:47730ms step_avg:43.95ms
step:1087/2000 train_time:47791ms step_avg:43.97ms
step:1088/2000 train_time:47850ms step_avg:43.98ms
step:1089/2000 train_time:47911ms step_avg:44.00ms
step:1090/2000 train_time:47971ms step_avg:44.01ms
step:1091/2000 train_time:48031ms step_avg:44.02ms
step:1092/2000 train_time:48090ms step_avg:44.04ms
step:1093/2000 train_time:48151ms step_avg:44.05ms
step:1094/2000 train_time:48211ms step_avg:44.07ms
step:1095/2000 train_time:48271ms step_avg:44.08ms
step:1096/2000 train_time:48330ms step_avg:44.10ms
step:1097/2000 train_time:48391ms step_avg:44.11ms
step:1098/2000 train_time:48450ms step_avg:44.13ms
step:1099/2000 train_time:48511ms step_avg:44.14ms
step:1100/2000 train_time:48570ms step_avg:44.15ms
step:1101/2000 train_time:48631ms step_avg:44.17ms
step:1102/2000 train_time:48690ms step_avg:44.18ms
step:1103/2000 train_time:48752ms step_avg:44.20ms
step:1104/2000 train_time:48812ms step_avg:44.21ms
step:1105/2000 train_time:48873ms step_avg:44.23ms
step:1106/2000 train_time:48932ms step_avg:44.24ms
step:1107/2000 train_time:48992ms step_avg:44.26ms
step:1108/2000 train_time:49052ms step_avg:44.27ms
step:1109/2000 train_time:49112ms step_avg:44.28ms
step:1110/2000 train_time:49171ms step_avg:44.30ms
step:1111/2000 train_time:49232ms step_avg:44.31ms
step:1112/2000 train_time:49291ms step_avg:44.33ms
step:1113/2000 train_time:49351ms step_avg:44.34ms
step:1114/2000 train_time:49410ms step_avg:44.35ms
step:1115/2000 train_time:49470ms step_avg:44.37ms
step:1116/2000 train_time:49529ms step_avg:44.38ms
step:1117/2000 train_time:49590ms step_avg:44.40ms
step:1118/2000 train_time:49650ms step_avg:44.41ms
step:1119/2000 train_time:49711ms step_avg:44.42ms
step:1120/2000 train_time:49770ms step_avg:44.44ms
step:1121/2000 train_time:49831ms step_avg:44.45ms
step:1122/2000 train_time:49890ms step_avg:44.47ms
step:1123/2000 train_time:49952ms step_avg:44.48ms
step:1124/2000 train_time:50011ms step_avg:44.49ms
step:1125/2000 train_time:50072ms step_avg:44.51ms
step:1126/2000 train_time:50131ms step_avg:44.52ms
step:1127/2000 train_time:50191ms step_avg:44.54ms
step:1128/2000 train_time:50250ms step_avg:44.55ms
step:1129/2000 train_time:50311ms step_avg:44.56ms
step:1130/2000 train_time:50370ms step_avg:44.58ms
step:1131/2000 train_time:50430ms step_avg:44.59ms
step:1132/2000 train_time:50489ms step_avg:44.60ms
step:1133/2000 train_time:50551ms step_avg:44.62ms
step:1134/2000 train_time:50611ms step_avg:44.63ms
step:1135/2000 train_time:50672ms step_avg:44.64ms
step:1136/2000 train_time:50731ms step_avg:44.66ms
step:1137/2000 train_time:50792ms step_avg:44.67ms
step:1138/2000 train_time:50851ms step_avg:44.68ms
step:1139/2000 train_time:50911ms step_avg:44.70ms
step:1140/2000 train_time:50971ms step_avg:44.71ms
step:1141/2000 train_time:51031ms step_avg:44.73ms
step:1142/2000 train_time:51091ms step_avg:44.74ms
step:1143/2000 train_time:51151ms step_avg:44.75ms
step:1144/2000 train_time:51211ms step_avg:44.76ms
step:1145/2000 train_time:51271ms step_avg:44.78ms
step:1146/2000 train_time:51330ms step_avg:44.79ms
step:1147/2000 train_time:51390ms step_avg:44.80ms
step:1148/2000 train_time:51450ms step_avg:44.82ms
step:1149/2000 train_time:51510ms step_avg:44.83ms
step:1150/2000 train_time:51570ms step_avg:44.84ms
step:1151/2000 train_time:51631ms step_avg:44.86ms
step:1152/2000 train_time:51690ms step_avg:44.87ms
step:1153/2000 train_time:51750ms step_avg:44.88ms
step:1154/2000 train_time:51810ms step_avg:44.90ms
step:1155/2000 train_time:51870ms step_avg:44.91ms
step:1156/2000 train_time:51930ms step_avg:44.92ms
step:1157/2000 train_time:51991ms step_avg:44.94ms
step:1158/2000 train_time:52051ms step_avg:44.95ms
step:1159/2000 train_time:52112ms step_avg:44.96ms
step:1160/2000 train_time:52171ms step_avg:44.98ms
step:1161/2000 train_time:52231ms step_avg:44.99ms
step:1162/2000 train_time:52290ms step_avg:45.00ms
step:1163/2000 train_time:52351ms step_avg:45.01ms
step:1164/2000 train_time:52411ms step_avg:45.03ms
step:1165/2000 train_time:52472ms step_avg:45.04ms
step:1166/2000 train_time:52531ms step_avg:45.05ms
step:1167/2000 train_time:52591ms step_avg:45.07ms
step:1168/2000 train_time:52651ms step_avg:45.08ms
step:1169/2000 train_time:52712ms step_avg:45.09ms
step:1170/2000 train_time:52771ms step_avg:45.10ms
step:1171/2000 train_time:52831ms step_avg:45.12ms
step:1172/2000 train_time:52890ms step_avg:45.13ms
step:1173/2000 train_time:52950ms step_avg:45.14ms
step:1174/2000 train_time:53010ms step_avg:45.15ms
step:1175/2000 train_time:53071ms step_avg:45.17ms
step:1176/2000 train_time:53130ms step_avg:45.18ms
step:1177/2000 train_time:53190ms step_avg:45.19ms
step:1178/2000 train_time:53249ms step_avg:45.20ms
step:1179/2000 train_time:53310ms step_avg:45.22ms
step:1180/2000 train_time:53370ms step_avg:45.23ms
step:1181/2000 train_time:53431ms step_avg:45.24ms
step:1182/2000 train_time:53490ms step_avg:45.25ms
step:1183/2000 train_time:53550ms step_avg:45.27ms
step:1184/2000 train_time:53610ms step_avg:45.28ms
step:1185/2000 train_time:53671ms step_avg:45.29ms
step:1186/2000 train_time:53730ms step_avg:45.30ms
step:1187/2000 train_time:53791ms step_avg:45.32ms
step:1188/2000 train_time:53850ms step_avg:45.33ms
step:1189/2000 train_time:53911ms step_avg:45.34ms
step:1190/2000 train_time:53970ms step_avg:45.35ms
step:1191/2000 train_time:54031ms step_avg:45.37ms
step:1192/2000 train_time:54090ms step_avg:45.38ms
step:1193/2000 train_time:54152ms step_avg:45.39ms
step:1194/2000 train_time:54211ms step_avg:45.40ms
step:1195/2000 train_time:54272ms step_avg:45.42ms
step:1196/2000 train_time:54331ms step_avg:45.43ms
step:1197/2000 train_time:54391ms step_avg:45.44ms
step:1198/2000 train_time:54451ms step_avg:45.45ms
step:1199/2000 train_time:54512ms step_avg:45.46ms
step:1200/2000 train_time:54571ms step_avg:45.48ms
step:1201/2000 train_time:54632ms step_avg:45.49ms
step:1202/2000 train_time:54692ms step_avg:45.50ms
step:1203/2000 train_time:54752ms step_avg:45.51ms
step:1204/2000 train_time:54812ms step_avg:45.52ms
step:1205/2000 train_time:54872ms step_avg:45.54ms
step:1206/2000 train_time:54931ms step_avg:45.55ms
step:1207/2000 train_time:54992ms step_avg:45.56ms
step:1208/2000 train_time:55051ms step_avg:45.57ms
step:1209/2000 train_time:55112ms step_avg:45.58ms
step:1210/2000 train_time:55171ms step_avg:45.60ms
step:1211/2000 train_time:55231ms step_avg:45.61ms
step:1212/2000 train_time:55291ms step_avg:45.62ms
step:1213/2000 train_time:55352ms step_avg:45.63ms
step:1214/2000 train_time:55411ms step_avg:45.64ms
step:1215/2000 train_time:55471ms step_avg:45.66ms
step:1216/2000 train_time:55530ms step_avg:45.67ms
step:1217/2000 train_time:55591ms step_avg:45.68ms
step:1218/2000 train_time:55651ms step_avg:45.69ms
step:1219/2000 train_time:55712ms step_avg:45.70ms
step:1220/2000 train_time:55771ms step_avg:45.71ms
step:1221/2000 train_time:55832ms step_avg:45.73ms
step:1222/2000 train_time:55891ms step_avg:45.74ms
step:1223/2000 train_time:55952ms step_avg:45.75ms
step:1224/2000 train_time:56012ms step_avg:45.76ms
step:1225/2000 train_time:56073ms step_avg:45.77ms
step:1226/2000 train_time:56132ms step_avg:45.78ms
step:1227/2000 train_time:56192ms step_avg:45.80ms
step:1228/2000 train_time:56253ms step_avg:45.81ms
step:1229/2000 train_time:56314ms step_avg:45.82ms
step:1230/2000 train_time:56373ms step_avg:45.83ms
step:1231/2000 train_time:56433ms step_avg:45.84ms
step:1232/2000 train_time:56492ms step_avg:45.85ms
step:1233/2000 train_time:56553ms step_avg:45.87ms
step:1234/2000 train_time:56612ms step_avg:45.88ms
step:1235/2000 train_time:56673ms step_avg:45.89ms
step:1236/2000 train_time:56732ms step_avg:45.90ms
step:1237/2000 train_time:56792ms step_avg:45.91ms
step:1238/2000 train_time:56851ms step_avg:45.92ms
step:1239/2000 train_time:56911ms step_avg:45.93ms
step:1240/2000 train_time:56971ms step_avg:45.94ms
step:1241/2000 train_time:57032ms step_avg:45.96ms
step:1242/2000 train_time:57091ms step_avg:45.97ms
step:1243/2000 train_time:57152ms step_avg:45.98ms
step:1244/2000 train_time:57211ms step_avg:45.99ms
step:1245/2000 train_time:57272ms step_avg:46.00ms
step:1246/2000 train_time:57331ms step_avg:46.01ms
step:1247/2000 train_time:57392ms step_avg:46.02ms
step:1248/2000 train_time:57452ms step_avg:46.03ms
step:1249/2000 train_time:57512ms step_avg:46.05ms
step:1250/2000 train_time:57572ms step_avg:46.06ms
step:1250/2000 val_loss:3.5584 train_time:57634ms step_avg:46.11ms
step:1251/2000 train_time:57653ms step_avg:46.09ms
step:1252/2000 train_time:57693ms step_avg:46.08ms
step:1253/2000 train_time:57757ms step_avg:46.09ms
step:1254/2000 train_time:57820ms step_avg:46.11ms
step:1255/2000 train_time:57881ms step_avg:46.12ms
step:1256/2000 train_time:57941ms step_avg:46.13ms
step:1257/2000 train_time:58001ms step_avg:46.14ms
step:1258/2000 train_time:58060ms step_avg:46.15ms
step:1259/2000 train_time:58120ms step_avg:46.16ms
step:1260/2000 train_time:58179ms step_avg:46.17ms
step:1261/2000 train_time:58239ms step_avg:46.18ms
step:1262/2000 train_time:58298ms step_avg:46.19ms
step:1263/2000 train_time:58358ms step_avg:46.21ms
step:1264/2000 train_time:58418ms step_avg:46.22ms
step:1265/2000 train_time:58478ms step_avg:46.23ms
step:1266/2000 train_time:58537ms step_avg:46.24ms
step:1267/2000 train_time:58599ms step_avg:46.25ms
step:1268/2000 train_time:58660ms step_avg:46.26ms
step:1269/2000 train_time:58722ms step_avg:46.27ms
step:1270/2000 train_time:58783ms step_avg:46.29ms
step:1271/2000 train_time:58844ms step_avg:46.30ms
step:1272/2000 train_time:58904ms step_avg:46.31ms
step:1273/2000 train_time:58965ms step_avg:46.32ms
step:1274/2000 train_time:59025ms step_avg:46.33ms
step:1275/2000 train_time:59084ms step_avg:46.34ms
step:1276/2000 train_time:59143ms step_avg:46.35ms
step:1277/2000 train_time:59203ms step_avg:46.36ms
step:1278/2000 train_time:59261ms step_avg:46.37ms
step:1279/2000 train_time:59321ms step_avg:46.38ms
step:1280/2000 train_time:59381ms step_avg:46.39ms
step:1281/2000 train_time:59441ms step_avg:46.40ms
step:1282/2000 train_time:59501ms step_avg:46.41ms
step:1283/2000 train_time:59562ms step_avg:46.42ms
step:1284/2000 train_time:59622ms step_avg:46.43ms
step:1285/2000 train_time:59683ms step_avg:46.45ms
step:1286/2000 train_time:59744ms step_avg:46.46ms
step:1287/2000 train_time:59805ms step_avg:46.47ms
step:1288/2000 train_time:59865ms step_avg:46.48ms
step:1289/2000 train_time:59925ms step_avg:46.49ms
step:1290/2000 train_time:59984ms step_avg:46.50ms
step:1291/2000 train_time:60045ms step_avg:46.51ms
step:1292/2000 train_time:60105ms step_avg:46.52ms
step:1293/2000 train_time:60165ms step_avg:46.53ms
step:1294/2000 train_time:60224ms step_avg:46.54ms
step:1295/2000 train_time:60283ms step_avg:46.55ms
step:1296/2000 train_time:60342ms step_avg:46.56ms
step:1297/2000 train_time:60403ms step_avg:46.57ms
step:1298/2000 train_time:60462ms step_avg:46.58ms
step:1299/2000 train_time:60524ms step_avg:46.59ms
step:1300/2000 train_time:60583ms step_avg:46.60ms
step:1301/2000 train_time:60644ms step_avg:46.61ms
step:1302/2000 train_time:60704ms step_avg:46.62ms
step:1303/2000 train_time:60766ms step_avg:46.64ms
step:1304/2000 train_time:60826ms step_avg:46.65ms
step:1305/2000 train_time:60886ms step_avg:46.66ms
step:1306/2000 train_time:60945ms step_avg:46.67ms
step:1307/2000 train_time:61005ms step_avg:46.68ms
step:1308/2000 train_time:61065ms step_avg:46.69ms
step:1309/2000 train_time:61153ms step_avg:46.72ms
step:1310/2000 train_time:61241ms step_avg:46.75ms
step:1311/2000 train_time:61328ms step_avg:46.78ms
step:1312/2000 train_time:61414ms step_avg:46.81ms
step:1313/2000 train_time:61502ms step_avg:46.84ms
step:1314/2000 train_time:61591ms step_avg:46.87ms
step:1315/2000 train_time:61679ms step_avg:46.90ms
step:1316/2000 train_time:61769ms step_avg:46.94ms
step:1317/2000 train_time:61858ms step_avg:46.97ms
step:1318/2000 train_time:61946ms step_avg:47.00ms
step:1319/2000 train_time:62034ms step_avg:47.03ms
step:1320/2000 train_time:62123ms step_avg:47.06ms
step:1321/2000 train_time:62212ms step_avg:47.09ms
step:1322/2000 train_time:62299ms step_avg:47.12ms
step:1323/2000 train_time:62386ms step_avg:47.15ms
step:1324/2000 train_time:62474ms step_avg:47.19ms
step:1325/2000 train_time:62562ms step_avg:47.22ms
step:1326/2000 train_time:62649ms step_avg:47.25ms
step:1327/2000 train_time:62739ms step_avg:47.28ms
step:1328/2000 train_time:62827ms step_avg:47.31ms
step:1329/2000 train_time:62916ms step_avg:47.34ms
step:1330/2000 train_time:63004ms step_avg:47.37ms
step:1331/2000 train_time:63092ms step_avg:47.40ms
step:1332/2000 train_time:63179ms step_avg:47.43ms
step:1333/2000 train_time:63268ms step_avg:47.46ms
step:1334/2000 train_time:63355ms step_avg:47.49ms
step:1335/2000 train_time:63443ms step_avg:47.52ms
step:1336/2000 train_time:63531ms step_avg:47.55ms
step:1337/2000 train_time:63619ms step_avg:47.58ms
step:1338/2000 train_time:63707ms step_avg:47.61ms
step:1339/2000 train_time:63796ms step_avg:47.64ms
step:1340/2000 train_time:63884ms step_avg:47.67ms
step:1341/2000 train_time:63973ms step_avg:47.71ms
step:1342/2000 train_time:64061ms step_avg:47.74ms
step:1343/2000 train_time:64150ms step_avg:47.77ms
step:1344/2000 train_time:64238ms step_avg:47.80ms
step:1345/2000 train_time:64326ms step_avg:47.83ms
step:1346/2000 train_time:64413ms step_avg:47.86ms
step:1347/2000 train_time:64501ms step_avg:47.88ms
step:1348/2000 train_time:64589ms step_avg:47.91ms
step:1349/2000 train_time:64677ms step_avg:47.94ms
step:1350/2000 train_time:64764ms step_avg:47.97ms
step:1351/2000 train_time:64853ms step_avg:48.00ms
step:1352/2000 train_time:64941ms step_avg:48.03ms
step:1353/2000 train_time:65030ms step_avg:48.06ms
step:1354/2000 train_time:65118ms step_avg:48.09ms
step:1355/2000 train_time:65206ms step_avg:48.12ms
step:1356/2000 train_time:65294ms step_avg:48.15ms
step:1357/2000 train_time:65382ms step_avg:48.18ms
step:1358/2000 train_time:65471ms step_avg:48.21ms
step:1359/2000 train_time:65558ms step_avg:48.24ms
step:1360/2000 train_time:65646ms step_avg:48.27ms
step:1361/2000 train_time:65734ms step_avg:48.30ms
step:1362/2000 train_time:65821ms step_avg:48.33ms
step:1363/2000 train_time:65910ms step_avg:48.36ms
step:1364/2000 train_time:65997ms step_avg:48.39ms
step:1365/2000 train_time:66086ms step_avg:48.41ms
step:1366/2000 train_time:66174ms step_avg:48.44ms
step:1367/2000 train_time:66262ms step_avg:48.47ms
step:1368/2000 train_time:66349ms step_avg:48.50ms
step:1369/2000 train_time:66438ms step_avg:48.53ms
step:1370/2000 train_time:66525ms step_avg:48.56ms
step:1371/2000 train_time:66613ms step_avg:48.59ms
step:1372/2000 train_time:66700ms step_avg:48.62ms
step:1373/2000 train_time:66790ms step_avg:48.64ms
step:1374/2000 train_time:66877ms step_avg:48.67ms
step:1375/2000 train_time:66965ms step_avg:48.70ms
step:1376/2000 train_time:67053ms step_avg:48.73ms
step:1377/2000 train_time:67141ms step_avg:48.76ms
step:1378/2000 train_time:67230ms step_avg:48.79ms
step:1379/2000 train_time:67318ms step_avg:48.82ms
step:1380/2000 train_time:67406ms step_avg:48.84ms
step:1381/2000 train_time:67494ms step_avg:48.87ms
step:1382/2000 train_time:67583ms step_avg:48.90ms
step:1383/2000 train_time:67671ms step_avg:48.93ms
step:1384/2000 train_time:67759ms step_avg:48.96ms
step:1385/2000 train_time:67847ms step_avg:48.99ms
step:1386/2000 train_time:67936ms step_avg:49.02ms
step:1387/2000 train_time:68025ms step_avg:49.04ms
step:1388/2000 train_time:68112ms step_avg:49.07ms
step:1389/2000 train_time:68200ms step_avg:49.10ms
step:1390/2000 train_time:68288ms step_avg:49.13ms
step:1391/2000 train_time:68376ms step_avg:49.16ms
step:1392/2000 train_time:68464ms step_avg:49.18ms
step:1393/2000 train_time:68553ms step_avg:49.21ms
step:1394/2000 train_time:68641ms step_avg:49.24ms
step:1395/2000 train_time:68730ms step_avg:49.27ms
step:1396/2000 train_time:68818ms step_avg:49.30ms
step:1397/2000 train_time:68906ms step_avg:49.32ms
step:1398/2000 train_time:68996ms step_avg:49.35ms
step:1399/2000 train_time:69084ms step_avg:49.38ms
step:1400/2000 train_time:69172ms step_avg:49.41ms
step:1401/2000 train_time:69260ms step_avg:49.44ms
step:1402/2000 train_time:69347ms step_avg:49.46ms
step:1403/2000 train_time:69434ms step_avg:49.49ms
step:1404/2000 train_time:69523ms step_avg:49.52ms
step:1405/2000 train_time:69611ms step_avg:49.55ms
step:1406/2000 train_time:69699ms step_avg:49.57ms
step:1407/2000 train_time:69787ms step_avg:49.60ms
step:1408/2000 train_time:69875ms step_avg:49.63ms
step:1409/2000 train_time:69963ms step_avg:49.65ms
step:1410/2000 train_time:70050ms step_avg:49.68ms
step:1411/2000 train_time:70139ms step_avg:49.71ms
step:1412/2000 train_time:70227ms step_avg:49.74ms
step:1413/2000 train_time:70315ms step_avg:49.76ms
step:1414/2000 train_time:70403ms step_avg:49.79ms
step:1415/2000 train_time:70492ms step_avg:49.82ms
step:1416/2000 train_time:70580ms step_avg:49.84ms
step:1417/2000 train_time:70668ms step_avg:49.87ms
step:1418/2000 train_time:70756ms step_avg:49.90ms
step:1419/2000 train_time:70843ms step_avg:49.92ms
step:1420/2000 train_time:70931ms step_avg:49.95ms
step:1421/2000 train_time:71019ms step_avg:49.98ms
step:1422/2000 train_time:71108ms step_avg:50.01ms
step:1423/2000 train_time:71196ms step_avg:50.03ms
step:1424/2000 train_time:71283ms step_avg:50.06ms
step:1425/2000 train_time:71372ms step_avg:50.09ms
step:1426/2000 train_time:71460ms step_avg:50.11ms
step:1427/2000 train_time:71548ms step_avg:50.14ms
step:1428/2000 train_time:71635ms step_avg:50.16ms
step:1429/2000 train_time:71724ms step_avg:50.19ms
step:1430/2000 train_time:71811ms step_avg:50.22ms
step:1431/2000 train_time:71899ms step_avg:50.24ms
step:1432/2000 train_time:71987ms step_avg:50.27ms
step:1433/2000 train_time:72075ms step_avg:50.30ms
step:1434/2000 train_time:72165ms step_avg:50.32ms
step:1435/2000 train_time:72252ms step_avg:50.35ms
step:1436/2000 train_time:72340ms step_avg:50.38ms
step:1437/2000 train_time:72429ms step_avg:50.40ms
step:1438/2000 train_time:72516ms step_avg:50.43ms
step:1439/2000 train_time:72604ms step_avg:50.45ms
step:1440/2000 train_time:72692ms step_avg:50.48ms
step:1441/2000 train_time:72780ms step_avg:50.51ms
step:1442/2000 train_time:72867ms step_avg:50.53ms
step:1443/2000 train_time:72956ms step_avg:50.56ms
step:1444/2000 train_time:73044ms step_avg:50.58ms
step:1445/2000 train_time:73134ms step_avg:50.61ms
step:1446/2000 train_time:73221ms step_avg:50.64ms
step:1447/2000 train_time:73309ms step_avg:50.66ms
step:1448/2000 train_time:73396ms step_avg:50.69ms
step:1449/2000 train_time:73485ms step_avg:50.71ms
step:1450/2000 train_time:73573ms step_avg:50.74ms
step:1451/2000 train_time:73660ms step_avg:50.77ms
step:1452/2000 train_time:73749ms step_avg:50.79ms
step:1453/2000 train_time:73837ms step_avg:50.82ms
step:1454/2000 train_time:73925ms step_avg:50.84ms
step:1455/2000 train_time:74013ms step_avg:50.87ms
step:1456/2000 train_time:74101ms step_avg:50.89ms
step:1457/2000 train_time:74190ms step_avg:50.92ms
step:1458/2000 train_time:74277ms step_avg:50.94ms
step:1459/2000 train_time:74365ms step_avg:50.97ms
step:1460/2000 train_time:74452ms step_avg:50.99ms
step:1461/2000 train_time:74540ms step_avg:51.02ms
step:1462/2000 train_time:74629ms step_avg:51.05ms
step:1463/2000 train_time:74716ms step_avg:51.07ms
step:1464/2000 train_time:74804ms step_avg:51.10ms
step:1465/2000 train_time:74893ms step_avg:51.12ms
step:1466/2000 train_time:74981ms step_avg:51.15ms
step:1467/2000 train_time:75070ms step_avg:51.17ms
step:1468/2000 train_time:75157ms step_avg:51.20ms
step:1469/2000 train_time:75246ms step_avg:51.22ms
step:1470/2000 train_time:75334ms step_avg:51.25ms
step:1471/2000 train_time:75421ms step_avg:51.27ms
step:1472/2000 train_time:75509ms step_avg:51.30ms
step:1473/2000 train_time:75597ms step_avg:51.32ms
step:1474/2000 train_time:75685ms step_avg:51.35ms
step:1475/2000 train_time:75773ms step_avg:51.37ms
step:1476/2000 train_time:75860ms step_avg:51.40ms
step:1477/2000 train_time:75949ms step_avg:51.42ms
step:1478/2000 train_time:76037ms step_avg:51.45ms
step:1479/2000 train_time:76126ms step_avg:51.47ms
step:1480/2000 train_time:76214ms step_avg:51.50ms
step:1481/2000 train_time:76302ms step_avg:51.52ms
step:1482/2000 train_time:76390ms step_avg:51.55ms
step:1483/2000 train_time:76477ms step_avg:51.57ms
step:1484/2000 train_time:76565ms step_avg:51.59ms
step:1485/2000 train_time:76654ms step_avg:51.62ms
step:1486/2000 train_time:76741ms step_avg:51.64ms
step:1487/2000 train_time:76830ms step_avg:51.67ms
step:1488/2000 train_time:76917ms step_avg:51.69ms
step:1489/2000 train_time:77006ms step_avg:51.72ms
step:1490/2000 train_time:77094ms step_avg:51.74ms
step:1491/2000 train_time:77182ms step_avg:51.77ms
step:1492/2000 train_time:77270ms step_avg:51.79ms
step:1493/2000 train_time:77359ms step_avg:51.81ms
step:1494/2000 train_time:77446ms step_avg:51.84ms
step:1495/2000 train_time:77534ms step_avg:51.86ms
step:1496/2000 train_time:77622ms step_avg:51.89ms
step:1497/2000 train_time:77710ms step_avg:51.91ms
step:1498/2000 train_time:77798ms step_avg:51.93ms
step:1499/2000 train_time:77886ms step_avg:51.96ms
step:1500/2000 train_time:77974ms step_avg:51.98ms
step:1500/2000 val_loss:3.4438 train_time:78064ms step_avg:52.04ms
step:1501/2000 train_time:78083ms step_avg:52.02ms
step:1502/2000 train_time:78154ms step_avg:52.03ms
step:1503/2000 train_time:78245ms step_avg:52.06ms
step:1504/2000 train_time:78333ms step_avg:52.08ms
step:1505/2000 train_time:78420ms step_avg:52.11ms
step:1506/2000 train_time:78507ms step_avg:52.13ms
step:1507/2000 train_time:78594ms step_avg:52.15ms
step:1508/2000 train_time:78681ms step_avg:52.18ms
step:1509/2000 train_time:78769ms step_avg:52.20ms
step:1510/2000 train_time:78856ms step_avg:52.22ms
step:1511/2000 train_time:78945ms step_avg:52.25ms
step:1512/2000 train_time:79035ms step_avg:52.27ms
step:1513/2000 train_time:79125ms step_avg:52.30ms
step:1514/2000 train_time:79214ms step_avg:52.32ms
step:1515/2000 train_time:79302ms step_avg:52.34ms
step:1516/2000 train_time:79390ms step_avg:52.37ms
step:1517/2000 train_time:79477ms step_avg:52.39ms
step:1518/2000 train_time:79565ms step_avg:52.41ms
step:1519/2000 train_time:79651ms step_avg:52.44ms
step:1520/2000 train_time:79738ms step_avg:52.46ms
step:1521/2000 train_time:79826ms step_avg:52.48ms
step:1522/2000 train_time:79913ms step_avg:52.50ms
step:1523/2000 train_time:80003ms step_avg:52.53ms
step:1524/2000 train_time:80091ms step_avg:52.55ms
step:1525/2000 train_time:80181ms step_avg:52.58ms
step:1526/2000 train_time:80268ms step_avg:52.60ms
step:1527/2000 train_time:80357ms step_avg:52.62ms
step:1528/2000 train_time:80444ms step_avg:52.65ms
step:1529/2000 train_time:80533ms step_avg:52.67ms
step:1530/2000 train_time:80620ms step_avg:52.69ms
step:1531/2000 train_time:80708ms step_avg:52.72ms
step:1532/2000 train_time:80795ms step_avg:52.74ms
step:1533/2000 train_time:80883ms step_avg:52.76ms
step:1534/2000 train_time:80971ms step_avg:52.78ms
step:1535/2000 train_time:81059ms step_avg:52.81ms
step:1536/2000 train_time:81148ms step_avg:52.83ms
step:1537/2000 train_time:81237ms step_avg:52.85ms
step:1538/2000 train_time:81324ms step_avg:52.88ms
step:1539/2000 train_time:81414ms step_avg:52.90ms
step:1540/2000 train_time:81501ms step_avg:52.92ms
step:1541/2000 train_time:81590ms step_avg:52.95ms
step:1542/2000 train_time:81677ms step_avg:52.97ms
step:1543/2000 train_time:81765ms step_avg:52.99ms
step:1544/2000 train_time:81852ms step_avg:53.01ms
step:1545/2000 train_time:81940ms step_avg:53.04ms
step:1546/2000 train_time:82029ms step_avg:53.06ms
step:1547/2000 train_time:82117ms step_avg:53.08ms
step:1548/2000 train_time:82205ms step_avg:53.10ms
step:1549/2000 train_time:82293ms step_avg:53.13ms
step:1550/2000 train_time:82381ms step_avg:53.15ms
step:1551/2000 train_time:82470ms step_avg:53.17ms
step:1552/2000 train_time:82558ms step_avg:53.19ms
step:1553/2000 train_time:82646ms step_avg:53.22ms
step:1554/2000 train_time:82734ms step_avg:53.24ms
step:1555/2000 train_time:82821ms step_avg:53.26ms
step:1556/2000 train_time:82909ms step_avg:53.28ms
step:1557/2000 train_time:82996ms step_avg:53.31ms
step:1558/2000 train_time:83084ms step_avg:53.33ms
step:1559/2000 train_time:83172ms step_avg:53.35ms
step:1560/2000 train_time:83261ms step_avg:53.37ms
step:1561/2000 train_time:83350ms step_avg:53.40ms
step:1562/2000 train_time:83439ms step_avg:53.42ms
step:1563/2000 train_time:83527ms step_avg:53.44ms
step:1564/2000 train_time:83614ms step_avg:53.46ms
step:1565/2000 train_time:83703ms step_avg:53.48ms
step:1566/2000 train_time:83791ms step_avg:53.51ms
step:1567/2000 train_time:83878ms step_avg:53.53ms
step:1568/2000 train_time:83966ms step_avg:53.55ms
step:1569/2000 train_time:84054ms step_avg:53.57ms
step:1570/2000 train_time:84143ms step_avg:53.59ms
step:1571/2000 train_time:84232ms step_avg:53.62ms
step:1572/2000 train_time:84320ms step_avg:53.64ms
step:1573/2000 train_time:84408ms step_avg:53.66ms
step:1574/2000 train_time:84496ms step_avg:53.68ms
step:1575/2000 train_time:84585ms step_avg:53.70ms
step:1576/2000 train_time:84672ms step_avg:53.73ms
step:1577/2000 train_time:84760ms step_avg:53.75ms
step:1578/2000 train_time:84848ms step_avg:53.77ms
step:1579/2000 train_time:84936ms step_avg:53.79ms
step:1580/2000 train_time:85023ms step_avg:53.81ms
step:1581/2000 train_time:85112ms step_avg:53.83ms
step:1582/2000 train_time:85200ms step_avg:53.86ms
step:1583/2000 train_time:85288ms step_avg:53.88ms
step:1584/2000 train_time:85376ms step_avg:53.90ms
step:1585/2000 train_time:85464ms step_avg:53.92ms
step:1586/2000 train_time:85552ms step_avg:53.94ms
step:1587/2000 train_time:85640ms step_avg:53.96ms
step:1588/2000 train_time:85728ms step_avg:53.99ms
step:1589/2000 train_time:85816ms step_avg:54.01ms
step:1590/2000 train_time:85904ms step_avg:54.03ms
step:1591/2000 train_time:85992ms step_avg:54.05ms
step:1592/2000 train_time:86080ms step_avg:54.07ms
step:1593/2000 train_time:86169ms step_avg:54.09ms
step:1594/2000 train_time:86257ms step_avg:54.11ms
step:1595/2000 train_time:86345ms step_avg:54.13ms
step:1596/2000 train_time:86433ms step_avg:54.16ms
step:1597/2000 train_time:86521ms step_avg:54.18ms
step:1598/2000 train_time:86609ms step_avg:54.20ms
step:1599/2000 train_time:86697ms step_avg:54.22ms
step:1600/2000 train_time:86785ms step_avg:54.24ms
step:1601/2000 train_time:86872ms step_avg:54.26ms
step:1602/2000 train_time:86960ms step_avg:54.28ms
step:1603/2000 train_time:87049ms step_avg:54.30ms
step:1604/2000 train_time:87136ms step_avg:54.32ms
step:1605/2000 train_time:87224ms step_avg:54.35ms
step:1606/2000 train_time:87311ms step_avg:54.37ms
step:1607/2000 train_time:87399ms step_avg:54.39ms
step:1608/2000 train_time:87487ms step_avg:54.41ms
step:1609/2000 train_time:87575ms step_avg:54.43ms
step:1610/2000 train_time:87663ms step_avg:54.45ms
step:1611/2000 train_time:87751ms step_avg:54.47ms
step:1612/2000 train_time:87838ms step_avg:54.49ms
step:1613/2000 train_time:87926ms step_avg:54.51ms
step:1614/2000 train_time:88013ms step_avg:54.53ms
step:1615/2000 train_time:88102ms step_avg:54.55ms
step:1616/2000 train_time:88191ms step_avg:54.57ms
step:1617/2000 train_time:88279ms step_avg:54.59ms
step:1618/2000 train_time:88368ms step_avg:54.62ms
step:1619/2000 train_time:88456ms step_avg:54.64ms
step:1620/2000 train_time:88544ms step_avg:54.66ms
step:1621/2000 train_time:88633ms step_avg:54.68ms
step:1622/2000 train_time:88722ms step_avg:54.70ms
step:1623/2000 train_time:88810ms step_avg:54.72ms
step:1624/2000 train_time:88897ms step_avg:54.74ms
step:1625/2000 train_time:88986ms step_avg:54.76ms
step:1626/2000 train_time:89073ms step_avg:54.78ms
step:1627/2000 train_time:89161ms step_avg:54.80ms
step:1628/2000 train_time:89250ms step_avg:54.82ms
step:1629/2000 train_time:89339ms step_avg:54.84ms
step:1630/2000 train_time:89427ms step_avg:54.86ms
step:1631/2000 train_time:89515ms step_avg:54.88ms
step:1632/2000 train_time:89603ms step_avg:54.90ms
step:1633/2000 train_time:89692ms step_avg:54.92ms
step:1634/2000 train_time:89779ms step_avg:54.94ms
step:1635/2000 train_time:89868ms step_avg:54.96ms
step:1636/2000 train_time:89955ms step_avg:54.98ms
step:1637/2000 train_time:90044ms step_avg:55.01ms
step:1638/2000 train_time:90131ms step_avg:55.03ms
step:1639/2000 train_time:90219ms step_avg:55.05ms
step:1640/2000 train_time:90308ms step_avg:55.07ms
step:1641/2000 train_time:90396ms step_avg:55.09ms
step:1642/2000 train_time:90484ms step_avg:55.11ms
step:1643/2000 train_time:90574ms step_avg:55.13ms
step:1644/2000 train_time:90662ms step_avg:55.15ms
step:1645/2000 train_time:90750ms step_avg:55.17ms
step:1646/2000 train_time:90838ms step_avg:55.19ms
step:1647/2000 train_time:90927ms step_avg:55.21ms
step:1648/2000 train_time:91015ms step_avg:55.23ms
step:1649/2000 train_time:91102ms step_avg:55.25ms
step:1650/2000 train_time:91190ms step_avg:55.27ms
step:1651/2000 train_time:91279ms step_avg:55.29ms
step:1652/2000 train_time:91368ms step_avg:55.31ms
step:1653/2000 train_time:91456ms step_avg:55.33ms
step:1654/2000 train_time:91544ms step_avg:55.35ms
step:1655/2000 train_time:91632ms step_avg:55.37ms
step:1656/2000 train_time:91720ms step_avg:55.39ms
step:1657/2000 train_time:91808ms step_avg:55.41ms
step:1658/2000 train_time:91896ms step_avg:55.43ms
step:1659/2000 train_time:91984ms step_avg:55.45ms
step:1660/2000 train_time:92072ms step_avg:55.46ms
step:1661/2000 train_time:92160ms step_avg:55.48ms
step:1662/2000 train_time:92248ms step_avg:55.50ms
step:1663/2000 train_time:92336ms step_avg:55.52ms
step:1664/2000 train_time:92424ms step_avg:55.54ms
step:1665/2000 train_time:92513ms step_avg:55.56ms
step:1666/2000 train_time:92600ms step_avg:55.58ms
step:1667/2000 train_time:92689ms step_avg:55.60ms
step:1668/2000 train_time:92776ms step_avg:55.62ms
step:1669/2000 train_time:92865ms step_avg:55.64ms
step:1670/2000 train_time:92953ms step_avg:55.66ms
step:1671/2000 train_time:93040ms step_avg:55.68ms
step:1672/2000 train_time:93128ms step_avg:55.70ms
step:1673/2000 train_time:93216ms step_avg:55.72ms
step:1674/2000 train_time:93305ms step_avg:55.74ms
step:1675/2000 train_time:93393ms step_avg:55.76ms
step:1676/2000 train_time:93480ms step_avg:55.78ms
step:1677/2000 train_time:93569ms step_avg:55.80ms
step:1678/2000 train_time:93657ms step_avg:55.81ms
step:1679/2000 train_time:93744ms step_avg:55.83ms
step:1680/2000 train_time:93833ms step_avg:55.85ms
step:1681/2000 train_time:93920ms step_avg:55.87ms
step:1682/2000 train_time:94008ms step_avg:55.89ms
step:1683/2000 train_time:94096ms step_avg:55.91ms
step:1684/2000 train_time:94184ms step_avg:55.93ms
step:1685/2000 train_time:94273ms step_avg:55.95ms
step:1686/2000 train_time:94362ms step_avg:55.97ms
step:1687/2000 train_time:94451ms step_avg:55.99ms
step:1688/2000 train_time:94539ms step_avg:56.01ms
step:1689/2000 train_time:94627ms step_avg:56.03ms
step:1690/2000 train_time:94714ms step_avg:56.04ms
step:1691/2000 train_time:94802ms step_avg:56.06ms
step:1692/2000 train_time:94890ms step_avg:56.08ms
step:1693/2000 train_time:94978ms step_avg:56.10ms
step:1694/2000 train_time:95067ms step_avg:56.12ms
step:1695/2000 train_time:95155ms step_avg:56.14ms
step:1696/2000 train_time:95244ms step_avg:56.16ms
step:1697/2000 train_time:95332ms step_avg:56.18ms
step:1698/2000 train_time:95420ms step_avg:56.20ms
step:1699/2000 train_time:95509ms step_avg:56.21ms
step:1700/2000 train_time:95596ms step_avg:56.23ms
step:1701/2000 train_time:95685ms step_avg:56.25ms
step:1702/2000 train_time:95773ms step_avg:56.27ms
step:1703/2000 train_time:95862ms step_avg:56.29ms
step:1704/2000 train_time:95951ms step_avg:56.31ms
step:1705/2000 train_time:96039ms step_avg:56.33ms
step:1706/2000 train_time:96127ms step_avg:56.35ms
step:1707/2000 train_time:96215ms step_avg:56.37ms
step:1708/2000 train_time:96304ms step_avg:56.38ms
step:1709/2000 train_time:96393ms step_avg:56.40ms
step:1710/2000 train_time:96481ms step_avg:56.42ms
step:1711/2000 train_time:96569ms step_avg:56.44ms
step:1712/2000 train_time:96658ms step_avg:56.46ms
step:1713/2000 train_time:96746ms step_avg:56.48ms
step:1714/2000 train_time:96834ms step_avg:56.50ms
step:1715/2000 train_time:96923ms step_avg:56.51ms
step:1716/2000 train_time:97010ms step_avg:56.53ms
step:1717/2000 train_time:97098ms step_avg:56.55ms
step:1718/2000 train_time:97186ms step_avg:56.57ms
step:1719/2000 train_time:97274ms step_avg:56.59ms
step:1720/2000 train_time:97363ms step_avg:56.61ms
step:1721/2000 train_time:97451ms step_avg:56.62ms
step:1722/2000 train_time:97539ms step_avg:56.64ms
step:1723/2000 train_time:97627ms step_avg:56.66ms
step:1724/2000 train_time:97714ms step_avg:56.68ms
step:1725/2000 train_time:97802ms step_avg:56.70ms
step:1726/2000 train_time:97891ms step_avg:56.72ms
step:1727/2000 train_time:97979ms step_avg:56.73ms
step:1728/2000 train_time:98068ms step_avg:56.75ms
step:1729/2000 train_time:98156ms step_avg:56.77ms
step:1730/2000 train_time:98243ms step_avg:56.79ms
step:1731/2000 train_time:98331ms step_avg:56.81ms
step:1732/2000 train_time:98419ms step_avg:56.82ms
step:1733/2000 train_time:98508ms step_avg:56.84ms
step:1734/2000 train_time:98595ms step_avg:56.86ms
step:1735/2000 train_time:98684ms step_avg:56.88ms
step:1736/2000 train_time:98771ms step_avg:56.90ms
step:1737/2000 train_time:98859ms step_avg:56.91ms
step:1738/2000 train_time:98948ms step_avg:56.93ms
step:1739/2000 train_time:99035ms step_avg:56.95ms
step:1740/2000 train_time:99122ms step_avg:56.97ms
step:1741/2000 train_time:99211ms step_avg:56.99ms
step:1742/2000 train_time:99299ms step_avg:57.00ms
step:1743/2000 train_time:99387ms step_avg:57.02ms
step:1744/2000 train_time:99475ms step_avg:57.04ms
step:1745/2000 train_time:99563ms step_avg:57.06ms
step:1746/2000 train_time:99652ms step_avg:57.07ms
step:1747/2000 train_time:99740ms step_avg:57.09ms
step:1748/2000 train_time:99828ms step_avg:57.11ms
step:1749/2000 train_time:99916ms step_avg:57.13ms
step:1750/2000 train_time:100003ms step_avg:57.14ms
step:1750/2000 val_loss:3.3467 train_time:100094ms step_avg:57.20ms
step:1751/2000 train_time:100113ms step_avg:57.17ms
step:1752/2000 train_time:100184ms step_avg:57.18ms
step:1753/2000 train_time:100276ms step_avg:57.20ms
step:1754/2000 train_time:100365ms step_avg:57.22ms
step:1755/2000 train_time:100454ms step_avg:57.24ms
step:1756/2000 train_time:100540ms step_avg:57.26ms
step:1757/2000 train_time:100627ms step_avg:57.27ms
step:1758/2000 train_time:100713ms step_avg:57.29ms
step:1759/2000 train_time:100801ms step_avg:57.31ms
step:1760/2000 train_time:100889ms step_avg:57.32ms
step:1761/2000 train_time:100976ms step_avg:57.34ms
step:1762/2000 train_time:101066ms step_avg:57.36ms
step:1763/2000 train_time:101157ms step_avg:57.38ms
step:1764/2000 train_time:101247ms step_avg:57.40ms
step:1765/2000 train_time:101336ms step_avg:57.41ms
step:1766/2000 train_time:101425ms step_avg:57.43ms
step:1767/2000 train_time:101513ms step_avg:57.45ms
step:1768/2000 train_time:101601ms step_avg:57.47ms
step:1769/2000 train_time:101688ms step_avg:57.48ms
step:1770/2000 train_time:101775ms step_avg:57.50ms
step:1771/2000 train_time:101862ms step_avg:57.52ms
step:1772/2000 train_time:101949ms step_avg:57.53ms
step:1773/2000 train_time:102038ms step_avg:57.55ms
step:1774/2000 train_time:102127ms step_avg:57.57ms
step:1775/2000 train_time:102216ms step_avg:57.59ms
step:1776/2000 train_time:102305ms step_avg:57.60ms
step:1777/2000 train_time:102394ms step_avg:57.62ms
step:1778/2000 train_time:102483ms step_avg:57.64ms
step:1779/2000 train_time:102571ms step_avg:57.66ms
step:1780/2000 train_time:102658ms step_avg:57.67ms
step:1781/2000 train_time:102746ms step_avg:57.69ms
step:1782/2000 train_time:102833ms step_avg:57.71ms
step:1783/2000 train_time:102920ms step_avg:57.72ms
step:1784/2000 train_time:103009ms step_avg:57.74ms
step:1785/2000 train_time:103097ms step_avg:57.76ms
step:1786/2000 train_time:103186ms step_avg:57.77ms
step:1787/2000 train_time:103274ms step_avg:57.79ms
step:1788/2000 train_time:103362ms step_avg:57.81ms
step:1789/2000 train_time:103451ms step_avg:57.83ms
step:1790/2000 train_time:103539ms step_avg:57.84ms
step:1791/2000 train_time:103627ms step_avg:57.86ms
step:1792/2000 train_time:103714ms step_avg:57.88ms
step:1793/2000 train_time:103802ms step_avg:57.89ms
step:1794/2000 train_time:103889ms step_avg:57.91ms
step:1795/2000 train_time:103977ms step_avg:57.93ms
step:1796/2000 train_time:104065ms step_avg:57.94ms
step:1797/2000 train_time:104154ms step_avg:57.96ms
step:1798/2000 train_time:104242ms step_avg:57.98ms
step:1799/2000 train_time:104330ms step_avg:57.99ms
step:1800/2000 train_time:104418ms step_avg:58.01ms
step:1801/2000 train_time:104507ms step_avg:58.03ms
step:1802/2000 train_time:104595ms step_avg:58.04ms
step:1803/2000 train_time:104682ms step_avg:58.06ms
step:1804/2000 train_time:104770ms step_avg:58.08ms
step:1805/2000 train_time:104858ms step_avg:58.09ms
step:1806/2000 train_time:104945ms step_avg:58.11ms
step:1807/2000 train_time:105033ms step_avg:58.13ms
step:1808/2000 train_time:105121ms step_avg:58.14ms
step:1809/2000 train_time:105209ms step_avg:58.16ms
step:1810/2000 train_time:105297ms step_avg:58.18ms
step:1811/2000 train_time:105386ms step_avg:58.19ms
step:1812/2000 train_time:105474ms step_avg:58.21ms
step:1813/2000 train_time:105562ms step_avg:58.22ms
step:1814/2000 train_time:105651ms step_avg:58.24ms
step:1815/2000 train_time:105739ms step_avg:58.26ms
step:1816/2000 train_time:105826ms step_avg:58.27ms
step:1817/2000 train_time:105914ms step_avg:58.29ms
step:1818/2000 train_time:106002ms step_avg:58.31ms
step:1819/2000 train_time:106090ms step_avg:58.32ms
step:1820/2000 train_time:106177ms step_avg:58.34ms
step:1821/2000 train_time:106266ms step_avg:58.36ms
step:1822/2000 train_time:106355ms step_avg:58.37ms
step:1823/2000 train_time:106443ms step_avg:58.39ms
step:1824/2000 train_time:106531ms step_avg:58.41ms
step:1825/2000 train_time:106619ms step_avg:58.42ms
step:1826/2000 train_time:106707ms step_avg:58.44ms
step:1827/2000 train_time:106795ms step_avg:58.45ms
step:1828/2000 train_time:106884ms step_avg:58.47ms
step:1829/2000 train_time:106972ms step_avg:58.49ms
step:1830/2000 train_time:107060ms step_avg:58.50ms
step:1831/2000 train_time:107148ms step_avg:58.52ms
step:1832/2000 train_time:107235ms step_avg:58.53ms
step:1833/2000 train_time:107324ms step_avg:58.55ms
step:1834/2000 train_time:107412ms step_avg:58.57ms
step:1835/2000 train_time:107501ms step_avg:58.58ms
step:1836/2000 train_time:107589ms step_avg:58.60ms
step:1837/2000 train_time:107678ms step_avg:58.62ms
step:1838/2000 train_time:107765ms step_avg:58.63ms
step:1839/2000 train_time:107854ms step_avg:58.65ms
step:1840/2000 train_time:107942ms step_avg:58.66ms
step:1841/2000 train_time:108030ms step_avg:58.68ms
step:1842/2000 train_time:108117ms step_avg:58.70ms
step:1843/2000 train_time:108206ms step_avg:58.71ms
step:1844/2000 train_time:108294ms step_avg:58.73ms
step:1845/2000 train_time:108382ms step_avg:58.74ms
step:1846/2000 train_time:108470ms step_avg:58.76ms
step:1847/2000 train_time:108557ms step_avg:58.78ms
step:1848/2000 train_time:108646ms step_avg:58.79ms
step:1849/2000 train_time:108733ms step_avg:58.81ms
step:1850/2000 train_time:108821ms step_avg:58.82ms
step:1851/2000 train_time:108909ms step_avg:58.84ms
step:1852/2000 train_time:108996ms step_avg:58.85ms
step:1853/2000 train_time:109085ms step_avg:58.87ms
step:1854/2000 train_time:109172ms step_avg:58.88ms
step:1855/2000 train_time:109260ms step_avg:58.90ms
step:1856/2000 train_time:109348ms step_avg:58.92ms
step:1857/2000 train_time:109436ms step_avg:58.93ms
step:1858/2000 train_time:109525ms step_avg:58.95ms
step:1859/2000 train_time:109614ms step_avg:58.96ms
step:1860/2000 train_time:109702ms step_avg:58.98ms
step:1861/2000 train_time:109790ms step_avg:59.00ms
step:1862/2000 train_time:109878ms step_avg:59.01ms
step:1863/2000 train_time:109966ms step_avg:59.03ms
step:1864/2000 train_time:110053ms step_avg:59.04ms
step:1865/2000 train_time:110141ms step_avg:59.06ms
step:1866/2000 train_time:110230ms step_avg:59.07ms
step:1867/2000 train_time:110318ms step_avg:59.09ms
step:1868/2000 train_time:110406ms step_avg:59.10ms
step:1869/2000 train_time:110495ms step_avg:59.12ms
step:1870/2000 train_time:110583ms step_avg:59.14ms
step:1871/2000 train_time:110672ms step_avg:59.15ms
step:1872/2000 train_time:110760ms step_avg:59.17ms
step:1873/2000 train_time:110848ms step_avg:59.18ms
step:1874/2000 train_time:110935ms step_avg:59.20ms
step:1875/2000 train_time:111024ms step_avg:59.21ms
step:1876/2000 train_time:111111ms step_avg:59.23ms
step:1877/2000 train_time:111199ms step_avg:59.24ms
step:1878/2000 train_time:111287ms step_avg:59.26ms
step:1879/2000 train_time:111375ms step_avg:59.27ms
step:1880/2000 train_time:111463ms step_avg:59.29ms
step:1881/2000 train_time:111552ms step_avg:59.30ms
step:1882/2000 train_time:111639ms step_avg:59.32ms
step:1883/2000 train_time:111727ms step_avg:59.33ms
step:1884/2000 train_time:111815ms step_avg:59.35ms
step:1885/2000 train_time:111903ms step_avg:59.36ms
step:1886/2000 train_time:111992ms step_avg:59.38ms
step:1887/2000 train_time:112080ms step_avg:59.40ms
step:1888/2000 train_time:112168ms step_avg:59.41ms
step:1889/2000 train_time:112257ms step_avg:59.43ms
step:1890/2000 train_time:112344ms step_avg:59.44ms
step:1891/2000 train_time:112432ms step_avg:59.46ms
step:1892/2000 train_time:112520ms step_avg:59.47ms
step:1893/2000 train_time:112608ms step_avg:59.49ms
step:1894/2000 train_time:112696ms step_avg:59.50ms
step:1895/2000 train_time:112783ms step_avg:59.52ms
step:1896/2000 train_time:112871ms step_avg:59.53ms
step:1897/2000 train_time:112959ms step_avg:59.55ms
step:1898/2000 train_time:113046ms step_avg:59.56ms
step:1899/2000 train_time:113135ms step_avg:59.58ms
step:1900/2000 train_time:113224ms step_avg:59.59ms
step:1901/2000 train_time:113313ms step_avg:59.61ms
step:1902/2000 train_time:113401ms step_avg:59.62ms
step:1903/2000 train_time:113490ms step_avg:59.64ms
step:1904/2000 train_time:113577ms step_avg:59.65ms
step:1905/2000 train_time:113665ms step_avg:59.67ms
step:1906/2000 train_time:113753ms step_avg:59.68ms
step:1907/2000 train_time:113841ms step_avg:59.70ms
step:1908/2000 train_time:113930ms step_avg:59.71ms
step:1909/2000 train_time:114017ms step_avg:59.73ms
step:1910/2000 train_time:114105ms step_avg:59.74ms
step:1911/2000 train_time:114194ms step_avg:59.76ms
step:1912/2000 train_time:114281ms step_avg:59.77ms
step:1913/2000 train_time:114369ms step_avg:59.79ms
step:1914/2000 train_time:114457ms step_avg:59.80ms
step:1915/2000 train_time:114544ms step_avg:59.81ms
step:1916/2000 train_time:114633ms step_avg:59.83ms
step:1917/2000 train_time:114721ms step_avg:59.84ms
step:1918/2000 train_time:114809ms step_avg:59.86ms
step:1919/2000 train_time:114897ms step_avg:59.87ms
step:1920/2000 train_time:114985ms step_avg:59.89ms
step:1921/2000 train_time:115073ms step_avg:59.90ms
step:1922/2000 train_time:115162ms step_avg:59.92ms
step:1923/2000 train_time:115250ms step_avg:59.93ms
step:1924/2000 train_time:115337ms step_avg:59.95ms
step:1925/2000 train_time:115425ms step_avg:59.96ms
step:1926/2000 train_time:115513ms step_avg:59.98ms
step:1927/2000 train_time:115600ms step_avg:59.99ms
step:1928/2000 train_time:115689ms step_avg:60.00ms
step:1929/2000 train_time:115777ms step_avg:60.02ms
step:1930/2000 train_time:115865ms step_avg:60.03ms
step:1931/2000 train_time:115953ms step_avg:60.05ms
step:1932/2000 train_time:116042ms step_avg:60.06ms
step:1933/2000 train_time:116131ms step_avg:60.08ms
step:1934/2000 train_time:116219ms step_avg:60.09ms
step:1935/2000 train_time:116308ms step_avg:60.11ms
step:1936/2000 train_time:116395ms step_avg:60.12ms
step:1937/2000 train_time:116483ms step_avg:60.14ms
step:1938/2000 train_time:116572ms step_avg:60.15ms
step:1939/2000 train_time:116659ms step_avg:60.16ms
step:1940/2000 train_time:116747ms step_avg:60.18ms
step:1941/2000 train_time:116835ms step_avg:60.19ms
step:1942/2000 train_time:116923ms step_avg:60.21ms
step:1943/2000 train_time:117012ms step_avg:60.22ms
step:1944/2000 train_time:117101ms step_avg:60.24ms
step:1945/2000 train_time:117189ms step_avg:60.25ms
step:1946/2000 train_time:117277ms step_avg:60.27ms
step:1947/2000 train_time:117365ms step_avg:60.28ms
step:1948/2000 train_time:117453ms step_avg:60.29ms
step:1949/2000 train_time:117542ms step_avg:60.31ms
step:1950/2000 train_time:117630ms step_avg:60.32ms
step:1951/2000 train_time:117718ms step_avg:60.34ms
step:1952/2000 train_time:117806ms step_avg:60.35ms
step:1953/2000 train_time:117894ms step_avg:60.37ms
step:1954/2000 train_time:117982ms step_avg:60.38ms
step:1955/2000 train_time:118070ms step_avg:60.39ms
step:1956/2000 train_time:118158ms step_avg:60.41ms
step:1957/2000 train_time:118247ms step_avg:60.42ms
step:1958/2000 train_time:118335ms step_avg:60.44ms
step:1959/2000 train_time:118422ms step_avg:60.45ms
step:1960/2000 train_time:118511ms step_avg:60.46ms
step:1961/2000 train_time:118599ms step_avg:60.48ms
step:1962/2000 train_time:118687ms step_avg:60.49ms
step:1963/2000 train_time:118776ms step_avg:60.51ms
step:1964/2000 train_time:118863ms step_avg:60.52ms
step:1965/2000 train_time:118953ms step_avg:60.54ms
step:1966/2000 train_time:119042ms step_avg:60.55ms
step:1967/2000 train_time:119132ms step_avg:60.57ms
step:1968/2000 train_time:119221ms step_avg:60.58ms
step:1969/2000 train_time:119311ms step_avg:60.59ms
step:1970/2000 train_time:119398ms step_avg:60.61ms
step:1971/2000 train_time:119488ms step_avg:60.62ms
step:1972/2000 train_time:119576ms step_avg:60.64ms
step:1973/2000 train_time:119664ms step_avg:60.65ms
step:1974/2000 train_time:119753ms step_avg:60.67ms
step:1975/2000 train_time:119841ms step_avg:60.68ms
step:1976/2000 train_time:119929ms step_avg:60.69ms
step:1977/2000 train_time:120017ms step_avg:60.71ms
step:1978/2000 train_time:120105ms step_avg:60.72ms
step:1979/2000 train_time:120194ms step_avg:60.73ms
step:1980/2000 train_time:120283ms step_avg:60.75ms
step:1981/2000 train_time:120372ms step_avg:60.76ms
step:1982/2000 train_time:120460ms step_avg:60.78ms
step:1983/2000 train_time:120548ms step_avg:60.79ms
step:1984/2000 train_time:120636ms step_avg:60.80ms
step:1985/2000 train_time:120725ms step_avg:60.82ms
step:1986/2000 train_time:120813ms step_avg:60.83ms
step:1987/2000 train_time:120901ms step_avg:60.85ms
step:1988/2000 train_time:120989ms step_avg:60.86ms
step:1989/2000 train_time:121078ms step_avg:60.87ms
step:1990/2000 train_time:121166ms step_avg:60.89ms
step:1991/2000 train_time:121255ms step_avg:60.90ms
step:1992/2000 train_time:121344ms step_avg:60.92ms
step:1993/2000 train_time:121433ms step_avg:60.93ms
step:1994/2000 train_time:121521ms step_avg:60.94ms
step:1995/2000 train_time:121610ms step_avg:60.96ms
step:1996/2000 train_time:121697ms step_avg:60.97ms
step:1997/2000 train_time:121786ms step_avg:60.98ms
step:1998/2000 train_time:121874ms step_avg:61.00ms
step:1999/2000 train_time:121961ms step_avg:61.01ms
step:2000/2000 train_time:122049ms step_avg:61.02ms
step:2000/2000 val_loss:3.2782 train_time:122140ms step_avg:61.07ms
peak memory allocated: 29512 MiB reserved: 35256 MiB

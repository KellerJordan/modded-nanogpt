import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        #ve = [None, ve[0], ve[1]] + [None] * (len(self.blocks) - 5) + [ve[0], ve[1]]
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2380  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.4  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further
    momentum_cd_steps = 50


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.7, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps  # More explicit denominator

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # step the optimizers
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 21:10:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                  Off |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                  Off |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                  Off |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                  Off |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                  Off |
| N/A   34C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                  Off |
| N/A   41C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                  Off |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                  Off |
| N/A   36C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          238082      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          238083      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          238084      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          238085      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          238086      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          238087      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          238088      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          238089      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          238083      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          238084      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          238085      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          238086      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          238087      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          238088      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          238089      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2420 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2420 train_time:94ms step_avg:93.82ms
step:2/2420 train_time:193ms step_avg:96.27ms
step:3/2420 train_time:215ms step_avg:71.65ms
step:4/2420 train_time:251ms step_avg:62.73ms
step:5/2420 train_time:308ms step_avg:61.56ms
step:6/2420 train_time:370ms step_avg:61.73ms
step:7/2420 train_time:430ms step_avg:61.37ms
step:8/2420 train_time:490ms step_avg:61.19ms
step:9/2420 train_time:548ms step_avg:60.83ms
step:10/2420 train_time:608ms step_avg:60.76ms
step:11/2420 train_time:666ms step_avg:60.50ms
step:12/2420 train_time:726ms step_avg:60.51ms
step:13/2420 train_time:784ms step_avg:60.33ms
step:14/2420 train_time:845ms step_avg:60.32ms
step:15/2420 train_time:903ms step_avg:60.18ms
step:16/2420 train_time:963ms step_avg:60.16ms
step:17/2420 train_time:1023ms step_avg:60.17ms
step:18/2420 train_time:1085ms step_avg:60.30ms
step:19/2420 train_time:1147ms step_avg:60.38ms
step:20/2420 train_time:1210ms step_avg:60.48ms
step:21/2420 train_time:1271ms step_avg:60.51ms
step:22/2420 train_time:1332ms step_avg:60.54ms
step:23/2420 train_time:1391ms step_avg:60.48ms
step:24/2420 train_time:1452ms step_avg:60.49ms
step:25/2420 train_time:1510ms step_avg:60.42ms
step:26/2420 train_time:1571ms step_avg:60.42ms
step:27/2420 train_time:1630ms step_avg:60.37ms
step:28/2420 train_time:1691ms step_avg:60.40ms
step:29/2420 train_time:1750ms step_avg:60.34ms
step:30/2420 train_time:1811ms step_avg:60.36ms
step:31/2420 train_time:1870ms step_avg:60.31ms
step:32/2420 train_time:1930ms step_avg:60.32ms
step:33/2420 train_time:1989ms step_avg:60.28ms
step:34/2420 train_time:2051ms step_avg:60.33ms
step:35/2420 train_time:2111ms step_avg:60.32ms
step:36/2420 train_time:2173ms step_avg:60.37ms
step:37/2420 train_time:2233ms step_avg:60.35ms
step:38/2420 train_time:2295ms step_avg:60.38ms
step:39/2420 train_time:2354ms step_avg:60.37ms
step:40/2420 train_time:2415ms step_avg:60.38ms
step:41/2420 train_time:2475ms step_avg:60.36ms
step:42/2420 train_time:2536ms step_avg:60.39ms
step:43/2420 train_time:2595ms step_avg:60.35ms
step:44/2420 train_time:2656ms step_avg:60.37ms
step:45/2420 train_time:2716ms step_avg:60.34ms
step:46/2420 train_time:2777ms step_avg:60.37ms
step:47/2420 train_time:2836ms step_avg:60.35ms
step:48/2420 train_time:2898ms step_avg:60.37ms
step:49/2420 train_time:2958ms step_avg:60.36ms
step:50/2420 train_time:3018ms step_avg:60.37ms
step:51/2420 train_time:3078ms step_avg:60.35ms
step:52/2420 train_time:3140ms step_avg:60.38ms
step:53/2420 train_time:3198ms step_avg:60.34ms
step:54/2420 train_time:3259ms step_avg:60.36ms
step:55/2420 train_time:3319ms step_avg:60.34ms
step:56/2420 train_time:3379ms step_avg:60.34ms
step:57/2420 train_time:3438ms step_avg:60.32ms
step:58/2420 train_time:3499ms step_avg:60.33ms
step:59/2420 train_time:3558ms step_avg:60.30ms
step:60/2420 train_time:3618ms step_avg:60.30ms
step:61/2420 train_time:3677ms step_avg:60.28ms
step:62/2420 train_time:3738ms step_avg:60.29ms
step:63/2420 train_time:3797ms step_avg:60.27ms
step:64/2420 train_time:3859ms step_avg:60.29ms
step:65/2420 train_time:3918ms step_avg:60.28ms
step:66/2420 train_time:3979ms step_avg:60.29ms
step:67/2420 train_time:4039ms step_avg:60.28ms
step:68/2420 train_time:4100ms step_avg:60.29ms
step:69/2420 train_time:4159ms step_avg:60.27ms
step:70/2420 train_time:4219ms step_avg:60.28ms
step:71/2420 train_time:4279ms step_avg:60.26ms
step:72/2420 train_time:4341ms step_avg:60.29ms
step:73/2420 train_time:4400ms step_avg:60.27ms
step:74/2420 train_time:4461ms step_avg:60.28ms
step:75/2420 train_time:4520ms step_avg:60.26ms
step:76/2420 train_time:4580ms step_avg:60.26ms
step:77/2420 train_time:4639ms step_avg:60.25ms
step:78/2420 train_time:4700ms step_avg:60.26ms
step:79/2420 train_time:4759ms step_avg:60.24ms
step:80/2420 train_time:4819ms step_avg:60.24ms
step:81/2420 train_time:4879ms step_avg:60.23ms
step:82/2420 train_time:4941ms step_avg:60.26ms
step:83/2420 train_time:4999ms step_avg:60.23ms
step:84/2420 train_time:5060ms step_avg:60.24ms
step:85/2420 train_time:5120ms step_avg:60.23ms
step:86/2420 train_time:5181ms step_avg:60.24ms
step:87/2420 train_time:5240ms step_avg:60.23ms
step:88/2420 train_time:5301ms step_avg:60.23ms
step:89/2420 train_time:5360ms step_avg:60.22ms
step:90/2420 train_time:5420ms step_avg:60.23ms
step:91/2420 train_time:5480ms step_avg:60.22ms
step:92/2420 train_time:5541ms step_avg:60.23ms
step:93/2420 train_time:5600ms step_avg:60.21ms
step:94/2420 train_time:5661ms step_avg:60.22ms
step:95/2420 train_time:5719ms step_avg:60.20ms
step:96/2420 train_time:5780ms step_avg:60.21ms
step:97/2420 train_time:5839ms step_avg:60.19ms
step:98/2420 train_time:5899ms step_avg:60.20ms
step:99/2420 train_time:5959ms step_avg:60.19ms
step:100/2420 train_time:6019ms step_avg:60.19ms
step:101/2420 train_time:6079ms step_avg:60.18ms
step:102/2420 train_time:6140ms step_avg:60.20ms
step:103/2420 train_time:6199ms step_avg:60.18ms
step:104/2420 train_time:6259ms step_avg:60.19ms
step:105/2420 train_time:6318ms step_avg:60.17ms
step:106/2420 train_time:6379ms step_avg:60.18ms
step:107/2420 train_time:6438ms step_avg:60.17ms
step:108/2420 train_time:6499ms step_avg:60.18ms
step:109/2420 train_time:6558ms step_avg:60.16ms
step:110/2420 train_time:6618ms step_avg:60.17ms
step:111/2420 train_time:6678ms step_avg:60.16ms
step:112/2420 train_time:6739ms step_avg:60.17ms
step:113/2420 train_time:6798ms step_avg:60.16ms
step:114/2420 train_time:6859ms step_avg:60.17ms
step:115/2420 train_time:6918ms step_avg:60.15ms
step:116/2420 train_time:6978ms step_avg:60.16ms
step:117/2420 train_time:7039ms step_avg:60.16ms
step:118/2420 train_time:7101ms step_avg:60.18ms
step:119/2420 train_time:7158ms step_avg:60.15ms
step:120/2420 train_time:7219ms step_avg:60.16ms
step:121/2420 train_time:7277ms step_avg:60.14ms
step:122/2420 train_time:7338ms step_avg:60.15ms
step:123/2420 train_time:7397ms step_avg:60.14ms
step:124/2420 train_time:7458ms step_avg:60.14ms
step:125/2420 train_time:7516ms step_avg:60.13ms
step:126/2420 train_time:7578ms step_avg:60.14ms
step:127/2420 train_time:7638ms step_avg:60.14ms
step:128/2420 train_time:7698ms step_avg:60.14ms
step:129/2420 train_time:7757ms step_avg:60.13ms
step:130/2420 train_time:7818ms step_avg:60.14ms
step:131/2420 train_time:7877ms step_avg:60.13ms
step:132/2420 train_time:7939ms step_avg:60.14ms
step:133/2420 train_time:7998ms step_avg:60.14ms
step:134/2420 train_time:8059ms step_avg:60.14ms
step:135/2420 train_time:8117ms step_avg:60.13ms
step:136/2420 train_time:8178ms step_avg:60.14ms
step:137/2420 train_time:8237ms step_avg:60.13ms
step:138/2420 train_time:8298ms step_avg:60.13ms
step:139/2420 train_time:8356ms step_avg:60.12ms
step:140/2420 train_time:8417ms step_avg:60.12ms
step:141/2420 train_time:8476ms step_avg:60.12ms
step:142/2420 train_time:8537ms step_avg:60.12ms
step:143/2420 train_time:8597ms step_avg:60.12ms
step:144/2420 train_time:8658ms step_avg:60.13ms
step:145/2420 train_time:8717ms step_avg:60.12ms
step:146/2420 train_time:8777ms step_avg:60.12ms
step:147/2420 train_time:8837ms step_avg:60.12ms
step:148/2420 train_time:8898ms step_avg:60.12ms
step:149/2420 train_time:8956ms step_avg:60.11ms
step:150/2420 train_time:9017ms step_avg:60.11ms
step:151/2420 train_time:9077ms step_avg:60.11ms
step:152/2420 train_time:9139ms step_avg:60.12ms
step:153/2420 train_time:9197ms step_avg:60.11ms
step:154/2420 train_time:9258ms step_avg:60.12ms
step:155/2420 train_time:9316ms step_avg:60.11ms
step:156/2420 train_time:9377ms step_avg:60.11ms
step:157/2420 train_time:9436ms step_avg:60.10ms
step:158/2420 train_time:9497ms step_avg:60.11ms
step:159/2420 train_time:9556ms step_avg:60.10ms
step:160/2420 train_time:9617ms step_avg:60.10ms
step:161/2420 train_time:9675ms step_avg:60.09ms
step:162/2420 train_time:9736ms step_avg:60.10ms
step:163/2420 train_time:9795ms step_avg:60.09ms
step:164/2420 train_time:9856ms step_avg:60.10ms
step:165/2420 train_time:9915ms step_avg:60.09ms
step:166/2420 train_time:9976ms step_avg:60.09ms
step:167/2420 train_time:10034ms step_avg:60.09ms
step:168/2420 train_time:10096ms step_avg:60.09ms
step:169/2420 train_time:10155ms step_avg:60.09ms
step:170/2420 train_time:10216ms step_avg:60.09ms
step:171/2420 train_time:10274ms step_avg:60.08ms
step:172/2420 train_time:10335ms step_avg:60.09ms
step:173/2420 train_time:10394ms step_avg:60.08ms
step:174/2420 train_time:10455ms step_avg:60.09ms
step:175/2420 train_time:10514ms step_avg:60.08ms
step:176/2420 train_time:10575ms step_avg:60.09ms
step:177/2420 train_time:10635ms step_avg:60.08ms
step:178/2420 train_time:10696ms step_avg:60.09ms
step:179/2420 train_time:10755ms step_avg:60.08ms
step:180/2420 train_time:10815ms step_avg:60.08ms
step:181/2420 train_time:10874ms step_avg:60.08ms
step:182/2420 train_time:10935ms step_avg:60.08ms
step:183/2420 train_time:10994ms step_avg:60.08ms
step:184/2420 train_time:11055ms step_avg:60.08ms
step:185/2420 train_time:11114ms step_avg:60.07ms
step:186/2420 train_time:11175ms step_avg:60.08ms
step:187/2420 train_time:11233ms step_avg:60.07ms
step:188/2420 train_time:11294ms step_avg:60.08ms
step:189/2420 train_time:11353ms step_avg:60.07ms
step:190/2420 train_time:11414ms step_avg:60.07ms
step:191/2420 train_time:11472ms step_avg:60.07ms
step:192/2420 train_time:11533ms step_avg:60.07ms
step:193/2420 train_time:11592ms step_avg:60.06ms
step:194/2420 train_time:11653ms step_avg:60.07ms
step:195/2420 train_time:11712ms step_avg:60.06ms
step:196/2420 train_time:11772ms step_avg:60.06ms
step:197/2420 train_time:11831ms step_avg:60.05ms
step:198/2420 train_time:11891ms step_avg:60.06ms
step:199/2420 train_time:11950ms step_avg:60.05ms
step:200/2420 train_time:12011ms step_avg:60.06ms
step:201/2420 train_time:12070ms step_avg:60.05ms
step:202/2420 train_time:12130ms step_avg:60.05ms
step:203/2420 train_time:12188ms step_avg:60.04ms
step:204/2420 train_time:12249ms step_avg:60.04ms
step:205/2420 train_time:12307ms step_avg:60.04ms
step:206/2420 train_time:12368ms step_avg:60.04ms
step:207/2420 train_time:12427ms step_avg:60.03ms
step:208/2420 train_time:12488ms step_avg:60.04ms
step:209/2420 train_time:12547ms step_avg:60.03ms
step:210/2420 train_time:12607ms step_avg:60.03ms
step:211/2420 train_time:12666ms step_avg:60.03ms
step:212/2420 train_time:12727ms step_avg:60.03ms
step:213/2420 train_time:12786ms step_avg:60.03ms
step:214/2420 train_time:12846ms step_avg:60.03ms
step:215/2420 train_time:12905ms step_avg:60.02ms
step:216/2420 train_time:12966ms step_avg:60.03ms
step:217/2420 train_time:13024ms step_avg:60.02ms
step:218/2420 train_time:13084ms step_avg:60.02ms
step:219/2420 train_time:13142ms step_avg:60.01ms
step:220/2420 train_time:13202ms step_avg:60.01ms
step:221/2420 train_time:13261ms step_avg:60.00ms
step:222/2420 train_time:13321ms step_avg:60.00ms
step:223/2420 train_time:13380ms step_avg:60.00ms
step:224/2420 train_time:13441ms step_avg:60.00ms
step:225/2420 train_time:13500ms step_avg:60.00ms
step:226/2420 train_time:13561ms step_avg:60.01ms
step:227/2420 train_time:13620ms step_avg:60.00ms
step:228/2420 train_time:13681ms step_avg:60.00ms
step:229/2420 train_time:13740ms step_avg:60.00ms
step:230/2420 train_time:13801ms step_avg:60.00ms
step:231/2420 train_time:13860ms step_avg:60.00ms
step:232/2420 train_time:13921ms step_avg:60.00ms
step:233/2420 train_time:13979ms step_avg:60.00ms
step:234/2420 train_time:14040ms step_avg:60.00ms
step:235/2420 train_time:14099ms step_avg:60.00ms
step:236/2420 train_time:14159ms step_avg:60.00ms
step:237/2420 train_time:14218ms step_avg:59.99ms
step:238/2420 train_time:14278ms step_avg:59.99ms
step:239/2420 train_time:14338ms step_avg:59.99ms
step:240/2420 train_time:14399ms step_avg:59.99ms
step:241/2420 train_time:14458ms step_avg:59.99ms
step:242/2420 train_time:14518ms step_avg:59.99ms
step:243/2420 train_time:14577ms step_avg:59.99ms
step:244/2420 train_time:14638ms step_avg:59.99ms
step:245/2420 train_time:14697ms step_avg:59.99ms
step:246/2420 train_time:14758ms step_avg:59.99ms
step:247/2420 train_time:14817ms step_avg:59.99ms
step:248/2420 train_time:14877ms step_avg:59.99ms
step:249/2420 train_time:14938ms step_avg:59.99ms
step:250/2420 train_time:14998ms step_avg:59.99ms
step:250/2420 val_loss:4.0953 train_time:15062ms step_avg:60.25ms
step:251/2420 train_time:15083ms step_avg:60.09ms
step:252/2420 train_time:15120ms step_avg:60.00ms
step:253/2420 train_time:15181ms step_avg:60.01ms
step:254/2420 train_time:15246ms step_avg:60.02ms
step:255/2420 train_time:15308ms step_avg:60.03ms
step:256/2420 train_time:15369ms step_avg:60.03ms
step:257/2420 train_time:15427ms step_avg:60.03ms
step:258/2420 train_time:15487ms step_avg:60.03ms
step:259/2420 train_time:15545ms step_avg:60.02ms
step:260/2420 train_time:15605ms step_avg:60.02ms
step:261/2420 train_time:15662ms step_avg:60.01ms
step:262/2420 train_time:15722ms step_avg:60.01ms
step:263/2420 train_time:15780ms step_avg:60.00ms
step:264/2420 train_time:15839ms step_avg:60.00ms
step:265/2420 train_time:15897ms step_avg:59.99ms
step:266/2420 train_time:15956ms step_avg:59.99ms
step:267/2420 train_time:16014ms step_avg:59.98ms
step:268/2420 train_time:16075ms step_avg:59.98ms
step:269/2420 train_time:16135ms step_avg:59.98ms
step:270/2420 train_time:16196ms step_avg:59.99ms
step:271/2420 train_time:16256ms step_avg:59.99ms
step:272/2420 train_time:16317ms step_avg:59.99ms
step:273/2420 train_time:16377ms step_avg:59.99ms
step:274/2420 train_time:16438ms step_avg:59.99ms
step:275/2420 train_time:16496ms step_avg:59.99ms
step:276/2420 train_time:16557ms step_avg:59.99ms
step:277/2420 train_time:16615ms step_avg:59.98ms
step:278/2420 train_time:16676ms step_avg:59.98ms
step:279/2420 train_time:16734ms step_avg:59.98ms
step:280/2420 train_time:16794ms step_avg:59.98ms
step:281/2420 train_time:16852ms step_avg:59.97ms
step:282/2420 train_time:16912ms step_avg:59.97ms
step:283/2420 train_time:16970ms step_avg:59.97ms
step:284/2420 train_time:17031ms step_avg:59.97ms
step:285/2420 train_time:17089ms step_avg:59.96ms
step:286/2420 train_time:17150ms step_avg:59.97ms
step:287/2420 train_time:17210ms step_avg:59.96ms
step:288/2420 train_time:17272ms step_avg:59.97ms
step:289/2420 train_time:17331ms step_avg:59.97ms
step:290/2420 train_time:17392ms step_avg:59.97ms
step:291/2420 train_time:17451ms step_avg:59.97ms
step:292/2420 train_time:17512ms step_avg:59.97ms
step:293/2420 train_time:17570ms step_avg:59.97ms
step:294/2420 train_time:17631ms step_avg:59.97ms
step:295/2420 train_time:17690ms step_avg:59.96ms
step:296/2420 train_time:17750ms step_avg:59.97ms
step:297/2420 train_time:17808ms step_avg:59.96ms
step:298/2420 train_time:17868ms step_avg:59.96ms
step:299/2420 train_time:17926ms step_avg:59.95ms
step:300/2420 train_time:17986ms step_avg:59.95ms
step:301/2420 train_time:18044ms step_avg:59.95ms
step:302/2420 train_time:18104ms step_avg:59.95ms
step:303/2420 train_time:18163ms step_avg:59.94ms
step:304/2420 train_time:18224ms step_avg:59.95ms
step:305/2420 train_time:18282ms step_avg:59.94ms
step:306/2420 train_time:18343ms step_avg:59.94ms
step:307/2420 train_time:18402ms step_avg:59.94ms
step:308/2420 train_time:18463ms step_avg:59.94ms
step:309/2420 train_time:18522ms step_avg:59.94ms
step:310/2420 train_time:18582ms step_avg:59.94ms
step:311/2420 train_time:18641ms step_avg:59.94ms
step:312/2420 train_time:18702ms step_avg:59.94ms
step:313/2420 train_time:18760ms step_avg:59.94ms
step:314/2420 train_time:18820ms step_avg:59.94ms
step:315/2420 train_time:18877ms step_avg:59.93ms
step:316/2420 train_time:18937ms step_avg:59.93ms
step:317/2420 train_time:18995ms step_avg:59.92ms
step:318/2420 train_time:19056ms step_avg:59.92ms
step:319/2420 train_time:19115ms step_avg:59.92ms
step:320/2420 train_time:19176ms step_avg:59.92ms
step:321/2420 train_time:19235ms step_avg:59.92ms
step:322/2420 train_time:19296ms step_avg:59.93ms
step:323/2420 train_time:19355ms step_avg:59.92ms
step:324/2420 train_time:19416ms step_avg:59.93ms
step:325/2420 train_time:19475ms step_avg:59.92ms
step:326/2420 train_time:19536ms step_avg:59.93ms
step:327/2420 train_time:19595ms step_avg:59.92ms
step:328/2420 train_time:19656ms step_avg:59.93ms
step:329/2420 train_time:19714ms step_avg:59.92ms
step:330/2420 train_time:19775ms step_avg:59.92ms
step:331/2420 train_time:19833ms step_avg:59.92ms
step:332/2420 train_time:19893ms step_avg:59.92ms
step:333/2420 train_time:19952ms step_avg:59.92ms
step:334/2420 train_time:20012ms step_avg:59.92ms
step:335/2420 train_time:20070ms step_avg:59.91ms
step:336/2420 train_time:20132ms step_avg:59.92ms
step:337/2420 train_time:20191ms step_avg:59.91ms
step:338/2420 train_time:20251ms step_avg:59.92ms
step:339/2420 train_time:20310ms step_avg:59.91ms
step:340/2420 train_time:20371ms step_avg:59.92ms
step:341/2420 train_time:20430ms step_avg:59.91ms
step:342/2420 train_time:20491ms step_avg:59.92ms
step:343/2420 train_time:20550ms step_avg:59.91ms
step:344/2420 train_time:20611ms step_avg:59.91ms
step:345/2420 train_time:20671ms step_avg:59.91ms
step:346/2420 train_time:20731ms step_avg:59.92ms
step:347/2420 train_time:20790ms step_avg:59.91ms
step:348/2420 train_time:20850ms step_avg:59.92ms
step:349/2420 train_time:20909ms step_avg:59.91ms
step:350/2420 train_time:20969ms step_avg:59.91ms
step:351/2420 train_time:21028ms step_avg:59.91ms
step:352/2420 train_time:21089ms step_avg:59.91ms
step:353/2420 train_time:21147ms step_avg:59.91ms
step:354/2420 train_time:21208ms step_avg:59.91ms
step:355/2420 train_time:21266ms step_avg:59.90ms
step:356/2420 train_time:21327ms step_avg:59.91ms
step:357/2420 train_time:21385ms step_avg:59.90ms
step:358/2420 train_time:21446ms step_avg:59.91ms
step:359/2420 train_time:21505ms step_avg:59.90ms
step:360/2420 train_time:21566ms step_avg:59.91ms
step:361/2420 train_time:21625ms step_avg:59.90ms
step:362/2420 train_time:21685ms step_avg:59.90ms
step:363/2420 train_time:21744ms step_avg:59.90ms
step:364/2420 train_time:21806ms step_avg:59.91ms
step:365/2420 train_time:21863ms step_avg:59.90ms
step:366/2420 train_time:21923ms step_avg:59.90ms
step:367/2420 train_time:21981ms step_avg:59.89ms
step:368/2420 train_time:22041ms step_avg:59.90ms
step:369/2420 train_time:22100ms step_avg:59.89ms
step:370/2420 train_time:22160ms step_avg:59.89ms
step:371/2420 train_time:22218ms step_avg:59.89ms
step:372/2420 train_time:22278ms step_avg:59.89ms
step:373/2420 train_time:22336ms step_avg:59.88ms
step:374/2420 train_time:22397ms step_avg:59.88ms
step:375/2420 train_time:22455ms step_avg:59.88ms
step:376/2420 train_time:22515ms step_avg:59.88ms
step:377/2420 train_time:22574ms step_avg:59.88ms
step:378/2420 train_time:22635ms step_avg:59.88ms
step:379/2420 train_time:22694ms step_avg:59.88ms
step:380/2420 train_time:22755ms step_avg:59.88ms
step:381/2420 train_time:22814ms step_avg:59.88ms
step:382/2420 train_time:22874ms step_avg:59.88ms
step:383/2420 train_time:22933ms step_avg:59.88ms
step:384/2420 train_time:22994ms step_avg:59.88ms
step:385/2420 train_time:23052ms step_avg:59.87ms
step:386/2420 train_time:23113ms step_avg:59.88ms
step:387/2420 train_time:23171ms step_avg:59.87ms
step:388/2420 train_time:23232ms step_avg:59.88ms
step:389/2420 train_time:23290ms step_avg:59.87ms
step:390/2420 train_time:23351ms step_avg:59.87ms
step:391/2420 train_time:23410ms step_avg:59.87ms
step:392/2420 train_time:23470ms step_avg:59.87ms
step:393/2420 train_time:23529ms step_avg:59.87ms
step:394/2420 train_time:23589ms step_avg:59.87ms
step:395/2420 train_time:23648ms step_avg:59.87ms
step:396/2420 train_time:23709ms step_avg:59.87ms
step:397/2420 train_time:23767ms step_avg:59.87ms
step:398/2420 train_time:23828ms step_avg:59.87ms
step:399/2420 train_time:23887ms step_avg:59.87ms
step:400/2420 train_time:23948ms step_avg:59.87ms
step:401/2420 train_time:24006ms step_avg:59.87ms
step:402/2420 train_time:24067ms step_avg:59.87ms
step:403/2420 train_time:24126ms step_avg:59.86ms
step:404/2420 train_time:24186ms step_avg:59.87ms
step:405/2420 train_time:24244ms step_avg:59.86ms
step:406/2420 train_time:24304ms step_avg:59.86ms
step:407/2420 train_time:24362ms step_avg:59.86ms
step:408/2420 train_time:24423ms step_avg:59.86ms
step:409/2420 train_time:24481ms step_avg:59.86ms
step:410/2420 train_time:24541ms step_avg:59.86ms
step:411/2420 train_time:24600ms step_avg:59.85ms
step:412/2420 train_time:24660ms step_avg:59.85ms
step:413/2420 train_time:24718ms step_avg:59.85ms
step:414/2420 train_time:24779ms step_avg:59.85ms
step:415/2420 train_time:24837ms step_avg:59.85ms
step:416/2420 train_time:24898ms step_avg:59.85ms
step:417/2420 train_time:24956ms step_avg:59.85ms
step:418/2420 train_time:25017ms step_avg:59.85ms
step:419/2420 train_time:25076ms step_avg:59.85ms
step:420/2420 train_time:25136ms step_avg:59.85ms
step:421/2420 train_time:25194ms step_avg:59.84ms
step:422/2420 train_time:25255ms step_avg:59.85ms
step:423/2420 train_time:25315ms step_avg:59.85ms
step:424/2420 train_time:25376ms step_avg:59.85ms
step:425/2420 train_time:25435ms step_avg:59.85ms
step:426/2420 train_time:25496ms step_avg:59.85ms
step:427/2420 train_time:25554ms step_avg:59.85ms
step:428/2420 train_time:25615ms step_avg:59.85ms
step:429/2420 train_time:25673ms step_avg:59.84ms
step:430/2420 train_time:25734ms step_avg:59.85ms
step:431/2420 train_time:25792ms step_avg:59.84ms
step:432/2420 train_time:25853ms step_avg:59.84ms
step:433/2420 train_time:25911ms step_avg:59.84ms
step:434/2420 train_time:25972ms step_avg:59.84ms
step:435/2420 train_time:26031ms step_avg:59.84ms
step:436/2420 train_time:26091ms step_avg:59.84ms
step:437/2420 train_time:26150ms step_avg:59.84ms
step:438/2420 train_time:26211ms step_avg:59.84ms
step:439/2420 train_time:26269ms step_avg:59.84ms
step:440/2420 train_time:26330ms step_avg:59.84ms
step:441/2420 train_time:26389ms step_avg:59.84ms
step:442/2420 train_time:26449ms step_avg:59.84ms
step:443/2420 train_time:26508ms step_avg:59.84ms
step:444/2420 train_time:26568ms step_avg:59.84ms
step:445/2420 train_time:26627ms step_avg:59.84ms
step:446/2420 train_time:26688ms step_avg:59.84ms
step:447/2420 train_time:26746ms step_avg:59.83ms
step:448/2420 train_time:26806ms step_avg:59.84ms
step:449/2420 train_time:26864ms step_avg:59.83ms
step:450/2420 train_time:26925ms step_avg:59.83ms
step:451/2420 train_time:26983ms step_avg:59.83ms
step:452/2420 train_time:27044ms step_avg:59.83ms
step:453/2420 train_time:27102ms step_avg:59.83ms
step:454/2420 train_time:27162ms step_avg:59.83ms
step:455/2420 train_time:27221ms step_avg:59.83ms
step:456/2420 train_time:27281ms step_avg:59.83ms
step:457/2420 train_time:27340ms step_avg:59.82ms
step:458/2420 train_time:27400ms step_avg:59.83ms
step:459/2420 train_time:27459ms step_avg:59.82ms
step:460/2420 train_time:27519ms step_avg:59.82ms
step:461/2420 train_time:27577ms step_avg:59.82ms
step:462/2420 train_time:27638ms step_avg:59.82ms
step:463/2420 train_time:27696ms step_avg:59.82ms
step:464/2420 train_time:27756ms step_avg:59.82ms
step:465/2420 train_time:27815ms step_avg:59.82ms
step:466/2420 train_time:27876ms step_avg:59.82ms
step:467/2420 train_time:27934ms step_avg:59.82ms
step:468/2420 train_time:27994ms step_avg:59.82ms
step:469/2420 train_time:28053ms step_avg:59.81ms
step:470/2420 train_time:28114ms step_avg:59.82ms
step:471/2420 train_time:28173ms step_avg:59.81ms
step:472/2420 train_time:28234ms step_avg:59.82ms
step:473/2420 train_time:28293ms step_avg:59.82ms
step:474/2420 train_time:28353ms step_avg:59.82ms
step:475/2420 train_time:28412ms step_avg:59.82ms
step:476/2420 train_time:28474ms step_avg:59.82ms
step:477/2420 train_time:28533ms step_avg:59.82ms
step:478/2420 train_time:28593ms step_avg:59.82ms
step:479/2420 train_time:28651ms step_avg:59.82ms
step:480/2420 train_time:28713ms step_avg:59.82ms
step:481/2420 train_time:28771ms step_avg:59.81ms
step:482/2420 train_time:28831ms step_avg:59.82ms
step:483/2420 train_time:28890ms step_avg:59.81ms
step:484/2420 train_time:28950ms step_avg:59.81ms
step:485/2420 train_time:29009ms step_avg:59.81ms
step:486/2420 train_time:29070ms step_avg:59.81ms
step:487/2420 train_time:29129ms step_avg:59.81ms
step:488/2420 train_time:29190ms step_avg:59.81ms
step:489/2420 train_time:29248ms step_avg:59.81ms
step:490/2420 train_time:29309ms step_avg:59.81ms
step:491/2420 train_time:29368ms step_avg:59.81ms
step:492/2420 train_time:29428ms step_avg:59.81ms
step:493/2420 train_time:29486ms step_avg:59.81ms
step:494/2420 train_time:29547ms step_avg:59.81ms
step:495/2420 train_time:29606ms step_avg:59.81ms
step:496/2420 train_time:29666ms step_avg:59.81ms
step:497/2420 train_time:29725ms step_avg:59.81ms
step:498/2420 train_time:29785ms step_avg:59.81ms
step:499/2420 train_time:29844ms step_avg:59.81ms
step:500/2420 train_time:29904ms step_avg:59.81ms
step:500/2420 val_loss:3.8265 train_time:29966ms step_avg:59.93ms
step:501/2420 train_time:29988ms step_avg:59.86ms
step:502/2420 train_time:30025ms step_avg:59.81ms
step:503/2420 train_time:30087ms step_avg:59.82ms
step:504/2420 train_time:30152ms step_avg:59.83ms
step:505/2420 train_time:30213ms step_avg:59.83ms
step:506/2420 train_time:30273ms step_avg:59.83ms
step:507/2420 train_time:30332ms step_avg:59.83ms
step:508/2420 train_time:30392ms step_avg:59.83ms
step:509/2420 train_time:30450ms step_avg:59.82ms
step:510/2420 train_time:30510ms step_avg:59.82ms
step:511/2420 train_time:30568ms step_avg:59.82ms
step:512/2420 train_time:30627ms step_avg:59.82ms
step:513/2420 train_time:30685ms step_avg:59.82ms
step:514/2420 train_time:30745ms step_avg:59.82ms
step:515/2420 train_time:30803ms step_avg:59.81ms
step:516/2420 train_time:30863ms step_avg:59.81ms
step:517/2420 train_time:30921ms step_avg:59.81ms
step:518/2420 train_time:30981ms step_avg:59.81ms
step:519/2420 train_time:31042ms step_avg:59.81ms
step:520/2420 train_time:31104ms step_avg:59.81ms
step:521/2420 train_time:31165ms step_avg:59.82ms
step:522/2420 train_time:31228ms step_avg:59.82ms
step:523/2420 train_time:31287ms step_avg:59.82ms
step:524/2420 train_time:31348ms step_avg:59.82ms
step:525/2420 train_time:31406ms step_avg:59.82ms
step:526/2420 train_time:31467ms step_avg:59.82ms
step:527/2420 train_time:31525ms step_avg:59.82ms
step:528/2420 train_time:31585ms step_avg:59.82ms
step:529/2420 train_time:31643ms step_avg:59.82ms
step:530/2420 train_time:31703ms step_avg:59.82ms
step:531/2420 train_time:31760ms step_avg:59.81ms
step:532/2420 train_time:31821ms step_avg:59.81ms
step:533/2420 train_time:31879ms step_avg:59.81ms
step:534/2420 train_time:31939ms step_avg:59.81ms
step:535/2420 train_time:31998ms step_avg:59.81ms
step:536/2420 train_time:32060ms step_avg:59.81ms
step:537/2420 train_time:32119ms step_avg:59.81ms
step:538/2420 train_time:32181ms step_avg:59.82ms
step:539/2420 train_time:32240ms step_avg:59.81ms
step:540/2420 train_time:32301ms step_avg:59.82ms
step:541/2420 train_time:32361ms step_avg:59.82ms
step:542/2420 train_time:32422ms step_avg:59.82ms
step:543/2420 train_time:32481ms step_avg:59.82ms
step:544/2420 train_time:32541ms step_avg:59.82ms
step:545/2420 train_time:32599ms step_avg:59.82ms
step:546/2420 train_time:32659ms step_avg:59.82ms
step:547/2420 train_time:32718ms step_avg:59.81ms
step:548/2420 train_time:32778ms step_avg:59.81ms
step:549/2420 train_time:32836ms step_avg:59.81ms
step:550/2420 train_time:32896ms step_avg:59.81ms
step:551/2420 train_time:32954ms step_avg:59.81ms
step:552/2420 train_time:33015ms step_avg:59.81ms
step:553/2420 train_time:33073ms step_avg:59.81ms
step:554/2420 train_time:33134ms step_avg:59.81ms
step:555/2420 train_time:33193ms step_avg:59.81ms
step:556/2420 train_time:33254ms step_avg:59.81ms
step:557/2420 train_time:33313ms step_avg:59.81ms
step:558/2420 train_time:33374ms step_avg:59.81ms
step:559/2420 train_time:33433ms step_avg:59.81ms
step:560/2420 train_time:33494ms step_avg:59.81ms
step:561/2420 train_time:33552ms step_avg:59.81ms
step:562/2420 train_time:33613ms step_avg:59.81ms
step:563/2420 train_time:33671ms step_avg:59.81ms
step:564/2420 train_time:33731ms step_avg:59.81ms
step:565/2420 train_time:33789ms step_avg:59.80ms
step:566/2420 train_time:33849ms step_avg:59.80ms
step:567/2420 train_time:33907ms step_avg:59.80ms
step:568/2420 train_time:33967ms step_avg:59.80ms
step:569/2420 train_time:34025ms step_avg:59.80ms
step:570/2420 train_time:34085ms step_avg:59.80ms
step:571/2420 train_time:34144ms step_avg:59.80ms
step:572/2420 train_time:34205ms step_avg:59.80ms
step:573/2420 train_time:34264ms step_avg:59.80ms
step:574/2420 train_time:34327ms step_avg:59.80ms
step:575/2420 train_time:34386ms step_avg:59.80ms
step:576/2420 train_time:34446ms step_avg:59.80ms
step:577/2420 train_time:34505ms step_avg:59.80ms
step:578/2420 train_time:34565ms step_avg:59.80ms
step:579/2420 train_time:34624ms step_avg:59.80ms
step:580/2420 train_time:34684ms step_avg:59.80ms
step:581/2420 train_time:34742ms step_avg:59.80ms
step:582/2420 train_time:34802ms step_avg:59.80ms
step:583/2420 train_time:34860ms step_avg:59.79ms
step:584/2420 train_time:34920ms step_avg:59.80ms
step:585/2420 train_time:34979ms step_avg:59.79ms
step:586/2420 train_time:35040ms step_avg:59.80ms
step:587/2420 train_time:35099ms step_avg:59.79ms
step:588/2420 train_time:35161ms step_avg:59.80ms
step:589/2420 train_time:35219ms step_avg:59.79ms
step:590/2420 train_time:35280ms step_avg:59.80ms
step:591/2420 train_time:35340ms step_avg:59.80ms
step:592/2420 train_time:35401ms step_avg:59.80ms
step:593/2420 train_time:35459ms step_avg:59.80ms
step:594/2420 train_time:35520ms step_avg:59.80ms
step:595/2420 train_time:35579ms step_avg:59.80ms
step:596/2420 train_time:35640ms step_avg:59.80ms
step:597/2420 train_time:35698ms step_avg:59.80ms
step:598/2420 train_time:35759ms step_avg:59.80ms
step:599/2420 train_time:35818ms step_avg:59.80ms
step:600/2420 train_time:35878ms step_avg:59.80ms
step:601/2420 train_time:35937ms step_avg:59.79ms
step:602/2420 train_time:35997ms step_avg:59.80ms
step:603/2420 train_time:36055ms step_avg:59.79ms
step:604/2420 train_time:36116ms step_avg:59.79ms
step:605/2420 train_time:36174ms step_avg:59.79ms
step:606/2420 train_time:36236ms step_avg:59.79ms
step:607/2420 train_time:36295ms step_avg:59.79ms
step:608/2420 train_time:36355ms step_avg:59.80ms
step:609/2420 train_time:36414ms step_avg:59.79ms
step:610/2420 train_time:36474ms step_avg:59.79ms
step:611/2420 train_time:36533ms step_avg:59.79ms
step:612/2420 train_time:36594ms step_avg:59.79ms
step:613/2420 train_time:36653ms step_avg:59.79ms
step:614/2420 train_time:36713ms step_avg:59.79ms
step:615/2420 train_time:36770ms step_avg:59.79ms
step:616/2420 train_time:36830ms step_avg:59.79ms
step:617/2420 train_time:36889ms step_avg:59.79ms
step:618/2420 train_time:36949ms step_avg:59.79ms
step:619/2420 train_time:37008ms step_avg:59.79ms
step:620/2420 train_time:37068ms step_avg:59.79ms
step:621/2420 train_time:37127ms step_avg:59.79ms
step:622/2420 train_time:37187ms step_avg:59.79ms
step:623/2420 train_time:37246ms step_avg:59.79ms
step:624/2420 train_time:37307ms step_avg:59.79ms
step:625/2420 train_time:37366ms step_avg:59.79ms
step:626/2420 train_time:37427ms step_avg:59.79ms
step:627/2420 train_time:37486ms step_avg:59.79ms
step:628/2420 train_time:37546ms step_avg:59.79ms
step:629/2420 train_time:37605ms step_avg:59.79ms
step:630/2420 train_time:37666ms step_avg:59.79ms
step:631/2420 train_time:37725ms step_avg:59.79ms
step:632/2420 train_time:37786ms step_avg:59.79ms
step:633/2420 train_time:37844ms step_avg:59.79ms
step:634/2420 train_time:37904ms step_avg:59.79ms
step:635/2420 train_time:37963ms step_avg:59.78ms
step:636/2420 train_time:38024ms step_avg:59.79ms
step:637/2420 train_time:38082ms step_avg:59.78ms
step:638/2420 train_time:38143ms step_avg:59.79ms
step:639/2420 train_time:38202ms step_avg:59.78ms
step:640/2420 train_time:38263ms step_avg:59.79ms
step:641/2420 train_time:38323ms step_avg:59.79ms
step:642/2420 train_time:38383ms step_avg:59.79ms
step:643/2420 train_time:38444ms step_avg:59.79ms
step:644/2420 train_time:38504ms step_avg:59.79ms
step:645/2420 train_time:38564ms step_avg:59.79ms
step:646/2420 train_time:38625ms step_avg:59.79ms
step:647/2420 train_time:38684ms step_avg:59.79ms
step:648/2420 train_time:38745ms step_avg:59.79ms
step:649/2420 train_time:38804ms step_avg:59.79ms
step:650/2420 train_time:38864ms step_avg:59.79ms
step:651/2420 train_time:38923ms step_avg:59.79ms
step:652/2420 train_time:38984ms step_avg:59.79ms
step:653/2420 train_time:39042ms step_avg:59.79ms
step:654/2420 train_time:39103ms step_avg:59.79ms
step:655/2420 train_time:39162ms step_avg:59.79ms
step:656/2420 train_time:39224ms step_avg:59.79ms
step:657/2420 train_time:39283ms step_avg:59.79ms
step:658/2420 train_time:39343ms step_avg:59.79ms
step:659/2420 train_time:39403ms step_avg:59.79ms
step:660/2420 train_time:39464ms step_avg:59.79ms
step:661/2420 train_time:39524ms step_avg:59.79ms
step:662/2420 train_time:39585ms step_avg:59.80ms
step:663/2420 train_time:39644ms step_avg:59.79ms
step:664/2420 train_time:39705ms step_avg:59.80ms
step:665/2420 train_time:39763ms step_avg:59.79ms
step:666/2420 train_time:39824ms step_avg:59.80ms
step:667/2420 train_time:39883ms step_avg:59.79ms
step:668/2420 train_time:39943ms step_avg:59.79ms
step:669/2420 train_time:40001ms step_avg:59.79ms
step:670/2420 train_time:40062ms step_avg:59.79ms
step:671/2420 train_time:40121ms step_avg:59.79ms
step:672/2420 train_time:40181ms step_avg:59.79ms
step:673/2420 train_time:40240ms step_avg:59.79ms
step:674/2420 train_time:40301ms step_avg:59.79ms
step:675/2420 train_time:40360ms step_avg:59.79ms
step:676/2420 train_time:40420ms step_avg:59.79ms
step:677/2420 train_time:40479ms step_avg:59.79ms
step:678/2420 train_time:40540ms step_avg:59.79ms
step:679/2420 train_time:40599ms step_avg:59.79ms
step:680/2420 train_time:40661ms step_avg:59.80ms
step:681/2420 train_time:40720ms step_avg:59.79ms
step:682/2420 train_time:40781ms step_avg:59.80ms
step:683/2420 train_time:40839ms step_avg:59.79ms
step:684/2420 train_time:40900ms step_avg:59.80ms
step:685/2420 train_time:40959ms step_avg:59.79ms
step:686/2420 train_time:41019ms step_avg:59.79ms
step:687/2420 train_time:41078ms step_avg:59.79ms
step:688/2420 train_time:41139ms step_avg:59.79ms
step:689/2420 train_time:41197ms step_avg:59.79ms
step:690/2420 train_time:41258ms step_avg:59.79ms
step:691/2420 train_time:41317ms step_avg:59.79ms
step:692/2420 train_time:41377ms step_avg:59.79ms
step:693/2420 train_time:41436ms step_avg:59.79ms
step:694/2420 train_time:41497ms step_avg:59.79ms
step:695/2420 train_time:41555ms step_avg:59.79ms
step:696/2420 train_time:41616ms step_avg:59.79ms
step:697/2420 train_time:41675ms step_avg:59.79ms
step:698/2420 train_time:41735ms step_avg:59.79ms
step:699/2420 train_time:41794ms step_avg:59.79ms
step:700/2420 train_time:41855ms step_avg:59.79ms
step:701/2420 train_time:41913ms step_avg:59.79ms
step:702/2420 train_time:41973ms step_avg:59.79ms
step:703/2420 train_time:42032ms step_avg:59.79ms
step:704/2420 train_time:42093ms step_avg:59.79ms
step:705/2420 train_time:42151ms step_avg:59.79ms
step:706/2420 train_time:42212ms step_avg:59.79ms
step:707/2420 train_time:42270ms step_avg:59.79ms
step:708/2420 train_time:42331ms step_avg:59.79ms
step:709/2420 train_time:42389ms step_avg:59.79ms
step:710/2420 train_time:42450ms step_avg:59.79ms
step:711/2420 train_time:42508ms step_avg:59.79ms
step:712/2420 train_time:42568ms step_avg:59.79ms
step:713/2420 train_time:42627ms step_avg:59.79ms
step:714/2420 train_time:42687ms step_avg:59.79ms
step:715/2420 train_time:42746ms step_avg:59.78ms
step:716/2420 train_time:42806ms step_avg:59.79ms
step:717/2420 train_time:42866ms step_avg:59.79ms
step:718/2420 train_time:42927ms step_avg:59.79ms
step:719/2420 train_time:42985ms step_avg:59.78ms
step:720/2420 train_time:43045ms step_avg:59.78ms
step:721/2420 train_time:43104ms step_avg:59.78ms
step:722/2420 train_time:43165ms step_avg:59.79ms
step:723/2420 train_time:43223ms step_avg:59.78ms
step:724/2420 train_time:43284ms step_avg:59.78ms
step:725/2420 train_time:43343ms step_avg:59.78ms
step:726/2420 train_time:43404ms step_avg:59.78ms
step:727/2420 train_time:43462ms step_avg:59.78ms
step:728/2420 train_time:43523ms step_avg:59.78ms
step:729/2420 train_time:43582ms step_avg:59.78ms
step:730/2420 train_time:43643ms step_avg:59.79ms
step:731/2420 train_time:43702ms step_avg:59.78ms
step:732/2420 train_time:43763ms step_avg:59.79ms
step:733/2420 train_time:43823ms step_avg:59.79ms
step:734/2420 train_time:43884ms step_avg:59.79ms
step:735/2420 train_time:43942ms step_avg:59.79ms
step:736/2420 train_time:44003ms step_avg:59.79ms
step:737/2420 train_time:44062ms step_avg:59.79ms
step:738/2420 train_time:44123ms step_avg:59.79ms
step:739/2420 train_time:44181ms step_avg:59.78ms
step:740/2420 train_time:44242ms step_avg:59.79ms
step:741/2420 train_time:44300ms step_avg:59.78ms
step:742/2420 train_time:44361ms step_avg:59.79ms
step:743/2420 train_time:44420ms step_avg:59.78ms
step:744/2420 train_time:44480ms step_avg:59.79ms
step:745/2420 train_time:44539ms step_avg:59.78ms
step:746/2420 train_time:44599ms step_avg:59.78ms
step:747/2420 train_time:44658ms step_avg:59.78ms
step:748/2420 train_time:44718ms step_avg:59.78ms
step:749/2420 train_time:44777ms step_avg:59.78ms
step:750/2420 train_time:44838ms step_avg:59.78ms
step:750/2420 val_loss:3.6943 train_time:44901ms step_avg:59.87ms
step:751/2420 train_time:44922ms step_avg:59.82ms
step:752/2420 train_time:44959ms step_avg:59.79ms
step:753/2420 train_time:45020ms step_avg:59.79ms
step:754/2420 train_time:45084ms step_avg:59.79ms
step:755/2420 train_time:45145ms step_avg:59.79ms
step:756/2420 train_time:45205ms step_avg:59.80ms
step:757/2420 train_time:45264ms step_avg:59.79ms
step:758/2420 train_time:45324ms step_avg:59.79ms
step:759/2420 train_time:45382ms step_avg:59.79ms
step:760/2420 train_time:45441ms step_avg:59.79ms
step:761/2420 train_time:45499ms step_avg:59.79ms
step:762/2420 train_time:45558ms step_avg:59.79ms
step:763/2420 train_time:45616ms step_avg:59.79ms
step:764/2420 train_time:45676ms step_avg:59.78ms
step:765/2420 train_time:45733ms step_avg:59.78ms
step:766/2420 train_time:45793ms step_avg:59.78ms
step:767/2420 train_time:45851ms step_avg:59.78ms
step:768/2420 train_time:45912ms step_avg:59.78ms
step:769/2420 train_time:45972ms step_avg:59.78ms
step:770/2420 train_time:46034ms step_avg:59.78ms
step:771/2420 train_time:46094ms step_avg:59.78ms
step:772/2420 train_time:46155ms step_avg:59.79ms
step:773/2420 train_time:46214ms step_avg:59.79ms
step:774/2420 train_time:46275ms step_avg:59.79ms
step:775/2420 train_time:46334ms step_avg:59.79ms
step:776/2420 train_time:46394ms step_avg:59.79ms
step:777/2420 train_time:46453ms step_avg:59.78ms
step:778/2420 train_time:46513ms step_avg:59.79ms
step:779/2420 train_time:46572ms step_avg:59.78ms
step:780/2420 train_time:46632ms step_avg:59.79ms
step:781/2420 train_time:46690ms step_avg:59.78ms
step:782/2420 train_time:46750ms step_avg:59.78ms
step:783/2420 train_time:46808ms step_avg:59.78ms
step:784/2420 train_time:46870ms step_avg:59.78ms
step:785/2420 train_time:46928ms step_avg:59.78ms
step:786/2420 train_time:46990ms step_avg:59.78ms
step:787/2420 train_time:47049ms step_avg:59.78ms
step:788/2420 train_time:47111ms step_avg:59.79ms
step:789/2420 train_time:47170ms step_avg:59.78ms
step:790/2420 train_time:47231ms step_avg:59.79ms
step:791/2420 train_time:47290ms step_avg:59.78ms
step:792/2420 train_time:47351ms step_avg:59.79ms
step:793/2420 train_time:47409ms step_avg:59.78ms
step:794/2420 train_time:47470ms step_avg:59.79ms
step:795/2420 train_time:47528ms step_avg:59.78ms
step:796/2420 train_time:47589ms step_avg:59.79ms
step:797/2420 train_time:47649ms step_avg:59.78ms
step:798/2420 train_time:47710ms step_avg:59.79ms
step:799/2420 train_time:47769ms step_avg:59.79ms
step:800/2420 train_time:47830ms step_avg:59.79ms
step:801/2420 train_time:47889ms step_avg:59.79ms
step:802/2420 train_time:47951ms step_avg:59.79ms
step:803/2420 train_time:48011ms step_avg:59.79ms
step:804/2420 train_time:48073ms step_avg:59.79ms
step:805/2420 train_time:48133ms step_avg:59.79ms
step:806/2420 train_time:48195ms step_avg:59.79ms
step:807/2420 train_time:48255ms step_avg:59.80ms
step:808/2420 train_time:48316ms step_avg:59.80ms
step:809/2420 train_time:48375ms step_avg:59.80ms
step:810/2420 train_time:48436ms step_avg:59.80ms
step:811/2420 train_time:48495ms step_avg:59.80ms
step:812/2420 train_time:48556ms step_avg:59.80ms
step:813/2420 train_time:48614ms step_avg:59.80ms
step:814/2420 train_time:48675ms step_avg:59.80ms
step:815/2420 train_time:48734ms step_avg:59.80ms
step:816/2420 train_time:48795ms step_avg:59.80ms
step:817/2420 train_time:48854ms step_avg:59.80ms
step:818/2420 train_time:48916ms step_avg:59.80ms
step:819/2420 train_time:48976ms step_avg:59.80ms
step:820/2420 train_time:49037ms step_avg:59.80ms
step:821/2420 train_time:49096ms step_avg:59.80ms
step:822/2420 train_time:49157ms step_avg:59.80ms
step:823/2420 train_time:49217ms step_avg:59.80ms
step:824/2420 train_time:49277ms step_avg:59.80ms
step:825/2420 train_time:49336ms step_avg:59.80ms
step:826/2420 train_time:49397ms step_avg:59.80ms
step:827/2420 train_time:49456ms step_avg:59.80ms
step:828/2420 train_time:49517ms step_avg:59.80ms
step:829/2420 train_time:49576ms step_avg:59.80ms
step:830/2420 train_time:49637ms step_avg:59.80ms
step:831/2420 train_time:49695ms step_avg:59.80ms
step:832/2420 train_time:49756ms step_avg:59.80ms
step:833/2420 train_time:49815ms step_avg:59.80ms
step:834/2420 train_time:49876ms step_avg:59.80ms
step:835/2420 train_time:49935ms step_avg:59.80ms
step:836/2420 train_time:49997ms step_avg:59.80ms
step:837/2420 train_time:50056ms step_avg:59.80ms
step:838/2420 train_time:50117ms step_avg:59.81ms
step:839/2420 train_time:50177ms step_avg:59.81ms
step:840/2420 train_time:50238ms step_avg:59.81ms
step:841/2420 train_time:50297ms step_avg:59.81ms
step:842/2420 train_time:50357ms step_avg:59.81ms
step:843/2420 train_time:50417ms step_avg:59.81ms
step:844/2420 train_time:50478ms step_avg:59.81ms
step:845/2420 train_time:50536ms step_avg:59.81ms
step:846/2420 train_time:50597ms step_avg:59.81ms
step:847/2420 train_time:50656ms step_avg:59.81ms
step:848/2420 train_time:50717ms step_avg:59.81ms
step:849/2420 train_time:50776ms step_avg:59.81ms
step:850/2420 train_time:50837ms step_avg:59.81ms
step:851/2420 train_time:50896ms step_avg:59.81ms
step:852/2420 train_time:50957ms step_avg:59.81ms
step:853/2420 train_time:51016ms step_avg:59.81ms
step:854/2420 train_time:51078ms step_avg:59.81ms
step:855/2420 train_time:51137ms step_avg:59.81ms
step:856/2420 train_time:51197ms step_avg:59.81ms
step:857/2420 train_time:51257ms step_avg:59.81ms
step:858/2420 train_time:51318ms step_avg:59.81ms
step:859/2420 train_time:51377ms step_avg:59.81ms
step:860/2420 train_time:51438ms step_avg:59.81ms
step:861/2420 train_time:51496ms step_avg:59.81ms
step:862/2420 train_time:51557ms step_avg:59.81ms
step:863/2420 train_time:51616ms step_avg:59.81ms
step:864/2420 train_time:51677ms step_avg:59.81ms
step:865/2420 train_time:51736ms step_avg:59.81ms
step:866/2420 train_time:51796ms step_avg:59.81ms
step:867/2420 train_time:51855ms step_avg:59.81ms
step:868/2420 train_time:51917ms step_avg:59.81ms
step:869/2420 train_time:51975ms step_avg:59.81ms
step:870/2420 train_time:52037ms step_avg:59.81ms
step:871/2420 train_time:52096ms step_avg:59.81ms
step:872/2420 train_time:52157ms step_avg:59.81ms
step:873/2420 train_time:52216ms step_avg:59.81ms
step:874/2420 train_time:52278ms step_avg:59.81ms
step:875/2420 train_time:52337ms step_avg:59.81ms
step:876/2420 train_time:52398ms step_avg:59.81ms
step:877/2420 train_time:52457ms step_avg:59.81ms
step:878/2420 train_time:52517ms step_avg:59.81ms
step:879/2420 train_time:52577ms step_avg:59.81ms
step:880/2420 train_time:52637ms step_avg:59.82ms
step:881/2420 train_time:52696ms step_avg:59.81ms
step:882/2420 train_time:52757ms step_avg:59.82ms
step:883/2420 train_time:52816ms step_avg:59.81ms
step:884/2420 train_time:52877ms step_avg:59.82ms
step:885/2420 train_time:52936ms step_avg:59.81ms
step:886/2420 train_time:52997ms step_avg:59.82ms
step:887/2420 train_time:53055ms step_avg:59.81ms
step:888/2420 train_time:53117ms step_avg:59.82ms
step:889/2420 train_time:53176ms step_avg:59.82ms
step:890/2420 train_time:53237ms step_avg:59.82ms
step:891/2420 train_time:53296ms step_avg:59.82ms
step:892/2420 train_time:53357ms step_avg:59.82ms
step:893/2420 train_time:53416ms step_avg:59.82ms
step:894/2420 train_time:53478ms step_avg:59.82ms
step:895/2420 train_time:53537ms step_avg:59.82ms
step:896/2420 train_time:53598ms step_avg:59.82ms
step:897/2420 train_time:53656ms step_avg:59.82ms
step:898/2420 train_time:53717ms step_avg:59.82ms
step:899/2420 train_time:53776ms step_avg:59.82ms
step:900/2420 train_time:53836ms step_avg:59.82ms
step:901/2420 train_time:53895ms step_avg:59.82ms
step:902/2420 train_time:53956ms step_avg:59.82ms
step:903/2420 train_time:54015ms step_avg:59.82ms
step:904/2420 train_time:54077ms step_avg:59.82ms
step:905/2420 train_time:54136ms step_avg:59.82ms
step:906/2420 train_time:54197ms step_avg:59.82ms
step:907/2420 train_time:54257ms step_avg:59.82ms
step:908/2420 train_time:54318ms step_avg:59.82ms
step:909/2420 train_time:54377ms step_avg:59.82ms
step:910/2420 train_time:54438ms step_avg:59.82ms
step:911/2420 train_time:54496ms step_avg:59.82ms
step:912/2420 train_time:54558ms step_avg:59.82ms
step:913/2420 train_time:54617ms step_avg:59.82ms
step:914/2420 train_time:54678ms step_avg:59.82ms
step:915/2420 train_time:54736ms step_avg:59.82ms
step:916/2420 train_time:54797ms step_avg:59.82ms
step:917/2420 train_time:54856ms step_avg:59.82ms
step:918/2420 train_time:54916ms step_avg:59.82ms
step:919/2420 train_time:54976ms step_avg:59.82ms
step:920/2420 train_time:55037ms step_avg:59.82ms
step:921/2420 train_time:55096ms step_avg:59.82ms
step:922/2420 train_time:55157ms step_avg:59.82ms
step:923/2420 train_time:55216ms step_avg:59.82ms
step:924/2420 train_time:55277ms step_avg:59.82ms
step:925/2420 train_time:55337ms step_avg:59.82ms
step:926/2420 train_time:55397ms step_avg:59.82ms
step:927/2420 train_time:55456ms step_avg:59.82ms
step:928/2420 train_time:55518ms step_avg:59.82ms
step:929/2420 train_time:55577ms step_avg:59.82ms
step:930/2420 train_time:55638ms step_avg:59.83ms
step:931/2420 train_time:55697ms step_avg:59.82ms
step:932/2420 train_time:55757ms step_avg:59.83ms
step:933/2420 train_time:55816ms step_avg:59.82ms
step:934/2420 train_time:55877ms step_avg:59.83ms
step:935/2420 train_time:55936ms step_avg:59.82ms
step:936/2420 train_time:55996ms step_avg:59.83ms
step:937/2420 train_time:56055ms step_avg:59.82ms
step:938/2420 train_time:56116ms step_avg:59.83ms
step:939/2420 train_time:56175ms step_avg:59.82ms
step:940/2420 train_time:56236ms step_avg:59.83ms
step:941/2420 train_time:56296ms step_avg:59.83ms
step:942/2420 train_time:56357ms step_avg:59.83ms
step:943/2420 train_time:56416ms step_avg:59.83ms
step:944/2420 train_time:56478ms step_avg:59.83ms
step:945/2420 train_time:56537ms step_avg:59.83ms
step:946/2420 train_time:56598ms step_avg:59.83ms
step:947/2420 train_time:56656ms step_avg:59.83ms
step:948/2420 train_time:56717ms step_avg:59.83ms
step:949/2420 train_time:56776ms step_avg:59.83ms
step:950/2420 train_time:56837ms step_avg:59.83ms
step:951/2420 train_time:56896ms step_avg:59.83ms
step:952/2420 train_time:56957ms step_avg:59.83ms
step:953/2420 train_time:57017ms step_avg:59.83ms
step:954/2420 train_time:57078ms step_avg:59.83ms
step:955/2420 train_time:57137ms step_avg:59.83ms
step:956/2420 train_time:57197ms step_avg:59.83ms
step:957/2420 train_time:57256ms step_avg:59.83ms
step:958/2420 train_time:57317ms step_avg:59.83ms
step:959/2420 train_time:57377ms step_avg:59.83ms
step:960/2420 train_time:57438ms step_avg:59.83ms
step:961/2420 train_time:57497ms step_avg:59.83ms
step:962/2420 train_time:57558ms step_avg:59.83ms
step:963/2420 train_time:57618ms step_avg:59.83ms
step:964/2420 train_time:57679ms step_avg:59.83ms
step:965/2420 train_time:57738ms step_avg:59.83ms
step:966/2420 train_time:57798ms step_avg:59.83ms
step:967/2420 train_time:57858ms step_avg:59.83ms
step:968/2420 train_time:57919ms step_avg:59.83ms
step:969/2420 train_time:57978ms step_avg:59.83ms
step:970/2420 train_time:58038ms step_avg:59.83ms
step:971/2420 train_time:58097ms step_avg:59.83ms
step:972/2420 train_time:58158ms step_avg:59.83ms
step:973/2420 train_time:58217ms step_avg:59.83ms
step:974/2420 train_time:58278ms step_avg:59.83ms
step:975/2420 train_time:58337ms step_avg:59.83ms
step:976/2420 train_time:58398ms step_avg:59.83ms
step:977/2420 train_time:58457ms step_avg:59.83ms
step:978/2420 train_time:58519ms step_avg:59.84ms
step:979/2420 train_time:58578ms step_avg:59.83ms
step:980/2420 train_time:58638ms step_avg:59.84ms
step:981/2420 train_time:58698ms step_avg:59.83ms
step:982/2420 train_time:58759ms step_avg:59.84ms
step:983/2420 train_time:58818ms step_avg:59.84ms
step:984/2420 train_time:58879ms step_avg:59.84ms
step:985/2420 train_time:58939ms step_avg:59.84ms
step:986/2420 train_time:59000ms step_avg:59.84ms
step:987/2420 train_time:59059ms step_avg:59.84ms
step:988/2420 train_time:59120ms step_avg:59.84ms
step:989/2420 train_time:59180ms step_avg:59.84ms
step:990/2420 train_time:59241ms step_avg:59.84ms
step:991/2420 train_time:59299ms step_avg:59.84ms
step:992/2420 train_time:59360ms step_avg:59.84ms
step:993/2420 train_time:59419ms step_avg:59.84ms
step:994/2420 train_time:59481ms step_avg:59.84ms
step:995/2420 train_time:59539ms step_avg:59.84ms
step:996/2420 train_time:59600ms step_avg:59.84ms
step:997/2420 train_time:59660ms step_avg:59.84ms
step:998/2420 train_time:59721ms step_avg:59.84ms
step:999/2420 train_time:59780ms step_avg:59.84ms
step:1000/2420 train_time:59841ms step_avg:59.84ms
step:1000/2420 val_loss:3.5841 train_time:59904ms step_avg:59.90ms
step:1001/2420 train_time:59929ms step_avg:59.87ms
step:1002/2420 train_time:59967ms step_avg:59.85ms
step:1003/2420 train_time:60026ms step_avg:59.85ms
step:1004/2420 train_time:60088ms step_avg:59.85ms
step:1005/2420 train_time:60149ms step_avg:59.85ms
step:1006/2420 train_time:60212ms step_avg:59.85ms
step:1007/2420 train_time:60270ms step_avg:59.85ms
step:1008/2420 train_time:60330ms step_avg:59.85ms
step:1009/2420 train_time:60389ms step_avg:59.85ms
step:1010/2420 train_time:60450ms step_avg:59.85ms
step:1011/2420 train_time:60508ms step_avg:59.85ms
step:1012/2420 train_time:60568ms step_avg:59.85ms
step:1013/2420 train_time:60626ms step_avg:59.85ms
step:1014/2420 train_time:60686ms step_avg:59.85ms
step:1015/2420 train_time:60745ms step_avg:59.85ms
step:1016/2420 train_time:60807ms step_avg:59.85ms
step:1017/2420 train_time:60868ms step_avg:59.85ms
step:1018/2420 train_time:60930ms step_avg:59.85ms
step:1019/2420 train_time:60989ms step_avg:59.85ms
step:1020/2420 train_time:61051ms step_avg:59.85ms
step:1021/2420 train_time:61110ms step_avg:59.85ms
step:1022/2420 train_time:61171ms step_avg:59.85ms
step:1023/2420 train_time:61230ms step_avg:59.85ms
step:1024/2420 train_time:61291ms step_avg:59.85ms
step:1025/2420 train_time:61350ms step_avg:59.85ms
step:1026/2420 train_time:61410ms step_avg:59.85ms
step:1027/2420 train_time:61469ms step_avg:59.85ms
step:1028/2420 train_time:61530ms step_avg:59.85ms
step:1029/2420 train_time:61588ms step_avg:59.85ms
step:1030/2420 train_time:61649ms step_avg:59.85ms
step:1031/2420 train_time:61708ms step_avg:59.85ms
step:1032/2420 train_time:61769ms step_avg:59.85ms
step:1033/2420 train_time:61829ms step_avg:59.85ms
step:1034/2420 train_time:61890ms step_avg:59.85ms
step:1035/2420 train_time:61949ms step_avg:59.85ms
step:1036/2420 train_time:62011ms step_avg:59.86ms
step:1037/2420 train_time:62071ms step_avg:59.86ms
step:1038/2420 train_time:62132ms step_avg:59.86ms
step:1039/2420 train_time:62191ms step_avg:59.86ms
step:1040/2420 train_time:62253ms step_avg:59.86ms
step:1041/2420 train_time:62312ms step_avg:59.86ms
step:1042/2420 train_time:62373ms step_avg:59.86ms
step:1043/2420 train_time:62431ms step_avg:59.86ms
step:1044/2420 train_time:62492ms step_avg:59.86ms
step:1045/2420 train_time:62551ms step_avg:59.86ms
step:1046/2420 train_time:62612ms step_avg:59.86ms
step:1047/2420 train_time:62671ms step_avg:59.86ms
step:1048/2420 train_time:62732ms step_avg:59.86ms
step:1049/2420 train_time:62792ms step_avg:59.86ms
step:1050/2420 train_time:62853ms step_avg:59.86ms
step:1051/2420 train_time:62913ms step_avg:59.86ms
step:1052/2420 train_time:62974ms step_avg:59.86ms
step:1053/2420 train_time:63034ms step_avg:59.86ms
step:1054/2420 train_time:63095ms step_avg:59.86ms
step:1055/2420 train_time:63154ms step_avg:59.86ms
step:1056/2420 train_time:63216ms step_avg:59.86ms
step:1057/2420 train_time:63276ms step_avg:59.86ms
step:1058/2420 train_time:63337ms step_avg:59.86ms
step:1059/2420 train_time:63396ms step_avg:59.86ms
step:1060/2420 train_time:63458ms step_avg:59.87ms
step:1061/2420 train_time:63516ms step_avg:59.86ms
step:1062/2420 train_time:63577ms step_avg:59.87ms
step:1063/2420 train_time:63636ms step_avg:59.86ms
step:1064/2420 train_time:63698ms step_avg:59.87ms
step:1065/2420 train_time:63758ms step_avg:59.87ms
step:1066/2420 train_time:63820ms step_avg:59.87ms
step:1067/2420 train_time:63879ms step_avg:59.87ms
step:1068/2420 train_time:63942ms step_avg:59.87ms
step:1069/2420 train_time:64002ms step_avg:59.87ms
step:1070/2420 train_time:64065ms step_avg:59.87ms
step:1071/2420 train_time:64124ms step_avg:59.87ms
step:1072/2420 train_time:64186ms step_avg:59.88ms
step:1073/2420 train_time:64246ms step_avg:59.88ms
step:1074/2420 train_time:64307ms step_avg:59.88ms
step:1075/2420 train_time:64367ms step_avg:59.88ms
step:1076/2420 train_time:64428ms step_avg:59.88ms
step:1077/2420 train_time:64487ms step_avg:59.88ms
step:1078/2420 train_time:64548ms step_avg:59.88ms
step:1079/2420 train_time:64606ms step_avg:59.88ms
step:1080/2420 train_time:64667ms step_avg:59.88ms
step:1081/2420 train_time:64726ms step_avg:59.88ms
step:1082/2420 train_time:64787ms step_avg:59.88ms
step:1083/2420 train_time:64847ms step_avg:59.88ms
step:1084/2420 train_time:64908ms step_avg:59.88ms
step:1085/2420 train_time:64968ms step_avg:59.88ms
step:1086/2420 train_time:65029ms step_avg:59.88ms
step:1087/2420 train_time:65089ms step_avg:59.88ms
step:1088/2420 train_time:65149ms step_avg:59.88ms
step:1089/2420 train_time:65209ms step_avg:59.88ms
step:1090/2420 train_time:65270ms step_avg:59.88ms
step:1091/2420 train_time:65329ms step_avg:59.88ms
step:1092/2420 train_time:65390ms step_avg:59.88ms
step:1093/2420 train_time:65450ms step_avg:59.88ms
step:1094/2420 train_time:65511ms step_avg:59.88ms
step:1095/2420 train_time:65570ms step_avg:59.88ms
step:1096/2420 train_time:65630ms step_avg:59.88ms
step:1097/2420 train_time:65689ms step_avg:59.88ms
step:1098/2420 train_time:65750ms step_avg:59.88ms
step:1099/2420 train_time:65809ms step_avg:59.88ms
step:1100/2420 train_time:65870ms step_avg:59.88ms
step:1101/2420 train_time:65929ms step_avg:59.88ms
step:1102/2420 train_time:65990ms step_avg:59.88ms
step:1103/2420 train_time:66050ms step_avg:59.88ms
step:1104/2420 train_time:66111ms step_avg:59.88ms
step:1105/2420 train_time:66171ms step_avg:59.88ms
step:1106/2420 train_time:66232ms step_avg:59.88ms
step:1107/2420 train_time:66292ms step_avg:59.88ms
step:1108/2420 train_time:66353ms step_avg:59.89ms
step:1109/2420 train_time:66413ms step_avg:59.89ms
step:1110/2420 train_time:66474ms step_avg:59.89ms
step:1111/2420 train_time:66533ms step_avg:59.89ms
step:1112/2420 train_time:66594ms step_avg:59.89ms
step:1113/2420 train_time:66653ms step_avg:59.89ms
step:1114/2420 train_time:66715ms step_avg:59.89ms
step:1115/2420 train_time:66775ms step_avg:59.89ms
step:1116/2420 train_time:66836ms step_avg:59.89ms
step:1117/2420 train_time:66896ms step_avg:59.89ms
step:1118/2420 train_time:66957ms step_avg:59.89ms
step:1119/2420 train_time:67017ms step_avg:59.89ms
step:1120/2420 train_time:67078ms step_avg:59.89ms
step:1121/2420 train_time:67138ms step_avg:59.89ms
step:1122/2420 train_time:67200ms step_avg:59.89ms
step:1123/2420 train_time:67259ms step_avg:59.89ms
step:1124/2420 train_time:67321ms step_avg:59.89ms
step:1125/2420 train_time:67380ms step_avg:59.89ms
step:1126/2420 train_time:67442ms step_avg:59.90ms
step:1127/2420 train_time:67501ms step_avg:59.89ms
step:1128/2420 train_time:67563ms step_avg:59.90ms
step:1129/2420 train_time:67622ms step_avg:59.90ms
step:1130/2420 train_time:67684ms step_avg:59.90ms
step:1131/2420 train_time:67743ms step_avg:59.90ms
step:1132/2420 train_time:67805ms step_avg:59.90ms
step:1133/2420 train_time:67864ms step_avg:59.90ms
step:1134/2420 train_time:67927ms step_avg:59.90ms
step:1135/2420 train_time:67986ms step_avg:59.90ms
step:1136/2420 train_time:68048ms step_avg:59.90ms
step:1137/2420 train_time:68107ms step_avg:59.90ms
step:1138/2420 train_time:68168ms step_avg:59.90ms
step:1139/2420 train_time:68227ms step_avg:59.90ms
step:1140/2420 train_time:68288ms step_avg:59.90ms
step:1141/2420 train_time:68348ms step_avg:59.90ms
step:1142/2420 train_time:68409ms step_avg:59.90ms
step:1143/2420 train_time:68468ms step_avg:59.90ms
step:1144/2420 train_time:68529ms step_avg:59.90ms
step:1145/2420 train_time:68588ms step_avg:59.90ms
step:1146/2420 train_time:68650ms step_avg:59.90ms
step:1147/2420 train_time:68709ms step_avg:59.90ms
step:1148/2420 train_time:68769ms step_avg:59.90ms
step:1149/2420 train_time:68828ms step_avg:59.90ms
step:1150/2420 train_time:68889ms step_avg:59.90ms
step:1151/2420 train_time:68949ms step_avg:59.90ms
step:1152/2420 train_time:69009ms step_avg:59.90ms
step:1153/2420 train_time:69068ms step_avg:59.90ms
step:1154/2420 train_time:69130ms step_avg:59.90ms
step:1155/2420 train_time:69188ms step_avg:59.90ms
step:1156/2420 train_time:69249ms step_avg:59.90ms
step:1157/2420 train_time:69308ms step_avg:59.90ms
step:1158/2420 train_time:69369ms step_avg:59.90ms
step:1159/2420 train_time:69428ms step_avg:59.90ms
step:1160/2420 train_time:69489ms step_avg:59.90ms
step:1161/2420 train_time:69548ms step_avg:59.90ms
step:1162/2420 train_time:69609ms step_avg:59.90ms
step:1163/2420 train_time:69668ms step_avg:59.90ms
step:1164/2420 train_time:69729ms step_avg:59.90ms
step:1165/2420 train_time:69788ms step_avg:59.90ms
step:1166/2420 train_time:69849ms step_avg:59.90ms
step:1167/2420 train_time:69908ms step_avg:59.90ms
step:1168/2420 train_time:69969ms step_avg:59.90ms
step:1169/2420 train_time:70029ms step_avg:59.90ms
step:1170/2420 train_time:70089ms step_avg:59.91ms
step:1171/2420 train_time:70149ms step_avg:59.90ms
step:1172/2420 train_time:70209ms step_avg:59.91ms
step:1173/2420 train_time:70268ms step_avg:59.90ms
step:1174/2420 train_time:70329ms step_avg:59.91ms
step:1175/2420 train_time:70388ms step_avg:59.90ms
step:1176/2420 train_time:70449ms step_avg:59.91ms
step:1177/2420 train_time:70508ms step_avg:59.91ms
step:1178/2420 train_time:70569ms step_avg:59.91ms
step:1179/2420 train_time:70628ms step_avg:59.90ms
step:1180/2420 train_time:70689ms step_avg:59.91ms
step:1181/2420 train_time:70748ms step_avg:59.90ms
step:1182/2420 train_time:70808ms step_avg:59.91ms
step:1183/2420 train_time:70867ms step_avg:59.90ms
step:1184/2420 train_time:70928ms step_avg:59.91ms
step:1185/2420 train_time:70988ms step_avg:59.91ms
step:1186/2420 train_time:71049ms step_avg:59.91ms
step:1187/2420 train_time:71108ms step_avg:59.91ms
step:1188/2420 train_time:71169ms step_avg:59.91ms
step:1189/2420 train_time:71228ms step_avg:59.91ms
step:1190/2420 train_time:71289ms step_avg:59.91ms
step:1191/2420 train_time:71348ms step_avg:59.91ms
step:1192/2420 train_time:71409ms step_avg:59.91ms
step:1193/2420 train_time:71467ms step_avg:59.91ms
step:1194/2420 train_time:71528ms step_avg:59.91ms
step:1195/2420 train_time:71587ms step_avg:59.91ms
step:1196/2420 train_time:71648ms step_avg:59.91ms
step:1197/2420 train_time:71708ms step_avg:59.91ms
step:1198/2420 train_time:71769ms step_avg:59.91ms
step:1199/2420 train_time:71828ms step_avg:59.91ms
step:1200/2420 train_time:71889ms step_avg:59.91ms
step:1201/2420 train_time:71948ms step_avg:59.91ms
step:1202/2420 train_time:72009ms step_avg:59.91ms
step:1203/2420 train_time:72069ms step_avg:59.91ms
step:1204/2420 train_time:72129ms step_avg:59.91ms
step:1205/2420 train_time:72189ms step_avg:59.91ms
step:1206/2420 train_time:72250ms step_avg:59.91ms
step:1207/2420 train_time:72309ms step_avg:59.91ms
step:1208/2420 train_time:72370ms step_avg:59.91ms
step:1209/2420 train_time:72429ms step_avg:59.91ms
step:1210/2420 train_time:72490ms step_avg:59.91ms
step:1211/2420 train_time:72549ms step_avg:59.91ms
step:1212/2420 train_time:72610ms step_avg:59.91ms
step:1213/2420 train_time:72669ms step_avg:59.91ms
step:1214/2420 train_time:72730ms step_avg:59.91ms
step:1215/2420 train_time:72789ms step_avg:59.91ms
step:1216/2420 train_time:72851ms step_avg:59.91ms
step:1217/2420 train_time:72909ms step_avg:59.91ms
step:1218/2420 train_time:72970ms step_avg:59.91ms
step:1219/2420 train_time:73029ms step_avg:59.91ms
step:1220/2420 train_time:73090ms step_avg:59.91ms
step:1221/2420 train_time:73150ms step_avg:59.91ms
step:1222/2420 train_time:73211ms step_avg:59.91ms
step:1223/2420 train_time:73270ms step_avg:59.91ms
step:1224/2420 train_time:73331ms step_avg:59.91ms
step:1225/2420 train_time:73390ms step_avg:59.91ms
step:1226/2420 train_time:73451ms step_avg:59.91ms
step:1227/2420 train_time:73510ms step_avg:59.91ms
step:1228/2420 train_time:73572ms step_avg:59.91ms
step:1229/2420 train_time:73631ms step_avg:59.91ms
step:1230/2420 train_time:73692ms step_avg:59.91ms
step:1231/2420 train_time:73751ms step_avg:59.91ms
step:1232/2420 train_time:73812ms step_avg:59.91ms
step:1233/2420 train_time:73872ms step_avg:59.91ms
step:1234/2420 train_time:73933ms step_avg:59.91ms
step:1235/2420 train_time:73993ms step_avg:59.91ms
step:1236/2420 train_time:74054ms step_avg:59.91ms
step:1237/2420 train_time:74113ms step_avg:59.91ms
step:1238/2420 train_time:74174ms step_avg:59.91ms
step:1239/2420 train_time:74234ms step_avg:59.91ms
step:1240/2420 train_time:74295ms step_avg:59.92ms
step:1241/2420 train_time:74354ms step_avg:59.91ms
step:1242/2420 train_time:74416ms step_avg:59.92ms
step:1243/2420 train_time:74475ms step_avg:59.92ms
step:1244/2420 train_time:74537ms step_avg:59.92ms
step:1245/2420 train_time:74596ms step_avg:59.92ms
step:1246/2420 train_time:74657ms step_avg:59.92ms
step:1247/2420 train_time:74716ms step_avg:59.92ms
step:1248/2420 train_time:74778ms step_avg:59.92ms
step:1249/2420 train_time:74838ms step_avg:59.92ms
step:1250/2420 train_time:74900ms step_avg:59.92ms
step:1250/2420 val_loss:3.5236 train_time:74963ms step_avg:59.97ms
step:1251/2420 train_time:74985ms step_avg:59.94ms
step:1252/2420 train_time:75025ms step_avg:59.92ms
step:1253/2420 train_time:75087ms step_avg:59.93ms
step:1254/2420 train_time:75151ms step_avg:59.93ms
step:1255/2420 train_time:75211ms step_avg:59.93ms
step:1256/2420 train_time:75272ms step_avg:59.93ms
step:1257/2420 train_time:75331ms step_avg:59.93ms
step:1258/2420 train_time:75392ms step_avg:59.93ms
step:1259/2420 train_time:75451ms step_avg:59.93ms
step:1260/2420 train_time:75511ms step_avg:59.93ms
step:1261/2420 train_time:75570ms step_avg:59.93ms
step:1262/2420 train_time:75631ms step_avg:59.93ms
step:1263/2420 train_time:75689ms step_avg:59.93ms
step:1264/2420 train_time:75749ms step_avg:59.93ms
step:1265/2420 train_time:75808ms step_avg:59.93ms
step:1266/2420 train_time:75868ms step_avg:59.93ms
step:1267/2420 train_time:75929ms step_avg:59.93ms
step:1268/2420 train_time:75992ms step_avg:59.93ms
step:1269/2420 train_time:76054ms step_avg:59.93ms
step:1270/2420 train_time:76116ms step_avg:59.93ms
step:1271/2420 train_time:76176ms step_avg:59.93ms
step:1272/2420 train_time:76238ms step_avg:59.94ms
step:1273/2420 train_time:76298ms step_avg:59.94ms
step:1274/2420 train_time:76359ms step_avg:59.94ms
step:1275/2420 train_time:76419ms step_avg:59.94ms
step:1276/2420 train_time:76480ms step_avg:59.94ms
step:1277/2420 train_time:76539ms step_avg:59.94ms
step:1278/2420 train_time:76600ms step_avg:59.94ms
step:1279/2420 train_time:76660ms step_avg:59.94ms
step:1280/2420 train_time:76721ms step_avg:59.94ms
step:1281/2420 train_time:76780ms step_avg:59.94ms
step:1282/2420 train_time:76841ms step_avg:59.94ms
step:1283/2420 train_time:76901ms step_avg:59.94ms
step:1284/2420 train_time:76963ms step_avg:59.94ms
step:1285/2420 train_time:77023ms step_avg:59.94ms
step:1286/2420 train_time:77085ms step_avg:59.94ms
step:1287/2420 train_time:77144ms step_avg:59.94ms
step:1288/2420 train_time:77205ms step_avg:59.94ms
step:1289/2420 train_time:77264ms step_avg:59.94ms
step:1290/2420 train_time:77324ms step_avg:59.94ms
step:1291/2420 train_time:77384ms step_avg:59.94ms
step:1292/2420 train_time:77444ms step_avg:59.94ms
step:1293/2420 train_time:77503ms step_avg:59.94ms
step:1294/2420 train_time:77564ms step_avg:59.94ms
step:1295/2420 train_time:77623ms step_avg:59.94ms
step:1296/2420 train_time:77685ms step_avg:59.94ms
step:1297/2420 train_time:77744ms step_avg:59.94ms
step:1298/2420 train_time:77804ms step_avg:59.94ms
step:1299/2420 train_time:77864ms step_avg:59.94ms
step:1300/2420 train_time:77925ms step_avg:59.94ms
step:1301/2420 train_time:77984ms step_avg:59.94ms
step:1302/2420 train_time:78045ms step_avg:59.94ms
step:1303/2420 train_time:78105ms step_avg:59.94ms
step:1304/2420 train_time:78166ms step_avg:59.94ms
step:1305/2420 train_time:78225ms step_avg:59.94ms
step:1306/2420 train_time:78286ms step_avg:59.94ms
step:1307/2420 train_time:78345ms step_avg:59.94ms
step:1308/2420 train_time:78406ms step_avg:59.94ms
step:1309/2420 train_time:78465ms step_avg:59.94ms
step:1310/2420 train_time:78526ms step_avg:59.94ms
step:1311/2420 train_time:78585ms step_avg:59.94ms
step:1312/2420 train_time:78646ms step_avg:59.94ms
step:1313/2420 train_time:78705ms step_avg:59.94ms
step:1314/2420 train_time:78766ms step_avg:59.94ms
step:1315/2420 train_time:78825ms step_avg:59.94ms
step:1316/2420 train_time:78886ms step_avg:59.94ms
step:1317/2420 train_time:78945ms step_avg:59.94ms
step:1318/2420 train_time:79006ms step_avg:59.94ms
step:1319/2420 train_time:79064ms step_avg:59.94ms
step:1320/2420 train_time:79125ms step_avg:59.94ms
step:1321/2420 train_time:79185ms step_avg:59.94ms
step:1322/2420 train_time:79246ms step_avg:59.94ms
step:1323/2420 train_time:79305ms step_avg:59.94ms
step:1324/2420 train_time:79366ms step_avg:59.94ms
step:1325/2420 train_time:79425ms step_avg:59.94ms
step:1326/2420 train_time:79486ms step_avg:59.94ms
step:1327/2420 train_time:79544ms step_avg:59.94ms
step:1328/2420 train_time:79605ms step_avg:59.94ms
step:1329/2420 train_time:79664ms step_avg:59.94ms
step:1330/2420 train_time:79725ms step_avg:59.94ms
step:1331/2420 train_time:79784ms step_avg:59.94ms
step:1332/2420 train_time:79845ms step_avg:59.94ms
step:1333/2420 train_time:79904ms step_avg:59.94ms
step:1334/2420 train_time:79966ms step_avg:59.94ms
step:1335/2420 train_time:80025ms step_avg:59.94ms
step:1336/2420 train_time:80087ms step_avg:59.95ms
step:1337/2420 train_time:80146ms step_avg:59.94ms
step:1338/2420 train_time:80206ms step_avg:59.94ms
step:1339/2420 train_time:80265ms step_avg:59.94ms
step:1340/2420 train_time:80326ms step_avg:59.94ms
step:1341/2420 train_time:80386ms step_avg:59.94ms
step:1342/2420 train_time:80446ms step_avg:59.95ms
step:1343/2420 train_time:80505ms step_avg:59.94ms
step:1344/2420 train_time:80566ms step_avg:59.95ms
step:1345/2420 train_time:80625ms step_avg:59.94ms
step:1346/2420 train_time:80686ms step_avg:59.95ms
step:1347/2420 train_time:80745ms step_avg:59.94ms
step:1348/2420 train_time:80806ms step_avg:59.95ms
step:1349/2420 train_time:80865ms step_avg:59.94ms
step:1350/2420 train_time:80926ms step_avg:59.95ms
step:1351/2420 train_time:80985ms step_avg:59.94ms
step:1352/2420 train_time:81046ms step_avg:59.95ms
step:1353/2420 train_time:81105ms step_avg:59.94ms
step:1354/2420 train_time:81166ms step_avg:59.95ms
step:1355/2420 train_time:81226ms step_avg:59.95ms
step:1356/2420 train_time:81286ms step_avg:59.95ms
step:1357/2420 train_time:81346ms step_avg:59.95ms
step:1358/2420 train_time:81406ms step_avg:59.95ms
step:1359/2420 train_time:81465ms step_avg:59.95ms
step:1360/2420 train_time:81526ms step_avg:59.95ms
step:1361/2420 train_time:81585ms step_avg:59.95ms
step:1362/2420 train_time:81646ms step_avg:59.95ms
step:1363/2420 train_time:81705ms step_avg:59.94ms
step:1364/2420 train_time:81766ms step_avg:59.95ms
step:1365/2420 train_time:81825ms step_avg:59.95ms
step:1366/2420 train_time:81886ms step_avg:59.95ms
step:1367/2420 train_time:81945ms step_avg:59.95ms
step:1368/2420 train_time:82006ms step_avg:59.95ms
step:1369/2420 train_time:82066ms step_avg:59.95ms
step:1370/2420 train_time:82127ms step_avg:59.95ms
step:1371/2420 train_time:82186ms step_avg:59.95ms
step:1372/2420 train_time:82247ms step_avg:59.95ms
step:1373/2420 train_time:82306ms step_avg:59.95ms
step:1374/2420 train_time:82367ms step_avg:59.95ms
step:1375/2420 train_time:82426ms step_avg:59.95ms
step:1376/2420 train_time:82487ms step_avg:59.95ms
step:1377/2420 train_time:82546ms step_avg:59.95ms
step:1378/2420 train_time:82606ms step_avg:59.95ms
step:1379/2420 train_time:82666ms step_avg:59.95ms
step:1380/2420 train_time:82726ms step_avg:59.95ms
step:1381/2420 train_time:82786ms step_avg:59.95ms
step:1382/2420 train_time:82847ms step_avg:59.95ms
step:1383/2420 train_time:82906ms step_avg:59.95ms
step:1384/2420 train_time:82968ms step_avg:59.95ms
step:1385/2420 train_time:83027ms step_avg:59.95ms
step:1386/2420 train_time:83088ms step_avg:59.95ms
step:1387/2420 train_time:83147ms step_avg:59.95ms
step:1388/2420 train_time:83208ms step_avg:59.95ms
step:1389/2420 train_time:83267ms step_avg:59.95ms
step:1390/2420 train_time:83328ms step_avg:59.95ms
step:1391/2420 train_time:83387ms step_avg:59.95ms
step:1392/2420 train_time:83448ms step_avg:59.95ms
step:1393/2420 train_time:83507ms step_avg:59.95ms
step:1394/2420 train_time:83568ms step_avg:59.95ms
step:1395/2420 train_time:83627ms step_avg:59.95ms
step:1396/2420 train_time:83689ms step_avg:59.95ms
step:1397/2420 train_time:83748ms step_avg:59.95ms
step:1398/2420 train_time:83809ms step_avg:59.95ms
step:1399/2420 train_time:83868ms step_avg:59.95ms
step:1400/2420 train_time:83929ms step_avg:59.95ms
step:1401/2420 train_time:83988ms step_avg:59.95ms
step:1402/2420 train_time:84049ms step_avg:59.95ms
step:1403/2420 train_time:84108ms step_avg:59.95ms
step:1404/2420 train_time:84170ms step_avg:59.95ms
step:1405/2420 train_time:84229ms step_avg:59.95ms
step:1406/2420 train_time:84290ms step_avg:59.95ms
step:1407/2420 train_time:84350ms step_avg:59.95ms
step:1408/2420 train_time:84410ms step_avg:59.95ms
step:1409/2420 train_time:84470ms step_avg:59.95ms
step:1410/2420 train_time:84531ms step_avg:59.95ms
step:1411/2420 train_time:84590ms step_avg:59.95ms
step:1412/2420 train_time:84652ms step_avg:59.95ms
step:1413/2420 train_time:84711ms step_avg:59.95ms
step:1414/2420 train_time:84772ms step_avg:59.95ms
step:1415/2420 train_time:84832ms step_avg:59.95ms
step:1416/2420 train_time:84894ms step_avg:59.95ms
step:1417/2420 train_time:84953ms step_avg:59.95ms
step:1418/2420 train_time:85015ms step_avg:59.95ms
step:1419/2420 train_time:85074ms step_avg:59.95ms
step:1420/2420 train_time:85136ms step_avg:59.95ms
step:1421/2420 train_time:85196ms step_avg:59.95ms
step:1422/2420 train_time:85257ms step_avg:59.96ms
step:1423/2420 train_time:85317ms step_avg:59.96ms
step:1424/2420 train_time:85379ms step_avg:59.96ms
step:1425/2420 train_time:85438ms step_avg:59.96ms
step:1426/2420 train_time:85500ms step_avg:59.96ms
step:1427/2420 train_time:85559ms step_avg:59.96ms
step:1428/2420 train_time:85621ms step_avg:59.96ms
step:1429/2420 train_time:85681ms step_avg:59.96ms
step:1430/2420 train_time:85743ms step_avg:59.96ms
step:1431/2420 train_time:85802ms step_avg:59.96ms
step:1432/2420 train_time:85863ms step_avg:59.96ms
step:1433/2420 train_time:85923ms step_avg:59.96ms
step:1434/2420 train_time:85984ms step_avg:59.96ms
step:1435/2420 train_time:86043ms step_avg:59.96ms
step:1436/2420 train_time:86104ms step_avg:59.96ms
step:1437/2420 train_time:86163ms step_avg:59.96ms
step:1438/2420 train_time:86224ms step_avg:59.96ms
step:1439/2420 train_time:86284ms step_avg:59.96ms
step:1440/2420 train_time:86345ms step_avg:59.96ms
step:1441/2420 train_time:86404ms step_avg:59.96ms
step:1442/2420 train_time:86465ms step_avg:59.96ms
step:1443/2420 train_time:86524ms step_avg:59.96ms
step:1444/2420 train_time:86585ms step_avg:59.96ms
step:1445/2420 train_time:86645ms step_avg:59.96ms
step:1446/2420 train_time:86706ms step_avg:59.96ms
step:1447/2420 train_time:86765ms step_avg:59.96ms
step:1448/2420 train_time:86826ms step_avg:59.96ms
step:1449/2420 train_time:86885ms step_avg:59.96ms
step:1450/2420 train_time:86946ms step_avg:59.96ms
step:1451/2420 train_time:87004ms step_avg:59.96ms
step:1452/2420 train_time:87065ms step_avg:59.96ms
step:1453/2420 train_time:87125ms step_avg:59.96ms
step:1454/2420 train_time:87186ms step_avg:59.96ms
step:1455/2420 train_time:87245ms step_avg:59.96ms
step:1456/2420 train_time:87305ms step_avg:59.96ms
step:1457/2420 train_time:87364ms step_avg:59.96ms
step:1458/2420 train_time:87425ms step_avg:59.96ms
step:1459/2420 train_time:87484ms step_avg:59.96ms
step:1460/2420 train_time:87545ms step_avg:59.96ms
step:1461/2420 train_time:87605ms step_avg:59.96ms
step:1462/2420 train_time:87666ms step_avg:59.96ms
step:1463/2420 train_time:87726ms step_avg:59.96ms
step:1464/2420 train_time:87787ms step_avg:59.96ms
step:1465/2420 train_time:87846ms step_avg:59.96ms
step:1466/2420 train_time:87906ms step_avg:59.96ms
step:1467/2420 train_time:87965ms step_avg:59.96ms
step:1468/2420 train_time:88026ms step_avg:59.96ms
step:1469/2420 train_time:88085ms step_avg:59.96ms
step:1470/2420 train_time:88146ms step_avg:59.96ms
step:1471/2420 train_time:88204ms step_avg:59.96ms
step:1472/2420 train_time:88266ms step_avg:59.96ms
step:1473/2420 train_time:88325ms step_avg:59.96ms
step:1474/2420 train_time:88386ms step_avg:59.96ms
step:1475/2420 train_time:88444ms step_avg:59.96ms
step:1476/2420 train_time:88505ms step_avg:59.96ms
step:1477/2420 train_time:88565ms step_avg:59.96ms
step:1478/2420 train_time:88625ms step_avg:59.96ms
step:1479/2420 train_time:88685ms step_avg:59.96ms
step:1480/2420 train_time:88746ms step_avg:59.96ms
step:1481/2420 train_time:88805ms step_avg:59.96ms
step:1482/2420 train_time:88866ms step_avg:59.96ms
step:1483/2420 train_time:88924ms step_avg:59.96ms
step:1484/2420 train_time:88985ms step_avg:59.96ms
step:1485/2420 train_time:89044ms step_avg:59.96ms
step:1486/2420 train_time:89105ms step_avg:59.96ms
step:1487/2420 train_time:89164ms step_avg:59.96ms
step:1488/2420 train_time:89225ms step_avg:59.96ms
step:1489/2420 train_time:89285ms step_avg:59.96ms
step:1490/2420 train_time:89345ms step_avg:59.96ms
step:1491/2420 train_time:89405ms step_avg:59.96ms
step:1492/2420 train_time:89466ms step_avg:59.96ms
step:1493/2420 train_time:89525ms step_avg:59.96ms
step:1494/2420 train_time:89586ms step_avg:59.96ms
step:1495/2420 train_time:89645ms step_avg:59.96ms
step:1496/2420 train_time:89706ms step_avg:59.96ms
step:1497/2420 train_time:89765ms step_avg:59.96ms
step:1498/2420 train_time:89826ms step_avg:59.96ms
step:1499/2420 train_time:89885ms step_avg:59.96ms
step:1500/2420 train_time:89945ms step_avg:59.96ms
step:1500/2420 val_loss:3.4761 train_time:90008ms step_avg:60.01ms
step:1501/2420 train_time:90030ms step_avg:59.98ms
step:1502/2420 train_time:90071ms step_avg:59.97ms
step:1503/2420 train_time:90134ms step_avg:59.97ms
step:1504/2420 train_time:90199ms step_avg:59.97ms
step:1505/2420 train_time:90261ms step_avg:59.97ms
step:1506/2420 train_time:90323ms step_avg:59.98ms
step:1507/2420 train_time:90383ms step_avg:59.98ms
step:1508/2420 train_time:90443ms step_avg:59.98ms
step:1509/2420 train_time:90502ms step_avg:59.97ms
step:1510/2420 train_time:90563ms step_avg:59.98ms
step:1511/2420 train_time:90622ms step_avg:59.97ms
step:1512/2420 train_time:90682ms step_avg:59.97ms
step:1513/2420 train_time:90741ms step_avg:59.97ms
step:1514/2420 train_time:90802ms step_avg:59.97ms
step:1515/2420 train_time:90861ms step_avg:59.97ms
step:1516/2420 train_time:90922ms step_avg:59.97ms
step:1517/2420 train_time:90982ms step_avg:59.97ms
step:1518/2420 train_time:91045ms step_avg:59.98ms
step:1519/2420 train_time:91105ms step_avg:59.98ms
step:1520/2420 train_time:91167ms step_avg:59.98ms
step:1521/2420 train_time:91227ms step_avg:59.98ms
step:1522/2420 train_time:91289ms step_avg:59.98ms
step:1523/2420 train_time:91349ms step_avg:59.98ms
step:1524/2420 train_time:91410ms step_avg:59.98ms
step:1525/2420 train_time:91469ms step_avg:59.98ms
step:1526/2420 train_time:91531ms step_avg:59.98ms
step:1527/2420 train_time:91590ms step_avg:59.98ms
step:1528/2420 train_time:91652ms step_avg:59.98ms
step:1529/2420 train_time:91711ms step_avg:59.98ms
step:1530/2420 train_time:91772ms step_avg:59.98ms
step:1531/2420 train_time:91831ms step_avg:59.98ms
step:1532/2420 train_time:91893ms step_avg:59.98ms
step:1533/2420 train_time:91952ms step_avg:59.98ms
step:1534/2420 train_time:92014ms step_avg:59.98ms
step:1535/2420 train_time:92074ms step_avg:59.98ms
step:1536/2420 train_time:92136ms step_avg:59.98ms
step:1537/2420 train_time:92196ms step_avg:59.98ms
step:1538/2420 train_time:92258ms step_avg:59.99ms
step:1539/2420 train_time:92318ms step_avg:59.99ms
step:1540/2420 train_time:92380ms step_avg:59.99ms
step:1541/2420 train_time:92440ms step_avg:59.99ms
step:1542/2420 train_time:92502ms step_avg:59.99ms
step:1543/2420 train_time:92561ms step_avg:59.99ms
step:1544/2420 train_time:92623ms step_avg:59.99ms
step:1545/2420 train_time:92682ms step_avg:59.99ms
step:1546/2420 train_time:92743ms step_avg:59.99ms
step:1547/2420 train_time:92803ms step_avg:59.99ms
step:1548/2420 train_time:92864ms step_avg:59.99ms
step:1549/2420 train_time:92923ms step_avg:59.99ms
step:1550/2420 train_time:92984ms step_avg:59.99ms
step:1551/2420 train_time:93043ms step_avg:59.99ms
step:1552/2420 train_time:93104ms step_avg:59.99ms
step:1553/2420 train_time:93164ms step_avg:59.99ms
step:1554/2420 train_time:93225ms step_avg:59.99ms
step:1555/2420 train_time:93285ms step_avg:59.99ms
step:1556/2420 train_time:93346ms step_avg:59.99ms
step:1557/2420 train_time:93405ms step_avg:59.99ms
step:1558/2420 train_time:93466ms step_avg:59.99ms
step:1559/2420 train_time:93526ms step_avg:59.99ms
step:1560/2420 train_time:93587ms step_avg:59.99ms
step:1561/2420 train_time:93646ms step_avg:59.99ms
step:1562/2420 train_time:93707ms step_avg:59.99ms
step:1563/2420 train_time:93766ms step_avg:59.99ms
step:1564/2420 train_time:93827ms step_avg:59.99ms
step:1565/2420 train_time:93886ms step_avg:59.99ms
step:1566/2420 train_time:93947ms step_avg:59.99ms
step:1567/2420 train_time:94006ms step_avg:59.99ms
step:1568/2420 train_time:94067ms step_avg:59.99ms
step:1569/2420 train_time:94126ms step_avg:59.99ms
step:1570/2420 train_time:94187ms step_avg:59.99ms
step:1571/2420 train_time:94247ms step_avg:59.99ms
step:1572/2420 train_time:94308ms step_avg:59.99ms
step:1573/2420 train_time:94367ms step_avg:59.99ms
step:1574/2420 train_time:94429ms step_avg:59.99ms
step:1575/2420 train_time:94488ms step_avg:59.99ms
step:1576/2420 train_time:94549ms step_avg:59.99ms
step:1577/2420 train_time:94608ms step_avg:59.99ms
step:1578/2420 train_time:94670ms step_avg:59.99ms
step:1579/2420 train_time:94729ms step_avg:59.99ms
step:1580/2420 train_time:94790ms step_avg:59.99ms
step:1581/2420 train_time:94849ms step_avg:59.99ms
step:1582/2420 train_time:94910ms step_avg:59.99ms
step:1583/2420 train_time:94969ms step_avg:59.99ms
step:1584/2420 train_time:95031ms step_avg:59.99ms
step:1585/2420 train_time:95090ms step_avg:59.99ms
step:1586/2420 train_time:95152ms step_avg:60.00ms
step:1587/2420 train_time:95212ms step_avg:59.99ms
step:1588/2420 train_time:95273ms step_avg:60.00ms
step:1589/2420 train_time:95333ms step_avg:60.00ms
step:1590/2420 train_time:95395ms step_avg:60.00ms
step:1591/2420 train_time:95455ms step_avg:60.00ms
step:1592/2420 train_time:95517ms step_avg:60.00ms
step:1593/2420 train_time:95577ms step_avg:60.00ms
step:1594/2420 train_time:95639ms step_avg:60.00ms
step:1595/2420 train_time:95700ms step_avg:60.00ms
step:1596/2420 train_time:95762ms step_avg:60.00ms
step:1597/2420 train_time:95822ms step_avg:60.00ms
step:1598/2420 train_time:95884ms step_avg:60.00ms
step:1599/2420 train_time:95944ms step_avg:60.00ms
step:1600/2420 train_time:96005ms step_avg:60.00ms
step:1601/2420 train_time:96064ms step_avg:60.00ms
step:1602/2420 train_time:96126ms step_avg:60.00ms
step:1603/2420 train_time:96185ms step_avg:60.00ms
step:1604/2420 train_time:96246ms step_avg:60.00ms
step:1605/2420 train_time:96305ms step_avg:60.00ms
step:1606/2420 train_time:96367ms step_avg:60.00ms
step:1607/2420 train_time:96426ms step_avg:60.00ms
step:1608/2420 train_time:96488ms step_avg:60.00ms
step:1609/2420 train_time:96548ms step_avg:60.01ms
step:1610/2420 train_time:96610ms step_avg:60.01ms
step:1611/2420 train_time:96670ms step_avg:60.01ms
step:1612/2420 train_time:96731ms step_avg:60.01ms
step:1613/2420 train_time:96791ms step_avg:60.01ms
step:1614/2420 train_time:96854ms step_avg:60.01ms
step:1615/2420 train_time:96914ms step_avg:60.01ms
step:1616/2420 train_time:96975ms step_avg:60.01ms
step:1617/2420 train_time:97035ms step_avg:60.01ms
step:1618/2420 train_time:97097ms step_avg:60.01ms
step:1619/2420 train_time:97157ms step_avg:60.01ms
step:1620/2420 train_time:97219ms step_avg:60.01ms
step:1621/2420 train_time:97279ms step_avg:60.01ms
step:1622/2420 train_time:97341ms step_avg:60.01ms
step:1623/2420 train_time:97401ms step_avg:60.01ms
step:1624/2420 train_time:97463ms step_avg:60.01ms
step:1625/2420 train_time:97524ms step_avg:60.01ms
step:1626/2420 train_time:97585ms step_avg:60.02ms
step:1627/2420 train_time:97645ms step_avg:60.02ms
step:1628/2420 train_time:97707ms step_avg:60.02ms
step:1629/2420 train_time:97766ms step_avg:60.02ms
step:1630/2420 train_time:97828ms step_avg:60.02ms
step:1631/2420 train_time:97887ms step_avg:60.02ms
step:1632/2420 train_time:97948ms step_avg:60.02ms
step:1633/2420 train_time:98008ms step_avg:60.02ms
step:1634/2420 train_time:98070ms step_avg:60.02ms
step:1635/2420 train_time:98130ms step_avg:60.02ms
step:1636/2420 train_time:98191ms step_avg:60.02ms
step:1637/2420 train_time:98251ms step_avg:60.02ms
step:1638/2420 train_time:98313ms step_avg:60.02ms
step:1639/2420 train_time:98373ms step_avg:60.02ms
step:1640/2420 train_time:98435ms step_avg:60.02ms
step:1641/2420 train_time:98495ms step_avg:60.02ms
step:1642/2420 train_time:98557ms step_avg:60.02ms
step:1643/2420 train_time:98617ms step_avg:60.02ms
step:1644/2420 train_time:98679ms step_avg:60.02ms
step:1645/2420 train_time:98739ms step_avg:60.02ms
step:1646/2420 train_time:98801ms step_avg:60.03ms
step:1647/2420 train_time:98862ms step_avg:60.03ms
step:1648/2420 train_time:98924ms step_avg:60.03ms
step:1649/2420 train_time:98984ms step_avg:60.03ms
step:1650/2420 train_time:99045ms step_avg:60.03ms
step:1651/2420 train_time:99104ms step_avg:60.03ms
step:1652/2420 train_time:99166ms step_avg:60.03ms
step:1653/2420 train_time:99225ms step_avg:60.03ms
step:1654/2420 train_time:99286ms step_avg:60.03ms
step:1655/2420 train_time:99346ms step_avg:60.03ms
step:1656/2420 train_time:99408ms step_avg:60.03ms
step:1657/2420 train_time:99467ms step_avg:60.03ms
step:1658/2420 train_time:99529ms step_avg:60.03ms
step:1659/2420 train_time:99588ms step_avg:60.03ms
step:1660/2420 train_time:99650ms step_avg:60.03ms
step:1661/2420 train_time:99710ms step_avg:60.03ms
step:1662/2420 train_time:99772ms step_avg:60.03ms
step:1663/2420 train_time:99833ms step_avg:60.03ms
step:1664/2420 train_time:99895ms step_avg:60.03ms
step:1665/2420 train_time:99955ms step_avg:60.03ms
step:1666/2420 train_time:100017ms step_avg:60.03ms
step:1667/2420 train_time:100077ms step_avg:60.03ms
step:1668/2420 train_time:100139ms step_avg:60.04ms
step:1669/2420 train_time:100199ms step_avg:60.04ms
step:1670/2420 train_time:100261ms step_avg:60.04ms
step:1671/2420 train_time:100320ms step_avg:60.04ms
step:1672/2420 train_time:100382ms step_avg:60.04ms
step:1673/2420 train_time:100442ms step_avg:60.04ms
step:1674/2420 train_time:100504ms step_avg:60.04ms
step:1675/2420 train_time:100564ms step_avg:60.04ms
step:1676/2420 train_time:100626ms step_avg:60.04ms
step:1677/2420 train_time:100686ms step_avg:60.04ms
step:1678/2420 train_time:100747ms step_avg:60.04ms
step:1679/2420 train_time:100807ms step_avg:60.04ms
step:1680/2420 train_time:100868ms step_avg:60.04ms
step:1681/2420 train_time:100928ms step_avg:60.04ms
step:1682/2420 train_time:100989ms step_avg:60.04ms
step:1683/2420 train_time:101050ms step_avg:60.04ms
step:1684/2420 train_time:101111ms step_avg:60.04ms
step:1685/2420 train_time:101171ms step_avg:60.04ms
step:1686/2420 train_time:101233ms step_avg:60.04ms
step:1687/2420 train_time:101293ms step_avg:60.04ms
step:1688/2420 train_time:101355ms step_avg:60.04ms
step:1689/2420 train_time:101415ms step_avg:60.04ms
step:1690/2420 train_time:101477ms step_avg:60.05ms
step:1691/2420 train_time:101538ms step_avg:60.05ms
step:1692/2420 train_time:101600ms step_avg:60.05ms
step:1693/2420 train_time:101660ms step_avg:60.05ms
step:1694/2420 train_time:101723ms step_avg:60.05ms
step:1695/2420 train_time:101783ms step_avg:60.05ms
step:1696/2420 train_time:101844ms step_avg:60.05ms
step:1697/2420 train_time:101904ms step_avg:60.05ms
step:1698/2420 train_time:101966ms step_avg:60.05ms
step:1699/2420 train_time:102025ms step_avg:60.05ms
step:1700/2420 train_time:102087ms step_avg:60.05ms
step:1701/2420 train_time:102146ms step_avg:60.05ms
step:1702/2420 train_time:102207ms step_avg:60.05ms
step:1703/2420 train_time:102267ms step_avg:60.05ms
step:1704/2420 train_time:102329ms step_avg:60.05ms
step:1705/2420 train_time:102390ms step_avg:60.05ms
step:1706/2420 train_time:102451ms step_avg:60.05ms
step:1707/2420 train_time:102511ms step_avg:60.05ms
step:1708/2420 train_time:102573ms step_avg:60.05ms
step:1709/2420 train_time:102634ms step_avg:60.05ms
step:1710/2420 train_time:102695ms step_avg:60.06ms
step:1711/2420 train_time:102755ms step_avg:60.06ms
step:1712/2420 train_time:102817ms step_avg:60.06ms
step:1713/2420 train_time:102877ms step_avg:60.06ms
step:1714/2420 train_time:102939ms step_avg:60.06ms
step:1715/2420 train_time:102999ms step_avg:60.06ms
step:1716/2420 train_time:103061ms step_avg:60.06ms
step:1717/2420 train_time:103122ms step_avg:60.06ms
step:1718/2420 train_time:103184ms step_avg:60.06ms
step:1719/2420 train_time:103244ms step_avg:60.06ms
step:1720/2420 train_time:103306ms step_avg:60.06ms
step:1721/2420 train_time:103365ms step_avg:60.06ms
step:1722/2420 train_time:103426ms step_avg:60.06ms
step:1723/2420 train_time:103485ms step_avg:60.06ms
step:1724/2420 train_time:103547ms step_avg:60.06ms
step:1725/2420 train_time:103607ms step_avg:60.06ms
step:1726/2420 train_time:103668ms step_avg:60.06ms
step:1727/2420 train_time:103728ms step_avg:60.06ms
step:1728/2420 train_time:103790ms step_avg:60.06ms
step:1729/2420 train_time:103851ms step_avg:60.06ms
step:1730/2420 train_time:103913ms step_avg:60.07ms
step:1731/2420 train_time:103973ms step_avg:60.07ms
step:1732/2420 train_time:104035ms step_avg:60.07ms
step:1733/2420 train_time:104095ms step_avg:60.07ms
step:1734/2420 train_time:104157ms step_avg:60.07ms
step:1735/2420 train_time:104217ms step_avg:60.07ms
step:1736/2420 train_time:104279ms step_avg:60.07ms
step:1737/2420 train_time:104339ms step_avg:60.07ms
step:1738/2420 train_time:104401ms step_avg:60.07ms
step:1739/2420 train_time:104461ms step_avg:60.07ms
step:1740/2420 train_time:104524ms step_avg:60.07ms
step:1741/2420 train_time:104583ms step_avg:60.07ms
step:1742/2420 train_time:104645ms step_avg:60.07ms
step:1743/2420 train_time:104704ms step_avg:60.07ms
step:1744/2420 train_time:104766ms step_avg:60.07ms
step:1745/2420 train_time:104825ms step_avg:60.07ms
step:1746/2420 train_time:104887ms step_avg:60.07ms
step:1747/2420 train_time:104946ms step_avg:60.07ms
step:1748/2420 train_time:105007ms step_avg:60.07ms
step:1749/2420 train_time:105067ms step_avg:60.07ms
step:1750/2420 train_time:105129ms step_avg:60.07ms
step:1750/2420 val_loss:3.4036 train_time:105192ms step_avg:60.11ms
step:1751/2420 train_time:105214ms step_avg:60.09ms
step:1752/2420 train_time:105255ms step_avg:60.08ms
step:1753/2420 train_time:105316ms step_avg:60.08ms
step:1754/2420 train_time:105379ms step_avg:60.08ms
step:1755/2420 train_time:105441ms step_avg:60.08ms
step:1756/2420 train_time:105504ms step_avg:60.08ms
step:1757/2420 train_time:105563ms step_avg:60.08ms
step:1758/2420 train_time:105625ms step_avg:60.08ms
step:1759/2420 train_time:105684ms step_avg:60.08ms
step:1760/2420 train_time:105745ms step_avg:60.08ms
step:1761/2420 train_time:105804ms step_avg:60.08ms
step:1762/2420 train_time:105865ms step_avg:60.08ms
step:1763/2420 train_time:105924ms step_avg:60.08ms
step:1764/2420 train_time:105986ms step_avg:60.08ms
step:1765/2420 train_time:106045ms step_avg:60.08ms
step:1766/2420 train_time:106108ms step_avg:60.08ms
step:1767/2420 train_time:106172ms step_avg:60.09ms
step:1768/2420 train_time:106235ms step_avg:60.09ms
step:1769/2420 train_time:106296ms step_avg:60.09ms
step:1770/2420 train_time:106357ms step_avg:60.09ms
step:1771/2420 train_time:106418ms step_avg:60.09ms
step:1772/2420 train_time:106480ms step_avg:60.09ms
step:1773/2420 train_time:106539ms step_avg:60.09ms
step:1774/2420 train_time:106601ms step_avg:60.09ms
step:1775/2420 train_time:106661ms step_avg:60.09ms
step:1776/2420 train_time:106722ms step_avg:60.09ms
step:1777/2420 train_time:106782ms step_avg:60.09ms
step:1778/2420 train_time:106843ms step_avg:60.09ms
step:1779/2420 train_time:106902ms step_avg:60.09ms
step:1780/2420 train_time:106963ms step_avg:60.09ms
step:1781/2420 train_time:107023ms step_avg:60.09ms
step:1782/2420 train_time:107086ms step_avg:60.09ms
step:1783/2420 train_time:107147ms step_avg:60.09ms
step:1784/2420 train_time:107210ms step_avg:60.10ms
step:1785/2420 train_time:107271ms step_avg:60.10ms
step:1786/2420 train_time:107333ms step_avg:60.10ms
step:1787/2420 train_time:107393ms step_avg:60.10ms
step:1788/2420 train_time:107454ms step_avg:60.10ms
step:1789/2420 train_time:107513ms step_avg:60.10ms
step:1790/2420 train_time:107575ms step_avg:60.10ms
step:1791/2420 train_time:107634ms step_avg:60.10ms
step:1792/2420 train_time:107695ms step_avg:60.10ms
step:1793/2420 train_time:107755ms step_avg:60.10ms
step:1794/2420 train_time:107817ms step_avg:60.10ms
step:1795/2420 train_time:107877ms step_avg:60.10ms
step:1796/2420 train_time:107938ms step_avg:60.10ms
step:1797/2420 train_time:107998ms step_avg:60.10ms
step:1798/2420 train_time:108061ms step_avg:60.10ms
step:1799/2420 train_time:108121ms step_avg:60.10ms
step:1800/2420 train_time:108183ms step_avg:60.10ms
step:1801/2420 train_time:108244ms step_avg:60.10ms
step:1802/2420 train_time:108308ms step_avg:60.10ms
step:1803/2420 train_time:108369ms step_avg:60.10ms
step:1804/2420 train_time:108431ms step_avg:60.11ms
step:1805/2420 train_time:108491ms step_avg:60.11ms
step:1806/2420 train_time:108552ms step_avg:60.11ms
step:1807/2420 train_time:108611ms step_avg:60.11ms
step:1808/2420 train_time:108672ms step_avg:60.11ms
step:1809/2420 train_time:108732ms step_avg:60.11ms
step:1810/2420 train_time:108793ms step_avg:60.11ms
step:1811/2420 train_time:108852ms step_avg:60.11ms
step:1812/2420 train_time:108914ms step_avg:60.11ms
step:1813/2420 train_time:108974ms step_avg:60.11ms
step:1814/2420 train_time:109035ms step_avg:60.11ms
step:1815/2420 train_time:109095ms step_avg:60.11ms
step:1816/2420 train_time:109157ms step_avg:60.11ms
step:1817/2420 train_time:109217ms step_avg:60.11ms
step:1818/2420 train_time:109279ms step_avg:60.11ms
step:1819/2420 train_time:109340ms step_avg:60.11ms
step:1820/2420 train_time:109402ms step_avg:60.11ms
step:1821/2420 train_time:109462ms step_avg:60.11ms
step:1822/2420 train_time:109524ms step_avg:60.11ms
step:1823/2420 train_time:109584ms step_avg:60.11ms
step:1824/2420 train_time:109646ms step_avg:60.11ms
step:1825/2420 train_time:109706ms step_avg:60.11ms
step:1826/2420 train_time:109768ms step_avg:60.11ms
step:1827/2420 train_time:109828ms step_avg:60.11ms
step:1828/2420 train_time:109890ms step_avg:60.11ms
step:1829/2420 train_time:109950ms step_avg:60.11ms
step:1830/2420 train_time:110011ms step_avg:60.12ms
step:1831/2420 train_time:110071ms step_avg:60.12ms
step:1832/2420 train_time:110132ms step_avg:60.12ms
step:1833/2420 train_time:110192ms step_avg:60.12ms
step:1834/2420 train_time:110253ms step_avg:60.12ms
step:1835/2420 train_time:110312ms step_avg:60.12ms
step:1836/2420 train_time:110374ms step_avg:60.12ms
step:1837/2420 train_time:110433ms step_avg:60.12ms
step:1838/2420 train_time:110495ms step_avg:60.12ms
step:1839/2420 train_time:110555ms step_avg:60.12ms
step:1840/2420 train_time:110617ms step_avg:60.12ms
step:1841/2420 train_time:110677ms step_avg:60.12ms
step:1842/2420 train_time:110739ms step_avg:60.12ms
step:1843/2420 train_time:110798ms step_avg:60.12ms
step:1844/2420 train_time:110860ms step_avg:60.12ms
step:1845/2420 train_time:110920ms step_avg:60.12ms
step:1846/2420 train_time:110981ms step_avg:60.12ms
step:1847/2420 train_time:111041ms step_avg:60.12ms
step:1848/2420 train_time:111103ms step_avg:60.12ms
step:1849/2420 train_time:111163ms step_avg:60.12ms
step:1850/2420 train_time:111225ms step_avg:60.12ms
step:1851/2420 train_time:111285ms step_avg:60.12ms
step:1852/2420 train_time:111346ms step_avg:60.12ms
step:1853/2420 train_time:111406ms step_avg:60.12ms
step:1854/2420 train_time:111468ms step_avg:60.12ms
step:1855/2420 train_time:111528ms step_avg:60.12ms
step:1856/2420 train_time:111591ms step_avg:60.12ms
step:1857/2420 train_time:111651ms step_avg:60.12ms
step:1858/2420 train_time:111712ms step_avg:60.13ms
step:1859/2420 train_time:111772ms step_avg:60.12ms
step:1860/2420 train_time:111833ms step_avg:60.13ms
step:1861/2420 train_time:111892ms step_avg:60.12ms
step:1862/2420 train_time:111953ms step_avg:60.13ms
step:1863/2420 train_time:112012ms step_avg:60.12ms
step:1864/2420 train_time:112074ms step_avg:60.13ms
step:1865/2420 train_time:112134ms step_avg:60.13ms
step:1866/2420 train_time:112195ms step_avg:60.13ms
step:1867/2420 train_time:112255ms step_avg:60.13ms
step:1868/2420 train_time:112316ms step_avg:60.13ms
step:1869/2420 train_time:112376ms step_avg:60.13ms
step:1870/2420 train_time:112438ms step_avg:60.13ms
step:1871/2420 train_time:112498ms step_avg:60.13ms
step:1872/2420 train_time:112560ms step_avg:60.13ms
step:1873/2420 train_time:112620ms step_avg:60.13ms
step:1874/2420 train_time:112682ms step_avg:60.13ms
step:1875/2420 train_time:112742ms step_avg:60.13ms
step:1876/2420 train_time:112804ms step_avg:60.13ms
step:1877/2420 train_time:112864ms step_avg:60.13ms
step:1878/2420 train_time:112925ms step_avg:60.13ms
step:1879/2420 train_time:112985ms step_avg:60.13ms
step:1880/2420 train_time:113047ms step_avg:60.13ms
step:1881/2420 train_time:113107ms step_avg:60.13ms
step:1882/2420 train_time:113169ms step_avg:60.13ms
step:1883/2420 train_time:113229ms step_avg:60.13ms
step:1884/2420 train_time:113291ms step_avg:60.13ms
step:1885/2420 train_time:113350ms step_avg:60.13ms
step:1886/2420 train_time:113411ms step_avg:60.13ms
step:1887/2420 train_time:113471ms step_avg:60.13ms
step:1888/2420 train_time:113532ms step_avg:60.13ms
step:1889/2420 train_time:113592ms step_avg:60.13ms
step:1890/2420 train_time:113653ms step_avg:60.13ms
step:1891/2420 train_time:113713ms step_avg:60.13ms
step:1892/2420 train_time:113774ms step_avg:60.13ms
step:1893/2420 train_time:113833ms step_avg:60.13ms
step:1894/2420 train_time:113895ms step_avg:60.13ms
step:1895/2420 train_time:113955ms step_avg:60.13ms
step:1896/2420 train_time:114017ms step_avg:60.14ms
step:1897/2420 train_time:114077ms step_avg:60.14ms
step:1898/2420 train_time:114139ms step_avg:60.14ms
step:1899/2420 train_time:114199ms step_avg:60.14ms
step:1900/2420 train_time:114261ms step_avg:60.14ms
step:1901/2420 train_time:114321ms step_avg:60.14ms
step:1902/2420 train_time:114382ms step_avg:60.14ms
step:1903/2420 train_time:114442ms step_avg:60.14ms
step:1904/2420 train_time:114504ms step_avg:60.14ms
step:1905/2420 train_time:114564ms step_avg:60.14ms
step:1906/2420 train_time:114626ms step_avg:60.14ms
step:1907/2420 train_time:114687ms step_avg:60.14ms
step:1908/2420 train_time:114748ms step_avg:60.14ms
step:1909/2420 train_time:114808ms step_avg:60.14ms
step:1910/2420 train_time:114870ms step_avg:60.14ms
step:1911/2420 train_time:114930ms step_avg:60.14ms
step:1912/2420 train_time:114992ms step_avg:60.14ms
step:1913/2420 train_time:115052ms step_avg:60.14ms
step:1914/2420 train_time:115113ms step_avg:60.14ms
step:1915/2420 train_time:115172ms step_avg:60.14ms
step:1916/2420 train_time:115234ms step_avg:60.14ms
step:1917/2420 train_time:115293ms step_avg:60.14ms
step:1918/2420 train_time:115355ms step_avg:60.14ms
step:1919/2420 train_time:115414ms step_avg:60.14ms
step:1920/2420 train_time:115476ms step_avg:60.14ms
step:1921/2420 train_time:115536ms step_avg:60.14ms
step:1922/2420 train_time:115598ms step_avg:60.14ms
step:1923/2420 train_time:115658ms step_avg:60.14ms
step:1924/2420 train_time:115720ms step_avg:60.15ms
step:1925/2420 train_time:115780ms step_avg:60.15ms
step:1926/2420 train_time:115842ms step_avg:60.15ms
step:1927/2420 train_time:115901ms step_avg:60.15ms
step:1928/2420 train_time:115963ms step_avg:60.15ms
step:1929/2420 train_time:116023ms step_avg:60.15ms
step:1930/2420 train_time:116085ms step_avg:60.15ms
step:1931/2420 train_time:116145ms step_avg:60.15ms
step:1932/2420 train_time:116207ms step_avg:60.15ms
step:1933/2420 train_time:116267ms step_avg:60.15ms
step:1934/2420 train_time:116329ms step_avg:60.15ms
step:1935/2420 train_time:116389ms step_avg:60.15ms
step:1936/2420 train_time:116450ms step_avg:60.15ms
step:1937/2420 train_time:116510ms step_avg:60.15ms
step:1938/2420 train_time:116572ms step_avg:60.15ms
step:1939/2420 train_time:116632ms step_avg:60.15ms
step:1940/2420 train_time:116693ms step_avg:60.15ms
step:1941/2420 train_time:116753ms step_avg:60.15ms
step:1942/2420 train_time:116815ms step_avg:60.15ms
step:1943/2420 train_time:116874ms step_avg:60.15ms
step:1944/2420 train_time:116935ms step_avg:60.15ms
step:1945/2420 train_time:116995ms step_avg:60.15ms
step:1946/2420 train_time:117056ms step_avg:60.15ms
step:1947/2420 train_time:117117ms step_avg:60.15ms
step:1948/2420 train_time:117178ms step_avg:60.15ms
step:1949/2420 train_time:117238ms step_avg:60.15ms
step:1950/2420 train_time:117300ms step_avg:60.15ms
step:1951/2420 train_time:117360ms step_avg:60.15ms
step:1952/2420 train_time:117422ms step_avg:60.15ms
step:1953/2420 train_time:117482ms step_avg:60.15ms
step:1954/2420 train_time:117544ms step_avg:60.16ms
step:1955/2420 train_time:117604ms step_avg:60.16ms
step:1956/2420 train_time:117665ms step_avg:60.16ms
step:1957/2420 train_time:117726ms step_avg:60.16ms
step:1958/2420 train_time:117788ms step_avg:60.16ms
step:1959/2420 train_time:117848ms step_avg:60.16ms
step:1960/2420 train_time:117910ms step_avg:60.16ms
step:1961/2420 train_time:117970ms step_avg:60.16ms
step:1962/2420 train_time:118031ms step_avg:60.16ms
step:1963/2420 train_time:118091ms step_avg:60.16ms
step:1964/2420 train_time:118153ms step_avg:60.16ms
step:1965/2420 train_time:118212ms step_avg:60.16ms
step:1966/2420 train_time:118273ms step_avg:60.16ms
step:1967/2420 train_time:118333ms step_avg:60.16ms
step:1968/2420 train_time:118394ms step_avg:60.16ms
step:1969/2420 train_time:118454ms step_avg:60.16ms
step:1970/2420 train_time:118516ms step_avg:60.16ms
step:1971/2420 train_time:118576ms step_avg:60.16ms
step:1972/2420 train_time:118637ms step_avg:60.16ms
step:1973/2420 train_time:118697ms step_avg:60.16ms
step:1974/2420 train_time:118759ms step_avg:60.16ms
step:1975/2420 train_time:118820ms step_avg:60.16ms
step:1976/2420 train_time:118882ms step_avg:60.16ms
step:1977/2420 train_time:118941ms step_avg:60.16ms
step:1978/2420 train_time:119003ms step_avg:60.16ms
step:1979/2420 train_time:119063ms step_avg:60.16ms
step:1980/2420 train_time:119125ms step_avg:60.16ms
step:1981/2420 train_time:119185ms step_avg:60.16ms
step:1982/2420 train_time:119247ms step_avg:60.17ms
step:1983/2420 train_time:119307ms step_avg:60.16ms
step:1984/2420 train_time:119370ms step_avg:60.17ms
step:1985/2420 train_time:119430ms step_avg:60.17ms
step:1986/2420 train_time:119492ms step_avg:60.17ms
step:1987/2420 train_time:119552ms step_avg:60.17ms
step:1988/2420 train_time:119613ms step_avg:60.17ms
step:1989/2420 train_time:119672ms step_avg:60.17ms
step:1990/2420 train_time:119733ms step_avg:60.17ms
step:1991/2420 train_time:119793ms step_avg:60.17ms
step:1992/2420 train_time:119855ms step_avg:60.17ms
step:1993/2420 train_time:119914ms step_avg:60.17ms
step:1994/2420 train_time:119976ms step_avg:60.17ms
step:1995/2420 train_time:120036ms step_avg:60.17ms
step:1996/2420 train_time:120098ms step_avg:60.17ms
step:1997/2420 train_time:120157ms step_avg:60.17ms
step:1998/2420 train_time:120219ms step_avg:60.17ms
step:1999/2420 train_time:120279ms step_avg:60.17ms
step:2000/2420 train_time:120341ms step_avg:60.17ms
step:2000/2420 val_loss:3.3493 train_time:120405ms step_avg:60.20ms
step:2001/2420 train_time:120427ms step_avg:60.18ms
step:2002/2420 train_time:120467ms step_avg:60.17ms
step:2003/2420 train_time:120529ms step_avg:60.17ms
step:2004/2420 train_time:120596ms step_avg:60.18ms
step:2005/2420 train_time:120656ms step_avg:60.18ms
step:2006/2420 train_time:120718ms step_avg:60.18ms
step:2007/2420 train_time:120777ms step_avg:60.18ms
step:2008/2420 train_time:120838ms step_avg:60.18ms
step:2009/2420 train_time:120897ms step_avg:60.18ms
step:2010/2420 train_time:120958ms step_avg:60.18ms
step:2011/2420 train_time:121018ms step_avg:60.18ms
step:2012/2420 train_time:121079ms step_avg:60.18ms
step:2013/2420 train_time:121138ms step_avg:60.18ms
step:2014/2420 train_time:121200ms step_avg:60.18ms
step:2015/2420 train_time:121260ms step_avg:60.18ms
step:2016/2420 train_time:121321ms step_avg:60.18ms
step:2017/2420 train_time:121382ms step_avg:60.18ms
step:2018/2420 train_time:121445ms step_avg:60.18ms
step:2019/2420 train_time:121505ms step_avg:60.18ms
step:2020/2420 train_time:121567ms step_avg:60.18ms
step:2021/2420 train_time:121628ms step_avg:60.18ms
step:2022/2420 train_time:121690ms step_avg:60.18ms
step:2023/2420 train_time:121750ms step_avg:60.18ms
step:2024/2420 train_time:121812ms step_avg:60.18ms
step:2025/2420 train_time:121872ms step_avg:60.18ms
step:2026/2420 train_time:121933ms step_avg:60.18ms
step:2027/2420 train_time:121994ms step_avg:60.18ms
step:2028/2420 train_time:122055ms step_avg:60.18ms
step:2029/2420 train_time:122114ms step_avg:60.18ms
step:2030/2420 train_time:122176ms step_avg:60.19ms
step:2031/2420 train_time:122235ms step_avg:60.18ms
step:2032/2420 train_time:122297ms step_avg:60.19ms
step:2033/2420 train_time:122358ms step_avg:60.19ms
step:2034/2420 train_time:122421ms step_avg:60.19ms
step:2035/2420 train_time:122482ms step_avg:60.19ms
step:2036/2420 train_time:122544ms step_avg:60.19ms
step:2037/2420 train_time:122605ms step_avg:60.19ms
step:2038/2420 train_time:122666ms step_avg:60.19ms
step:2039/2420 train_time:122726ms step_avg:60.19ms
step:2040/2420 train_time:122788ms step_avg:60.19ms
step:2041/2420 train_time:122847ms step_avg:60.19ms
step:2042/2420 train_time:122909ms step_avg:60.19ms
step:2043/2420 train_time:122969ms step_avg:60.19ms
step:2044/2420 train_time:123031ms step_avg:60.19ms
step:2045/2420 train_time:123092ms step_avg:60.19ms
step:2046/2420 train_time:123153ms step_avg:60.19ms
step:2047/2420 train_time:123213ms step_avg:60.19ms
step:2048/2420 train_time:123274ms step_avg:60.19ms
step:2049/2420 train_time:123334ms step_avg:60.19ms
step:2050/2420 train_time:123397ms step_avg:60.19ms
step:2051/2420 train_time:123459ms step_avg:60.19ms
step:2052/2420 train_time:123522ms step_avg:60.20ms
step:2053/2420 train_time:123581ms step_avg:60.20ms
step:2054/2420 train_time:123643ms step_avg:60.20ms
step:2055/2420 train_time:123703ms step_avg:60.20ms
step:2056/2420 train_time:123765ms step_avg:60.20ms
step:2057/2420 train_time:123825ms step_avg:60.20ms
step:2058/2420 train_time:123886ms step_avg:60.20ms
step:2059/2420 train_time:123945ms step_avg:60.20ms
step:2060/2420 train_time:124007ms step_avg:60.20ms
step:2061/2420 train_time:124067ms step_avg:60.20ms
step:2062/2420 train_time:124128ms step_avg:60.20ms
step:2063/2420 train_time:124188ms step_avg:60.20ms
step:2064/2420 train_time:124251ms step_avg:60.20ms
step:2065/2420 train_time:124311ms step_avg:60.20ms
step:2066/2420 train_time:124373ms step_avg:60.20ms
step:2067/2420 train_time:124433ms step_avg:60.20ms
step:2068/2420 train_time:124496ms step_avg:60.20ms
step:2069/2420 train_time:124557ms step_avg:60.20ms
step:2070/2420 train_time:124620ms step_avg:60.20ms
step:2071/2420 train_time:124680ms step_avg:60.20ms
step:2072/2420 train_time:124742ms step_avg:60.20ms
step:2073/2420 train_time:124802ms step_avg:60.20ms
step:2074/2420 train_time:124863ms step_avg:60.20ms
step:2075/2420 train_time:124923ms step_avg:60.20ms
step:2076/2420 train_time:124984ms step_avg:60.20ms
step:2077/2420 train_time:125043ms step_avg:60.20ms
step:2078/2420 train_time:125105ms step_avg:60.20ms
step:2079/2420 train_time:125164ms step_avg:60.20ms
step:2080/2420 train_time:125226ms step_avg:60.20ms
step:2081/2420 train_time:125286ms step_avg:60.20ms
step:2082/2420 train_time:125348ms step_avg:60.21ms
step:2083/2420 train_time:125408ms step_avg:60.21ms
step:2084/2420 train_time:125471ms step_avg:60.21ms
step:2085/2420 train_time:125531ms step_avg:60.21ms
step:2086/2420 train_time:125593ms step_avg:60.21ms
step:2087/2420 train_time:125653ms step_avg:60.21ms
step:2088/2420 train_time:125716ms step_avg:60.21ms
step:2089/2420 train_time:125776ms step_avg:60.21ms
step:2090/2420 train_time:125837ms step_avg:60.21ms
step:2091/2420 train_time:125897ms step_avg:60.21ms
step:2092/2420 train_time:125959ms step_avg:60.21ms
step:2093/2420 train_time:126020ms step_avg:60.21ms
step:2094/2420 train_time:126082ms step_avg:60.21ms
step:2095/2420 train_time:126142ms step_avg:60.21ms
step:2096/2420 train_time:126203ms step_avg:60.21ms
step:2097/2420 train_time:126263ms step_avg:60.21ms
step:2098/2420 train_time:126324ms step_avg:60.21ms
step:2099/2420 train_time:126383ms step_avg:60.21ms
step:2100/2420 train_time:126444ms step_avg:60.21ms
step:2101/2420 train_time:126505ms step_avg:60.21ms
step:2102/2420 train_time:126566ms step_avg:60.21ms
step:2103/2420 train_time:126627ms step_avg:60.21ms
step:2104/2420 train_time:126689ms step_avg:60.21ms
step:2105/2420 train_time:126748ms step_avg:60.21ms
step:2106/2420 train_time:126810ms step_avg:60.21ms
step:2107/2420 train_time:126871ms step_avg:60.21ms
step:2108/2420 train_time:126932ms step_avg:60.21ms
step:2109/2420 train_time:126992ms step_avg:60.21ms
step:2110/2420 train_time:127054ms step_avg:60.22ms
step:2111/2420 train_time:127114ms step_avg:60.22ms
step:2112/2420 train_time:127176ms step_avg:60.22ms
step:2113/2420 train_time:127236ms step_avg:60.22ms
step:2114/2420 train_time:127298ms step_avg:60.22ms
step:2115/2420 train_time:127358ms step_avg:60.22ms
step:2116/2420 train_time:127420ms step_avg:60.22ms
step:2117/2420 train_time:127481ms step_avg:60.22ms
step:2118/2420 train_time:127542ms step_avg:60.22ms
step:2119/2420 train_time:127602ms step_avg:60.22ms
step:2120/2420 train_time:127663ms step_avg:60.22ms
step:2121/2420 train_time:127723ms step_avg:60.22ms
step:2122/2420 train_time:127784ms step_avg:60.22ms
step:2123/2420 train_time:127844ms step_avg:60.22ms
step:2124/2420 train_time:127905ms step_avg:60.22ms
step:2125/2420 train_time:127965ms step_avg:60.22ms
step:2126/2420 train_time:128027ms step_avg:60.22ms
step:2127/2420 train_time:128087ms step_avg:60.22ms
step:2128/2420 train_time:128149ms step_avg:60.22ms
step:2129/2420 train_time:128209ms step_avg:60.22ms
step:2130/2420 train_time:128271ms step_avg:60.22ms
step:2131/2420 train_time:128331ms step_avg:60.22ms
step:2132/2420 train_time:128393ms step_avg:60.22ms
step:2133/2420 train_time:128453ms step_avg:60.22ms
step:2134/2420 train_time:128515ms step_avg:60.22ms
step:2135/2420 train_time:128576ms step_avg:60.22ms
step:2136/2420 train_time:128638ms step_avg:60.22ms
step:2137/2420 train_time:128698ms step_avg:60.22ms
step:2138/2420 train_time:128760ms step_avg:60.22ms
step:2139/2420 train_time:128820ms step_avg:60.22ms
step:2140/2420 train_time:128881ms step_avg:60.22ms
step:2141/2420 train_time:128941ms step_avg:60.22ms
step:2142/2420 train_time:129002ms step_avg:60.23ms
step:2143/2420 train_time:129062ms step_avg:60.22ms
step:2144/2420 train_time:129123ms step_avg:60.23ms
step:2145/2420 train_time:129183ms step_avg:60.22ms
step:2146/2420 train_time:129244ms step_avg:60.23ms
step:2147/2420 train_time:129303ms step_avg:60.23ms
step:2148/2420 train_time:129365ms step_avg:60.23ms
step:2149/2420 train_time:129425ms step_avg:60.23ms
step:2150/2420 train_time:129487ms step_avg:60.23ms
step:2151/2420 train_time:129547ms step_avg:60.23ms
step:2152/2420 train_time:129610ms step_avg:60.23ms
step:2153/2420 train_time:129669ms step_avg:60.23ms
step:2154/2420 train_time:129731ms step_avg:60.23ms
step:2155/2420 train_time:129791ms step_avg:60.23ms
step:2156/2420 train_time:129853ms step_avg:60.23ms
step:2157/2420 train_time:129913ms step_avg:60.23ms
step:2158/2420 train_time:129975ms step_avg:60.23ms
step:2159/2420 train_time:130035ms step_avg:60.23ms
step:2160/2420 train_time:130097ms step_avg:60.23ms
step:2161/2420 train_time:130157ms step_avg:60.23ms
step:2162/2420 train_time:130220ms step_avg:60.23ms
step:2163/2420 train_time:130281ms step_avg:60.23ms
step:2164/2420 train_time:130343ms step_avg:60.23ms
step:2165/2420 train_time:130402ms step_avg:60.23ms
step:2166/2420 train_time:130464ms step_avg:60.23ms
step:2167/2420 train_time:130524ms step_avg:60.23ms
step:2168/2420 train_time:130585ms step_avg:60.23ms
step:2169/2420 train_time:130645ms step_avg:60.23ms
step:2170/2420 train_time:130707ms step_avg:60.23ms
step:2171/2420 train_time:130767ms step_avg:60.23ms
step:2172/2420 train_time:130829ms step_avg:60.23ms
step:2173/2420 train_time:130889ms step_avg:60.23ms
step:2174/2420 train_time:130951ms step_avg:60.23ms
step:2175/2420 train_time:131011ms step_avg:60.23ms
step:2176/2420 train_time:131073ms step_avg:60.24ms
step:2177/2420 train_time:131133ms step_avg:60.24ms
step:2178/2420 train_time:131195ms step_avg:60.24ms
step:2179/2420 train_time:131255ms step_avg:60.24ms
step:2180/2420 train_time:131317ms step_avg:60.24ms
step:2181/2420 train_time:131377ms step_avg:60.24ms
step:2182/2420 train_time:131439ms step_avg:60.24ms
step:2183/2420 train_time:131500ms step_avg:60.24ms
step:2184/2420 train_time:131562ms step_avg:60.24ms
step:2185/2420 train_time:131622ms step_avg:60.24ms
step:2186/2420 train_time:131683ms step_avg:60.24ms
step:2187/2420 train_time:131743ms step_avg:60.24ms
step:2188/2420 train_time:131804ms step_avg:60.24ms
step:2189/2420 train_time:131864ms step_avg:60.24ms
step:2190/2420 train_time:131925ms step_avg:60.24ms
step:2191/2420 train_time:131985ms step_avg:60.24ms
step:2192/2420 train_time:132047ms step_avg:60.24ms
step:2193/2420 train_time:132107ms step_avg:60.24ms
step:2194/2420 train_time:132169ms step_avg:60.24ms
step:2195/2420 train_time:132229ms step_avg:60.24ms
step:2196/2420 train_time:132291ms step_avg:60.24ms
step:2197/2420 train_time:132352ms step_avg:60.24ms
step:2198/2420 train_time:132414ms step_avg:60.24ms
step:2199/2420 train_time:132474ms step_avg:60.24ms
step:2200/2420 train_time:132536ms step_avg:60.24ms
step:2201/2420 train_time:132596ms step_avg:60.24ms
step:2202/2420 train_time:132658ms step_avg:60.24ms
step:2203/2420 train_time:132719ms step_avg:60.24ms
step:2204/2420 train_time:132781ms step_avg:60.25ms
step:2205/2420 train_time:132841ms step_avg:60.25ms
step:2206/2420 train_time:132902ms step_avg:60.25ms
step:2207/2420 train_time:132962ms step_avg:60.25ms
step:2208/2420 train_time:133023ms step_avg:60.25ms
step:2209/2420 train_time:133083ms step_avg:60.25ms
step:2210/2420 train_time:133144ms step_avg:60.25ms
step:2211/2420 train_time:133203ms step_avg:60.25ms
step:2212/2420 train_time:133265ms step_avg:60.25ms
step:2213/2420 train_time:133324ms step_avg:60.25ms
step:2214/2420 train_time:133386ms step_avg:60.25ms
step:2215/2420 train_time:133447ms step_avg:60.25ms
step:2216/2420 train_time:133509ms step_avg:60.25ms
step:2217/2420 train_time:133569ms step_avg:60.25ms
step:2218/2420 train_time:133630ms step_avg:60.25ms
step:2219/2420 train_time:133691ms step_avg:60.25ms
step:2220/2420 train_time:133753ms step_avg:60.25ms
step:2221/2420 train_time:133813ms step_avg:60.25ms
step:2222/2420 train_time:133876ms step_avg:60.25ms
step:2223/2420 train_time:133936ms step_avg:60.25ms
step:2224/2420 train_time:133997ms step_avg:60.25ms
step:2225/2420 train_time:134058ms step_avg:60.25ms
step:2226/2420 train_time:134120ms step_avg:60.25ms
step:2227/2420 train_time:134181ms step_avg:60.25ms
step:2228/2420 train_time:134242ms step_avg:60.25ms
step:2229/2420 train_time:134302ms step_avg:60.25ms
step:2230/2420 train_time:134364ms step_avg:60.25ms
step:2231/2420 train_time:134423ms step_avg:60.25ms
step:2232/2420 train_time:134484ms step_avg:60.25ms
step:2233/2420 train_time:134544ms step_avg:60.25ms
step:2234/2420 train_time:134605ms step_avg:60.25ms
step:2235/2420 train_time:134665ms step_avg:60.25ms
step:2236/2420 train_time:134727ms step_avg:60.25ms
step:2237/2420 train_time:134787ms step_avg:60.25ms
step:2238/2420 train_time:134850ms step_avg:60.25ms
step:2239/2420 train_time:134910ms step_avg:60.25ms
step:2240/2420 train_time:134972ms step_avg:60.26ms
step:2241/2420 train_time:135032ms step_avg:60.26ms
step:2242/2420 train_time:135094ms step_avg:60.26ms
step:2243/2420 train_time:135154ms step_avg:60.26ms
step:2244/2420 train_time:135216ms step_avg:60.26ms
step:2245/2420 train_time:135276ms step_avg:60.26ms
step:2246/2420 train_time:135339ms step_avg:60.26ms
step:2247/2420 train_time:135399ms step_avg:60.26ms
step:2248/2420 train_time:135461ms step_avg:60.26ms
step:2249/2420 train_time:135521ms step_avg:60.26ms
step:2250/2420 train_time:135583ms step_avg:60.26ms
step:2250/2420 val_loss:3.3047 train_time:135646ms step_avg:60.29ms
step:2251/2420 train_time:135668ms step_avg:60.27ms
step:2252/2420 train_time:135709ms step_avg:60.26ms
step:2253/2420 train_time:135771ms step_avg:60.26ms
step:2254/2420 train_time:135836ms step_avg:60.26ms
step:2255/2420 train_time:135898ms step_avg:60.27ms
step:2256/2420 train_time:135960ms step_avg:60.27ms
step:2257/2420 train_time:136019ms step_avg:60.27ms
step:2258/2420 train_time:136080ms step_avg:60.27ms
step:2259/2420 train_time:136140ms step_avg:60.27ms
step:2260/2420 train_time:136201ms step_avg:60.27ms
step:2261/2420 train_time:136260ms step_avg:60.27ms
step:2262/2420 train_time:136322ms step_avg:60.27ms
step:2263/2420 train_time:136381ms step_avg:60.27ms
step:2264/2420 train_time:136443ms step_avg:60.27ms
step:2265/2420 train_time:136502ms step_avg:60.27ms
step:2266/2420 train_time:136563ms step_avg:60.27ms
step:2267/2420 train_time:136624ms step_avg:60.27ms
step:2268/2420 train_time:136687ms step_avg:60.27ms
step:2269/2420 train_time:136749ms step_avg:60.27ms
step:2270/2420 train_time:136812ms step_avg:60.27ms
step:2271/2420 train_time:136872ms step_avg:60.27ms
step:2272/2420 train_time:136934ms step_avg:60.27ms
step:2273/2420 train_time:136993ms step_avg:60.27ms
step:2274/2420 train_time:137054ms step_avg:60.27ms
step:2275/2420 train_time:137113ms step_avg:60.27ms
step:2276/2420 train_time:137174ms step_avg:60.27ms
step:2277/2420 train_time:137233ms step_avg:60.27ms
step:2278/2420 train_time:137294ms step_avg:60.27ms
step:2279/2420 train_time:137353ms step_avg:60.27ms
step:2280/2420 train_time:137415ms step_avg:60.27ms
step:2281/2420 train_time:137474ms step_avg:60.27ms
step:2282/2420 train_time:137536ms step_avg:60.27ms
step:2283/2420 train_time:137596ms step_avg:60.27ms
step:2284/2420 train_time:137658ms step_avg:60.27ms
step:2285/2420 train_time:137719ms step_avg:60.27ms
step:2286/2420 train_time:137782ms step_avg:60.27ms
step:2287/2420 train_time:137843ms step_avg:60.27ms
step:2288/2420 train_time:137905ms step_avg:60.27ms
step:2289/2420 train_time:137965ms step_avg:60.27ms
step:2290/2420 train_time:138027ms step_avg:60.27ms
step:2291/2420 train_time:138087ms step_avg:60.27ms
step:2292/2420 train_time:138148ms step_avg:60.27ms
step:2293/2420 train_time:138207ms step_avg:60.27ms
step:2294/2420 train_time:138268ms step_avg:60.27ms
step:2295/2420 train_time:138328ms step_avg:60.27ms
step:2296/2420 train_time:138389ms step_avg:60.27ms
step:2297/2420 train_time:138448ms step_avg:60.27ms
step:2298/2420 train_time:138509ms step_avg:60.27ms
step:2299/2420 train_time:138569ms step_avg:60.27ms
step:2300/2420 train_time:138630ms step_avg:60.27ms
step:2301/2420 train_time:138691ms step_avg:60.27ms
step:2302/2420 train_time:138753ms step_avg:60.27ms
step:2303/2420 train_time:138813ms step_avg:60.27ms
step:2304/2420 train_time:138875ms step_avg:60.28ms
step:2305/2420 train_time:138935ms step_avg:60.28ms
step:2306/2420 train_time:138997ms step_avg:60.28ms
step:2307/2420 train_time:139057ms step_avg:60.28ms
step:2308/2420 train_time:139119ms step_avg:60.28ms
step:2309/2420 train_time:139178ms step_avg:60.28ms
step:2310/2420 train_time:139241ms step_avg:60.28ms
step:2311/2420 train_time:139301ms step_avg:60.28ms
step:2312/2420 train_time:139363ms step_avg:60.28ms
step:2313/2420 train_time:139422ms step_avg:60.28ms
step:2314/2420 train_time:139484ms step_avg:60.28ms
step:2315/2420 train_time:139544ms step_avg:60.28ms
step:2316/2420 train_time:139606ms step_avg:60.28ms
step:2317/2420 train_time:139666ms step_avg:60.28ms
step:2318/2420 train_time:139728ms step_avg:60.28ms
step:2319/2420 train_time:139788ms step_avg:60.28ms
step:2320/2420 train_time:139850ms step_avg:60.28ms
step:2321/2420 train_time:139910ms step_avg:60.28ms
step:2322/2420 train_time:139972ms step_avg:60.28ms
step:2323/2420 train_time:140031ms step_avg:60.28ms
step:2324/2420 train_time:140092ms step_avg:60.28ms
step:2325/2420 train_time:140151ms step_avg:60.28ms
step:2326/2420 train_time:140213ms step_avg:60.28ms
step:2327/2420 train_time:140273ms step_avg:60.28ms
step:2328/2420 train_time:140335ms step_avg:60.28ms
step:2329/2420 train_time:140394ms step_avg:60.28ms
step:2330/2420 train_time:140456ms step_avg:60.28ms
step:2331/2420 train_time:140516ms step_avg:60.28ms
step:2332/2420 train_time:140578ms step_avg:60.28ms
step:2333/2420 train_time:140639ms step_avg:60.28ms
step:2334/2420 train_time:140701ms step_avg:60.28ms
step:2335/2420 train_time:140761ms step_avg:60.28ms
step:2336/2420 train_time:140823ms step_avg:60.28ms
step:2337/2420 train_time:140883ms step_avg:60.28ms
step:2338/2420 train_time:140946ms step_avg:60.29ms
step:2339/2420 train_time:141007ms step_avg:60.29ms
step:2340/2420 train_time:141068ms step_avg:60.29ms
step:2341/2420 train_time:141128ms step_avg:60.29ms
step:2342/2420 train_time:141189ms step_avg:60.29ms
step:2343/2420 train_time:141249ms step_avg:60.29ms
step:2344/2420 train_time:141310ms step_avg:60.29ms
step:2345/2420 train_time:141369ms step_avg:60.29ms
step:2346/2420 train_time:141431ms step_avg:60.29ms
step:2347/2420 train_time:141490ms step_avg:60.29ms
step:2348/2420 train_time:141551ms step_avg:60.29ms
step:2349/2420 train_time:141611ms step_avg:60.29ms
step:2350/2420 train_time:141672ms step_avg:60.29ms
step:2351/2420 train_time:141732ms step_avg:60.29ms
step:2352/2420 train_time:141794ms step_avg:60.29ms
step:2353/2420 train_time:141854ms step_avg:60.29ms
step:2354/2420 train_time:141916ms step_avg:60.29ms
step:2355/2420 train_time:141976ms step_avg:60.29ms
step:2356/2420 train_time:142037ms step_avg:60.29ms
step:2357/2420 train_time:142098ms step_avg:60.29ms
step:2358/2420 train_time:142160ms step_avg:60.29ms
step:2359/2420 train_time:142220ms step_avg:60.29ms
step:2360/2420 train_time:142282ms step_avg:60.29ms
step:2361/2420 train_time:142342ms step_avg:60.29ms
step:2362/2420 train_time:142404ms step_avg:60.29ms
step:2363/2420 train_time:142464ms step_avg:60.29ms
step:2364/2420 train_time:142526ms step_avg:60.29ms
step:2365/2420 train_time:142585ms step_avg:60.29ms
step:2366/2420 train_time:142648ms step_avg:60.29ms
step:2367/2420 train_time:142708ms step_avg:60.29ms
step:2368/2420 train_time:142770ms step_avg:60.29ms
step:2369/2420 train_time:142829ms step_avg:60.29ms
step:2370/2420 train_time:142891ms step_avg:60.29ms
step:2371/2420 train_time:142950ms step_avg:60.29ms
step:2372/2420 train_time:143012ms step_avg:60.29ms
step:2373/2420 train_time:143071ms step_avg:60.29ms
step:2374/2420 train_time:143132ms step_avg:60.29ms
step:2375/2420 train_time:143192ms step_avg:60.29ms
step:2376/2420 train_time:143254ms step_avg:60.29ms
step:2377/2420 train_time:143313ms step_avg:60.29ms
step:2378/2420 train_time:143376ms step_avg:60.29ms
step:2379/2420 train_time:143436ms step_avg:60.29ms
step:2380/2420 train_time:143498ms step_avg:60.29ms
step:2381/2420 train_time:143558ms step_avg:60.29ms
step:2382/2420 train_time:143620ms step_avg:60.29ms
step:2383/2420 train_time:143680ms step_avg:60.29ms
step:2384/2420 train_time:143976ms step_avg:60.39ms
step:2385/2420 train_time:144396ms step_avg:60.54ms
step:2386/2420 train_time:144419ms step_avg:60.53ms
step:2387/2420 train_time:144477ms step_avg:60.53ms
step:2388/2420 train_time:144537ms step_avg:60.53ms
step:2389/2420 train_time:144596ms step_avg:60.53ms
step:2390/2420 train_time:144657ms step_avg:60.53ms
step:2391/2420 train_time:144715ms step_avg:60.53ms
step:2392/2420 train_time:144776ms step_avg:60.53ms
step:2393/2420 train_time:144834ms step_avg:60.52ms
step:2394/2420 train_time:144895ms step_avg:60.52ms
step:2395/2420 train_time:144954ms step_avg:60.52ms
step:2396/2420 train_time:145014ms step_avg:60.52ms
step:2397/2420 train_time:145073ms step_avg:60.52ms
step:2398/2420 train_time:145133ms step_avg:60.52ms
step:2399/2420 train_time:145192ms step_avg:60.52ms
step:2400/2420 train_time:145254ms step_avg:60.52ms
step:2401/2420 train_time:145322ms step_avg:60.53ms
step:2402/2420 train_time:145391ms step_avg:60.53ms
step:2403/2420 train_time:145452ms step_avg:60.53ms
step:2404/2420 train_time:145513ms step_avg:60.53ms
step:2405/2420 train_time:145574ms step_avg:60.53ms
step:2406/2420 train_time:145636ms step_avg:60.53ms
step:2407/2420 train_time:145695ms step_avg:60.53ms
step:2408/2420 train_time:145756ms step_avg:60.53ms
step:2409/2420 train_time:145815ms step_avg:60.53ms
step:2410/2420 train_time:145876ms step_avg:60.53ms
step:2411/2420 train_time:145934ms step_avg:60.53ms
step:2412/2420 train_time:145995ms step_avg:60.53ms
step:2413/2420 train_time:146054ms step_avg:60.53ms
step:2414/2420 train_time:146115ms step_avg:60.53ms
step:2415/2420 train_time:146174ms step_avg:60.53ms
step:2416/2420 train_time:146237ms step_avg:60.53ms
step:2417/2420 train_time:146300ms step_avg:60.53ms
step:2418/2420 train_time:146363ms step_avg:60.53ms
step:2419/2420 train_time:146426ms step_avg:60.53ms
step:2420/2420 train_time:146489ms step_avg:60.53ms
step:2420/2420 val_loss:3.2794 train_time:146553ms step_avg:60.56ms
peak memory allocated: 29226 MiB reserved: 44036 MiB

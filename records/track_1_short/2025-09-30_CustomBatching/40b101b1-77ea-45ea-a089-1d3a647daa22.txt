import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        #ve = [None, ve[0], ve[1]] + [None] * (len(self.blocks) - 5) + [ve[0], ve[1]]
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2380  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.4  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further
    momentum_cd_steps = 50


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.7, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps  # More explicit denominator

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # step the optimizers
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 21:22:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                  Off |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                  Off |
| N/A   40C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                  Off |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                  Off |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                  Off |
| N/A   35C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                  Off |
| N/A   42C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                  Off |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                  Off |
| N/A   36C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          244200      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          244201      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          244202      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          244203      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          244204      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          244205      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          244206      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          244207      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          244201      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          244202      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          244203      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          244204      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          244205      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          244206      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          244207      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2420 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2420 train_time:107ms step_avg:106.79ms
step:2/2420 train_time:186ms step_avg:93.22ms
step:3/2420 train_time:210ms step_avg:70.10ms
step:4/2420 train_time:242ms step_avg:60.58ms
step:5/2420 train_time:299ms step_avg:59.82ms
step:6/2420 train_time:365ms step_avg:60.79ms
step:7/2420 train_time:426ms step_avg:60.82ms
step:8/2420 train_time:486ms step_avg:60.70ms
step:9/2420 train_time:544ms step_avg:60.44ms
step:10/2420 train_time:604ms step_avg:60.38ms
step:11/2420 train_time:662ms step_avg:60.20ms
step:12/2420 train_time:722ms step_avg:60.19ms
step:13/2420 train_time:781ms step_avg:60.07ms
step:14/2420 train_time:841ms step_avg:60.09ms
step:15/2420 train_time:900ms step_avg:59.99ms
step:16/2420 train_time:960ms step_avg:60.02ms
step:17/2420 train_time:1019ms step_avg:59.93ms
step:18/2420 train_time:1083ms step_avg:60.14ms
step:19/2420 train_time:1146ms step_avg:60.31ms
step:20/2420 train_time:1209ms step_avg:60.45ms
step:21/2420 train_time:1268ms step_avg:60.39ms
step:22/2420 train_time:1330ms step_avg:60.44ms
step:23/2420 train_time:1389ms step_avg:60.38ms
step:24/2420 train_time:1449ms step_avg:60.39ms
step:25/2420 train_time:1508ms step_avg:60.34ms
step:26/2420 train_time:1569ms step_avg:60.34ms
step:27/2420 train_time:1627ms step_avg:60.27ms
step:28/2420 train_time:1688ms step_avg:60.28ms
step:29/2420 train_time:1746ms step_avg:60.21ms
step:30/2420 train_time:1807ms step_avg:60.23ms
step:31/2420 train_time:1866ms step_avg:60.18ms
step:32/2420 train_time:1926ms step_avg:60.19ms
step:33/2420 train_time:1985ms step_avg:60.14ms
step:34/2420 train_time:2046ms step_avg:60.17ms
step:35/2420 train_time:2106ms step_avg:60.16ms
step:36/2420 train_time:2167ms step_avg:60.19ms
step:37/2420 train_time:2226ms step_avg:60.17ms
step:38/2420 train_time:2287ms step_avg:60.20ms
step:39/2420 train_time:2346ms step_avg:60.16ms
step:40/2420 train_time:2408ms step_avg:60.19ms
step:41/2420 train_time:2466ms step_avg:60.15ms
step:42/2420 train_time:2527ms step_avg:60.16ms
step:43/2420 train_time:2585ms step_avg:60.13ms
step:44/2420 train_time:2646ms step_avg:60.13ms
step:45/2420 train_time:2704ms step_avg:60.09ms
step:46/2420 train_time:2764ms step_avg:60.10ms
step:47/2420 train_time:2823ms step_avg:60.07ms
step:48/2420 train_time:2884ms step_avg:60.09ms
step:49/2420 train_time:2942ms step_avg:60.05ms
step:50/2420 train_time:3003ms step_avg:60.06ms
step:51/2420 train_time:3063ms step_avg:60.05ms
step:52/2420 train_time:3124ms step_avg:60.07ms
step:53/2420 train_time:3183ms step_avg:60.06ms
step:54/2420 train_time:3245ms step_avg:60.09ms
step:55/2420 train_time:3305ms step_avg:60.09ms
step:56/2420 train_time:3367ms step_avg:60.12ms
step:57/2420 train_time:3426ms step_avg:60.10ms
step:58/2420 train_time:3487ms step_avg:60.12ms
step:59/2420 train_time:3545ms step_avg:60.09ms
step:60/2420 train_time:3606ms step_avg:60.09ms
step:61/2420 train_time:3665ms step_avg:60.08ms
step:62/2420 train_time:3725ms step_avg:60.08ms
step:63/2420 train_time:3785ms step_avg:60.07ms
step:64/2420 train_time:3845ms step_avg:60.08ms
step:65/2420 train_time:3903ms step_avg:60.05ms
step:66/2420 train_time:3963ms step_avg:60.05ms
step:67/2420 train_time:4022ms step_avg:60.03ms
step:68/2420 train_time:4084ms step_avg:60.06ms
step:69/2420 train_time:4143ms step_avg:60.04ms
step:70/2420 train_time:4204ms step_avg:60.05ms
step:71/2420 train_time:4263ms step_avg:60.04ms
step:72/2420 train_time:4324ms step_avg:60.05ms
step:73/2420 train_time:4383ms step_avg:60.04ms
step:74/2420 train_time:4444ms step_avg:60.05ms
step:75/2420 train_time:4503ms step_avg:60.04ms
step:76/2420 train_time:4564ms step_avg:60.06ms
step:77/2420 train_time:4623ms step_avg:60.04ms
step:78/2420 train_time:4684ms step_avg:60.05ms
step:79/2420 train_time:4743ms step_avg:60.04ms
step:80/2420 train_time:4804ms step_avg:60.05ms
step:81/2420 train_time:4862ms step_avg:60.02ms
step:82/2420 train_time:4923ms step_avg:60.03ms
step:83/2420 train_time:4982ms step_avg:60.02ms
step:84/2420 train_time:5043ms step_avg:60.03ms
step:85/2420 train_time:5101ms step_avg:60.01ms
step:86/2420 train_time:5162ms step_avg:60.02ms
step:87/2420 train_time:5221ms step_avg:60.01ms
step:88/2420 train_time:5283ms step_avg:60.04ms
step:89/2420 train_time:5343ms step_avg:60.03ms
step:90/2420 train_time:5404ms step_avg:60.04ms
step:91/2420 train_time:5463ms step_avg:60.03ms
step:92/2420 train_time:5524ms step_avg:60.04ms
step:93/2420 train_time:5583ms step_avg:60.04ms
step:94/2420 train_time:5644ms step_avg:60.04ms
step:95/2420 train_time:5703ms step_avg:60.03ms
step:96/2420 train_time:5764ms step_avg:60.04ms
step:97/2420 train_time:5822ms step_avg:60.02ms
step:98/2420 train_time:5884ms step_avg:60.04ms
step:99/2420 train_time:5942ms step_avg:60.02ms
step:100/2420 train_time:6003ms step_avg:60.03ms
step:101/2420 train_time:6062ms step_avg:60.02ms
step:102/2420 train_time:6124ms step_avg:60.03ms
step:103/2420 train_time:6182ms step_avg:60.02ms
step:104/2420 train_time:6244ms step_avg:60.04ms
step:105/2420 train_time:6304ms step_avg:60.04ms
step:106/2420 train_time:6365ms step_avg:60.04ms
step:107/2420 train_time:6424ms step_avg:60.04ms
step:108/2420 train_time:6485ms step_avg:60.05ms
step:109/2420 train_time:6544ms step_avg:60.03ms
step:110/2420 train_time:6604ms step_avg:60.04ms
step:111/2420 train_time:6663ms step_avg:60.02ms
step:112/2420 train_time:6723ms step_avg:60.03ms
step:113/2420 train_time:6782ms step_avg:60.02ms
step:114/2420 train_time:6843ms step_avg:60.03ms
step:115/2420 train_time:6901ms step_avg:60.01ms
step:116/2420 train_time:6962ms step_avg:60.02ms
step:117/2420 train_time:7021ms step_avg:60.00ms
step:118/2420 train_time:7083ms step_avg:60.02ms
step:119/2420 train_time:7142ms step_avg:60.01ms
step:120/2420 train_time:7203ms step_avg:60.02ms
step:121/2420 train_time:7262ms step_avg:60.02ms
step:122/2420 train_time:7323ms step_avg:60.02ms
step:123/2420 train_time:7382ms step_avg:60.02ms
step:124/2420 train_time:7443ms step_avg:60.03ms
step:125/2420 train_time:7502ms step_avg:60.02ms
step:126/2420 train_time:7563ms step_avg:60.02ms
step:127/2420 train_time:7622ms step_avg:60.01ms
step:128/2420 train_time:7683ms step_avg:60.02ms
step:129/2420 train_time:7741ms step_avg:60.01ms
step:130/2420 train_time:7802ms step_avg:60.02ms
step:131/2420 train_time:7860ms step_avg:60.00ms
step:132/2420 train_time:7921ms step_avg:60.01ms
step:133/2420 train_time:7980ms step_avg:60.00ms
step:134/2420 train_time:8041ms step_avg:60.01ms
step:135/2420 train_time:8099ms step_avg:60.00ms
step:136/2420 train_time:8160ms step_avg:60.00ms
step:137/2420 train_time:8219ms step_avg:59.99ms
step:138/2420 train_time:8281ms step_avg:60.00ms
step:139/2420 train_time:8340ms step_avg:60.00ms
step:140/2420 train_time:8401ms step_avg:60.01ms
step:141/2420 train_time:8460ms step_avg:60.00ms
step:142/2420 train_time:8521ms step_avg:60.01ms
step:143/2420 train_time:8581ms step_avg:60.01ms
step:144/2420 train_time:8642ms step_avg:60.01ms
step:145/2420 train_time:8701ms step_avg:60.01ms
step:146/2420 train_time:8762ms step_avg:60.01ms
step:147/2420 train_time:8820ms step_avg:60.00ms
step:148/2420 train_time:8881ms step_avg:60.01ms
step:149/2420 train_time:8939ms step_avg:60.00ms
step:150/2420 train_time:9000ms step_avg:60.00ms
step:151/2420 train_time:9059ms step_avg:59.99ms
step:152/2420 train_time:9120ms step_avg:60.00ms
step:153/2420 train_time:9179ms step_avg:60.00ms
step:154/2420 train_time:9240ms step_avg:60.00ms
step:155/2420 train_time:9299ms step_avg:59.99ms
step:156/2420 train_time:9360ms step_avg:60.00ms
step:157/2420 train_time:9420ms step_avg:60.00ms
step:158/2420 train_time:9481ms step_avg:60.01ms
step:159/2420 train_time:9540ms step_avg:60.00ms
step:160/2420 train_time:9601ms step_avg:60.00ms
step:161/2420 train_time:9659ms step_avg:59.99ms
step:162/2420 train_time:9720ms step_avg:60.00ms
step:163/2420 train_time:9779ms step_avg:59.99ms
step:164/2420 train_time:9840ms step_avg:60.00ms
step:165/2420 train_time:9898ms step_avg:59.99ms
step:166/2420 train_time:9959ms step_avg:59.99ms
step:167/2420 train_time:10018ms step_avg:59.99ms
step:168/2420 train_time:10079ms step_avg:59.99ms
step:169/2420 train_time:10137ms step_avg:59.98ms
step:170/2420 train_time:10198ms step_avg:59.99ms
step:171/2420 train_time:10257ms step_avg:59.98ms
step:172/2420 train_time:10318ms step_avg:59.99ms
step:173/2420 train_time:10378ms step_avg:59.99ms
step:174/2420 train_time:10439ms step_avg:60.00ms
step:175/2420 train_time:10499ms step_avg:59.99ms
step:176/2420 train_time:10559ms step_avg:60.00ms
step:177/2420 train_time:10619ms step_avg:59.99ms
step:178/2420 train_time:10679ms step_avg:60.00ms
step:179/2420 train_time:10739ms step_avg:59.99ms
step:180/2420 train_time:10800ms step_avg:60.00ms
step:181/2420 train_time:10859ms step_avg:60.00ms
step:182/2420 train_time:10921ms step_avg:60.00ms
step:183/2420 train_time:10980ms step_avg:60.00ms
step:184/2420 train_time:11040ms step_avg:60.00ms
step:185/2420 train_time:11099ms step_avg:59.99ms
step:186/2420 train_time:11159ms step_avg:59.99ms
step:187/2420 train_time:11218ms step_avg:59.99ms
step:188/2420 train_time:11279ms step_avg:60.00ms
step:189/2420 train_time:11338ms step_avg:59.99ms
step:190/2420 train_time:11399ms step_avg:60.00ms
step:191/2420 train_time:11458ms step_avg:59.99ms
step:192/2420 train_time:11519ms step_avg:60.00ms
step:193/2420 train_time:11578ms step_avg:59.99ms
step:194/2420 train_time:11639ms step_avg:59.99ms
step:195/2420 train_time:11698ms step_avg:59.99ms
step:196/2420 train_time:11758ms step_avg:59.99ms
step:197/2420 train_time:11817ms step_avg:59.99ms
step:198/2420 train_time:11878ms step_avg:59.99ms
step:199/2420 train_time:11937ms step_avg:59.98ms
step:200/2420 train_time:11998ms step_avg:59.99ms
step:201/2420 train_time:12056ms step_avg:59.98ms
step:202/2420 train_time:12117ms step_avg:59.98ms
step:203/2420 train_time:12175ms step_avg:59.98ms
step:204/2420 train_time:12235ms step_avg:59.98ms
step:205/2420 train_time:12294ms step_avg:59.97ms
step:206/2420 train_time:12354ms step_avg:59.97ms
step:207/2420 train_time:12413ms step_avg:59.97ms
step:208/2420 train_time:12473ms step_avg:59.97ms
step:209/2420 train_time:12532ms step_avg:59.96ms
step:210/2420 train_time:12593ms step_avg:59.97ms
step:211/2420 train_time:12652ms step_avg:59.96ms
step:212/2420 train_time:12713ms step_avg:59.97ms
step:213/2420 train_time:12771ms step_avg:59.96ms
step:214/2420 train_time:12832ms step_avg:59.96ms
step:215/2420 train_time:12891ms step_avg:59.96ms
step:216/2420 train_time:12951ms step_avg:59.96ms
step:217/2420 train_time:13010ms step_avg:59.95ms
step:218/2420 train_time:13070ms step_avg:59.96ms
step:219/2420 train_time:13128ms step_avg:59.95ms
step:220/2420 train_time:13189ms step_avg:59.95ms
step:221/2420 train_time:13248ms step_avg:59.94ms
step:222/2420 train_time:13308ms step_avg:59.95ms
step:223/2420 train_time:13367ms step_avg:59.94ms
step:224/2420 train_time:13427ms step_avg:59.94ms
step:225/2420 train_time:13486ms step_avg:59.94ms
step:226/2420 train_time:13546ms step_avg:59.94ms
step:227/2420 train_time:13605ms step_avg:59.93ms
step:228/2420 train_time:13665ms step_avg:59.93ms
step:229/2420 train_time:13723ms step_avg:59.93ms
step:230/2420 train_time:13784ms step_avg:59.93ms
step:231/2420 train_time:13842ms step_avg:59.92ms
step:232/2420 train_time:13903ms step_avg:59.93ms
step:233/2420 train_time:13961ms step_avg:59.92ms
step:234/2420 train_time:14021ms step_avg:59.92ms
step:235/2420 train_time:14080ms step_avg:59.92ms
step:236/2420 train_time:14141ms step_avg:59.92ms
step:237/2420 train_time:14199ms step_avg:59.91ms
step:238/2420 train_time:14260ms step_avg:59.92ms
step:239/2420 train_time:14319ms step_avg:59.91ms
step:240/2420 train_time:14380ms step_avg:59.92ms
step:241/2420 train_time:14439ms step_avg:59.91ms
step:242/2420 train_time:14499ms step_avg:59.92ms
step:243/2420 train_time:14558ms step_avg:59.91ms
step:244/2420 train_time:14618ms step_avg:59.91ms
step:245/2420 train_time:14677ms step_avg:59.91ms
step:246/2420 train_time:14738ms step_avg:59.91ms
step:247/2420 train_time:14796ms step_avg:59.90ms
step:248/2420 train_time:14857ms step_avg:59.91ms
step:249/2420 train_time:14916ms step_avg:59.90ms
step:250/2420 train_time:14977ms step_avg:59.91ms
step:250/2420 val_loss:4.0925 train_time:15040ms step_avg:60.16ms
step:251/2420 train_time:15060ms step_avg:60.00ms
step:252/2420 train_time:15097ms step_avg:59.91ms
step:253/2420 train_time:15159ms step_avg:59.92ms
step:254/2420 train_time:15226ms step_avg:59.95ms
step:255/2420 train_time:15288ms step_avg:59.95ms
step:256/2420 train_time:15348ms step_avg:59.95ms
step:257/2420 train_time:15407ms step_avg:59.95ms
step:258/2420 train_time:15467ms step_avg:59.95ms
step:259/2420 train_time:15525ms step_avg:59.94ms
step:260/2420 train_time:15584ms step_avg:59.94ms
step:261/2420 train_time:15642ms step_avg:59.93ms
step:262/2420 train_time:15701ms step_avg:59.93ms
step:263/2420 train_time:15759ms step_avg:59.92ms
step:264/2420 train_time:15819ms step_avg:59.92ms
step:265/2420 train_time:15876ms step_avg:59.91ms
step:266/2420 train_time:15936ms step_avg:59.91ms
step:267/2420 train_time:15993ms step_avg:59.90ms
step:268/2420 train_time:16053ms step_avg:59.90ms
step:269/2420 train_time:16112ms step_avg:59.90ms
step:270/2420 train_time:16175ms step_avg:59.91ms
step:271/2420 train_time:16235ms step_avg:59.91ms
step:272/2420 train_time:16297ms step_avg:59.91ms
step:273/2420 train_time:16355ms step_avg:59.91ms
step:274/2420 train_time:16417ms step_avg:59.91ms
step:275/2420 train_time:16475ms step_avg:59.91ms
step:276/2420 train_time:16536ms step_avg:59.91ms
step:277/2420 train_time:16595ms step_avg:59.91ms
step:278/2420 train_time:16656ms step_avg:59.91ms
step:279/2420 train_time:16714ms step_avg:59.91ms
step:280/2420 train_time:16773ms step_avg:59.91ms
step:281/2420 train_time:16831ms step_avg:59.90ms
step:282/2420 train_time:16891ms step_avg:59.90ms
step:283/2420 train_time:16949ms step_avg:59.89ms
step:284/2420 train_time:17009ms step_avg:59.89ms
step:285/2420 train_time:17068ms step_avg:59.89ms
step:286/2420 train_time:17129ms step_avg:59.89ms
step:287/2420 train_time:17188ms step_avg:59.89ms
step:288/2420 train_time:17250ms step_avg:59.89ms
step:289/2420 train_time:17309ms step_avg:59.89ms
step:290/2420 train_time:17370ms step_avg:59.90ms
step:291/2420 train_time:17429ms step_avg:59.89ms
step:292/2420 train_time:17490ms step_avg:59.90ms
step:293/2420 train_time:17549ms step_avg:59.89ms
step:294/2420 train_time:17610ms step_avg:59.90ms
step:295/2420 train_time:17668ms step_avg:59.89ms
step:296/2420 train_time:17729ms step_avg:59.89ms
step:297/2420 train_time:17787ms step_avg:59.89ms
step:298/2420 train_time:17847ms step_avg:59.89ms
step:299/2420 train_time:17905ms step_avg:59.88ms
step:300/2420 train_time:17965ms step_avg:59.88ms
step:301/2420 train_time:18023ms step_avg:59.88ms
step:302/2420 train_time:18084ms step_avg:59.88ms
step:303/2420 train_time:18142ms step_avg:59.87ms
step:304/2420 train_time:18203ms step_avg:59.88ms
step:305/2420 train_time:18261ms step_avg:59.87ms
step:306/2420 train_time:18322ms step_avg:59.88ms
step:307/2420 train_time:18381ms step_avg:59.87ms
step:308/2420 train_time:18442ms step_avg:59.88ms
step:309/2420 train_time:18501ms step_avg:59.87ms
step:310/2420 train_time:18562ms step_avg:59.88ms
step:311/2420 train_time:18620ms step_avg:59.87ms
step:312/2420 train_time:18681ms step_avg:59.87ms
step:313/2420 train_time:18739ms step_avg:59.87ms
step:314/2420 train_time:18799ms step_avg:59.87ms
step:315/2420 train_time:18857ms step_avg:59.86ms
step:316/2420 train_time:18918ms step_avg:59.87ms
step:317/2420 train_time:18976ms step_avg:59.86ms
step:318/2420 train_time:19036ms step_avg:59.86ms
step:319/2420 train_time:19094ms step_avg:59.86ms
step:320/2420 train_time:19155ms step_avg:59.86ms
step:321/2420 train_time:19214ms step_avg:59.86ms
step:322/2420 train_time:19275ms step_avg:59.86ms
step:323/2420 train_time:19334ms step_avg:59.86ms
step:324/2420 train_time:19395ms step_avg:59.86ms
step:325/2420 train_time:19453ms step_avg:59.86ms
step:326/2420 train_time:19515ms step_avg:59.86ms
step:327/2420 train_time:19573ms step_avg:59.86ms
step:328/2420 train_time:19634ms step_avg:59.86ms
step:329/2420 train_time:19693ms step_avg:59.86ms
step:330/2420 train_time:19753ms step_avg:59.86ms
step:331/2420 train_time:19812ms step_avg:59.86ms
step:332/2420 train_time:19872ms step_avg:59.86ms
step:333/2420 train_time:19930ms step_avg:59.85ms
step:334/2420 train_time:19991ms step_avg:59.85ms
step:335/2420 train_time:20049ms step_avg:59.85ms
step:336/2420 train_time:20109ms step_avg:59.85ms
step:337/2420 train_time:20168ms step_avg:59.85ms
step:338/2420 train_time:20229ms step_avg:59.85ms
step:339/2420 train_time:20288ms step_avg:59.85ms
step:340/2420 train_time:20349ms step_avg:59.85ms
step:341/2420 train_time:20408ms step_avg:59.85ms
step:342/2420 train_time:20469ms step_avg:59.85ms
step:343/2420 train_time:20528ms step_avg:59.85ms
step:344/2420 train_time:20588ms step_avg:59.85ms
step:345/2420 train_time:20647ms step_avg:59.85ms
step:346/2420 train_time:20708ms step_avg:59.85ms
step:347/2420 train_time:20766ms step_avg:59.84ms
step:348/2420 train_time:20826ms step_avg:59.85ms
step:349/2420 train_time:20885ms step_avg:59.84ms
step:350/2420 train_time:20945ms step_avg:59.84ms
step:351/2420 train_time:21004ms step_avg:59.84ms
step:352/2420 train_time:21064ms step_avg:59.84ms
step:353/2420 train_time:21121ms step_avg:59.83ms
step:354/2420 train_time:21182ms step_avg:59.84ms
step:355/2420 train_time:21240ms step_avg:59.83ms
step:356/2420 train_time:21301ms step_avg:59.83ms
step:357/2420 train_time:21359ms step_avg:59.83ms
step:358/2420 train_time:21419ms step_avg:59.83ms
step:359/2420 train_time:21478ms step_avg:59.83ms
step:360/2420 train_time:21539ms step_avg:59.83ms
step:361/2420 train_time:21597ms step_avg:59.83ms
step:362/2420 train_time:21657ms step_avg:59.83ms
step:363/2420 train_time:21716ms step_avg:59.82ms
step:364/2420 train_time:21777ms step_avg:59.83ms
step:365/2420 train_time:21835ms step_avg:59.82ms
step:366/2420 train_time:21896ms step_avg:59.83ms
step:367/2420 train_time:21955ms step_avg:59.82ms
step:368/2420 train_time:22016ms step_avg:59.82ms
step:369/2420 train_time:22074ms step_avg:59.82ms
step:370/2420 train_time:22135ms step_avg:59.82ms
step:371/2420 train_time:22194ms step_avg:59.82ms
step:372/2420 train_time:22255ms step_avg:59.83ms
step:373/2420 train_time:22314ms step_avg:59.82ms
step:374/2420 train_time:22374ms step_avg:59.82ms
step:375/2420 train_time:22432ms step_avg:59.82ms
step:376/2420 train_time:22493ms step_avg:59.82ms
step:377/2420 train_time:22553ms step_avg:59.82ms
step:378/2420 train_time:22614ms step_avg:59.82ms
step:379/2420 train_time:22673ms step_avg:59.82ms
step:380/2420 train_time:22733ms step_avg:59.82ms
step:381/2420 train_time:22791ms step_avg:59.82ms
step:382/2420 train_time:22852ms step_avg:59.82ms
step:383/2420 train_time:22911ms step_avg:59.82ms
step:384/2420 train_time:22971ms step_avg:59.82ms
step:385/2420 train_time:23030ms step_avg:59.82ms
step:386/2420 train_time:23090ms step_avg:59.82ms
step:387/2420 train_time:23149ms step_avg:59.82ms
step:388/2420 train_time:23209ms step_avg:59.82ms
step:389/2420 train_time:23268ms step_avg:59.81ms
step:390/2420 train_time:23328ms step_avg:59.82ms
step:391/2420 train_time:23387ms step_avg:59.81ms
step:392/2420 train_time:23448ms step_avg:59.82ms
step:393/2420 train_time:23507ms step_avg:59.81ms
step:394/2420 train_time:23568ms step_avg:59.82ms
step:395/2420 train_time:23627ms step_avg:59.81ms
step:396/2420 train_time:23688ms step_avg:59.82ms
step:397/2420 train_time:23746ms step_avg:59.81ms
step:398/2420 train_time:23807ms step_avg:59.82ms
step:399/2420 train_time:23865ms step_avg:59.81ms
step:400/2420 train_time:23925ms step_avg:59.81ms
step:401/2420 train_time:23984ms step_avg:59.81ms
step:402/2420 train_time:24044ms step_avg:59.81ms
step:403/2420 train_time:24102ms step_avg:59.81ms
step:404/2420 train_time:24163ms step_avg:59.81ms
step:405/2420 train_time:24221ms step_avg:59.80ms
step:406/2420 train_time:24281ms step_avg:59.81ms
step:407/2420 train_time:24340ms step_avg:59.80ms
step:408/2420 train_time:24400ms step_avg:59.80ms
step:409/2420 train_time:24458ms step_avg:59.80ms
step:410/2420 train_time:24518ms step_avg:59.80ms
step:411/2420 train_time:24576ms step_avg:59.80ms
step:412/2420 train_time:24637ms step_avg:59.80ms
step:413/2420 train_time:24695ms step_avg:59.80ms
step:414/2420 train_time:24756ms step_avg:59.80ms
step:415/2420 train_time:24814ms step_avg:59.79ms
step:416/2420 train_time:24876ms step_avg:59.80ms
step:417/2420 train_time:24934ms step_avg:59.79ms
step:418/2420 train_time:24994ms step_avg:59.80ms
step:419/2420 train_time:25053ms step_avg:59.79ms
step:420/2420 train_time:25114ms step_avg:59.80ms
step:421/2420 train_time:25173ms step_avg:59.79ms
step:422/2420 train_time:25233ms step_avg:59.79ms
step:423/2420 train_time:25291ms step_avg:59.79ms
step:424/2420 train_time:25352ms step_avg:59.79ms
step:425/2420 train_time:25411ms step_avg:59.79ms
step:426/2420 train_time:25472ms step_avg:59.79ms
step:427/2420 train_time:25530ms step_avg:59.79ms
step:428/2420 train_time:25591ms step_avg:59.79ms
step:429/2420 train_time:25650ms step_avg:59.79ms
step:430/2420 train_time:25710ms step_avg:59.79ms
step:431/2420 train_time:25769ms step_avg:59.79ms
step:432/2420 train_time:25830ms step_avg:59.79ms
step:433/2420 train_time:25888ms step_avg:59.79ms
step:434/2420 train_time:25949ms step_avg:59.79ms
step:435/2420 train_time:26007ms step_avg:59.79ms
step:436/2420 train_time:26068ms step_avg:59.79ms
step:437/2420 train_time:26126ms step_avg:59.79ms
step:438/2420 train_time:26187ms step_avg:59.79ms
step:439/2420 train_time:26245ms step_avg:59.78ms
step:440/2420 train_time:26305ms step_avg:59.78ms
step:441/2420 train_time:26363ms step_avg:59.78ms
step:442/2420 train_time:26424ms step_avg:59.78ms
step:443/2420 train_time:26482ms step_avg:59.78ms
step:444/2420 train_time:26543ms step_avg:59.78ms
step:445/2420 train_time:26601ms step_avg:59.78ms
step:446/2420 train_time:26661ms step_avg:59.78ms
step:447/2420 train_time:26719ms step_avg:59.78ms
step:448/2420 train_time:26780ms step_avg:59.78ms
step:449/2420 train_time:26838ms step_avg:59.77ms
step:450/2420 train_time:26899ms step_avg:59.78ms
step:451/2420 train_time:26958ms step_avg:59.77ms
step:452/2420 train_time:27018ms step_avg:59.77ms
step:453/2420 train_time:27077ms step_avg:59.77ms
step:454/2420 train_time:27137ms step_avg:59.77ms
step:455/2420 train_time:27196ms step_avg:59.77ms
step:456/2420 train_time:27256ms step_avg:59.77ms
step:457/2420 train_time:27315ms step_avg:59.77ms
step:458/2420 train_time:27376ms step_avg:59.77ms
step:459/2420 train_time:27434ms step_avg:59.77ms
step:460/2420 train_time:27494ms step_avg:59.77ms
step:461/2420 train_time:27553ms step_avg:59.77ms
step:462/2420 train_time:27613ms step_avg:59.77ms
step:463/2420 train_time:27672ms step_avg:59.77ms
step:464/2420 train_time:27732ms step_avg:59.77ms
step:465/2420 train_time:27792ms step_avg:59.77ms
step:466/2420 train_time:27853ms step_avg:59.77ms
step:467/2420 train_time:27912ms step_avg:59.77ms
step:468/2420 train_time:27972ms step_avg:59.77ms
step:469/2420 train_time:28031ms step_avg:59.77ms
step:470/2420 train_time:28091ms step_avg:59.77ms
step:471/2420 train_time:28150ms step_avg:59.77ms
step:472/2420 train_time:28210ms step_avg:59.77ms
step:473/2420 train_time:28269ms step_avg:59.77ms
step:474/2420 train_time:28330ms step_avg:59.77ms
step:475/2420 train_time:28389ms step_avg:59.77ms
step:476/2420 train_time:28450ms step_avg:59.77ms
step:477/2420 train_time:28509ms step_avg:59.77ms
step:478/2420 train_time:28569ms step_avg:59.77ms
step:479/2420 train_time:28628ms step_avg:59.77ms
step:480/2420 train_time:28688ms step_avg:59.77ms
step:481/2420 train_time:28747ms step_avg:59.76ms
step:482/2420 train_time:28807ms step_avg:59.77ms
step:483/2420 train_time:28866ms step_avg:59.76ms
step:484/2420 train_time:28926ms step_avg:59.77ms
step:485/2420 train_time:28985ms step_avg:59.76ms
step:486/2420 train_time:29046ms step_avg:59.77ms
step:487/2420 train_time:29105ms step_avg:59.76ms
step:488/2420 train_time:29165ms step_avg:59.76ms
step:489/2420 train_time:29224ms step_avg:59.76ms
step:490/2420 train_time:29285ms step_avg:59.76ms
step:491/2420 train_time:29343ms step_avg:59.76ms
step:492/2420 train_time:29404ms step_avg:59.76ms
step:493/2420 train_time:29462ms step_avg:59.76ms
step:494/2420 train_time:29522ms step_avg:59.76ms
step:495/2420 train_time:29581ms step_avg:59.76ms
step:496/2420 train_time:29641ms step_avg:59.76ms
step:497/2420 train_time:29700ms step_avg:59.76ms
step:498/2420 train_time:29760ms step_avg:59.76ms
step:499/2420 train_time:29818ms step_avg:59.76ms
step:500/2420 train_time:29879ms step_avg:59.76ms
step:500/2420 val_loss:3.8213 train_time:29942ms step_avg:59.88ms
step:501/2420 train_time:29961ms step_avg:59.80ms
step:502/2420 train_time:30000ms step_avg:59.76ms
step:503/2420 train_time:30064ms step_avg:59.77ms
step:504/2420 train_time:30128ms step_avg:59.78ms
step:505/2420 train_time:30188ms step_avg:59.78ms
step:506/2420 train_time:30248ms step_avg:59.78ms
step:507/2420 train_time:30307ms step_avg:59.78ms
step:508/2420 train_time:30367ms step_avg:59.78ms
step:509/2420 train_time:30424ms step_avg:59.77ms
step:510/2420 train_time:30484ms step_avg:59.77ms
step:511/2420 train_time:30542ms step_avg:59.77ms
step:512/2420 train_time:30602ms step_avg:59.77ms
step:513/2420 train_time:30660ms step_avg:59.77ms
step:514/2420 train_time:30720ms step_avg:59.77ms
step:515/2420 train_time:30779ms step_avg:59.76ms
step:516/2420 train_time:30838ms step_avg:59.76ms
step:517/2420 train_time:30897ms step_avg:59.76ms
step:518/2420 train_time:30958ms step_avg:59.76ms
step:519/2420 train_time:31018ms step_avg:59.76ms
step:520/2420 train_time:31080ms step_avg:59.77ms
step:521/2420 train_time:31140ms step_avg:59.77ms
step:522/2420 train_time:31203ms step_avg:59.78ms
step:523/2420 train_time:31261ms step_avg:59.77ms
step:524/2420 train_time:31321ms step_avg:59.77ms
step:525/2420 train_time:31380ms step_avg:59.77ms
step:526/2420 train_time:31440ms step_avg:59.77ms
step:527/2420 train_time:31498ms step_avg:59.77ms
step:528/2420 train_time:31558ms step_avg:59.77ms
step:529/2420 train_time:31617ms step_avg:59.77ms
step:530/2420 train_time:31677ms step_avg:59.77ms
step:531/2420 train_time:31735ms step_avg:59.76ms
step:532/2420 train_time:31795ms step_avg:59.77ms
step:533/2420 train_time:31853ms step_avg:59.76ms
step:534/2420 train_time:31913ms step_avg:59.76ms
step:535/2420 train_time:31972ms step_avg:59.76ms
step:536/2420 train_time:32033ms step_avg:59.76ms
step:537/2420 train_time:32092ms step_avg:59.76ms
step:538/2420 train_time:32154ms step_avg:59.77ms
step:539/2420 train_time:32213ms step_avg:59.76ms
step:540/2420 train_time:32274ms step_avg:59.77ms
step:541/2420 train_time:32333ms step_avg:59.76ms
step:542/2420 train_time:32393ms step_avg:59.77ms
step:543/2420 train_time:32451ms step_avg:59.76ms
step:544/2420 train_time:32511ms step_avg:59.76ms
step:545/2420 train_time:32569ms step_avg:59.76ms
step:546/2420 train_time:32630ms step_avg:59.76ms
step:547/2420 train_time:32688ms step_avg:59.76ms
step:548/2420 train_time:32748ms step_avg:59.76ms
step:549/2420 train_time:32806ms step_avg:59.76ms
step:550/2420 train_time:32867ms step_avg:59.76ms
step:551/2420 train_time:32925ms step_avg:59.76ms
step:552/2420 train_time:32986ms step_avg:59.76ms
step:553/2420 train_time:33046ms step_avg:59.76ms
step:554/2420 train_time:33107ms step_avg:59.76ms
step:555/2420 train_time:33167ms step_avg:59.76ms
step:556/2420 train_time:33228ms step_avg:59.76ms
step:557/2420 train_time:33287ms step_avg:59.76ms
step:558/2420 train_time:33347ms step_avg:59.76ms
step:559/2420 train_time:33406ms step_avg:59.76ms
step:560/2420 train_time:33467ms step_avg:59.76ms
step:561/2420 train_time:33526ms step_avg:59.76ms
step:562/2420 train_time:33586ms step_avg:59.76ms
step:563/2420 train_time:33644ms step_avg:59.76ms
step:564/2420 train_time:33705ms step_avg:59.76ms
step:565/2420 train_time:33763ms step_avg:59.76ms
step:566/2420 train_time:33823ms step_avg:59.76ms
step:567/2420 train_time:33881ms step_avg:59.76ms
step:568/2420 train_time:33942ms step_avg:59.76ms
step:569/2420 train_time:34000ms step_avg:59.75ms
step:570/2420 train_time:34061ms step_avg:59.76ms
step:571/2420 train_time:34120ms step_avg:59.76ms
step:572/2420 train_time:34181ms step_avg:59.76ms
step:573/2420 train_time:34240ms step_avg:59.76ms
step:574/2420 train_time:34301ms step_avg:59.76ms
step:575/2420 train_time:34360ms step_avg:59.76ms
step:576/2420 train_time:34421ms step_avg:59.76ms
step:577/2420 train_time:34480ms step_avg:59.76ms
step:578/2420 train_time:34540ms step_avg:59.76ms
step:579/2420 train_time:34599ms step_avg:59.76ms
step:580/2420 train_time:34659ms step_avg:59.76ms
step:581/2420 train_time:34717ms step_avg:59.75ms
step:582/2420 train_time:34778ms step_avg:59.76ms
step:583/2420 train_time:34836ms step_avg:59.75ms
step:584/2420 train_time:34896ms step_avg:59.75ms
step:585/2420 train_time:34955ms step_avg:59.75ms
step:586/2420 train_time:35015ms step_avg:59.75ms
step:587/2420 train_time:35074ms step_avg:59.75ms
step:588/2420 train_time:35135ms step_avg:59.75ms
step:589/2420 train_time:35194ms step_avg:59.75ms
step:590/2420 train_time:35254ms step_avg:59.75ms
step:591/2420 train_time:35313ms step_avg:59.75ms
step:592/2420 train_time:35374ms step_avg:59.75ms
step:593/2420 train_time:35433ms step_avg:59.75ms
step:594/2420 train_time:35493ms step_avg:59.75ms
step:595/2420 train_time:35552ms step_avg:59.75ms
step:596/2420 train_time:35612ms step_avg:59.75ms
step:597/2420 train_time:35670ms step_avg:59.75ms
step:598/2420 train_time:35731ms step_avg:59.75ms
step:599/2420 train_time:35789ms step_avg:59.75ms
step:600/2420 train_time:35849ms step_avg:59.75ms
step:601/2420 train_time:35908ms step_avg:59.75ms
step:602/2420 train_time:35968ms step_avg:59.75ms
step:603/2420 train_time:36026ms step_avg:59.75ms
step:604/2420 train_time:36087ms step_avg:59.75ms
step:605/2420 train_time:36145ms step_avg:59.74ms
step:606/2420 train_time:36206ms step_avg:59.75ms
step:607/2420 train_time:36265ms step_avg:59.74ms
step:608/2420 train_time:36326ms step_avg:59.75ms
step:609/2420 train_time:36385ms step_avg:59.75ms
step:610/2420 train_time:36446ms step_avg:59.75ms
step:611/2420 train_time:36504ms step_avg:59.75ms
step:612/2420 train_time:36566ms step_avg:59.75ms
step:613/2420 train_time:36625ms step_avg:59.75ms
step:614/2420 train_time:36685ms step_avg:59.75ms
step:615/2420 train_time:36743ms step_avg:59.74ms
step:616/2420 train_time:36804ms step_avg:59.75ms
step:617/2420 train_time:36863ms step_avg:59.75ms
step:618/2420 train_time:36923ms step_avg:59.75ms
step:619/2420 train_time:36981ms step_avg:59.74ms
step:620/2420 train_time:37042ms step_avg:59.75ms
step:621/2420 train_time:37101ms step_avg:59.74ms
step:622/2420 train_time:37162ms step_avg:59.75ms
step:623/2420 train_time:37221ms step_avg:59.74ms
step:624/2420 train_time:37281ms step_avg:59.75ms
step:625/2420 train_time:37340ms step_avg:59.74ms
step:626/2420 train_time:37401ms step_avg:59.75ms
step:627/2420 train_time:37460ms step_avg:59.74ms
step:628/2420 train_time:37521ms step_avg:59.75ms
step:629/2420 train_time:37579ms step_avg:59.74ms
step:630/2420 train_time:37640ms step_avg:59.75ms
step:631/2420 train_time:37699ms step_avg:59.74ms
step:632/2420 train_time:37759ms step_avg:59.75ms
step:633/2420 train_time:37818ms step_avg:59.74ms
step:634/2420 train_time:37879ms step_avg:59.75ms
step:635/2420 train_time:37938ms step_avg:59.74ms
step:636/2420 train_time:37999ms step_avg:59.75ms
step:637/2420 train_time:38057ms step_avg:59.74ms
step:638/2420 train_time:38118ms step_avg:59.75ms
step:639/2420 train_time:38177ms step_avg:59.74ms
step:640/2420 train_time:38238ms step_avg:59.75ms
step:641/2420 train_time:38296ms step_avg:59.74ms
step:642/2420 train_time:38357ms step_avg:59.75ms
step:643/2420 train_time:38416ms step_avg:59.74ms
step:644/2420 train_time:38477ms step_avg:59.75ms
step:645/2420 train_time:38536ms step_avg:59.74ms
step:646/2420 train_time:38596ms step_avg:59.75ms
step:647/2420 train_time:38655ms step_avg:59.74ms
step:648/2420 train_time:38715ms step_avg:59.75ms
step:649/2420 train_time:38773ms step_avg:59.74ms
step:650/2420 train_time:38833ms step_avg:59.74ms
step:651/2420 train_time:38892ms step_avg:59.74ms
step:652/2420 train_time:38953ms step_avg:59.74ms
step:653/2420 train_time:39011ms step_avg:59.74ms
step:654/2420 train_time:39071ms step_avg:59.74ms
step:655/2420 train_time:39129ms step_avg:59.74ms
step:656/2420 train_time:39190ms step_avg:59.74ms
step:657/2420 train_time:39248ms step_avg:59.74ms
step:658/2420 train_time:39309ms step_avg:59.74ms
step:659/2420 train_time:39367ms step_avg:59.74ms
step:660/2420 train_time:39428ms step_avg:59.74ms
step:661/2420 train_time:39487ms step_avg:59.74ms
step:662/2420 train_time:39547ms step_avg:59.74ms
step:663/2420 train_time:39606ms step_avg:59.74ms
step:664/2420 train_time:39667ms step_avg:59.74ms
step:665/2420 train_time:39726ms step_avg:59.74ms
step:666/2420 train_time:39786ms step_avg:59.74ms
step:667/2420 train_time:39845ms step_avg:59.74ms
step:668/2420 train_time:39907ms step_avg:59.74ms
step:669/2420 train_time:39966ms step_avg:59.74ms
step:670/2420 train_time:40027ms step_avg:59.74ms
step:671/2420 train_time:40085ms step_avg:59.74ms
step:672/2420 train_time:40146ms step_avg:59.74ms
step:673/2420 train_time:40205ms step_avg:59.74ms
step:674/2420 train_time:40267ms step_avg:59.74ms
step:675/2420 train_time:40325ms step_avg:59.74ms
step:676/2420 train_time:40385ms step_avg:59.74ms
step:677/2420 train_time:40443ms step_avg:59.74ms
step:678/2420 train_time:40504ms step_avg:59.74ms
step:679/2420 train_time:40564ms step_avg:59.74ms
step:680/2420 train_time:40624ms step_avg:59.74ms
step:681/2420 train_time:40683ms step_avg:59.74ms
step:682/2420 train_time:40743ms step_avg:59.74ms
step:683/2420 train_time:40802ms step_avg:59.74ms
step:684/2420 train_time:40864ms step_avg:59.74ms
step:685/2420 train_time:40923ms step_avg:59.74ms
step:686/2420 train_time:40984ms step_avg:59.74ms
step:687/2420 train_time:41043ms step_avg:59.74ms
step:688/2420 train_time:41103ms step_avg:59.74ms
step:689/2420 train_time:41163ms step_avg:59.74ms
step:690/2420 train_time:41224ms step_avg:59.74ms
step:691/2420 train_time:41282ms step_avg:59.74ms
step:692/2420 train_time:41342ms step_avg:59.74ms
step:693/2420 train_time:41401ms step_avg:59.74ms
step:694/2420 train_time:41462ms step_avg:59.74ms
step:695/2420 train_time:41521ms step_avg:59.74ms
step:696/2420 train_time:41582ms step_avg:59.74ms
step:697/2420 train_time:41640ms step_avg:59.74ms
step:698/2420 train_time:41700ms step_avg:59.74ms
step:699/2420 train_time:41760ms step_avg:59.74ms
step:700/2420 train_time:41820ms step_avg:59.74ms
step:701/2420 train_time:41879ms step_avg:59.74ms
step:702/2420 train_time:41940ms step_avg:59.74ms
step:703/2420 train_time:41999ms step_avg:59.74ms
step:704/2420 train_time:42059ms step_avg:59.74ms
step:705/2420 train_time:42118ms step_avg:59.74ms
step:706/2420 train_time:42179ms step_avg:59.74ms
step:707/2420 train_time:42237ms step_avg:59.74ms
step:708/2420 train_time:42298ms step_avg:59.74ms
step:709/2420 train_time:42357ms step_avg:59.74ms
step:710/2420 train_time:42417ms step_avg:59.74ms
step:711/2420 train_time:42476ms step_avg:59.74ms
step:712/2420 train_time:42536ms step_avg:59.74ms
step:713/2420 train_time:42594ms step_avg:59.74ms
step:714/2420 train_time:42655ms step_avg:59.74ms
step:715/2420 train_time:42714ms step_avg:59.74ms
step:716/2420 train_time:42774ms step_avg:59.74ms
step:717/2420 train_time:42833ms step_avg:59.74ms
step:718/2420 train_time:42893ms step_avg:59.74ms
step:719/2420 train_time:42952ms step_avg:59.74ms
step:720/2420 train_time:43012ms step_avg:59.74ms
step:721/2420 train_time:43070ms step_avg:59.74ms
step:722/2420 train_time:43131ms step_avg:59.74ms
step:723/2420 train_time:43189ms step_avg:59.74ms
step:724/2420 train_time:43250ms step_avg:59.74ms
step:725/2420 train_time:43308ms step_avg:59.74ms
step:726/2420 train_time:43369ms step_avg:59.74ms
step:727/2420 train_time:43427ms step_avg:59.73ms
step:728/2420 train_time:43487ms step_avg:59.74ms
step:729/2420 train_time:43545ms step_avg:59.73ms
step:730/2420 train_time:43606ms step_avg:59.73ms
step:731/2420 train_time:43664ms step_avg:59.73ms
step:732/2420 train_time:43725ms step_avg:59.73ms
step:733/2420 train_time:43784ms step_avg:59.73ms
step:734/2420 train_time:43845ms step_avg:59.73ms
step:735/2420 train_time:43904ms step_avg:59.73ms
step:736/2420 train_time:43964ms step_avg:59.73ms
step:737/2420 train_time:44023ms step_avg:59.73ms
step:738/2420 train_time:44084ms step_avg:59.73ms
step:739/2420 train_time:44143ms step_avg:59.73ms
step:740/2420 train_time:44204ms step_avg:59.73ms
step:741/2420 train_time:44263ms step_avg:59.73ms
step:742/2420 train_time:44323ms step_avg:59.74ms
step:743/2420 train_time:44382ms step_avg:59.73ms
step:744/2420 train_time:44443ms step_avg:59.74ms
step:745/2420 train_time:44502ms step_avg:59.73ms
step:746/2420 train_time:44562ms step_avg:59.73ms
step:747/2420 train_time:44621ms step_avg:59.73ms
step:748/2420 train_time:44681ms step_avg:59.73ms
step:749/2420 train_time:44740ms step_avg:59.73ms
step:750/2420 train_time:44801ms step_avg:59.73ms
step:750/2420 val_loss:3.6905 train_time:44864ms step_avg:59.82ms
step:751/2420 train_time:44883ms step_avg:59.76ms
step:752/2420 train_time:44924ms step_avg:59.74ms
step:753/2420 train_time:44985ms step_avg:59.74ms
step:754/2420 train_time:45051ms step_avg:59.75ms
step:755/2420 train_time:45111ms step_avg:59.75ms
step:756/2420 train_time:45172ms step_avg:59.75ms
step:757/2420 train_time:45230ms step_avg:59.75ms
step:758/2420 train_time:45290ms step_avg:59.75ms
step:759/2420 train_time:45349ms step_avg:59.75ms
step:760/2420 train_time:45409ms step_avg:59.75ms
step:761/2420 train_time:45466ms step_avg:59.75ms
step:762/2420 train_time:45526ms step_avg:59.75ms
step:763/2420 train_time:45583ms step_avg:59.74ms
step:764/2420 train_time:45643ms step_avg:59.74ms
step:765/2420 train_time:45700ms step_avg:59.74ms
step:766/2420 train_time:45760ms step_avg:59.74ms
step:767/2420 train_time:45819ms step_avg:59.74ms
step:768/2420 train_time:45880ms step_avg:59.74ms
step:769/2420 train_time:45940ms step_avg:59.74ms
step:770/2420 train_time:46002ms step_avg:59.74ms
step:771/2420 train_time:46062ms step_avg:59.74ms
step:772/2420 train_time:46124ms step_avg:59.75ms
step:773/2420 train_time:46182ms step_avg:59.74ms
step:774/2420 train_time:46242ms step_avg:59.74ms
step:775/2420 train_time:46301ms step_avg:59.74ms
step:776/2420 train_time:46361ms step_avg:59.74ms
step:777/2420 train_time:46419ms step_avg:59.74ms
step:778/2420 train_time:46479ms step_avg:59.74ms
step:779/2420 train_time:46537ms step_avg:59.74ms
step:780/2420 train_time:46597ms step_avg:59.74ms
step:781/2420 train_time:46654ms step_avg:59.74ms
step:782/2420 train_time:46715ms step_avg:59.74ms
step:783/2420 train_time:46773ms step_avg:59.74ms
step:784/2420 train_time:46834ms step_avg:59.74ms
step:785/2420 train_time:46894ms step_avg:59.74ms
step:786/2420 train_time:46955ms step_avg:59.74ms
step:787/2420 train_time:47015ms step_avg:59.74ms
step:788/2420 train_time:47075ms step_avg:59.74ms
step:789/2420 train_time:47135ms step_avg:59.74ms
step:790/2420 train_time:47196ms step_avg:59.74ms
step:791/2420 train_time:47255ms step_avg:59.74ms
step:792/2420 train_time:47316ms step_avg:59.74ms
step:793/2420 train_time:47375ms step_avg:59.74ms
step:794/2420 train_time:47435ms step_avg:59.74ms
step:795/2420 train_time:47494ms step_avg:59.74ms
step:796/2420 train_time:47554ms step_avg:59.74ms
step:797/2420 train_time:47613ms step_avg:59.74ms
step:798/2420 train_time:47674ms step_avg:59.74ms
step:799/2420 train_time:47732ms step_avg:59.74ms
step:800/2420 train_time:47793ms step_avg:59.74ms
step:801/2420 train_time:47853ms step_avg:59.74ms
step:802/2420 train_time:47914ms step_avg:59.74ms
step:803/2420 train_time:47975ms step_avg:59.74ms
step:804/2420 train_time:48036ms step_avg:59.75ms
step:805/2420 train_time:48096ms step_avg:59.75ms
step:806/2420 train_time:48157ms step_avg:59.75ms
step:807/2420 train_time:48216ms step_avg:59.75ms
step:808/2420 train_time:48277ms step_avg:59.75ms
step:809/2420 train_time:48336ms step_avg:59.75ms
step:810/2420 train_time:48398ms step_avg:59.75ms
step:811/2420 train_time:48457ms step_avg:59.75ms
step:812/2420 train_time:48517ms step_avg:59.75ms
step:813/2420 train_time:48576ms step_avg:59.75ms
step:814/2420 train_time:48637ms step_avg:59.75ms
step:815/2420 train_time:48696ms step_avg:59.75ms
step:816/2420 train_time:48757ms step_avg:59.75ms
step:817/2420 train_time:48816ms step_avg:59.75ms
step:818/2420 train_time:48877ms step_avg:59.75ms
step:819/2420 train_time:48936ms step_avg:59.75ms
step:820/2420 train_time:48998ms step_avg:59.75ms
step:821/2420 train_time:49057ms step_avg:59.75ms
step:822/2420 train_time:49118ms step_avg:59.75ms
step:823/2420 train_time:49178ms step_avg:59.75ms
step:824/2420 train_time:49239ms step_avg:59.76ms
step:825/2420 train_time:49299ms step_avg:59.76ms
step:826/2420 train_time:49360ms step_avg:59.76ms
step:827/2420 train_time:49419ms step_avg:59.76ms
step:828/2420 train_time:49480ms step_avg:59.76ms
step:829/2420 train_time:49539ms step_avg:59.76ms
step:830/2420 train_time:49600ms step_avg:59.76ms
step:831/2420 train_time:49659ms step_avg:59.76ms
step:832/2420 train_time:49720ms step_avg:59.76ms
step:833/2420 train_time:49779ms step_avg:59.76ms
step:834/2420 train_time:49841ms step_avg:59.76ms
step:835/2420 train_time:49900ms step_avg:59.76ms
step:836/2420 train_time:49961ms step_avg:59.76ms
step:837/2420 train_time:50021ms step_avg:59.76ms
step:838/2420 train_time:50082ms step_avg:59.76ms
step:839/2420 train_time:50142ms step_avg:59.76ms
step:840/2420 train_time:50203ms step_avg:59.77ms
step:841/2420 train_time:50262ms step_avg:59.76ms
step:842/2420 train_time:50323ms step_avg:59.77ms
step:843/2420 train_time:50382ms step_avg:59.77ms
step:844/2420 train_time:50443ms step_avg:59.77ms
step:845/2420 train_time:50502ms step_avg:59.77ms
step:846/2420 train_time:50563ms step_avg:59.77ms
step:847/2420 train_time:50622ms step_avg:59.77ms
step:848/2420 train_time:50684ms step_avg:59.77ms
step:849/2420 train_time:50743ms step_avg:59.77ms
step:850/2420 train_time:50804ms step_avg:59.77ms
step:851/2420 train_time:50864ms step_avg:59.77ms
step:852/2420 train_time:50925ms step_avg:59.77ms
step:853/2420 train_time:50985ms step_avg:59.77ms
step:854/2420 train_time:51046ms step_avg:59.77ms
step:855/2420 train_time:51105ms step_avg:59.77ms
step:856/2420 train_time:51167ms step_avg:59.77ms
step:857/2420 train_time:51226ms step_avg:59.77ms
step:858/2420 train_time:51287ms step_avg:59.78ms
step:859/2420 train_time:51347ms step_avg:59.78ms
step:860/2420 train_time:51408ms step_avg:59.78ms
step:861/2420 train_time:51467ms step_avg:59.78ms
step:862/2420 train_time:51529ms step_avg:59.78ms
step:863/2420 train_time:51588ms step_avg:59.78ms
step:864/2420 train_time:51649ms step_avg:59.78ms
step:865/2420 train_time:51709ms step_avg:59.78ms
step:866/2420 train_time:51771ms step_avg:59.78ms
step:867/2420 train_time:51830ms step_avg:59.78ms
step:868/2420 train_time:51891ms step_avg:59.78ms
step:869/2420 train_time:51951ms step_avg:59.78ms
step:870/2420 train_time:52013ms step_avg:59.78ms
step:871/2420 train_time:52073ms step_avg:59.79ms
step:872/2420 train_time:52134ms step_avg:59.79ms
step:873/2420 train_time:52194ms step_avg:59.79ms
step:874/2420 train_time:52256ms step_avg:59.79ms
step:875/2420 train_time:52315ms step_avg:59.79ms
step:876/2420 train_time:52376ms step_avg:59.79ms
step:877/2420 train_time:52436ms step_avg:59.79ms
step:878/2420 train_time:52497ms step_avg:59.79ms
step:879/2420 train_time:52557ms step_avg:59.79ms
step:880/2420 train_time:52618ms step_avg:59.79ms
step:881/2420 train_time:52677ms step_avg:59.79ms
step:882/2420 train_time:52738ms step_avg:59.79ms
step:883/2420 train_time:52797ms step_avg:59.79ms
step:884/2420 train_time:52858ms step_avg:59.79ms
step:885/2420 train_time:52917ms step_avg:59.79ms
step:886/2420 train_time:52978ms step_avg:59.79ms
step:887/2420 train_time:53037ms step_avg:59.79ms
step:888/2420 train_time:53099ms step_avg:59.80ms
step:889/2420 train_time:53158ms step_avg:59.79ms
step:890/2420 train_time:53219ms step_avg:59.80ms
step:891/2420 train_time:53278ms step_avg:59.80ms
step:892/2420 train_time:53339ms step_avg:59.80ms
step:893/2420 train_time:53398ms step_avg:59.80ms
step:894/2420 train_time:53459ms step_avg:59.80ms
step:895/2420 train_time:53519ms step_avg:59.80ms
step:896/2420 train_time:53580ms step_avg:59.80ms
step:897/2420 train_time:53639ms step_avg:59.80ms
step:898/2420 train_time:53700ms step_avg:59.80ms
step:899/2420 train_time:53759ms step_avg:59.80ms
step:900/2420 train_time:53819ms step_avg:59.80ms
step:901/2420 train_time:53879ms step_avg:59.80ms
step:902/2420 train_time:53940ms step_avg:59.80ms
step:903/2420 train_time:53999ms step_avg:59.80ms
step:904/2420 train_time:54060ms step_avg:59.80ms
step:905/2420 train_time:54119ms step_avg:59.80ms
step:906/2420 train_time:54180ms step_avg:59.80ms
step:907/2420 train_time:54239ms step_avg:59.80ms
step:908/2420 train_time:54300ms step_avg:59.80ms
step:909/2420 train_time:54359ms step_avg:59.80ms
step:910/2420 train_time:54421ms step_avg:59.80ms
step:911/2420 train_time:54480ms step_avg:59.80ms
step:912/2420 train_time:54541ms step_avg:59.80ms
step:913/2420 train_time:54600ms step_avg:59.80ms
step:914/2420 train_time:54661ms step_avg:59.80ms
step:915/2420 train_time:54720ms step_avg:59.80ms
step:916/2420 train_time:54781ms step_avg:59.80ms
step:917/2420 train_time:54840ms step_avg:59.80ms
step:918/2420 train_time:54901ms step_avg:59.81ms
step:919/2420 train_time:54961ms step_avg:59.80ms
step:920/2420 train_time:55022ms step_avg:59.81ms
step:921/2420 train_time:55081ms step_avg:59.81ms
step:922/2420 train_time:55142ms step_avg:59.81ms
step:923/2420 train_time:55201ms step_avg:59.81ms
step:924/2420 train_time:55262ms step_avg:59.81ms
step:925/2420 train_time:55321ms step_avg:59.81ms
step:926/2420 train_time:55382ms step_avg:59.81ms
step:927/2420 train_time:55441ms step_avg:59.81ms
step:928/2420 train_time:55503ms step_avg:59.81ms
step:929/2420 train_time:55562ms step_avg:59.81ms
step:930/2420 train_time:55623ms step_avg:59.81ms
step:931/2420 train_time:55683ms step_avg:59.81ms
step:932/2420 train_time:55744ms step_avg:59.81ms
step:933/2420 train_time:55803ms step_avg:59.81ms
step:934/2420 train_time:55864ms step_avg:59.81ms
step:935/2420 train_time:55923ms step_avg:59.81ms
step:936/2420 train_time:55984ms step_avg:59.81ms
step:937/2420 train_time:56043ms step_avg:59.81ms
step:938/2420 train_time:56104ms step_avg:59.81ms
step:939/2420 train_time:56163ms step_avg:59.81ms
step:940/2420 train_time:56224ms step_avg:59.81ms
step:941/2420 train_time:56284ms step_avg:59.81ms
step:942/2420 train_time:56345ms step_avg:59.81ms
step:943/2420 train_time:56404ms step_avg:59.81ms
step:944/2420 train_time:56465ms step_avg:59.82ms
step:945/2420 train_time:56525ms step_avg:59.81ms
step:946/2420 train_time:56587ms step_avg:59.82ms
step:947/2420 train_time:56646ms step_avg:59.82ms
step:948/2420 train_time:56707ms step_avg:59.82ms
step:949/2420 train_time:56766ms step_avg:59.82ms
step:950/2420 train_time:56828ms step_avg:59.82ms
step:951/2420 train_time:56887ms step_avg:59.82ms
step:952/2420 train_time:56949ms step_avg:59.82ms
step:953/2420 train_time:57008ms step_avg:59.82ms
step:954/2420 train_time:57069ms step_avg:59.82ms
step:955/2420 train_time:57129ms step_avg:59.82ms
step:956/2420 train_time:57190ms step_avg:59.82ms
step:957/2420 train_time:57250ms step_avg:59.82ms
step:958/2420 train_time:57311ms step_avg:59.82ms
step:959/2420 train_time:57371ms step_avg:59.82ms
step:960/2420 train_time:57433ms step_avg:59.83ms
step:961/2420 train_time:57492ms step_avg:59.83ms
step:962/2420 train_time:57554ms step_avg:59.83ms
step:963/2420 train_time:57614ms step_avg:59.83ms
step:964/2420 train_time:57675ms step_avg:59.83ms
step:965/2420 train_time:57735ms step_avg:59.83ms
step:966/2420 train_time:57795ms step_avg:59.83ms
step:967/2420 train_time:57855ms step_avg:59.83ms
step:968/2420 train_time:57916ms step_avg:59.83ms
step:969/2420 train_time:57975ms step_avg:59.83ms
step:970/2420 train_time:58037ms step_avg:59.83ms
step:971/2420 train_time:58096ms step_avg:59.83ms
step:972/2420 train_time:58157ms step_avg:59.83ms
step:973/2420 train_time:58217ms step_avg:59.83ms
step:974/2420 train_time:58278ms step_avg:59.83ms
step:975/2420 train_time:58337ms step_avg:59.83ms
step:976/2420 train_time:58398ms step_avg:59.83ms
step:977/2420 train_time:58458ms step_avg:59.83ms
step:978/2420 train_time:58519ms step_avg:59.84ms
step:979/2420 train_time:58578ms step_avg:59.83ms
step:980/2420 train_time:58640ms step_avg:59.84ms
step:981/2420 train_time:58700ms step_avg:59.84ms
step:982/2420 train_time:58760ms step_avg:59.84ms
step:983/2420 train_time:58820ms step_avg:59.84ms
step:984/2420 train_time:58881ms step_avg:59.84ms
step:985/2420 train_time:58940ms step_avg:59.84ms
step:986/2420 train_time:59000ms step_avg:59.84ms
step:987/2420 train_time:59060ms step_avg:59.84ms
step:988/2420 train_time:59121ms step_avg:59.84ms
step:989/2420 train_time:59180ms step_avg:59.84ms
step:990/2420 train_time:59242ms step_avg:59.84ms
step:991/2420 train_time:59301ms step_avg:59.84ms
step:992/2420 train_time:59362ms step_avg:59.84ms
step:993/2420 train_time:59421ms step_avg:59.84ms
step:994/2420 train_time:59482ms step_avg:59.84ms
step:995/2420 train_time:59541ms step_avg:59.84ms
step:996/2420 train_time:59602ms step_avg:59.84ms
step:997/2420 train_time:59661ms step_avg:59.84ms
step:998/2420 train_time:59722ms step_avg:59.84ms
step:999/2420 train_time:59781ms step_avg:59.84ms
step:1000/2420 train_time:59842ms step_avg:59.84ms
step:1000/2420 val_loss:3.5791 train_time:59905ms step_avg:59.91ms
step:1001/2420 train_time:59925ms step_avg:59.86ms
step:1002/2420 train_time:59964ms step_avg:59.84ms
step:1003/2420 train_time:60022ms step_avg:59.84ms
step:1004/2420 train_time:60085ms step_avg:59.85ms
step:1005/2420 train_time:60146ms step_avg:59.85ms
step:1006/2420 train_time:60209ms step_avg:59.85ms
step:1007/2420 train_time:60268ms step_avg:59.85ms
step:1008/2420 train_time:60329ms step_avg:59.85ms
step:1009/2420 train_time:60388ms step_avg:59.85ms
step:1010/2420 train_time:60448ms step_avg:59.85ms
step:1011/2420 train_time:60507ms step_avg:59.85ms
step:1012/2420 train_time:60567ms step_avg:59.85ms
step:1013/2420 train_time:60626ms step_avg:59.85ms
step:1014/2420 train_time:60687ms step_avg:59.85ms
step:1015/2420 train_time:60746ms step_avg:59.85ms
step:1016/2420 train_time:60807ms step_avg:59.85ms
step:1017/2420 train_time:60870ms step_avg:59.85ms
step:1018/2420 train_time:60932ms step_avg:59.85ms
step:1019/2420 train_time:60993ms step_avg:59.86ms
step:1020/2420 train_time:61056ms step_avg:59.86ms
step:1021/2420 train_time:61116ms step_avg:59.86ms
step:1022/2420 train_time:61177ms step_avg:59.86ms
step:1023/2420 train_time:61236ms step_avg:59.86ms
step:1024/2420 train_time:61297ms step_avg:59.86ms
step:1025/2420 train_time:61356ms step_avg:59.86ms
step:1026/2420 train_time:61417ms step_avg:59.86ms
step:1027/2420 train_time:61476ms step_avg:59.86ms
step:1028/2420 train_time:61536ms step_avg:59.86ms
step:1029/2420 train_time:61595ms step_avg:59.86ms
step:1030/2420 train_time:61656ms step_avg:59.86ms
step:1031/2420 train_time:61714ms step_avg:59.86ms
step:1032/2420 train_time:61776ms step_avg:59.86ms
step:1033/2420 train_time:61836ms step_avg:59.86ms
step:1034/2420 train_time:61898ms step_avg:59.86ms
step:1035/2420 train_time:61957ms step_avg:59.86ms
step:1036/2420 train_time:62019ms step_avg:59.86ms
step:1037/2420 train_time:62078ms step_avg:59.86ms
step:1038/2420 train_time:62140ms step_avg:59.86ms
step:1039/2420 train_time:62200ms step_avg:59.86ms
step:1040/2420 train_time:62262ms step_avg:59.87ms
step:1041/2420 train_time:62320ms step_avg:59.87ms
step:1042/2420 train_time:62382ms step_avg:59.87ms
step:1043/2420 train_time:62440ms step_avg:59.87ms
step:1044/2420 train_time:62501ms step_avg:59.87ms
step:1045/2420 train_time:62560ms step_avg:59.87ms
step:1046/2420 train_time:62620ms step_avg:59.87ms
step:1047/2420 train_time:62679ms step_avg:59.87ms
step:1048/2420 train_time:62741ms step_avg:59.87ms
step:1049/2420 train_time:62800ms step_avg:59.87ms
step:1050/2420 train_time:62862ms step_avg:59.87ms
step:1051/2420 train_time:62921ms step_avg:59.87ms
step:1052/2420 train_time:62983ms step_avg:59.87ms
step:1053/2420 train_time:63042ms step_avg:59.87ms
step:1054/2420 train_time:63104ms step_avg:59.87ms
step:1055/2420 train_time:63164ms step_avg:59.87ms
step:1056/2420 train_time:63226ms step_avg:59.87ms
step:1057/2420 train_time:63285ms step_avg:59.87ms
step:1058/2420 train_time:63347ms step_avg:59.87ms
step:1059/2420 train_time:63406ms step_avg:59.87ms
step:1060/2420 train_time:63467ms step_avg:59.87ms
step:1061/2420 train_time:63526ms step_avg:59.87ms
step:1062/2420 train_time:63588ms step_avg:59.88ms
step:1063/2420 train_time:63648ms step_avg:59.88ms
step:1064/2420 train_time:63709ms step_avg:59.88ms
step:1065/2420 train_time:63769ms step_avg:59.88ms
step:1066/2420 train_time:63830ms step_avg:59.88ms
step:1067/2420 train_time:63890ms step_avg:59.88ms
step:1068/2420 train_time:63952ms step_avg:59.88ms
step:1069/2420 train_time:64012ms step_avg:59.88ms
step:1070/2420 train_time:64074ms step_avg:59.88ms
step:1071/2420 train_time:64133ms step_avg:59.88ms
step:1072/2420 train_time:64195ms step_avg:59.88ms
step:1073/2420 train_time:64255ms step_avg:59.88ms
step:1074/2420 train_time:64316ms step_avg:59.88ms
step:1075/2420 train_time:64375ms step_avg:59.88ms
step:1076/2420 train_time:64436ms step_avg:59.88ms
step:1077/2420 train_time:64495ms step_avg:59.88ms
step:1078/2420 train_time:64556ms step_avg:59.88ms
step:1079/2420 train_time:64615ms step_avg:59.88ms
step:1080/2420 train_time:64676ms step_avg:59.89ms
step:1081/2420 train_time:64735ms step_avg:59.88ms
step:1082/2420 train_time:64796ms step_avg:59.89ms
step:1083/2420 train_time:64855ms step_avg:59.88ms
step:1084/2420 train_time:64916ms step_avg:59.89ms
step:1085/2420 train_time:64975ms step_avg:59.88ms
step:1086/2420 train_time:65037ms step_avg:59.89ms
step:1087/2420 train_time:65096ms step_avg:59.89ms
step:1088/2420 train_time:65157ms step_avg:59.89ms
step:1089/2420 train_time:65217ms step_avg:59.89ms
step:1090/2420 train_time:65278ms step_avg:59.89ms
step:1091/2420 train_time:65337ms step_avg:59.89ms
step:1092/2420 train_time:65398ms step_avg:59.89ms
step:1093/2420 train_time:65457ms step_avg:59.89ms
step:1094/2420 train_time:65518ms step_avg:59.89ms
step:1095/2420 train_time:65576ms step_avg:59.89ms
step:1096/2420 train_time:65637ms step_avg:59.89ms
step:1097/2420 train_time:65696ms step_avg:59.89ms
step:1098/2420 train_time:65757ms step_avg:59.89ms
step:1099/2420 train_time:65817ms step_avg:59.89ms
step:1100/2420 train_time:65878ms step_avg:59.89ms
step:1101/2420 train_time:65937ms step_avg:59.89ms
step:1102/2420 train_time:65999ms step_avg:59.89ms
step:1103/2420 train_time:66058ms step_avg:59.89ms
step:1104/2420 train_time:66120ms step_avg:59.89ms
step:1105/2420 train_time:66179ms step_avg:59.89ms
step:1106/2420 train_time:66240ms step_avg:59.89ms
step:1107/2420 train_time:66299ms step_avg:59.89ms
step:1108/2420 train_time:66360ms step_avg:59.89ms
step:1109/2420 train_time:66419ms step_avg:59.89ms
step:1110/2420 train_time:66480ms step_avg:59.89ms
step:1111/2420 train_time:66539ms step_avg:59.89ms
step:1112/2420 train_time:66600ms step_avg:59.89ms
step:1113/2420 train_time:66659ms step_avg:59.89ms
step:1114/2420 train_time:66721ms step_avg:59.89ms
step:1115/2420 train_time:66780ms step_avg:59.89ms
step:1116/2420 train_time:66841ms step_avg:59.89ms
step:1117/2420 train_time:66900ms step_avg:59.89ms
step:1118/2420 train_time:66961ms step_avg:59.89ms
step:1119/2420 train_time:67021ms step_avg:59.89ms
step:1120/2420 train_time:67082ms step_avg:59.89ms
step:1121/2420 train_time:67142ms step_avg:59.89ms
step:1122/2420 train_time:67204ms step_avg:59.90ms
step:1123/2420 train_time:67263ms step_avg:59.90ms
step:1124/2420 train_time:67324ms step_avg:59.90ms
step:1125/2420 train_time:67383ms step_avg:59.90ms
step:1126/2420 train_time:67445ms step_avg:59.90ms
step:1127/2420 train_time:67504ms step_avg:59.90ms
step:1128/2420 train_time:67565ms step_avg:59.90ms
step:1129/2420 train_time:67624ms step_avg:59.90ms
step:1130/2420 train_time:67686ms step_avg:59.90ms
step:1131/2420 train_time:67745ms step_avg:59.90ms
step:1132/2420 train_time:67806ms step_avg:59.90ms
step:1133/2420 train_time:67866ms step_avg:59.90ms
step:1134/2420 train_time:67927ms step_avg:59.90ms
step:1135/2420 train_time:67987ms step_avg:59.90ms
step:1136/2420 train_time:68048ms step_avg:59.90ms
step:1137/2420 train_time:68108ms step_avg:59.90ms
step:1138/2420 train_time:68169ms step_avg:59.90ms
step:1139/2420 train_time:68229ms step_avg:59.90ms
step:1140/2420 train_time:68291ms step_avg:59.90ms
step:1141/2420 train_time:68350ms step_avg:59.90ms
step:1142/2420 train_time:68412ms step_avg:59.91ms
step:1143/2420 train_time:68471ms step_avg:59.91ms
step:1144/2420 train_time:68532ms step_avg:59.91ms
step:1145/2420 train_time:68591ms step_avg:59.91ms
step:1146/2420 train_time:68653ms step_avg:59.91ms
step:1147/2420 train_time:68712ms step_avg:59.91ms
step:1148/2420 train_time:68773ms step_avg:59.91ms
step:1149/2420 train_time:68832ms step_avg:59.91ms
step:1150/2420 train_time:68894ms step_avg:59.91ms
step:1151/2420 train_time:68953ms step_avg:59.91ms
step:1152/2420 train_time:69015ms step_avg:59.91ms
step:1153/2420 train_time:69074ms step_avg:59.91ms
step:1154/2420 train_time:69135ms step_avg:59.91ms
step:1155/2420 train_time:69195ms step_avg:59.91ms
step:1156/2420 train_time:69256ms step_avg:59.91ms
step:1157/2420 train_time:69315ms step_avg:59.91ms
step:1158/2420 train_time:69376ms step_avg:59.91ms
step:1159/2420 train_time:69435ms step_avg:59.91ms
step:1160/2420 train_time:69496ms step_avg:59.91ms
step:1161/2420 train_time:69555ms step_avg:59.91ms
step:1162/2420 train_time:69616ms step_avg:59.91ms
step:1163/2420 train_time:69675ms step_avg:59.91ms
step:1164/2420 train_time:69736ms step_avg:59.91ms
step:1165/2420 train_time:69796ms step_avg:59.91ms
step:1166/2420 train_time:69857ms step_avg:59.91ms
step:1167/2420 train_time:69916ms step_avg:59.91ms
step:1168/2420 train_time:69977ms step_avg:59.91ms
step:1169/2420 train_time:70037ms step_avg:59.91ms
step:1170/2420 train_time:70098ms step_avg:59.91ms
step:1171/2420 train_time:70157ms step_avg:59.91ms
step:1172/2420 train_time:70218ms step_avg:59.91ms
step:1173/2420 train_time:70277ms step_avg:59.91ms
step:1174/2420 train_time:70338ms step_avg:59.91ms
step:1175/2420 train_time:70398ms step_avg:59.91ms
step:1176/2420 train_time:70459ms step_avg:59.91ms
step:1177/2420 train_time:70518ms step_avg:59.91ms
step:1178/2420 train_time:70579ms step_avg:59.91ms
step:1179/2420 train_time:70638ms step_avg:59.91ms
step:1180/2420 train_time:70699ms step_avg:59.91ms
step:1181/2420 train_time:70758ms step_avg:59.91ms
step:1182/2420 train_time:70818ms step_avg:59.91ms
step:1183/2420 train_time:70878ms step_avg:59.91ms
step:1184/2420 train_time:70939ms step_avg:59.91ms
step:1185/2420 train_time:70998ms step_avg:59.91ms
step:1186/2420 train_time:71059ms step_avg:59.91ms
step:1187/2420 train_time:71118ms step_avg:59.91ms
step:1188/2420 train_time:71179ms step_avg:59.92ms
step:1189/2420 train_time:71238ms step_avg:59.91ms
step:1190/2420 train_time:71300ms step_avg:59.92ms
step:1191/2420 train_time:71359ms step_avg:59.92ms
step:1192/2420 train_time:71420ms step_avg:59.92ms
step:1193/2420 train_time:71479ms step_avg:59.92ms
step:1194/2420 train_time:71541ms step_avg:59.92ms
step:1195/2420 train_time:71600ms step_avg:59.92ms
step:1196/2420 train_time:71661ms step_avg:59.92ms
step:1197/2420 train_time:71720ms step_avg:59.92ms
step:1198/2420 train_time:71782ms step_avg:59.92ms
step:1199/2420 train_time:71841ms step_avg:59.92ms
step:1200/2420 train_time:71903ms step_avg:59.92ms
step:1201/2420 train_time:71962ms step_avg:59.92ms
step:1202/2420 train_time:72023ms step_avg:59.92ms
step:1203/2420 train_time:72082ms step_avg:59.92ms
step:1204/2420 train_time:72144ms step_avg:59.92ms
step:1205/2420 train_time:72203ms step_avg:59.92ms
step:1206/2420 train_time:72264ms step_avg:59.92ms
step:1207/2420 train_time:72324ms step_avg:59.92ms
step:1208/2420 train_time:72385ms step_avg:59.92ms
step:1209/2420 train_time:72445ms step_avg:59.92ms
step:1210/2420 train_time:72506ms step_avg:59.92ms
step:1211/2420 train_time:72565ms step_avg:59.92ms
step:1212/2420 train_time:72627ms step_avg:59.92ms
step:1213/2420 train_time:72687ms step_avg:59.92ms
step:1214/2420 train_time:72749ms step_avg:59.92ms
step:1215/2420 train_time:72808ms step_avg:59.92ms
step:1216/2420 train_time:72870ms step_avg:59.93ms
step:1217/2420 train_time:72929ms step_avg:59.93ms
step:1218/2420 train_time:72991ms step_avg:59.93ms
step:1219/2420 train_time:73051ms step_avg:59.93ms
step:1220/2420 train_time:73113ms step_avg:59.93ms
step:1221/2420 train_time:73173ms step_avg:59.93ms
step:1222/2420 train_time:73234ms step_avg:59.93ms
step:1223/2420 train_time:73294ms step_avg:59.93ms
step:1224/2420 train_time:73354ms step_avg:59.93ms
step:1225/2420 train_time:73414ms step_avg:59.93ms
step:1226/2420 train_time:73476ms step_avg:59.93ms
step:1227/2420 train_time:73535ms step_avg:59.93ms
step:1228/2420 train_time:73596ms step_avg:59.93ms
step:1229/2420 train_time:73656ms step_avg:59.93ms
step:1230/2420 train_time:73716ms step_avg:59.93ms
step:1231/2420 train_time:73775ms step_avg:59.93ms
step:1232/2420 train_time:73836ms step_avg:59.93ms
step:1233/2420 train_time:73895ms step_avg:59.93ms
step:1234/2420 train_time:73956ms step_avg:59.93ms
step:1235/2420 train_time:74016ms step_avg:59.93ms
step:1236/2420 train_time:74077ms step_avg:59.93ms
step:1237/2420 train_time:74136ms step_avg:59.93ms
step:1238/2420 train_time:74197ms step_avg:59.93ms
step:1239/2420 train_time:74257ms step_avg:59.93ms
step:1240/2420 train_time:74317ms step_avg:59.93ms
step:1241/2420 train_time:74377ms step_avg:59.93ms
step:1242/2420 train_time:74438ms step_avg:59.93ms
step:1243/2420 train_time:74498ms step_avg:59.93ms
step:1244/2420 train_time:74558ms step_avg:59.93ms
step:1245/2420 train_time:74618ms step_avg:59.93ms
step:1246/2420 train_time:74679ms step_avg:59.93ms
step:1247/2420 train_time:74738ms step_avg:59.93ms
step:1248/2420 train_time:74800ms step_avg:59.94ms
step:1249/2420 train_time:74859ms step_avg:59.93ms
step:1250/2420 train_time:74920ms step_avg:59.94ms
step:1250/2420 val_loss:3.5205 train_time:74983ms step_avg:59.99ms
step:1251/2420 train_time:75003ms step_avg:59.95ms
step:1252/2420 train_time:75044ms step_avg:59.94ms
step:1253/2420 train_time:75108ms step_avg:59.94ms
step:1254/2420 train_time:75174ms step_avg:59.95ms
step:1255/2420 train_time:75233ms step_avg:59.95ms
step:1256/2420 train_time:75294ms step_avg:59.95ms
step:1257/2420 train_time:75352ms step_avg:59.95ms
step:1258/2420 train_time:75413ms step_avg:59.95ms
step:1259/2420 train_time:75472ms step_avg:59.95ms
step:1260/2420 train_time:75532ms step_avg:59.95ms
step:1261/2420 train_time:75591ms step_avg:59.95ms
step:1262/2420 train_time:75652ms step_avg:59.95ms
step:1263/2420 train_time:75710ms step_avg:59.94ms
step:1264/2420 train_time:75771ms step_avg:59.95ms
step:1265/2420 train_time:75830ms step_avg:59.94ms
step:1266/2420 train_time:75891ms step_avg:59.95ms
step:1267/2420 train_time:75950ms step_avg:59.94ms
step:1268/2420 train_time:76013ms step_avg:59.95ms
step:1269/2420 train_time:76074ms step_avg:59.95ms
step:1270/2420 train_time:76136ms step_avg:59.95ms
step:1271/2420 train_time:76196ms step_avg:59.95ms
step:1272/2420 train_time:76257ms step_avg:59.95ms
step:1273/2420 train_time:76316ms step_avg:59.95ms
step:1274/2420 train_time:76377ms step_avg:59.95ms
step:1275/2420 train_time:76436ms step_avg:59.95ms
step:1276/2420 train_time:76497ms step_avg:59.95ms
step:1277/2420 train_time:76555ms step_avg:59.95ms
step:1278/2420 train_time:76616ms step_avg:59.95ms
step:1279/2420 train_time:76675ms step_avg:59.95ms
step:1280/2420 train_time:76736ms step_avg:59.95ms
step:1281/2420 train_time:76795ms step_avg:59.95ms
step:1282/2420 train_time:76856ms step_avg:59.95ms
step:1283/2420 train_time:76915ms step_avg:59.95ms
step:1284/2420 train_time:76977ms step_avg:59.95ms
step:1285/2420 train_time:77036ms step_avg:59.95ms
step:1286/2420 train_time:77098ms step_avg:59.95ms
step:1287/2420 train_time:77157ms step_avg:59.95ms
step:1288/2420 train_time:77218ms step_avg:59.95ms
step:1289/2420 train_time:77277ms step_avg:59.95ms
step:1290/2420 train_time:77338ms step_avg:59.95ms
step:1291/2420 train_time:77397ms step_avg:59.95ms
step:1292/2420 train_time:77458ms step_avg:59.95ms
step:1293/2420 train_time:77517ms step_avg:59.95ms
step:1294/2420 train_time:77577ms step_avg:59.95ms
step:1295/2420 train_time:77636ms step_avg:59.95ms
step:1296/2420 train_time:77697ms step_avg:59.95ms
step:1297/2420 train_time:77757ms step_avg:59.95ms
step:1298/2420 train_time:77818ms step_avg:59.95ms
step:1299/2420 train_time:77877ms step_avg:59.95ms
step:1300/2420 train_time:77939ms step_avg:59.95ms
step:1301/2420 train_time:77998ms step_avg:59.95ms
step:1302/2420 train_time:78059ms step_avg:59.95ms
step:1303/2420 train_time:78119ms step_avg:59.95ms
step:1304/2420 train_time:78180ms step_avg:59.95ms
step:1305/2420 train_time:78239ms step_avg:59.95ms
step:1306/2420 train_time:78301ms step_avg:59.96ms
step:1307/2420 train_time:78360ms step_avg:59.95ms
step:1308/2420 train_time:78422ms step_avg:59.96ms
step:1309/2420 train_time:78481ms step_avg:59.95ms
step:1310/2420 train_time:78542ms step_avg:59.96ms
step:1311/2420 train_time:78601ms step_avg:59.96ms
step:1312/2420 train_time:78663ms step_avg:59.96ms
step:1313/2420 train_time:78722ms step_avg:59.96ms
step:1314/2420 train_time:78783ms step_avg:59.96ms
step:1315/2420 train_time:78843ms step_avg:59.96ms
step:1316/2420 train_time:78904ms step_avg:59.96ms
step:1317/2420 train_time:78964ms step_avg:59.96ms
step:1318/2420 train_time:79026ms step_avg:59.96ms
step:1319/2420 train_time:79085ms step_avg:59.96ms
step:1320/2420 train_time:79146ms step_avg:59.96ms
step:1321/2420 train_time:79206ms step_avg:59.96ms
step:1322/2420 train_time:79268ms step_avg:59.96ms
step:1323/2420 train_time:79328ms step_avg:59.96ms
step:1324/2420 train_time:79390ms step_avg:59.96ms
step:1325/2420 train_time:79450ms step_avg:59.96ms
step:1326/2420 train_time:79511ms step_avg:59.96ms
step:1327/2420 train_time:79571ms step_avg:59.96ms
step:1328/2420 train_time:79632ms step_avg:59.96ms
step:1329/2420 train_time:79691ms step_avg:59.96ms
step:1330/2420 train_time:79752ms step_avg:59.96ms
step:1331/2420 train_time:79811ms step_avg:59.96ms
step:1332/2420 train_time:79872ms step_avg:59.96ms
step:1333/2420 train_time:79932ms step_avg:59.96ms
step:1334/2420 train_time:79993ms step_avg:59.96ms
step:1335/2420 train_time:80052ms step_avg:59.96ms
step:1336/2420 train_time:80113ms step_avg:59.96ms
step:1337/2420 train_time:80172ms step_avg:59.96ms
step:1338/2420 train_time:80234ms step_avg:59.97ms
step:1339/2420 train_time:80293ms step_avg:59.96ms
step:1340/2420 train_time:80354ms step_avg:59.97ms
step:1341/2420 train_time:80413ms step_avg:59.96ms
step:1342/2420 train_time:80474ms step_avg:59.97ms
step:1343/2420 train_time:80534ms step_avg:59.97ms
step:1344/2420 train_time:80595ms step_avg:59.97ms
step:1345/2420 train_time:80654ms step_avg:59.97ms
step:1346/2420 train_time:80714ms step_avg:59.97ms
step:1347/2420 train_time:80773ms step_avg:59.97ms
step:1348/2420 train_time:80834ms step_avg:59.97ms
step:1349/2420 train_time:80893ms step_avg:59.97ms
step:1350/2420 train_time:80954ms step_avg:59.97ms
step:1351/2420 train_time:81013ms step_avg:59.97ms
step:1352/2420 train_time:81074ms step_avg:59.97ms
step:1353/2420 train_time:81133ms step_avg:59.97ms
step:1354/2420 train_time:81194ms step_avg:59.97ms
step:1355/2420 train_time:81253ms step_avg:59.97ms
step:1356/2420 train_time:81314ms step_avg:59.97ms
step:1357/2420 train_time:81374ms step_avg:59.97ms
step:1358/2420 train_time:81435ms step_avg:59.97ms
step:1359/2420 train_time:81493ms step_avg:59.97ms
step:1360/2420 train_time:81554ms step_avg:59.97ms
step:1361/2420 train_time:81613ms step_avg:59.97ms
step:1362/2420 train_time:81674ms step_avg:59.97ms
step:1363/2420 train_time:81733ms step_avg:59.97ms
step:1364/2420 train_time:81794ms step_avg:59.97ms
step:1365/2420 train_time:81853ms step_avg:59.97ms
step:1366/2420 train_time:81914ms step_avg:59.97ms
step:1367/2420 train_time:81973ms step_avg:59.97ms
step:1368/2420 train_time:82034ms step_avg:59.97ms
step:1369/2420 train_time:82093ms step_avg:59.97ms
step:1370/2420 train_time:82154ms step_avg:59.97ms
step:1371/2420 train_time:82213ms step_avg:59.97ms
step:1372/2420 train_time:82274ms step_avg:59.97ms
step:1373/2420 train_time:82333ms step_avg:59.97ms
step:1374/2420 train_time:82394ms step_avg:59.97ms
step:1375/2420 train_time:82453ms step_avg:59.97ms
step:1376/2420 train_time:82514ms step_avg:59.97ms
step:1377/2420 train_time:82573ms step_avg:59.97ms
step:1378/2420 train_time:82634ms step_avg:59.97ms
step:1379/2420 train_time:82693ms step_avg:59.97ms
step:1380/2420 train_time:82754ms step_avg:59.97ms
step:1381/2420 train_time:82813ms step_avg:59.97ms
step:1382/2420 train_time:82874ms step_avg:59.97ms
step:1383/2420 train_time:82933ms step_avg:59.97ms
step:1384/2420 train_time:82994ms step_avg:59.97ms
step:1385/2420 train_time:83053ms step_avg:59.97ms
step:1386/2420 train_time:83114ms step_avg:59.97ms
step:1387/2420 train_time:83173ms step_avg:59.97ms
step:1388/2420 train_time:83234ms step_avg:59.97ms
step:1389/2420 train_time:83293ms step_avg:59.97ms
step:1390/2420 train_time:83354ms step_avg:59.97ms
step:1391/2420 train_time:83413ms step_avg:59.97ms
step:1392/2420 train_time:83474ms step_avg:59.97ms
step:1393/2420 train_time:83533ms step_avg:59.97ms
step:1394/2420 train_time:83594ms step_avg:59.97ms
step:1395/2420 train_time:83654ms step_avg:59.97ms
step:1396/2420 train_time:83714ms step_avg:59.97ms
step:1397/2420 train_time:83774ms step_avg:59.97ms
step:1398/2420 train_time:83835ms step_avg:59.97ms
step:1399/2420 train_time:83894ms step_avg:59.97ms
step:1400/2420 train_time:83955ms step_avg:59.97ms
step:1401/2420 train_time:84013ms step_avg:59.97ms
step:1402/2420 train_time:84074ms step_avg:59.97ms
step:1403/2420 train_time:84133ms step_avg:59.97ms
step:1404/2420 train_time:84194ms step_avg:59.97ms
step:1405/2420 train_time:84254ms step_avg:59.97ms
step:1406/2420 train_time:84315ms step_avg:59.97ms
step:1407/2420 train_time:84374ms step_avg:59.97ms
step:1408/2420 train_time:84435ms step_avg:59.97ms
step:1409/2420 train_time:84494ms step_avg:59.97ms
step:1410/2420 train_time:84555ms step_avg:59.97ms
step:1411/2420 train_time:84614ms step_avg:59.97ms
step:1412/2420 train_time:84675ms step_avg:59.97ms
step:1413/2420 train_time:84734ms step_avg:59.97ms
step:1414/2420 train_time:84794ms step_avg:59.97ms
step:1415/2420 train_time:84854ms step_avg:59.97ms
step:1416/2420 train_time:84915ms step_avg:59.97ms
step:1417/2420 train_time:84974ms step_avg:59.97ms
step:1418/2420 train_time:85035ms step_avg:59.97ms
step:1419/2420 train_time:85093ms step_avg:59.97ms
step:1420/2420 train_time:85155ms step_avg:59.97ms
step:1421/2420 train_time:85214ms step_avg:59.97ms
step:1422/2420 train_time:85275ms step_avg:59.97ms
step:1423/2420 train_time:85333ms step_avg:59.97ms
step:1424/2420 train_time:85394ms step_avg:59.97ms
step:1425/2420 train_time:85453ms step_avg:59.97ms
step:1426/2420 train_time:85515ms step_avg:59.97ms
step:1427/2420 train_time:85574ms step_avg:59.97ms
step:1428/2420 train_time:85635ms step_avg:59.97ms
step:1429/2420 train_time:85694ms step_avg:59.97ms
step:1430/2420 train_time:85755ms step_avg:59.97ms
step:1431/2420 train_time:85815ms step_avg:59.97ms
step:1432/2420 train_time:85876ms step_avg:59.97ms
step:1433/2420 train_time:85935ms step_avg:59.97ms
step:1434/2420 train_time:85996ms step_avg:59.97ms
step:1435/2420 train_time:86056ms step_avg:59.97ms
step:1436/2420 train_time:86117ms step_avg:59.97ms
step:1437/2420 train_time:86176ms step_avg:59.97ms
step:1438/2420 train_time:86237ms step_avg:59.97ms
step:1439/2420 train_time:86296ms step_avg:59.97ms
step:1440/2420 train_time:86357ms step_avg:59.97ms
step:1441/2420 train_time:86416ms step_avg:59.97ms
step:1442/2420 train_time:86478ms step_avg:59.97ms
step:1443/2420 train_time:86537ms step_avg:59.97ms
step:1444/2420 train_time:86598ms step_avg:59.97ms
step:1445/2420 train_time:86657ms step_avg:59.97ms
step:1446/2420 train_time:86718ms step_avg:59.97ms
step:1447/2420 train_time:86777ms step_avg:59.97ms
step:1448/2420 train_time:86839ms step_avg:59.97ms
step:1449/2420 train_time:86898ms step_avg:59.97ms
step:1450/2420 train_time:86959ms step_avg:59.97ms
step:1451/2420 train_time:87018ms step_avg:59.97ms
step:1452/2420 train_time:87079ms step_avg:59.97ms
step:1453/2420 train_time:87138ms step_avg:59.97ms
step:1454/2420 train_time:87200ms step_avg:59.97ms
step:1455/2420 train_time:87259ms step_avg:59.97ms
step:1456/2420 train_time:87320ms step_avg:59.97ms
step:1457/2420 train_time:87380ms step_avg:59.97ms
step:1458/2420 train_time:87441ms step_avg:59.97ms
step:1459/2420 train_time:87501ms step_avg:59.97ms
step:1460/2420 train_time:87562ms step_avg:59.97ms
step:1461/2420 train_time:87621ms step_avg:59.97ms
step:1462/2420 train_time:87683ms step_avg:59.97ms
step:1463/2420 train_time:87743ms step_avg:59.97ms
step:1464/2420 train_time:87804ms step_avg:59.98ms
step:1465/2420 train_time:87863ms step_avg:59.98ms
step:1466/2420 train_time:87925ms step_avg:59.98ms
step:1467/2420 train_time:87984ms step_avg:59.98ms
step:1468/2420 train_time:88045ms step_avg:59.98ms
step:1469/2420 train_time:88105ms step_avg:59.98ms
step:1470/2420 train_time:88167ms step_avg:59.98ms
step:1471/2420 train_time:88227ms step_avg:59.98ms
step:1472/2420 train_time:88289ms step_avg:59.98ms
step:1473/2420 train_time:88349ms step_avg:59.98ms
step:1474/2420 train_time:88410ms step_avg:59.98ms
step:1475/2420 train_time:88470ms step_avg:59.98ms
step:1476/2420 train_time:88532ms step_avg:59.98ms
step:1477/2420 train_time:88592ms step_avg:59.98ms
step:1478/2420 train_time:88653ms step_avg:59.98ms
step:1479/2420 train_time:88712ms step_avg:59.98ms
step:1480/2420 train_time:88773ms step_avg:59.98ms
step:1481/2420 train_time:88833ms step_avg:59.98ms
step:1482/2420 train_time:88894ms step_avg:59.98ms
step:1483/2420 train_time:88953ms step_avg:59.98ms
step:1484/2420 train_time:89014ms step_avg:59.98ms
step:1485/2420 train_time:89074ms step_avg:59.98ms
step:1486/2420 train_time:89134ms step_avg:59.98ms
step:1487/2420 train_time:89194ms step_avg:59.98ms
step:1488/2420 train_time:89255ms step_avg:59.98ms
step:1489/2420 train_time:89315ms step_avg:59.98ms
step:1490/2420 train_time:89376ms step_avg:59.98ms
step:1491/2420 train_time:89435ms step_avg:59.98ms
step:1492/2420 train_time:89497ms step_avg:59.98ms
step:1493/2420 train_time:89556ms step_avg:59.98ms
step:1494/2420 train_time:89617ms step_avg:59.98ms
step:1495/2420 train_time:89676ms step_avg:59.98ms
step:1496/2420 train_time:89737ms step_avg:59.98ms
step:1497/2420 train_time:89796ms step_avg:59.98ms
step:1498/2420 train_time:89857ms step_avg:59.98ms
step:1499/2420 train_time:89916ms step_avg:59.98ms
step:1500/2420 train_time:89978ms step_avg:59.99ms
step:1500/2420 val_loss:3.4723 train_time:90041ms step_avg:60.03ms
step:1501/2420 train_time:90060ms step_avg:60.00ms
step:1502/2420 train_time:90102ms step_avg:59.99ms
step:1503/2420 train_time:90166ms step_avg:59.99ms
step:1504/2420 train_time:90230ms step_avg:59.99ms
step:1505/2420 train_time:90290ms step_avg:59.99ms
step:1506/2420 train_time:90351ms step_avg:59.99ms
step:1507/2420 train_time:90410ms step_avg:59.99ms
step:1508/2420 train_time:90470ms step_avg:59.99ms
step:1509/2420 train_time:90529ms step_avg:59.99ms
step:1510/2420 train_time:90589ms step_avg:59.99ms
step:1511/2420 train_time:90648ms step_avg:59.99ms
step:1512/2420 train_time:90709ms step_avg:59.99ms
step:1513/2420 train_time:90768ms step_avg:59.99ms
step:1514/2420 train_time:90829ms step_avg:59.99ms
step:1515/2420 train_time:90888ms step_avg:59.99ms
step:1516/2420 train_time:90950ms step_avg:59.99ms
step:1517/2420 train_time:91010ms step_avg:59.99ms
step:1518/2420 train_time:91072ms step_avg:59.99ms
step:1519/2420 train_time:91133ms step_avg:60.00ms
step:1520/2420 train_time:91195ms step_avg:60.00ms
step:1521/2420 train_time:91255ms step_avg:60.00ms
step:1522/2420 train_time:91317ms step_avg:60.00ms
step:1523/2420 train_time:91376ms step_avg:60.00ms
step:1524/2420 train_time:91437ms step_avg:60.00ms
step:1525/2420 train_time:91496ms step_avg:60.00ms
step:1526/2420 train_time:91558ms step_avg:60.00ms
step:1527/2420 train_time:91617ms step_avg:60.00ms
step:1528/2420 train_time:91679ms step_avg:60.00ms
step:1529/2420 train_time:91738ms step_avg:60.00ms
step:1530/2420 train_time:91799ms step_avg:60.00ms
step:1531/2420 train_time:91858ms step_avg:60.00ms
step:1532/2420 train_time:91919ms step_avg:60.00ms
step:1533/2420 train_time:91979ms step_avg:60.00ms
step:1534/2420 train_time:92040ms step_avg:60.00ms
step:1535/2420 train_time:92101ms step_avg:60.00ms
step:1536/2420 train_time:92163ms step_avg:60.00ms
step:1537/2420 train_time:92223ms step_avg:60.00ms
step:1538/2420 train_time:92285ms step_avg:60.00ms
step:1539/2420 train_time:92344ms step_avg:60.00ms
step:1540/2420 train_time:92406ms step_avg:60.00ms
step:1541/2420 train_time:92466ms step_avg:60.00ms
step:1542/2420 train_time:92526ms step_avg:60.00ms
step:1543/2420 train_time:92585ms step_avg:60.00ms
step:1544/2420 train_time:92647ms step_avg:60.00ms
step:1545/2420 train_time:92706ms step_avg:60.00ms
step:1546/2420 train_time:92769ms step_avg:60.01ms
step:1547/2420 train_time:92828ms step_avg:60.01ms
step:1548/2420 train_time:92889ms step_avg:60.01ms
step:1549/2420 train_time:92948ms step_avg:60.00ms
step:1550/2420 train_time:93009ms step_avg:60.01ms
step:1551/2420 train_time:93068ms step_avg:60.01ms
step:1552/2420 train_time:93129ms step_avg:60.01ms
step:1553/2420 train_time:93189ms step_avg:60.01ms
step:1554/2420 train_time:93250ms step_avg:60.01ms
step:1555/2420 train_time:93309ms step_avg:60.01ms
step:1556/2420 train_time:93370ms step_avg:60.01ms
step:1557/2420 train_time:93430ms step_avg:60.01ms
step:1558/2420 train_time:93490ms step_avg:60.01ms
step:1559/2420 train_time:93549ms step_avg:60.01ms
step:1560/2420 train_time:93610ms step_avg:60.01ms
step:1561/2420 train_time:93670ms step_avg:60.01ms
step:1562/2420 train_time:93731ms step_avg:60.01ms
step:1563/2420 train_time:93790ms step_avg:60.01ms
step:1564/2420 train_time:93851ms step_avg:60.01ms
step:1565/2420 train_time:93911ms step_avg:60.01ms
step:1566/2420 train_time:93972ms step_avg:60.01ms
step:1567/2420 train_time:94031ms step_avg:60.01ms
step:1568/2420 train_time:94092ms step_avg:60.01ms
step:1569/2420 train_time:94151ms step_avg:60.01ms
step:1570/2420 train_time:94213ms step_avg:60.01ms
step:1571/2420 train_time:94272ms step_avg:60.01ms
step:1572/2420 train_time:94333ms step_avg:60.01ms
step:1573/2420 train_time:94392ms step_avg:60.01ms
step:1574/2420 train_time:94453ms step_avg:60.01ms
step:1575/2420 train_time:94512ms step_avg:60.01ms
step:1576/2420 train_time:94574ms step_avg:60.01ms
step:1577/2420 train_time:94633ms step_avg:60.01ms
step:1578/2420 train_time:94694ms step_avg:60.01ms
step:1579/2420 train_time:94753ms step_avg:60.01ms
step:1580/2420 train_time:94815ms step_avg:60.01ms
step:1581/2420 train_time:94874ms step_avg:60.01ms
step:1582/2420 train_time:94935ms step_avg:60.01ms
step:1583/2420 train_time:94994ms step_avg:60.01ms
step:1584/2420 train_time:95055ms step_avg:60.01ms
step:1585/2420 train_time:95114ms step_avg:60.01ms
step:1586/2420 train_time:95176ms step_avg:60.01ms
step:1587/2420 train_time:95235ms step_avg:60.01ms
step:1588/2420 train_time:95296ms step_avg:60.01ms
step:1589/2420 train_time:95356ms step_avg:60.01ms
step:1590/2420 train_time:95418ms step_avg:60.01ms
step:1591/2420 train_time:95477ms step_avg:60.01ms
step:1592/2420 train_time:95539ms step_avg:60.01ms
step:1593/2420 train_time:95599ms step_avg:60.01ms
step:1594/2420 train_time:95661ms step_avg:60.01ms
step:1595/2420 train_time:95721ms step_avg:60.01ms
step:1596/2420 train_time:95783ms step_avg:60.01ms
step:1597/2420 train_time:95843ms step_avg:60.01ms
step:1598/2420 train_time:95905ms step_avg:60.02ms
step:1599/2420 train_time:95964ms step_avg:60.02ms
step:1600/2420 train_time:96027ms step_avg:60.02ms
step:1601/2420 train_time:96086ms step_avg:60.02ms
step:1602/2420 train_time:96148ms step_avg:60.02ms
step:1603/2420 train_time:96208ms step_avg:60.02ms
step:1604/2420 train_time:96270ms step_avg:60.02ms
step:1605/2420 train_time:96330ms step_avg:60.02ms
step:1606/2420 train_time:96392ms step_avg:60.02ms
step:1607/2420 train_time:96452ms step_avg:60.02ms
step:1608/2420 train_time:96513ms step_avg:60.02ms
step:1609/2420 train_time:96573ms step_avg:60.02ms
step:1610/2420 train_time:96634ms step_avg:60.02ms
step:1611/2420 train_time:96694ms step_avg:60.02ms
step:1612/2420 train_time:96755ms step_avg:60.02ms
step:1613/2420 train_time:96815ms step_avg:60.02ms
step:1614/2420 train_time:96877ms step_avg:60.02ms
step:1615/2420 train_time:96937ms step_avg:60.02ms
step:1616/2420 train_time:96999ms step_avg:60.02ms
step:1617/2420 train_time:97059ms step_avg:60.02ms
step:1618/2420 train_time:97121ms step_avg:60.03ms
step:1619/2420 train_time:97181ms step_avg:60.03ms
step:1620/2420 train_time:97244ms step_avg:60.03ms
step:1621/2420 train_time:97304ms step_avg:60.03ms
step:1622/2420 train_time:97366ms step_avg:60.03ms
step:1623/2420 train_time:97426ms step_avg:60.03ms
step:1624/2420 train_time:97489ms step_avg:60.03ms
step:1625/2420 train_time:97548ms step_avg:60.03ms
step:1626/2420 train_time:97610ms step_avg:60.03ms
step:1627/2420 train_time:97670ms step_avg:60.03ms
step:1628/2420 train_time:97731ms step_avg:60.03ms
step:1629/2420 train_time:97791ms step_avg:60.03ms
step:1630/2420 train_time:97853ms step_avg:60.03ms
step:1631/2420 train_time:97913ms step_avg:60.03ms
step:1632/2420 train_time:97974ms step_avg:60.03ms
step:1633/2420 train_time:98034ms step_avg:60.03ms
step:1634/2420 train_time:98096ms step_avg:60.03ms
step:1635/2420 train_time:98156ms step_avg:60.03ms
step:1636/2420 train_time:98218ms step_avg:60.04ms
step:1637/2420 train_time:98278ms step_avg:60.04ms
step:1638/2420 train_time:98340ms step_avg:60.04ms
step:1639/2420 train_time:98401ms step_avg:60.04ms
step:1640/2420 train_time:98463ms step_avg:60.04ms
step:1641/2420 train_time:98523ms step_avg:60.04ms
step:1642/2420 train_time:98586ms step_avg:60.04ms
step:1643/2420 train_time:98646ms step_avg:60.04ms
step:1644/2420 train_time:98708ms step_avg:60.04ms
step:1645/2420 train_time:98768ms step_avg:60.04ms
step:1646/2420 train_time:98830ms step_avg:60.04ms
step:1647/2420 train_time:98890ms step_avg:60.04ms
step:1648/2420 train_time:98951ms step_avg:60.04ms
step:1649/2420 train_time:99011ms step_avg:60.04ms
step:1650/2420 train_time:99072ms step_avg:60.04ms
step:1651/2420 train_time:99132ms step_avg:60.04ms
step:1652/2420 train_time:99194ms step_avg:60.04ms
step:1653/2420 train_time:99253ms step_avg:60.04ms
step:1654/2420 train_time:99315ms step_avg:60.05ms
step:1655/2420 train_time:99375ms step_avg:60.05ms
step:1656/2420 train_time:99437ms step_avg:60.05ms
step:1657/2420 train_time:99496ms step_avg:60.05ms
step:1658/2420 train_time:99558ms step_avg:60.05ms
step:1659/2420 train_time:99618ms step_avg:60.05ms
step:1660/2420 train_time:99680ms step_avg:60.05ms
step:1661/2420 train_time:99740ms step_avg:60.05ms
step:1662/2420 train_time:99801ms step_avg:60.05ms
step:1663/2420 train_time:99862ms step_avg:60.05ms
step:1664/2420 train_time:99924ms step_avg:60.05ms
step:1665/2420 train_time:99984ms step_avg:60.05ms
step:1666/2420 train_time:100046ms step_avg:60.05ms
step:1667/2420 train_time:100106ms step_avg:60.05ms
step:1668/2420 train_time:100169ms step_avg:60.05ms
step:1669/2420 train_time:100229ms step_avg:60.05ms
step:1670/2420 train_time:100292ms step_avg:60.05ms
step:1671/2420 train_time:100351ms step_avg:60.05ms
step:1672/2420 train_time:100412ms step_avg:60.05ms
step:1673/2420 train_time:100471ms step_avg:60.05ms
step:1674/2420 train_time:100533ms step_avg:60.06ms
step:1675/2420 train_time:100592ms step_avg:60.06ms
step:1676/2420 train_time:100654ms step_avg:60.06ms
step:1677/2420 train_time:100713ms step_avg:60.06ms
step:1678/2420 train_time:100775ms step_avg:60.06ms
step:1679/2420 train_time:100835ms step_avg:60.06ms
step:1680/2420 train_time:100897ms step_avg:60.06ms
step:1681/2420 train_time:100957ms step_avg:60.06ms
step:1682/2420 train_time:101019ms step_avg:60.06ms
step:1683/2420 train_time:101079ms step_avg:60.06ms
step:1684/2420 train_time:101141ms step_avg:60.06ms
step:1685/2420 train_time:101200ms step_avg:60.06ms
step:1686/2420 train_time:101262ms step_avg:60.06ms
step:1687/2420 train_time:101322ms step_avg:60.06ms
step:1688/2420 train_time:101385ms step_avg:60.06ms
step:1689/2420 train_time:101445ms step_avg:60.06ms
step:1690/2420 train_time:101507ms step_avg:60.06ms
step:1691/2420 train_time:101567ms step_avg:60.06ms
step:1692/2420 train_time:101629ms step_avg:60.06ms
step:1693/2420 train_time:101689ms step_avg:60.06ms
step:1694/2420 train_time:101751ms step_avg:60.07ms
step:1695/2420 train_time:101811ms step_avg:60.07ms
step:1696/2420 train_time:101873ms step_avg:60.07ms
step:1697/2420 train_time:101932ms step_avg:60.07ms
step:1698/2420 train_time:101994ms step_avg:60.07ms
step:1699/2420 train_time:102053ms step_avg:60.07ms
step:1700/2420 train_time:102115ms step_avg:60.07ms
step:1701/2420 train_time:102175ms step_avg:60.07ms
step:1702/2420 train_time:102236ms step_avg:60.07ms
step:1703/2420 train_time:102296ms step_avg:60.07ms
step:1704/2420 train_time:102358ms step_avg:60.07ms
step:1705/2420 train_time:102418ms step_avg:60.07ms
step:1706/2420 train_time:102480ms step_avg:60.07ms
step:1707/2420 train_time:102541ms step_avg:60.07ms
step:1708/2420 train_time:102603ms step_avg:60.07ms
step:1709/2420 train_time:102663ms step_avg:60.07ms
step:1710/2420 train_time:102725ms step_avg:60.07ms
step:1711/2420 train_time:102786ms step_avg:60.07ms
step:1712/2420 train_time:102848ms step_avg:60.07ms
step:1713/2420 train_time:102908ms step_avg:60.07ms
step:1714/2420 train_time:102970ms step_avg:60.08ms
step:1715/2420 train_time:103029ms step_avg:60.08ms
step:1716/2420 train_time:103091ms step_avg:60.08ms
step:1717/2420 train_time:103150ms step_avg:60.08ms
step:1718/2420 train_time:103211ms step_avg:60.08ms
step:1719/2420 train_time:103271ms step_avg:60.08ms
step:1720/2420 train_time:103333ms step_avg:60.08ms
step:1721/2420 train_time:103393ms step_avg:60.08ms
step:1722/2420 train_time:103455ms step_avg:60.08ms
step:1723/2420 train_time:103514ms step_avg:60.08ms
step:1724/2420 train_time:103576ms step_avg:60.08ms
step:1725/2420 train_time:103636ms step_avg:60.08ms
step:1726/2420 train_time:103697ms step_avg:60.08ms
step:1727/2420 train_time:103758ms step_avg:60.08ms
step:1728/2420 train_time:103819ms step_avg:60.08ms
step:1729/2420 train_time:103879ms step_avg:60.08ms
step:1730/2420 train_time:103941ms step_avg:60.08ms
step:1731/2420 train_time:104001ms step_avg:60.08ms
step:1732/2420 train_time:104063ms step_avg:60.08ms
step:1733/2420 train_time:104123ms step_avg:60.08ms
step:1734/2420 train_time:104185ms step_avg:60.08ms
step:1735/2420 train_time:104245ms step_avg:60.08ms
step:1736/2420 train_time:104307ms step_avg:60.08ms
step:1737/2420 train_time:104367ms step_avg:60.08ms
step:1738/2420 train_time:104429ms step_avg:60.09ms
step:1739/2420 train_time:104488ms step_avg:60.09ms
step:1740/2420 train_time:104550ms step_avg:60.09ms
step:1741/2420 train_time:104610ms step_avg:60.09ms
step:1742/2420 train_time:104672ms step_avg:60.09ms
step:1743/2420 train_time:104731ms step_avg:60.09ms
step:1744/2420 train_time:104793ms step_avg:60.09ms
step:1745/2420 train_time:104853ms step_avg:60.09ms
step:1746/2420 train_time:104915ms step_avg:60.09ms
step:1747/2420 train_time:104974ms step_avg:60.09ms
step:1748/2420 train_time:105036ms step_avg:60.09ms
step:1749/2420 train_time:105096ms step_avg:60.09ms
step:1750/2420 train_time:105158ms step_avg:60.09ms
step:1750/2420 val_loss:3.4000 train_time:105223ms step_avg:60.13ms
step:1751/2420 train_time:105242ms step_avg:60.10ms
step:1752/2420 train_time:105283ms step_avg:60.09ms
step:1753/2420 train_time:105343ms step_avg:60.09ms
step:1754/2420 train_time:105405ms step_avg:60.09ms
step:1755/2420 train_time:105466ms step_avg:60.09ms
step:1756/2420 train_time:105528ms step_avg:60.10ms
step:1757/2420 train_time:105587ms step_avg:60.10ms
step:1758/2420 train_time:105648ms step_avg:60.10ms
step:1759/2420 train_time:105707ms step_avg:60.10ms
step:1760/2420 train_time:105768ms step_avg:60.10ms
step:1761/2420 train_time:105827ms step_avg:60.09ms
step:1762/2420 train_time:105888ms step_avg:60.10ms
step:1763/2420 train_time:105947ms step_avg:60.09ms
step:1764/2420 train_time:106009ms step_avg:60.10ms
step:1765/2420 train_time:106068ms step_avg:60.10ms
step:1766/2420 train_time:106132ms step_avg:60.10ms
step:1767/2420 train_time:106195ms step_avg:60.10ms
step:1768/2420 train_time:106258ms step_avg:60.10ms
step:1769/2420 train_time:106318ms step_avg:60.10ms
step:1770/2420 train_time:106381ms step_avg:60.10ms
step:1771/2420 train_time:106442ms step_avg:60.10ms
step:1772/2420 train_time:106504ms step_avg:60.10ms
step:1773/2420 train_time:106564ms step_avg:60.10ms
step:1774/2420 train_time:106625ms step_avg:60.10ms
step:1775/2420 train_time:106684ms step_avg:60.10ms
step:1776/2420 train_time:106746ms step_avg:60.10ms
step:1777/2420 train_time:106804ms step_avg:60.10ms
step:1778/2420 train_time:106865ms step_avg:60.10ms
step:1779/2420 train_time:106925ms step_avg:60.10ms
step:1780/2420 train_time:106985ms step_avg:60.10ms
step:1781/2420 train_time:107045ms step_avg:60.10ms
step:1782/2420 train_time:107107ms step_avg:60.10ms
step:1783/2420 train_time:107168ms step_avg:60.11ms
step:1784/2420 train_time:107231ms step_avg:60.11ms
step:1785/2420 train_time:107292ms step_avg:60.11ms
step:1786/2420 train_time:107354ms step_avg:60.11ms
step:1787/2420 train_time:107414ms step_avg:60.11ms
step:1788/2420 train_time:107476ms step_avg:60.11ms
step:1789/2420 train_time:107536ms step_avg:60.11ms
step:1790/2420 train_time:107598ms step_avg:60.11ms
step:1791/2420 train_time:107659ms step_avg:60.11ms
step:1792/2420 train_time:107720ms step_avg:60.11ms
step:1793/2420 train_time:107780ms step_avg:60.11ms
step:1794/2420 train_time:107842ms step_avg:60.11ms
step:1795/2420 train_time:107901ms step_avg:60.11ms
step:1796/2420 train_time:107963ms step_avg:60.11ms
step:1797/2420 train_time:108022ms step_avg:60.11ms
step:1798/2420 train_time:108084ms step_avg:60.11ms
step:1799/2420 train_time:108144ms step_avg:60.11ms
step:1800/2420 train_time:108206ms step_avg:60.11ms
step:1801/2420 train_time:108266ms step_avg:60.11ms
step:1802/2420 train_time:108327ms step_avg:60.12ms
step:1803/2420 train_time:108387ms step_avg:60.11ms
step:1804/2420 train_time:108449ms step_avg:60.12ms
step:1805/2420 train_time:108509ms step_avg:60.12ms
step:1806/2420 train_time:108571ms step_avg:60.12ms
step:1807/2420 train_time:108631ms step_avg:60.12ms
step:1808/2420 train_time:108693ms step_avg:60.12ms
step:1809/2420 train_time:108753ms step_avg:60.12ms
step:1810/2420 train_time:108815ms step_avg:60.12ms
step:1811/2420 train_time:108875ms step_avg:60.12ms
step:1812/2420 train_time:108937ms step_avg:60.12ms
step:1813/2420 train_time:108997ms step_avg:60.12ms
step:1814/2420 train_time:109059ms step_avg:60.12ms
step:1815/2420 train_time:109119ms step_avg:60.12ms
step:1816/2420 train_time:109182ms step_avg:60.12ms
step:1817/2420 train_time:109242ms step_avg:60.12ms
step:1818/2420 train_time:109304ms step_avg:60.12ms
step:1819/2420 train_time:109363ms step_avg:60.12ms
step:1820/2420 train_time:109425ms step_avg:60.12ms
step:1821/2420 train_time:109484ms step_avg:60.12ms
step:1822/2420 train_time:109545ms step_avg:60.12ms
step:1823/2420 train_time:109605ms step_avg:60.12ms
step:1824/2420 train_time:109666ms step_avg:60.12ms
step:1825/2420 train_time:109726ms step_avg:60.12ms
step:1826/2420 train_time:109788ms step_avg:60.12ms
step:1827/2420 train_time:109848ms step_avg:60.12ms
step:1828/2420 train_time:109911ms step_avg:60.13ms
step:1829/2420 train_time:109971ms step_avg:60.13ms
step:1830/2420 train_time:110032ms step_avg:60.13ms
step:1831/2420 train_time:110092ms step_avg:60.13ms
step:1832/2420 train_time:110154ms step_avg:60.13ms
step:1833/2420 train_time:110214ms step_avg:60.13ms
step:1834/2420 train_time:110276ms step_avg:60.13ms
step:1835/2420 train_time:110336ms step_avg:60.13ms
step:1836/2420 train_time:110398ms step_avg:60.13ms
step:1837/2420 train_time:110458ms step_avg:60.13ms
step:1838/2420 train_time:110520ms step_avg:60.13ms
step:1839/2420 train_time:110580ms step_avg:60.13ms
step:1840/2420 train_time:110642ms step_avg:60.13ms
step:1841/2420 train_time:110702ms step_avg:60.13ms
step:1842/2420 train_time:110764ms step_avg:60.13ms
step:1843/2420 train_time:110823ms step_avg:60.13ms
step:1844/2420 train_time:110884ms step_avg:60.13ms
step:1845/2420 train_time:110943ms step_avg:60.13ms
step:1846/2420 train_time:111005ms step_avg:60.13ms
step:1847/2420 train_time:111064ms step_avg:60.13ms
step:1848/2420 train_time:111127ms step_avg:60.13ms
step:1849/2420 train_time:111186ms step_avg:60.13ms
step:1850/2420 train_time:111249ms step_avg:60.13ms
step:1851/2420 train_time:111308ms step_avg:60.13ms
step:1852/2420 train_time:111370ms step_avg:60.14ms
step:1853/2420 train_time:111430ms step_avg:60.14ms
step:1854/2420 train_time:111492ms step_avg:60.14ms
step:1855/2420 train_time:111552ms step_avg:60.14ms
step:1856/2420 train_time:111614ms step_avg:60.14ms
step:1857/2420 train_time:111674ms step_avg:60.14ms
step:1858/2420 train_time:111736ms step_avg:60.14ms
step:1859/2420 train_time:111797ms step_avg:60.14ms
step:1860/2420 train_time:111858ms step_avg:60.14ms
step:1861/2420 train_time:111918ms step_avg:60.14ms
step:1862/2420 train_time:111980ms step_avg:60.14ms
step:1863/2420 train_time:112040ms step_avg:60.14ms
step:1864/2420 train_time:112102ms step_avg:60.14ms
step:1865/2420 train_time:112162ms step_avg:60.14ms
step:1866/2420 train_time:112224ms step_avg:60.14ms
step:1867/2420 train_time:112283ms step_avg:60.14ms
step:1868/2420 train_time:112345ms step_avg:60.14ms
step:1869/2420 train_time:112405ms step_avg:60.14ms
step:1870/2420 train_time:112466ms step_avg:60.14ms
step:1871/2420 train_time:112526ms step_avg:60.14ms
step:1872/2420 train_time:112587ms step_avg:60.14ms
step:1873/2420 train_time:112647ms step_avg:60.14ms
step:1874/2420 train_time:112709ms step_avg:60.14ms
step:1875/2420 train_time:112769ms step_avg:60.14ms
step:1876/2420 train_time:112831ms step_avg:60.14ms
step:1877/2420 train_time:112890ms step_avg:60.14ms
step:1878/2420 train_time:112952ms step_avg:60.14ms
step:1879/2420 train_time:113012ms step_avg:60.14ms
step:1880/2420 train_time:113074ms step_avg:60.15ms
step:1881/2420 train_time:113134ms step_avg:60.15ms
step:1882/2420 train_time:113196ms step_avg:60.15ms
step:1883/2420 train_time:113256ms step_avg:60.15ms
step:1884/2420 train_time:113318ms step_avg:60.15ms
step:1885/2420 train_time:113378ms step_avg:60.15ms
step:1886/2420 train_time:113440ms step_avg:60.15ms
step:1887/2420 train_time:113500ms step_avg:60.15ms
step:1888/2420 train_time:113562ms step_avg:60.15ms
step:1889/2420 train_time:113622ms step_avg:60.15ms
step:1890/2420 train_time:113684ms step_avg:60.15ms
step:1891/2420 train_time:113744ms step_avg:60.15ms
step:1892/2420 train_time:113805ms step_avg:60.15ms
step:1893/2420 train_time:113865ms step_avg:60.15ms
step:1894/2420 train_time:113926ms step_avg:60.15ms
step:1895/2420 train_time:113985ms step_avg:60.15ms
step:1896/2420 train_time:114048ms step_avg:60.15ms
step:1897/2420 train_time:114108ms step_avg:60.15ms
step:1898/2420 train_time:114170ms step_avg:60.15ms
step:1899/2420 train_time:114230ms step_avg:60.15ms
step:1900/2420 train_time:114292ms step_avg:60.15ms
step:1901/2420 train_time:114352ms step_avg:60.15ms
step:1902/2420 train_time:114414ms step_avg:60.15ms
step:1903/2420 train_time:114474ms step_avg:60.15ms
step:1904/2420 train_time:114536ms step_avg:60.16ms
step:1905/2420 train_time:114595ms step_avg:60.16ms
step:1906/2420 train_time:114657ms step_avg:60.16ms
step:1907/2420 train_time:114717ms step_avg:60.16ms
step:1908/2420 train_time:114779ms step_avg:60.16ms
step:1909/2420 train_time:114839ms step_avg:60.16ms
step:1910/2420 train_time:114900ms step_avg:60.16ms
step:1911/2420 train_time:114960ms step_avg:60.16ms
step:1912/2420 train_time:115021ms step_avg:60.16ms
step:1913/2420 train_time:115082ms step_avg:60.16ms
step:1914/2420 train_time:115144ms step_avg:60.16ms
step:1915/2420 train_time:115204ms step_avg:60.16ms
step:1916/2420 train_time:115265ms step_avg:60.16ms
step:1917/2420 train_time:115325ms step_avg:60.16ms
step:1918/2420 train_time:115386ms step_avg:60.16ms
step:1919/2420 train_time:115446ms step_avg:60.16ms
step:1920/2420 train_time:115509ms step_avg:60.16ms
step:1921/2420 train_time:115569ms step_avg:60.16ms
step:1922/2420 train_time:115631ms step_avg:60.16ms
step:1923/2420 train_time:115690ms step_avg:60.16ms
step:1924/2420 train_time:115752ms step_avg:60.16ms
step:1925/2420 train_time:115812ms step_avg:60.16ms
step:1926/2420 train_time:115874ms step_avg:60.16ms
step:1927/2420 train_time:115933ms step_avg:60.16ms
step:1928/2420 train_time:115995ms step_avg:60.16ms
step:1929/2420 train_time:116055ms step_avg:60.16ms
step:1930/2420 train_time:116116ms step_avg:60.16ms
step:1931/2420 train_time:116176ms step_avg:60.16ms
step:1932/2420 train_time:116239ms step_avg:60.16ms
step:1933/2420 train_time:116299ms step_avg:60.16ms
step:1934/2420 train_time:116360ms step_avg:60.17ms
step:1935/2420 train_time:116419ms step_avg:60.17ms
step:1936/2420 train_time:116481ms step_avg:60.17ms
step:1937/2420 train_time:116542ms step_avg:60.17ms
step:1938/2420 train_time:116604ms step_avg:60.17ms
step:1939/2420 train_time:116664ms step_avg:60.17ms
step:1940/2420 train_time:116725ms step_avg:60.17ms
step:1941/2420 train_time:116785ms step_avg:60.17ms
step:1942/2420 train_time:116846ms step_avg:60.17ms
step:1943/2420 train_time:116905ms step_avg:60.17ms
step:1944/2420 train_time:116967ms step_avg:60.17ms
step:1945/2420 train_time:117026ms step_avg:60.17ms
step:1946/2420 train_time:117088ms step_avg:60.17ms
step:1947/2420 train_time:117148ms step_avg:60.17ms
step:1948/2420 train_time:117210ms step_avg:60.17ms
step:1949/2420 train_time:117270ms step_avg:60.17ms
step:1950/2420 train_time:117333ms step_avg:60.17ms
step:1951/2420 train_time:117392ms step_avg:60.17ms
step:1952/2420 train_time:117454ms step_avg:60.17ms
step:1953/2420 train_time:117514ms step_avg:60.17ms
step:1954/2420 train_time:117577ms step_avg:60.17ms
step:1955/2420 train_time:117637ms step_avg:60.17ms
step:1956/2420 train_time:117699ms step_avg:60.17ms
step:1957/2420 train_time:117759ms step_avg:60.17ms
step:1958/2420 train_time:117820ms step_avg:60.17ms
step:1959/2420 train_time:117880ms step_avg:60.17ms
step:1960/2420 train_time:117942ms step_avg:60.17ms
step:1961/2420 train_time:118002ms step_avg:60.17ms
step:1962/2420 train_time:118064ms step_avg:60.18ms
step:1963/2420 train_time:118124ms step_avg:60.18ms
step:1964/2420 train_time:118185ms step_avg:60.18ms
step:1965/2420 train_time:118244ms step_avg:60.17ms
step:1966/2420 train_time:118305ms step_avg:60.18ms
step:1967/2420 train_time:118365ms step_avg:60.18ms
step:1968/2420 train_time:118426ms step_avg:60.18ms
step:1969/2420 train_time:118486ms step_avg:60.18ms
step:1970/2420 train_time:118548ms step_avg:60.18ms
step:1971/2420 train_time:118607ms step_avg:60.18ms
step:1972/2420 train_time:118670ms step_avg:60.18ms
step:1973/2420 train_time:118729ms step_avg:60.18ms
step:1974/2420 train_time:118791ms step_avg:60.18ms
step:1975/2420 train_time:118851ms step_avg:60.18ms
step:1976/2420 train_time:118913ms step_avg:60.18ms
step:1977/2420 train_time:118973ms step_avg:60.18ms
step:1978/2420 train_time:119035ms step_avg:60.18ms
step:1979/2420 train_time:119095ms step_avg:60.18ms
step:1980/2420 train_time:119157ms step_avg:60.18ms
step:1981/2420 train_time:119217ms step_avg:60.18ms
step:1982/2420 train_time:119279ms step_avg:60.18ms
step:1983/2420 train_time:119339ms step_avg:60.18ms
step:1984/2420 train_time:119401ms step_avg:60.18ms
step:1985/2420 train_time:119461ms step_avg:60.18ms
step:1986/2420 train_time:119522ms step_avg:60.18ms
step:1987/2420 train_time:119582ms step_avg:60.18ms
step:1988/2420 train_time:119643ms step_avg:60.18ms
step:1989/2420 train_time:119703ms step_avg:60.18ms
step:1990/2420 train_time:119764ms step_avg:60.18ms
step:1991/2420 train_time:119824ms step_avg:60.18ms
step:1992/2420 train_time:119886ms step_avg:60.18ms
step:1993/2420 train_time:119946ms step_avg:60.18ms
step:1994/2420 train_time:120008ms step_avg:60.18ms
step:1995/2420 train_time:120068ms step_avg:60.18ms
step:1996/2420 train_time:120130ms step_avg:60.19ms
step:1997/2420 train_time:120190ms step_avg:60.19ms
step:1998/2420 train_time:120251ms step_avg:60.19ms
step:1999/2420 train_time:120311ms step_avg:60.19ms
step:2000/2420 train_time:120373ms step_avg:60.19ms
step:2000/2420 val_loss:3.3453 train_time:120437ms step_avg:60.22ms
step:2001/2420 train_time:120457ms step_avg:60.20ms
step:2002/2420 train_time:120499ms step_avg:60.19ms
step:2003/2420 train_time:120563ms step_avg:60.19ms
step:2004/2420 train_time:120629ms step_avg:60.19ms
step:2005/2420 train_time:120690ms step_avg:60.19ms
step:2006/2420 train_time:120753ms step_avg:60.20ms
step:2007/2420 train_time:120812ms step_avg:60.20ms
step:2008/2420 train_time:120873ms step_avg:60.20ms
step:2009/2420 train_time:120932ms step_avg:60.20ms
step:2010/2420 train_time:120993ms step_avg:60.20ms
step:2011/2420 train_time:121052ms step_avg:60.19ms
step:2012/2420 train_time:121113ms step_avg:60.20ms
step:2013/2420 train_time:121172ms step_avg:60.19ms
step:2014/2420 train_time:121232ms step_avg:60.19ms
step:2015/2420 train_time:121291ms step_avg:60.19ms
step:2016/2420 train_time:121352ms step_avg:60.19ms
step:2017/2420 train_time:121413ms step_avg:60.20ms
step:2018/2420 train_time:121476ms step_avg:60.20ms
step:2019/2420 train_time:121539ms step_avg:60.20ms
step:2020/2420 train_time:121602ms step_avg:60.20ms
step:2021/2420 train_time:121663ms step_avg:60.20ms
step:2022/2420 train_time:121726ms step_avg:60.20ms
step:2023/2420 train_time:121787ms step_avg:60.20ms
step:2024/2420 train_time:121848ms step_avg:60.20ms
step:2025/2420 train_time:121907ms step_avg:60.20ms
step:2026/2420 train_time:121969ms step_avg:60.20ms
step:2027/2420 train_time:122028ms step_avg:60.20ms
step:2028/2420 train_time:122089ms step_avg:60.20ms
step:2029/2420 train_time:122148ms step_avg:60.20ms
step:2030/2420 train_time:122209ms step_avg:60.20ms
step:2031/2420 train_time:122269ms step_avg:60.20ms
step:2032/2420 train_time:122330ms step_avg:60.20ms
step:2033/2420 train_time:122390ms step_avg:60.20ms
step:2034/2420 train_time:122452ms step_avg:60.20ms
step:2035/2420 train_time:122513ms step_avg:60.20ms
step:2036/2420 train_time:122575ms step_avg:60.20ms
step:2037/2420 train_time:122636ms step_avg:60.20ms
step:2038/2420 train_time:122698ms step_avg:60.21ms
step:2039/2420 train_time:122759ms step_avg:60.21ms
step:2040/2420 train_time:122820ms step_avg:60.21ms
step:2041/2420 train_time:122880ms step_avg:60.21ms
step:2042/2420 train_time:122942ms step_avg:60.21ms
step:2043/2420 train_time:123002ms step_avg:60.21ms
step:2044/2420 train_time:123064ms step_avg:60.21ms
step:2045/2420 train_time:123123ms step_avg:60.21ms
step:2046/2420 train_time:123184ms step_avg:60.21ms
step:2047/2420 train_time:123244ms step_avg:60.21ms
step:2048/2420 train_time:123306ms step_avg:60.21ms
step:2049/2420 train_time:123367ms step_avg:60.21ms
step:2050/2420 train_time:123429ms step_avg:60.21ms
step:2051/2420 train_time:123490ms step_avg:60.21ms
step:2052/2420 train_time:123552ms step_avg:60.21ms
step:2053/2420 train_time:123612ms step_avg:60.21ms
step:2054/2420 train_time:123675ms step_avg:60.21ms
step:2055/2420 train_time:123735ms step_avg:60.21ms
step:2056/2420 train_time:123797ms step_avg:60.21ms
step:2057/2420 train_time:123856ms step_avg:60.21ms
step:2058/2420 train_time:123918ms step_avg:60.21ms
step:2059/2420 train_time:123978ms step_avg:60.21ms
step:2060/2420 train_time:124039ms step_avg:60.21ms
step:2061/2420 train_time:124099ms step_avg:60.21ms
step:2062/2420 train_time:124161ms step_avg:60.21ms
step:2063/2420 train_time:124221ms step_avg:60.21ms
step:2064/2420 train_time:124283ms step_avg:60.21ms
step:2065/2420 train_time:124343ms step_avg:60.21ms
step:2066/2420 train_time:124406ms step_avg:60.22ms
step:2067/2420 train_time:124466ms step_avg:60.22ms
step:2068/2420 train_time:124528ms step_avg:60.22ms
step:2069/2420 train_time:124588ms step_avg:60.22ms
step:2070/2420 train_time:124650ms step_avg:60.22ms
step:2071/2420 train_time:124711ms step_avg:60.22ms
step:2072/2420 train_time:124773ms step_avg:60.22ms
step:2073/2420 train_time:124833ms step_avg:60.22ms
step:2074/2420 train_time:124894ms step_avg:60.22ms
step:2075/2420 train_time:124954ms step_avg:60.22ms
step:2076/2420 train_time:125015ms step_avg:60.22ms
step:2077/2420 train_time:125075ms step_avg:60.22ms
step:2078/2420 train_time:125137ms step_avg:60.22ms
step:2079/2420 train_time:125196ms step_avg:60.22ms
step:2080/2420 train_time:125258ms step_avg:60.22ms
step:2081/2420 train_time:125318ms step_avg:60.22ms
step:2082/2420 train_time:125381ms step_avg:60.22ms
step:2083/2420 train_time:125441ms step_avg:60.22ms
step:2084/2420 train_time:125504ms step_avg:60.22ms
step:2085/2420 train_time:125564ms step_avg:60.22ms
step:2086/2420 train_time:125626ms step_avg:60.22ms
step:2087/2420 train_time:125687ms step_avg:60.22ms
step:2088/2420 train_time:125749ms step_avg:60.22ms
step:2089/2420 train_time:125809ms step_avg:60.22ms
step:2090/2420 train_time:125871ms step_avg:60.23ms
step:2091/2420 train_time:125931ms step_avg:60.23ms
step:2092/2420 train_time:125993ms step_avg:60.23ms
step:2093/2420 train_time:126052ms step_avg:60.23ms
step:2094/2420 train_time:126114ms step_avg:60.23ms
step:2095/2420 train_time:126174ms step_avg:60.23ms
step:2096/2420 train_time:126235ms step_avg:60.23ms
step:2097/2420 train_time:126294ms step_avg:60.23ms
step:2098/2420 train_time:126356ms step_avg:60.23ms
step:2099/2420 train_time:126416ms step_avg:60.23ms
step:2100/2420 train_time:126478ms step_avg:60.23ms
step:2101/2420 train_time:126539ms step_avg:60.23ms
step:2102/2420 train_time:126601ms step_avg:60.23ms
step:2103/2420 train_time:126662ms step_avg:60.23ms
step:2104/2420 train_time:126725ms step_avg:60.23ms
step:2105/2420 train_time:126785ms step_avg:60.23ms
step:2106/2420 train_time:126847ms step_avg:60.23ms
step:2107/2420 train_time:126907ms step_avg:60.23ms
step:2108/2420 train_time:126971ms step_avg:60.23ms
step:2109/2420 train_time:127031ms step_avg:60.23ms
step:2110/2420 train_time:127093ms step_avg:60.23ms
step:2111/2420 train_time:127153ms step_avg:60.23ms
step:2112/2420 train_time:127214ms step_avg:60.23ms
step:2113/2420 train_time:127274ms step_avg:60.23ms
step:2114/2420 train_time:127335ms step_avg:60.23ms
step:2115/2420 train_time:127394ms step_avg:60.23ms
step:2116/2420 train_time:127455ms step_avg:60.23ms
step:2117/2420 train_time:127515ms step_avg:60.23ms
step:2118/2420 train_time:127578ms step_avg:60.23ms
step:2119/2420 train_time:127639ms step_avg:60.24ms
step:2120/2420 train_time:127701ms step_avg:60.24ms
step:2121/2420 train_time:127762ms step_avg:60.24ms
step:2122/2420 train_time:127824ms step_avg:60.24ms
step:2123/2420 train_time:127884ms step_avg:60.24ms
step:2124/2420 train_time:127946ms step_avg:60.24ms
step:2125/2420 train_time:128006ms step_avg:60.24ms
step:2126/2420 train_time:128069ms step_avg:60.24ms
step:2127/2420 train_time:128129ms step_avg:60.24ms
step:2128/2420 train_time:128191ms step_avg:60.24ms
step:2129/2420 train_time:128251ms step_avg:60.24ms
step:2130/2420 train_time:128313ms step_avg:60.24ms
step:2131/2420 train_time:128373ms step_avg:60.24ms
step:2132/2420 train_time:128434ms step_avg:60.24ms
step:2133/2420 train_time:128494ms step_avg:60.24ms
step:2134/2420 train_time:128555ms step_avg:60.24ms
step:2135/2420 train_time:128614ms step_avg:60.24ms
step:2136/2420 train_time:128676ms step_avg:60.24ms
step:2137/2420 train_time:128737ms step_avg:60.24ms
step:2138/2420 train_time:128799ms step_avg:60.24ms
step:2139/2420 train_time:128859ms step_avg:60.24ms
step:2140/2420 train_time:128922ms step_avg:60.24ms
step:2141/2420 train_time:128982ms step_avg:60.24ms
step:2142/2420 train_time:129044ms step_avg:60.24ms
step:2143/2420 train_time:129106ms step_avg:60.25ms
step:2144/2420 train_time:129167ms step_avg:60.25ms
step:2145/2420 train_time:129228ms step_avg:60.25ms
step:2146/2420 train_time:129289ms step_avg:60.25ms
step:2147/2420 train_time:129349ms step_avg:60.25ms
step:2148/2420 train_time:129412ms step_avg:60.25ms
step:2149/2420 train_time:129473ms step_avg:60.25ms
step:2150/2420 train_time:129534ms step_avg:60.25ms
step:2151/2420 train_time:129593ms step_avg:60.25ms
step:2152/2420 train_time:129655ms step_avg:60.25ms
step:2153/2420 train_time:129715ms step_avg:60.25ms
step:2154/2420 train_time:129777ms step_avg:60.25ms
step:2155/2420 train_time:129836ms step_avg:60.25ms
step:2156/2420 train_time:129899ms step_avg:60.25ms
step:2157/2420 train_time:129959ms step_avg:60.25ms
step:2158/2420 train_time:130021ms step_avg:60.25ms
step:2159/2420 train_time:130082ms step_avg:60.25ms
step:2160/2420 train_time:130144ms step_avg:60.25ms
step:2161/2420 train_time:130204ms step_avg:60.25ms
step:2162/2420 train_time:130266ms step_avg:60.25ms
step:2163/2420 train_time:130326ms step_avg:60.25ms
step:2164/2420 train_time:130388ms step_avg:60.25ms
step:2165/2420 train_time:130449ms step_avg:60.25ms
step:2166/2420 train_time:130511ms step_avg:60.25ms
step:2167/2420 train_time:130571ms step_avg:60.25ms
step:2168/2420 train_time:130633ms step_avg:60.25ms
step:2169/2420 train_time:130692ms step_avg:60.25ms
step:2170/2420 train_time:130754ms step_avg:60.26ms
step:2171/2420 train_time:130813ms step_avg:60.25ms
step:2172/2420 train_time:130875ms step_avg:60.26ms
step:2173/2420 train_time:130934ms step_avg:60.26ms
step:2174/2420 train_time:130996ms step_avg:60.26ms
step:2175/2420 train_time:131057ms step_avg:60.26ms
step:2176/2420 train_time:131118ms step_avg:60.26ms
step:2177/2420 train_time:131179ms step_avg:60.26ms
step:2178/2420 train_time:131241ms step_avg:60.26ms
step:2179/2420 train_time:131301ms step_avg:60.26ms
step:2180/2420 train_time:131363ms step_avg:60.26ms
step:2181/2420 train_time:131423ms step_avg:60.26ms
step:2182/2420 train_time:131485ms step_avg:60.26ms
step:2183/2420 train_time:131545ms step_avg:60.26ms
step:2184/2420 train_time:131608ms step_avg:60.26ms
step:2185/2420 train_time:131668ms step_avg:60.26ms
step:2186/2420 train_time:131731ms step_avg:60.26ms
step:2187/2420 train_time:131791ms step_avg:60.26ms
step:2188/2420 train_time:131853ms step_avg:60.26ms
step:2189/2420 train_time:131913ms step_avg:60.26ms
step:2190/2420 train_time:131975ms step_avg:60.26ms
step:2191/2420 train_time:132035ms step_avg:60.26ms
step:2192/2420 train_time:132096ms step_avg:60.26ms
step:2193/2420 train_time:132156ms step_avg:60.26ms
step:2194/2420 train_time:132218ms step_avg:60.26ms
step:2195/2420 train_time:132277ms step_avg:60.26ms
step:2196/2420 train_time:132339ms step_avg:60.26ms
step:2197/2420 train_time:132399ms step_avg:60.26ms
step:2198/2420 train_time:132462ms step_avg:60.26ms
step:2199/2420 train_time:132522ms step_avg:60.26ms
step:2200/2420 train_time:132584ms step_avg:60.27ms
step:2201/2420 train_time:132644ms step_avg:60.27ms
step:2202/2420 train_time:132706ms step_avg:60.27ms
step:2203/2420 train_time:132766ms step_avg:60.27ms
step:2204/2420 train_time:132828ms step_avg:60.27ms
step:2205/2420 train_time:132889ms step_avg:60.27ms
step:2206/2420 train_time:132951ms step_avg:60.27ms
step:2207/2420 train_time:133011ms step_avg:60.27ms
step:2208/2420 train_time:133073ms step_avg:60.27ms
step:2209/2420 train_time:133132ms step_avg:60.27ms
step:2210/2420 train_time:133194ms step_avg:60.27ms
step:2211/2420 train_time:133254ms step_avg:60.27ms
step:2212/2420 train_time:133315ms step_avg:60.27ms
step:2213/2420 train_time:133375ms step_avg:60.27ms
step:2214/2420 train_time:133436ms step_avg:60.27ms
step:2215/2420 train_time:133496ms step_avg:60.27ms
step:2216/2420 train_time:133559ms step_avg:60.27ms
step:2217/2420 train_time:133618ms step_avg:60.27ms
step:2218/2420 train_time:133681ms step_avg:60.27ms
step:2219/2420 train_time:133742ms step_avg:60.27ms
step:2220/2420 train_time:133804ms step_avg:60.27ms
step:2221/2420 train_time:133864ms step_avg:60.27ms
step:2222/2420 train_time:133926ms step_avg:60.27ms
step:2223/2420 train_time:133986ms step_avg:60.27ms
step:2224/2420 train_time:134048ms step_avg:60.27ms
step:2225/2420 train_time:134108ms step_avg:60.27ms
step:2226/2420 train_time:134171ms step_avg:60.27ms
step:2227/2420 train_time:134232ms step_avg:60.27ms
step:2228/2420 train_time:134293ms step_avg:60.28ms
step:2229/2420 train_time:134353ms step_avg:60.27ms
step:2230/2420 train_time:134414ms step_avg:60.28ms
step:2231/2420 train_time:134473ms step_avg:60.27ms
step:2232/2420 train_time:134535ms step_avg:60.28ms
step:2233/2420 train_time:134595ms step_avg:60.28ms
step:2234/2420 train_time:134656ms step_avg:60.28ms
step:2235/2420 train_time:134717ms step_avg:60.28ms
step:2236/2420 train_time:134779ms step_avg:60.28ms
step:2237/2420 train_time:134840ms step_avg:60.28ms
step:2238/2420 train_time:134902ms step_avg:60.28ms
step:2239/2420 train_time:134962ms step_avg:60.28ms
step:2240/2420 train_time:135023ms step_avg:60.28ms
step:2241/2420 train_time:135083ms step_avg:60.28ms
step:2242/2420 train_time:135145ms step_avg:60.28ms
step:2243/2420 train_time:135206ms step_avg:60.28ms
step:2244/2420 train_time:135269ms step_avg:60.28ms
step:2245/2420 train_time:135330ms step_avg:60.28ms
step:2246/2420 train_time:135392ms step_avg:60.28ms
step:2247/2420 train_time:135452ms step_avg:60.28ms
step:2248/2420 train_time:135514ms step_avg:60.28ms
step:2249/2420 train_time:135573ms step_avg:60.28ms
step:2250/2420 train_time:135635ms step_avg:60.28ms
step:2250/2420 val_loss:3.3005 train_time:135699ms step_avg:60.31ms
step:2251/2420 train_time:135718ms step_avg:60.29ms
step:2252/2420 train_time:135760ms step_avg:60.28ms
step:2253/2420 train_time:135823ms step_avg:60.29ms
step:2254/2420 train_time:135886ms step_avg:60.29ms
step:2255/2420 train_time:135946ms step_avg:60.29ms
step:2256/2420 train_time:136009ms step_avg:60.29ms
step:2257/2420 train_time:136068ms step_avg:60.29ms
step:2258/2420 train_time:136129ms step_avg:60.29ms
step:2259/2420 train_time:136188ms step_avg:60.29ms
step:2260/2420 train_time:136249ms step_avg:60.29ms
step:2261/2420 train_time:136308ms step_avg:60.29ms
step:2262/2420 train_time:136369ms step_avg:60.29ms
step:2263/2420 train_time:136429ms step_avg:60.29ms
step:2264/2420 train_time:136490ms step_avg:60.29ms
step:2265/2420 train_time:136549ms step_avg:60.29ms
step:2266/2420 train_time:136610ms step_avg:60.29ms
step:2267/2420 train_time:136671ms step_avg:60.29ms
step:2268/2420 train_time:136734ms step_avg:60.29ms
step:2269/2420 train_time:136796ms step_avg:60.29ms
step:2270/2420 train_time:136859ms step_avg:60.29ms
step:2271/2420 train_time:136921ms step_avg:60.29ms
step:2272/2420 train_time:136983ms step_avg:60.29ms
step:2273/2420 train_time:137042ms step_avg:60.29ms
step:2274/2420 train_time:137103ms step_avg:60.29ms
step:2275/2420 train_time:137162ms step_avg:60.29ms
step:2276/2420 train_time:137223ms step_avg:60.29ms
step:2277/2420 train_time:137282ms step_avg:60.29ms
step:2278/2420 train_time:137344ms step_avg:60.29ms
step:2279/2420 train_time:137403ms step_avg:60.29ms
step:2280/2420 train_time:137464ms step_avg:60.29ms
step:2281/2420 train_time:137523ms step_avg:60.29ms
step:2282/2420 train_time:137585ms step_avg:60.29ms
step:2283/2420 train_time:137645ms step_avg:60.29ms
step:2284/2420 train_time:137708ms step_avg:60.29ms
step:2285/2420 train_time:137769ms step_avg:60.29ms
step:2286/2420 train_time:137831ms step_avg:60.29ms
step:2287/2420 train_time:137892ms step_avg:60.29ms
step:2288/2420 train_time:137955ms step_avg:60.30ms
step:2289/2420 train_time:138016ms step_avg:60.30ms
step:2290/2420 train_time:138077ms step_avg:60.30ms
step:2291/2420 train_time:138137ms step_avg:60.30ms
step:2292/2420 train_time:138199ms step_avg:60.30ms
step:2293/2420 train_time:138259ms step_avg:60.30ms
step:2294/2420 train_time:138321ms step_avg:60.30ms
step:2295/2420 train_time:138380ms step_avg:60.30ms
step:2296/2420 train_time:138442ms step_avg:60.30ms
step:2297/2420 train_time:138501ms step_avg:60.30ms
step:2298/2420 train_time:138562ms step_avg:60.30ms
step:2299/2420 train_time:138621ms step_avg:60.30ms
step:2300/2420 train_time:138683ms step_avg:60.30ms
step:2301/2420 train_time:138742ms step_avg:60.30ms
step:2302/2420 train_time:138805ms step_avg:60.30ms
step:2303/2420 train_time:138865ms step_avg:60.30ms
step:2304/2420 train_time:138928ms step_avg:60.30ms
step:2305/2420 train_time:138988ms step_avg:60.30ms
step:2306/2420 train_time:139050ms step_avg:60.30ms
step:2307/2420 train_time:139110ms step_avg:60.30ms
step:2308/2420 train_time:139171ms step_avg:60.30ms
step:2309/2420 train_time:139231ms step_avg:60.30ms
step:2310/2420 train_time:139293ms step_avg:60.30ms
step:2311/2420 train_time:139353ms step_avg:60.30ms
step:2312/2420 train_time:139414ms step_avg:60.30ms
step:2313/2420 train_time:139474ms step_avg:60.30ms
step:2314/2420 train_time:139535ms step_avg:60.30ms
step:2315/2420 train_time:139596ms step_avg:60.30ms
step:2316/2420 train_time:139657ms step_avg:60.30ms
step:2317/2420 train_time:139718ms step_avg:60.30ms
step:2318/2420 train_time:139780ms step_avg:60.30ms
step:2319/2420 train_time:139840ms step_avg:60.30ms
step:2320/2420 train_time:139903ms step_avg:60.30ms
step:2321/2420 train_time:139962ms step_avg:60.30ms
step:2322/2420 train_time:140024ms step_avg:60.30ms
step:2323/2420 train_time:140084ms step_avg:60.30ms
step:2324/2420 train_time:140145ms step_avg:60.30ms
step:2325/2420 train_time:140205ms step_avg:60.30ms
step:2326/2420 train_time:140267ms step_avg:60.30ms
step:2327/2420 train_time:140326ms step_avg:60.30ms
step:2328/2420 train_time:140388ms step_avg:60.30ms
step:2329/2420 train_time:140448ms step_avg:60.30ms
step:2330/2420 train_time:140509ms step_avg:60.30ms
step:2331/2420 train_time:140570ms step_avg:60.30ms
step:2332/2420 train_time:140632ms step_avg:60.31ms
step:2333/2420 train_time:140692ms step_avg:60.31ms
step:2334/2420 train_time:140754ms step_avg:60.31ms
step:2335/2420 train_time:140815ms step_avg:60.31ms
step:2336/2420 train_time:140878ms step_avg:60.31ms
step:2337/2420 train_time:140938ms step_avg:60.31ms
step:2338/2420 train_time:141002ms step_avg:60.31ms
step:2339/2420 train_time:141062ms step_avg:60.31ms
step:2340/2420 train_time:141123ms step_avg:60.31ms
step:2341/2420 train_time:141182ms step_avg:60.31ms
step:2342/2420 train_time:141243ms step_avg:60.31ms
step:2343/2420 train_time:141303ms step_avg:60.31ms
step:2344/2420 train_time:141364ms step_avg:60.31ms
step:2345/2420 train_time:141423ms step_avg:60.31ms
step:2346/2420 train_time:141485ms step_avg:60.31ms
step:2347/2420 train_time:141544ms step_avg:60.31ms
step:2348/2420 train_time:141607ms step_avg:60.31ms
step:2349/2420 train_time:141667ms step_avg:60.31ms
step:2350/2420 train_time:141729ms step_avg:60.31ms
step:2351/2420 train_time:141789ms step_avg:60.31ms
step:2352/2420 train_time:141851ms step_avg:60.31ms
step:2353/2420 train_time:141911ms step_avg:60.31ms
step:2354/2420 train_time:141974ms step_avg:60.31ms
step:2355/2420 train_time:142035ms step_avg:60.31ms
step:2356/2420 train_time:142097ms step_avg:60.31ms
step:2357/2420 train_time:142157ms step_avg:60.31ms
step:2358/2420 train_time:142219ms step_avg:60.31ms
step:2359/2420 train_time:142278ms step_avg:60.31ms
step:2360/2420 train_time:142340ms step_avg:60.31ms
step:2361/2420 train_time:142400ms step_avg:60.31ms
step:2362/2420 train_time:142461ms step_avg:60.31ms
step:2363/2420 train_time:142521ms step_avg:60.31ms
step:2364/2420 train_time:142582ms step_avg:60.31ms
step:2365/2420 train_time:142642ms step_avg:60.31ms
step:2366/2420 train_time:142703ms step_avg:60.31ms
step:2367/2420 train_time:142763ms step_avg:60.31ms
step:2368/2420 train_time:142825ms step_avg:60.31ms
step:2369/2420 train_time:142884ms step_avg:60.31ms
step:2370/2420 train_time:142947ms step_avg:60.32ms
step:2371/2420 train_time:143007ms step_avg:60.31ms
step:2372/2420 train_time:143069ms step_avg:60.32ms
step:2373/2420 train_time:143129ms step_avg:60.32ms
step:2374/2420 train_time:143191ms step_avg:60.32ms
step:2375/2420 train_time:143251ms step_avg:60.32ms
step:2376/2420 train_time:143313ms step_avg:60.32ms
step:2377/2420 train_time:143372ms step_avg:60.32ms
step:2378/2420 train_time:143435ms step_avg:60.32ms
step:2379/2420 train_time:143496ms step_avg:60.32ms
step:2380/2420 train_time:143557ms step_avg:60.32ms
step:2381/2420 train_time:143618ms step_avg:60.32ms
step:2382/2420 train_time:143679ms step_avg:60.32ms
step:2383/2420 train_time:143739ms step_avg:60.32ms
step:2384/2420 train_time:144132ms step_avg:60.46ms
step:2385/2420 train_time:144189ms step_avg:60.46ms
step:2386/2420 train_time:144249ms step_avg:60.46ms
step:2387/2420 train_time:144308ms step_avg:60.46ms
step:2388/2420 train_time:144369ms step_avg:60.46ms
step:2389/2420 train_time:144650ms step_avg:60.55ms
step:2390/2420 train_time:144710ms step_avg:60.55ms
step:2391/2420 train_time:144768ms step_avg:60.55ms
step:2392/2420 train_time:145134ms step_avg:60.67ms
step:2393/2420 train_time:145192ms step_avg:60.67ms
step:2394/2420 train_time:145252ms step_avg:60.67ms
step:2395/2420 train_time:145311ms step_avg:60.67ms
step:2396/2420 train_time:145371ms step_avg:60.67ms
step:2397/2420 train_time:145430ms step_avg:60.67ms
step:2398/2420 train_time:145490ms step_avg:60.67ms
step:2399/2420 train_time:145549ms step_avg:60.67ms
step:2400/2420 train_time:145610ms step_avg:60.67ms
step:2401/2420 train_time:145669ms step_avg:60.67ms
step:2402/2420 train_time:145729ms step_avg:60.67ms
step:2403/2420 train_time:145788ms step_avg:60.67ms
step:2404/2420 train_time:145849ms step_avg:60.67ms
step:2405/2420 train_time:145908ms step_avg:60.67ms
step:2406/2420 train_time:145970ms step_avg:60.67ms
step:2407/2420 train_time:146036ms step_avg:60.67ms
step:2408/2420 train_time:146103ms step_avg:60.67ms
step:2409/2420 train_time:146164ms step_avg:60.67ms
step:2410/2420 train_time:146228ms step_avg:60.68ms
step:2411/2420 train_time:146287ms step_avg:60.68ms
step:2412/2420 train_time:146349ms step_avg:60.68ms
step:2413/2420 train_time:146408ms step_avg:60.67ms
step:2414/2420 train_time:146470ms step_avg:60.68ms
step:2415/2420 train_time:146529ms step_avg:60.67ms
step:2416/2420 train_time:146590ms step_avg:60.67ms
step:2417/2420 train_time:146648ms step_avg:60.67ms
step:2418/2420 train_time:146709ms step_avg:60.67ms
step:2419/2420 train_time:146769ms step_avg:60.67ms
step:2420/2420 train_time:146830ms step_avg:60.67ms
step:2420/2420 val_loss:3.2747 train_time:146893ms step_avg:60.70ms
peak memory allocated: 29512 MiB reserved: 44036 MiB

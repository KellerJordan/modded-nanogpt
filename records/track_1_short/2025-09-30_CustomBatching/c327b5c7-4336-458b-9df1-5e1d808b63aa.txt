import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        #ve = [None, ve[0], ve[1]] + [None] * (len(self.blocks) - 5) + [ve[0], ve[1]]
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2380  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.4  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further
    momentum_cd_steps = 50


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.7, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps  # More explicit denominator

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # step the optimizers
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 21:18:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                  Off |
| N/A   35C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                  Off |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                  Off |
| N/A   41C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                  Off |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                  Off |
| N/A   34C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                  Off |
| N/A   41C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                  Off |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                  Off |
| N/A   36C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          242154      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          242155      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          242156      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          242157      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          242158      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          242159      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          242160      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          242161      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          242155      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          242156      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          242157      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          242158      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          242159      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          242160      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          242161      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2420 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2420 train_time:95ms step_avg:95.15ms
step:2/2420 train_time:190ms step_avg:95.22ms
step:3/2420 train_time:213ms step_avg:70.88ms
step:4/2420 train_time:247ms step_avg:61.70ms
step:5/2420 train_time:304ms step_avg:60.75ms
step:6/2420 train_time:365ms step_avg:60.81ms
step:7/2420 train_time:423ms step_avg:60.45ms
step:8/2420 train_time:483ms step_avg:60.38ms
step:9/2420 train_time:542ms step_avg:60.19ms
step:10/2420 train_time:602ms step_avg:60.21ms
step:11/2420 train_time:660ms step_avg:60.04ms
step:12/2420 train_time:721ms step_avg:60.04ms
step:13/2420 train_time:779ms step_avg:59.89ms
step:14/2420 train_time:839ms step_avg:59.93ms
step:15/2420 train_time:897ms step_avg:59.81ms
step:16/2420 train_time:958ms step_avg:59.86ms
step:17/2420 train_time:1017ms step_avg:59.82ms
step:18/2420 train_time:1080ms step_avg:59.98ms
step:19/2420 train_time:1142ms step_avg:60.12ms
step:20/2420 train_time:1206ms step_avg:60.31ms
step:21/2420 train_time:1266ms step_avg:60.28ms
step:22/2420 train_time:1327ms step_avg:60.30ms
step:23/2420 train_time:1385ms step_avg:60.21ms
step:24/2420 train_time:1446ms step_avg:60.23ms
step:25/2420 train_time:1505ms step_avg:60.19ms
step:26/2420 train_time:1566ms step_avg:60.21ms
step:27/2420 train_time:1624ms step_avg:60.14ms
step:28/2420 train_time:1684ms step_avg:60.14ms
step:29/2420 train_time:1743ms step_avg:60.10ms
step:30/2420 train_time:1803ms step_avg:60.11ms
step:31/2420 train_time:1862ms step_avg:60.08ms
step:32/2420 train_time:1924ms step_avg:60.11ms
step:33/2420 train_time:1983ms step_avg:60.08ms
step:34/2420 train_time:2045ms step_avg:60.14ms
step:35/2420 train_time:2105ms step_avg:60.15ms
step:36/2420 train_time:2168ms step_avg:60.21ms
step:37/2420 train_time:2228ms step_avg:60.21ms
step:38/2420 train_time:2288ms step_avg:60.22ms
step:39/2420 train_time:2347ms step_avg:60.19ms
step:40/2420 train_time:2408ms step_avg:60.19ms
step:41/2420 train_time:2467ms step_avg:60.17ms
step:42/2420 train_time:2527ms step_avg:60.17ms
step:43/2420 train_time:2585ms step_avg:60.12ms
step:44/2420 train_time:2646ms step_avg:60.14ms
step:45/2420 train_time:2705ms step_avg:60.11ms
step:46/2420 train_time:2766ms step_avg:60.12ms
step:47/2420 train_time:2824ms step_avg:60.10ms
step:48/2420 train_time:2885ms step_avg:60.10ms
step:49/2420 train_time:2943ms step_avg:60.07ms
step:50/2420 train_time:3004ms step_avg:60.09ms
step:51/2420 train_time:3064ms step_avg:60.08ms
step:52/2420 train_time:3126ms step_avg:60.11ms
step:53/2420 train_time:3185ms step_avg:60.10ms
step:54/2420 train_time:3247ms step_avg:60.13ms
step:55/2420 train_time:3307ms step_avg:60.12ms
step:56/2420 train_time:3368ms step_avg:60.15ms
step:57/2420 train_time:3427ms step_avg:60.13ms
step:58/2420 train_time:3488ms step_avg:60.15ms
step:59/2420 train_time:3547ms step_avg:60.11ms
step:60/2420 train_time:3607ms step_avg:60.12ms
step:61/2420 train_time:3666ms step_avg:60.09ms
step:62/2420 train_time:3726ms step_avg:60.09ms
step:63/2420 train_time:3784ms step_avg:60.07ms
step:64/2420 train_time:3845ms step_avg:60.08ms
step:65/2420 train_time:3904ms step_avg:60.06ms
step:66/2420 train_time:3965ms step_avg:60.07ms
step:67/2420 train_time:4024ms step_avg:60.06ms
step:68/2420 train_time:4085ms step_avg:60.07ms
step:69/2420 train_time:4144ms step_avg:60.05ms
step:70/2420 train_time:4206ms step_avg:60.09ms
step:71/2420 train_time:4266ms step_avg:60.08ms
step:72/2420 train_time:4327ms step_avg:60.10ms
step:73/2420 train_time:4386ms step_avg:60.08ms
step:74/2420 train_time:4446ms step_avg:60.08ms
step:75/2420 train_time:4505ms step_avg:60.07ms
step:76/2420 train_time:4566ms step_avg:60.08ms
step:77/2420 train_time:4625ms step_avg:60.06ms
step:78/2420 train_time:4685ms step_avg:60.07ms
step:79/2420 train_time:4744ms step_avg:60.05ms
step:80/2420 train_time:4804ms step_avg:60.05ms
step:81/2420 train_time:4863ms step_avg:60.04ms
step:82/2420 train_time:4924ms step_avg:60.05ms
step:83/2420 train_time:4983ms step_avg:60.04ms
step:84/2420 train_time:5044ms step_avg:60.05ms
step:85/2420 train_time:5103ms step_avg:60.04ms
step:86/2420 train_time:5166ms step_avg:60.07ms
step:87/2420 train_time:5226ms step_avg:60.07ms
step:88/2420 train_time:5287ms step_avg:60.08ms
step:89/2420 train_time:5346ms step_avg:60.07ms
step:90/2420 train_time:5407ms step_avg:60.08ms
step:91/2420 train_time:5467ms step_avg:60.07ms
step:92/2420 train_time:5527ms step_avg:60.07ms
step:93/2420 train_time:5585ms step_avg:60.05ms
step:94/2420 train_time:5645ms step_avg:60.06ms
step:95/2420 train_time:5704ms step_avg:60.04ms
step:96/2420 train_time:5765ms step_avg:60.05ms
step:97/2420 train_time:5823ms step_avg:60.03ms
step:98/2420 train_time:5884ms step_avg:60.04ms
step:99/2420 train_time:5942ms step_avg:60.02ms
step:100/2420 train_time:6003ms step_avg:60.03ms
step:101/2420 train_time:6063ms step_avg:60.03ms
step:102/2420 train_time:6124ms step_avg:60.04ms
step:103/2420 train_time:6183ms step_avg:60.03ms
step:104/2420 train_time:6244ms step_avg:60.04ms
step:105/2420 train_time:6303ms step_avg:60.03ms
step:106/2420 train_time:6365ms step_avg:60.05ms
step:107/2420 train_time:6424ms step_avg:60.04ms
step:108/2420 train_time:6485ms step_avg:60.05ms
step:109/2420 train_time:6543ms step_avg:60.03ms
step:110/2420 train_time:6604ms step_avg:60.04ms
step:111/2420 train_time:6663ms step_avg:60.03ms
step:112/2420 train_time:6724ms step_avg:60.03ms
step:113/2420 train_time:6782ms step_avg:60.02ms
step:114/2420 train_time:6843ms step_avg:60.02ms
step:115/2420 train_time:6901ms step_avg:60.01ms
step:116/2420 train_time:6962ms step_avg:60.02ms
step:117/2420 train_time:7021ms step_avg:60.00ms
step:118/2420 train_time:7081ms step_avg:60.01ms
step:119/2420 train_time:7140ms step_avg:60.00ms
step:120/2420 train_time:7202ms step_avg:60.02ms
step:121/2420 train_time:7262ms step_avg:60.01ms
step:122/2420 train_time:7323ms step_avg:60.02ms
step:123/2420 train_time:7383ms step_avg:60.02ms
step:124/2420 train_time:7444ms step_avg:60.03ms
step:125/2420 train_time:7502ms step_avg:60.02ms
step:126/2420 train_time:7563ms step_avg:60.02ms
step:127/2420 train_time:7622ms step_avg:60.01ms
step:128/2420 train_time:7682ms step_avg:60.02ms
step:129/2420 train_time:7741ms step_avg:60.01ms
step:130/2420 train_time:7802ms step_avg:60.01ms
step:131/2420 train_time:7861ms step_avg:60.01ms
step:132/2420 train_time:7922ms step_avg:60.01ms
step:133/2420 train_time:7980ms step_avg:60.00ms
step:134/2420 train_time:8040ms step_avg:60.00ms
step:135/2420 train_time:8100ms step_avg:60.00ms
step:136/2420 train_time:8161ms step_avg:60.01ms
step:137/2420 train_time:8220ms step_avg:60.00ms
step:138/2420 train_time:8281ms step_avg:60.01ms
step:139/2420 train_time:8340ms step_avg:60.00ms
step:140/2420 train_time:8402ms step_avg:60.02ms
step:141/2420 train_time:8462ms step_avg:60.01ms
step:142/2420 train_time:8524ms step_avg:60.02ms
step:143/2420 train_time:8583ms step_avg:60.02ms
step:144/2420 train_time:8644ms step_avg:60.03ms
step:145/2420 train_time:8702ms step_avg:60.02ms
step:146/2420 train_time:8763ms step_avg:60.02ms
step:147/2420 train_time:8822ms step_avg:60.01ms
step:148/2420 train_time:8882ms step_avg:60.01ms
step:149/2420 train_time:8942ms step_avg:60.01ms
step:150/2420 train_time:9003ms step_avg:60.02ms
step:151/2420 train_time:9063ms step_avg:60.02ms
step:152/2420 train_time:9123ms step_avg:60.02ms
step:153/2420 train_time:9182ms step_avg:60.01ms
step:154/2420 train_time:9243ms step_avg:60.02ms
step:155/2420 train_time:9302ms step_avg:60.01ms
step:156/2420 train_time:9364ms step_avg:60.02ms
step:157/2420 train_time:9423ms step_avg:60.02ms
step:158/2420 train_time:9484ms step_avg:60.02ms
step:159/2420 train_time:9542ms step_avg:60.01ms
step:160/2420 train_time:9603ms step_avg:60.02ms
step:161/2420 train_time:9662ms step_avg:60.02ms
step:162/2420 train_time:9723ms step_avg:60.02ms
step:163/2420 train_time:9782ms step_avg:60.01ms
step:164/2420 train_time:9842ms step_avg:60.01ms
step:165/2420 train_time:9900ms step_avg:60.00ms
step:166/2420 train_time:9961ms step_avg:60.00ms
step:167/2420 train_time:10020ms step_avg:60.00ms
step:168/2420 train_time:10080ms step_avg:60.00ms
step:169/2420 train_time:10139ms step_avg:60.00ms
step:170/2420 train_time:10200ms step_avg:60.00ms
step:171/2420 train_time:10259ms step_avg:59.99ms
step:172/2420 train_time:10320ms step_avg:60.00ms
step:173/2420 train_time:10379ms step_avg:59.99ms
step:174/2420 train_time:10440ms step_avg:60.00ms
step:175/2420 train_time:10499ms step_avg:59.99ms
step:176/2420 train_time:10559ms step_avg:60.00ms
step:177/2420 train_time:10619ms step_avg:59.99ms
step:178/2420 train_time:10680ms step_avg:60.00ms
step:179/2420 train_time:10739ms step_avg:59.99ms
step:180/2420 train_time:10800ms step_avg:60.00ms
step:181/2420 train_time:10858ms step_avg:59.99ms
step:182/2420 train_time:10918ms step_avg:59.99ms
step:183/2420 train_time:10977ms step_avg:59.98ms
step:184/2420 train_time:11037ms step_avg:59.99ms
step:185/2420 train_time:11096ms step_avg:59.98ms
step:186/2420 train_time:11157ms step_avg:59.98ms
step:187/2420 train_time:11216ms step_avg:59.98ms
step:188/2420 train_time:11276ms step_avg:59.98ms
step:189/2420 train_time:11336ms step_avg:59.98ms
step:190/2420 train_time:11397ms step_avg:59.98ms
step:191/2420 train_time:11455ms step_avg:59.98ms
step:192/2420 train_time:11517ms step_avg:59.98ms
step:193/2420 train_time:11576ms step_avg:59.98ms
step:194/2420 train_time:11637ms step_avg:59.98ms
step:195/2420 train_time:11696ms step_avg:59.98ms
step:196/2420 train_time:11756ms step_avg:59.98ms
step:197/2420 train_time:11815ms step_avg:59.97ms
step:198/2420 train_time:11875ms step_avg:59.97ms
step:199/2420 train_time:11934ms step_avg:59.97ms
step:200/2420 train_time:11994ms step_avg:59.97ms
step:201/2420 train_time:12053ms step_avg:59.96ms
step:202/2420 train_time:12113ms step_avg:59.97ms
step:203/2420 train_time:12172ms step_avg:59.96ms
step:204/2420 train_time:12232ms step_avg:59.96ms
step:205/2420 train_time:12291ms step_avg:59.96ms
step:206/2420 train_time:12352ms step_avg:59.96ms
step:207/2420 train_time:12410ms step_avg:59.95ms
step:208/2420 train_time:12471ms step_avg:59.95ms
step:209/2420 train_time:12529ms step_avg:59.95ms
step:210/2420 train_time:12590ms step_avg:59.95ms
step:211/2420 train_time:12648ms step_avg:59.94ms
step:212/2420 train_time:12708ms step_avg:59.94ms
step:213/2420 train_time:12766ms step_avg:59.94ms
step:214/2420 train_time:12826ms step_avg:59.94ms
step:215/2420 train_time:12884ms step_avg:59.93ms
step:216/2420 train_time:12945ms step_avg:59.93ms
step:217/2420 train_time:13003ms step_avg:59.92ms
step:218/2420 train_time:13065ms step_avg:59.93ms
step:219/2420 train_time:13124ms step_avg:59.93ms
step:220/2420 train_time:13185ms step_avg:59.93ms
step:221/2420 train_time:13244ms step_avg:59.93ms
step:222/2420 train_time:13305ms step_avg:59.93ms
step:223/2420 train_time:13364ms step_avg:59.93ms
step:224/2420 train_time:13425ms step_avg:59.93ms
step:225/2420 train_time:13483ms step_avg:59.92ms
step:226/2420 train_time:13543ms step_avg:59.93ms
step:227/2420 train_time:13602ms step_avg:59.92ms
step:228/2420 train_time:13662ms step_avg:59.92ms
step:229/2420 train_time:13721ms step_avg:59.92ms
step:230/2420 train_time:13781ms step_avg:59.92ms
step:231/2420 train_time:13839ms step_avg:59.91ms
step:232/2420 train_time:13899ms step_avg:59.91ms
step:233/2420 train_time:13958ms step_avg:59.90ms
step:234/2420 train_time:14019ms step_avg:59.91ms
step:235/2420 train_time:14078ms step_avg:59.91ms
step:236/2420 train_time:14138ms step_avg:59.91ms
step:237/2420 train_time:14198ms step_avg:59.91ms
step:238/2420 train_time:14259ms step_avg:59.91ms
step:239/2420 train_time:14318ms step_avg:59.91ms
step:240/2420 train_time:14379ms step_avg:59.91ms
step:241/2420 train_time:14438ms step_avg:59.91ms
step:242/2420 train_time:14498ms step_avg:59.91ms
step:243/2420 train_time:14558ms step_avg:59.91ms
step:244/2420 train_time:14618ms step_avg:59.91ms
step:245/2420 train_time:14677ms step_avg:59.91ms
step:246/2420 train_time:14737ms step_avg:59.91ms
step:247/2420 train_time:14796ms step_avg:59.90ms
step:248/2420 train_time:14856ms step_avg:59.90ms
step:249/2420 train_time:14915ms step_avg:59.90ms
step:250/2420 train_time:14975ms step_avg:59.90ms
step:250/2420 val_loss:4.0947 train_time:15038ms step_avg:60.15ms
step:251/2420 train_time:15062ms step_avg:60.01ms
step:252/2420 train_time:15095ms step_avg:59.90ms
step:253/2420 train_time:15157ms step_avg:59.91ms
step:254/2420 train_time:15222ms step_avg:59.93ms
step:255/2420 train_time:15283ms step_avg:59.93ms
step:256/2420 train_time:15344ms step_avg:59.94ms
step:257/2420 train_time:15403ms step_avg:59.93ms
step:258/2420 train_time:15463ms step_avg:59.93ms
step:259/2420 train_time:15521ms step_avg:59.93ms
step:260/2420 train_time:15581ms step_avg:59.93ms
step:261/2420 train_time:15638ms step_avg:59.92ms
step:262/2420 train_time:15698ms step_avg:59.91ms
step:263/2420 train_time:15755ms step_avg:59.91ms
step:264/2420 train_time:15815ms step_avg:59.90ms
step:265/2420 train_time:15872ms step_avg:59.89ms
step:266/2420 train_time:15932ms step_avg:59.90ms
step:267/2420 train_time:15991ms step_avg:59.89ms
step:268/2420 train_time:16051ms step_avg:59.89ms
step:269/2420 train_time:16111ms step_avg:59.89ms
step:270/2420 train_time:16173ms step_avg:59.90ms
step:271/2420 train_time:16233ms step_avg:59.90ms
step:272/2420 train_time:16294ms step_avg:59.91ms
step:273/2420 train_time:16353ms step_avg:59.90ms
step:274/2420 train_time:16414ms step_avg:59.91ms
step:275/2420 train_time:16473ms step_avg:59.90ms
step:276/2420 train_time:16533ms step_avg:59.90ms
step:277/2420 train_time:16592ms step_avg:59.90ms
step:278/2420 train_time:16652ms step_avg:59.90ms
step:279/2420 train_time:16709ms step_avg:59.89ms
step:280/2420 train_time:16769ms step_avg:59.89ms
step:281/2420 train_time:16827ms step_avg:59.88ms
step:282/2420 train_time:16887ms step_avg:59.88ms
step:283/2420 train_time:16945ms step_avg:59.88ms
step:284/2420 train_time:17005ms step_avg:59.88ms
step:285/2420 train_time:17064ms step_avg:59.87ms
step:286/2420 train_time:17125ms step_avg:59.88ms
step:287/2420 train_time:17184ms step_avg:59.87ms
step:288/2420 train_time:17244ms step_avg:59.88ms
step:289/2420 train_time:17303ms step_avg:59.87ms
step:290/2420 train_time:17365ms step_avg:59.88ms
step:291/2420 train_time:17425ms step_avg:59.88ms
step:292/2420 train_time:17485ms step_avg:59.88ms
step:293/2420 train_time:17544ms step_avg:59.88ms
step:294/2420 train_time:17605ms step_avg:59.88ms
step:295/2420 train_time:17663ms step_avg:59.87ms
step:296/2420 train_time:17723ms step_avg:59.87ms
step:297/2420 train_time:17781ms step_avg:59.87ms
step:298/2420 train_time:17841ms step_avg:59.87ms
step:299/2420 train_time:17899ms step_avg:59.86ms
step:300/2420 train_time:17959ms step_avg:59.86ms
step:301/2420 train_time:18017ms step_avg:59.86ms
step:302/2420 train_time:18078ms step_avg:59.86ms
step:303/2420 train_time:18136ms step_avg:59.85ms
step:304/2420 train_time:18196ms step_avg:59.86ms
step:305/2420 train_time:18255ms step_avg:59.85ms
step:306/2420 train_time:18315ms step_avg:59.85ms
step:307/2420 train_time:18374ms step_avg:59.85ms
step:308/2420 train_time:18434ms step_avg:59.85ms
step:309/2420 train_time:18493ms step_avg:59.85ms
step:310/2420 train_time:18553ms step_avg:59.85ms
step:311/2420 train_time:18612ms step_avg:59.84ms
step:312/2420 train_time:18672ms step_avg:59.85ms
step:313/2420 train_time:18730ms step_avg:59.84ms
step:314/2420 train_time:18790ms step_avg:59.84ms
step:315/2420 train_time:18849ms step_avg:59.84ms
step:316/2420 train_time:18908ms step_avg:59.84ms
step:317/2420 train_time:18968ms step_avg:59.84ms
step:318/2420 train_time:19029ms step_avg:59.84ms
step:319/2420 train_time:19087ms step_avg:59.84ms
step:320/2420 train_time:19148ms step_avg:59.84ms
step:321/2420 train_time:19207ms step_avg:59.83ms
step:322/2420 train_time:19267ms step_avg:59.84ms
step:323/2420 train_time:19327ms step_avg:59.84ms
step:324/2420 train_time:19388ms step_avg:59.84ms
step:325/2420 train_time:19446ms step_avg:59.83ms
step:326/2420 train_time:19506ms step_avg:59.84ms
step:327/2420 train_time:19565ms step_avg:59.83ms
step:328/2420 train_time:19625ms step_avg:59.83ms
step:329/2420 train_time:19683ms step_avg:59.83ms
step:330/2420 train_time:19744ms step_avg:59.83ms
step:331/2420 train_time:19802ms step_avg:59.83ms
step:332/2420 train_time:19863ms step_avg:59.83ms
step:333/2420 train_time:19921ms step_avg:59.82ms
step:334/2420 train_time:19981ms step_avg:59.82ms
step:335/2420 train_time:20039ms step_avg:59.82ms
step:336/2420 train_time:20099ms step_avg:59.82ms
step:337/2420 train_time:20158ms step_avg:59.81ms
step:338/2420 train_time:20218ms step_avg:59.82ms
step:339/2420 train_time:20277ms step_avg:59.81ms
step:340/2420 train_time:20337ms step_avg:59.81ms
step:341/2420 train_time:20395ms step_avg:59.81ms
step:342/2420 train_time:20456ms step_avg:59.81ms
step:343/2420 train_time:20514ms step_avg:59.81ms
step:344/2420 train_time:20575ms step_avg:59.81ms
step:345/2420 train_time:20633ms step_avg:59.81ms
step:346/2420 train_time:20694ms step_avg:59.81ms
step:347/2420 train_time:20753ms step_avg:59.81ms
step:348/2420 train_time:20812ms step_avg:59.81ms
step:349/2420 train_time:20871ms step_avg:59.80ms
step:350/2420 train_time:20931ms step_avg:59.80ms
step:351/2420 train_time:20990ms step_avg:59.80ms
step:352/2420 train_time:21051ms step_avg:59.80ms
step:353/2420 train_time:21110ms step_avg:59.80ms
step:354/2420 train_time:21170ms step_avg:59.80ms
step:355/2420 train_time:21229ms step_avg:59.80ms
step:356/2420 train_time:21290ms step_avg:59.80ms
step:357/2420 train_time:21348ms step_avg:59.80ms
step:358/2420 train_time:21408ms step_avg:59.80ms
step:359/2420 train_time:21467ms step_avg:59.80ms
step:360/2420 train_time:21527ms step_avg:59.80ms
step:361/2420 train_time:21586ms step_avg:59.79ms
step:362/2420 train_time:21646ms step_avg:59.80ms
step:363/2420 train_time:21705ms step_avg:59.79ms
step:364/2420 train_time:21765ms step_avg:59.79ms
step:365/2420 train_time:21824ms step_avg:59.79ms
step:366/2420 train_time:21884ms step_avg:59.79ms
step:367/2420 train_time:21943ms step_avg:59.79ms
step:368/2420 train_time:22003ms step_avg:59.79ms
step:369/2420 train_time:22062ms step_avg:59.79ms
step:370/2420 train_time:22122ms step_avg:59.79ms
step:371/2420 train_time:22181ms step_avg:59.79ms
step:372/2420 train_time:22241ms step_avg:59.79ms
step:373/2420 train_time:22300ms step_avg:59.78ms
step:374/2420 train_time:22360ms step_avg:59.79ms
step:375/2420 train_time:22418ms step_avg:59.78ms
step:376/2420 train_time:22478ms step_avg:59.78ms
step:377/2420 train_time:22537ms step_avg:59.78ms
step:378/2420 train_time:22597ms step_avg:59.78ms
step:379/2420 train_time:22655ms step_avg:59.78ms
step:380/2420 train_time:22716ms step_avg:59.78ms
step:381/2420 train_time:22774ms step_avg:59.77ms
step:382/2420 train_time:22835ms step_avg:59.78ms
step:383/2420 train_time:22894ms step_avg:59.78ms
step:384/2420 train_time:22954ms step_avg:59.78ms
step:385/2420 train_time:23013ms step_avg:59.77ms
step:386/2420 train_time:23073ms step_avg:59.77ms
step:387/2420 train_time:23132ms step_avg:59.77ms
step:388/2420 train_time:23193ms step_avg:59.78ms
step:389/2420 train_time:23252ms step_avg:59.77ms
step:390/2420 train_time:23312ms step_avg:59.77ms
step:391/2420 train_time:23370ms step_avg:59.77ms
step:392/2420 train_time:23431ms step_avg:59.77ms
step:393/2420 train_time:23489ms step_avg:59.77ms
step:394/2420 train_time:23550ms step_avg:59.77ms
step:395/2420 train_time:23608ms step_avg:59.77ms
step:396/2420 train_time:23669ms step_avg:59.77ms
step:397/2420 train_time:23727ms step_avg:59.77ms
step:398/2420 train_time:23788ms step_avg:59.77ms
step:399/2420 train_time:23846ms step_avg:59.77ms
step:400/2420 train_time:23907ms step_avg:59.77ms
step:401/2420 train_time:23965ms step_avg:59.76ms
step:402/2420 train_time:24026ms step_avg:59.77ms
step:403/2420 train_time:24086ms step_avg:59.77ms
step:404/2420 train_time:24146ms step_avg:59.77ms
step:405/2420 train_time:24205ms step_avg:59.76ms
step:406/2420 train_time:24265ms step_avg:59.77ms
step:407/2420 train_time:24323ms step_avg:59.76ms
step:408/2420 train_time:24384ms step_avg:59.76ms
step:409/2420 train_time:24442ms step_avg:59.76ms
step:410/2420 train_time:24503ms step_avg:59.76ms
step:411/2420 train_time:24562ms step_avg:59.76ms
step:412/2420 train_time:24622ms step_avg:59.76ms
step:413/2420 train_time:24681ms step_avg:59.76ms
step:414/2420 train_time:24741ms step_avg:59.76ms
step:415/2420 train_time:24799ms step_avg:59.76ms
step:416/2420 train_time:24860ms step_avg:59.76ms
step:417/2420 train_time:24918ms step_avg:59.76ms
step:418/2420 train_time:24978ms step_avg:59.76ms
step:419/2420 train_time:25037ms step_avg:59.75ms
step:420/2420 train_time:25097ms step_avg:59.75ms
step:421/2420 train_time:25155ms step_avg:59.75ms
step:422/2420 train_time:25216ms step_avg:59.75ms
step:423/2420 train_time:25274ms step_avg:59.75ms
step:424/2420 train_time:25335ms step_avg:59.75ms
step:425/2420 train_time:25394ms step_avg:59.75ms
step:426/2420 train_time:25455ms step_avg:59.75ms
step:427/2420 train_time:25514ms step_avg:59.75ms
step:428/2420 train_time:25574ms step_avg:59.75ms
step:429/2420 train_time:25633ms step_avg:59.75ms
step:430/2420 train_time:25694ms step_avg:59.75ms
step:431/2420 train_time:25752ms step_avg:59.75ms
step:432/2420 train_time:25812ms step_avg:59.75ms
step:433/2420 train_time:25870ms step_avg:59.75ms
step:434/2420 train_time:25931ms step_avg:59.75ms
step:435/2420 train_time:25990ms step_avg:59.75ms
step:436/2420 train_time:26051ms step_avg:59.75ms
step:437/2420 train_time:26109ms step_avg:59.75ms
step:438/2420 train_time:26170ms step_avg:59.75ms
step:439/2420 train_time:26229ms step_avg:59.75ms
step:440/2420 train_time:26289ms step_avg:59.75ms
step:441/2420 train_time:26348ms step_avg:59.75ms
step:442/2420 train_time:26408ms step_avg:59.75ms
step:443/2420 train_time:26467ms step_avg:59.74ms
step:444/2420 train_time:26527ms step_avg:59.75ms
step:445/2420 train_time:26586ms step_avg:59.74ms
step:446/2420 train_time:26647ms step_avg:59.75ms
step:447/2420 train_time:26706ms step_avg:59.74ms
step:448/2420 train_time:26767ms step_avg:59.75ms
step:449/2420 train_time:26825ms step_avg:59.74ms
step:450/2420 train_time:26886ms step_avg:59.75ms
step:451/2420 train_time:26945ms step_avg:59.74ms
step:452/2420 train_time:27005ms step_avg:59.75ms
step:453/2420 train_time:27064ms step_avg:59.74ms
step:454/2420 train_time:27125ms step_avg:59.75ms
step:455/2420 train_time:27183ms step_avg:59.74ms
step:456/2420 train_time:27245ms step_avg:59.75ms
step:457/2420 train_time:27303ms step_avg:59.74ms
step:458/2420 train_time:27364ms step_avg:59.75ms
step:459/2420 train_time:27422ms step_avg:59.74ms
step:460/2420 train_time:27483ms step_avg:59.75ms
step:461/2420 train_time:27541ms step_avg:59.74ms
step:462/2420 train_time:27603ms step_avg:59.75ms
step:463/2420 train_time:27662ms step_avg:59.74ms
step:464/2420 train_time:27722ms step_avg:59.75ms
step:465/2420 train_time:27781ms step_avg:59.74ms
step:466/2420 train_time:27841ms step_avg:59.74ms
step:467/2420 train_time:27899ms step_avg:59.74ms
step:468/2420 train_time:27959ms step_avg:59.74ms
step:469/2420 train_time:28017ms step_avg:59.74ms
step:470/2420 train_time:28078ms step_avg:59.74ms
step:471/2420 train_time:28136ms step_avg:59.74ms
step:472/2420 train_time:28196ms step_avg:59.74ms
step:473/2420 train_time:28254ms step_avg:59.73ms
step:474/2420 train_time:28314ms step_avg:59.73ms
step:475/2420 train_time:28372ms step_avg:59.73ms
step:476/2420 train_time:28433ms step_avg:59.73ms
step:477/2420 train_time:28492ms step_avg:59.73ms
step:478/2420 train_time:28553ms step_avg:59.73ms
step:479/2420 train_time:28611ms step_avg:59.73ms
step:480/2420 train_time:28672ms step_avg:59.73ms
step:481/2420 train_time:28731ms step_avg:59.73ms
step:482/2420 train_time:28791ms step_avg:59.73ms
step:483/2420 train_time:28850ms step_avg:59.73ms
step:484/2420 train_time:28910ms step_avg:59.73ms
step:485/2420 train_time:28969ms step_avg:59.73ms
step:486/2420 train_time:29030ms step_avg:59.73ms
step:487/2420 train_time:29088ms step_avg:59.73ms
step:488/2420 train_time:29149ms step_avg:59.73ms
step:489/2420 train_time:29208ms step_avg:59.73ms
step:490/2420 train_time:29268ms step_avg:59.73ms
step:491/2420 train_time:29327ms step_avg:59.73ms
step:492/2420 train_time:29388ms step_avg:59.73ms
step:493/2420 train_time:29447ms step_avg:59.73ms
step:494/2420 train_time:29507ms step_avg:59.73ms
step:495/2420 train_time:29566ms step_avg:59.73ms
step:496/2420 train_time:29627ms step_avg:59.73ms
step:497/2420 train_time:29685ms step_avg:59.73ms
step:498/2420 train_time:29746ms step_avg:59.73ms
step:499/2420 train_time:29805ms step_avg:59.73ms
step:500/2420 train_time:29865ms step_avg:59.73ms
step:500/2420 val_loss:3.8311 train_time:29928ms step_avg:59.86ms
step:501/2420 train_time:29949ms step_avg:59.78ms
step:502/2420 train_time:29987ms step_avg:59.74ms
step:503/2420 train_time:30048ms step_avg:59.74ms
step:504/2420 train_time:30111ms step_avg:59.74ms
step:505/2420 train_time:30170ms step_avg:59.74ms
step:506/2420 train_time:30231ms step_avg:59.75ms
step:507/2420 train_time:30289ms step_avg:59.74ms
step:508/2420 train_time:30349ms step_avg:59.74ms
step:509/2420 train_time:30407ms step_avg:59.74ms
step:510/2420 train_time:30467ms step_avg:59.74ms
step:511/2420 train_time:30525ms step_avg:59.73ms
step:512/2420 train_time:30585ms step_avg:59.74ms
step:513/2420 train_time:30643ms step_avg:59.73ms
step:514/2420 train_time:30703ms step_avg:59.73ms
step:515/2420 train_time:30761ms step_avg:59.73ms
step:516/2420 train_time:30820ms step_avg:59.73ms
step:517/2420 train_time:30880ms step_avg:59.73ms
step:518/2420 train_time:30941ms step_avg:59.73ms
step:519/2420 train_time:31002ms step_avg:59.73ms
step:520/2420 train_time:31063ms step_avg:59.74ms
step:521/2420 train_time:31124ms step_avg:59.74ms
step:522/2420 train_time:31185ms step_avg:59.74ms
step:523/2420 train_time:31244ms step_avg:59.74ms
step:524/2420 train_time:31305ms step_avg:59.74ms
step:525/2420 train_time:31363ms step_avg:59.74ms
step:526/2420 train_time:31423ms step_avg:59.74ms
step:527/2420 train_time:31481ms step_avg:59.74ms
step:528/2420 train_time:31542ms step_avg:59.74ms
step:529/2420 train_time:31600ms step_avg:59.74ms
step:530/2420 train_time:31660ms step_avg:59.74ms
step:531/2420 train_time:31718ms step_avg:59.73ms
step:532/2420 train_time:31778ms step_avg:59.73ms
step:533/2420 train_time:31836ms step_avg:59.73ms
step:534/2420 train_time:31897ms step_avg:59.73ms
step:535/2420 train_time:31956ms step_avg:59.73ms
step:536/2420 train_time:32017ms step_avg:59.73ms
step:537/2420 train_time:32076ms step_avg:59.73ms
step:538/2420 train_time:32137ms step_avg:59.73ms
step:539/2420 train_time:32196ms step_avg:59.73ms
step:540/2420 train_time:32258ms step_avg:59.74ms
step:541/2420 train_time:32317ms step_avg:59.74ms
step:542/2420 train_time:32378ms step_avg:59.74ms
step:543/2420 train_time:32437ms step_avg:59.74ms
step:544/2420 train_time:32497ms step_avg:59.74ms
step:545/2420 train_time:32555ms step_avg:59.73ms
step:546/2420 train_time:32615ms step_avg:59.73ms
step:547/2420 train_time:32673ms step_avg:59.73ms
step:548/2420 train_time:32733ms step_avg:59.73ms
step:549/2420 train_time:32791ms step_avg:59.73ms
step:550/2420 train_time:32851ms step_avg:59.73ms
step:551/2420 train_time:32909ms step_avg:59.73ms
step:552/2420 train_time:32969ms step_avg:59.73ms
step:553/2420 train_time:33028ms step_avg:59.73ms
step:554/2420 train_time:33089ms step_avg:59.73ms
step:555/2420 train_time:33147ms step_avg:59.72ms
step:556/2420 train_time:33208ms step_avg:59.73ms
step:557/2420 train_time:33267ms step_avg:59.72ms
step:558/2420 train_time:33328ms step_avg:59.73ms
step:559/2420 train_time:33386ms step_avg:59.73ms
step:560/2420 train_time:33447ms step_avg:59.73ms
step:561/2420 train_time:33506ms step_avg:59.72ms
step:562/2420 train_time:33567ms step_avg:59.73ms
step:563/2420 train_time:33625ms step_avg:59.72ms
step:564/2420 train_time:33686ms step_avg:59.73ms
step:565/2420 train_time:33745ms step_avg:59.73ms
step:566/2420 train_time:33806ms step_avg:59.73ms
step:567/2420 train_time:33865ms step_avg:59.73ms
step:568/2420 train_time:33926ms step_avg:59.73ms
step:569/2420 train_time:33985ms step_avg:59.73ms
step:570/2420 train_time:34046ms step_avg:59.73ms
step:571/2420 train_time:34105ms step_avg:59.73ms
step:572/2420 train_time:34166ms step_avg:59.73ms
step:573/2420 train_time:34225ms step_avg:59.73ms
step:574/2420 train_time:34286ms step_avg:59.73ms
step:575/2420 train_time:34345ms step_avg:59.73ms
step:576/2420 train_time:34406ms step_avg:59.73ms
step:577/2420 train_time:34465ms step_avg:59.73ms
step:578/2420 train_time:34525ms step_avg:59.73ms
step:579/2420 train_time:34584ms step_avg:59.73ms
step:580/2420 train_time:34645ms step_avg:59.73ms
step:581/2420 train_time:34703ms step_avg:59.73ms
step:582/2420 train_time:34764ms step_avg:59.73ms
step:583/2420 train_time:34823ms step_avg:59.73ms
step:584/2420 train_time:34884ms step_avg:59.73ms
step:585/2420 train_time:34943ms step_avg:59.73ms
step:586/2420 train_time:35005ms step_avg:59.74ms
step:587/2420 train_time:35064ms step_avg:59.73ms
step:588/2420 train_time:35124ms step_avg:59.73ms
step:589/2420 train_time:35183ms step_avg:59.73ms
step:590/2420 train_time:35244ms step_avg:59.74ms
step:591/2420 train_time:35303ms step_avg:59.73ms
step:592/2420 train_time:35363ms step_avg:59.74ms
step:593/2420 train_time:35422ms step_avg:59.73ms
step:594/2420 train_time:35483ms step_avg:59.74ms
step:595/2420 train_time:35541ms step_avg:59.73ms
step:596/2420 train_time:35602ms step_avg:59.74ms
step:597/2420 train_time:35661ms step_avg:59.73ms
step:598/2420 train_time:35722ms step_avg:59.74ms
step:599/2420 train_time:35780ms step_avg:59.73ms
step:600/2420 train_time:35841ms step_avg:59.74ms
step:601/2420 train_time:35900ms step_avg:59.73ms
step:602/2420 train_time:35962ms step_avg:59.74ms
step:603/2420 train_time:36020ms step_avg:59.74ms
step:604/2420 train_time:36082ms step_avg:59.74ms
step:605/2420 train_time:36140ms step_avg:59.74ms
step:606/2420 train_time:36201ms step_avg:59.74ms
step:607/2420 train_time:36260ms step_avg:59.74ms
step:608/2420 train_time:36320ms step_avg:59.74ms
step:609/2420 train_time:36379ms step_avg:59.74ms
step:610/2420 train_time:36440ms step_avg:59.74ms
step:611/2420 train_time:36499ms step_avg:59.74ms
step:612/2420 train_time:36560ms step_avg:59.74ms
step:613/2420 train_time:36618ms step_avg:59.74ms
step:614/2420 train_time:36679ms step_avg:59.74ms
step:615/2420 train_time:36737ms step_avg:59.74ms
step:616/2420 train_time:36798ms step_avg:59.74ms
step:617/2420 train_time:36857ms step_avg:59.74ms
step:618/2420 train_time:36918ms step_avg:59.74ms
step:619/2420 train_time:36976ms step_avg:59.74ms
step:620/2420 train_time:37037ms step_avg:59.74ms
step:621/2420 train_time:37095ms step_avg:59.73ms
step:622/2420 train_time:37156ms step_avg:59.74ms
step:623/2420 train_time:37214ms step_avg:59.73ms
step:624/2420 train_time:37274ms step_avg:59.73ms
step:625/2420 train_time:37333ms step_avg:59.73ms
step:626/2420 train_time:37393ms step_avg:59.73ms
step:627/2420 train_time:37451ms step_avg:59.73ms
step:628/2420 train_time:37511ms step_avg:59.73ms
step:629/2420 train_time:37570ms step_avg:59.73ms
step:630/2420 train_time:37630ms step_avg:59.73ms
step:631/2420 train_time:37688ms step_avg:59.73ms
step:632/2420 train_time:37748ms step_avg:59.73ms
step:633/2420 train_time:37807ms step_avg:59.73ms
step:634/2420 train_time:37867ms step_avg:59.73ms
step:635/2420 train_time:37926ms step_avg:59.73ms
step:636/2420 train_time:37987ms step_avg:59.73ms
step:637/2420 train_time:38046ms step_avg:59.73ms
step:638/2420 train_time:38107ms step_avg:59.73ms
step:639/2420 train_time:38165ms step_avg:59.73ms
step:640/2420 train_time:38225ms step_avg:59.73ms
step:641/2420 train_time:38285ms step_avg:59.73ms
step:642/2420 train_time:38346ms step_avg:59.73ms
step:643/2420 train_time:38405ms step_avg:59.73ms
step:644/2420 train_time:38465ms step_avg:59.73ms
step:645/2420 train_time:38524ms step_avg:59.73ms
step:646/2420 train_time:38584ms step_avg:59.73ms
step:647/2420 train_time:38643ms step_avg:59.73ms
step:648/2420 train_time:38703ms step_avg:59.73ms
step:649/2420 train_time:38762ms step_avg:59.73ms
step:650/2420 train_time:38822ms step_avg:59.73ms
step:651/2420 train_time:38881ms step_avg:59.73ms
step:652/2420 train_time:38942ms step_avg:59.73ms
step:653/2420 train_time:39000ms step_avg:59.73ms
step:654/2420 train_time:39061ms step_avg:59.73ms
step:655/2420 train_time:39120ms step_avg:59.72ms
step:656/2420 train_time:39181ms step_avg:59.73ms
step:657/2420 train_time:39240ms step_avg:59.73ms
step:658/2420 train_time:39301ms step_avg:59.73ms
step:659/2420 train_time:39360ms step_avg:59.73ms
step:660/2420 train_time:39421ms step_avg:59.73ms
step:661/2420 train_time:39479ms step_avg:59.73ms
step:662/2420 train_time:39540ms step_avg:59.73ms
step:663/2420 train_time:39599ms step_avg:59.73ms
step:664/2420 train_time:39660ms step_avg:59.73ms
step:665/2420 train_time:39718ms step_avg:59.73ms
step:666/2420 train_time:39779ms step_avg:59.73ms
step:667/2420 train_time:39837ms step_avg:59.73ms
step:668/2420 train_time:39897ms step_avg:59.73ms
step:669/2420 train_time:39956ms step_avg:59.73ms
step:670/2420 train_time:40017ms step_avg:59.73ms
step:671/2420 train_time:40076ms step_avg:59.73ms
step:672/2420 train_time:40137ms step_avg:59.73ms
step:673/2420 train_time:40195ms step_avg:59.73ms
step:674/2420 train_time:40256ms step_avg:59.73ms
step:675/2420 train_time:40315ms step_avg:59.73ms
step:676/2420 train_time:40376ms step_avg:59.73ms
step:677/2420 train_time:40434ms step_avg:59.73ms
step:678/2420 train_time:40495ms step_avg:59.73ms
step:679/2420 train_time:40553ms step_avg:59.72ms
step:680/2420 train_time:40613ms step_avg:59.73ms
step:681/2420 train_time:40672ms step_avg:59.72ms
step:682/2420 train_time:40732ms step_avg:59.72ms
step:683/2420 train_time:40790ms step_avg:59.72ms
step:684/2420 train_time:40850ms step_avg:59.72ms
step:685/2420 train_time:40908ms step_avg:59.72ms
step:686/2420 train_time:40969ms step_avg:59.72ms
step:687/2420 train_time:41027ms step_avg:59.72ms
step:688/2420 train_time:41088ms step_avg:59.72ms
step:689/2420 train_time:41148ms step_avg:59.72ms
step:690/2420 train_time:41208ms step_avg:59.72ms
step:691/2420 train_time:41267ms step_avg:59.72ms
step:692/2420 train_time:41328ms step_avg:59.72ms
step:693/2420 train_time:41386ms step_avg:59.72ms
step:694/2420 train_time:41448ms step_avg:59.72ms
step:695/2420 train_time:41507ms step_avg:59.72ms
step:696/2420 train_time:41567ms step_avg:59.72ms
step:697/2420 train_time:41626ms step_avg:59.72ms
step:698/2420 train_time:41686ms step_avg:59.72ms
step:699/2420 train_time:41745ms step_avg:59.72ms
step:700/2420 train_time:41805ms step_avg:59.72ms
step:701/2420 train_time:41863ms step_avg:59.72ms
step:702/2420 train_time:41924ms step_avg:59.72ms
step:703/2420 train_time:41982ms step_avg:59.72ms
step:704/2420 train_time:42043ms step_avg:59.72ms
step:705/2420 train_time:42102ms step_avg:59.72ms
step:706/2420 train_time:42163ms step_avg:59.72ms
step:707/2420 train_time:42222ms step_avg:59.72ms
step:708/2420 train_time:42282ms step_avg:59.72ms
step:709/2420 train_time:42341ms step_avg:59.72ms
step:710/2420 train_time:42402ms step_avg:59.72ms
step:711/2420 train_time:42461ms step_avg:59.72ms
step:712/2420 train_time:42522ms step_avg:59.72ms
step:713/2420 train_time:42582ms step_avg:59.72ms
step:714/2420 train_time:42643ms step_avg:59.72ms
step:715/2420 train_time:42702ms step_avg:59.72ms
step:716/2420 train_time:42762ms step_avg:59.72ms
step:717/2420 train_time:42821ms step_avg:59.72ms
step:718/2420 train_time:42882ms step_avg:59.72ms
step:719/2420 train_time:42941ms step_avg:59.72ms
step:720/2420 train_time:43001ms step_avg:59.72ms
step:721/2420 train_time:43060ms step_avg:59.72ms
step:722/2420 train_time:43121ms step_avg:59.72ms
step:723/2420 train_time:43180ms step_avg:59.72ms
step:724/2420 train_time:43241ms step_avg:59.72ms
step:725/2420 train_time:43300ms step_avg:59.72ms
step:726/2420 train_time:43361ms step_avg:59.73ms
step:727/2420 train_time:43420ms step_avg:59.72ms
step:728/2420 train_time:43481ms step_avg:59.73ms
step:729/2420 train_time:43540ms step_avg:59.73ms
step:730/2420 train_time:43601ms step_avg:59.73ms
step:731/2420 train_time:43660ms step_avg:59.73ms
step:732/2420 train_time:43720ms step_avg:59.73ms
step:733/2420 train_time:43779ms step_avg:59.73ms
step:734/2420 train_time:43839ms step_avg:59.73ms
step:735/2420 train_time:43898ms step_avg:59.73ms
step:736/2420 train_time:43958ms step_avg:59.73ms
step:737/2420 train_time:44017ms step_avg:59.72ms
step:738/2420 train_time:44078ms step_avg:59.73ms
step:739/2420 train_time:44136ms step_avg:59.72ms
step:740/2420 train_time:44197ms step_avg:59.73ms
step:741/2420 train_time:44256ms step_avg:59.72ms
step:742/2420 train_time:44317ms step_avg:59.73ms
step:743/2420 train_time:44376ms step_avg:59.72ms
step:744/2420 train_time:44436ms step_avg:59.73ms
step:745/2420 train_time:44494ms step_avg:59.72ms
step:746/2420 train_time:44555ms step_avg:59.73ms
step:747/2420 train_time:44614ms step_avg:59.72ms
step:748/2420 train_time:44674ms step_avg:59.72ms
step:749/2420 train_time:44733ms step_avg:59.72ms
step:750/2420 train_time:44793ms step_avg:59.72ms
step:750/2420 val_loss:3.6934 train_time:44856ms step_avg:59.81ms
step:751/2420 train_time:44878ms step_avg:59.76ms
step:752/2420 train_time:44917ms step_avg:59.73ms
step:753/2420 train_time:44979ms step_avg:59.73ms
step:754/2420 train_time:45044ms step_avg:59.74ms
step:755/2420 train_time:45103ms step_avg:59.74ms
step:756/2420 train_time:45164ms step_avg:59.74ms
step:757/2420 train_time:45222ms step_avg:59.74ms
step:758/2420 train_time:45282ms step_avg:59.74ms
step:759/2420 train_time:45340ms step_avg:59.74ms
step:760/2420 train_time:45400ms step_avg:59.74ms
step:761/2420 train_time:45457ms step_avg:59.73ms
step:762/2420 train_time:45517ms step_avg:59.73ms
step:763/2420 train_time:45575ms step_avg:59.73ms
step:764/2420 train_time:45634ms step_avg:59.73ms
step:765/2420 train_time:45692ms step_avg:59.73ms
step:766/2420 train_time:45752ms step_avg:59.73ms
step:767/2420 train_time:45812ms step_avg:59.73ms
step:768/2420 train_time:45873ms step_avg:59.73ms
step:769/2420 train_time:45933ms step_avg:59.73ms
step:770/2420 train_time:45996ms step_avg:59.73ms
step:771/2420 train_time:46056ms step_avg:59.73ms
step:772/2420 train_time:46117ms step_avg:59.74ms
step:773/2420 train_time:46176ms step_avg:59.74ms
step:774/2420 train_time:46237ms step_avg:59.74ms
step:775/2420 train_time:46296ms step_avg:59.74ms
step:776/2420 train_time:46356ms step_avg:59.74ms
step:777/2420 train_time:46414ms step_avg:59.73ms
step:778/2420 train_time:46474ms step_avg:59.74ms
step:779/2420 train_time:46533ms step_avg:59.73ms
step:780/2420 train_time:46592ms step_avg:59.73ms
step:781/2420 train_time:46650ms step_avg:59.73ms
step:782/2420 train_time:46710ms step_avg:59.73ms
step:783/2420 train_time:46769ms step_avg:59.73ms
step:784/2420 train_time:46829ms step_avg:59.73ms
step:785/2420 train_time:46888ms step_avg:59.73ms
step:786/2420 train_time:46948ms step_avg:59.73ms
step:787/2420 train_time:47008ms step_avg:59.73ms
step:788/2420 train_time:47069ms step_avg:59.73ms
step:789/2420 train_time:47128ms step_avg:59.73ms
step:790/2420 train_time:47189ms step_avg:59.73ms
step:791/2420 train_time:47248ms step_avg:59.73ms
step:792/2420 train_time:47308ms step_avg:59.73ms
step:793/2420 train_time:47366ms step_avg:59.73ms
step:794/2420 train_time:47427ms step_avg:59.73ms
step:795/2420 train_time:47485ms step_avg:59.73ms
step:796/2420 train_time:47545ms step_avg:59.73ms
step:797/2420 train_time:47604ms step_avg:59.73ms
step:798/2420 train_time:47664ms step_avg:59.73ms
step:799/2420 train_time:47723ms step_avg:59.73ms
step:800/2420 train_time:47784ms step_avg:59.73ms
step:801/2420 train_time:47844ms step_avg:59.73ms
step:802/2420 train_time:47906ms step_avg:59.73ms
step:803/2420 train_time:47965ms step_avg:59.73ms
step:804/2420 train_time:48026ms step_avg:59.73ms
step:805/2420 train_time:48086ms step_avg:59.73ms
step:806/2420 train_time:48147ms step_avg:59.74ms
step:807/2420 train_time:48206ms step_avg:59.74ms
step:808/2420 train_time:48268ms step_avg:59.74ms
step:809/2420 train_time:48327ms step_avg:59.74ms
step:810/2420 train_time:48387ms step_avg:59.74ms
step:811/2420 train_time:48446ms step_avg:59.74ms
step:812/2420 train_time:48507ms step_avg:59.74ms
step:813/2420 train_time:48566ms step_avg:59.74ms
step:814/2420 train_time:48626ms step_avg:59.74ms
step:815/2420 train_time:48685ms step_avg:59.74ms
step:816/2420 train_time:48746ms step_avg:59.74ms
step:817/2420 train_time:48805ms step_avg:59.74ms
step:818/2420 train_time:48866ms step_avg:59.74ms
step:819/2420 train_time:48926ms step_avg:59.74ms
step:820/2420 train_time:48987ms step_avg:59.74ms
step:821/2420 train_time:49047ms step_avg:59.74ms
step:822/2420 train_time:49108ms step_avg:59.74ms
step:823/2420 train_time:49167ms step_avg:59.74ms
step:824/2420 train_time:49228ms step_avg:59.74ms
step:825/2420 train_time:49287ms step_avg:59.74ms
step:826/2420 train_time:49348ms step_avg:59.74ms
step:827/2420 train_time:49408ms step_avg:59.74ms
step:828/2420 train_time:49469ms step_avg:59.75ms
step:829/2420 train_time:49528ms step_avg:59.74ms
step:830/2420 train_time:49590ms step_avg:59.75ms
step:831/2420 train_time:49649ms step_avg:59.75ms
step:832/2420 train_time:49710ms step_avg:59.75ms
step:833/2420 train_time:49770ms step_avg:59.75ms
step:834/2420 train_time:49831ms step_avg:59.75ms
step:835/2420 train_time:49890ms step_avg:59.75ms
step:836/2420 train_time:49952ms step_avg:59.75ms
step:837/2420 train_time:50011ms step_avg:59.75ms
step:838/2420 train_time:50073ms step_avg:59.75ms
step:839/2420 train_time:50133ms step_avg:59.75ms
step:840/2420 train_time:50195ms step_avg:59.76ms
step:841/2420 train_time:50254ms step_avg:59.75ms
step:842/2420 train_time:50315ms step_avg:59.76ms
step:843/2420 train_time:50375ms step_avg:59.76ms
step:844/2420 train_time:50436ms step_avg:59.76ms
step:845/2420 train_time:50495ms step_avg:59.76ms
step:846/2420 train_time:50556ms step_avg:59.76ms
step:847/2420 train_time:50616ms step_avg:59.76ms
step:848/2420 train_time:50677ms step_avg:59.76ms
step:849/2420 train_time:50736ms step_avg:59.76ms
step:850/2420 train_time:50797ms step_avg:59.76ms
step:851/2420 train_time:50857ms step_avg:59.76ms
step:852/2420 train_time:50920ms step_avg:59.76ms
step:853/2420 train_time:50979ms step_avg:59.76ms
step:854/2420 train_time:51041ms step_avg:59.77ms
step:855/2420 train_time:51101ms step_avg:59.77ms
step:856/2420 train_time:51163ms step_avg:59.77ms
step:857/2420 train_time:51222ms step_avg:59.77ms
step:858/2420 train_time:51283ms step_avg:59.77ms
step:859/2420 train_time:51342ms step_avg:59.77ms
step:860/2420 train_time:51404ms step_avg:59.77ms
step:861/2420 train_time:51463ms step_avg:59.77ms
step:862/2420 train_time:51524ms step_avg:59.77ms
step:863/2420 train_time:51583ms step_avg:59.77ms
step:864/2420 train_time:51644ms step_avg:59.77ms
step:865/2420 train_time:51704ms step_avg:59.77ms
step:866/2420 train_time:51765ms step_avg:59.78ms
step:867/2420 train_time:51825ms step_avg:59.77ms
step:868/2420 train_time:51885ms step_avg:59.78ms
step:869/2420 train_time:51945ms step_avg:59.78ms
step:870/2420 train_time:52006ms step_avg:59.78ms
step:871/2420 train_time:52065ms step_avg:59.78ms
step:872/2420 train_time:52126ms step_avg:59.78ms
step:873/2420 train_time:52186ms step_avg:59.78ms
step:874/2420 train_time:52248ms step_avg:59.78ms
step:875/2420 train_time:52307ms step_avg:59.78ms
step:876/2420 train_time:52368ms step_avg:59.78ms
step:877/2420 train_time:52427ms step_avg:59.78ms
step:878/2420 train_time:52488ms step_avg:59.78ms
step:879/2420 train_time:52548ms step_avg:59.78ms
step:880/2420 train_time:52609ms step_avg:59.78ms
step:881/2420 train_time:52668ms step_avg:59.78ms
step:882/2420 train_time:52729ms step_avg:59.78ms
step:883/2420 train_time:52788ms step_avg:59.78ms
step:884/2420 train_time:52849ms step_avg:59.78ms
step:885/2420 train_time:52909ms step_avg:59.78ms
step:886/2420 train_time:52970ms step_avg:59.79ms
step:887/2420 train_time:53029ms step_avg:59.78ms
step:888/2420 train_time:53090ms step_avg:59.79ms
step:889/2420 train_time:53150ms step_avg:59.79ms
step:890/2420 train_time:53211ms step_avg:59.79ms
step:891/2420 train_time:53270ms step_avg:59.79ms
step:892/2420 train_time:53331ms step_avg:59.79ms
step:893/2420 train_time:53391ms step_avg:59.79ms
step:894/2420 train_time:53452ms step_avg:59.79ms
step:895/2420 train_time:53511ms step_avg:59.79ms
step:896/2420 train_time:53573ms step_avg:59.79ms
step:897/2420 train_time:53632ms step_avg:59.79ms
step:898/2420 train_time:53693ms step_avg:59.79ms
step:899/2420 train_time:53753ms step_avg:59.79ms
step:900/2420 train_time:53814ms step_avg:59.79ms
step:901/2420 train_time:53874ms step_avg:59.79ms
step:902/2420 train_time:53935ms step_avg:59.79ms
step:903/2420 train_time:53995ms step_avg:59.80ms
step:904/2420 train_time:54056ms step_avg:59.80ms
step:905/2420 train_time:54116ms step_avg:59.80ms
step:906/2420 train_time:54177ms step_avg:59.80ms
step:907/2420 train_time:54237ms step_avg:59.80ms
step:908/2420 train_time:54299ms step_avg:59.80ms
step:909/2420 train_time:54359ms step_avg:59.80ms
step:910/2420 train_time:54421ms step_avg:59.80ms
step:911/2420 train_time:54480ms step_avg:59.80ms
step:912/2420 train_time:54542ms step_avg:59.80ms
step:913/2420 train_time:54601ms step_avg:59.80ms
step:914/2420 train_time:54662ms step_avg:59.80ms
step:915/2420 train_time:54721ms step_avg:59.80ms
step:916/2420 train_time:54783ms step_avg:59.81ms
step:917/2420 train_time:54842ms step_avg:59.81ms
step:918/2420 train_time:54903ms step_avg:59.81ms
step:919/2420 train_time:54962ms step_avg:59.81ms
step:920/2420 train_time:55023ms step_avg:59.81ms
step:921/2420 train_time:55082ms step_avg:59.81ms
step:922/2420 train_time:55144ms step_avg:59.81ms
step:923/2420 train_time:55204ms step_avg:59.81ms
step:924/2420 train_time:55265ms step_avg:59.81ms
step:925/2420 train_time:55324ms step_avg:59.81ms
step:926/2420 train_time:55386ms step_avg:59.81ms
step:927/2420 train_time:55445ms step_avg:59.81ms
step:928/2420 train_time:55506ms step_avg:59.81ms
step:929/2420 train_time:55565ms step_avg:59.81ms
step:930/2420 train_time:55626ms step_avg:59.81ms
step:931/2420 train_time:55685ms step_avg:59.81ms
step:932/2420 train_time:55745ms step_avg:59.81ms
step:933/2420 train_time:55805ms step_avg:59.81ms
step:934/2420 train_time:55866ms step_avg:59.81ms
step:935/2420 train_time:55925ms step_avg:59.81ms
step:936/2420 train_time:55986ms step_avg:59.81ms
step:937/2420 train_time:56045ms step_avg:59.81ms
step:938/2420 train_time:56107ms step_avg:59.82ms
step:939/2420 train_time:56166ms step_avg:59.81ms
step:940/2420 train_time:56227ms step_avg:59.82ms
step:941/2420 train_time:56286ms step_avg:59.82ms
step:942/2420 train_time:56347ms step_avg:59.82ms
step:943/2420 train_time:56406ms step_avg:59.82ms
step:944/2420 train_time:56467ms step_avg:59.82ms
step:945/2420 train_time:56526ms step_avg:59.82ms
step:946/2420 train_time:56587ms step_avg:59.82ms
step:947/2420 train_time:56646ms step_avg:59.82ms
step:948/2420 train_time:56708ms step_avg:59.82ms
step:949/2420 train_time:56767ms step_avg:59.82ms
step:950/2420 train_time:56829ms step_avg:59.82ms
step:951/2420 train_time:56888ms step_avg:59.82ms
step:952/2420 train_time:56949ms step_avg:59.82ms
step:953/2420 train_time:57008ms step_avg:59.82ms
step:954/2420 train_time:57069ms step_avg:59.82ms
step:955/2420 train_time:57128ms step_avg:59.82ms
step:956/2420 train_time:57190ms step_avg:59.82ms
step:957/2420 train_time:57249ms step_avg:59.82ms
step:958/2420 train_time:57311ms step_avg:59.82ms
step:959/2420 train_time:57371ms step_avg:59.82ms
step:960/2420 train_time:57432ms step_avg:59.83ms
step:961/2420 train_time:57491ms step_avg:59.82ms
step:962/2420 train_time:57553ms step_avg:59.83ms
step:963/2420 train_time:57612ms step_avg:59.83ms
step:964/2420 train_time:57673ms step_avg:59.83ms
step:965/2420 train_time:57733ms step_avg:59.83ms
step:966/2420 train_time:57794ms step_avg:59.83ms
step:967/2420 train_time:57854ms step_avg:59.83ms
step:968/2420 train_time:57915ms step_avg:59.83ms
step:969/2420 train_time:57974ms step_avg:59.83ms
step:970/2420 train_time:58035ms step_avg:59.83ms
step:971/2420 train_time:58095ms step_avg:59.83ms
step:972/2420 train_time:58156ms step_avg:59.83ms
step:973/2420 train_time:58216ms step_avg:59.83ms
step:974/2420 train_time:58277ms step_avg:59.83ms
step:975/2420 train_time:58337ms step_avg:59.83ms
step:976/2420 train_time:58398ms step_avg:59.83ms
step:977/2420 train_time:58458ms step_avg:59.83ms
step:978/2420 train_time:58520ms step_avg:59.84ms
step:979/2420 train_time:58579ms step_avg:59.84ms
step:980/2420 train_time:58641ms step_avg:59.84ms
step:981/2420 train_time:58700ms step_avg:59.84ms
step:982/2420 train_time:58762ms step_avg:59.84ms
step:983/2420 train_time:58822ms step_avg:59.84ms
step:984/2420 train_time:58883ms step_avg:59.84ms
step:985/2420 train_time:58942ms step_avg:59.84ms
step:986/2420 train_time:59004ms step_avg:59.84ms
step:987/2420 train_time:59063ms step_avg:59.84ms
step:988/2420 train_time:59124ms step_avg:59.84ms
step:989/2420 train_time:59183ms step_avg:59.84ms
step:990/2420 train_time:59244ms step_avg:59.84ms
step:991/2420 train_time:59304ms step_avg:59.84ms
step:992/2420 train_time:59365ms step_avg:59.84ms
step:993/2420 train_time:59425ms step_avg:59.84ms
step:994/2420 train_time:59485ms step_avg:59.84ms
step:995/2420 train_time:59544ms step_avg:59.84ms
step:996/2420 train_time:59605ms step_avg:59.84ms
step:997/2420 train_time:59665ms step_avg:59.84ms
step:998/2420 train_time:59726ms step_avg:59.85ms
step:999/2420 train_time:59785ms step_avg:59.84ms
step:1000/2420 train_time:59845ms step_avg:59.85ms
step:1000/2420 val_loss:3.5807 train_time:59909ms step_avg:59.91ms
step:1001/2420 train_time:59929ms step_avg:59.87ms
step:1002/2420 train_time:59967ms step_avg:59.85ms
step:1003/2420 train_time:60027ms step_avg:59.85ms
step:1004/2420 train_time:60090ms step_avg:59.85ms
step:1005/2420 train_time:60151ms step_avg:59.85ms
step:1006/2420 train_time:60213ms step_avg:59.85ms
step:1007/2420 train_time:60272ms step_avg:59.85ms
step:1008/2420 train_time:60332ms step_avg:59.85ms
step:1009/2420 train_time:60391ms step_avg:59.85ms
step:1010/2420 train_time:60452ms step_avg:59.85ms
step:1011/2420 train_time:60511ms step_avg:59.85ms
step:1012/2420 train_time:60572ms step_avg:59.85ms
step:1013/2420 train_time:60631ms step_avg:59.85ms
step:1014/2420 train_time:60691ms step_avg:59.85ms
step:1015/2420 train_time:60749ms step_avg:59.85ms
step:1016/2420 train_time:60815ms step_avg:59.86ms
step:1017/2420 train_time:60879ms step_avg:59.86ms
step:1018/2420 train_time:60942ms step_avg:59.86ms
step:1019/2420 train_time:61002ms step_avg:59.86ms
step:1020/2420 train_time:61064ms step_avg:59.87ms
step:1021/2420 train_time:61123ms step_avg:59.87ms
step:1022/2420 train_time:61184ms step_avg:59.87ms
step:1023/2420 train_time:61244ms step_avg:59.87ms
step:1024/2420 train_time:61304ms step_avg:59.87ms
step:1025/2420 train_time:61363ms step_avg:59.87ms
step:1026/2420 train_time:61424ms step_avg:59.87ms
step:1027/2420 train_time:61482ms step_avg:59.87ms
step:1028/2420 train_time:61542ms step_avg:59.87ms
step:1029/2420 train_time:61601ms step_avg:59.86ms
step:1030/2420 train_time:61661ms step_avg:59.87ms
step:1031/2420 train_time:61720ms step_avg:59.86ms
step:1032/2420 train_time:61782ms step_avg:59.87ms
step:1033/2420 train_time:61842ms step_avg:59.87ms
step:1034/2420 train_time:61904ms step_avg:59.87ms
step:1035/2420 train_time:61964ms step_avg:59.87ms
step:1036/2420 train_time:62026ms step_avg:59.87ms
step:1037/2420 train_time:62086ms step_avg:59.87ms
step:1038/2420 train_time:62147ms step_avg:59.87ms
step:1039/2420 train_time:62206ms step_avg:59.87ms
step:1040/2420 train_time:62268ms step_avg:59.87ms
step:1041/2420 train_time:62327ms step_avg:59.87ms
step:1042/2420 train_time:62387ms step_avg:59.87ms
step:1043/2420 train_time:62446ms step_avg:59.87ms
step:1044/2420 train_time:62507ms step_avg:59.87ms
step:1045/2420 train_time:62566ms step_avg:59.87ms
step:1046/2420 train_time:62626ms step_avg:59.87ms
step:1047/2420 train_time:62685ms step_avg:59.87ms
step:1048/2420 train_time:62747ms step_avg:59.87ms
step:1049/2420 train_time:62807ms step_avg:59.87ms
step:1050/2420 train_time:62869ms step_avg:59.88ms
step:1051/2420 train_time:62929ms step_avg:59.88ms
step:1052/2420 train_time:62991ms step_avg:59.88ms
step:1053/2420 train_time:63051ms step_avg:59.88ms
step:1054/2420 train_time:63112ms step_avg:59.88ms
step:1055/2420 train_time:63172ms step_avg:59.88ms
step:1056/2420 train_time:63233ms step_avg:59.88ms
step:1057/2420 train_time:63293ms step_avg:59.88ms
step:1058/2420 train_time:63354ms step_avg:59.88ms
step:1059/2420 train_time:63413ms step_avg:59.88ms
step:1060/2420 train_time:63474ms step_avg:59.88ms
step:1061/2420 train_time:63534ms step_avg:59.88ms
step:1062/2420 train_time:63595ms step_avg:59.88ms
step:1063/2420 train_time:63654ms step_avg:59.88ms
step:1064/2420 train_time:63716ms step_avg:59.88ms
step:1065/2420 train_time:63776ms step_avg:59.88ms
step:1066/2420 train_time:63837ms step_avg:59.88ms
step:1067/2420 train_time:63898ms step_avg:59.89ms
step:1068/2420 train_time:63960ms step_avg:59.89ms
step:1069/2420 train_time:64020ms step_avg:59.89ms
step:1070/2420 train_time:64081ms step_avg:59.89ms
step:1071/2420 train_time:64141ms step_avg:59.89ms
step:1072/2420 train_time:64202ms step_avg:59.89ms
step:1073/2420 train_time:64261ms step_avg:59.89ms
step:1074/2420 train_time:64322ms step_avg:59.89ms
step:1075/2420 train_time:64382ms step_avg:59.89ms
step:1076/2420 train_time:64442ms step_avg:59.89ms
step:1077/2420 train_time:64501ms step_avg:59.89ms
step:1078/2420 train_time:64562ms step_avg:59.89ms
step:1079/2420 train_time:64620ms step_avg:59.89ms
step:1080/2420 train_time:64681ms step_avg:59.89ms
step:1081/2420 train_time:64741ms step_avg:59.89ms
step:1082/2420 train_time:64802ms step_avg:59.89ms
step:1083/2420 train_time:64861ms step_avg:59.89ms
step:1084/2420 train_time:64922ms step_avg:59.89ms
step:1085/2420 train_time:64982ms step_avg:59.89ms
step:1086/2420 train_time:65043ms step_avg:59.89ms
step:1087/2420 train_time:65102ms step_avg:59.89ms
step:1088/2420 train_time:65163ms step_avg:59.89ms
step:1089/2420 train_time:65222ms step_avg:59.89ms
step:1090/2420 train_time:65283ms step_avg:59.89ms
step:1091/2420 train_time:65342ms step_avg:59.89ms
step:1092/2420 train_time:65403ms step_avg:59.89ms
step:1093/2420 train_time:65461ms step_avg:59.89ms
step:1094/2420 train_time:65523ms step_avg:59.89ms
step:1095/2420 train_time:65581ms step_avg:59.89ms
step:1096/2420 train_time:65643ms step_avg:59.89ms
step:1097/2420 train_time:65702ms step_avg:59.89ms
step:1098/2420 train_time:65763ms step_avg:59.89ms
step:1099/2420 train_time:65823ms step_avg:59.89ms
step:1100/2420 train_time:65884ms step_avg:59.89ms
step:1101/2420 train_time:65943ms step_avg:59.89ms
step:1102/2420 train_time:66005ms step_avg:59.90ms
step:1103/2420 train_time:66064ms step_avg:59.89ms
step:1104/2420 train_time:66125ms step_avg:59.90ms
step:1105/2420 train_time:66185ms step_avg:59.90ms
step:1106/2420 train_time:66246ms step_avg:59.90ms
step:1107/2420 train_time:66305ms step_avg:59.90ms
step:1108/2420 train_time:66366ms step_avg:59.90ms
step:1109/2420 train_time:66425ms step_avg:59.90ms
step:1110/2420 train_time:66486ms step_avg:59.90ms
step:1111/2420 train_time:66545ms step_avg:59.90ms
step:1112/2420 train_time:66607ms step_avg:59.90ms
step:1113/2420 train_time:66666ms step_avg:59.90ms
step:1114/2420 train_time:66727ms step_avg:59.90ms
step:1115/2420 train_time:66787ms step_avg:59.90ms
step:1116/2420 train_time:66848ms step_avg:59.90ms
step:1117/2420 train_time:66908ms step_avg:59.90ms
step:1118/2420 train_time:66969ms step_avg:59.90ms
step:1119/2420 train_time:67029ms step_avg:59.90ms
step:1120/2420 train_time:67090ms step_avg:59.90ms
step:1121/2420 train_time:67150ms step_avg:59.90ms
step:1122/2420 train_time:67211ms step_avg:59.90ms
step:1123/2420 train_time:67271ms step_avg:59.90ms
step:1124/2420 train_time:67332ms step_avg:59.90ms
step:1125/2420 train_time:67392ms step_avg:59.90ms
step:1126/2420 train_time:67453ms step_avg:59.91ms
step:1127/2420 train_time:67513ms step_avg:59.90ms
step:1128/2420 train_time:67574ms step_avg:59.91ms
step:1129/2420 train_time:67634ms step_avg:59.91ms
step:1130/2420 train_time:67696ms step_avg:59.91ms
step:1131/2420 train_time:67756ms step_avg:59.91ms
step:1132/2420 train_time:67817ms step_avg:59.91ms
step:1133/2420 train_time:67877ms step_avg:59.91ms
step:1134/2420 train_time:67939ms step_avg:59.91ms
step:1135/2420 train_time:67998ms step_avg:59.91ms
step:1136/2420 train_time:68060ms step_avg:59.91ms
step:1137/2420 train_time:68119ms step_avg:59.91ms
step:1138/2420 train_time:68181ms step_avg:59.91ms
step:1139/2420 train_time:68240ms step_avg:59.91ms
step:1140/2420 train_time:68301ms step_avg:59.91ms
step:1141/2420 train_time:68361ms step_avg:59.91ms
step:1142/2420 train_time:68421ms step_avg:59.91ms
step:1143/2420 train_time:68481ms step_avg:59.91ms
step:1144/2420 train_time:68542ms step_avg:59.91ms
step:1145/2420 train_time:68601ms step_avg:59.91ms
step:1146/2420 train_time:68662ms step_avg:59.91ms
step:1147/2420 train_time:68721ms step_avg:59.91ms
step:1148/2420 train_time:68782ms step_avg:59.91ms
step:1149/2420 train_time:68841ms step_avg:59.91ms
step:1150/2420 train_time:68902ms step_avg:59.91ms
step:1151/2420 train_time:68962ms step_avg:59.91ms
step:1152/2420 train_time:69023ms step_avg:59.92ms
step:1153/2420 train_time:69082ms step_avg:59.91ms
step:1154/2420 train_time:69143ms step_avg:59.92ms
step:1155/2420 train_time:69202ms step_avg:59.92ms
step:1156/2420 train_time:69264ms step_avg:59.92ms
step:1157/2420 train_time:69322ms step_avg:59.92ms
step:1158/2420 train_time:69383ms step_avg:59.92ms
step:1159/2420 train_time:69443ms step_avg:59.92ms
step:1160/2420 train_time:69504ms step_avg:59.92ms
step:1161/2420 train_time:69562ms step_avg:59.92ms
step:1162/2420 train_time:69623ms step_avg:59.92ms
step:1163/2420 train_time:69683ms step_avg:59.92ms
step:1164/2420 train_time:69743ms step_avg:59.92ms
step:1165/2420 train_time:69803ms step_avg:59.92ms
step:1166/2420 train_time:69864ms step_avg:59.92ms
step:1167/2420 train_time:69923ms step_avg:59.92ms
step:1168/2420 train_time:69984ms step_avg:59.92ms
step:1169/2420 train_time:70044ms step_avg:59.92ms
step:1170/2420 train_time:70106ms step_avg:59.92ms
step:1171/2420 train_time:70165ms step_avg:59.92ms
step:1172/2420 train_time:70226ms step_avg:59.92ms
step:1173/2420 train_time:70285ms step_avg:59.92ms
step:1174/2420 train_time:70346ms step_avg:59.92ms
step:1175/2420 train_time:70406ms step_avg:59.92ms
step:1176/2420 train_time:70467ms step_avg:59.92ms
step:1177/2420 train_time:70526ms step_avg:59.92ms
step:1178/2420 train_time:70587ms step_avg:59.92ms
step:1179/2420 train_time:70646ms step_avg:59.92ms
step:1180/2420 train_time:70707ms step_avg:59.92ms
step:1181/2420 train_time:70767ms step_avg:59.92ms
step:1182/2420 train_time:70828ms step_avg:59.92ms
step:1183/2420 train_time:70887ms step_avg:59.92ms
step:1184/2420 train_time:70949ms step_avg:59.92ms
step:1185/2420 train_time:71008ms step_avg:59.92ms
step:1186/2420 train_time:71070ms step_avg:59.92ms
step:1187/2420 train_time:71129ms step_avg:59.92ms
step:1188/2420 train_time:71190ms step_avg:59.92ms
step:1189/2420 train_time:71249ms step_avg:59.92ms
step:1190/2420 train_time:71311ms step_avg:59.93ms
step:1191/2420 train_time:71370ms step_avg:59.92ms
step:1192/2420 train_time:71431ms step_avg:59.93ms
step:1193/2420 train_time:71490ms step_avg:59.92ms
step:1194/2420 train_time:71551ms step_avg:59.93ms
step:1195/2420 train_time:71611ms step_avg:59.93ms
step:1196/2420 train_time:71672ms step_avg:59.93ms
step:1197/2420 train_time:71732ms step_avg:59.93ms
step:1198/2420 train_time:71794ms step_avg:59.93ms
step:1199/2420 train_time:71853ms step_avg:59.93ms
step:1200/2420 train_time:71915ms step_avg:59.93ms
step:1201/2420 train_time:71974ms step_avg:59.93ms
step:1202/2420 train_time:72036ms step_avg:59.93ms
step:1203/2420 train_time:72096ms step_avg:59.93ms
step:1204/2420 train_time:72158ms step_avg:59.93ms
step:1205/2420 train_time:72218ms step_avg:59.93ms
step:1206/2420 train_time:72279ms step_avg:59.93ms
step:1207/2420 train_time:72340ms step_avg:59.93ms
step:1208/2420 train_time:72401ms step_avg:59.93ms
step:1209/2420 train_time:72460ms step_avg:59.93ms
step:1210/2420 train_time:72521ms step_avg:59.93ms
step:1211/2420 train_time:72581ms step_avg:59.93ms
step:1212/2420 train_time:72642ms step_avg:59.94ms
step:1213/2420 train_time:72701ms step_avg:59.93ms
step:1214/2420 train_time:72761ms step_avg:59.94ms
step:1215/2420 train_time:72821ms step_avg:59.93ms
step:1216/2420 train_time:72882ms step_avg:59.94ms
step:1217/2420 train_time:72941ms step_avg:59.94ms
step:1218/2420 train_time:73002ms step_avg:59.94ms
step:1219/2420 train_time:73061ms step_avg:59.94ms
step:1220/2420 train_time:73122ms step_avg:59.94ms
step:1221/2420 train_time:73181ms step_avg:59.94ms
step:1222/2420 train_time:73243ms step_avg:59.94ms
step:1223/2420 train_time:73302ms step_avg:59.94ms
step:1224/2420 train_time:73363ms step_avg:59.94ms
step:1225/2420 train_time:73422ms step_avg:59.94ms
step:1226/2420 train_time:73483ms step_avg:59.94ms
step:1227/2420 train_time:73542ms step_avg:59.94ms
step:1228/2420 train_time:73603ms step_avg:59.94ms
step:1229/2420 train_time:73663ms step_avg:59.94ms
step:1230/2420 train_time:73724ms step_avg:59.94ms
step:1231/2420 train_time:73783ms step_avg:59.94ms
step:1232/2420 train_time:73844ms step_avg:59.94ms
step:1233/2420 train_time:73903ms step_avg:59.94ms
step:1234/2420 train_time:73964ms step_avg:59.94ms
step:1235/2420 train_time:74023ms step_avg:59.94ms
step:1236/2420 train_time:74084ms step_avg:59.94ms
step:1237/2420 train_time:74143ms step_avg:59.94ms
step:1238/2420 train_time:74205ms step_avg:59.94ms
step:1239/2420 train_time:74264ms step_avg:59.94ms
step:1240/2420 train_time:74325ms step_avg:59.94ms
step:1241/2420 train_time:74384ms step_avg:59.94ms
step:1242/2420 train_time:74445ms step_avg:59.94ms
step:1243/2420 train_time:74504ms step_avg:59.94ms
step:1244/2420 train_time:74565ms step_avg:59.94ms
step:1245/2420 train_time:74625ms step_avg:59.94ms
step:1246/2420 train_time:74686ms step_avg:59.94ms
step:1247/2420 train_time:74745ms step_avg:59.94ms
step:1248/2420 train_time:74806ms step_avg:59.94ms
step:1249/2420 train_time:74866ms step_avg:59.94ms
step:1250/2420 train_time:74927ms step_avg:59.94ms
step:1250/2420 val_loss:3.5237 train_time:74990ms step_avg:59.99ms
step:1251/2420 train_time:75010ms step_avg:59.96ms
step:1252/2420 train_time:75052ms step_avg:59.95ms
step:1253/2420 train_time:75116ms step_avg:59.95ms
step:1254/2420 train_time:75180ms step_avg:59.95ms
step:1255/2420 train_time:75241ms step_avg:59.95ms
step:1256/2420 train_time:75302ms step_avg:59.95ms
step:1257/2420 train_time:75361ms step_avg:59.95ms
step:1258/2420 train_time:75421ms step_avg:59.95ms
step:1259/2420 train_time:75480ms step_avg:59.95ms
step:1260/2420 train_time:75540ms step_avg:59.95ms
step:1261/2420 train_time:75598ms step_avg:59.95ms
step:1262/2420 train_time:75659ms step_avg:59.95ms
step:1263/2420 train_time:75717ms step_avg:59.95ms
step:1264/2420 train_time:75778ms step_avg:59.95ms
step:1265/2420 train_time:75836ms step_avg:59.95ms
step:1266/2420 train_time:75897ms step_avg:59.95ms
step:1267/2420 train_time:75956ms step_avg:59.95ms
step:1268/2420 train_time:76019ms step_avg:59.95ms
step:1269/2420 train_time:76080ms step_avg:59.95ms
step:1270/2420 train_time:76142ms step_avg:59.95ms
step:1271/2420 train_time:76201ms step_avg:59.95ms
step:1272/2420 train_time:76263ms step_avg:59.95ms
step:1273/2420 train_time:76322ms step_avg:59.95ms
step:1274/2420 train_time:76383ms step_avg:59.96ms
step:1275/2420 train_time:76442ms step_avg:59.95ms
step:1276/2420 train_time:76503ms step_avg:59.96ms
step:1277/2420 train_time:76561ms step_avg:59.95ms
step:1278/2420 train_time:76622ms step_avg:59.95ms
step:1279/2420 train_time:76681ms step_avg:59.95ms
step:1280/2420 train_time:76742ms step_avg:59.95ms
step:1281/2420 train_time:76801ms step_avg:59.95ms
step:1282/2420 train_time:76861ms step_avg:59.95ms
step:1283/2420 train_time:76921ms step_avg:59.95ms
step:1284/2420 train_time:76983ms step_avg:59.96ms
step:1285/2420 train_time:77043ms step_avg:59.96ms
step:1286/2420 train_time:77104ms step_avg:59.96ms
step:1287/2420 train_time:77164ms step_avg:59.96ms
step:1288/2420 train_time:77225ms step_avg:59.96ms
step:1289/2420 train_time:77284ms step_avg:59.96ms
step:1290/2420 train_time:77346ms step_avg:59.96ms
step:1291/2420 train_time:77405ms step_avg:59.96ms
step:1292/2420 train_time:77466ms step_avg:59.96ms
step:1293/2420 train_time:77526ms step_avg:59.96ms
step:1294/2420 train_time:77586ms step_avg:59.96ms
step:1295/2420 train_time:77646ms step_avg:59.96ms
step:1296/2420 train_time:77707ms step_avg:59.96ms
step:1297/2420 train_time:77765ms step_avg:59.96ms
step:1298/2420 train_time:77826ms step_avg:59.96ms
step:1299/2420 train_time:77885ms step_avg:59.96ms
step:1300/2420 train_time:77946ms step_avg:59.96ms
step:1301/2420 train_time:78006ms step_avg:59.96ms
step:1302/2420 train_time:78068ms step_avg:59.96ms
step:1303/2420 train_time:78128ms step_avg:59.96ms
step:1304/2420 train_time:78189ms step_avg:59.96ms
step:1305/2420 train_time:78250ms step_avg:59.96ms
step:1306/2420 train_time:78311ms step_avg:59.96ms
step:1307/2420 train_time:78370ms step_avg:59.96ms
step:1308/2420 train_time:78432ms step_avg:59.96ms
step:1309/2420 train_time:78492ms step_avg:59.96ms
step:1310/2420 train_time:78553ms step_avg:59.96ms
step:1311/2420 train_time:78613ms step_avg:59.96ms
step:1312/2420 train_time:78675ms step_avg:59.97ms
step:1313/2420 train_time:78734ms step_avg:59.96ms
step:1314/2420 train_time:78795ms step_avg:59.97ms
step:1315/2420 train_time:78855ms step_avg:59.97ms
step:1316/2420 train_time:78917ms step_avg:59.97ms
step:1317/2420 train_time:78977ms step_avg:59.97ms
step:1318/2420 train_time:79038ms step_avg:59.97ms
step:1319/2420 train_time:79097ms step_avg:59.97ms
step:1320/2420 train_time:79158ms step_avg:59.97ms
step:1321/2420 train_time:79218ms step_avg:59.97ms
step:1322/2420 train_time:79279ms step_avg:59.97ms
step:1323/2420 train_time:79338ms step_avg:59.97ms
step:1324/2420 train_time:79399ms step_avg:59.97ms
step:1325/2420 train_time:79458ms step_avg:59.97ms
step:1326/2420 train_time:79519ms step_avg:59.97ms
step:1327/2420 train_time:79579ms step_avg:59.97ms
step:1328/2420 train_time:79640ms step_avg:59.97ms
step:1329/2420 train_time:79698ms step_avg:59.97ms
step:1330/2420 train_time:79759ms step_avg:59.97ms
step:1331/2420 train_time:79818ms step_avg:59.97ms
step:1332/2420 train_time:79879ms step_avg:59.97ms
step:1333/2420 train_time:79938ms step_avg:59.97ms
step:1334/2420 train_time:80000ms step_avg:59.97ms
step:1335/2420 train_time:80059ms step_avg:59.97ms
step:1336/2420 train_time:80119ms step_avg:59.97ms
step:1337/2420 train_time:80179ms step_avg:59.97ms
step:1338/2420 train_time:80239ms step_avg:59.97ms
step:1339/2420 train_time:80299ms step_avg:59.97ms
step:1340/2420 train_time:80360ms step_avg:59.97ms
step:1341/2420 train_time:80419ms step_avg:59.97ms
step:1342/2420 train_time:80481ms step_avg:59.97ms
step:1343/2420 train_time:80540ms step_avg:59.97ms
step:1344/2420 train_time:80602ms step_avg:59.97ms
step:1345/2420 train_time:80661ms step_avg:59.97ms
step:1346/2420 train_time:80722ms step_avg:59.97ms
step:1347/2420 train_time:80781ms step_avg:59.97ms
step:1348/2420 train_time:80841ms step_avg:59.97ms
step:1349/2420 train_time:80900ms step_avg:59.97ms
step:1350/2420 train_time:80961ms step_avg:59.97ms
step:1351/2420 train_time:81020ms step_avg:59.97ms
step:1352/2420 train_time:81082ms step_avg:59.97ms
step:1353/2420 train_time:81141ms step_avg:59.97ms
step:1354/2420 train_time:81202ms step_avg:59.97ms
step:1355/2420 train_time:81262ms step_avg:59.97ms
step:1356/2420 train_time:81323ms step_avg:59.97ms
step:1357/2420 train_time:81382ms step_avg:59.97ms
step:1358/2420 train_time:81444ms step_avg:59.97ms
step:1359/2420 train_time:81503ms step_avg:59.97ms
step:1360/2420 train_time:81564ms step_avg:59.97ms
step:1361/2420 train_time:81623ms step_avg:59.97ms
step:1362/2420 train_time:81684ms step_avg:59.97ms
step:1363/2420 train_time:81743ms step_avg:59.97ms
step:1364/2420 train_time:81804ms step_avg:59.97ms
step:1365/2420 train_time:81862ms step_avg:59.97ms
step:1366/2420 train_time:81924ms step_avg:59.97ms
step:1367/2420 train_time:81983ms step_avg:59.97ms
step:1368/2420 train_time:82044ms step_avg:59.97ms
step:1369/2420 train_time:82103ms step_avg:59.97ms
step:1370/2420 train_time:82165ms step_avg:59.97ms
step:1371/2420 train_time:82224ms step_avg:59.97ms
step:1372/2420 train_time:82286ms step_avg:59.97ms
step:1373/2420 train_time:82345ms step_avg:59.97ms
step:1374/2420 train_time:82406ms step_avg:59.98ms
step:1375/2420 train_time:82465ms step_avg:59.97ms
step:1376/2420 train_time:82527ms step_avg:59.98ms
step:1377/2420 train_time:82586ms step_avg:59.98ms
step:1378/2420 train_time:82647ms step_avg:59.98ms
step:1379/2420 train_time:82706ms step_avg:59.98ms
step:1380/2420 train_time:82767ms step_avg:59.98ms
step:1381/2420 train_time:82827ms step_avg:59.98ms
step:1382/2420 train_time:82888ms step_avg:59.98ms
step:1383/2420 train_time:82947ms step_avg:59.98ms
step:1384/2420 train_time:83009ms step_avg:59.98ms
step:1385/2420 train_time:83068ms step_avg:59.98ms
step:1386/2420 train_time:83130ms step_avg:59.98ms
step:1387/2420 train_time:83189ms step_avg:59.98ms
step:1388/2420 train_time:83250ms step_avg:59.98ms
step:1389/2420 train_time:83309ms step_avg:59.98ms
step:1390/2420 train_time:83371ms step_avg:59.98ms
step:1391/2420 train_time:83431ms step_avg:59.98ms
step:1392/2420 train_time:83492ms step_avg:59.98ms
step:1393/2420 train_time:83552ms step_avg:59.98ms
step:1394/2420 train_time:83614ms step_avg:59.98ms
step:1395/2420 train_time:83673ms step_avg:59.98ms
step:1396/2420 train_time:83735ms step_avg:59.98ms
step:1397/2420 train_time:83794ms step_avg:59.98ms
step:1398/2420 train_time:83855ms step_avg:59.98ms
step:1399/2420 train_time:83915ms step_avg:59.98ms
step:1400/2420 train_time:83977ms step_avg:59.98ms
step:1401/2420 train_time:84036ms step_avg:59.98ms
step:1402/2420 train_time:84097ms step_avg:59.98ms
step:1403/2420 train_time:84156ms step_avg:59.98ms
step:1404/2420 train_time:84218ms step_avg:59.98ms
step:1405/2420 train_time:84277ms step_avg:59.98ms
step:1406/2420 train_time:84338ms step_avg:59.98ms
step:1407/2420 train_time:84398ms step_avg:59.98ms
step:1408/2420 train_time:84459ms step_avg:59.99ms
step:1409/2420 train_time:84518ms step_avg:59.98ms
step:1410/2420 train_time:84580ms step_avg:59.99ms
step:1411/2420 train_time:84639ms step_avg:59.98ms
step:1412/2420 train_time:84700ms step_avg:59.99ms
step:1413/2420 train_time:84759ms step_avg:59.98ms
step:1414/2420 train_time:84820ms step_avg:59.99ms
step:1415/2420 train_time:84879ms step_avg:59.99ms
step:1416/2420 train_time:84940ms step_avg:59.99ms
step:1417/2420 train_time:85000ms step_avg:59.99ms
step:1418/2420 train_time:85060ms step_avg:59.99ms
step:1419/2420 train_time:85119ms step_avg:59.99ms
step:1420/2420 train_time:85180ms step_avg:59.99ms
step:1421/2420 train_time:85239ms step_avg:59.99ms
step:1422/2420 train_time:85300ms step_avg:59.99ms
step:1423/2420 train_time:85359ms step_avg:59.99ms
step:1424/2420 train_time:85420ms step_avg:59.99ms
step:1425/2420 train_time:85479ms step_avg:59.99ms
step:1426/2420 train_time:85540ms step_avg:59.99ms
step:1427/2420 train_time:85599ms step_avg:59.99ms
step:1428/2420 train_time:85660ms step_avg:59.99ms
step:1429/2420 train_time:85719ms step_avg:59.99ms
step:1430/2420 train_time:85780ms step_avg:59.99ms
step:1431/2420 train_time:85839ms step_avg:59.99ms
step:1432/2420 train_time:85900ms step_avg:59.99ms
step:1433/2420 train_time:85959ms step_avg:59.99ms
step:1434/2420 train_time:86020ms step_avg:59.99ms
step:1435/2420 train_time:86080ms step_avg:59.99ms
step:1436/2420 train_time:86141ms step_avg:59.99ms
step:1437/2420 train_time:86199ms step_avg:59.99ms
step:1438/2420 train_time:86260ms step_avg:59.99ms
step:1439/2420 train_time:86319ms step_avg:59.99ms
step:1440/2420 train_time:86380ms step_avg:59.99ms
step:1441/2420 train_time:86439ms step_avg:59.99ms
step:1442/2420 train_time:86500ms step_avg:59.99ms
step:1443/2420 train_time:86559ms step_avg:59.99ms
step:1444/2420 train_time:86620ms step_avg:59.99ms
step:1445/2420 train_time:86680ms step_avg:59.99ms
step:1446/2420 train_time:86740ms step_avg:59.99ms
step:1447/2420 train_time:86800ms step_avg:59.99ms
step:1448/2420 train_time:86861ms step_avg:59.99ms
step:1449/2420 train_time:86920ms step_avg:59.99ms
step:1450/2420 train_time:86981ms step_avg:59.99ms
step:1451/2420 train_time:87041ms step_avg:59.99ms
step:1452/2420 train_time:87101ms step_avg:59.99ms
step:1453/2420 train_time:87160ms step_avg:59.99ms
step:1454/2420 train_time:87221ms step_avg:59.99ms
step:1455/2420 train_time:87280ms step_avg:59.99ms
step:1456/2420 train_time:87341ms step_avg:59.99ms
step:1457/2420 train_time:87401ms step_avg:59.99ms
step:1458/2420 train_time:87463ms step_avg:59.99ms
step:1459/2420 train_time:87522ms step_avg:59.99ms
step:1460/2420 train_time:87583ms step_avg:59.99ms
step:1461/2420 train_time:87642ms step_avg:59.99ms
step:1462/2420 train_time:87703ms step_avg:59.99ms
step:1463/2420 train_time:87762ms step_avg:59.99ms
step:1464/2420 train_time:87823ms step_avg:59.99ms
step:1465/2420 train_time:87882ms step_avg:59.99ms
step:1466/2420 train_time:87943ms step_avg:59.99ms
step:1467/2420 train_time:88002ms step_avg:59.99ms
step:1468/2420 train_time:88064ms step_avg:59.99ms
step:1469/2420 train_time:88123ms step_avg:59.99ms
step:1470/2420 train_time:88184ms step_avg:59.99ms
step:1471/2420 train_time:88243ms step_avg:59.99ms
step:1472/2420 train_time:88304ms step_avg:59.99ms
step:1473/2420 train_time:88363ms step_avg:59.99ms
step:1474/2420 train_time:88424ms step_avg:59.99ms
step:1475/2420 train_time:88484ms step_avg:59.99ms
step:1476/2420 train_time:88545ms step_avg:59.99ms
step:1477/2420 train_time:88604ms step_avg:59.99ms
step:1478/2420 train_time:88665ms step_avg:59.99ms
step:1479/2420 train_time:88724ms step_avg:59.99ms
step:1480/2420 train_time:88786ms step_avg:59.99ms
step:1481/2420 train_time:88845ms step_avg:59.99ms
step:1482/2420 train_time:88906ms step_avg:59.99ms
step:1483/2420 train_time:88966ms step_avg:59.99ms
step:1484/2420 train_time:89027ms step_avg:59.99ms
step:1485/2420 train_time:89086ms step_avg:59.99ms
step:1486/2420 train_time:89148ms step_avg:59.99ms
step:1487/2420 train_time:89207ms step_avg:59.99ms
step:1488/2420 train_time:89269ms step_avg:59.99ms
step:1489/2420 train_time:89328ms step_avg:59.99ms
step:1490/2420 train_time:89390ms step_avg:59.99ms
step:1491/2420 train_time:89450ms step_avg:59.99ms
step:1492/2420 train_time:89511ms step_avg:59.99ms
step:1493/2420 train_time:89571ms step_avg:59.99ms
step:1494/2420 train_time:89633ms step_avg:60.00ms
step:1495/2420 train_time:89693ms step_avg:60.00ms
step:1496/2420 train_time:89755ms step_avg:60.00ms
step:1497/2420 train_time:89814ms step_avg:60.00ms
step:1498/2420 train_time:89876ms step_avg:60.00ms
step:1499/2420 train_time:89936ms step_avg:60.00ms
step:1500/2420 train_time:89998ms step_avg:60.00ms
step:1500/2420 val_loss:3.4748 train_time:90061ms step_avg:60.04ms
step:1501/2420 train_time:90081ms step_avg:60.01ms
step:1502/2420 train_time:90124ms step_avg:60.00ms
step:1503/2420 train_time:90187ms step_avg:60.00ms
step:1504/2420 train_time:90250ms step_avg:60.01ms
step:1505/2420 train_time:90311ms step_avg:60.01ms
step:1506/2420 train_time:90374ms step_avg:60.01ms
step:1507/2420 train_time:90434ms step_avg:60.01ms
step:1508/2420 train_time:90494ms step_avg:60.01ms
step:1509/2420 train_time:90553ms step_avg:60.01ms
step:1510/2420 train_time:90614ms step_avg:60.01ms
step:1511/2420 train_time:90673ms step_avg:60.01ms
step:1512/2420 train_time:90734ms step_avg:60.01ms
step:1513/2420 train_time:90793ms step_avg:60.01ms
step:1514/2420 train_time:90855ms step_avg:60.01ms
step:1515/2420 train_time:90913ms step_avg:60.01ms
step:1516/2420 train_time:90974ms step_avg:60.01ms
step:1517/2420 train_time:91034ms step_avg:60.01ms
step:1518/2420 train_time:91097ms step_avg:60.01ms
step:1519/2420 train_time:91158ms step_avg:60.01ms
step:1520/2420 train_time:91221ms step_avg:60.01ms
step:1521/2420 train_time:91281ms step_avg:60.01ms
step:1522/2420 train_time:91342ms step_avg:60.01ms
step:1523/2420 train_time:91401ms step_avg:60.01ms
step:1524/2420 train_time:91463ms step_avg:60.01ms
step:1525/2420 train_time:91522ms step_avg:60.01ms
step:1526/2420 train_time:91583ms step_avg:60.01ms
step:1527/2420 train_time:91642ms step_avg:60.01ms
step:1528/2420 train_time:91703ms step_avg:60.02ms
step:1529/2420 train_time:91762ms step_avg:60.01ms
step:1530/2420 train_time:91824ms step_avg:60.02ms
step:1531/2420 train_time:91883ms step_avg:60.02ms
step:1532/2420 train_time:91944ms step_avg:60.02ms
step:1533/2420 train_time:92004ms step_avg:60.02ms
step:1534/2420 train_time:92065ms step_avg:60.02ms
step:1535/2420 train_time:92125ms step_avg:60.02ms
step:1536/2420 train_time:92187ms step_avg:60.02ms
step:1537/2420 train_time:92247ms step_avg:60.02ms
step:1538/2420 train_time:92309ms step_avg:60.02ms
step:1539/2420 train_time:92368ms step_avg:60.02ms
step:1540/2420 train_time:92430ms step_avg:60.02ms
step:1541/2420 train_time:92489ms step_avg:60.02ms
step:1542/2420 train_time:92551ms step_avg:60.02ms
step:1543/2420 train_time:92610ms step_avg:60.02ms
step:1544/2420 train_time:92671ms step_avg:60.02ms
step:1545/2420 train_time:92731ms step_avg:60.02ms
step:1546/2420 train_time:92792ms step_avg:60.02ms
step:1547/2420 train_time:92852ms step_avg:60.02ms
step:1548/2420 train_time:92913ms step_avg:60.02ms
step:1549/2420 train_time:92973ms step_avg:60.02ms
step:1550/2420 train_time:93035ms step_avg:60.02ms
step:1551/2420 train_time:93094ms step_avg:60.02ms
step:1552/2420 train_time:93156ms step_avg:60.02ms
step:1553/2420 train_time:93216ms step_avg:60.02ms
step:1554/2420 train_time:93279ms step_avg:60.02ms
step:1555/2420 train_time:93339ms step_avg:60.02ms
step:1556/2420 train_time:93400ms step_avg:60.03ms
step:1557/2420 train_time:93459ms step_avg:60.02ms
step:1558/2420 train_time:93520ms step_avg:60.03ms
step:1559/2420 train_time:93579ms step_avg:60.03ms
step:1560/2420 train_time:93640ms step_avg:60.03ms
step:1561/2420 train_time:93700ms step_avg:60.03ms
step:1562/2420 train_time:93761ms step_avg:60.03ms
step:1563/2420 train_time:93820ms step_avg:60.03ms
step:1564/2420 train_time:93881ms step_avg:60.03ms
step:1565/2420 train_time:93940ms step_avg:60.03ms
step:1566/2420 train_time:94000ms step_avg:60.03ms
step:1567/2420 train_time:94059ms step_avg:60.02ms
step:1568/2420 train_time:94120ms step_avg:60.03ms
step:1569/2420 train_time:94180ms step_avg:60.03ms
step:1570/2420 train_time:94242ms step_avg:60.03ms
step:1571/2420 train_time:94301ms step_avg:60.03ms
step:1572/2420 train_time:94362ms step_avg:60.03ms
step:1573/2420 train_time:94422ms step_avg:60.03ms
step:1574/2420 train_time:94484ms step_avg:60.03ms
step:1575/2420 train_time:94543ms step_avg:60.03ms
step:1576/2420 train_time:94604ms step_avg:60.03ms
step:1577/2420 train_time:94663ms step_avg:60.03ms
step:1578/2420 train_time:94725ms step_avg:60.03ms
step:1579/2420 train_time:94784ms step_avg:60.03ms
step:1580/2420 train_time:94845ms step_avg:60.03ms
step:1581/2420 train_time:94904ms step_avg:60.03ms
step:1582/2420 train_time:94965ms step_avg:60.03ms
step:1583/2420 train_time:95025ms step_avg:60.03ms
step:1584/2420 train_time:95086ms step_avg:60.03ms
step:1585/2420 train_time:95146ms step_avg:60.03ms
step:1586/2420 train_time:95208ms step_avg:60.03ms
step:1587/2420 train_time:95268ms step_avg:60.03ms
step:1588/2420 train_time:95329ms step_avg:60.03ms
step:1589/2420 train_time:95389ms step_avg:60.03ms
step:1590/2420 train_time:95451ms step_avg:60.03ms
step:1591/2420 train_time:95511ms step_avg:60.03ms
step:1592/2420 train_time:95574ms step_avg:60.03ms
step:1593/2420 train_time:95634ms step_avg:60.03ms
step:1594/2420 train_time:95696ms step_avg:60.04ms
step:1595/2420 train_time:95756ms step_avg:60.04ms
step:1596/2420 train_time:95818ms step_avg:60.04ms
step:1597/2420 train_time:95878ms step_avg:60.04ms
step:1598/2420 train_time:95939ms step_avg:60.04ms
step:1599/2420 train_time:95999ms step_avg:60.04ms
step:1600/2420 train_time:96060ms step_avg:60.04ms
step:1601/2420 train_time:96120ms step_avg:60.04ms
step:1602/2420 train_time:96181ms step_avg:60.04ms
step:1603/2420 train_time:96241ms step_avg:60.04ms
step:1604/2420 train_time:96303ms step_avg:60.04ms
step:1605/2420 train_time:96363ms step_avg:60.04ms
step:1606/2420 train_time:96424ms step_avg:60.04ms
step:1607/2420 train_time:96485ms step_avg:60.04ms
step:1608/2420 train_time:96547ms step_avg:60.04ms
step:1609/2420 train_time:96606ms step_avg:60.04ms
step:1610/2420 train_time:96668ms step_avg:60.04ms
step:1611/2420 train_time:96728ms step_avg:60.04ms
step:1612/2420 train_time:96790ms step_avg:60.04ms
step:1613/2420 train_time:96851ms step_avg:60.04ms
step:1614/2420 train_time:96913ms step_avg:60.05ms
step:1615/2420 train_time:96974ms step_avg:60.05ms
step:1616/2420 train_time:97036ms step_avg:60.05ms
step:1617/2420 train_time:97096ms step_avg:60.05ms
step:1618/2420 train_time:97158ms step_avg:60.05ms
step:1619/2420 train_time:97219ms step_avg:60.05ms
step:1620/2420 train_time:97280ms step_avg:60.05ms
step:1621/2420 train_time:97340ms step_avg:60.05ms
step:1622/2420 train_time:97402ms step_avg:60.05ms
step:1623/2420 train_time:97461ms step_avg:60.05ms
step:1624/2420 train_time:97523ms step_avg:60.05ms
step:1625/2420 train_time:97583ms step_avg:60.05ms
step:1626/2420 train_time:97645ms step_avg:60.05ms
step:1627/2420 train_time:97705ms step_avg:60.05ms
step:1628/2420 train_time:97767ms step_avg:60.05ms
step:1629/2420 train_time:97828ms step_avg:60.05ms
step:1630/2420 train_time:97890ms step_avg:60.06ms
step:1631/2420 train_time:97950ms step_avg:60.06ms
step:1632/2420 train_time:98013ms step_avg:60.06ms
step:1633/2420 train_time:98072ms step_avg:60.06ms
step:1634/2420 train_time:98134ms step_avg:60.06ms
step:1635/2420 train_time:98194ms step_avg:60.06ms
step:1636/2420 train_time:98257ms step_avg:60.06ms
step:1637/2420 train_time:98317ms step_avg:60.06ms
step:1638/2420 train_time:98378ms step_avg:60.06ms
step:1639/2420 train_time:98438ms step_avg:60.06ms
step:1640/2420 train_time:98500ms step_avg:60.06ms
step:1641/2420 train_time:98560ms step_avg:60.06ms
step:1642/2420 train_time:98622ms step_avg:60.06ms
step:1643/2420 train_time:98681ms step_avg:60.06ms
step:1644/2420 train_time:98743ms step_avg:60.06ms
step:1645/2420 train_time:98802ms step_avg:60.06ms
step:1646/2420 train_time:98864ms step_avg:60.06ms
step:1647/2420 train_time:98924ms step_avg:60.06ms
step:1648/2420 train_time:98986ms step_avg:60.06ms
step:1649/2420 train_time:99046ms step_avg:60.06ms
step:1650/2420 train_time:99108ms step_avg:60.07ms
step:1651/2420 train_time:99169ms step_avg:60.07ms
step:1652/2420 train_time:99231ms step_avg:60.07ms
step:1653/2420 train_time:99290ms step_avg:60.07ms
step:1654/2420 train_time:99353ms step_avg:60.07ms
step:1655/2420 train_time:99414ms step_avg:60.07ms
step:1656/2420 train_time:99476ms step_avg:60.07ms
step:1657/2420 train_time:99536ms step_avg:60.07ms
step:1658/2420 train_time:99598ms step_avg:60.07ms
step:1659/2420 train_time:99658ms step_avg:60.07ms
step:1660/2420 train_time:99720ms step_avg:60.07ms
step:1661/2420 train_time:99779ms step_avg:60.07ms
step:1662/2420 train_time:99841ms step_avg:60.07ms
step:1663/2420 train_time:99900ms step_avg:60.07ms
step:1664/2420 train_time:99962ms step_avg:60.07ms
step:1665/2420 train_time:100021ms step_avg:60.07ms
step:1666/2420 train_time:100082ms step_avg:60.07ms
step:1667/2420 train_time:100143ms step_avg:60.07ms
step:1668/2420 train_time:100205ms step_avg:60.07ms
step:1669/2420 train_time:100265ms step_avg:60.07ms
step:1670/2420 train_time:100327ms step_avg:60.08ms
step:1671/2420 train_time:100387ms step_avg:60.08ms
step:1672/2420 train_time:100449ms step_avg:60.08ms
step:1673/2420 train_time:100510ms step_avg:60.08ms
step:1674/2420 train_time:100572ms step_avg:60.08ms
step:1675/2420 train_time:100632ms step_avg:60.08ms
step:1676/2420 train_time:100694ms step_avg:60.08ms
step:1677/2420 train_time:100754ms step_avg:60.08ms
step:1678/2420 train_time:100817ms step_avg:60.08ms
step:1679/2420 train_time:100877ms step_avg:60.08ms
step:1680/2420 train_time:100939ms step_avg:60.08ms
step:1681/2420 train_time:100999ms step_avg:60.08ms
step:1682/2420 train_time:101060ms step_avg:60.08ms
step:1683/2420 train_time:101120ms step_avg:60.08ms
step:1684/2420 train_time:101181ms step_avg:60.08ms
step:1685/2420 train_time:101241ms step_avg:60.08ms
step:1686/2420 train_time:101303ms step_avg:60.08ms
step:1687/2420 train_time:101363ms step_avg:60.08ms
step:1688/2420 train_time:101425ms step_avg:60.09ms
step:1689/2420 train_time:101485ms step_avg:60.09ms
step:1690/2420 train_time:101547ms step_avg:60.09ms
step:1691/2420 train_time:101607ms step_avg:60.09ms
step:1692/2420 train_time:101669ms step_avg:60.09ms
step:1693/2420 train_time:101729ms step_avg:60.09ms
step:1694/2420 train_time:101792ms step_avg:60.09ms
step:1695/2420 train_time:101852ms step_avg:60.09ms
step:1696/2420 train_time:101914ms step_avg:60.09ms
step:1697/2420 train_time:101974ms step_avg:60.09ms
step:1698/2420 train_time:102036ms step_avg:60.09ms
step:1699/2420 train_time:102096ms step_avg:60.09ms
step:1700/2420 train_time:102158ms step_avg:60.09ms
step:1701/2420 train_time:102218ms step_avg:60.09ms
step:1702/2420 train_time:102279ms step_avg:60.09ms
step:1703/2420 train_time:102339ms step_avg:60.09ms
step:1704/2420 train_time:102400ms step_avg:60.09ms
step:1705/2420 train_time:102460ms step_avg:60.09ms
step:1706/2420 train_time:102521ms step_avg:60.09ms
step:1707/2420 train_time:102581ms step_avg:60.09ms
step:1708/2420 train_time:102643ms step_avg:60.10ms
step:1709/2420 train_time:102703ms step_avg:60.10ms
step:1710/2420 train_time:102765ms step_avg:60.10ms
step:1711/2420 train_time:102826ms step_avg:60.10ms
step:1712/2420 train_time:102888ms step_avg:60.10ms
step:1713/2420 train_time:102948ms step_avg:60.10ms
step:1714/2420 train_time:103010ms step_avg:60.10ms
step:1715/2420 train_time:103070ms step_avg:60.10ms
step:1716/2420 train_time:103132ms step_avg:60.10ms
step:1717/2420 train_time:103192ms step_avg:60.10ms
step:1718/2420 train_time:103254ms step_avg:60.10ms
step:1719/2420 train_time:103315ms step_avg:60.10ms
step:1720/2420 train_time:103377ms step_avg:60.10ms
step:1721/2420 train_time:103437ms step_avg:60.10ms
step:1722/2420 train_time:103499ms step_avg:60.10ms
step:1723/2420 train_time:103558ms step_avg:60.10ms
step:1724/2420 train_time:103621ms step_avg:60.10ms
step:1725/2420 train_time:103681ms step_avg:60.10ms
step:1726/2420 train_time:103743ms step_avg:60.11ms
step:1727/2420 train_time:103802ms step_avg:60.11ms
step:1728/2420 train_time:103864ms step_avg:60.11ms
step:1729/2420 train_time:103924ms step_avg:60.11ms
step:1730/2420 train_time:103986ms step_avg:60.11ms
step:1731/2420 train_time:104046ms step_avg:60.11ms
step:1732/2420 train_time:104108ms step_avg:60.11ms
step:1733/2420 train_time:104168ms step_avg:60.11ms
step:1734/2420 train_time:104230ms step_avg:60.11ms
step:1735/2420 train_time:104290ms step_avg:60.11ms
step:1736/2420 train_time:104352ms step_avg:60.11ms
step:1737/2420 train_time:104413ms step_avg:60.11ms
step:1738/2420 train_time:104475ms step_avg:60.11ms
step:1739/2420 train_time:104535ms step_avg:60.11ms
step:1740/2420 train_time:104597ms step_avg:60.11ms
step:1741/2420 train_time:104658ms step_avg:60.11ms
step:1742/2420 train_time:104721ms step_avg:60.12ms
step:1743/2420 train_time:104781ms step_avg:60.12ms
step:1744/2420 train_time:104842ms step_avg:60.12ms
step:1745/2420 train_time:104901ms step_avg:60.12ms
step:1746/2420 train_time:104963ms step_avg:60.12ms
step:1747/2420 train_time:105022ms step_avg:60.12ms
step:1748/2420 train_time:105084ms step_avg:60.12ms
step:1749/2420 train_time:105144ms step_avg:60.12ms
step:1750/2420 train_time:105206ms step_avg:60.12ms
step:1750/2420 val_loss:3.4019 train_time:105271ms step_avg:60.15ms
step:1751/2420 train_time:105291ms step_avg:60.13ms
step:1752/2420 train_time:105330ms step_avg:60.12ms
step:1753/2420 train_time:105390ms step_avg:60.12ms
step:1754/2420 train_time:105453ms step_avg:60.12ms
step:1755/2420 train_time:105513ms step_avg:60.12ms
step:1756/2420 train_time:105576ms step_avg:60.12ms
step:1757/2420 train_time:105635ms step_avg:60.12ms
step:1758/2420 train_time:105696ms step_avg:60.12ms
step:1759/2420 train_time:105756ms step_avg:60.12ms
step:1760/2420 train_time:105816ms step_avg:60.12ms
step:1761/2420 train_time:105875ms step_avg:60.12ms
step:1762/2420 train_time:105936ms step_avg:60.12ms
step:1763/2420 train_time:105995ms step_avg:60.12ms
step:1764/2420 train_time:106057ms step_avg:60.12ms
step:1765/2420 train_time:106116ms step_avg:60.12ms
step:1766/2420 train_time:106180ms step_avg:60.12ms
step:1767/2420 train_time:106244ms step_avg:60.13ms
step:1768/2420 train_time:106307ms step_avg:60.13ms
step:1769/2420 train_time:106368ms step_avg:60.13ms
step:1770/2420 train_time:106430ms step_avg:60.13ms
step:1771/2420 train_time:106490ms step_avg:60.13ms
step:1772/2420 train_time:106552ms step_avg:60.13ms
step:1773/2420 train_time:106612ms step_avg:60.13ms
step:1774/2420 train_time:106673ms step_avg:60.13ms
step:1775/2420 train_time:106733ms step_avg:60.13ms
step:1776/2420 train_time:106795ms step_avg:60.13ms
step:1777/2420 train_time:106854ms step_avg:60.13ms
step:1778/2420 train_time:106915ms step_avg:60.13ms
step:1779/2420 train_time:106974ms step_avg:60.13ms
step:1780/2420 train_time:107036ms step_avg:60.13ms
step:1781/2420 train_time:107095ms step_avg:60.13ms
step:1782/2420 train_time:107158ms step_avg:60.13ms
step:1783/2420 train_time:107220ms step_avg:60.13ms
step:1784/2420 train_time:107283ms step_avg:60.14ms
step:1785/2420 train_time:107344ms step_avg:60.14ms
step:1786/2420 train_time:107406ms step_avg:60.14ms
step:1787/2420 train_time:107467ms step_avg:60.14ms
step:1788/2420 train_time:107528ms step_avg:60.14ms
step:1789/2420 train_time:107588ms step_avg:60.14ms
step:1790/2420 train_time:107649ms step_avg:60.14ms
step:1791/2420 train_time:107708ms step_avg:60.14ms
step:1792/2420 train_time:107769ms step_avg:60.14ms
step:1793/2420 train_time:107829ms step_avg:60.14ms
step:1794/2420 train_time:107890ms step_avg:60.14ms
step:1795/2420 train_time:107949ms step_avg:60.14ms
step:1796/2420 train_time:108010ms step_avg:60.14ms
step:1797/2420 train_time:108069ms step_avg:60.14ms
step:1798/2420 train_time:108131ms step_avg:60.14ms
step:1799/2420 train_time:108192ms step_avg:60.14ms
step:1800/2420 train_time:108255ms step_avg:60.14ms
step:1801/2420 train_time:108315ms step_avg:60.14ms
step:1802/2420 train_time:108378ms step_avg:60.14ms
step:1803/2420 train_time:108438ms step_avg:60.14ms
step:1804/2420 train_time:108500ms step_avg:60.14ms
step:1805/2420 train_time:108560ms step_avg:60.14ms
step:1806/2420 train_time:108622ms step_avg:60.15ms
step:1807/2420 train_time:108682ms step_avg:60.14ms
step:1808/2420 train_time:108744ms step_avg:60.15ms
step:1809/2420 train_time:108804ms step_avg:60.15ms
step:1810/2420 train_time:108865ms step_avg:60.15ms
step:1811/2420 train_time:108925ms step_avg:60.15ms
step:1812/2420 train_time:108987ms step_avg:60.15ms
step:1813/2420 train_time:109046ms step_avg:60.15ms
step:1814/2420 train_time:109108ms step_avg:60.15ms
step:1815/2420 train_time:109168ms step_avg:60.15ms
step:1816/2420 train_time:109230ms step_avg:60.15ms
step:1817/2420 train_time:109290ms step_avg:60.15ms
step:1818/2420 train_time:109352ms step_avg:60.15ms
step:1819/2420 train_time:109411ms step_avg:60.15ms
step:1820/2420 train_time:109473ms step_avg:60.15ms
step:1821/2420 train_time:109533ms step_avg:60.15ms
step:1822/2420 train_time:109595ms step_avg:60.15ms
step:1823/2420 train_time:109655ms step_avg:60.15ms
step:1824/2420 train_time:109717ms step_avg:60.15ms
step:1825/2420 train_time:109777ms step_avg:60.15ms
step:1826/2420 train_time:109838ms step_avg:60.15ms
step:1827/2420 train_time:109898ms step_avg:60.15ms
step:1828/2420 train_time:109960ms step_avg:60.15ms
step:1829/2420 train_time:110020ms step_avg:60.15ms
step:1830/2420 train_time:110082ms step_avg:60.15ms
step:1831/2420 train_time:110142ms step_avg:60.15ms
step:1832/2420 train_time:110204ms step_avg:60.16ms
step:1833/2420 train_time:110265ms step_avg:60.16ms
step:1834/2420 train_time:110327ms step_avg:60.16ms
step:1835/2420 train_time:110387ms step_avg:60.16ms
step:1836/2420 train_time:110448ms step_avg:60.16ms
step:1837/2420 train_time:110508ms step_avg:60.16ms
step:1838/2420 train_time:110569ms step_avg:60.16ms
step:1839/2420 train_time:110628ms step_avg:60.16ms
step:1840/2420 train_time:110690ms step_avg:60.16ms
step:1841/2420 train_time:110749ms step_avg:60.16ms
step:1842/2420 train_time:110811ms step_avg:60.16ms
step:1843/2420 train_time:110871ms step_avg:60.16ms
step:1844/2420 train_time:110934ms step_avg:60.16ms
step:1845/2420 train_time:110994ms step_avg:60.16ms
step:1846/2420 train_time:111055ms step_avg:60.16ms
step:1847/2420 train_time:111115ms step_avg:60.16ms
step:1848/2420 train_time:111177ms step_avg:60.16ms
step:1849/2420 train_time:111237ms step_avg:60.16ms
step:1850/2420 train_time:111299ms step_avg:60.16ms
step:1851/2420 train_time:111360ms step_avg:60.16ms
step:1852/2420 train_time:111422ms step_avg:60.16ms
step:1853/2420 train_time:111482ms step_avg:60.16ms
step:1854/2420 train_time:111544ms step_avg:60.16ms
step:1855/2420 train_time:111604ms step_avg:60.16ms
step:1856/2420 train_time:111666ms step_avg:60.17ms
step:1857/2420 train_time:111726ms step_avg:60.16ms
step:1858/2420 train_time:111788ms step_avg:60.17ms
step:1859/2420 train_time:111847ms step_avg:60.17ms
step:1860/2420 train_time:111908ms step_avg:60.17ms
step:1861/2420 train_time:111968ms step_avg:60.17ms
step:1862/2420 train_time:112029ms step_avg:60.17ms
step:1863/2420 train_time:112089ms step_avg:60.17ms
step:1864/2420 train_time:112152ms step_avg:60.17ms
step:1865/2420 train_time:112211ms step_avg:60.17ms
step:1866/2420 train_time:112273ms step_avg:60.17ms
step:1867/2420 train_time:112333ms step_avg:60.17ms
step:1868/2420 train_time:112395ms step_avg:60.17ms
step:1869/2420 train_time:112455ms step_avg:60.17ms
step:1870/2420 train_time:112517ms step_avg:60.17ms
step:1871/2420 train_time:112576ms step_avg:60.17ms
step:1872/2420 train_time:112638ms step_avg:60.17ms
step:1873/2420 train_time:112698ms step_avg:60.17ms
step:1874/2420 train_time:112760ms step_avg:60.17ms
step:1875/2420 train_time:112821ms step_avg:60.17ms
step:1876/2420 train_time:112883ms step_avg:60.17ms
step:1877/2420 train_time:112942ms step_avg:60.17ms
step:1878/2420 train_time:113005ms step_avg:60.17ms
step:1879/2420 train_time:113065ms step_avg:60.17ms
step:1880/2420 train_time:113127ms step_avg:60.17ms
step:1881/2420 train_time:113187ms step_avg:60.17ms
step:1882/2420 train_time:113249ms step_avg:60.17ms
step:1883/2420 train_time:113308ms step_avg:60.17ms
step:1884/2420 train_time:113370ms step_avg:60.18ms
step:1885/2420 train_time:113429ms step_avg:60.17ms
step:1886/2420 train_time:113491ms step_avg:60.18ms
step:1887/2420 train_time:113551ms step_avg:60.18ms
step:1888/2420 train_time:113613ms step_avg:60.18ms
step:1889/2420 train_time:113673ms step_avg:60.18ms
step:1890/2420 train_time:113735ms step_avg:60.18ms
step:1891/2420 train_time:113795ms step_avg:60.18ms
step:1892/2420 train_time:113857ms step_avg:60.18ms
step:1893/2420 train_time:113917ms step_avg:60.18ms
step:1894/2420 train_time:113979ms step_avg:60.18ms
step:1895/2420 train_time:114039ms step_avg:60.18ms
step:1896/2420 train_time:114101ms step_avg:60.18ms
step:1897/2420 train_time:114161ms step_avg:60.18ms
step:1898/2420 train_time:114223ms step_avg:60.18ms
step:1899/2420 train_time:114283ms step_avg:60.18ms
step:1900/2420 train_time:114345ms step_avg:60.18ms
step:1901/2420 train_time:114405ms step_avg:60.18ms
step:1902/2420 train_time:114467ms step_avg:60.18ms
step:1903/2420 train_time:114527ms step_avg:60.18ms
step:1904/2420 train_time:114589ms step_avg:60.18ms
step:1905/2420 train_time:114648ms step_avg:60.18ms
step:1906/2420 train_time:114709ms step_avg:60.18ms
step:1907/2420 train_time:114769ms step_avg:60.18ms
step:1908/2420 train_time:114830ms step_avg:60.18ms
step:1909/2420 train_time:114891ms step_avg:60.18ms
step:1910/2420 train_time:114953ms step_avg:60.18ms
step:1911/2420 train_time:115012ms step_avg:60.18ms
step:1912/2420 train_time:115074ms step_avg:60.19ms
step:1913/2420 train_time:115134ms step_avg:60.18ms
step:1914/2420 train_time:115196ms step_avg:60.19ms
step:1915/2420 train_time:115255ms step_avg:60.19ms
step:1916/2420 train_time:115318ms step_avg:60.19ms
step:1917/2420 train_time:115378ms step_avg:60.19ms
step:1918/2420 train_time:115440ms step_avg:60.19ms
step:1919/2420 train_time:115500ms step_avg:60.19ms
step:1920/2420 train_time:115562ms step_avg:60.19ms
step:1921/2420 train_time:115622ms step_avg:60.19ms
step:1922/2420 train_time:115684ms step_avg:60.19ms
step:1923/2420 train_time:115743ms step_avg:60.19ms
step:1924/2420 train_time:115805ms step_avg:60.19ms
step:1925/2420 train_time:115866ms step_avg:60.19ms
step:1926/2420 train_time:115928ms step_avg:60.19ms
step:1927/2420 train_time:115987ms step_avg:60.19ms
step:1928/2420 train_time:116048ms step_avg:60.19ms
step:1929/2420 train_time:116108ms step_avg:60.19ms
step:1930/2420 train_time:116169ms step_avg:60.19ms
step:1931/2420 train_time:116228ms step_avg:60.19ms
step:1932/2420 train_time:116290ms step_avg:60.19ms
step:1933/2420 train_time:116350ms step_avg:60.19ms
step:1934/2420 train_time:116411ms step_avg:60.19ms
step:1935/2420 train_time:116471ms step_avg:60.19ms
step:1936/2420 train_time:116533ms step_avg:60.19ms
step:1937/2420 train_time:116593ms step_avg:60.19ms
step:1938/2420 train_time:116655ms step_avg:60.19ms
step:1939/2420 train_time:116715ms step_avg:60.19ms
step:1940/2420 train_time:116776ms step_avg:60.19ms
step:1941/2420 train_time:116837ms step_avg:60.19ms
step:1942/2420 train_time:116899ms step_avg:60.20ms
step:1943/2420 train_time:116959ms step_avg:60.19ms
step:1944/2420 train_time:117020ms step_avg:60.20ms
step:1945/2420 train_time:117081ms step_avg:60.20ms
step:1946/2420 train_time:117143ms step_avg:60.20ms
step:1947/2420 train_time:117203ms step_avg:60.20ms
step:1948/2420 train_time:117265ms step_avg:60.20ms
step:1949/2420 train_time:117325ms step_avg:60.20ms
step:1950/2420 train_time:117387ms step_avg:60.20ms
step:1951/2420 train_time:117447ms step_avg:60.20ms
step:1952/2420 train_time:117508ms step_avg:60.20ms
step:1953/2420 train_time:117568ms step_avg:60.20ms
step:1954/2420 train_time:117629ms step_avg:60.20ms
step:1955/2420 train_time:117689ms step_avg:60.20ms
step:1956/2420 train_time:117750ms step_avg:60.20ms
step:1957/2420 train_time:117810ms step_avg:60.20ms
step:1958/2420 train_time:117872ms step_avg:60.20ms
step:1959/2420 train_time:117932ms step_avg:60.20ms
step:1960/2420 train_time:117993ms step_avg:60.20ms
step:1961/2420 train_time:118053ms step_avg:60.20ms
step:1962/2420 train_time:118115ms step_avg:60.20ms
step:1963/2420 train_time:118175ms step_avg:60.20ms
step:1964/2420 train_time:118237ms step_avg:60.20ms
step:1965/2420 train_time:118297ms step_avg:60.20ms
step:1966/2420 train_time:118359ms step_avg:60.20ms
step:1967/2420 train_time:118419ms step_avg:60.20ms
step:1968/2420 train_time:118481ms step_avg:60.20ms
step:1969/2420 train_time:118542ms step_avg:60.20ms
step:1970/2420 train_time:118604ms step_avg:60.20ms
step:1971/2420 train_time:118664ms step_avg:60.20ms
step:1972/2420 train_time:118726ms step_avg:60.21ms
step:1973/2420 train_time:118787ms step_avg:60.21ms
step:1974/2420 train_time:118848ms step_avg:60.21ms
step:1975/2420 train_time:118907ms step_avg:60.21ms
step:1976/2420 train_time:118969ms step_avg:60.21ms
step:1977/2420 train_time:119029ms step_avg:60.21ms
step:1978/2420 train_time:119090ms step_avg:60.21ms
step:1979/2420 train_time:119149ms step_avg:60.21ms
step:1980/2420 train_time:119211ms step_avg:60.21ms
step:1981/2420 train_time:119271ms step_avg:60.21ms
step:1982/2420 train_time:119332ms step_avg:60.21ms
step:1983/2420 train_time:119392ms step_avg:60.21ms
step:1984/2420 train_time:119454ms step_avg:60.21ms
step:1985/2420 train_time:119514ms step_avg:60.21ms
step:1986/2420 train_time:119577ms step_avg:60.21ms
step:1987/2420 train_time:119637ms step_avg:60.21ms
step:1988/2420 train_time:119699ms step_avg:60.21ms
step:1989/2420 train_time:119759ms step_avg:60.21ms
step:1990/2420 train_time:119820ms step_avg:60.21ms
step:1991/2420 train_time:119880ms step_avg:60.21ms
step:1992/2420 train_time:119942ms step_avg:60.21ms
step:1993/2420 train_time:120002ms step_avg:60.21ms
step:1994/2420 train_time:120065ms step_avg:60.21ms
step:1995/2420 train_time:120124ms step_avg:60.21ms
step:1996/2420 train_time:120186ms step_avg:60.21ms
step:1997/2420 train_time:120246ms step_avg:60.21ms
step:1998/2420 train_time:120308ms step_avg:60.21ms
step:1999/2420 train_time:120367ms step_avg:60.21ms
step:2000/2420 train_time:120428ms step_avg:60.21ms
step:2000/2420 val_loss:3.3474 train_time:120492ms step_avg:60.25ms
step:2001/2420 train_time:120515ms step_avg:60.23ms
step:2002/2420 train_time:120555ms step_avg:60.22ms
step:2003/2420 train_time:120618ms step_avg:60.22ms
step:2004/2420 train_time:120682ms step_avg:60.22ms
step:2005/2420 train_time:120742ms step_avg:60.22ms
step:2006/2420 train_time:120804ms step_avg:60.22ms
step:2007/2420 train_time:120863ms step_avg:60.22ms
step:2008/2420 train_time:120923ms step_avg:60.22ms
step:2009/2420 train_time:120983ms step_avg:60.22ms
step:2010/2420 train_time:121044ms step_avg:60.22ms
step:2011/2420 train_time:121103ms step_avg:60.22ms
step:2012/2420 train_time:121164ms step_avg:60.22ms
step:2013/2420 train_time:121223ms step_avg:60.22ms
step:2014/2420 train_time:121284ms step_avg:60.22ms
step:2015/2420 train_time:121344ms step_avg:60.22ms
step:2016/2420 train_time:121405ms step_avg:60.22ms
step:2017/2420 train_time:121466ms step_avg:60.22ms
step:2018/2420 train_time:121531ms step_avg:60.22ms
step:2019/2420 train_time:121592ms step_avg:60.22ms
step:2020/2420 train_time:121656ms step_avg:60.23ms
step:2021/2420 train_time:121717ms step_avg:60.23ms
step:2022/2420 train_time:121779ms step_avg:60.23ms
step:2023/2420 train_time:121839ms step_avg:60.23ms
step:2024/2420 train_time:121901ms step_avg:60.23ms
step:2025/2420 train_time:121960ms step_avg:60.23ms
step:2026/2420 train_time:122021ms step_avg:60.23ms
step:2027/2420 train_time:122081ms step_avg:60.23ms
step:2028/2420 train_time:122141ms step_avg:60.23ms
step:2029/2420 train_time:122200ms step_avg:60.23ms
step:2030/2420 train_time:122261ms step_avg:60.23ms
step:2031/2420 train_time:122320ms step_avg:60.23ms
step:2032/2420 train_time:122382ms step_avg:60.23ms
step:2033/2420 train_time:122443ms step_avg:60.23ms
step:2034/2420 train_time:122506ms step_avg:60.23ms
step:2035/2420 train_time:122567ms step_avg:60.23ms
step:2036/2420 train_time:122631ms step_avg:60.23ms
step:2037/2420 train_time:122691ms step_avg:60.23ms
step:2038/2420 train_time:122753ms step_avg:60.23ms
step:2039/2420 train_time:122813ms step_avg:60.23ms
step:2040/2420 train_time:122874ms step_avg:60.23ms
step:2041/2420 train_time:122935ms step_avg:60.23ms
step:2042/2420 train_time:122997ms step_avg:60.23ms
step:2043/2420 train_time:123057ms step_avg:60.23ms
step:2044/2420 train_time:123119ms step_avg:60.23ms
step:2045/2420 train_time:123178ms step_avg:60.23ms
step:2046/2420 train_time:123240ms step_avg:60.23ms
step:2047/2420 train_time:123300ms step_avg:60.23ms
step:2048/2420 train_time:123361ms step_avg:60.23ms
step:2049/2420 train_time:123420ms step_avg:60.23ms
step:2050/2420 train_time:123483ms step_avg:60.24ms
step:2051/2420 train_time:123543ms step_avg:60.24ms
step:2052/2420 train_time:123605ms step_avg:60.24ms
step:2053/2420 train_time:123665ms step_avg:60.24ms
step:2054/2420 train_time:123728ms step_avg:60.24ms
step:2055/2420 train_time:123788ms step_avg:60.24ms
step:2056/2420 train_time:123850ms step_avg:60.24ms
step:2057/2420 train_time:123910ms step_avg:60.24ms
step:2058/2420 train_time:123972ms step_avg:60.24ms
step:2059/2420 train_time:124032ms step_avg:60.24ms
step:2060/2420 train_time:124094ms step_avg:60.24ms
step:2061/2420 train_time:124154ms step_avg:60.24ms
step:2062/2420 train_time:124216ms step_avg:60.24ms
step:2063/2420 train_time:124276ms step_avg:60.24ms
step:2064/2420 train_time:124338ms step_avg:60.24ms
step:2065/2420 train_time:124398ms step_avg:60.24ms
step:2066/2420 train_time:124460ms step_avg:60.24ms
step:2067/2420 train_time:124519ms step_avg:60.24ms
step:2068/2420 train_time:124581ms step_avg:60.24ms
step:2069/2420 train_time:124640ms step_avg:60.24ms
step:2070/2420 train_time:124702ms step_avg:60.24ms
step:2071/2420 train_time:124762ms step_avg:60.24ms
step:2072/2420 train_time:124824ms step_avg:60.24ms
step:2073/2420 train_time:124885ms step_avg:60.24ms
step:2074/2420 train_time:124946ms step_avg:60.24ms
step:2075/2420 train_time:125007ms step_avg:60.24ms
step:2076/2420 train_time:125069ms step_avg:60.25ms
step:2077/2420 train_time:125129ms step_avg:60.25ms
step:2078/2420 train_time:125191ms step_avg:60.25ms
step:2079/2420 train_time:125251ms step_avg:60.25ms
step:2080/2420 train_time:125313ms step_avg:60.25ms
step:2081/2420 train_time:125374ms step_avg:60.25ms
step:2082/2420 train_time:125436ms step_avg:60.25ms
step:2083/2420 train_time:125497ms step_avg:60.25ms
step:2084/2420 train_time:125559ms step_avg:60.25ms
step:2085/2420 train_time:125618ms step_avg:60.25ms
step:2086/2420 train_time:125680ms step_avg:60.25ms
step:2087/2420 train_time:125740ms step_avg:60.25ms
step:2088/2420 train_time:125802ms step_avg:60.25ms
step:2089/2420 train_time:125861ms step_avg:60.25ms
step:2090/2420 train_time:125923ms step_avg:60.25ms
step:2091/2420 train_time:125983ms step_avg:60.25ms
step:2092/2420 train_time:126045ms step_avg:60.25ms
step:2093/2420 train_time:126106ms step_avg:60.25ms
step:2094/2420 train_time:126168ms step_avg:60.25ms
step:2095/2420 train_time:126228ms step_avg:60.25ms
step:2096/2420 train_time:126290ms step_avg:60.25ms
step:2097/2420 train_time:126350ms step_avg:60.25ms
step:2098/2420 train_time:126412ms step_avg:60.25ms
step:2099/2420 train_time:126473ms step_avg:60.25ms
step:2100/2420 train_time:126535ms step_avg:60.25ms
step:2101/2420 train_time:126596ms step_avg:60.25ms
step:2102/2420 train_time:126657ms step_avg:60.26ms
step:2103/2420 train_time:126717ms step_avg:60.26ms
step:2104/2420 train_time:126779ms step_avg:60.26ms
step:2105/2420 train_time:126838ms step_avg:60.26ms
step:2106/2420 train_time:126900ms step_avg:60.26ms
step:2107/2420 train_time:126960ms step_avg:60.26ms
step:2108/2420 train_time:127021ms step_avg:60.26ms
step:2109/2420 train_time:127081ms step_avg:60.26ms
step:2110/2420 train_time:127143ms step_avg:60.26ms
step:2111/2420 train_time:127202ms step_avg:60.26ms
step:2112/2420 train_time:127265ms step_avg:60.26ms
step:2113/2420 train_time:127325ms step_avg:60.26ms
step:2114/2420 train_time:127388ms step_avg:60.26ms
step:2115/2420 train_time:127448ms step_avg:60.26ms
step:2116/2420 train_time:127510ms step_avg:60.26ms
step:2117/2420 train_time:127570ms step_avg:60.26ms
step:2118/2420 train_time:127633ms step_avg:60.26ms
step:2119/2420 train_time:127693ms step_avg:60.26ms
step:2120/2420 train_time:127755ms step_avg:60.26ms
step:2121/2420 train_time:127815ms step_avg:60.26ms
step:2122/2420 train_time:127877ms step_avg:60.26ms
step:2123/2420 train_time:127937ms step_avg:60.26ms
step:2124/2420 train_time:127999ms step_avg:60.26ms
step:2125/2420 train_time:128059ms step_avg:60.26ms
step:2126/2420 train_time:128120ms step_avg:60.26ms
step:2127/2420 train_time:128180ms step_avg:60.26ms
step:2128/2420 train_time:128242ms step_avg:60.26ms
step:2129/2420 train_time:128302ms step_avg:60.26ms
step:2130/2420 train_time:128364ms step_avg:60.26ms
step:2131/2420 train_time:128424ms step_avg:60.26ms
step:2132/2420 train_time:128486ms step_avg:60.27ms
step:2133/2420 train_time:128546ms step_avg:60.27ms
step:2134/2420 train_time:128608ms step_avg:60.27ms
step:2135/2420 train_time:128669ms step_avg:60.27ms
step:2136/2420 train_time:128731ms step_avg:60.27ms
step:2137/2420 train_time:128791ms step_avg:60.27ms
step:2138/2420 train_time:128854ms step_avg:60.27ms
step:2139/2420 train_time:128914ms step_avg:60.27ms
step:2140/2420 train_time:128976ms step_avg:60.27ms
step:2141/2420 train_time:129037ms step_avg:60.27ms
step:2142/2420 train_time:129098ms step_avg:60.27ms
step:2143/2420 train_time:129158ms step_avg:60.27ms
step:2144/2420 train_time:129219ms step_avg:60.27ms
step:2145/2420 train_time:129279ms step_avg:60.27ms
step:2146/2420 train_time:129340ms step_avg:60.27ms
step:2147/2420 train_time:129400ms step_avg:60.27ms
step:2148/2420 train_time:129461ms step_avg:60.27ms
step:2149/2420 train_time:129521ms step_avg:60.27ms
step:2150/2420 train_time:129583ms step_avg:60.27ms
step:2151/2420 train_time:129643ms step_avg:60.27ms
step:2152/2420 train_time:129706ms step_avg:60.27ms
step:2153/2420 train_time:129766ms step_avg:60.27ms
step:2154/2420 train_time:129828ms step_avg:60.27ms
step:2155/2420 train_time:129888ms step_avg:60.27ms
step:2156/2420 train_time:129951ms step_avg:60.27ms
step:2157/2420 train_time:130012ms step_avg:60.27ms
step:2158/2420 train_time:130074ms step_avg:60.28ms
step:2159/2420 train_time:130134ms step_avg:60.28ms
step:2160/2420 train_time:130197ms step_avg:60.28ms
step:2161/2420 train_time:130257ms step_avg:60.28ms
step:2162/2420 train_time:130319ms step_avg:60.28ms
step:2163/2420 train_time:130378ms step_avg:60.28ms
step:2164/2420 train_time:130440ms step_avg:60.28ms
step:2165/2420 train_time:130500ms step_avg:60.28ms
step:2166/2420 train_time:130561ms step_avg:60.28ms
step:2167/2420 train_time:130621ms step_avg:60.28ms
step:2168/2420 train_time:130683ms step_avg:60.28ms
step:2169/2420 train_time:130743ms step_avg:60.28ms
step:2170/2420 train_time:130805ms step_avg:60.28ms
step:2171/2420 train_time:130864ms step_avg:60.28ms
step:2172/2420 train_time:130927ms step_avg:60.28ms
step:2173/2420 train_time:130987ms step_avg:60.28ms
step:2174/2420 train_time:131050ms step_avg:60.28ms
step:2175/2420 train_time:131110ms step_avg:60.28ms
step:2176/2420 train_time:131172ms step_avg:60.28ms
step:2177/2420 train_time:131233ms step_avg:60.28ms
step:2178/2420 train_time:131295ms step_avg:60.28ms
step:2179/2420 train_time:131356ms step_avg:60.28ms
step:2180/2420 train_time:131418ms step_avg:60.28ms
step:2181/2420 train_time:131478ms step_avg:60.28ms
step:2182/2420 train_time:131540ms step_avg:60.28ms
step:2183/2420 train_time:131600ms step_avg:60.28ms
step:2184/2420 train_time:131661ms step_avg:60.28ms
step:2185/2420 train_time:131720ms step_avg:60.28ms
step:2186/2420 train_time:131782ms step_avg:60.28ms
step:2187/2420 train_time:131842ms step_avg:60.28ms
step:2188/2420 train_time:131904ms step_avg:60.29ms
step:2189/2420 train_time:131964ms step_avg:60.29ms
step:2190/2420 train_time:132026ms step_avg:60.29ms
step:2191/2420 train_time:132087ms step_avg:60.29ms
step:2192/2420 train_time:132149ms step_avg:60.29ms
step:2193/2420 train_time:132209ms step_avg:60.29ms
step:2194/2420 train_time:132271ms step_avg:60.29ms
step:2195/2420 train_time:132332ms step_avg:60.29ms
step:2196/2420 train_time:132395ms step_avg:60.29ms
step:2197/2420 train_time:132455ms step_avg:60.29ms
step:2198/2420 train_time:132516ms step_avg:60.29ms
step:2199/2420 train_time:132576ms step_avg:60.29ms
step:2200/2420 train_time:132638ms step_avg:60.29ms
step:2201/2420 train_time:132697ms step_avg:60.29ms
step:2202/2420 train_time:132759ms step_avg:60.29ms
step:2203/2420 train_time:132819ms step_avg:60.29ms
step:2204/2420 train_time:132880ms step_avg:60.29ms
step:2205/2420 train_time:132940ms step_avg:60.29ms
step:2206/2420 train_time:133001ms step_avg:60.29ms
step:2207/2420 train_time:133061ms step_avg:60.29ms
step:2208/2420 train_time:133122ms step_avg:60.29ms
step:2209/2420 train_time:133183ms step_avg:60.29ms
step:2210/2420 train_time:133245ms step_avg:60.29ms
step:2211/2420 train_time:133305ms step_avg:60.29ms
step:2212/2420 train_time:133368ms step_avg:60.29ms
step:2213/2420 train_time:133428ms step_avg:60.29ms
step:2214/2420 train_time:133491ms step_avg:60.29ms
step:2215/2420 train_time:133551ms step_avg:60.29ms
step:2216/2420 train_time:133613ms step_avg:60.29ms
step:2217/2420 train_time:133673ms step_avg:60.29ms
step:2218/2420 train_time:133735ms step_avg:60.30ms
step:2219/2420 train_time:133796ms step_avg:60.30ms
step:2220/2420 train_time:133858ms step_avg:60.30ms
step:2221/2420 train_time:133918ms step_avg:60.30ms
step:2222/2420 train_time:133979ms step_avg:60.30ms
step:2223/2420 train_time:134038ms step_avg:60.30ms
step:2224/2420 train_time:134100ms step_avg:60.30ms
step:2225/2420 train_time:134160ms step_avg:60.30ms
step:2226/2420 train_time:134221ms step_avg:60.30ms
step:2227/2420 train_time:134281ms step_avg:60.30ms
step:2228/2420 train_time:134343ms step_avg:60.30ms
step:2229/2420 train_time:134403ms step_avg:60.30ms
step:2230/2420 train_time:134466ms step_avg:60.30ms
step:2231/2420 train_time:134526ms step_avg:60.30ms
step:2232/2420 train_time:134588ms step_avg:60.30ms
step:2233/2420 train_time:134648ms step_avg:60.30ms
step:2234/2420 train_time:134711ms step_avg:60.30ms
step:2235/2420 train_time:134770ms step_avg:60.30ms
step:2236/2420 train_time:134832ms step_avg:60.30ms
step:2237/2420 train_time:134893ms step_avg:60.30ms
step:2238/2420 train_time:134955ms step_avg:60.30ms
step:2239/2420 train_time:135015ms step_avg:60.30ms
step:2240/2420 train_time:135078ms step_avg:60.30ms
step:2241/2420 train_time:135137ms step_avg:60.30ms
step:2242/2420 train_time:135199ms step_avg:60.30ms
step:2243/2420 train_time:135259ms step_avg:60.30ms
step:2244/2420 train_time:135321ms step_avg:60.30ms
step:2245/2420 train_time:135380ms step_avg:60.30ms
step:2246/2420 train_time:135442ms step_avg:60.30ms
step:2247/2420 train_time:135502ms step_avg:60.30ms
step:2248/2420 train_time:135564ms step_avg:60.30ms
step:2249/2420 train_time:135624ms step_avg:60.30ms
step:2250/2420 train_time:135687ms step_avg:60.31ms
step:2250/2420 val_loss:3.3021 train_time:135751ms step_avg:60.33ms
step:2251/2420 train_time:135771ms step_avg:60.32ms
step:2252/2420 train_time:135811ms step_avg:60.31ms
step:2253/2420 train_time:135874ms step_avg:60.31ms
step:2254/2420 train_time:135938ms step_avg:60.31ms
step:2255/2420 train_time:135997ms step_avg:60.31ms
step:2256/2420 train_time:136059ms step_avg:60.31ms
step:2257/2420 train_time:136118ms step_avg:60.31ms
step:2258/2420 train_time:136179ms step_avg:60.31ms
step:2259/2420 train_time:136238ms step_avg:60.31ms
step:2260/2420 train_time:136299ms step_avg:60.31ms
step:2261/2420 train_time:136359ms step_avg:60.31ms
step:2262/2420 train_time:136420ms step_avg:60.31ms
step:2263/2420 train_time:136479ms step_avg:60.31ms
step:2264/2420 train_time:136540ms step_avg:60.31ms
step:2265/2420 train_time:136600ms step_avg:60.31ms
step:2266/2420 train_time:136662ms step_avg:60.31ms
step:2267/2420 train_time:136723ms step_avg:60.31ms
step:2268/2420 train_time:136786ms step_avg:60.31ms
step:2269/2420 train_time:136848ms step_avg:60.31ms
step:2270/2420 train_time:136911ms step_avg:60.31ms
step:2271/2420 train_time:136972ms step_avg:60.31ms
step:2272/2420 train_time:137035ms step_avg:60.31ms
step:2273/2420 train_time:137094ms step_avg:60.31ms
step:2274/2420 train_time:137154ms step_avg:60.31ms
step:2275/2420 train_time:137214ms step_avg:60.31ms
step:2276/2420 train_time:137275ms step_avg:60.31ms
step:2277/2420 train_time:137334ms step_avg:60.31ms
step:2278/2420 train_time:137395ms step_avg:60.31ms
step:2279/2420 train_time:137454ms step_avg:60.31ms
step:2280/2420 train_time:137515ms step_avg:60.31ms
step:2281/2420 train_time:137574ms step_avg:60.31ms
step:2282/2420 train_time:137635ms step_avg:60.31ms
step:2283/2420 train_time:137696ms step_avg:60.31ms
step:2284/2420 train_time:137759ms step_avg:60.31ms
step:2285/2420 train_time:137819ms step_avg:60.31ms
step:2286/2420 train_time:137882ms step_avg:60.32ms
step:2287/2420 train_time:137943ms step_avg:60.32ms
step:2288/2420 train_time:138005ms step_avg:60.32ms
step:2289/2420 train_time:138065ms step_avg:60.32ms
step:2290/2420 train_time:138127ms step_avg:60.32ms
step:2291/2420 train_time:138187ms step_avg:60.32ms
step:2292/2420 train_time:138249ms step_avg:60.32ms
step:2293/2420 train_time:138308ms step_avg:60.32ms
step:2294/2420 train_time:138370ms step_avg:60.32ms
step:2295/2420 train_time:138430ms step_avg:60.32ms
step:2296/2420 train_time:138491ms step_avg:60.32ms
step:2297/2420 train_time:138551ms step_avg:60.32ms
step:2298/2420 train_time:138612ms step_avg:60.32ms
step:2299/2420 train_time:138672ms step_avg:60.32ms
step:2300/2420 train_time:138734ms step_avg:60.32ms
step:2301/2420 train_time:138794ms step_avg:60.32ms
step:2302/2420 train_time:138856ms step_avg:60.32ms
step:2303/2420 train_time:138916ms step_avg:60.32ms
step:2304/2420 train_time:138978ms step_avg:60.32ms
step:2305/2420 train_time:139039ms step_avg:60.32ms
step:2306/2420 train_time:139101ms step_avg:60.32ms
step:2307/2420 train_time:139160ms step_avg:60.32ms
step:2308/2420 train_time:139222ms step_avg:60.32ms
step:2309/2420 train_time:139282ms step_avg:60.32ms
step:2310/2420 train_time:139344ms step_avg:60.32ms
step:2311/2420 train_time:139404ms step_avg:60.32ms
step:2312/2420 train_time:139465ms step_avg:60.32ms
step:2313/2420 train_time:139525ms step_avg:60.32ms
step:2314/2420 train_time:139587ms step_avg:60.32ms
step:2315/2420 train_time:139647ms step_avg:60.32ms
step:2316/2420 train_time:139708ms step_avg:60.32ms
step:2317/2420 train_time:139769ms step_avg:60.32ms
step:2318/2420 train_time:139832ms step_avg:60.32ms
step:2319/2420 train_time:139892ms step_avg:60.32ms
step:2320/2420 train_time:139954ms step_avg:60.33ms
step:2321/2420 train_time:140014ms step_avg:60.32ms
step:2322/2420 train_time:140076ms step_avg:60.33ms
step:2323/2420 train_time:140135ms step_avg:60.33ms
step:2324/2420 train_time:140197ms step_avg:60.33ms
step:2325/2420 train_time:140256ms step_avg:60.33ms
step:2326/2420 train_time:140318ms step_avg:60.33ms
step:2327/2420 train_time:140377ms step_avg:60.33ms
step:2328/2420 train_time:140439ms step_avg:60.33ms
step:2329/2420 train_time:140499ms step_avg:60.33ms
step:2330/2420 train_time:140561ms step_avg:60.33ms
step:2331/2420 train_time:140621ms step_avg:60.33ms
step:2332/2420 train_time:140683ms step_avg:60.33ms
step:2333/2420 train_time:140743ms step_avg:60.33ms
step:2334/2420 train_time:140805ms step_avg:60.33ms
step:2335/2420 train_time:140866ms step_avg:60.33ms
step:2336/2420 train_time:140929ms step_avg:60.33ms
step:2337/2420 train_time:140989ms step_avg:60.33ms
step:2338/2420 train_time:141051ms step_avg:60.33ms
step:2339/2420 train_time:141111ms step_avg:60.33ms
step:2340/2420 train_time:141172ms step_avg:60.33ms
step:2341/2420 train_time:141232ms step_avg:60.33ms
step:2342/2420 train_time:141293ms step_avg:60.33ms
step:2343/2420 train_time:141353ms step_avg:60.33ms
step:2344/2420 train_time:141414ms step_avg:60.33ms
step:2345/2420 train_time:141473ms step_avg:60.33ms
step:2346/2420 train_time:141534ms step_avg:60.33ms
step:2347/2420 train_time:141594ms step_avg:60.33ms
step:2348/2420 train_time:141656ms step_avg:60.33ms
step:2349/2420 train_time:141716ms step_avg:60.33ms
step:2350/2420 train_time:141777ms step_avg:60.33ms
step:2351/2420 train_time:141838ms step_avg:60.33ms
step:2352/2420 train_time:141901ms step_avg:60.33ms
step:2353/2420 train_time:141960ms step_avg:60.33ms
step:2354/2420 train_time:142023ms step_avg:60.33ms
step:2355/2420 train_time:142083ms step_avg:60.33ms
step:2356/2420 train_time:142145ms step_avg:60.33ms
step:2357/2420 train_time:142204ms step_avg:60.33ms
step:2358/2420 train_time:142267ms step_avg:60.33ms
step:2359/2420 train_time:142327ms step_avg:60.33ms
step:2360/2420 train_time:142388ms step_avg:60.33ms
step:2361/2420 train_time:142449ms step_avg:60.33ms
step:2362/2420 train_time:142511ms step_avg:60.33ms
step:2363/2420 train_time:142570ms step_avg:60.33ms
step:2364/2420 train_time:142632ms step_avg:60.34ms
step:2365/2420 train_time:142692ms step_avg:60.33ms
step:2366/2420 train_time:142753ms step_avg:60.34ms
step:2367/2420 train_time:142813ms step_avg:60.34ms
step:2368/2420 train_time:142875ms step_avg:60.34ms
step:2369/2420 train_time:142935ms step_avg:60.34ms
step:2370/2420 train_time:142996ms step_avg:60.34ms
step:2371/2420 train_time:143056ms step_avg:60.34ms
step:2372/2420 train_time:143118ms step_avg:60.34ms
step:2373/2420 train_time:143178ms step_avg:60.34ms
step:2374/2420 train_time:143240ms step_avg:60.34ms
step:2375/2420 train_time:143300ms step_avg:60.34ms
step:2376/2420 train_time:143362ms step_avg:60.34ms
step:2377/2420 train_time:143422ms step_avg:60.34ms
step:2378/2420 train_time:143484ms step_avg:60.34ms
step:2379/2420 train_time:143544ms step_avg:60.34ms
step:2380/2420 train_time:143605ms step_avg:60.34ms
step:2381/2420 train_time:143666ms step_avg:60.34ms
step:2382/2420 train_time:143728ms step_avg:60.34ms
step:2383/2420 train_time:143788ms step_avg:60.34ms
step:2384/2420 train_time:143850ms step_avg:60.34ms
step:2385/2420 train_time:143909ms step_avg:60.34ms
step:2386/2420 train_time:143971ms step_avg:60.34ms
step:2387/2420 train_time:144400ms step_avg:60.49ms
step:2388/2420 train_time:144452ms step_avg:60.49ms
step:2389/2420 train_time:144510ms step_avg:60.49ms
step:2390/2420 train_time:144860ms step_avg:60.61ms
step:2391/2420 train_time:144917ms step_avg:60.61ms
step:2392/2420 train_time:144977ms step_avg:60.61ms
step:2393/2420 train_time:145035ms step_avg:60.61ms
step:2394/2420 train_time:145096ms step_avg:60.61ms
step:2395/2420 train_time:145155ms step_avg:60.61ms
step:2396/2420 train_time:145215ms step_avg:60.61ms
step:2397/2420 train_time:145274ms step_avg:60.61ms
step:2398/2420 train_time:145334ms step_avg:60.61ms
step:2399/2420 train_time:145393ms step_avg:60.61ms
step:2400/2420 train_time:145454ms step_avg:60.61ms
step:2401/2420 train_time:145512ms step_avg:60.60ms
step:2402/2420 train_time:145573ms step_avg:60.60ms
step:2403/2420 train_time:145632ms step_avg:60.60ms
step:2404/2420 train_time:145693ms step_avg:60.60ms
step:2405/2420 train_time:145760ms step_avg:60.61ms
step:2406/2420 train_time:145826ms step_avg:60.61ms
step:2407/2420 train_time:145889ms step_avg:60.61ms
step:2408/2420 train_time:145951ms step_avg:60.61ms
step:2409/2420 train_time:146012ms step_avg:60.61ms
step:2410/2420 train_time:146073ms step_avg:60.61ms
step:2411/2420 train_time:146133ms step_avg:60.61ms
step:2412/2420 train_time:146193ms step_avg:60.61ms
step:2413/2420 train_time:146252ms step_avg:60.61ms
step:2414/2420 train_time:146314ms step_avg:60.61ms
step:2415/2420 train_time:146373ms step_avg:60.61ms
step:2416/2420 train_time:146434ms step_avg:60.61ms
step:2417/2420 train_time:146492ms step_avg:60.61ms
step:2418/2420 train_time:146553ms step_avg:60.61ms
step:2419/2420 train_time:146612ms step_avg:60.61ms
step:2420/2420 train_time:146673ms step_avg:60.61ms
step:2420/2420 val_loss:3.2767 train_time:146739ms step_avg:60.64ms
peak memory allocated: 29512 MiB reserved: 44036 MiB

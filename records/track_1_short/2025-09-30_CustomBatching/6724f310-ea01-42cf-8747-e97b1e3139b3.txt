import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        #ve = [None, ve[0], ve[1]] + [None] * (len(self.blocks) - 5) + [ve[0], ve[1]]
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2380  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.4  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further
    momentum_cd_steps = 50


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.7, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps  # More explicit denominator

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # step the optimizers
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 21:26:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                  Off |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                  Off |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                  Off |
| N/A   41C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                  Off |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                  Off |
| N/A   35C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                  Off |
| N/A   41C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                  Off |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                  Off |
| N/A   36C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          246244      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          246245      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          246246      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          246247      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          246248      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          246249      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          246250      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          246251      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          246245      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          246246      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          246247      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          246248      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          246249      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          246250      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          246251      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2420 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2420 train_time:103ms step_avg:103.30ms
step:2/2420 train_time:192ms step_avg:95.83ms
step:3/2420 train_time:214ms step_avg:71.22ms
step:4/2420 train_time:248ms step_avg:61.89ms
step:5/2420 train_time:305ms step_avg:60.93ms
step:6/2420 train_time:364ms step_avg:60.72ms
step:7/2420 train_time:433ms step_avg:61.93ms
step:8/2420 train_time:493ms step_avg:61.65ms
step:9/2420 train_time:551ms step_avg:61.23ms
step:10/2420 train_time:611ms step_avg:61.13ms
step:11/2420 train_time:670ms step_avg:60.94ms
step:12/2420 train_time:731ms step_avg:60.94ms
step:13/2420 train_time:789ms step_avg:60.72ms
step:14/2420 train_time:850ms step_avg:60.70ms
step:15/2420 train_time:908ms step_avg:60.55ms
step:16/2420 train_time:969ms step_avg:60.57ms
step:17/2420 train_time:1029ms step_avg:60.51ms
step:18/2420 train_time:1091ms step_avg:60.63ms
step:19/2420 train_time:1153ms step_avg:60.68ms
step:20/2420 train_time:1216ms step_avg:60.79ms
step:21/2420 train_time:1276ms step_avg:60.74ms
step:22/2420 train_time:1336ms step_avg:60.73ms
step:23/2420 train_time:1395ms step_avg:60.66ms
step:24/2420 train_time:1456ms step_avg:60.66ms
step:25/2420 train_time:1514ms step_avg:60.57ms
step:26/2420 train_time:1575ms step_avg:60.56ms
step:27/2420 train_time:1633ms step_avg:60.48ms
step:28/2420 train_time:1694ms step_avg:60.50ms
step:29/2420 train_time:1752ms step_avg:60.43ms
step:30/2420 train_time:1813ms step_avg:60.42ms
step:31/2420 train_time:1871ms step_avg:60.34ms
step:32/2420 train_time:1931ms step_avg:60.35ms
step:33/2420 train_time:1990ms step_avg:60.30ms
step:34/2420 train_time:2051ms step_avg:60.31ms
step:35/2420 train_time:2112ms step_avg:60.34ms
step:36/2420 train_time:2175ms step_avg:60.41ms
step:37/2420 train_time:2235ms step_avg:60.40ms
step:38/2420 train_time:2296ms step_avg:60.42ms
step:39/2420 train_time:2355ms step_avg:60.39ms
step:40/2420 train_time:2416ms step_avg:60.39ms
step:41/2420 train_time:2475ms step_avg:60.37ms
step:42/2420 train_time:2536ms step_avg:60.37ms
step:43/2420 train_time:2594ms step_avg:60.33ms
step:44/2420 train_time:2655ms step_avg:60.34ms
step:45/2420 train_time:2714ms step_avg:60.30ms
step:46/2420 train_time:2775ms step_avg:60.32ms
step:47/2420 train_time:2833ms step_avg:60.28ms
step:48/2420 train_time:2894ms step_avg:60.28ms
step:49/2420 train_time:2952ms step_avg:60.25ms
step:50/2420 train_time:3013ms step_avg:60.27ms
step:51/2420 train_time:3074ms step_avg:60.27ms
step:52/2420 train_time:3135ms step_avg:60.29ms
step:53/2420 train_time:3194ms step_avg:60.26ms
step:54/2420 train_time:3255ms step_avg:60.28ms
step:55/2420 train_time:3314ms step_avg:60.26ms
step:56/2420 train_time:3376ms step_avg:60.29ms
step:57/2420 train_time:3435ms step_avg:60.26ms
step:58/2420 train_time:3495ms step_avg:60.26ms
step:59/2420 train_time:3554ms step_avg:60.24ms
step:60/2420 train_time:3614ms step_avg:60.24ms
step:61/2420 train_time:3674ms step_avg:60.23ms
step:62/2420 train_time:3734ms step_avg:60.23ms
step:63/2420 train_time:3793ms step_avg:60.21ms
step:64/2420 train_time:3854ms step_avg:60.21ms
step:65/2420 train_time:3912ms step_avg:60.19ms
step:66/2420 train_time:3973ms step_avg:60.20ms
step:67/2420 train_time:4033ms step_avg:60.20ms
step:68/2420 train_time:4094ms step_avg:60.20ms
step:69/2420 train_time:4154ms step_avg:60.20ms
step:70/2420 train_time:4214ms step_avg:60.20ms
step:71/2420 train_time:4274ms step_avg:60.20ms
step:72/2420 train_time:4335ms step_avg:60.21ms
step:73/2420 train_time:4394ms step_avg:60.19ms
step:74/2420 train_time:4454ms step_avg:60.19ms
step:75/2420 train_time:4513ms step_avg:60.17ms
step:76/2420 train_time:4574ms step_avg:60.18ms
step:77/2420 train_time:4633ms step_avg:60.16ms
step:78/2420 train_time:4693ms step_avg:60.16ms
step:79/2420 train_time:4751ms step_avg:60.14ms
step:80/2420 train_time:4811ms step_avg:60.14ms
step:81/2420 train_time:4870ms step_avg:60.12ms
step:82/2420 train_time:4930ms step_avg:60.13ms
step:83/2420 train_time:4989ms step_avg:60.11ms
step:84/2420 train_time:5050ms step_avg:60.12ms
step:85/2420 train_time:5109ms step_avg:60.11ms
step:86/2420 train_time:5171ms step_avg:60.13ms
step:87/2420 train_time:5231ms step_avg:60.12ms
step:88/2420 train_time:5292ms step_avg:60.14ms
step:89/2420 train_time:5351ms step_avg:60.13ms
step:90/2420 train_time:5412ms step_avg:60.13ms
step:91/2420 train_time:5471ms step_avg:60.12ms
step:92/2420 train_time:5532ms step_avg:60.13ms
step:93/2420 train_time:5591ms step_avg:60.12ms
step:94/2420 train_time:5652ms step_avg:60.12ms
step:95/2420 train_time:5711ms step_avg:60.11ms
step:96/2420 train_time:5771ms step_avg:60.12ms
step:97/2420 train_time:5830ms step_avg:60.10ms
step:98/2420 train_time:5891ms step_avg:60.11ms
step:99/2420 train_time:5949ms step_avg:60.09ms
step:100/2420 train_time:6010ms step_avg:60.10ms
step:101/2420 train_time:6069ms step_avg:60.09ms
step:102/2420 train_time:6131ms step_avg:60.10ms
step:103/2420 train_time:6189ms step_avg:60.09ms
step:104/2420 train_time:6250ms step_avg:60.10ms
step:105/2420 train_time:6310ms step_avg:60.09ms
step:106/2420 train_time:6371ms step_avg:60.10ms
step:107/2420 train_time:6430ms step_avg:60.10ms
step:108/2420 train_time:6491ms step_avg:60.10ms
step:109/2420 train_time:6550ms step_avg:60.09ms
step:110/2420 train_time:6610ms step_avg:60.09ms
step:111/2420 train_time:6669ms step_avg:60.08ms
step:112/2420 train_time:6730ms step_avg:60.09ms
step:113/2420 train_time:6788ms step_avg:60.07ms
step:114/2420 train_time:6848ms step_avg:60.07ms
step:115/2420 train_time:6907ms step_avg:60.07ms
step:116/2420 train_time:6968ms step_avg:60.07ms
step:117/2420 train_time:7027ms step_avg:60.06ms
step:118/2420 train_time:7087ms step_avg:60.06ms
step:119/2420 train_time:7147ms step_avg:60.06ms
step:120/2420 train_time:7208ms step_avg:60.06ms
step:121/2420 train_time:7266ms step_avg:60.05ms
step:122/2420 train_time:7327ms step_avg:60.06ms
step:123/2420 train_time:7387ms step_avg:60.05ms
step:124/2420 train_time:7447ms step_avg:60.06ms
step:125/2420 train_time:7507ms step_avg:60.05ms
step:126/2420 train_time:7567ms step_avg:60.06ms
step:127/2420 train_time:7626ms step_avg:60.05ms
step:128/2420 train_time:7687ms step_avg:60.06ms
step:129/2420 train_time:7746ms step_avg:60.05ms
step:130/2420 train_time:7806ms step_avg:60.05ms
step:131/2420 train_time:7866ms step_avg:60.04ms
step:132/2420 train_time:7927ms step_avg:60.05ms
step:133/2420 train_time:7986ms step_avg:60.05ms
step:134/2420 train_time:8047ms step_avg:60.05ms
step:135/2420 train_time:8106ms step_avg:60.04ms
step:136/2420 train_time:8167ms step_avg:60.05ms
step:137/2420 train_time:8226ms step_avg:60.05ms
step:138/2420 train_time:8287ms step_avg:60.05ms
step:139/2420 train_time:8346ms step_avg:60.04ms
step:140/2420 train_time:8407ms step_avg:60.05ms
step:141/2420 train_time:8467ms step_avg:60.05ms
step:142/2420 train_time:8527ms step_avg:60.05ms
step:143/2420 train_time:8587ms step_avg:60.05ms
step:144/2420 train_time:8647ms step_avg:60.05ms
step:145/2420 train_time:8706ms step_avg:60.04ms
step:146/2420 train_time:8767ms step_avg:60.05ms
step:147/2420 train_time:8826ms step_avg:60.04ms
step:148/2420 train_time:8887ms step_avg:60.05ms
step:149/2420 train_time:8945ms step_avg:60.04ms
step:150/2420 train_time:9007ms step_avg:60.04ms
step:151/2420 train_time:9065ms step_avg:60.04ms
step:152/2420 train_time:9126ms step_avg:60.04ms
step:153/2420 train_time:9185ms step_avg:60.03ms
step:154/2420 train_time:9245ms step_avg:60.03ms
step:155/2420 train_time:9305ms step_avg:60.03ms
step:156/2420 train_time:9366ms step_avg:60.04ms
step:157/2420 train_time:9425ms step_avg:60.03ms
step:158/2420 train_time:9486ms step_avg:60.04ms
step:159/2420 train_time:9545ms step_avg:60.03ms
step:160/2420 train_time:9606ms step_avg:60.04ms
step:161/2420 train_time:9665ms step_avg:60.03ms
step:162/2420 train_time:9725ms step_avg:60.03ms
step:163/2420 train_time:9784ms step_avg:60.03ms
step:164/2420 train_time:9845ms step_avg:60.03ms
step:165/2420 train_time:9904ms step_avg:60.02ms
step:166/2420 train_time:9965ms step_avg:60.03ms
step:167/2420 train_time:10023ms step_avg:60.02ms
step:168/2420 train_time:10084ms step_avg:60.03ms
step:169/2420 train_time:10143ms step_avg:60.02ms
step:170/2420 train_time:10204ms step_avg:60.02ms
step:171/2420 train_time:10263ms step_avg:60.02ms
step:172/2420 train_time:10323ms step_avg:60.02ms
step:173/2420 train_time:10382ms step_avg:60.01ms
step:174/2420 train_time:10443ms step_avg:60.02ms
step:175/2420 train_time:10501ms step_avg:60.01ms
step:176/2420 train_time:10563ms step_avg:60.01ms
step:177/2420 train_time:10621ms step_avg:60.00ms
step:178/2420 train_time:10682ms step_avg:60.01ms
step:179/2420 train_time:10741ms step_avg:60.01ms
step:180/2420 train_time:10802ms step_avg:60.01ms
step:181/2420 train_time:10860ms step_avg:60.00ms
step:182/2420 train_time:10921ms step_avg:60.00ms
step:183/2420 train_time:10979ms step_avg:59.99ms
step:184/2420 train_time:11039ms step_avg:60.00ms
step:185/2420 train_time:11098ms step_avg:59.99ms
step:186/2420 train_time:11158ms step_avg:59.99ms
step:187/2420 train_time:11216ms step_avg:59.98ms
step:188/2420 train_time:11277ms step_avg:59.98ms
step:189/2420 train_time:11335ms step_avg:59.97ms
step:190/2420 train_time:11395ms step_avg:59.98ms
step:191/2420 train_time:11454ms step_avg:59.97ms
step:192/2420 train_time:11514ms step_avg:59.97ms
step:193/2420 train_time:11572ms step_avg:59.96ms
step:194/2420 train_time:11633ms step_avg:59.97ms
step:195/2420 train_time:11692ms step_avg:59.96ms
step:196/2420 train_time:11752ms step_avg:59.96ms
step:197/2420 train_time:11810ms step_avg:59.95ms
step:198/2420 train_time:11872ms step_avg:59.96ms
step:199/2420 train_time:11930ms step_avg:59.95ms
step:200/2420 train_time:11990ms step_avg:59.95ms
step:201/2420 train_time:12048ms step_avg:59.94ms
step:202/2420 train_time:12109ms step_avg:59.95ms
step:203/2420 train_time:12168ms step_avg:59.94ms
step:204/2420 train_time:12229ms step_avg:59.95ms
step:205/2420 train_time:12288ms step_avg:59.94ms
step:206/2420 train_time:12348ms step_avg:59.94ms
step:207/2420 train_time:12407ms step_avg:59.94ms
step:208/2420 train_time:12468ms step_avg:59.94ms
step:209/2420 train_time:12527ms step_avg:59.94ms
step:210/2420 train_time:12588ms step_avg:59.94ms
step:211/2420 train_time:12647ms step_avg:59.94ms
step:212/2420 train_time:12707ms step_avg:59.94ms
step:213/2420 train_time:12766ms step_avg:59.93ms
step:214/2420 train_time:12827ms step_avg:59.94ms
step:215/2420 train_time:12886ms step_avg:59.93ms
step:216/2420 train_time:12945ms step_avg:59.93ms
step:217/2420 train_time:13004ms step_avg:59.92ms
step:218/2420 train_time:13065ms step_avg:59.93ms
step:219/2420 train_time:13124ms step_avg:59.92ms
step:220/2420 train_time:13184ms step_avg:59.93ms
step:221/2420 train_time:13242ms step_avg:59.92ms
step:222/2420 train_time:13303ms step_avg:59.92ms
step:223/2420 train_time:13362ms step_avg:59.92ms
step:224/2420 train_time:13422ms step_avg:59.92ms
step:225/2420 train_time:13481ms step_avg:59.92ms
step:226/2420 train_time:13541ms step_avg:59.92ms
step:227/2420 train_time:13600ms step_avg:59.91ms
step:228/2420 train_time:13661ms step_avg:59.92ms
step:229/2420 train_time:13719ms step_avg:59.91ms
step:230/2420 train_time:13780ms step_avg:59.91ms
step:231/2420 train_time:13839ms step_avg:59.91ms
step:232/2420 train_time:13899ms step_avg:59.91ms
step:233/2420 train_time:13957ms step_avg:59.90ms
step:234/2420 train_time:14018ms step_avg:59.90ms
step:235/2420 train_time:14076ms step_avg:59.90ms
step:236/2420 train_time:14137ms step_avg:59.90ms
step:237/2420 train_time:14195ms step_avg:59.89ms
step:238/2420 train_time:14255ms step_avg:59.90ms
step:239/2420 train_time:14314ms step_avg:59.89ms
step:240/2420 train_time:14374ms step_avg:59.89ms
step:241/2420 train_time:14433ms step_avg:59.89ms
step:242/2420 train_time:14494ms step_avg:59.89ms
step:243/2420 train_time:14552ms step_avg:59.88ms
step:244/2420 train_time:14613ms step_avg:59.89ms
step:245/2420 train_time:14673ms step_avg:59.89ms
step:246/2420 train_time:14733ms step_avg:59.89ms
step:247/2420 train_time:14792ms step_avg:59.89ms
step:248/2420 train_time:14852ms step_avg:59.89ms
step:249/2420 train_time:14910ms step_avg:59.88ms
step:250/2420 train_time:14970ms step_avg:59.88ms
step:250/2420 val_loss:4.0812 train_time:15033ms step_avg:60.13ms
step:251/2420 train_time:15054ms step_avg:59.98ms
step:252/2420 train_time:15092ms step_avg:59.89ms
step:253/2420 train_time:15153ms step_avg:59.89ms
step:254/2420 train_time:15218ms step_avg:59.91ms
step:255/2420 train_time:15279ms step_avg:59.92ms
step:256/2420 train_time:15339ms step_avg:59.92ms
step:257/2420 train_time:15397ms step_avg:59.91ms
step:258/2420 train_time:15457ms step_avg:59.91ms
step:259/2420 train_time:15515ms step_avg:59.90ms
step:260/2420 train_time:15574ms step_avg:59.90ms
step:261/2420 train_time:15632ms step_avg:59.89ms
step:262/2420 train_time:15692ms step_avg:59.89ms
step:263/2420 train_time:15750ms step_avg:59.88ms
step:264/2420 train_time:15810ms step_avg:59.88ms
step:265/2420 train_time:15867ms step_avg:59.88ms
step:266/2420 train_time:15927ms step_avg:59.88ms
step:267/2420 train_time:15985ms step_avg:59.87ms
step:268/2420 train_time:16045ms step_avg:59.87ms
step:269/2420 train_time:16104ms step_avg:59.87ms
step:270/2420 train_time:16166ms step_avg:59.88ms
step:271/2420 train_time:16226ms step_avg:59.88ms
step:272/2420 train_time:16287ms step_avg:59.88ms
step:273/2420 train_time:16346ms step_avg:59.88ms
step:274/2420 train_time:16407ms step_avg:59.88ms
step:275/2420 train_time:16466ms step_avg:59.88ms
step:276/2420 train_time:16527ms step_avg:59.88ms
step:277/2420 train_time:16585ms step_avg:59.88ms
step:278/2420 train_time:16645ms step_avg:59.88ms
step:279/2420 train_time:16704ms step_avg:59.87ms
step:280/2420 train_time:16763ms step_avg:59.87ms
step:281/2420 train_time:16821ms step_avg:59.86ms
step:282/2420 train_time:16881ms step_avg:59.86ms
step:283/2420 train_time:16939ms step_avg:59.85ms
step:284/2420 train_time:16999ms step_avg:59.86ms
step:285/2420 train_time:17058ms step_avg:59.85ms
step:286/2420 train_time:17119ms step_avg:59.86ms
step:287/2420 train_time:17179ms step_avg:59.86ms
step:288/2420 train_time:17241ms step_avg:59.86ms
step:289/2420 train_time:17300ms step_avg:59.86ms
step:290/2420 train_time:17360ms step_avg:59.86ms
step:291/2420 train_time:17419ms step_avg:59.86ms
step:292/2420 train_time:17479ms step_avg:59.86ms
step:293/2420 train_time:17538ms step_avg:59.86ms
step:294/2420 train_time:17599ms step_avg:59.86ms
step:295/2420 train_time:17657ms step_avg:59.86ms
step:296/2420 train_time:17718ms step_avg:59.86ms
step:297/2420 train_time:17777ms step_avg:59.86ms
step:298/2420 train_time:17837ms step_avg:59.86ms
step:299/2420 train_time:17895ms step_avg:59.85ms
step:300/2420 train_time:17955ms step_avg:59.85ms
step:301/2420 train_time:18013ms step_avg:59.84ms
step:302/2420 train_time:18073ms step_avg:59.85ms
step:303/2420 train_time:18132ms step_avg:59.84ms
step:304/2420 train_time:18192ms step_avg:59.84ms
step:305/2420 train_time:18250ms step_avg:59.84ms
step:306/2420 train_time:18311ms step_avg:59.84ms
step:307/2420 train_time:18369ms step_avg:59.83ms
step:308/2420 train_time:18429ms step_avg:59.84ms
step:309/2420 train_time:18488ms step_avg:59.83ms
step:310/2420 train_time:18549ms step_avg:59.83ms
step:311/2420 train_time:18608ms step_avg:59.83ms
step:312/2420 train_time:18669ms step_avg:59.83ms
step:313/2420 train_time:18728ms step_avg:59.83ms
step:314/2420 train_time:18788ms step_avg:59.83ms
step:315/2420 train_time:18847ms step_avg:59.83ms
step:316/2420 train_time:18907ms step_avg:59.83ms
step:317/2420 train_time:18965ms step_avg:59.83ms
step:318/2420 train_time:19025ms step_avg:59.83ms
step:319/2420 train_time:19083ms step_avg:59.82ms
step:320/2420 train_time:19143ms step_avg:59.82ms
step:321/2420 train_time:19202ms step_avg:59.82ms
step:322/2420 train_time:19263ms step_avg:59.82ms
step:323/2420 train_time:19322ms step_avg:59.82ms
step:324/2420 train_time:19382ms step_avg:59.82ms
step:325/2420 train_time:19442ms step_avg:59.82ms
step:326/2420 train_time:19503ms step_avg:59.83ms
step:327/2420 train_time:19562ms step_avg:59.82ms
step:328/2420 train_time:19623ms step_avg:59.83ms
step:329/2420 train_time:19682ms step_avg:59.82ms
step:330/2420 train_time:19743ms step_avg:59.83ms
step:331/2420 train_time:19802ms step_avg:59.82ms
step:332/2420 train_time:19862ms step_avg:59.83ms
step:333/2420 train_time:19921ms step_avg:59.82ms
step:334/2420 train_time:19981ms step_avg:59.82ms
step:335/2420 train_time:20039ms step_avg:59.82ms
step:336/2420 train_time:20099ms step_avg:59.82ms
step:337/2420 train_time:20158ms step_avg:59.82ms
step:338/2420 train_time:20219ms step_avg:59.82ms
step:339/2420 train_time:20277ms step_avg:59.82ms
step:340/2420 train_time:20338ms step_avg:59.82ms
step:341/2420 train_time:20397ms step_avg:59.82ms
step:342/2420 train_time:20458ms step_avg:59.82ms
step:343/2420 train_time:20517ms step_avg:59.82ms
step:344/2420 train_time:20578ms step_avg:59.82ms
step:345/2420 train_time:20637ms step_avg:59.82ms
step:346/2420 train_time:20698ms step_avg:59.82ms
step:347/2420 train_time:20756ms step_avg:59.82ms
step:348/2420 train_time:20817ms step_avg:59.82ms
step:349/2420 train_time:20875ms step_avg:59.81ms
step:350/2420 train_time:20935ms step_avg:59.81ms
step:351/2420 train_time:20993ms step_avg:59.81ms
step:352/2420 train_time:21054ms step_avg:59.81ms
step:353/2420 train_time:21112ms step_avg:59.81ms
step:354/2420 train_time:21172ms step_avg:59.81ms
step:355/2420 train_time:21230ms step_avg:59.80ms
step:356/2420 train_time:21290ms step_avg:59.80ms
step:357/2420 train_time:21348ms step_avg:59.80ms
step:358/2420 train_time:21409ms step_avg:59.80ms
step:359/2420 train_time:21467ms step_avg:59.80ms
step:360/2420 train_time:21528ms step_avg:59.80ms
step:361/2420 train_time:21586ms step_avg:59.80ms
step:362/2420 train_time:21647ms step_avg:59.80ms
step:363/2420 train_time:21706ms step_avg:59.80ms
step:364/2420 train_time:21767ms step_avg:59.80ms
step:365/2420 train_time:21826ms step_avg:59.80ms
step:366/2420 train_time:21887ms step_avg:59.80ms
step:367/2420 train_time:21946ms step_avg:59.80ms
step:368/2420 train_time:22006ms step_avg:59.80ms
step:369/2420 train_time:22065ms step_avg:59.80ms
step:370/2420 train_time:22125ms step_avg:59.80ms
step:371/2420 train_time:22183ms step_avg:59.79ms
step:372/2420 train_time:22244ms step_avg:59.80ms
step:373/2420 train_time:22304ms step_avg:59.80ms
step:374/2420 train_time:22365ms step_avg:59.80ms
step:375/2420 train_time:22424ms step_avg:59.80ms
step:376/2420 train_time:22485ms step_avg:59.80ms
step:377/2420 train_time:22543ms step_avg:59.80ms
step:378/2420 train_time:22604ms step_avg:59.80ms
step:379/2420 train_time:22662ms step_avg:59.80ms
step:380/2420 train_time:22723ms step_avg:59.80ms
step:381/2420 train_time:22781ms step_avg:59.79ms
step:382/2420 train_time:22842ms step_avg:59.79ms
step:383/2420 train_time:22900ms step_avg:59.79ms
step:384/2420 train_time:22960ms step_avg:59.79ms
step:385/2420 train_time:23018ms step_avg:59.79ms
step:386/2420 train_time:23079ms step_avg:59.79ms
step:387/2420 train_time:23137ms step_avg:59.79ms
step:388/2420 train_time:23198ms step_avg:59.79ms
step:389/2420 train_time:23256ms step_avg:59.78ms
step:390/2420 train_time:23317ms step_avg:59.79ms
step:391/2420 train_time:23376ms step_avg:59.78ms
step:392/2420 train_time:23436ms step_avg:59.79ms
step:393/2420 train_time:23495ms step_avg:59.78ms
step:394/2420 train_time:23556ms step_avg:59.79ms
step:395/2420 train_time:23615ms step_avg:59.78ms
step:396/2420 train_time:23675ms step_avg:59.79ms
step:397/2420 train_time:23733ms step_avg:59.78ms
step:398/2420 train_time:23794ms step_avg:59.78ms
step:399/2420 train_time:23852ms step_avg:59.78ms
step:400/2420 train_time:23912ms step_avg:59.78ms
step:401/2420 train_time:23970ms step_avg:59.78ms
step:402/2420 train_time:24030ms step_avg:59.78ms
step:403/2420 train_time:24088ms step_avg:59.77ms
step:404/2420 train_time:24149ms step_avg:59.77ms
step:405/2420 train_time:24207ms step_avg:59.77ms
step:406/2420 train_time:24268ms step_avg:59.77ms
step:407/2420 train_time:24326ms step_avg:59.77ms
step:408/2420 train_time:24386ms step_avg:59.77ms
step:409/2420 train_time:24444ms step_avg:59.77ms
step:410/2420 train_time:24505ms step_avg:59.77ms
step:411/2420 train_time:24564ms step_avg:59.77ms
step:412/2420 train_time:24624ms step_avg:59.77ms
step:413/2420 train_time:24682ms step_avg:59.76ms
step:414/2420 train_time:24743ms step_avg:59.77ms
step:415/2420 train_time:24802ms step_avg:59.76ms
step:416/2420 train_time:24862ms step_avg:59.76ms
step:417/2420 train_time:24921ms step_avg:59.76ms
step:418/2420 train_time:24981ms step_avg:59.76ms
step:419/2420 train_time:25040ms step_avg:59.76ms
step:420/2420 train_time:25101ms step_avg:59.76ms
step:421/2420 train_time:25160ms step_avg:59.76ms
step:422/2420 train_time:25220ms step_avg:59.76ms
step:423/2420 train_time:25278ms step_avg:59.76ms
step:424/2420 train_time:25339ms step_avg:59.76ms
step:425/2420 train_time:25397ms step_avg:59.76ms
step:426/2420 train_time:25458ms step_avg:59.76ms
step:427/2420 train_time:25517ms step_avg:59.76ms
step:428/2420 train_time:25578ms step_avg:59.76ms
step:429/2420 train_time:25636ms step_avg:59.76ms
step:430/2420 train_time:25697ms step_avg:59.76ms
step:431/2420 train_time:25755ms step_avg:59.76ms
step:432/2420 train_time:25816ms step_avg:59.76ms
step:433/2420 train_time:25875ms step_avg:59.76ms
step:434/2420 train_time:25935ms step_avg:59.76ms
step:435/2420 train_time:25993ms step_avg:59.75ms
step:436/2420 train_time:26054ms step_avg:59.76ms
step:437/2420 train_time:26113ms step_avg:59.75ms
step:438/2420 train_time:26173ms step_avg:59.75ms
step:439/2420 train_time:26231ms step_avg:59.75ms
step:440/2420 train_time:26291ms step_avg:59.75ms
step:441/2420 train_time:26349ms step_avg:59.75ms
step:442/2420 train_time:26409ms step_avg:59.75ms
step:443/2420 train_time:26468ms step_avg:59.75ms
step:444/2420 train_time:26528ms step_avg:59.75ms
step:445/2420 train_time:26587ms step_avg:59.75ms
step:446/2420 train_time:26648ms step_avg:59.75ms
step:447/2420 train_time:26707ms step_avg:59.75ms
step:448/2420 train_time:26768ms step_avg:59.75ms
step:449/2420 train_time:26827ms step_avg:59.75ms
step:450/2420 train_time:26888ms step_avg:59.75ms
step:451/2420 train_time:26947ms step_avg:59.75ms
step:452/2420 train_time:27007ms step_avg:59.75ms
step:453/2420 train_time:27066ms step_avg:59.75ms
step:454/2420 train_time:27127ms step_avg:59.75ms
step:455/2420 train_time:27185ms step_avg:59.75ms
step:456/2420 train_time:27245ms step_avg:59.75ms
step:457/2420 train_time:27304ms step_avg:59.75ms
step:458/2420 train_time:27364ms step_avg:59.75ms
step:459/2420 train_time:27422ms step_avg:59.74ms
step:460/2420 train_time:27482ms step_avg:59.74ms
step:461/2420 train_time:27541ms step_avg:59.74ms
step:462/2420 train_time:27602ms step_avg:59.74ms
step:463/2420 train_time:27661ms step_avg:59.74ms
step:464/2420 train_time:27721ms step_avg:59.74ms
step:465/2420 train_time:27780ms step_avg:59.74ms
step:466/2420 train_time:27841ms step_avg:59.74ms
step:467/2420 train_time:27900ms step_avg:59.74ms
step:468/2420 train_time:27961ms step_avg:59.75ms
step:469/2420 train_time:28019ms step_avg:59.74ms
step:470/2420 train_time:28079ms step_avg:59.74ms
step:471/2420 train_time:28138ms step_avg:59.74ms
step:472/2420 train_time:28198ms step_avg:59.74ms
step:473/2420 train_time:28257ms step_avg:59.74ms
step:474/2420 train_time:28318ms step_avg:59.74ms
step:475/2420 train_time:28376ms step_avg:59.74ms
step:476/2420 train_time:28436ms step_avg:59.74ms
step:477/2420 train_time:28495ms step_avg:59.74ms
step:478/2420 train_time:28555ms step_avg:59.74ms
step:479/2420 train_time:28615ms step_avg:59.74ms
step:480/2420 train_time:28675ms step_avg:59.74ms
step:481/2420 train_time:28734ms step_avg:59.74ms
step:482/2420 train_time:28794ms step_avg:59.74ms
step:483/2420 train_time:28853ms step_avg:59.74ms
step:484/2420 train_time:28914ms step_avg:59.74ms
step:485/2420 train_time:28972ms step_avg:59.74ms
step:486/2420 train_time:29033ms step_avg:59.74ms
step:487/2420 train_time:29091ms step_avg:59.74ms
step:488/2420 train_time:29151ms step_avg:59.74ms
step:489/2420 train_time:29210ms step_avg:59.73ms
step:490/2420 train_time:29270ms step_avg:59.73ms
step:491/2420 train_time:29328ms step_avg:59.73ms
step:492/2420 train_time:29388ms step_avg:59.73ms
step:493/2420 train_time:29446ms step_avg:59.73ms
step:494/2420 train_time:29507ms step_avg:59.73ms
step:495/2420 train_time:29565ms step_avg:59.73ms
step:496/2420 train_time:29626ms step_avg:59.73ms
step:497/2420 train_time:29685ms step_avg:59.73ms
step:498/2420 train_time:29746ms step_avg:59.73ms
step:499/2420 train_time:29804ms step_avg:59.73ms
step:500/2420 train_time:29865ms step_avg:59.73ms
step:500/2420 val_loss:3.8263 train_time:29927ms step_avg:59.85ms
step:501/2420 train_time:29949ms step_avg:59.78ms
step:502/2420 train_time:29988ms step_avg:59.74ms
step:503/2420 train_time:30049ms step_avg:59.74ms
step:504/2420 train_time:30112ms step_avg:59.75ms
step:505/2420 train_time:30171ms step_avg:59.74ms
step:506/2420 train_time:30231ms step_avg:59.74ms
step:507/2420 train_time:30289ms step_avg:59.74ms
step:508/2420 train_time:30349ms step_avg:59.74ms
step:509/2420 train_time:30407ms step_avg:59.74ms
step:510/2420 train_time:30466ms step_avg:59.74ms
step:511/2420 train_time:30524ms step_avg:59.73ms
step:512/2420 train_time:30584ms step_avg:59.73ms
step:513/2420 train_time:30641ms step_avg:59.73ms
step:514/2420 train_time:30701ms step_avg:59.73ms
step:515/2420 train_time:30759ms step_avg:59.73ms
step:516/2420 train_time:30819ms step_avg:59.73ms
step:517/2420 train_time:30878ms step_avg:59.73ms
step:518/2420 train_time:30939ms step_avg:59.73ms
step:519/2420 train_time:31000ms step_avg:59.73ms
step:520/2420 train_time:31061ms step_avg:59.73ms
step:521/2420 train_time:31120ms step_avg:59.73ms
step:522/2420 train_time:31181ms step_avg:59.73ms
step:523/2420 train_time:31240ms step_avg:59.73ms
step:524/2420 train_time:31300ms step_avg:59.73ms
step:525/2420 train_time:31360ms step_avg:59.73ms
step:526/2420 train_time:31420ms step_avg:59.73ms
step:527/2420 train_time:31478ms step_avg:59.73ms
step:528/2420 train_time:31538ms step_avg:59.73ms
step:529/2420 train_time:31596ms step_avg:59.73ms
step:530/2420 train_time:31656ms step_avg:59.73ms
step:531/2420 train_time:31714ms step_avg:59.73ms
step:532/2420 train_time:31774ms step_avg:59.73ms
step:533/2420 train_time:31832ms step_avg:59.72ms
step:534/2420 train_time:31894ms step_avg:59.73ms
step:535/2420 train_time:31953ms step_avg:59.72ms
step:536/2420 train_time:32014ms step_avg:59.73ms
step:537/2420 train_time:32074ms step_avg:59.73ms
step:538/2420 train_time:32135ms step_avg:59.73ms
step:539/2420 train_time:32194ms step_avg:59.73ms
step:540/2420 train_time:32255ms step_avg:59.73ms
step:541/2420 train_time:32314ms step_avg:59.73ms
step:542/2420 train_time:32374ms step_avg:59.73ms
step:543/2420 train_time:32433ms step_avg:59.73ms
step:544/2420 train_time:32493ms step_avg:59.73ms
step:545/2420 train_time:32551ms step_avg:59.73ms
step:546/2420 train_time:32611ms step_avg:59.73ms
step:547/2420 train_time:32670ms step_avg:59.73ms
step:548/2420 train_time:32730ms step_avg:59.73ms
step:549/2420 train_time:32787ms step_avg:59.72ms
step:550/2420 train_time:32847ms step_avg:59.72ms
step:551/2420 train_time:32906ms step_avg:59.72ms
step:552/2420 train_time:32967ms step_avg:59.72ms
step:553/2420 train_time:33026ms step_avg:59.72ms
step:554/2420 train_time:33086ms step_avg:59.72ms
step:555/2420 train_time:33146ms step_avg:59.72ms
step:556/2420 train_time:33206ms step_avg:59.72ms
step:557/2420 train_time:33265ms step_avg:59.72ms
step:558/2420 train_time:33325ms step_avg:59.72ms
step:559/2420 train_time:33384ms step_avg:59.72ms
step:560/2420 train_time:33444ms step_avg:59.72ms
step:561/2420 train_time:33502ms step_avg:59.72ms
step:562/2420 train_time:33563ms step_avg:59.72ms
step:563/2420 train_time:33621ms step_avg:59.72ms
step:564/2420 train_time:33682ms step_avg:59.72ms
step:565/2420 train_time:33740ms step_avg:59.72ms
step:566/2420 train_time:33801ms step_avg:59.72ms
step:567/2420 train_time:33860ms step_avg:59.72ms
step:568/2420 train_time:33921ms step_avg:59.72ms
step:569/2420 train_time:33979ms step_avg:59.72ms
step:570/2420 train_time:34040ms step_avg:59.72ms
step:571/2420 train_time:34099ms step_avg:59.72ms
step:572/2420 train_time:34160ms step_avg:59.72ms
step:573/2420 train_time:34218ms step_avg:59.72ms
step:574/2420 train_time:34279ms step_avg:59.72ms
step:575/2420 train_time:34338ms step_avg:59.72ms
step:576/2420 train_time:34398ms step_avg:59.72ms
step:577/2420 train_time:34457ms step_avg:59.72ms
step:578/2420 train_time:34518ms step_avg:59.72ms
step:579/2420 train_time:34575ms step_avg:59.72ms
step:580/2420 train_time:34636ms step_avg:59.72ms
step:581/2420 train_time:34694ms step_avg:59.71ms
step:582/2420 train_time:34755ms step_avg:59.72ms
step:583/2420 train_time:34813ms step_avg:59.71ms
step:584/2420 train_time:34874ms step_avg:59.72ms
step:585/2420 train_time:34932ms step_avg:59.71ms
step:586/2420 train_time:34993ms step_avg:59.72ms
step:587/2420 train_time:35053ms step_avg:59.72ms
step:588/2420 train_time:35114ms step_avg:59.72ms
step:589/2420 train_time:35173ms step_avg:59.72ms
step:590/2420 train_time:35234ms step_avg:59.72ms
step:591/2420 train_time:35293ms step_avg:59.72ms
step:592/2420 train_time:35354ms step_avg:59.72ms
step:593/2420 train_time:35413ms step_avg:59.72ms
step:594/2420 train_time:35473ms step_avg:59.72ms
step:595/2420 train_time:35531ms step_avg:59.72ms
step:596/2420 train_time:35591ms step_avg:59.72ms
step:597/2420 train_time:35650ms step_avg:59.72ms
step:598/2420 train_time:35710ms step_avg:59.72ms
step:599/2420 train_time:35769ms step_avg:59.71ms
step:600/2420 train_time:35829ms step_avg:59.72ms
step:601/2420 train_time:35887ms step_avg:59.71ms
step:602/2420 train_time:35948ms step_avg:59.71ms
step:603/2420 train_time:36007ms step_avg:59.71ms
step:604/2420 train_time:36067ms step_avg:59.71ms
step:605/2420 train_time:36126ms step_avg:59.71ms
step:606/2420 train_time:36186ms step_avg:59.71ms
step:607/2420 train_time:36245ms step_avg:59.71ms
step:608/2420 train_time:36305ms step_avg:59.71ms
step:609/2420 train_time:36364ms step_avg:59.71ms
step:610/2420 train_time:36424ms step_avg:59.71ms
step:611/2420 train_time:36482ms step_avg:59.71ms
step:612/2420 train_time:36542ms step_avg:59.71ms
step:613/2420 train_time:36601ms step_avg:59.71ms
step:614/2420 train_time:36662ms step_avg:59.71ms
step:615/2420 train_time:36720ms step_avg:59.71ms
step:616/2420 train_time:36781ms step_avg:59.71ms
step:617/2420 train_time:36839ms step_avg:59.71ms
step:618/2420 train_time:36900ms step_avg:59.71ms
step:619/2420 train_time:36959ms step_avg:59.71ms
step:620/2420 train_time:37019ms step_avg:59.71ms
step:621/2420 train_time:37078ms step_avg:59.71ms
step:622/2420 train_time:37138ms step_avg:59.71ms
step:623/2420 train_time:37197ms step_avg:59.71ms
step:624/2420 train_time:37259ms step_avg:59.71ms
step:625/2420 train_time:37318ms step_avg:59.71ms
step:626/2420 train_time:37378ms step_avg:59.71ms
step:627/2420 train_time:37437ms step_avg:59.71ms
step:628/2420 train_time:37498ms step_avg:59.71ms
step:629/2420 train_time:37557ms step_avg:59.71ms
step:630/2420 train_time:37617ms step_avg:59.71ms
step:631/2420 train_time:37675ms step_avg:59.71ms
step:632/2420 train_time:37736ms step_avg:59.71ms
step:633/2420 train_time:37795ms step_avg:59.71ms
step:634/2420 train_time:37856ms step_avg:59.71ms
step:635/2420 train_time:37915ms step_avg:59.71ms
step:636/2420 train_time:37975ms step_avg:59.71ms
step:637/2420 train_time:38034ms step_avg:59.71ms
step:638/2420 train_time:38094ms step_avg:59.71ms
step:639/2420 train_time:38153ms step_avg:59.71ms
step:640/2420 train_time:38214ms step_avg:59.71ms
step:641/2420 train_time:38273ms step_avg:59.71ms
step:642/2420 train_time:38334ms step_avg:59.71ms
step:643/2420 train_time:38393ms step_avg:59.71ms
step:644/2420 train_time:38453ms step_avg:59.71ms
step:645/2420 train_time:38511ms step_avg:59.71ms
step:646/2420 train_time:38572ms step_avg:59.71ms
step:647/2420 train_time:38631ms step_avg:59.71ms
step:648/2420 train_time:38691ms step_avg:59.71ms
step:649/2420 train_time:38749ms step_avg:59.71ms
step:650/2420 train_time:38810ms step_avg:59.71ms
step:651/2420 train_time:38868ms step_avg:59.71ms
step:652/2420 train_time:38929ms step_avg:59.71ms
step:653/2420 train_time:38988ms step_avg:59.71ms
step:654/2420 train_time:39049ms step_avg:59.71ms
step:655/2420 train_time:39107ms step_avg:59.71ms
step:656/2420 train_time:39167ms step_avg:59.71ms
step:657/2420 train_time:39226ms step_avg:59.71ms
step:658/2420 train_time:39287ms step_avg:59.71ms
step:659/2420 train_time:39346ms step_avg:59.71ms
step:660/2420 train_time:39406ms step_avg:59.71ms
step:661/2420 train_time:39464ms step_avg:59.70ms
step:662/2420 train_time:39525ms step_avg:59.70ms
step:663/2420 train_time:39583ms step_avg:59.70ms
step:664/2420 train_time:39643ms step_avg:59.70ms
step:665/2420 train_time:39702ms step_avg:59.70ms
step:666/2420 train_time:39763ms step_avg:59.70ms
step:667/2420 train_time:39821ms step_avg:59.70ms
step:668/2420 train_time:39882ms step_avg:59.70ms
step:669/2420 train_time:39941ms step_avg:59.70ms
step:670/2420 train_time:40002ms step_avg:59.70ms
step:671/2420 train_time:40061ms step_avg:59.70ms
step:672/2420 train_time:40121ms step_avg:59.70ms
step:673/2420 train_time:40180ms step_avg:59.70ms
step:674/2420 train_time:40241ms step_avg:59.70ms
step:675/2420 train_time:40299ms step_avg:59.70ms
step:676/2420 train_time:40360ms step_avg:59.70ms
step:677/2420 train_time:40419ms step_avg:59.70ms
step:678/2420 train_time:40480ms step_avg:59.70ms
step:679/2420 train_time:40538ms step_avg:59.70ms
step:680/2420 train_time:40599ms step_avg:59.70ms
step:681/2420 train_time:40659ms step_avg:59.70ms
step:682/2420 train_time:40719ms step_avg:59.71ms
step:683/2420 train_time:40778ms step_avg:59.70ms
step:684/2420 train_time:40838ms step_avg:59.70ms
step:685/2420 train_time:40898ms step_avg:59.70ms
step:686/2420 train_time:40958ms step_avg:59.71ms
step:687/2420 train_time:41016ms step_avg:59.70ms
step:688/2420 train_time:41077ms step_avg:59.70ms
step:689/2420 train_time:41135ms step_avg:59.70ms
step:690/2420 train_time:41196ms step_avg:59.70ms
step:691/2420 train_time:41255ms step_avg:59.70ms
step:692/2420 train_time:41315ms step_avg:59.70ms
step:693/2420 train_time:41375ms step_avg:59.70ms
step:694/2420 train_time:41436ms step_avg:59.71ms
step:695/2420 train_time:41495ms step_avg:59.70ms
step:696/2420 train_time:41555ms step_avg:59.71ms
step:697/2420 train_time:41614ms step_avg:59.70ms
step:698/2420 train_time:41675ms step_avg:59.71ms
step:699/2420 train_time:41733ms step_avg:59.70ms
step:700/2420 train_time:41794ms step_avg:59.71ms
step:701/2420 train_time:41852ms step_avg:59.70ms
step:702/2420 train_time:41912ms step_avg:59.70ms
step:703/2420 train_time:41971ms step_avg:59.70ms
step:704/2420 train_time:42032ms step_avg:59.70ms
step:705/2420 train_time:42091ms step_avg:59.70ms
step:706/2420 train_time:42151ms step_avg:59.70ms
step:707/2420 train_time:42210ms step_avg:59.70ms
step:708/2420 train_time:42271ms step_avg:59.70ms
step:709/2420 train_time:42330ms step_avg:59.70ms
step:710/2420 train_time:42391ms step_avg:59.71ms
step:711/2420 train_time:42449ms step_avg:59.70ms
step:712/2420 train_time:42510ms step_avg:59.71ms
step:713/2420 train_time:42569ms step_avg:59.70ms
step:714/2420 train_time:42629ms step_avg:59.70ms
step:715/2420 train_time:42688ms step_avg:59.70ms
step:716/2420 train_time:42748ms step_avg:59.70ms
step:717/2420 train_time:42807ms step_avg:59.70ms
step:718/2420 train_time:42867ms step_avg:59.70ms
step:719/2420 train_time:42926ms step_avg:59.70ms
step:720/2420 train_time:42986ms step_avg:59.70ms
step:721/2420 train_time:43044ms step_avg:59.70ms
step:722/2420 train_time:43104ms step_avg:59.70ms
step:723/2420 train_time:43163ms step_avg:59.70ms
step:724/2420 train_time:43223ms step_avg:59.70ms
step:725/2420 train_time:43282ms step_avg:59.70ms
step:726/2420 train_time:43342ms step_avg:59.70ms
step:727/2420 train_time:43401ms step_avg:59.70ms
step:728/2420 train_time:43461ms step_avg:59.70ms
step:729/2420 train_time:43520ms step_avg:59.70ms
step:730/2420 train_time:43580ms step_avg:59.70ms
step:731/2420 train_time:43638ms step_avg:59.70ms
step:732/2420 train_time:43699ms step_avg:59.70ms
step:733/2420 train_time:43757ms step_avg:59.70ms
step:734/2420 train_time:43818ms step_avg:59.70ms
step:735/2420 train_time:43876ms step_avg:59.70ms
step:736/2420 train_time:43937ms step_avg:59.70ms
step:737/2420 train_time:43996ms step_avg:59.70ms
step:738/2420 train_time:44058ms step_avg:59.70ms
step:739/2420 train_time:44116ms step_avg:59.70ms
step:740/2420 train_time:44177ms step_avg:59.70ms
step:741/2420 train_time:44236ms step_avg:59.70ms
step:742/2420 train_time:44297ms step_avg:59.70ms
step:743/2420 train_time:44356ms step_avg:59.70ms
step:744/2420 train_time:44417ms step_avg:59.70ms
step:745/2420 train_time:44476ms step_avg:59.70ms
step:746/2420 train_time:44536ms step_avg:59.70ms
step:747/2420 train_time:44595ms step_avg:59.70ms
step:748/2420 train_time:44656ms step_avg:59.70ms
step:749/2420 train_time:44714ms step_avg:59.70ms
step:750/2420 train_time:44775ms step_avg:59.70ms
step:750/2420 val_loss:3.6890 train_time:44837ms step_avg:59.78ms
step:751/2420 train_time:44858ms step_avg:59.73ms
step:752/2420 train_time:44897ms step_avg:59.70ms
step:753/2420 train_time:44956ms step_avg:59.70ms
step:754/2420 train_time:45019ms step_avg:59.71ms
step:755/2420 train_time:45078ms step_avg:59.71ms
step:756/2420 train_time:45138ms step_avg:59.71ms
step:757/2420 train_time:45196ms step_avg:59.70ms
step:758/2420 train_time:45255ms step_avg:59.70ms
step:759/2420 train_time:45314ms step_avg:59.70ms
step:760/2420 train_time:45374ms step_avg:59.70ms
step:761/2420 train_time:45431ms step_avg:59.70ms
step:762/2420 train_time:45491ms step_avg:59.70ms
step:763/2420 train_time:45549ms step_avg:59.70ms
step:764/2420 train_time:45609ms step_avg:59.70ms
step:765/2420 train_time:45667ms step_avg:59.70ms
step:766/2420 train_time:45728ms step_avg:59.70ms
step:767/2420 train_time:45787ms step_avg:59.70ms
step:768/2420 train_time:45849ms step_avg:59.70ms
step:769/2420 train_time:45910ms step_avg:59.70ms
step:770/2420 train_time:45972ms step_avg:59.70ms
step:771/2420 train_time:46032ms step_avg:59.70ms
step:772/2420 train_time:46093ms step_avg:59.71ms
step:773/2420 train_time:46152ms step_avg:59.71ms
step:774/2420 train_time:46213ms step_avg:59.71ms
step:775/2420 train_time:46271ms step_avg:59.70ms
step:776/2420 train_time:46331ms step_avg:59.70ms
step:777/2420 train_time:46389ms step_avg:59.70ms
step:778/2420 train_time:46449ms step_avg:59.70ms
step:779/2420 train_time:46507ms step_avg:59.70ms
step:780/2420 train_time:46567ms step_avg:59.70ms
step:781/2420 train_time:46625ms step_avg:59.70ms
step:782/2420 train_time:46685ms step_avg:59.70ms
step:783/2420 train_time:46743ms step_avg:59.70ms
step:784/2420 train_time:46804ms step_avg:59.70ms
step:785/2420 train_time:46864ms step_avg:59.70ms
step:786/2420 train_time:46925ms step_avg:59.70ms
step:787/2420 train_time:46985ms step_avg:59.70ms
step:788/2420 train_time:47046ms step_avg:59.70ms
step:789/2420 train_time:47106ms step_avg:59.70ms
step:790/2420 train_time:47168ms step_avg:59.71ms
step:791/2420 train_time:47226ms step_avg:59.70ms
step:792/2420 train_time:47286ms step_avg:59.70ms
step:793/2420 train_time:47344ms step_avg:59.70ms
step:794/2420 train_time:47405ms step_avg:59.70ms
step:795/2420 train_time:47464ms step_avg:59.70ms
step:796/2420 train_time:47524ms step_avg:59.70ms
step:797/2420 train_time:47583ms step_avg:59.70ms
step:798/2420 train_time:47643ms step_avg:59.70ms
step:799/2420 train_time:47702ms step_avg:59.70ms
step:800/2420 train_time:47764ms step_avg:59.70ms
step:801/2420 train_time:47824ms step_avg:59.71ms
step:802/2420 train_time:47885ms step_avg:59.71ms
step:803/2420 train_time:47946ms step_avg:59.71ms
step:804/2420 train_time:48008ms step_avg:59.71ms
step:805/2420 train_time:48068ms step_avg:59.71ms
step:806/2420 train_time:48130ms step_avg:59.71ms
step:807/2420 train_time:48190ms step_avg:59.71ms
step:808/2420 train_time:48251ms step_avg:59.72ms
step:809/2420 train_time:48310ms step_avg:59.72ms
step:810/2420 train_time:48371ms step_avg:59.72ms
step:811/2420 train_time:48430ms step_avg:59.72ms
step:812/2420 train_time:48491ms step_avg:59.72ms
step:813/2420 train_time:48550ms step_avg:59.72ms
step:814/2420 train_time:48611ms step_avg:59.72ms
step:815/2420 train_time:48670ms step_avg:59.72ms
step:816/2420 train_time:48731ms step_avg:59.72ms
step:817/2420 train_time:48789ms step_avg:59.72ms
step:818/2420 train_time:48851ms step_avg:59.72ms
step:819/2420 train_time:48911ms step_avg:59.72ms
step:820/2420 train_time:48972ms step_avg:59.72ms
step:821/2420 train_time:49033ms step_avg:59.72ms
step:822/2420 train_time:49094ms step_avg:59.73ms
step:823/2420 train_time:49153ms step_avg:59.72ms
step:824/2420 train_time:49214ms step_avg:59.73ms
step:825/2420 train_time:49273ms step_avg:59.72ms
step:826/2420 train_time:49334ms step_avg:59.73ms
step:827/2420 train_time:49393ms step_avg:59.73ms
step:828/2420 train_time:49454ms step_avg:59.73ms
step:829/2420 train_time:49513ms step_avg:59.73ms
step:830/2420 train_time:49574ms step_avg:59.73ms
step:831/2420 train_time:49633ms step_avg:59.73ms
step:832/2420 train_time:49694ms step_avg:59.73ms
step:833/2420 train_time:49754ms step_avg:59.73ms
step:834/2420 train_time:49815ms step_avg:59.73ms
step:835/2420 train_time:49875ms step_avg:59.73ms
step:836/2420 train_time:49935ms step_avg:59.73ms
step:837/2420 train_time:49996ms step_avg:59.73ms
step:838/2420 train_time:50057ms step_avg:59.73ms
step:839/2420 train_time:50117ms step_avg:59.73ms
step:840/2420 train_time:50178ms step_avg:59.74ms
step:841/2420 train_time:50237ms step_avg:59.74ms
step:842/2420 train_time:50298ms step_avg:59.74ms
step:843/2420 train_time:50357ms step_avg:59.74ms
step:844/2420 train_time:50418ms step_avg:59.74ms
step:845/2420 train_time:50478ms step_avg:59.74ms
step:846/2420 train_time:50539ms step_avg:59.74ms
step:847/2420 train_time:50599ms step_avg:59.74ms
step:848/2420 train_time:50660ms step_avg:59.74ms
step:849/2420 train_time:50719ms step_avg:59.74ms
step:850/2420 train_time:50781ms step_avg:59.74ms
step:851/2420 train_time:50840ms step_avg:59.74ms
step:852/2420 train_time:50902ms step_avg:59.74ms
step:853/2420 train_time:50962ms step_avg:59.74ms
step:854/2420 train_time:51023ms step_avg:59.75ms
step:855/2420 train_time:51082ms step_avg:59.75ms
step:856/2420 train_time:51143ms step_avg:59.75ms
step:857/2420 train_time:51203ms step_avg:59.75ms
step:858/2420 train_time:51265ms step_avg:59.75ms
step:859/2420 train_time:51324ms step_avg:59.75ms
step:860/2420 train_time:51385ms step_avg:59.75ms
step:861/2420 train_time:51445ms step_avg:59.75ms
step:862/2420 train_time:51507ms step_avg:59.75ms
step:863/2420 train_time:51567ms step_avg:59.75ms
step:864/2420 train_time:51628ms step_avg:59.75ms
step:865/2420 train_time:51687ms step_avg:59.75ms
step:866/2420 train_time:51749ms step_avg:59.76ms
step:867/2420 train_time:51808ms step_avg:59.76ms
step:868/2420 train_time:51871ms step_avg:59.76ms
step:869/2420 train_time:51930ms step_avg:59.76ms
step:870/2420 train_time:51991ms step_avg:59.76ms
step:871/2420 train_time:52050ms step_avg:59.76ms
step:872/2420 train_time:52112ms step_avg:59.76ms
step:873/2420 train_time:52171ms step_avg:59.76ms
step:874/2420 train_time:52232ms step_avg:59.76ms
step:875/2420 train_time:52291ms step_avg:59.76ms
step:876/2420 train_time:52352ms step_avg:59.76ms
step:877/2420 train_time:52412ms step_avg:59.76ms
step:878/2420 train_time:52473ms step_avg:59.76ms
step:879/2420 train_time:52533ms step_avg:59.76ms
step:880/2420 train_time:52593ms step_avg:59.77ms
step:881/2420 train_time:52652ms step_avg:59.76ms
step:882/2420 train_time:52714ms step_avg:59.77ms
step:883/2420 train_time:52773ms step_avg:59.77ms
step:884/2420 train_time:52835ms step_avg:59.77ms
step:885/2420 train_time:52894ms step_avg:59.77ms
step:886/2420 train_time:52955ms step_avg:59.77ms
step:887/2420 train_time:53014ms step_avg:59.77ms
step:888/2420 train_time:53075ms step_avg:59.77ms
step:889/2420 train_time:53134ms step_avg:59.77ms
step:890/2420 train_time:53195ms step_avg:59.77ms
step:891/2420 train_time:53254ms step_avg:59.77ms
step:892/2420 train_time:53316ms step_avg:59.77ms
step:893/2420 train_time:53375ms step_avg:59.77ms
step:894/2420 train_time:53437ms step_avg:59.77ms
step:895/2420 train_time:53496ms step_avg:59.77ms
step:896/2420 train_time:53557ms step_avg:59.77ms
step:897/2420 train_time:53616ms step_avg:59.77ms
step:898/2420 train_time:53677ms step_avg:59.77ms
step:899/2420 train_time:53736ms step_avg:59.77ms
step:900/2420 train_time:53797ms step_avg:59.77ms
step:901/2420 train_time:53857ms step_avg:59.77ms
step:902/2420 train_time:53918ms step_avg:59.78ms
step:903/2420 train_time:53977ms step_avg:59.78ms
step:904/2420 train_time:54038ms step_avg:59.78ms
step:905/2420 train_time:54098ms step_avg:59.78ms
step:906/2420 train_time:54159ms step_avg:59.78ms
step:907/2420 train_time:54218ms step_avg:59.78ms
step:908/2420 train_time:54280ms step_avg:59.78ms
step:909/2420 train_time:54339ms step_avg:59.78ms
step:910/2420 train_time:54401ms step_avg:59.78ms
step:911/2420 train_time:54460ms step_avg:59.78ms
step:912/2420 train_time:54521ms step_avg:59.78ms
step:913/2420 train_time:54581ms step_avg:59.78ms
step:914/2420 train_time:54642ms step_avg:59.78ms
step:915/2420 train_time:54701ms step_avg:59.78ms
step:916/2420 train_time:54763ms step_avg:59.78ms
step:917/2420 train_time:54822ms step_avg:59.78ms
step:918/2420 train_time:54884ms step_avg:59.79ms
step:919/2420 train_time:54944ms step_avg:59.79ms
step:920/2420 train_time:55005ms step_avg:59.79ms
step:921/2420 train_time:55064ms step_avg:59.79ms
step:922/2420 train_time:55126ms step_avg:59.79ms
step:923/2420 train_time:55185ms step_avg:59.79ms
step:924/2420 train_time:55247ms step_avg:59.79ms
step:925/2420 train_time:55307ms step_avg:59.79ms
step:926/2420 train_time:55369ms step_avg:59.79ms
step:927/2420 train_time:55429ms step_avg:59.79ms
step:928/2420 train_time:55490ms step_avg:59.79ms
step:929/2420 train_time:55549ms step_avg:59.79ms
step:930/2420 train_time:55610ms step_avg:59.80ms
step:931/2420 train_time:55670ms step_avg:59.80ms
step:932/2420 train_time:55731ms step_avg:59.80ms
step:933/2420 train_time:55790ms step_avg:59.80ms
step:934/2420 train_time:55851ms step_avg:59.80ms
step:935/2420 train_time:55910ms step_avg:59.80ms
step:936/2420 train_time:55972ms step_avg:59.80ms
step:937/2420 train_time:56031ms step_avg:59.80ms
step:938/2420 train_time:56092ms step_avg:59.80ms
step:939/2420 train_time:56151ms step_avg:59.80ms
step:940/2420 train_time:56213ms step_avg:59.80ms
step:941/2420 train_time:56273ms step_avg:59.80ms
step:942/2420 train_time:56334ms step_avg:59.80ms
step:943/2420 train_time:56393ms step_avg:59.80ms
step:944/2420 train_time:56454ms step_avg:59.80ms
step:945/2420 train_time:56513ms step_avg:59.80ms
step:946/2420 train_time:56574ms step_avg:59.80ms
step:947/2420 train_time:56633ms step_avg:59.80ms
step:948/2420 train_time:56694ms step_avg:59.80ms
step:949/2420 train_time:56753ms step_avg:59.80ms
step:950/2420 train_time:56814ms step_avg:59.80ms
step:951/2420 train_time:56874ms step_avg:59.80ms
step:952/2420 train_time:56935ms step_avg:59.81ms
step:953/2420 train_time:56994ms step_avg:59.80ms
step:954/2420 train_time:57055ms step_avg:59.81ms
step:955/2420 train_time:57114ms step_avg:59.81ms
step:956/2420 train_time:57175ms step_avg:59.81ms
step:957/2420 train_time:57235ms step_avg:59.81ms
step:958/2420 train_time:57296ms step_avg:59.81ms
step:959/2420 train_time:57355ms step_avg:59.81ms
step:960/2420 train_time:57416ms step_avg:59.81ms
step:961/2420 train_time:57475ms step_avg:59.81ms
step:962/2420 train_time:57536ms step_avg:59.81ms
step:963/2420 train_time:57595ms step_avg:59.81ms
step:964/2420 train_time:57656ms step_avg:59.81ms
step:965/2420 train_time:57715ms step_avg:59.81ms
step:966/2420 train_time:57776ms step_avg:59.81ms
step:967/2420 train_time:57835ms step_avg:59.81ms
step:968/2420 train_time:57897ms step_avg:59.81ms
step:969/2420 train_time:57956ms step_avg:59.81ms
step:970/2420 train_time:58017ms step_avg:59.81ms
step:971/2420 train_time:58077ms step_avg:59.81ms
step:972/2420 train_time:58138ms step_avg:59.81ms
step:973/2420 train_time:58197ms step_avg:59.81ms
step:974/2420 train_time:58258ms step_avg:59.81ms
step:975/2420 train_time:58318ms step_avg:59.81ms
step:976/2420 train_time:58379ms step_avg:59.81ms
step:977/2420 train_time:58438ms step_avg:59.81ms
step:978/2420 train_time:58499ms step_avg:59.82ms
step:979/2420 train_time:58559ms step_avg:59.82ms
step:980/2420 train_time:58620ms step_avg:59.82ms
step:981/2420 train_time:58679ms step_avg:59.82ms
step:982/2420 train_time:58741ms step_avg:59.82ms
step:983/2420 train_time:58800ms step_avg:59.82ms
step:984/2420 train_time:58861ms step_avg:59.82ms
step:985/2420 train_time:58920ms step_avg:59.82ms
step:986/2420 train_time:58982ms step_avg:59.82ms
step:987/2420 train_time:59041ms step_avg:59.82ms
step:988/2420 train_time:59102ms step_avg:59.82ms
step:989/2420 train_time:59162ms step_avg:59.82ms
step:990/2420 train_time:59223ms step_avg:59.82ms
step:991/2420 train_time:59283ms step_avg:59.82ms
step:992/2420 train_time:59344ms step_avg:59.82ms
step:993/2420 train_time:59403ms step_avg:59.82ms
step:994/2420 train_time:59465ms step_avg:59.82ms
step:995/2420 train_time:59525ms step_avg:59.82ms
step:996/2420 train_time:59586ms step_avg:59.83ms
step:997/2420 train_time:59646ms step_avg:59.83ms
step:998/2420 train_time:59707ms step_avg:59.83ms
step:999/2420 train_time:59766ms step_avg:59.83ms
step:1000/2420 train_time:59828ms step_avg:59.83ms
step:1000/2420 val_loss:3.5779 train_time:59891ms step_avg:59.89ms
step:1001/2420 train_time:59912ms step_avg:59.85ms
step:1002/2420 train_time:59951ms step_avg:59.83ms
step:1003/2420 train_time:60010ms step_avg:59.83ms
step:1004/2420 train_time:60072ms step_avg:59.83ms
step:1005/2420 train_time:60133ms step_avg:59.83ms
step:1006/2420 train_time:60196ms step_avg:59.84ms
step:1007/2420 train_time:60254ms step_avg:59.84ms
step:1008/2420 train_time:60315ms step_avg:59.84ms
step:1009/2420 train_time:60373ms step_avg:59.83ms
step:1010/2420 train_time:60434ms step_avg:59.84ms
step:1011/2420 train_time:60493ms step_avg:59.83ms
step:1012/2420 train_time:60553ms step_avg:59.83ms
step:1013/2420 train_time:60611ms step_avg:59.83ms
step:1014/2420 train_time:60672ms step_avg:59.83ms
step:1015/2420 train_time:60731ms step_avg:59.83ms
step:1016/2420 train_time:60792ms step_avg:59.83ms
step:1017/2420 train_time:60853ms step_avg:59.84ms
step:1018/2420 train_time:60915ms step_avg:59.84ms
step:1019/2420 train_time:60976ms step_avg:59.84ms
step:1020/2420 train_time:61037ms step_avg:59.84ms
step:1021/2420 train_time:61098ms step_avg:59.84ms
step:1022/2420 train_time:61160ms step_avg:59.84ms
step:1023/2420 train_time:61219ms step_avg:59.84ms
step:1024/2420 train_time:61280ms step_avg:59.84ms
step:1025/2420 train_time:61340ms step_avg:59.84ms
step:1026/2420 train_time:61401ms step_avg:59.84ms
step:1027/2420 train_time:61460ms step_avg:59.84ms
step:1028/2420 train_time:61521ms step_avg:59.85ms
step:1029/2420 train_time:61580ms step_avg:59.84ms
step:1030/2420 train_time:61641ms step_avg:59.85ms
step:1031/2420 train_time:61700ms step_avg:59.85ms
step:1032/2420 train_time:61762ms step_avg:59.85ms
step:1033/2420 train_time:61822ms step_avg:59.85ms
step:1034/2420 train_time:61884ms step_avg:59.85ms
step:1035/2420 train_time:61944ms step_avg:59.85ms
step:1036/2420 train_time:62007ms step_avg:59.85ms
step:1037/2420 train_time:62066ms step_avg:59.85ms
step:1038/2420 train_time:62128ms step_avg:59.85ms
step:1039/2420 train_time:62188ms step_avg:59.85ms
step:1040/2420 train_time:62250ms step_avg:59.86ms
step:1041/2420 train_time:62309ms step_avg:59.86ms
step:1042/2420 train_time:62370ms step_avg:59.86ms
step:1043/2420 train_time:62429ms step_avg:59.86ms
step:1044/2420 train_time:62490ms step_avg:59.86ms
step:1045/2420 train_time:62549ms step_avg:59.86ms
step:1046/2420 train_time:62610ms step_avg:59.86ms
step:1047/2420 train_time:62669ms step_avg:59.86ms
step:1048/2420 train_time:62729ms step_avg:59.86ms
step:1049/2420 train_time:62789ms step_avg:59.86ms
step:1050/2420 train_time:62850ms step_avg:59.86ms
step:1051/2420 train_time:62909ms step_avg:59.86ms
step:1052/2420 train_time:62971ms step_avg:59.86ms
step:1053/2420 train_time:63030ms step_avg:59.86ms
step:1054/2420 train_time:63091ms step_avg:59.86ms
step:1055/2420 train_time:63150ms step_avg:59.86ms
step:1056/2420 train_time:63212ms step_avg:59.86ms
step:1057/2420 train_time:63271ms step_avg:59.86ms
step:1058/2420 train_time:63332ms step_avg:59.86ms
step:1059/2420 train_time:63391ms step_avg:59.86ms
step:1060/2420 train_time:63452ms step_avg:59.86ms
step:1061/2420 train_time:63511ms step_avg:59.86ms
step:1062/2420 train_time:63572ms step_avg:59.86ms
step:1063/2420 train_time:63631ms step_avg:59.86ms
step:1064/2420 train_time:63692ms step_avg:59.86ms
step:1065/2420 train_time:63751ms step_avg:59.86ms
step:1066/2420 train_time:63813ms step_avg:59.86ms
step:1067/2420 train_time:63872ms step_avg:59.86ms
step:1068/2420 train_time:63933ms step_avg:59.86ms
step:1069/2420 train_time:63993ms step_avg:59.86ms
step:1070/2420 train_time:64054ms step_avg:59.86ms
step:1071/2420 train_time:64114ms step_avg:59.86ms
step:1072/2420 train_time:64175ms step_avg:59.87ms
step:1073/2420 train_time:64235ms step_avg:59.86ms
step:1074/2420 train_time:64297ms step_avg:59.87ms
step:1075/2420 train_time:64355ms step_avg:59.87ms
step:1076/2420 train_time:64416ms step_avg:59.87ms
step:1077/2420 train_time:64476ms step_avg:59.87ms
step:1078/2420 train_time:64537ms step_avg:59.87ms
step:1079/2420 train_time:64597ms step_avg:59.87ms
step:1080/2420 train_time:64658ms step_avg:59.87ms
step:1081/2420 train_time:64717ms step_avg:59.87ms
step:1082/2420 train_time:64778ms step_avg:59.87ms
step:1083/2420 train_time:64838ms step_avg:59.87ms
step:1084/2420 train_time:64899ms step_avg:59.87ms
step:1085/2420 train_time:64959ms step_avg:59.87ms
step:1086/2420 train_time:65020ms step_avg:59.87ms
step:1087/2420 train_time:65080ms step_avg:59.87ms
step:1088/2420 train_time:65142ms step_avg:59.87ms
step:1089/2420 train_time:65201ms step_avg:59.87ms
step:1090/2420 train_time:65263ms step_avg:59.87ms
step:1091/2420 train_time:65323ms step_avg:59.87ms
step:1092/2420 train_time:65385ms step_avg:59.88ms
step:1093/2420 train_time:65445ms step_avg:59.88ms
step:1094/2420 train_time:65506ms step_avg:59.88ms
step:1095/2420 train_time:65566ms step_avg:59.88ms
step:1096/2420 train_time:65627ms step_avg:59.88ms
step:1097/2420 train_time:65686ms step_avg:59.88ms
step:1098/2420 train_time:65747ms step_avg:59.88ms
step:1099/2420 train_time:65807ms step_avg:59.88ms
step:1100/2420 train_time:65868ms step_avg:59.88ms
step:1101/2420 train_time:65928ms step_avg:59.88ms
step:1102/2420 train_time:65989ms step_avg:59.88ms
step:1103/2420 train_time:66048ms step_avg:59.88ms
step:1104/2420 train_time:66109ms step_avg:59.88ms
step:1105/2420 train_time:66169ms step_avg:59.88ms
step:1106/2420 train_time:66230ms step_avg:59.88ms
step:1107/2420 train_time:66289ms step_avg:59.88ms
step:1108/2420 train_time:66350ms step_avg:59.88ms
step:1109/2420 train_time:66409ms step_avg:59.88ms
step:1110/2420 train_time:66470ms step_avg:59.88ms
step:1111/2420 train_time:66530ms step_avg:59.88ms
step:1112/2420 train_time:66591ms step_avg:59.88ms
step:1113/2420 train_time:66650ms step_avg:59.88ms
step:1114/2420 train_time:66711ms step_avg:59.88ms
step:1115/2420 train_time:66770ms step_avg:59.88ms
step:1116/2420 train_time:66831ms step_avg:59.88ms
step:1117/2420 train_time:66890ms step_avg:59.88ms
step:1118/2420 train_time:66951ms step_avg:59.88ms
step:1119/2420 train_time:67010ms step_avg:59.88ms
step:1120/2420 train_time:67071ms step_avg:59.89ms
step:1121/2420 train_time:67130ms step_avg:59.88ms
step:1122/2420 train_time:67191ms step_avg:59.89ms
step:1123/2420 train_time:67250ms step_avg:59.88ms
step:1124/2420 train_time:67311ms step_avg:59.89ms
step:1125/2420 train_time:67370ms step_avg:59.88ms
step:1126/2420 train_time:67432ms step_avg:59.89ms
step:1127/2420 train_time:67491ms step_avg:59.89ms
step:1128/2420 train_time:67552ms step_avg:59.89ms
step:1129/2420 train_time:67612ms step_avg:59.89ms
step:1130/2420 train_time:67673ms step_avg:59.89ms
step:1131/2420 train_time:67732ms step_avg:59.89ms
step:1132/2420 train_time:67793ms step_avg:59.89ms
step:1133/2420 train_time:67852ms step_avg:59.89ms
step:1134/2420 train_time:67914ms step_avg:59.89ms
step:1135/2420 train_time:67973ms step_avg:59.89ms
step:1136/2420 train_time:68034ms step_avg:59.89ms
step:1137/2420 train_time:68093ms step_avg:59.89ms
step:1138/2420 train_time:68154ms step_avg:59.89ms
step:1139/2420 train_time:68213ms step_avg:59.89ms
step:1140/2420 train_time:68275ms step_avg:59.89ms
step:1141/2420 train_time:68335ms step_avg:59.89ms
step:1142/2420 train_time:68396ms step_avg:59.89ms
step:1143/2420 train_time:68456ms step_avg:59.89ms
step:1144/2420 train_time:68517ms step_avg:59.89ms
step:1145/2420 train_time:68576ms step_avg:59.89ms
step:1146/2420 train_time:68637ms step_avg:59.89ms
step:1147/2420 train_time:68697ms step_avg:59.89ms
step:1148/2420 train_time:68758ms step_avg:59.89ms
step:1149/2420 train_time:68818ms step_avg:59.89ms
step:1150/2420 train_time:68879ms step_avg:59.89ms
step:1151/2420 train_time:68938ms step_avg:59.89ms
step:1152/2420 train_time:68999ms step_avg:59.90ms
step:1153/2420 train_time:69059ms step_avg:59.89ms
step:1154/2420 train_time:69120ms step_avg:59.90ms
step:1155/2420 train_time:69180ms step_avg:59.90ms
step:1156/2420 train_time:69241ms step_avg:59.90ms
step:1157/2420 train_time:69301ms step_avg:59.90ms
step:1158/2420 train_time:69362ms step_avg:59.90ms
step:1159/2420 train_time:69422ms step_avg:59.90ms
step:1160/2420 train_time:69484ms step_avg:59.90ms
step:1161/2420 train_time:69543ms step_avg:59.90ms
step:1162/2420 train_time:69604ms step_avg:59.90ms
step:1163/2420 train_time:69663ms step_avg:59.90ms
step:1164/2420 train_time:69725ms step_avg:59.90ms
step:1165/2420 train_time:69785ms step_avg:59.90ms
step:1166/2420 train_time:69847ms step_avg:59.90ms
step:1167/2420 train_time:69907ms step_avg:59.90ms
step:1168/2420 train_time:69968ms step_avg:59.90ms
step:1169/2420 train_time:70028ms step_avg:59.90ms
step:1170/2420 train_time:70089ms step_avg:59.91ms
step:1171/2420 train_time:70149ms step_avg:59.90ms
step:1172/2420 train_time:70210ms step_avg:59.91ms
step:1173/2420 train_time:70269ms step_avg:59.91ms
step:1174/2420 train_time:70330ms step_avg:59.91ms
step:1175/2420 train_time:70389ms step_avg:59.91ms
step:1176/2420 train_time:70450ms step_avg:59.91ms
step:1177/2420 train_time:70509ms step_avg:59.91ms
step:1178/2420 train_time:70570ms step_avg:59.91ms
step:1179/2420 train_time:70629ms step_avg:59.91ms
step:1180/2420 train_time:70690ms step_avg:59.91ms
step:1181/2420 train_time:70750ms step_avg:59.91ms
step:1182/2420 train_time:70811ms step_avg:59.91ms
step:1183/2420 train_time:70870ms step_avg:59.91ms
step:1184/2420 train_time:70932ms step_avg:59.91ms
step:1185/2420 train_time:70991ms step_avg:59.91ms
step:1186/2420 train_time:71052ms step_avg:59.91ms
step:1187/2420 train_time:71111ms step_avg:59.91ms
step:1188/2420 train_time:71172ms step_avg:59.91ms
step:1189/2420 train_time:71231ms step_avg:59.91ms
step:1190/2420 train_time:71292ms step_avg:59.91ms
step:1191/2420 train_time:71351ms step_avg:59.91ms
step:1192/2420 train_time:71412ms step_avg:59.91ms
step:1193/2420 train_time:71471ms step_avg:59.91ms
step:1194/2420 train_time:71532ms step_avg:59.91ms
step:1195/2420 train_time:71591ms step_avg:59.91ms
step:1196/2420 train_time:71652ms step_avg:59.91ms
step:1197/2420 train_time:71711ms step_avg:59.91ms
step:1198/2420 train_time:71772ms step_avg:59.91ms
step:1199/2420 train_time:71832ms step_avg:59.91ms
step:1200/2420 train_time:71893ms step_avg:59.91ms
step:1201/2420 train_time:71952ms step_avg:59.91ms
step:1202/2420 train_time:72013ms step_avg:59.91ms
step:1203/2420 train_time:72072ms step_avg:59.91ms
step:1204/2420 train_time:72133ms step_avg:59.91ms
step:1205/2420 train_time:72192ms step_avg:59.91ms
step:1206/2420 train_time:72253ms step_avg:59.91ms
step:1207/2420 train_time:72313ms step_avg:59.91ms
step:1208/2420 train_time:72374ms step_avg:59.91ms
step:1209/2420 train_time:72433ms step_avg:59.91ms
step:1210/2420 train_time:72494ms step_avg:59.91ms
step:1211/2420 train_time:72554ms step_avg:59.91ms
step:1212/2420 train_time:72615ms step_avg:59.91ms
step:1213/2420 train_time:72674ms step_avg:59.91ms
step:1214/2420 train_time:72736ms step_avg:59.91ms
step:1215/2420 train_time:72796ms step_avg:59.91ms
step:1216/2420 train_time:72858ms step_avg:59.92ms
step:1217/2420 train_time:72917ms step_avg:59.92ms
step:1218/2420 train_time:72978ms step_avg:59.92ms
step:1219/2420 train_time:73037ms step_avg:59.92ms
step:1220/2420 train_time:73098ms step_avg:59.92ms
step:1221/2420 train_time:73158ms step_avg:59.92ms
step:1222/2420 train_time:73220ms step_avg:59.92ms
step:1223/2420 train_time:73279ms step_avg:59.92ms
step:1224/2420 train_time:73340ms step_avg:59.92ms
step:1225/2420 train_time:73400ms step_avg:59.92ms
step:1226/2420 train_time:73461ms step_avg:59.92ms
step:1227/2420 train_time:73521ms step_avg:59.92ms
step:1228/2420 train_time:73582ms step_avg:59.92ms
step:1229/2420 train_time:73642ms step_avg:59.92ms
step:1230/2420 train_time:73704ms step_avg:59.92ms
step:1231/2420 train_time:73764ms step_avg:59.92ms
step:1232/2420 train_time:73826ms step_avg:59.92ms
step:1233/2420 train_time:73885ms step_avg:59.92ms
step:1234/2420 train_time:73947ms step_avg:59.92ms
step:1235/2420 train_time:74006ms step_avg:59.92ms
step:1236/2420 train_time:74067ms step_avg:59.92ms
step:1237/2420 train_time:74126ms step_avg:59.92ms
step:1238/2420 train_time:74188ms step_avg:59.93ms
step:1239/2420 train_time:74247ms step_avg:59.93ms
step:1240/2420 train_time:74308ms step_avg:59.93ms
step:1241/2420 train_time:74367ms step_avg:59.93ms
step:1242/2420 train_time:74428ms step_avg:59.93ms
step:1243/2420 train_time:74487ms step_avg:59.93ms
step:1244/2420 train_time:74548ms step_avg:59.93ms
step:1245/2420 train_time:74608ms step_avg:59.93ms
step:1246/2420 train_time:74668ms step_avg:59.93ms
step:1247/2420 train_time:74728ms step_avg:59.93ms
step:1248/2420 train_time:74790ms step_avg:59.93ms
step:1249/2420 train_time:74850ms step_avg:59.93ms
step:1250/2420 train_time:74910ms step_avg:59.93ms
step:1250/2420 val_loss:3.5190 train_time:74974ms step_avg:59.98ms
step:1251/2420 train_time:74995ms step_avg:59.95ms
step:1252/2420 train_time:75037ms step_avg:59.93ms
step:1253/2420 train_time:75099ms step_avg:59.94ms
step:1254/2420 train_time:75162ms step_avg:59.94ms
step:1255/2420 train_time:75222ms step_avg:59.94ms
step:1256/2420 train_time:75283ms step_avg:59.94ms
step:1257/2420 train_time:75342ms step_avg:59.94ms
step:1258/2420 train_time:75402ms step_avg:59.94ms
step:1259/2420 train_time:75462ms step_avg:59.94ms
step:1260/2420 train_time:75523ms step_avg:59.94ms
step:1261/2420 train_time:75582ms step_avg:59.94ms
step:1262/2420 train_time:75642ms step_avg:59.94ms
step:1263/2420 train_time:75701ms step_avg:59.94ms
step:1264/2420 train_time:75762ms step_avg:59.94ms
step:1265/2420 train_time:75820ms step_avg:59.94ms
step:1266/2420 train_time:75881ms step_avg:59.94ms
step:1267/2420 train_time:75942ms step_avg:59.94ms
step:1268/2420 train_time:76005ms step_avg:59.94ms
step:1269/2420 train_time:76065ms step_avg:59.94ms
step:1270/2420 train_time:76128ms step_avg:59.94ms
step:1271/2420 train_time:76187ms step_avg:59.94ms
step:1272/2420 train_time:76248ms step_avg:59.94ms
step:1273/2420 train_time:76308ms step_avg:59.94ms
step:1274/2420 train_time:76368ms step_avg:59.94ms
step:1275/2420 train_time:76428ms step_avg:59.94ms
step:1276/2420 train_time:76489ms step_avg:59.94ms
step:1277/2420 train_time:76548ms step_avg:59.94ms
step:1278/2420 train_time:76608ms step_avg:59.94ms
step:1279/2420 train_time:76667ms step_avg:59.94ms
step:1280/2420 train_time:76728ms step_avg:59.94ms
step:1281/2420 train_time:76787ms step_avg:59.94ms
step:1282/2420 train_time:76848ms step_avg:59.94ms
step:1283/2420 train_time:76908ms step_avg:59.94ms
step:1284/2420 train_time:76969ms step_avg:59.94ms
step:1285/2420 train_time:77029ms step_avg:59.94ms
step:1286/2420 train_time:77090ms step_avg:59.95ms
step:1287/2420 train_time:77150ms step_avg:59.95ms
step:1288/2420 train_time:77212ms step_avg:59.95ms
step:1289/2420 train_time:77271ms step_avg:59.95ms
step:1290/2420 train_time:77332ms step_avg:59.95ms
step:1291/2420 train_time:77391ms step_avg:59.95ms
step:1292/2420 train_time:77452ms step_avg:59.95ms
step:1293/2420 train_time:77511ms step_avg:59.95ms
step:1294/2420 train_time:77572ms step_avg:59.95ms
step:1295/2420 train_time:77631ms step_avg:59.95ms
step:1296/2420 train_time:77692ms step_avg:59.95ms
step:1297/2420 train_time:77751ms step_avg:59.95ms
step:1298/2420 train_time:77813ms step_avg:59.95ms
step:1299/2420 train_time:77872ms step_avg:59.95ms
step:1300/2420 train_time:77933ms step_avg:59.95ms
step:1301/2420 train_time:77993ms step_avg:59.95ms
step:1302/2420 train_time:78054ms step_avg:59.95ms
step:1303/2420 train_time:78114ms step_avg:59.95ms
step:1304/2420 train_time:78176ms step_avg:59.95ms
step:1305/2420 train_time:78236ms step_avg:59.95ms
step:1306/2420 train_time:78296ms step_avg:59.95ms
step:1307/2420 train_time:78356ms step_avg:59.95ms
step:1308/2420 train_time:78417ms step_avg:59.95ms
step:1309/2420 train_time:78476ms step_avg:59.95ms
step:1310/2420 train_time:78538ms step_avg:59.95ms
step:1311/2420 train_time:78597ms step_avg:59.95ms
step:1312/2420 train_time:78658ms step_avg:59.95ms
step:1313/2420 train_time:78718ms step_avg:59.95ms
step:1314/2420 train_time:78779ms step_avg:59.95ms
step:1315/2420 train_time:78838ms step_avg:59.95ms
step:1316/2420 train_time:78900ms step_avg:59.95ms
step:1317/2420 train_time:78960ms step_avg:59.95ms
step:1318/2420 train_time:79022ms step_avg:59.96ms
step:1319/2420 train_time:79081ms step_avg:59.96ms
step:1320/2420 train_time:79143ms step_avg:59.96ms
step:1321/2420 train_time:79202ms step_avg:59.96ms
step:1322/2420 train_time:79263ms step_avg:59.96ms
step:1323/2420 train_time:79323ms step_avg:59.96ms
step:1324/2420 train_time:79384ms step_avg:59.96ms
step:1325/2420 train_time:79444ms step_avg:59.96ms
step:1326/2420 train_time:79505ms step_avg:59.96ms
step:1327/2420 train_time:79564ms step_avg:59.96ms
step:1328/2420 train_time:79625ms step_avg:59.96ms
step:1329/2420 train_time:79684ms step_avg:59.96ms
step:1330/2420 train_time:79746ms step_avg:59.96ms
step:1331/2420 train_time:79804ms step_avg:59.96ms
step:1332/2420 train_time:79865ms step_avg:59.96ms
step:1333/2420 train_time:79925ms step_avg:59.96ms
step:1334/2420 train_time:79986ms step_avg:59.96ms
step:1335/2420 train_time:80046ms step_avg:59.96ms
step:1336/2420 train_time:80107ms step_avg:59.96ms
step:1337/2420 train_time:80166ms step_avg:59.96ms
step:1338/2420 train_time:80227ms step_avg:59.96ms
step:1339/2420 train_time:80286ms step_avg:59.96ms
step:1340/2420 train_time:80347ms step_avg:59.96ms
step:1341/2420 train_time:80407ms step_avg:59.96ms
step:1342/2420 train_time:80468ms step_avg:59.96ms
step:1343/2420 train_time:80528ms step_avg:59.96ms
step:1344/2420 train_time:80588ms step_avg:59.96ms
step:1345/2420 train_time:80647ms step_avg:59.96ms
step:1346/2420 train_time:80709ms step_avg:59.96ms
step:1347/2420 train_time:80768ms step_avg:59.96ms
step:1348/2420 train_time:80829ms step_avg:59.96ms
step:1349/2420 train_time:80888ms step_avg:59.96ms
step:1350/2420 train_time:80949ms step_avg:59.96ms
step:1351/2420 train_time:81008ms step_avg:59.96ms
step:1352/2420 train_time:81069ms step_avg:59.96ms
step:1353/2420 train_time:81129ms step_avg:59.96ms
step:1354/2420 train_time:81190ms step_avg:59.96ms
step:1355/2420 train_time:81250ms step_avg:59.96ms
step:1356/2420 train_time:81311ms step_avg:59.96ms
step:1357/2420 train_time:81371ms step_avg:59.96ms
step:1358/2420 train_time:81433ms step_avg:59.97ms
step:1359/2420 train_time:81492ms step_avg:59.96ms
step:1360/2420 train_time:81553ms step_avg:59.97ms
step:1361/2420 train_time:81611ms step_avg:59.96ms
step:1362/2420 train_time:81673ms step_avg:59.97ms
step:1363/2420 train_time:81731ms step_avg:59.96ms
step:1364/2420 train_time:81792ms step_avg:59.97ms
step:1365/2420 train_time:81852ms step_avg:59.96ms
step:1366/2420 train_time:81913ms step_avg:59.97ms
step:1367/2420 train_time:81973ms step_avg:59.97ms
step:1368/2420 train_time:82034ms step_avg:59.97ms
step:1369/2420 train_time:82093ms step_avg:59.97ms
step:1370/2420 train_time:82155ms step_avg:59.97ms
step:1371/2420 train_time:82214ms step_avg:59.97ms
step:1372/2420 train_time:82275ms step_avg:59.97ms
step:1373/2420 train_time:82335ms step_avg:59.97ms
step:1374/2420 train_time:82396ms step_avg:59.97ms
step:1375/2420 train_time:82456ms step_avg:59.97ms
step:1376/2420 train_time:82517ms step_avg:59.97ms
step:1377/2420 train_time:82576ms step_avg:59.97ms
step:1378/2420 train_time:82637ms step_avg:59.97ms
step:1379/2420 train_time:82697ms step_avg:59.97ms
step:1380/2420 train_time:82758ms step_avg:59.97ms
step:1381/2420 train_time:82818ms step_avg:59.97ms
step:1382/2420 train_time:82879ms step_avg:59.97ms
step:1383/2420 train_time:82939ms step_avg:59.97ms
step:1384/2420 train_time:83000ms step_avg:59.97ms
step:1385/2420 train_time:83060ms step_avg:59.97ms
step:1386/2420 train_time:83122ms step_avg:59.97ms
step:1387/2420 train_time:83182ms step_avg:59.97ms
step:1388/2420 train_time:83243ms step_avg:59.97ms
step:1389/2420 train_time:83303ms step_avg:59.97ms
step:1390/2420 train_time:83364ms step_avg:59.97ms
step:1391/2420 train_time:83423ms step_avg:59.97ms
step:1392/2420 train_time:83485ms step_avg:59.97ms
step:1393/2420 train_time:83544ms step_avg:59.97ms
step:1394/2420 train_time:83606ms step_avg:59.98ms
step:1395/2420 train_time:83665ms step_avg:59.98ms
step:1396/2420 train_time:83727ms step_avg:59.98ms
step:1397/2420 train_time:83786ms step_avg:59.98ms
step:1398/2420 train_time:83847ms step_avg:59.98ms
step:1399/2420 train_time:83906ms step_avg:59.98ms
step:1400/2420 train_time:83967ms step_avg:59.98ms
step:1401/2420 train_time:84026ms step_avg:59.98ms
step:1402/2420 train_time:84087ms step_avg:59.98ms
step:1403/2420 train_time:84147ms step_avg:59.98ms
step:1404/2420 train_time:84208ms step_avg:59.98ms
step:1405/2420 train_time:84268ms step_avg:59.98ms
step:1406/2420 train_time:84330ms step_avg:59.98ms
step:1407/2420 train_time:84390ms step_avg:59.98ms
step:1408/2420 train_time:84451ms step_avg:59.98ms
step:1409/2420 train_time:84510ms step_avg:59.98ms
step:1410/2420 train_time:84571ms step_avg:59.98ms
step:1411/2420 train_time:84630ms step_avg:59.98ms
step:1412/2420 train_time:84691ms step_avg:59.98ms
step:1413/2420 train_time:84750ms step_avg:59.98ms
step:1414/2420 train_time:84811ms step_avg:59.98ms
step:1415/2420 train_time:84870ms step_avg:59.98ms
step:1416/2420 train_time:84931ms step_avg:59.98ms
step:1417/2420 train_time:84990ms step_avg:59.98ms
step:1418/2420 train_time:85051ms step_avg:59.98ms
step:1419/2420 train_time:85110ms step_avg:59.98ms
step:1420/2420 train_time:85171ms step_avg:59.98ms
step:1421/2420 train_time:85231ms step_avg:59.98ms
step:1422/2420 train_time:85293ms step_avg:59.98ms
step:1423/2420 train_time:85353ms step_avg:59.98ms
step:1424/2420 train_time:85414ms step_avg:59.98ms
step:1425/2420 train_time:85474ms step_avg:59.98ms
step:1426/2420 train_time:85535ms step_avg:59.98ms
step:1427/2420 train_time:85594ms step_avg:59.98ms
step:1428/2420 train_time:85655ms step_avg:59.98ms
step:1429/2420 train_time:85715ms step_avg:59.98ms
step:1430/2420 train_time:85776ms step_avg:59.98ms
step:1431/2420 train_time:85835ms step_avg:59.98ms
step:1432/2420 train_time:85896ms step_avg:59.98ms
step:1433/2420 train_time:85956ms step_avg:59.98ms
step:1434/2420 train_time:86017ms step_avg:59.98ms
step:1435/2420 train_time:86076ms step_avg:59.98ms
step:1436/2420 train_time:86138ms step_avg:59.98ms
step:1437/2420 train_time:86198ms step_avg:59.98ms
step:1438/2420 train_time:86260ms step_avg:59.99ms
step:1439/2420 train_time:86320ms step_avg:59.99ms
step:1440/2420 train_time:86381ms step_avg:59.99ms
step:1441/2420 train_time:86440ms step_avg:59.99ms
step:1442/2420 train_time:86502ms step_avg:59.99ms
step:1443/2420 train_time:86561ms step_avg:59.99ms
step:1444/2420 train_time:86623ms step_avg:59.99ms
step:1445/2420 train_time:86682ms step_avg:59.99ms
step:1446/2420 train_time:86743ms step_avg:59.99ms
step:1447/2420 train_time:86803ms step_avg:59.99ms
step:1448/2420 train_time:86864ms step_avg:59.99ms
step:1449/2420 train_time:86923ms step_avg:59.99ms
step:1450/2420 train_time:86985ms step_avg:59.99ms
step:1451/2420 train_time:87044ms step_avg:59.99ms
step:1452/2420 train_time:87105ms step_avg:59.99ms
step:1453/2420 train_time:87165ms step_avg:59.99ms
step:1454/2420 train_time:87226ms step_avg:59.99ms
step:1455/2420 train_time:87286ms step_avg:59.99ms
step:1456/2420 train_time:87347ms step_avg:59.99ms
step:1457/2420 train_time:87406ms step_avg:59.99ms
step:1458/2420 train_time:87468ms step_avg:59.99ms
step:1459/2420 train_time:87527ms step_avg:59.99ms
step:1460/2420 train_time:87588ms step_avg:59.99ms
step:1461/2420 train_time:87647ms step_avg:59.99ms
step:1462/2420 train_time:87708ms step_avg:59.99ms
step:1463/2420 train_time:87767ms step_avg:59.99ms
step:1464/2420 train_time:87828ms step_avg:59.99ms
step:1465/2420 train_time:87888ms step_avg:59.99ms
step:1466/2420 train_time:87950ms step_avg:59.99ms
step:1467/2420 train_time:88010ms step_avg:59.99ms
step:1468/2420 train_time:88071ms step_avg:59.99ms
step:1469/2420 train_time:88130ms step_avg:59.99ms
step:1470/2420 train_time:88191ms step_avg:59.99ms
step:1471/2420 train_time:88250ms step_avg:59.99ms
step:1472/2420 train_time:88311ms step_avg:59.99ms
step:1473/2420 train_time:88370ms step_avg:59.99ms
step:1474/2420 train_time:88432ms step_avg:59.99ms
step:1475/2420 train_time:88491ms step_avg:59.99ms
step:1476/2420 train_time:88552ms step_avg:59.99ms
step:1477/2420 train_time:88611ms step_avg:59.99ms
step:1478/2420 train_time:88672ms step_avg:59.99ms
step:1479/2420 train_time:88731ms step_avg:59.99ms
step:1480/2420 train_time:88793ms step_avg:60.00ms
step:1481/2420 train_time:88853ms step_avg:60.00ms
step:1482/2420 train_time:88914ms step_avg:60.00ms
step:1483/2420 train_time:88973ms step_avg:60.00ms
step:1484/2420 train_time:89034ms step_avg:60.00ms
step:1485/2420 train_time:89094ms step_avg:60.00ms
step:1486/2420 train_time:89155ms step_avg:60.00ms
step:1487/2420 train_time:89215ms step_avg:60.00ms
step:1488/2420 train_time:89277ms step_avg:60.00ms
step:1489/2420 train_time:89335ms step_avg:60.00ms
step:1490/2420 train_time:89397ms step_avg:60.00ms
step:1491/2420 train_time:89456ms step_avg:60.00ms
step:1492/2420 train_time:89518ms step_avg:60.00ms
step:1493/2420 train_time:89577ms step_avg:60.00ms
step:1494/2420 train_time:89638ms step_avg:60.00ms
step:1495/2420 train_time:89699ms step_avg:60.00ms
step:1496/2420 train_time:89760ms step_avg:60.00ms
step:1497/2420 train_time:89820ms step_avg:60.00ms
step:1498/2420 train_time:89882ms step_avg:60.00ms
step:1499/2420 train_time:89941ms step_avg:60.00ms
step:1500/2420 train_time:90002ms step_avg:60.00ms
step:1500/2420 val_loss:3.4710 train_time:90066ms step_avg:60.04ms
step:1501/2420 train_time:90086ms step_avg:60.02ms
step:1502/2420 train_time:90126ms step_avg:60.00ms
step:1503/2420 train_time:90188ms step_avg:60.01ms
step:1504/2420 train_time:90254ms step_avg:60.01ms
step:1505/2420 train_time:90316ms step_avg:60.01ms
step:1506/2420 train_time:90378ms step_avg:60.01ms
step:1507/2420 train_time:90437ms step_avg:60.01ms
step:1508/2420 train_time:90497ms step_avg:60.01ms
step:1509/2420 train_time:90556ms step_avg:60.01ms
step:1510/2420 train_time:90616ms step_avg:60.01ms
step:1511/2420 train_time:90675ms step_avg:60.01ms
step:1512/2420 train_time:90735ms step_avg:60.01ms
step:1513/2420 train_time:90794ms step_avg:60.01ms
step:1514/2420 train_time:90855ms step_avg:60.01ms
step:1515/2420 train_time:90915ms step_avg:60.01ms
step:1516/2420 train_time:90976ms step_avg:60.01ms
step:1517/2420 train_time:91036ms step_avg:60.01ms
step:1518/2420 train_time:91098ms step_avg:60.01ms
step:1519/2420 train_time:91159ms step_avg:60.01ms
step:1520/2420 train_time:91222ms step_avg:60.01ms
step:1521/2420 train_time:91282ms step_avg:60.01ms
step:1522/2420 train_time:91344ms step_avg:60.02ms
step:1523/2420 train_time:91404ms step_avg:60.02ms
step:1524/2420 train_time:91464ms step_avg:60.02ms
step:1525/2420 train_time:91524ms step_avg:60.02ms
step:1526/2420 train_time:91585ms step_avg:60.02ms
step:1527/2420 train_time:91643ms step_avg:60.02ms
step:1528/2420 train_time:91705ms step_avg:60.02ms
step:1529/2420 train_time:91764ms step_avg:60.02ms
step:1530/2420 train_time:91825ms step_avg:60.02ms
step:1531/2420 train_time:91884ms step_avg:60.02ms
step:1532/2420 train_time:91945ms step_avg:60.02ms
step:1533/2420 train_time:92005ms step_avg:60.02ms
step:1534/2420 train_time:92066ms step_avg:60.02ms
step:1535/2420 train_time:92126ms step_avg:60.02ms
step:1536/2420 train_time:92188ms step_avg:60.02ms
step:1537/2420 train_time:92248ms step_avg:60.02ms
step:1538/2420 train_time:92310ms step_avg:60.02ms
step:1539/2420 train_time:92370ms step_avg:60.02ms
step:1540/2420 train_time:92431ms step_avg:60.02ms
step:1541/2420 train_time:92490ms step_avg:60.02ms
step:1542/2420 train_time:92551ms step_avg:60.02ms
step:1543/2420 train_time:92611ms step_avg:60.02ms
step:1544/2420 train_time:92672ms step_avg:60.02ms
step:1545/2420 train_time:92732ms step_avg:60.02ms
step:1546/2420 train_time:92792ms step_avg:60.02ms
step:1547/2420 train_time:92851ms step_avg:60.02ms
step:1548/2420 train_time:92912ms step_avg:60.02ms
step:1549/2420 train_time:92971ms step_avg:60.02ms
step:1550/2420 train_time:93033ms step_avg:60.02ms
step:1551/2420 train_time:93093ms step_avg:60.02ms
step:1552/2420 train_time:93154ms step_avg:60.02ms
step:1553/2420 train_time:93214ms step_avg:60.02ms
step:1554/2420 train_time:93276ms step_avg:60.02ms
step:1555/2420 train_time:93336ms step_avg:60.02ms
step:1556/2420 train_time:93397ms step_avg:60.02ms
step:1557/2420 train_time:93456ms step_avg:60.02ms
step:1558/2420 train_time:93517ms step_avg:60.02ms
step:1559/2420 train_time:93576ms step_avg:60.02ms
step:1560/2420 train_time:93637ms step_avg:60.02ms
step:1561/2420 train_time:93696ms step_avg:60.02ms
step:1562/2420 train_time:93757ms step_avg:60.02ms
step:1563/2420 train_time:93816ms step_avg:60.02ms
step:1564/2420 train_time:93878ms step_avg:60.02ms
step:1565/2420 train_time:93937ms step_avg:60.02ms
step:1566/2420 train_time:93998ms step_avg:60.02ms
step:1567/2420 train_time:94058ms step_avg:60.02ms
step:1568/2420 train_time:94119ms step_avg:60.02ms
step:1569/2420 train_time:94178ms step_avg:60.02ms
step:1570/2420 train_time:94239ms step_avg:60.03ms
step:1571/2420 train_time:94298ms step_avg:60.02ms
step:1572/2420 train_time:94360ms step_avg:60.03ms
step:1573/2420 train_time:94420ms step_avg:60.03ms
step:1574/2420 train_time:94481ms step_avg:60.03ms
step:1575/2420 train_time:94540ms step_avg:60.03ms
step:1576/2420 train_time:94601ms step_avg:60.03ms
step:1577/2420 train_time:94661ms step_avg:60.03ms
step:1578/2420 train_time:94722ms step_avg:60.03ms
step:1579/2420 train_time:94781ms step_avg:60.03ms
step:1580/2420 train_time:94843ms step_avg:60.03ms
step:1581/2420 train_time:94903ms step_avg:60.03ms
step:1582/2420 train_time:94964ms step_avg:60.03ms
step:1583/2420 train_time:95024ms step_avg:60.03ms
step:1584/2420 train_time:95085ms step_avg:60.03ms
step:1585/2420 train_time:95144ms step_avg:60.03ms
step:1586/2420 train_time:95206ms step_avg:60.03ms
step:1587/2420 train_time:95266ms step_avg:60.03ms
step:1588/2420 train_time:95327ms step_avg:60.03ms
step:1589/2420 train_time:95387ms step_avg:60.03ms
step:1590/2420 train_time:95449ms step_avg:60.03ms
step:1591/2420 train_time:95509ms step_avg:60.03ms
step:1592/2420 train_time:95571ms step_avg:60.03ms
step:1593/2420 train_time:95631ms step_avg:60.03ms
step:1594/2420 train_time:95693ms step_avg:60.03ms
step:1595/2420 train_time:95753ms step_avg:60.03ms
step:1596/2420 train_time:95814ms step_avg:60.03ms
step:1597/2420 train_time:95874ms step_avg:60.03ms
step:1598/2420 train_time:95935ms step_avg:60.03ms
step:1599/2420 train_time:95995ms step_avg:60.03ms
step:1600/2420 train_time:96056ms step_avg:60.04ms
step:1601/2420 train_time:96116ms step_avg:60.04ms
step:1602/2420 train_time:96178ms step_avg:60.04ms
step:1603/2420 train_time:96237ms step_avg:60.04ms
step:1604/2420 train_time:96299ms step_avg:60.04ms
step:1605/2420 train_time:96359ms step_avg:60.04ms
step:1606/2420 train_time:96420ms step_avg:60.04ms
step:1607/2420 train_time:96480ms step_avg:60.04ms
step:1608/2420 train_time:96541ms step_avg:60.04ms
step:1609/2420 train_time:96602ms step_avg:60.04ms
step:1610/2420 train_time:96664ms step_avg:60.04ms
step:1611/2420 train_time:96723ms step_avg:60.04ms
step:1612/2420 train_time:96784ms step_avg:60.04ms
step:1613/2420 train_time:96844ms step_avg:60.04ms
step:1614/2420 train_time:96907ms step_avg:60.04ms
step:1615/2420 train_time:96967ms step_avg:60.04ms
step:1616/2420 train_time:97028ms step_avg:60.04ms
step:1617/2420 train_time:97089ms step_avg:60.04ms
step:1618/2420 train_time:97151ms step_avg:60.04ms
step:1619/2420 train_time:97211ms step_avg:60.04ms
step:1620/2420 train_time:97274ms step_avg:60.05ms
step:1621/2420 train_time:97334ms step_avg:60.05ms
step:1622/2420 train_time:97396ms step_avg:60.05ms
step:1623/2420 train_time:97456ms step_avg:60.05ms
step:1624/2420 train_time:97518ms step_avg:60.05ms
step:1625/2420 train_time:97578ms step_avg:60.05ms
step:1626/2420 train_time:97639ms step_avg:60.05ms
step:1627/2420 train_time:97699ms step_avg:60.05ms
step:1628/2420 train_time:97762ms step_avg:60.05ms
step:1629/2420 train_time:97821ms step_avg:60.05ms
step:1630/2420 train_time:97883ms step_avg:60.05ms
step:1631/2420 train_time:97943ms step_avg:60.05ms
step:1632/2420 train_time:98005ms step_avg:60.05ms
step:1633/2420 train_time:98065ms step_avg:60.05ms
step:1634/2420 train_time:98127ms step_avg:60.05ms
step:1635/2420 train_time:98187ms step_avg:60.05ms
step:1636/2420 train_time:98249ms step_avg:60.05ms
step:1637/2420 train_time:98309ms step_avg:60.05ms
step:1638/2420 train_time:98371ms step_avg:60.06ms
step:1639/2420 train_time:98432ms step_avg:60.06ms
step:1640/2420 train_time:98494ms step_avg:60.06ms
step:1641/2420 train_time:98554ms step_avg:60.06ms
step:1642/2420 train_time:98616ms step_avg:60.06ms
step:1643/2420 train_time:98676ms step_avg:60.06ms
step:1644/2420 train_time:98738ms step_avg:60.06ms
step:1645/2420 train_time:98798ms step_avg:60.06ms
step:1646/2420 train_time:98859ms step_avg:60.06ms
step:1647/2420 train_time:98918ms step_avg:60.06ms
step:1648/2420 train_time:98980ms step_avg:60.06ms
step:1649/2420 train_time:99040ms step_avg:60.06ms
step:1650/2420 train_time:99102ms step_avg:60.06ms
step:1651/2420 train_time:99163ms step_avg:60.06ms
step:1652/2420 train_time:99224ms step_avg:60.06ms
step:1653/2420 train_time:99284ms step_avg:60.06ms
step:1654/2420 train_time:99346ms step_avg:60.06ms
step:1655/2420 train_time:99407ms step_avg:60.06ms
step:1656/2420 train_time:99469ms step_avg:60.07ms
step:1657/2420 train_time:99530ms step_avg:60.07ms
step:1658/2420 train_time:99592ms step_avg:60.07ms
step:1659/2420 train_time:99652ms step_avg:60.07ms
step:1660/2420 train_time:99714ms step_avg:60.07ms
step:1661/2420 train_time:99775ms step_avg:60.07ms
step:1662/2420 train_time:99836ms step_avg:60.07ms
step:1663/2420 train_time:99896ms step_avg:60.07ms
step:1664/2420 train_time:99957ms step_avg:60.07ms
step:1665/2420 train_time:100017ms step_avg:60.07ms
step:1666/2420 train_time:100078ms step_avg:60.07ms
step:1667/2420 train_time:100137ms step_avg:60.07ms
step:1668/2420 train_time:100199ms step_avg:60.07ms
step:1669/2420 train_time:100259ms step_avg:60.07ms
step:1670/2420 train_time:100321ms step_avg:60.07ms
step:1671/2420 train_time:100381ms step_avg:60.07ms
step:1672/2420 train_time:100443ms step_avg:60.07ms
step:1673/2420 train_time:100503ms step_avg:60.07ms
step:1674/2420 train_time:100565ms step_avg:60.07ms
step:1675/2420 train_time:100625ms step_avg:60.07ms
step:1676/2420 train_time:100687ms step_avg:60.08ms
step:1677/2420 train_time:100747ms step_avg:60.08ms
step:1678/2420 train_time:100809ms step_avg:60.08ms
step:1679/2420 train_time:100870ms step_avg:60.08ms
step:1680/2420 train_time:100933ms step_avg:60.08ms
step:1681/2420 train_time:100993ms step_avg:60.08ms
step:1682/2420 train_time:101054ms step_avg:60.08ms
step:1683/2420 train_time:101114ms step_avg:60.08ms
step:1684/2420 train_time:101175ms step_avg:60.08ms
step:1685/2420 train_time:101235ms step_avg:60.08ms
step:1686/2420 train_time:101297ms step_avg:60.08ms
step:1687/2420 train_time:101357ms step_avg:60.08ms
step:1688/2420 train_time:101418ms step_avg:60.08ms
step:1689/2420 train_time:101477ms step_avg:60.08ms
step:1690/2420 train_time:101539ms step_avg:60.08ms
step:1691/2420 train_time:101599ms step_avg:60.08ms
step:1692/2420 train_time:101660ms step_avg:60.08ms
step:1693/2420 train_time:101720ms step_avg:60.08ms
step:1694/2420 train_time:101782ms step_avg:60.08ms
step:1695/2420 train_time:101843ms step_avg:60.08ms
step:1696/2420 train_time:101905ms step_avg:60.09ms
step:1697/2420 train_time:101965ms step_avg:60.09ms
step:1698/2420 train_time:102026ms step_avg:60.09ms
step:1699/2420 train_time:102087ms step_avg:60.09ms
step:1700/2420 train_time:102148ms step_avg:60.09ms
step:1701/2420 train_time:102208ms step_avg:60.09ms
step:1702/2420 train_time:102271ms step_avg:60.09ms
step:1703/2420 train_time:102331ms step_avg:60.09ms
step:1704/2420 train_time:102392ms step_avg:60.09ms
step:1705/2420 train_time:102452ms step_avg:60.09ms
step:1706/2420 train_time:102514ms step_avg:60.09ms
step:1707/2420 train_time:102575ms step_avg:60.09ms
step:1708/2420 train_time:102636ms step_avg:60.09ms
step:1709/2420 train_time:102696ms step_avg:60.09ms
step:1710/2420 train_time:102758ms step_avg:60.09ms
step:1711/2420 train_time:102817ms step_avg:60.09ms
step:1712/2420 train_time:102879ms step_avg:60.09ms
step:1713/2420 train_time:102938ms step_avg:60.09ms
step:1714/2420 train_time:103000ms step_avg:60.09ms
step:1715/2420 train_time:103060ms step_avg:60.09ms
step:1716/2420 train_time:103122ms step_avg:60.09ms
step:1717/2420 train_time:103182ms step_avg:60.09ms
step:1718/2420 train_time:103244ms step_avg:60.10ms
step:1719/2420 train_time:103305ms step_avg:60.10ms
step:1720/2420 train_time:103367ms step_avg:60.10ms
step:1721/2420 train_time:103427ms step_avg:60.10ms
step:1722/2420 train_time:103489ms step_avg:60.10ms
step:1723/2420 train_time:103549ms step_avg:60.10ms
step:1724/2420 train_time:103611ms step_avg:60.10ms
step:1725/2420 train_time:103672ms step_avg:60.10ms
step:1726/2420 train_time:103734ms step_avg:60.10ms
step:1727/2420 train_time:103794ms step_avg:60.10ms
step:1728/2420 train_time:103856ms step_avg:60.10ms
step:1729/2420 train_time:103916ms step_avg:60.10ms
step:1730/2420 train_time:103977ms step_avg:60.10ms
step:1731/2420 train_time:104037ms step_avg:60.10ms
step:1732/2420 train_time:104098ms step_avg:60.10ms
step:1733/2420 train_time:104159ms step_avg:60.10ms
step:1734/2420 train_time:104221ms step_avg:60.10ms
step:1735/2420 train_time:104281ms step_avg:60.10ms
step:1736/2420 train_time:104343ms step_avg:60.11ms
step:1737/2420 train_time:104402ms step_avg:60.10ms
step:1738/2420 train_time:104465ms step_avg:60.11ms
step:1739/2420 train_time:104525ms step_avg:60.11ms
step:1740/2420 train_time:104586ms step_avg:60.11ms
step:1741/2420 train_time:104646ms step_avg:60.11ms
step:1742/2420 train_time:104709ms step_avg:60.11ms
step:1743/2420 train_time:104770ms step_avg:60.11ms
step:1744/2420 train_time:104831ms step_avg:60.11ms
step:1745/2420 train_time:104891ms step_avg:60.11ms
step:1746/2420 train_time:104953ms step_avg:60.11ms
step:1747/2420 train_time:105013ms step_avg:60.11ms
step:1748/2420 train_time:105075ms step_avg:60.11ms
step:1749/2420 train_time:105135ms step_avg:60.11ms
step:1750/2420 train_time:105197ms step_avg:60.11ms
step:1750/2420 val_loss:3.3991 train_time:105261ms step_avg:60.15ms
step:1751/2420 train_time:105281ms step_avg:60.13ms
step:1752/2420 train_time:105321ms step_avg:60.11ms
step:1753/2420 train_time:105381ms step_avg:60.11ms
step:1754/2420 train_time:105444ms step_avg:60.12ms
step:1755/2420 train_time:105505ms step_avg:60.12ms
step:1756/2420 train_time:105567ms step_avg:60.12ms
step:1757/2420 train_time:105627ms step_avg:60.12ms
step:1758/2420 train_time:105688ms step_avg:60.12ms
step:1759/2420 train_time:105748ms step_avg:60.12ms
step:1760/2420 train_time:105809ms step_avg:60.12ms
step:1761/2420 train_time:105867ms step_avg:60.12ms
step:1762/2420 train_time:105928ms step_avg:60.12ms
step:1763/2420 train_time:105988ms step_avg:60.12ms
step:1764/2420 train_time:106049ms step_avg:60.12ms
step:1765/2420 train_time:106107ms step_avg:60.12ms
step:1766/2420 train_time:106170ms step_avg:60.12ms
step:1767/2420 train_time:106233ms step_avg:60.12ms
step:1768/2420 train_time:106296ms step_avg:60.12ms
step:1769/2420 train_time:106357ms step_avg:60.12ms
step:1770/2420 train_time:106419ms step_avg:60.12ms
step:1771/2420 train_time:106480ms step_avg:60.12ms
step:1772/2420 train_time:106542ms step_avg:60.13ms
step:1773/2420 train_time:106601ms step_avg:60.12ms
step:1774/2420 train_time:106662ms step_avg:60.13ms
step:1775/2420 train_time:106723ms step_avg:60.13ms
step:1776/2420 train_time:106784ms step_avg:60.13ms
step:1777/2420 train_time:106843ms step_avg:60.13ms
step:1778/2420 train_time:106904ms step_avg:60.13ms
step:1779/2420 train_time:106963ms step_avg:60.13ms
step:1780/2420 train_time:107024ms step_avg:60.13ms
step:1781/2420 train_time:107083ms step_avg:60.13ms
step:1782/2420 train_time:107145ms step_avg:60.13ms
step:1783/2420 train_time:107206ms step_avg:60.13ms
step:1784/2420 train_time:107269ms step_avg:60.13ms
step:1785/2420 train_time:107330ms step_avg:60.13ms
step:1786/2420 train_time:107392ms step_avg:60.13ms
step:1787/2420 train_time:107453ms step_avg:60.13ms
step:1788/2420 train_time:107515ms step_avg:60.13ms
step:1789/2420 train_time:107575ms step_avg:60.13ms
step:1790/2420 train_time:107637ms step_avg:60.13ms
step:1791/2420 train_time:107697ms step_avg:60.13ms
step:1792/2420 train_time:107759ms step_avg:60.13ms
step:1793/2420 train_time:107819ms step_avg:60.13ms
step:1794/2420 train_time:107881ms step_avg:60.13ms
step:1795/2420 train_time:107940ms step_avg:60.13ms
step:1796/2420 train_time:108002ms step_avg:60.13ms
step:1797/2420 train_time:108061ms step_avg:60.13ms
step:1798/2420 train_time:108122ms step_avg:60.13ms
step:1799/2420 train_time:108182ms step_avg:60.13ms
step:1800/2420 train_time:108243ms step_avg:60.14ms
step:1801/2420 train_time:108304ms step_avg:60.14ms
step:1802/2420 train_time:108366ms step_avg:60.14ms
step:1803/2420 train_time:108426ms step_avg:60.14ms
step:1804/2420 train_time:108489ms step_avg:60.14ms
step:1805/2420 train_time:108549ms step_avg:60.14ms
step:1806/2420 train_time:108611ms step_avg:60.14ms
step:1807/2420 train_time:108671ms step_avg:60.14ms
step:1808/2420 train_time:108733ms step_avg:60.14ms
step:1809/2420 train_time:108793ms step_avg:60.14ms
step:1810/2420 train_time:108855ms step_avg:60.14ms
step:1811/2420 train_time:108914ms step_avg:60.14ms
step:1812/2420 train_time:108977ms step_avg:60.14ms
step:1813/2420 train_time:109036ms step_avg:60.14ms
step:1814/2420 train_time:109098ms step_avg:60.14ms
step:1815/2420 train_time:109159ms step_avg:60.14ms
step:1816/2420 train_time:109221ms step_avg:60.14ms
step:1817/2420 train_time:109280ms step_avg:60.14ms
step:1818/2420 train_time:109343ms step_avg:60.14ms
step:1819/2420 train_time:109402ms step_avg:60.14ms
step:1820/2420 train_time:109463ms step_avg:60.14ms
step:1821/2420 train_time:109523ms step_avg:60.14ms
step:1822/2420 train_time:109584ms step_avg:60.14ms
step:1823/2420 train_time:109644ms step_avg:60.14ms
step:1824/2420 train_time:109706ms step_avg:60.15ms
step:1825/2420 train_time:109765ms step_avg:60.15ms
step:1826/2420 train_time:109827ms step_avg:60.15ms
step:1827/2420 train_time:109887ms step_avg:60.15ms
step:1828/2420 train_time:109949ms step_avg:60.15ms
step:1829/2420 train_time:110009ms step_avg:60.15ms
step:1830/2420 train_time:110070ms step_avg:60.15ms
step:1831/2420 train_time:110130ms step_avg:60.15ms
step:1832/2420 train_time:110192ms step_avg:60.15ms
step:1833/2420 train_time:110252ms step_avg:60.15ms
step:1834/2420 train_time:110314ms step_avg:60.15ms
step:1835/2420 train_time:110374ms step_avg:60.15ms
step:1836/2420 train_time:110436ms step_avg:60.15ms
step:1837/2420 train_time:110497ms step_avg:60.15ms
step:1838/2420 train_time:110559ms step_avg:60.15ms
step:1839/2420 train_time:110619ms step_avg:60.15ms
step:1840/2420 train_time:110681ms step_avg:60.15ms
step:1841/2420 train_time:110741ms step_avg:60.15ms
step:1842/2420 train_time:110802ms step_avg:60.15ms
step:1843/2420 train_time:110862ms step_avg:60.15ms
step:1844/2420 train_time:110923ms step_avg:60.15ms
step:1845/2420 train_time:110983ms step_avg:60.15ms
step:1846/2420 train_time:111045ms step_avg:60.15ms
step:1847/2420 train_time:111105ms step_avg:60.15ms
step:1848/2420 train_time:111167ms step_avg:60.16ms
step:1849/2420 train_time:111226ms step_avg:60.15ms
step:1850/2420 train_time:111288ms step_avg:60.16ms
step:1851/2420 train_time:111348ms step_avg:60.16ms
step:1852/2420 train_time:111410ms step_avg:60.16ms
step:1853/2420 train_time:111471ms step_avg:60.16ms
step:1854/2420 train_time:111532ms step_avg:60.16ms
step:1855/2420 train_time:111592ms step_avg:60.16ms
step:1856/2420 train_time:111654ms step_avg:60.16ms
step:1857/2420 train_time:111715ms step_avg:60.16ms
step:1858/2420 train_time:111776ms step_avg:60.16ms
step:1859/2420 train_time:111836ms step_avg:60.16ms
step:1860/2420 train_time:111899ms step_avg:60.16ms
step:1861/2420 train_time:111958ms step_avg:60.16ms
step:1862/2420 train_time:112020ms step_avg:60.16ms
step:1863/2420 train_time:112080ms step_avg:60.16ms
step:1864/2420 train_time:112141ms step_avg:60.16ms
step:1865/2420 train_time:112201ms step_avg:60.16ms
step:1866/2420 train_time:112262ms step_avg:60.16ms
step:1867/2420 train_time:112322ms step_avg:60.16ms
step:1868/2420 train_time:112384ms step_avg:60.16ms
step:1869/2420 train_time:112444ms step_avg:60.16ms
step:1870/2420 train_time:112506ms step_avg:60.16ms
step:1871/2420 train_time:112566ms step_avg:60.16ms
step:1872/2420 train_time:112627ms step_avg:60.16ms
step:1873/2420 train_time:112687ms step_avg:60.16ms
step:1874/2420 train_time:112749ms step_avg:60.16ms
step:1875/2420 train_time:112809ms step_avg:60.16ms
step:1876/2420 train_time:112871ms step_avg:60.17ms
step:1877/2420 train_time:112931ms step_avg:60.17ms
step:1878/2420 train_time:112993ms step_avg:60.17ms
step:1879/2420 train_time:113053ms step_avg:60.17ms
step:1880/2420 train_time:113114ms step_avg:60.17ms
step:1881/2420 train_time:113174ms step_avg:60.17ms
step:1882/2420 train_time:113236ms step_avg:60.17ms
step:1883/2420 train_time:113296ms step_avg:60.17ms
step:1884/2420 train_time:113358ms step_avg:60.17ms
step:1885/2420 train_time:113418ms step_avg:60.17ms
step:1886/2420 train_time:113479ms step_avg:60.17ms
step:1887/2420 train_time:113540ms step_avg:60.17ms
step:1888/2420 train_time:113602ms step_avg:60.17ms
step:1889/2420 train_time:113661ms step_avg:60.17ms
step:1890/2420 train_time:113723ms step_avg:60.17ms
step:1891/2420 train_time:113782ms step_avg:60.17ms
step:1892/2420 train_time:113844ms step_avg:60.17ms
step:1893/2420 train_time:113903ms step_avg:60.17ms
step:1894/2420 train_time:113965ms step_avg:60.17ms
step:1895/2420 train_time:114025ms step_avg:60.17ms
step:1896/2420 train_time:114087ms step_avg:60.17ms
step:1897/2420 train_time:114147ms step_avg:60.17ms
step:1898/2420 train_time:114209ms step_avg:60.17ms
step:1899/2420 train_time:114269ms step_avg:60.17ms
step:1900/2420 train_time:114331ms step_avg:60.17ms
step:1901/2420 train_time:114391ms step_avg:60.17ms
step:1902/2420 train_time:114452ms step_avg:60.17ms
step:1903/2420 train_time:114512ms step_avg:60.17ms
step:1904/2420 train_time:114575ms step_avg:60.18ms
step:1905/2420 train_time:114635ms step_avg:60.18ms
step:1906/2420 train_time:114697ms step_avg:60.18ms
step:1907/2420 train_time:114757ms step_avg:60.18ms
step:1908/2420 train_time:114819ms step_avg:60.18ms
step:1909/2420 train_time:114879ms step_avg:60.18ms
step:1910/2420 train_time:114942ms step_avg:60.18ms
step:1911/2420 train_time:115001ms step_avg:60.18ms
step:1912/2420 train_time:115062ms step_avg:60.18ms
step:1913/2420 train_time:115122ms step_avg:60.18ms
step:1914/2420 train_time:115183ms step_avg:60.18ms
step:1915/2420 train_time:115243ms step_avg:60.18ms
step:1916/2420 train_time:115305ms step_avg:60.18ms
step:1917/2420 train_time:115365ms step_avg:60.18ms
step:1918/2420 train_time:115427ms step_avg:60.18ms
step:1919/2420 train_time:115487ms step_avg:60.18ms
step:1920/2420 train_time:115549ms step_avg:60.18ms
step:1921/2420 train_time:115610ms step_avg:60.18ms
step:1922/2420 train_time:115672ms step_avg:60.18ms
step:1923/2420 train_time:115731ms step_avg:60.18ms
step:1924/2420 train_time:115793ms step_avg:60.18ms
step:1925/2420 train_time:115853ms step_avg:60.18ms
step:1926/2420 train_time:115915ms step_avg:60.18ms
step:1927/2420 train_time:115976ms step_avg:60.18ms
step:1928/2420 train_time:116039ms step_avg:60.19ms
step:1929/2420 train_time:116100ms step_avg:60.19ms
step:1930/2420 train_time:116161ms step_avg:60.19ms
step:1931/2420 train_time:116221ms step_avg:60.19ms
step:1932/2420 train_time:116282ms step_avg:60.19ms
step:1933/2420 train_time:116342ms step_avg:60.19ms
step:1934/2420 train_time:116403ms step_avg:60.19ms
step:1935/2420 train_time:116463ms step_avg:60.19ms
step:1936/2420 train_time:116524ms step_avg:60.19ms
step:1937/2420 train_time:116584ms step_avg:60.19ms
step:1938/2420 train_time:116647ms step_avg:60.19ms
step:1939/2420 train_time:116707ms step_avg:60.19ms
step:1940/2420 train_time:116769ms step_avg:60.19ms
step:1941/2420 train_time:116829ms step_avg:60.19ms
step:1942/2420 train_time:116890ms step_avg:60.19ms
step:1943/2420 train_time:116951ms step_avg:60.19ms
step:1944/2420 train_time:117013ms step_avg:60.19ms
step:1945/2420 train_time:117073ms step_avg:60.19ms
step:1946/2420 train_time:117136ms step_avg:60.19ms
step:1947/2420 train_time:117196ms step_avg:60.19ms
step:1948/2420 train_time:117258ms step_avg:60.19ms
step:1949/2420 train_time:117318ms step_avg:60.19ms
step:1950/2420 train_time:117380ms step_avg:60.19ms
step:1951/2420 train_time:117440ms step_avg:60.19ms
step:1952/2420 train_time:117501ms step_avg:60.20ms
step:1953/2420 train_time:117561ms step_avg:60.19ms
step:1954/2420 train_time:117622ms step_avg:60.20ms
step:1955/2420 train_time:117682ms step_avg:60.20ms
step:1956/2420 train_time:117743ms step_avg:60.20ms
step:1957/2420 train_time:117803ms step_avg:60.20ms
step:1958/2420 train_time:117864ms step_avg:60.20ms
step:1959/2420 train_time:117925ms step_avg:60.20ms
step:1960/2420 train_time:117987ms step_avg:60.20ms
step:1961/2420 train_time:118047ms step_avg:60.20ms
step:1962/2420 train_time:118109ms step_avg:60.20ms
step:1963/2420 train_time:118168ms step_avg:60.20ms
step:1964/2420 train_time:118231ms step_avg:60.20ms
step:1965/2420 train_time:118291ms step_avg:60.20ms
step:1966/2420 train_time:118352ms step_avg:60.20ms
step:1967/2420 train_time:118413ms step_avg:60.20ms
step:1968/2420 train_time:118475ms step_avg:60.20ms
step:1969/2420 train_time:118535ms step_avg:60.20ms
step:1970/2420 train_time:118597ms step_avg:60.20ms
step:1971/2420 train_time:118657ms step_avg:60.20ms
step:1972/2420 train_time:118719ms step_avg:60.20ms
step:1973/2420 train_time:118780ms step_avg:60.20ms
step:1974/2420 train_time:118842ms step_avg:60.20ms
step:1975/2420 train_time:118901ms step_avg:60.20ms
step:1976/2420 train_time:118963ms step_avg:60.20ms
step:1977/2420 train_time:119023ms step_avg:60.20ms
step:1978/2420 train_time:119084ms step_avg:60.20ms
step:1979/2420 train_time:119144ms step_avg:60.20ms
step:1980/2420 train_time:119207ms step_avg:60.21ms
step:1981/2420 train_time:119267ms step_avg:60.21ms
step:1982/2420 train_time:119328ms step_avg:60.21ms
step:1983/2420 train_time:119388ms step_avg:60.21ms
step:1984/2420 train_time:119450ms step_avg:60.21ms
step:1985/2420 train_time:119510ms step_avg:60.21ms
step:1986/2420 train_time:119572ms step_avg:60.21ms
step:1987/2420 train_time:119632ms step_avg:60.21ms
step:1988/2420 train_time:119695ms step_avg:60.21ms
step:1989/2420 train_time:119755ms step_avg:60.21ms
step:1990/2420 train_time:119816ms step_avg:60.21ms
step:1991/2420 train_time:119876ms step_avg:60.21ms
step:1992/2420 train_time:119938ms step_avg:60.21ms
step:1993/2420 train_time:119999ms step_avg:60.21ms
step:1994/2420 train_time:120063ms step_avg:60.21ms
step:1995/2420 train_time:120123ms step_avg:60.21ms
step:1996/2420 train_time:120184ms step_avg:60.21ms
step:1997/2420 train_time:120244ms step_avg:60.21ms
step:1998/2420 train_time:120306ms step_avg:60.21ms
step:1999/2420 train_time:120365ms step_avg:60.21ms
step:2000/2420 train_time:120427ms step_avg:60.21ms
step:2000/2420 val_loss:3.3448 train_time:120491ms step_avg:60.25ms
step:2001/2420 train_time:120512ms step_avg:60.23ms
step:2002/2420 train_time:120551ms step_avg:60.22ms
step:2003/2420 train_time:120616ms step_avg:60.22ms
step:2004/2420 train_time:120681ms step_avg:60.22ms
step:2005/2420 train_time:120742ms step_avg:60.22ms
step:2006/2420 train_time:120804ms step_avg:60.22ms
step:2007/2420 train_time:120864ms step_avg:60.22ms
step:2008/2420 train_time:120925ms step_avg:60.22ms
step:2009/2420 train_time:120985ms step_avg:60.22ms
step:2010/2420 train_time:121046ms step_avg:60.22ms
step:2011/2420 train_time:121106ms step_avg:60.22ms
step:2012/2420 train_time:121167ms step_avg:60.22ms
step:2013/2420 train_time:121226ms step_avg:60.22ms
step:2014/2420 train_time:121288ms step_avg:60.22ms
step:2015/2420 train_time:121346ms step_avg:60.22ms
step:2016/2420 train_time:121407ms step_avg:60.22ms
step:2017/2420 train_time:121468ms step_avg:60.22ms
step:2018/2420 train_time:121531ms step_avg:60.22ms
step:2019/2420 train_time:121593ms step_avg:60.22ms
step:2020/2420 train_time:121655ms step_avg:60.23ms
step:2021/2420 train_time:121717ms step_avg:60.23ms
step:2022/2420 train_time:121779ms step_avg:60.23ms
step:2023/2420 train_time:121840ms step_avg:60.23ms
step:2024/2420 train_time:121901ms step_avg:60.23ms
step:2025/2420 train_time:121961ms step_avg:60.23ms
step:2026/2420 train_time:122022ms step_avg:60.23ms
step:2027/2420 train_time:122082ms step_avg:60.23ms
step:2028/2420 train_time:122143ms step_avg:60.23ms
step:2029/2420 train_time:122202ms step_avg:60.23ms
step:2030/2420 train_time:122264ms step_avg:60.23ms
step:2031/2420 train_time:122323ms step_avg:60.23ms
step:2032/2420 train_time:122385ms step_avg:60.23ms
step:2033/2420 train_time:122445ms step_avg:60.23ms
step:2034/2420 train_time:122508ms step_avg:60.23ms
step:2035/2420 train_time:122569ms step_avg:60.23ms
step:2036/2420 train_time:122631ms step_avg:60.23ms
step:2037/2420 train_time:122692ms step_avg:60.23ms
step:2038/2420 train_time:122754ms step_avg:60.23ms
step:2039/2420 train_time:122813ms step_avg:60.23ms
step:2040/2420 train_time:122875ms step_avg:60.23ms
step:2041/2420 train_time:122935ms step_avg:60.23ms
step:2042/2420 train_time:122996ms step_avg:60.23ms
step:2043/2420 train_time:123056ms step_avg:60.23ms
step:2044/2420 train_time:123117ms step_avg:60.23ms
step:2045/2420 train_time:123177ms step_avg:60.23ms
step:2046/2420 train_time:123240ms step_avg:60.23ms
step:2047/2420 train_time:123300ms step_avg:60.23ms
step:2048/2420 train_time:123361ms step_avg:60.23ms
step:2049/2420 train_time:123421ms step_avg:60.23ms
step:2050/2420 train_time:123484ms step_avg:60.24ms
step:2051/2420 train_time:123545ms step_avg:60.24ms
step:2052/2420 train_time:123608ms step_avg:60.24ms
step:2053/2420 train_time:123668ms step_avg:60.24ms
step:2054/2420 train_time:123730ms step_avg:60.24ms
step:2055/2420 train_time:123791ms step_avg:60.24ms
step:2056/2420 train_time:123853ms step_avg:60.24ms
step:2057/2420 train_time:123912ms step_avg:60.24ms
step:2058/2420 train_time:123974ms step_avg:60.24ms
step:2059/2420 train_time:124033ms step_avg:60.24ms
step:2060/2420 train_time:124094ms step_avg:60.24ms
step:2061/2420 train_time:124154ms step_avg:60.24ms
step:2062/2420 train_time:124215ms step_avg:60.24ms
step:2063/2420 train_time:124275ms step_avg:60.24ms
step:2064/2420 train_time:124338ms step_avg:60.24ms
step:2065/2420 train_time:124398ms step_avg:60.24ms
step:2066/2420 train_time:124461ms step_avg:60.24ms
step:2067/2420 train_time:124521ms step_avg:60.24ms
step:2068/2420 train_time:124584ms step_avg:60.24ms
step:2069/2420 train_time:124645ms step_avg:60.24ms
step:2070/2420 train_time:124707ms step_avg:60.24ms
step:2071/2420 train_time:124767ms step_avg:60.25ms
step:2072/2420 train_time:124829ms step_avg:60.25ms
step:2073/2420 train_time:124890ms step_avg:60.25ms
step:2074/2420 train_time:124952ms step_avg:60.25ms
step:2075/2420 train_time:125012ms step_avg:60.25ms
step:2076/2420 train_time:125073ms step_avg:60.25ms
step:2077/2420 train_time:125133ms step_avg:60.25ms
step:2078/2420 train_time:125194ms step_avg:60.25ms
step:2079/2420 train_time:125254ms step_avg:60.25ms
step:2080/2420 train_time:125316ms step_avg:60.25ms
step:2081/2420 train_time:125376ms step_avg:60.25ms
step:2082/2420 train_time:125438ms step_avg:60.25ms
step:2083/2420 train_time:125498ms step_avg:60.25ms
step:2084/2420 train_time:125560ms step_avg:60.25ms
step:2085/2420 train_time:125620ms step_avg:60.25ms
step:2086/2420 train_time:125682ms step_avg:60.25ms
step:2087/2420 train_time:125742ms step_avg:60.25ms
step:2088/2420 train_time:125805ms step_avg:60.25ms
step:2089/2420 train_time:125865ms step_avg:60.25ms
step:2090/2420 train_time:125927ms step_avg:60.25ms
step:2091/2420 train_time:125987ms step_avg:60.25ms
step:2092/2420 train_time:126049ms step_avg:60.25ms
step:2093/2420 train_time:126109ms step_avg:60.25ms
step:2094/2420 train_time:126172ms step_avg:60.25ms
step:2095/2420 train_time:126231ms step_avg:60.25ms
step:2096/2420 train_time:126293ms step_avg:60.25ms
step:2097/2420 train_time:126353ms step_avg:60.25ms
step:2098/2420 train_time:126415ms step_avg:60.26ms
step:2099/2420 train_time:126475ms step_avg:60.25ms
step:2100/2420 train_time:126537ms step_avg:60.26ms
step:2101/2420 train_time:126597ms step_avg:60.26ms
step:2102/2420 train_time:126660ms step_avg:60.26ms
step:2103/2420 train_time:126721ms step_avg:60.26ms
step:2104/2420 train_time:126783ms step_avg:60.26ms
step:2105/2420 train_time:126843ms step_avg:60.26ms
step:2106/2420 train_time:126906ms step_avg:60.26ms
step:2107/2420 train_time:126966ms step_avg:60.26ms
step:2108/2420 train_time:127028ms step_avg:60.26ms
step:2109/2420 train_time:127088ms step_avg:60.26ms
step:2110/2420 train_time:127150ms step_avg:60.26ms
step:2111/2420 train_time:127209ms step_avg:60.26ms
step:2112/2420 train_time:127272ms step_avg:60.26ms
step:2113/2420 train_time:127332ms step_avg:60.26ms
step:2114/2420 train_time:127394ms step_avg:60.26ms
step:2115/2420 train_time:127454ms step_avg:60.26ms
step:2116/2420 train_time:127515ms step_avg:60.26ms
step:2117/2420 train_time:127576ms step_avg:60.26ms
step:2118/2420 train_time:127638ms step_avg:60.26ms
step:2119/2420 train_time:127697ms step_avg:60.26ms
step:2120/2420 train_time:127760ms step_avg:60.26ms
step:2121/2420 train_time:127820ms step_avg:60.26ms
step:2122/2420 train_time:127882ms step_avg:60.26ms
step:2123/2420 train_time:127942ms step_avg:60.26ms
step:2124/2420 train_time:128005ms step_avg:60.27ms
step:2125/2420 train_time:128065ms step_avg:60.27ms
step:2126/2420 train_time:128128ms step_avg:60.27ms
step:2127/2420 train_time:128188ms step_avg:60.27ms
step:2128/2420 train_time:128250ms step_avg:60.27ms
step:2129/2420 train_time:128311ms step_avg:60.27ms
step:2130/2420 train_time:128373ms step_avg:60.27ms
step:2131/2420 train_time:128433ms step_avg:60.27ms
step:2132/2420 train_time:128494ms step_avg:60.27ms
step:2133/2420 train_time:128554ms step_avg:60.27ms
step:2134/2420 train_time:128615ms step_avg:60.27ms
step:2135/2420 train_time:128675ms step_avg:60.27ms
step:2136/2420 train_time:128737ms step_avg:60.27ms
step:2137/2420 train_time:128797ms step_avg:60.27ms
step:2138/2420 train_time:128859ms step_avg:60.27ms
step:2139/2420 train_time:128919ms step_avg:60.27ms
step:2140/2420 train_time:128981ms step_avg:60.27ms
step:2141/2420 train_time:129041ms step_avg:60.27ms
step:2142/2420 train_time:129104ms step_avg:60.27ms
step:2143/2420 train_time:129164ms step_avg:60.27ms
step:2144/2420 train_time:129227ms step_avg:60.27ms
step:2145/2420 train_time:129287ms step_avg:60.27ms
step:2146/2420 train_time:129349ms step_avg:60.27ms
step:2147/2420 train_time:129410ms step_avg:60.27ms
step:2148/2420 train_time:129473ms step_avg:60.28ms
step:2149/2420 train_time:129533ms step_avg:60.28ms
step:2150/2420 train_time:129595ms step_avg:60.28ms
step:2151/2420 train_time:129655ms step_avg:60.28ms
step:2152/2420 train_time:129716ms step_avg:60.28ms
step:2153/2420 train_time:129776ms step_avg:60.28ms
step:2154/2420 train_time:129837ms step_avg:60.28ms
step:2155/2420 train_time:129898ms step_avg:60.28ms
step:2156/2420 train_time:129960ms step_avg:60.28ms
step:2157/2420 train_time:130020ms step_avg:60.28ms
step:2158/2420 train_time:130082ms step_avg:60.28ms
step:2159/2420 train_time:130142ms step_avg:60.28ms
step:2160/2420 train_time:130204ms step_avg:60.28ms
step:2161/2420 train_time:130265ms step_avg:60.28ms
step:2162/2420 train_time:130327ms step_avg:60.28ms
step:2163/2420 train_time:130387ms step_avg:60.28ms
step:2164/2420 train_time:130450ms step_avg:60.28ms
step:2165/2420 train_time:130510ms step_avg:60.28ms
step:2166/2420 train_time:130573ms step_avg:60.28ms
step:2167/2420 train_time:130632ms step_avg:60.28ms
step:2168/2420 train_time:130694ms step_avg:60.28ms
step:2169/2420 train_time:130753ms step_avg:60.28ms
step:2170/2420 train_time:130815ms step_avg:60.28ms
step:2171/2420 train_time:130874ms step_avg:60.28ms
step:2172/2420 train_time:130935ms step_avg:60.28ms
step:2173/2420 train_time:130996ms step_avg:60.28ms
step:2174/2420 train_time:131059ms step_avg:60.28ms
step:2175/2420 train_time:131119ms step_avg:60.28ms
step:2176/2420 train_time:131181ms step_avg:60.29ms
step:2177/2420 train_time:131241ms step_avg:60.29ms
step:2178/2420 train_time:131304ms step_avg:60.29ms
step:2179/2420 train_time:131364ms step_avg:60.29ms
step:2180/2420 train_time:131426ms step_avg:60.29ms
step:2181/2420 train_time:131487ms step_avg:60.29ms
step:2182/2420 train_time:131549ms step_avg:60.29ms
step:2183/2420 train_time:131609ms step_avg:60.29ms
step:2184/2420 train_time:131671ms step_avg:60.29ms
step:2185/2420 train_time:131731ms step_avg:60.29ms
step:2186/2420 train_time:131793ms step_avg:60.29ms
step:2187/2420 train_time:131853ms step_avg:60.29ms
step:2188/2420 train_time:131914ms step_avg:60.29ms
step:2189/2420 train_time:131974ms step_avg:60.29ms
step:2190/2420 train_time:132035ms step_avg:60.29ms
step:2191/2420 train_time:132095ms step_avg:60.29ms
step:2192/2420 train_time:132157ms step_avg:60.29ms
step:2193/2420 train_time:132218ms step_avg:60.29ms
step:2194/2420 train_time:132280ms step_avg:60.29ms
step:2195/2420 train_time:132340ms step_avg:60.29ms
step:2196/2420 train_time:132402ms step_avg:60.29ms
step:2197/2420 train_time:132462ms step_avg:60.29ms
step:2198/2420 train_time:132525ms step_avg:60.29ms
step:2199/2420 train_time:132586ms step_avg:60.29ms
step:2200/2420 train_time:132647ms step_avg:60.29ms
step:2201/2420 train_time:132707ms step_avg:60.29ms
step:2202/2420 train_time:132770ms step_avg:60.29ms
step:2203/2420 train_time:132830ms step_avg:60.29ms
step:2204/2420 train_time:132892ms step_avg:60.30ms
step:2205/2420 train_time:132951ms step_avg:60.30ms
step:2206/2420 train_time:133013ms step_avg:60.30ms
step:2207/2420 train_time:133074ms step_avg:60.30ms
step:2208/2420 train_time:133135ms step_avg:60.30ms
step:2209/2420 train_time:133195ms step_avg:60.30ms
step:2210/2420 train_time:133257ms step_avg:60.30ms
step:2211/2420 train_time:133317ms step_avg:60.30ms
step:2212/2420 train_time:133380ms step_avg:60.30ms
step:2213/2420 train_time:133440ms step_avg:60.30ms
step:2214/2420 train_time:133502ms step_avg:60.30ms
step:2215/2420 train_time:133562ms step_avg:60.30ms
step:2216/2420 train_time:133624ms step_avg:60.30ms
step:2217/2420 train_time:133684ms step_avg:60.30ms
step:2218/2420 train_time:133746ms step_avg:60.30ms
step:2219/2420 train_time:133806ms step_avg:60.30ms
step:2220/2420 train_time:133869ms step_avg:60.30ms
step:2221/2420 train_time:133929ms step_avg:60.30ms
step:2222/2420 train_time:133991ms step_avg:60.30ms
step:2223/2420 train_time:134051ms step_avg:60.30ms
step:2224/2420 train_time:134113ms step_avg:60.30ms
step:2225/2420 train_time:134172ms step_avg:60.30ms
step:2226/2420 train_time:134233ms step_avg:60.30ms
step:2227/2420 train_time:134293ms step_avg:60.30ms
step:2228/2420 train_time:134354ms step_avg:60.30ms
step:2229/2420 train_time:134414ms step_avg:60.30ms
step:2230/2420 train_time:134476ms step_avg:60.30ms
step:2231/2420 train_time:134537ms step_avg:60.30ms
step:2232/2420 train_time:134599ms step_avg:60.30ms
step:2233/2420 train_time:134659ms step_avg:60.30ms
step:2234/2420 train_time:134721ms step_avg:60.30ms
step:2235/2420 train_time:134781ms step_avg:60.30ms
step:2236/2420 train_time:134843ms step_avg:60.31ms
step:2237/2420 train_time:134903ms step_avg:60.31ms
step:2238/2420 train_time:134967ms step_avg:60.31ms
step:2239/2420 train_time:135027ms step_avg:60.31ms
step:2240/2420 train_time:135089ms step_avg:60.31ms
step:2241/2420 train_time:135149ms step_avg:60.31ms
step:2242/2420 train_time:135211ms step_avg:60.31ms
step:2243/2420 train_time:135271ms step_avg:60.31ms
step:2244/2420 train_time:135333ms step_avg:60.31ms
step:2245/2420 train_time:135393ms step_avg:60.31ms
step:2246/2420 train_time:135454ms step_avg:60.31ms
step:2247/2420 train_time:135514ms step_avg:60.31ms
step:2248/2420 train_time:135575ms step_avg:60.31ms
step:2249/2420 train_time:135635ms step_avg:60.31ms
step:2250/2420 train_time:135697ms step_avg:60.31ms
step:2250/2420 val_loss:3.3001 train_time:135761ms step_avg:60.34ms
step:2251/2420 train_time:135782ms step_avg:60.32ms
step:2252/2420 train_time:135822ms step_avg:60.31ms
step:2253/2420 train_time:135883ms step_avg:60.31ms
step:2254/2420 train_time:135947ms step_avg:60.31ms
step:2255/2420 train_time:136007ms step_avg:60.31ms
step:2256/2420 train_time:136068ms step_avg:60.31ms
step:2257/2420 train_time:136127ms step_avg:60.31ms
step:2258/2420 train_time:136188ms step_avg:60.31ms
step:2259/2420 train_time:136248ms step_avg:60.31ms
step:2260/2420 train_time:136309ms step_avg:60.31ms
step:2261/2420 train_time:136369ms step_avg:60.31ms
step:2262/2420 train_time:136429ms step_avg:60.31ms
step:2263/2420 train_time:136488ms step_avg:60.31ms
step:2264/2420 train_time:136549ms step_avg:60.31ms
step:2265/2420 train_time:136608ms step_avg:60.31ms
step:2266/2420 train_time:136671ms step_avg:60.31ms
step:2267/2420 train_time:136734ms step_avg:60.31ms
step:2268/2420 train_time:136796ms step_avg:60.32ms
step:2269/2420 train_time:136857ms step_avg:60.32ms
step:2270/2420 train_time:136920ms step_avg:60.32ms
step:2271/2420 train_time:136981ms step_avg:60.32ms
step:2272/2420 train_time:137043ms step_avg:60.32ms
step:2273/2420 train_time:137102ms step_avg:60.32ms
step:2274/2420 train_time:137164ms step_avg:60.32ms
step:2275/2420 train_time:137224ms step_avg:60.32ms
step:2276/2420 train_time:137286ms step_avg:60.32ms
step:2277/2420 train_time:137345ms step_avg:60.32ms
step:2278/2420 train_time:137407ms step_avg:60.32ms
step:2279/2420 train_time:137467ms step_avg:60.32ms
step:2280/2420 train_time:137528ms step_avg:60.32ms
step:2281/2420 train_time:137587ms step_avg:60.32ms
step:2282/2420 train_time:137648ms step_avg:60.32ms
step:2283/2420 train_time:137708ms step_avg:60.32ms
step:2284/2420 train_time:137771ms step_avg:60.32ms
step:2285/2420 train_time:137831ms step_avg:60.32ms
step:2286/2420 train_time:137894ms step_avg:60.32ms
step:2287/2420 train_time:137954ms step_avg:60.32ms
step:2288/2420 train_time:138016ms step_avg:60.32ms
step:2289/2420 train_time:138075ms step_avg:60.32ms
step:2290/2420 train_time:138137ms step_avg:60.32ms
step:2291/2420 train_time:138198ms step_avg:60.32ms
step:2292/2420 train_time:138260ms step_avg:60.32ms
step:2293/2420 train_time:138320ms step_avg:60.32ms
step:2294/2420 train_time:138382ms step_avg:60.32ms
step:2295/2420 train_time:138442ms step_avg:60.32ms
step:2296/2420 train_time:138503ms step_avg:60.32ms
step:2297/2420 train_time:138563ms step_avg:60.32ms
step:2298/2420 train_time:138625ms step_avg:60.32ms
step:2299/2420 train_time:138684ms step_avg:60.32ms
step:2300/2420 train_time:138746ms step_avg:60.32ms
step:2301/2420 train_time:138806ms step_avg:60.32ms
step:2302/2420 train_time:138867ms step_avg:60.32ms
step:2303/2420 train_time:138927ms step_avg:60.32ms
step:2304/2420 train_time:138989ms step_avg:60.33ms
step:2305/2420 train_time:139050ms step_avg:60.33ms
step:2306/2420 train_time:139112ms step_avg:60.33ms
step:2307/2420 train_time:139172ms step_avg:60.33ms
step:2308/2420 train_time:139234ms step_avg:60.33ms
step:2309/2420 train_time:139294ms step_avg:60.33ms
step:2310/2420 train_time:139356ms step_avg:60.33ms
step:2311/2420 train_time:139416ms step_avg:60.33ms
step:2312/2420 train_time:139477ms step_avg:60.33ms
step:2313/2420 train_time:139538ms step_avg:60.33ms
step:2314/2420 train_time:139600ms step_avg:60.33ms
step:2315/2420 train_time:139660ms step_avg:60.33ms
step:2316/2420 train_time:139722ms step_avg:60.33ms
step:2317/2420 train_time:139782ms step_avg:60.33ms
step:2318/2420 train_time:139844ms step_avg:60.33ms
step:2319/2420 train_time:139904ms step_avg:60.33ms
step:2320/2420 train_time:139966ms step_avg:60.33ms
step:2321/2420 train_time:140026ms step_avg:60.33ms
step:2322/2420 train_time:140088ms step_avg:60.33ms
step:2323/2420 train_time:140147ms step_avg:60.33ms
step:2324/2420 train_time:140209ms step_avg:60.33ms
step:2325/2420 train_time:140269ms step_avg:60.33ms
step:2326/2420 train_time:140331ms step_avg:60.33ms
step:2327/2420 train_time:140391ms step_avg:60.33ms
step:2328/2420 train_time:140452ms step_avg:60.33ms
step:2329/2420 train_time:140513ms step_avg:60.33ms
step:2330/2420 train_time:140575ms step_avg:60.33ms
step:2331/2420 train_time:140636ms step_avg:60.33ms
step:2332/2420 train_time:140697ms step_avg:60.33ms
step:2333/2420 train_time:140757ms step_avg:60.33ms
step:2334/2420 train_time:140820ms step_avg:60.33ms
step:2335/2420 train_time:140880ms step_avg:60.33ms
step:2336/2420 train_time:140942ms step_avg:60.33ms
step:2337/2420 train_time:141003ms step_avg:60.33ms
step:2338/2420 train_time:141065ms step_avg:60.34ms
step:2339/2420 train_time:141125ms step_avg:60.34ms
step:2340/2420 train_time:141186ms step_avg:60.34ms
step:2341/2420 train_time:141246ms step_avg:60.34ms
step:2342/2420 train_time:141307ms step_avg:60.34ms
step:2343/2420 train_time:141367ms step_avg:60.34ms
step:2344/2420 train_time:141428ms step_avg:60.34ms
step:2345/2420 train_time:141488ms step_avg:60.34ms
step:2346/2420 train_time:141549ms step_avg:60.34ms
step:2347/2420 train_time:141609ms step_avg:60.34ms
step:2348/2420 train_time:141672ms step_avg:60.34ms
step:2349/2420 train_time:141732ms step_avg:60.34ms
step:2350/2420 train_time:141795ms step_avg:60.34ms
step:2351/2420 train_time:141854ms step_avg:60.34ms
step:2352/2420 train_time:141916ms step_avg:60.34ms
step:2353/2420 train_time:141976ms step_avg:60.34ms
step:2354/2420 train_time:142039ms step_avg:60.34ms
step:2355/2420 train_time:142099ms step_avg:60.34ms
step:2356/2420 train_time:142161ms step_avg:60.34ms
step:2357/2420 train_time:142220ms step_avg:60.34ms
step:2358/2420 train_time:142282ms step_avg:60.34ms
step:2359/2420 train_time:142343ms step_avg:60.34ms
step:2360/2420 train_time:142406ms step_avg:60.34ms
step:2361/2420 train_time:142465ms step_avg:60.34ms
step:2362/2420 train_time:142527ms step_avg:60.34ms
step:2363/2420 train_time:142587ms step_avg:60.34ms
step:2364/2420 train_time:142648ms step_avg:60.34ms
step:2365/2420 train_time:142708ms step_avg:60.34ms
step:2366/2420 train_time:142770ms step_avg:60.34ms
step:2367/2420 train_time:142830ms step_avg:60.34ms
step:2368/2420 train_time:142892ms step_avg:60.34ms
step:2369/2420 train_time:142952ms step_avg:60.34ms
step:2370/2420 train_time:143015ms step_avg:60.34ms
step:2371/2420 train_time:143075ms step_avg:60.34ms
step:2372/2420 train_time:143137ms step_avg:60.34ms
step:2373/2420 train_time:143197ms step_avg:60.34ms
step:2374/2420 train_time:143259ms step_avg:60.34ms
step:2375/2420 train_time:143319ms step_avg:60.34ms
step:2376/2420 train_time:143380ms step_avg:60.35ms
step:2377/2420 train_time:143441ms step_avg:60.35ms
step:2378/2420 train_time:143503ms step_avg:60.35ms
step:2379/2420 train_time:143564ms step_avg:60.35ms
step:2380/2420 train_time:143625ms step_avg:60.35ms
step:2381/2420 train_time:143685ms step_avg:60.35ms
step:2382/2420 train_time:143747ms step_avg:60.35ms
step:2383/2420 train_time:143806ms step_avg:60.35ms
step:2384/2420 train_time:143867ms step_avg:60.35ms
step:2385/2420 train_time:144187ms step_avg:60.46ms
step:2386/2420 train_time:144272ms step_avg:60.47ms
step:2387/2420 train_time:144330ms step_avg:60.46ms
step:2388/2420 train_time:144390ms step_avg:60.46ms
step:2389/2420 train_time:144726ms step_avg:60.58ms
step:2390/2420 train_time:144786ms step_avg:60.58ms
step:2391/2420 train_time:145143ms step_avg:60.70ms
step:2392/2420 train_time:145175ms step_avg:60.69ms
step:2393/2420 train_time:145233ms step_avg:60.69ms
step:2394/2420 train_time:145294ms step_avg:60.69ms
step:2395/2420 train_time:145352ms step_avg:60.69ms
step:2396/2420 train_time:145413ms step_avg:60.69ms
step:2397/2420 train_time:145471ms step_avg:60.69ms
step:2398/2420 train_time:145532ms step_avg:60.69ms
step:2399/2420 train_time:145591ms step_avg:60.69ms
step:2400/2420 train_time:145652ms step_avg:60.69ms
step:2401/2420 train_time:145711ms step_avg:60.69ms
step:2402/2420 train_time:145772ms step_avg:60.69ms
step:2403/2420 train_time:145830ms step_avg:60.69ms
step:2404/2420 train_time:145891ms step_avg:60.69ms
step:2405/2420 train_time:145951ms step_avg:60.69ms
step:2406/2420 train_time:146017ms step_avg:60.69ms
step:2407/2420 train_time:146083ms step_avg:60.69ms
step:2408/2420 train_time:146146ms step_avg:60.69ms
step:2409/2420 train_time:146207ms step_avg:60.69ms
step:2410/2420 train_time:146269ms step_avg:60.69ms
step:2411/2420 train_time:146328ms step_avg:60.69ms
step:2412/2420 train_time:146389ms step_avg:60.69ms
step:2413/2420 train_time:146448ms step_avg:60.69ms
step:2414/2420 train_time:146509ms step_avg:60.69ms
step:2415/2420 train_time:146568ms step_avg:60.69ms
step:2416/2420 train_time:146629ms step_avg:60.69ms
step:2417/2420 train_time:146687ms step_avg:60.69ms
step:2418/2420 train_time:146749ms step_avg:60.69ms
step:2419/2420 train_time:146808ms step_avg:60.69ms
step:2420/2420 train_time:146868ms step_avg:60.69ms
step:2420/2420 val_loss:3.2745 train_time:146933ms step_avg:60.72ms
peak memory allocated: 29512 MiB reserved: 44036 MiB

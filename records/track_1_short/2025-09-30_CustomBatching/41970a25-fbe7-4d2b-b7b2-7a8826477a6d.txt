import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        #ve = [None, ve[0], ve[1]] + [None] * (len(self.blocks) - 5) + [ve[0], ve[1]]
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2380  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.4  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further
    momentum_cd_steps = 50


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.7, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps  # More explicit denominator

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # step the optimizers
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 21:14:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                  Off |
| N/A   35C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                  Off |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                  Off |
| N/A   41C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                  Off |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                  Off |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                  Off |
| N/A   41C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                  Off |
| N/A   39C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                  Off |
| N/A   36C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          240106      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          240107      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          240108      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          240109      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          240110      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          240111      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          240112      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          240113      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          240107      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          240108      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          240109      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          240110      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          240111      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          240112      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          240113      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2420 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2420 train_time:95ms step_avg:95.23ms
step:2/2420 train_time:189ms step_avg:94.39ms
step:3/2420 train_time:210ms step_avg:70.00ms
step:4/2420 train_time:244ms step_avg:61.05ms
step:5/2420 train_time:301ms step_avg:60.17ms
step:6/2420 train_time:383ms step_avg:63.76ms
step:7/2420 train_time:428ms step_avg:61.16ms
step:8/2420 train_time:488ms step_avg:61.01ms
step:9/2420 train_time:546ms step_avg:60.63ms
step:10/2420 train_time:605ms step_avg:60.55ms
step:11/2420 train_time:664ms step_avg:60.34ms
step:12/2420 train_time:724ms step_avg:60.34ms
step:13/2420 train_time:782ms step_avg:60.19ms
step:14/2420 train_time:843ms step_avg:60.19ms
step:15/2420 train_time:901ms step_avg:60.05ms
step:16/2420 train_time:961ms step_avg:60.06ms
step:17/2420 train_time:1020ms step_avg:59.99ms
step:18/2420 train_time:1083ms step_avg:60.18ms
step:19/2420 train_time:1146ms step_avg:60.33ms
step:20/2420 train_time:1210ms step_avg:60.52ms
step:21/2420 train_time:1270ms step_avg:60.49ms
step:22/2420 train_time:1332ms step_avg:60.53ms
step:23/2420 train_time:1391ms step_avg:60.50ms
step:24/2420 train_time:1452ms step_avg:60.52ms
step:25/2420 train_time:1511ms step_avg:60.45ms
step:26/2420 train_time:1572ms step_avg:60.45ms
step:27/2420 train_time:1631ms step_avg:60.40ms
step:28/2420 train_time:1692ms step_avg:60.41ms
step:29/2420 train_time:1750ms step_avg:60.34ms
step:30/2420 train_time:1811ms step_avg:60.36ms
step:31/2420 train_time:1870ms step_avg:60.34ms
step:32/2420 train_time:1931ms step_avg:60.34ms
step:33/2420 train_time:1990ms step_avg:60.31ms
step:34/2420 train_time:2052ms step_avg:60.34ms
step:35/2420 train_time:2112ms step_avg:60.35ms
step:36/2420 train_time:2175ms step_avg:60.41ms
step:37/2420 train_time:2234ms step_avg:60.38ms
step:38/2420 train_time:2294ms step_avg:60.38ms
step:39/2420 train_time:2354ms step_avg:60.35ms
step:40/2420 train_time:2415ms step_avg:60.37ms
step:41/2420 train_time:2474ms step_avg:60.34ms
step:42/2420 train_time:2534ms step_avg:60.35ms
step:43/2420 train_time:2593ms step_avg:60.31ms
step:44/2420 train_time:2654ms step_avg:60.32ms
step:45/2420 train_time:2713ms step_avg:60.29ms
step:46/2420 train_time:2774ms step_avg:60.30ms
step:47/2420 train_time:2833ms step_avg:60.27ms
step:48/2420 train_time:2893ms step_avg:60.28ms
step:49/2420 train_time:2952ms step_avg:60.25ms
step:50/2420 train_time:3013ms step_avg:60.26ms
step:51/2420 train_time:3073ms step_avg:60.25ms
step:52/2420 train_time:3135ms step_avg:60.29ms
step:53/2420 train_time:3195ms step_avg:60.28ms
step:54/2420 train_time:3255ms step_avg:60.28ms
step:55/2420 train_time:3315ms step_avg:60.27ms
step:56/2420 train_time:3376ms step_avg:60.29ms
step:57/2420 train_time:3435ms step_avg:60.26ms
step:58/2420 train_time:3495ms step_avg:60.26ms
step:59/2420 train_time:3555ms step_avg:60.26ms
step:60/2420 train_time:3615ms step_avg:60.26ms
step:61/2420 train_time:3674ms step_avg:60.23ms
step:62/2420 train_time:3734ms step_avg:60.23ms
step:63/2420 train_time:3793ms step_avg:60.21ms
step:64/2420 train_time:3854ms step_avg:60.22ms
step:65/2420 train_time:3912ms step_avg:60.19ms
step:66/2420 train_time:3973ms step_avg:60.20ms
step:67/2420 train_time:4032ms step_avg:60.18ms
step:68/2420 train_time:4094ms step_avg:60.20ms
step:69/2420 train_time:4153ms step_avg:60.19ms
step:70/2420 train_time:4215ms step_avg:60.22ms
step:71/2420 train_time:4275ms step_avg:60.21ms
step:72/2420 train_time:4335ms step_avg:60.21ms
step:73/2420 train_time:4394ms step_avg:60.20ms
step:74/2420 train_time:4455ms step_avg:60.20ms
step:75/2420 train_time:4514ms step_avg:60.19ms
step:76/2420 train_time:4575ms step_avg:60.20ms
step:77/2420 train_time:4633ms step_avg:60.17ms
step:78/2420 train_time:4694ms step_avg:60.17ms
step:79/2420 train_time:4752ms step_avg:60.15ms
step:80/2420 train_time:4813ms step_avg:60.16ms
step:81/2420 train_time:4872ms step_avg:60.15ms
step:82/2420 train_time:4933ms step_avg:60.16ms
step:83/2420 train_time:4992ms step_avg:60.15ms
step:84/2420 train_time:5052ms step_avg:60.15ms
step:85/2420 train_time:5112ms step_avg:60.14ms
step:86/2420 train_time:5174ms step_avg:60.16ms
step:87/2420 train_time:5233ms step_avg:60.15ms
step:88/2420 train_time:5294ms step_avg:60.16ms
step:89/2420 train_time:5353ms step_avg:60.15ms
step:90/2420 train_time:5414ms step_avg:60.16ms
step:91/2420 train_time:5474ms step_avg:60.15ms
step:92/2420 train_time:5535ms step_avg:60.16ms
step:93/2420 train_time:5593ms step_avg:60.14ms
step:94/2420 train_time:5654ms step_avg:60.14ms
step:95/2420 train_time:5713ms step_avg:60.13ms
step:96/2420 train_time:5773ms step_avg:60.14ms
step:97/2420 train_time:5832ms step_avg:60.12ms
step:98/2420 train_time:5892ms step_avg:60.13ms
step:99/2420 train_time:5951ms step_avg:60.11ms
step:100/2420 train_time:6012ms step_avg:60.12ms
step:101/2420 train_time:6071ms step_avg:60.11ms
step:102/2420 train_time:6132ms step_avg:60.12ms
step:103/2420 train_time:6192ms step_avg:60.11ms
step:104/2420 train_time:6253ms step_avg:60.12ms
step:105/2420 train_time:6312ms step_avg:60.12ms
step:106/2420 train_time:6374ms step_avg:60.13ms
step:107/2420 train_time:6433ms step_avg:60.12ms
step:108/2420 train_time:6494ms step_avg:60.13ms
step:109/2420 train_time:6552ms step_avg:60.11ms
step:110/2420 train_time:6613ms step_avg:60.12ms
step:111/2420 train_time:6672ms step_avg:60.11ms
step:112/2420 train_time:6732ms step_avg:60.11ms
step:113/2420 train_time:6790ms step_avg:60.09ms
step:114/2420 train_time:6851ms step_avg:60.09ms
step:115/2420 train_time:6909ms step_avg:60.08ms
step:116/2420 train_time:6970ms step_avg:60.09ms
step:117/2420 train_time:7029ms step_avg:60.08ms
step:118/2420 train_time:7090ms step_avg:60.09ms
step:119/2420 train_time:7149ms step_avg:60.08ms
step:120/2420 train_time:7210ms step_avg:60.08ms
step:121/2420 train_time:7269ms step_avg:60.08ms
step:122/2420 train_time:7331ms step_avg:60.09ms
step:123/2420 train_time:7390ms step_avg:60.08ms
step:124/2420 train_time:7451ms step_avg:60.09ms
step:125/2420 train_time:7510ms step_avg:60.08ms
step:126/2420 train_time:7571ms step_avg:60.09ms
step:127/2420 train_time:7631ms step_avg:60.08ms
step:128/2420 train_time:7691ms step_avg:60.09ms
step:129/2420 train_time:7750ms step_avg:60.08ms
step:130/2420 train_time:7811ms step_avg:60.08ms
step:131/2420 train_time:7869ms step_avg:60.07ms
step:132/2420 train_time:7930ms step_avg:60.07ms
step:133/2420 train_time:7989ms step_avg:60.06ms
step:134/2420 train_time:8049ms step_avg:60.07ms
step:135/2420 train_time:8108ms step_avg:60.06ms
step:136/2420 train_time:8169ms step_avg:60.07ms
step:137/2420 train_time:8229ms step_avg:60.06ms
step:138/2420 train_time:8290ms step_avg:60.07ms
step:139/2420 train_time:8349ms step_avg:60.06ms
step:140/2420 train_time:8410ms step_avg:60.07ms
step:141/2420 train_time:8469ms step_avg:60.06ms
step:142/2420 train_time:8530ms step_avg:60.07ms
step:143/2420 train_time:8589ms step_avg:60.06ms
step:144/2420 train_time:8650ms step_avg:60.07ms
step:145/2420 train_time:8709ms step_avg:60.06ms
step:146/2420 train_time:8769ms step_avg:60.06ms
step:147/2420 train_time:8828ms step_avg:60.05ms
step:148/2420 train_time:8889ms step_avg:60.06ms
step:149/2420 train_time:8948ms step_avg:60.05ms
step:150/2420 train_time:9008ms step_avg:60.05ms
step:151/2420 train_time:9067ms step_avg:60.04ms
step:152/2420 train_time:9128ms step_avg:60.05ms
step:153/2420 train_time:9187ms step_avg:60.05ms
step:154/2420 train_time:9248ms step_avg:60.05ms
step:155/2420 train_time:9307ms step_avg:60.04ms
step:156/2420 train_time:9368ms step_avg:60.05ms
step:157/2420 train_time:9427ms step_avg:60.05ms
step:158/2420 train_time:9488ms step_avg:60.05ms
step:159/2420 train_time:9547ms step_avg:60.04ms
step:160/2420 train_time:9608ms step_avg:60.05ms
step:161/2420 train_time:9667ms step_avg:60.05ms
step:162/2420 train_time:9728ms step_avg:60.05ms
step:163/2420 train_time:9787ms step_avg:60.05ms
step:164/2420 train_time:9848ms step_avg:60.05ms
step:165/2420 train_time:9906ms step_avg:60.04ms
step:166/2420 train_time:9967ms step_avg:60.04ms
step:167/2420 train_time:10025ms step_avg:60.03ms
step:168/2420 train_time:10086ms step_avg:60.04ms
step:169/2420 train_time:10145ms step_avg:60.03ms
step:170/2420 train_time:10206ms step_avg:60.03ms
step:171/2420 train_time:10265ms step_avg:60.03ms
step:172/2420 train_time:10325ms step_avg:60.03ms
step:173/2420 train_time:10384ms step_avg:60.02ms
step:174/2420 train_time:10445ms step_avg:60.03ms
step:175/2420 train_time:10503ms step_avg:60.02ms
step:176/2420 train_time:10564ms step_avg:60.02ms
step:177/2420 train_time:10622ms step_avg:60.01ms
step:178/2420 train_time:10683ms step_avg:60.02ms
step:179/2420 train_time:10742ms step_avg:60.01ms
step:180/2420 train_time:10803ms step_avg:60.01ms
step:181/2420 train_time:10861ms step_avg:60.01ms
step:182/2420 train_time:10922ms step_avg:60.01ms
step:183/2420 train_time:10980ms step_avg:60.00ms
step:184/2420 train_time:11040ms step_avg:60.00ms
step:185/2420 train_time:11098ms step_avg:59.99ms
step:186/2420 train_time:11159ms step_avg:59.99ms
step:187/2420 train_time:11217ms step_avg:59.99ms
step:188/2420 train_time:11278ms step_avg:59.99ms
step:189/2420 train_time:11336ms step_avg:59.98ms
step:190/2420 train_time:11396ms step_avg:59.98ms
step:191/2420 train_time:11454ms step_avg:59.97ms
step:192/2420 train_time:11515ms step_avg:59.97ms
step:193/2420 train_time:11574ms step_avg:59.97ms
step:194/2420 train_time:11635ms step_avg:59.98ms
step:195/2420 train_time:11694ms step_avg:59.97ms
step:196/2420 train_time:11755ms step_avg:59.97ms
step:197/2420 train_time:11814ms step_avg:59.97ms
step:198/2420 train_time:11875ms step_avg:59.98ms
step:199/2420 train_time:11934ms step_avg:59.97ms
step:200/2420 train_time:11994ms step_avg:59.97ms
step:201/2420 train_time:12052ms step_avg:59.96ms
step:202/2420 train_time:12113ms step_avg:59.96ms
step:203/2420 train_time:12173ms step_avg:59.96ms
step:204/2420 train_time:12233ms step_avg:59.97ms
step:205/2420 train_time:12292ms step_avg:59.96ms
step:206/2420 train_time:12353ms step_avg:59.97ms
step:207/2420 train_time:12412ms step_avg:59.96ms
step:208/2420 train_time:12473ms step_avg:59.97ms
step:209/2420 train_time:12532ms step_avg:59.96ms
step:210/2420 train_time:12593ms step_avg:59.97ms
step:211/2420 train_time:12652ms step_avg:59.96ms
step:212/2420 train_time:12713ms step_avg:59.97ms
step:213/2420 train_time:12772ms step_avg:59.96ms
step:214/2420 train_time:12833ms step_avg:59.97ms
step:215/2420 train_time:12892ms step_avg:59.96ms
step:216/2420 train_time:12952ms step_avg:59.96ms
step:217/2420 train_time:13011ms step_avg:59.96ms
step:218/2420 train_time:13073ms step_avg:59.97ms
step:219/2420 train_time:13131ms step_avg:59.96ms
step:220/2420 train_time:13191ms step_avg:59.96ms
step:221/2420 train_time:13250ms step_avg:59.95ms
step:222/2420 train_time:13311ms step_avg:59.96ms
step:223/2420 train_time:13370ms step_avg:59.95ms
step:224/2420 train_time:13430ms step_avg:59.96ms
step:225/2420 train_time:13488ms step_avg:59.95ms
step:226/2420 train_time:13549ms step_avg:59.95ms
step:227/2420 train_time:13608ms step_avg:59.95ms
step:228/2420 train_time:13669ms step_avg:59.95ms
step:229/2420 train_time:13728ms step_avg:59.95ms
step:230/2420 train_time:13790ms step_avg:59.96ms
step:231/2420 train_time:13848ms step_avg:59.95ms
step:232/2420 train_time:13909ms step_avg:59.95ms
step:233/2420 train_time:13968ms step_avg:59.95ms
step:234/2420 train_time:14029ms step_avg:59.95ms
step:235/2420 train_time:14088ms step_avg:59.95ms
step:236/2420 train_time:14148ms step_avg:59.95ms
step:237/2420 train_time:14207ms step_avg:59.95ms
step:238/2420 train_time:14268ms step_avg:59.95ms
step:239/2420 train_time:14327ms step_avg:59.94ms
step:240/2420 train_time:14387ms step_avg:59.95ms
step:241/2420 train_time:14446ms step_avg:59.94ms
step:242/2420 train_time:14507ms step_avg:59.94ms
step:243/2420 train_time:14565ms step_avg:59.94ms
step:244/2420 train_time:14626ms step_avg:59.94ms
step:245/2420 train_time:14684ms step_avg:59.94ms
step:246/2420 train_time:14745ms step_avg:59.94ms
step:247/2420 train_time:14804ms step_avg:59.93ms
step:248/2420 train_time:14864ms step_avg:59.94ms
step:249/2420 train_time:14922ms step_avg:59.93ms
step:250/2420 train_time:14983ms step_avg:59.93ms
step:250/2420 val_loss:4.0948 train_time:15046ms step_avg:60.19ms
step:251/2420 train_time:15067ms step_avg:60.03ms
step:252/2420 train_time:15104ms step_avg:59.93ms
step:253/2420 train_time:15165ms step_avg:59.94ms
step:254/2420 train_time:15234ms step_avg:59.97ms
step:255/2420 train_time:15295ms step_avg:59.98ms
step:256/2420 train_time:15356ms step_avg:59.98ms
step:257/2420 train_time:15415ms step_avg:59.98ms
step:258/2420 train_time:15475ms step_avg:59.98ms
step:259/2420 train_time:15532ms step_avg:59.97ms
step:260/2420 train_time:15592ms step_avg:59.97ms
step:261/2420 train_time:15649ms step_avg:59.96ms
step:262/2420 train_time:15710ms step_avg:59.96ms
step:263/2420 train_time:15767ms step_avg:59.95ms
step:264/2420 train_time:15826ms step_avg:59.95ms
step:265/2420 train_time:15884ms step_avg:59.94ms
step:266/2420 train_time:15943ms step_avg:59.94ms
step:267/2420 train_time:16001ms step_avg:59.93ms
step:268/2420 train_time:16061ms step_avg:59.93ms
step:269/2420 train_time:16121ms step_avg:59.93ms
step:270/2420 train_time:16185ms step_avg:59.95ms
step:271/2420 train_time:16246ms step_avg:59.95ms
step:272/2420 train_time:16309ms step_avg:59.96ms
step:273/2420 train_time:16369ms step_avg:59.96ms
step:274/2420 train_time:16430ms step_avg:59.96ms
step:275/2420 train_time:16488ms step_avg:59.96ms
step:276/2420 train_time:16549ms step_avg:59.96ms
step:277/2420 train_time:16608ms step_avg:59.96ms
step:278/2420 train_time:16668ms step_avg:59.96ms
step:279/2420 train_time:16726ms step_avg:59.95ms
step:280/2420 train_time:16785ms step_avg:59.95ms
step:281/2420 train_time:16843ms step_avg:59.94ms
step:282/2420 train_time:16903ms step_avg:59.94ms
step:283/2420 train_time:16960ms step_avg:59.93ms
step:284/2420 train_time:17020ms step_avg:59.93ms
step:285/2420 train_time:17078ms step_avg:59.92ms
step:286/2420 train_time:17139ms step_avg:59.93ms
step:287/2420 train_time:17199ms step_avg:59.93ms
step:288/2420 train_time:17261ms step_avg:59.93ms
step:289/2420 train_time:17320ms step_avg:59.93ms
step:290/2420 train_time:17382ms step_avg:59.94ms
step:291/2420 train_time:17441ms step_avg:59.93ms
step:292/2420 train_time:17502ms step_avg:59.94ms
step:293/2420 train_time:17560ms step_avg:59.93ms
step:294/2420 train_time:17621ms step_avg:59.94ms
step:295/2420 train_time:17680ms step_avg:59.93ms
step:296/2420 train_time:17740ms step_avg:59.93ms
step:297/2420 train_time:17798ms step_avg:59.93ms
step:298/2420 train_time:17858ms step_avg:59.93ms
step:299/2420 train_time:17916ms step_avg:59.92ms
step:300/2420 train_time:17975ms step_avg:59.92ms
step:301/2420 train_time:18033ms step_avg:59.91ms
step:302/2420 train_time:18093ms step_avg:59.91ms
step:303/2420 train_time:18151ms step_avg:59.91ms
step:304/2420 train_time:18212ms step_avg:59.91ms
step:305/2420 train_time:18271ms step_avg:59.90ms
step:306/2420 train_time:18331ms step_avg:59.90ms
step:307/2420 train_time:18389ms step_avg:59.90ms
step:308/2420 train_time:18450ms step_avg:59.90ms
step:309/2420 train_time:18509ms step_avg:59.90ms
step:310/2420 train_time:18570ms step_avg:59.90ms
step:311/2420 train_time:18628ms step_avg:59.90ms
step:312/2420 train_time:18689ms step_avg:59.90ms
step:313/2420 train_time:18747ms step_avg:59.89ms
step:314/2420 train_time:18808ms step_avg:59.90ms
step:315/2420 train_time:18866ms step_avg:59.89ms
step:316/2420 train_time:18927ms step_avg:59.89ms
step:317/2420 train_time:18986ms step_avg:59.89ms
step:318/2420 train_time:19047ms step_avg:59.90ms
step:319/2420 train_time:19105ms step_avg:59.89ms
step:320/2420 train_time:19166ms step_avg:59.89ms
step:321/2420 train_time:19224ms step_avg:59.89ms
step:322/2420 train_time:19285ms step_avg:59.89ms
step:323/2420 train_time:19344ms step_avg:59.89ms
step:324/2420 train_time:19405ms step_avg:59.89ms
step:325/2420 train_time:19465ms step_avg:59.89ms
step:326/2420 train_time:19525ms step_avg:59.89ms
step:327/2420 train_time:19584ms step_avg:59.89ms
step:328/2420 train_time:19644ms step_avg:59.89ms
step:329/2420 train_time:19703ms step_avg:59.89ms
step:330/2420 train_time:19764ms step_avg:59.89ms
step:331/2420 train_time:19822ms step_avg:59.89ms
step:332/2420 train_time:19882ms step_avg:59.89ms
step:333/2420 train_time:19941ms step_avg:59.88ms
step:334/2420 train_time:20001ms step_avg:59.88ms
step:335/2420 train_time:20059ms step_avg:59.88ms
step:336/2420 train_time:20120ms step_avg:59.88ms
step:337/2420 train_time:20178ms step_avg:59.88ms
step:338/2420 train_time:20239ms step_avg:59.88ms
step:339/2420 train_time:20298ms step_avg:59.87ms
step:340/2420 train_time:20358ms step_avg:59.88ms
step:341/2420 train_time:20417ms step_avg:59.87ms
step:342/2420 train_time:20478ms step_avg:59.88ms
step:343/2420 train_time:20537ms step_avg:59.87ms
step:344/2420 train_time:20597ms step_avg:59.88ms
step:345/2420 train_time:20656ms step_avg:59.87ms
step:346/2420 train_time:20716ms step_avg:59.87ms
step:347/2420 train_time:20774ms step_avg:59.87ms
step:348/2420 train_time:20834ms step_avg:59.87ms
step:349/2420 train_time:20893ms step_avg:59.86ms
step:350/2420 train_time:20953ms step_avg:59.87ms
step:351/2420 train_time:21011ms step_avg:59.86ms
step:352/2420 train_time:21071ms step_avg:59.86ms
step:353/2420 train_time:21129ms step_avg:59.86ms
step:354/2420 train_time:21190ms step_avg:59.86ms
step:355/2420 train_time:21249ms step_avg:59.86ms
step:356/2420 train_time:21310ms step_avg:59.86ms
step:357/2420 train_time:21369ms step_avg:59.86ms
step:358/2420 train_time:21430ms step_avg:59.86ms
step:359/2420 train_time:21488ms step_avg:59.86ms
step:360/2420 train_time:21549ms step_avg:59.86ms
step:361/2420 train_time:21607ms step_avg:59.85ms
step:362/2420 train_time:21668ms step_avg:59.86ms
step:363/2420 train_time:21726ms step_avg:59.85ms
step:364/2420 train_time:21787ms step_avg:59.86ms
step:365/2420 train_time:21846ms step_avg:59.85ms
step:366/2420 train_time:21906ms step_avg:59.85ms
step:367/2420 train_time:21964ms step_avg:59.85ms
step:368/2420 train_time:22024ms step_avg:59.85ms
step:369/2420 train_time:22083ms step_avg:59.84ms
step:370/2420 train_time:22143ms step_avg:59.85ms
step:371/2420 train_time:22201ms step_avg:59.84ms
step:372/2420 train_time:22262ms step_avg:59.84ms
step:373/2420 train_time:22321ms step_avg:59.84ms
step:374/2420 train_time:22382ms step_avg:59.84ms
step:375/2420 train_time:22441ms step_avg:59.84ms
step:376/2420 train_time:22501ms step_avg:59.84ms
step:377/2420 train_time:22560ms step_avg:59.84ms
step:378/2420 train_time:22620ms step_avg:59.84ms
step:379/2420 train_time:22679ms step_avg:59.84ms
step:380/2420 train_time:22739ms step_avg:59.84ms
step:381/2420 train_time:22797ms step_avg:59.84ms
step:382/2420 train_time:22858ms step_avg:59.84ms
step:383/2420 train_time:22916ms step_avg:59.83ms
step:384/2420 train_time:22976ms step_avg:59.83ms
step:385/2420 train_time:23034ms step_avg:59.83ms
step:386/2420 train_time:23095ms step_avg:59.83ms
step:387/2420 train_time:23153ms step_avg:59.83ms
step:388/2420 train_time:23213ms step_avg:59.83ms
step:389/2420 train_time:23272ms step_avg:59.82ms
step:390/2420 train_time:23333ms step_avg:59.83ms
step:391/2420 train_time:23392ms step_avg:59.83ms
step:392/2420 train_time:23452ms step_avg:59.83ms
step:393/2420 train_time:23510ms step_avg:59.82ms
step:394/2420 train_time:23570ms step_avg:59.82ms
step:395/2420 train_time:23628ms step_avg:59.82ms
step:396/2420 train_time:23689ms step_avg:59.82ms
step:397/2420 train_time:23748ms step_avg:59.82ms
step:398/2420 train_time:23809ms step_avg:59.82ms
step:399/2420 train_time:23867ms step_avg:59.82ms
step:400/2420 train_time:23927ms step_avg:59.82ms
step:401/2420 train_time:23986ms step_avg:59.81ms
step:402/2420 train_time:24046ms step_avg:59.82ms
step:403/2420 train_time:24105ms step_avg:59.81ms
step:404/2420 train_time:24166ms step_avg:59.82ms
step:405/2420 train_time:24224ms step_avg:59.81ms
step:406/2420 train_time:24285ms step_avg:59.82ms
step:407/2420 train_time:24344ms step_avg:59.81ms
step:408/2420 train_time:24405ms step_avg:59.82ms
step:409/2420 train_time:24464ms step_avg:59.81ms
step:410/2420 train_time:24524ms step_avg:59.81ms
step:411/2420 train_time:24582ms step_avg:59.81ms
step:412/2420 train_time:24643ms step_avg:59.81ms
step:413/2420 train_time:24701ms step_avg:59.81ms
step:414/2420 train_time:24762ms step_avg:59.81ms
step:415/2420 train_time:24821ms step_avg:59.81ms
step:416/2420 train_time:24881ms step_avg:59.81ms
step:417/2420 train_time:24940ms step_avg:59.81ms
step:418/2420 train_time:25000ms step_avg:59.81ms
step:419/2420 train_time:25059ms step_avg:59.81ms
step:420/2420 train_time:25119ms step_avg:59.81ms
step:421/2420 train_time:25178ms step_avg:59.80ms
step:422/2420 train_time:25238ms step_avg:59.81ms
step:423/2420 train_time:25297ms step_avg:59.80ms
step:424/2420 train_time:25357ms step_avg:59.80ms
step:425/2420 train_time:25416ms step_avg:59.80ms
step:426/2420 train_time:25476ms step_avg:59.80ms
step:427/2420 train_time:25534ms step_avg:59.80ms
step:428/2420 train_time:25594ms step_avg:59.80ms
step:429/2420 train_time:25653ms step_avg:59.80ms
step:430/2420 train_time:25713ms step_avg:59.80ms
step:431/2420 train_time:25771ms step_avg:59.79ms
step:432/2420 train_time:25831ms step_avg:59.79ms
step:433/2420 train_time:25889ms step_avg:59.79ms
step:434/2420 train_time:25949ms step_avg:59.79ms
step:435/2420 train_time:26008ms step_avg:59.79ms
step:436/2420 train_time:26069ms step_avg:59.79ms
step:437/2420 train_time:26128ms step_avg:59.79ms
step:438/2420 train_time:26189ms step_avg:59.79ms
step:439/2420 train_time:26248ms step_avg:59.79ms
step:440/2420 train_time:26310ms step_avg:59.79ms
step:441/2420 train_time:26368ms step_avg:59.79ms
step:442/2420 train_time:26429ms step_avg:59.79ms
step:443/2420 train_time:26487ms step_avg:59.79ms
step:444/2420 train_time:26548ms step_avg:59.79ms
step:445/2420 train_time:26608ms step_avg:59.79ms
step:446/2420 train_time:26668ms step_avg:59.79ms
step:447/2420 train_time:26726ms step_avg:59.79ms
step:448/2420 train_time:26785ms step_avg:59.79ms
step:449/2420 train_time:26844ms step_avg:59.79ms
step:450/2420 train_time:26904ms step_avg:59.79ms
step:451/2420 train_time:26963ms step_avg:59.78ms
step:452/2420 train_time:27023ms step_avg:59.79ms
step:453/2420 train_time:27082ms step_avg:59.78ms
step:454/2420 train_time:27142ms step_avg:59.78ms
step:455/2420 train_time:27201ms step_avg:59.78ms
step:456/2420 train_time:27262ms step_avg:59.78ms
step:457/2420 train_time:27321ms step_avg:59.78ms
step:458/2420 train_time:27381ms step_avg:59.78ms
step:459/2420 train_time:27440ms step_avg:59.78ms
step:460/2420 train_time:27500ms step_avg:59.78ms
step:461/2420 train_time:27559ms step_avg:59.78ms
step:462/2420 train_time:27620ms step_avg:59.78ms
step:463/2420 train_time:27679ms step_avg:59.78ms
step:464/2420 train_time:27739ms step_avg:59.78ms
step:465/2420 train_time:27798ms step_avg:59.78ms
step:466/2420 train_time:27857ms step_avg:59.78ms
step:467/2420 train_time:27916ms step_avg:59.78ms
step:468/2420 train_time:27976ms step_avg:59.78ms
step:469/2420 train_time:28034ms step_avg:59.77ms
step:470/2420 train_time:28094ms step_avg:59.77ms
step:471/2420 train_time:28152ms step_avg:59.77ms
step:472/2420 train_time:28213ms step_avg:59.77ms
step:473/2420 train_time:28271ms step_avg:59.77ms
step:474/2420 train_time:28331ms step_avg:59.77ms
step:475/2420 train_time:28389ms step_avg:59.77ms
step:476/2420 train_time:28450ms step_avg:59.77ms
step:477/2420 train_time:28509ms step_avg:59.77ms
step:478/2420 train_time:28570ms step_avg:59.77ms
step:479/2420 train_time:28628ms step_avg:59.77ms
step:480/2420 train_time:28688ms step_avg:59.77ms
step:481/2420 train_time:28747ms step_avg:59.77ms
step:482/2420 train_time:28808ms step_avg:59.77ms
step:483/2420 train_time:28866ms step_avg:59.76ms
step:484/2420 train_time:28927ms step_avg:59.77ms
step:485/2420 train_time:28985ms step_avg:59.76ms
step:486/2420 train_time:29045ms step_avg:59.76ms
step:487/2420 train_time:29104ms step_avg:59.76ms
step:488/2420 train_time:29165ms step_avg:59.76ms
step:489/2420 train_time:29223ms step_avg:59.76ms
step:490/2420 train_time:29284ms step_avg:59.76ms
step:491/2420 train_time:29343ms step_avg:59.76ms
step:492/2420 train_time:29403ms step_avg:59.76ms
step:493/2420 train_time:29462ms step_avg:59.76ms
step:494/2420 train_time:29523ms step_avg:59.76ms
step:495/2420 train_time:29582ms step_avg:59.76ms
step:496/2420 train_time:29642ms step_avg:59.76ms
step:497/2420 train_time:29701ms step_avg:59.76ms
step:498/2420 train_time:29761ms step_avg:59.76ms
step:499/2420 train_time:29820ms step_avg:59.76ms
step:500/2420 train_time:29880ms step_avg:59.76ms
step:500/2420 val_loss:3.8200 train_time:29943ms step_avg:59.89ms
step:501/2420 train_time:29964ms step_avg:59.81ms
step:502/2420 train_time:30004ms step_avg:59.77ms
step:503/2420 train_time:30065ms step_avg:59.77ms
step:504/2420 train_time:30128ms step_avg:59.78ms
step:505/2420 train_time:30186ms step_avg:59.77ms
step:506/2420 train_time:30247ms step_avg:59.78ms
step:507/2420 train_time:30305ms step_avg:59.77ms
step:508/2420 train_time:30364ms step_avg:59.77ms
step:509/2420 train_time:30422ms step_avg:59.77ms
step:510/2420 train_time:30482ms step_avg:59.77ms
step:511/2420 train_time:30539ms step_avg:59.76ms
step:512/2420 train_time:30599ms step_avg:59.76ms
step:513/2420 train_time:30657ms step_avg:59.76ms
step:514/2420 train_time:30716ms step_avg:59.76ms
step:515/2420 train_time:30774ms step_avg:59.76ms
step:516/2420 train_time:30834ms step_avg:59.76ms
step:517/2420 train_time:30893ms step_avg:59.75ms
step:518/2420 train_time:30955ms step_avg:59.76ms
step:519/2420 train_time:31015ms step_avg:59.76ms
step:520/2420 train_time:31079ms step_avg:59.77ms
step:521/2420 train_time:31138ms step_avg:59.77ms
step:522/2420 train_time:31199ms step_avg:59.77ms
step:523/2420 train_time:31259ms step_avg:59.77ms
step:524/2420 train_time:31319ms step_avg:59.77ms
step:525/2420 train_time:31378ms step_avg:59.77ms
step:526/2420 train_time:31438ms step_avg:59.77ms
step:527/2420 train_time:31496ms step_avg:59.77ms
step:528/2420 train_time:31557ms step_avg:59.77ms
step:529/2420 train_time:31615ms step_avg:59.76ms
step:530/2420 train_time:31675ms step_avg:59.76ms
step:531/2420 train_time:31733ms step_avg:59.76ms
step:532/2420 train_time:31793ms step_avg:59.76ms
step:533/2420 train_time:31851ms step_avg:59.76ms
step:534/2420 train_time:31911ms step_avg:59.76ms
step:535/2420 train_time:31971ms step_avg:59.76ms
step:536/2420 train_time:32032ms step_avg:59.76ms
step:537/2420 train_time:32091ms step_avg:59.76ms
step:538/2420 train_time:32152ms step_avg:59.76ms
step:539/2420 train_time:32211ms step_avg:59.76ms
step:540/2420 train_time:32271ms step_avg:59.76ms
step:541/2420 train_time:32330ms step_avg:59.76ms
step:542/2420 train_time:32390ms step_avg:59.76ms
step:543/2420 train_time:32449ms step_avg:59.76ms
step:544/2420 train_time:32509ms step_avg:59.76ms
step:545/2420 train_time:32568ms step_avg:59.76ms
step:546/2420 train_time:32628ms step_avg:59.76ms
step:547/2420 train_time:32685ms step_avg:59.75ms
step:548/2420 train_time:32745ms step_avg:59.75ms
step:549/2420 train_time:32803ms step_avg:59.75ms
step:550/2420 train_time:32863ms step_avg:59.75ms
step:551/2420 train_time:32921ms step_avg:59.75ms
step:552/2420 train_time:32982ms step_avg:59.75ms
step:553/2420 train_time:33040ms step_avg:59.75ms
step:554/2420 train_time:33102ms step_avg:59.75ms
step:555/2420 train_time:33161ms step_avg:59.75ms
step:556/2420 train_time:33222ms step_avg:59.75ms
step:557/2420 train_time:33281ms step_avg:59.75ms
step:558/2420 train_time:33341ms step_avg:59.75ms
step:559/2420 train_time:33401ms step_avg:59.75ms
step:560/2420 train_time:33462ms step_avg:59.75ms
step:561/2420 train_time:33520ms step_avg:59.75ms
step:562/2420 train_time:33581ms step_avg:59.75ms
step:563/2420 train_time:33639ms step_avg:59.75ms
step:564/2420 train_time:33699ms step_avg:59.75ms
step:565/2420 train_time:33758ms step_avg:59.75ms
step:566/2420 train_time:33819ms step_avg:59.75ms
step:567/2420 train_time:33877ms step_avg:59.75ms
step:568/2420 train_time:33937ms step_avg:59.75ms
step:569/2420 train_time:33996ms step_avg:59.75ms
step:570/2420 train_time:34057ms step_avg:59.75ms
step:571/2420 train_time:34116ms step_avg:59.75ms
step:572/2420 train_time:34177ms step_avg:59.75ms
step:573/2420 train_time:34236ms step_avg:59.75ms
step:574/2420 train_time:34296ms step_avg:59.75ms
step:575/2420 train_time:34356ms step_avg:59.75ms
step:576/2420 train_time:34417ms step_avg:59.75ms
step:577/2420 train_time:34475ms step_avg:59.75ms
step:578/2420 train_time:34536ms step_avg:59.75ms
step:579/2420 train_time:34594ms step_avg:59.75ms
step:580/2420 train_time:34655ms step_avg:59.75ms
step:581/2420 train_time:34713ms step_avg:59.75ms
step:582/2420 train_time:34774ms step_avg:59.75ms
step:583/2420 train_time:34832ms step_avg:59.75ms
step:584/2420 train_time:34893ms step_avg:59.75ms
step:585/2420 train_time:34951ms step_avg:59.75ms
step:586/2420 train_time:35011ms step_avg:59.75ms
step:587/2420 train_time:35070ms step_avg:59.74ms
step:588/2420 train_time:35131ms step_avg:59.75ms
step:589/2420 train_time:35189ms step_avg:59.74ms
step:590/2420 train_time:35249ms step_avg:59.74ms
step:591/2420 train_time:35308ms step_avg:59.74ms
step:592/2420 train_time:35368ms step_avg:59.74ms
step:593/2420 train_time:35427ms step_avg:59.74ms
step:594/2420 train_time:35487ms step_avg:59.74ms
step:595/2420 train_time:35545ms step_avg:59.74ms
step:596/2420 train_time:35606ms step_avg:59.74ms
step:597/2420 train_time:35664ms step_avg:59.74ms
step:598/2420 train_time:35723ms step_avg:59.74ms
step:599/2420 train_time:35782ms step_avg:59.74ms
step:600/2420 train_time:35842ms step_avg:59.74ms
step:601/2420 train_time:35900ms step_avg:59.73ms
step:602/2420 train_time:35962ms step_avg:59.74ms
step:603/2420 train_time:36021ms step_avg:59.74ms
step:604/2420 train_time:36081ms step_avg:59.74ms
step:605/2420 train_time:36140ms step_avg:59.74ms
step:606/2420 train_time:36201ms step_avg:59.74ms
step:607/2420 train_time:36260ms step_avg:59.74ms
step:608/2420 train_time:36321ms step_avg:59.74ms
step:609/2420 train_time:36380ms step_avg:59.74ms
step:610/2420 train_time:36441ms step_avg:59.74ms
step:611/2420 train_time:36499ms step_avg:59.74ms
step:612/2420 train_time:36560ms step_avg:59.74ms
step:613/2420 train_time:36620ms step_avg:59.74ms
step:614/2420 train_time:36680ms step_avg:59.74ms
step:615/2420 train_time:36738ms step_avg:59.74ms
step:616/2420 train_time:36799ms step_avg:59.74ms
step:617/2420 train_time:36857ms step_avg:59.74ms
step:618/2420 train_time:36918ms step_avg:59.74ms
step:619/2420 train_time:36976ms step_avg:59.73ms
step:620/2420 train_time:37036ms step_avg:59.74ms
step:621/2420 train_time:37095ms step_avg:59.73ms
step:622/2420 train_time:37156ms step_avg:59.74ms
step:623/2420 train_time:37215ms step_avg:59.73ms
step:624/2420 train_time:37276ms step_avg:59.74ms
step:625/2420 train_time:37335ms step_avg:59.74ms
step:626/2420 train_time:37395ms step_avg:59.74ms
step:627/2420 train_time:37454ms step_avg:59.74ms
step:628/2420 train_time:37515ms step_avg:59.74ms
step:629/2420 train_time:37574ms step_avg:59.74ms
step:630/2420 train_time:37634ms step_avg:59.74ms
step:631/2420 train_time:37693ms step_avg:59.74ms
step:632/2420 train_time:37754ms step_avg:59.74ms
step:633/2420 train_time:37812ms step_avg:59.74ms
step:634/2420 train_time:37874ms step_avg:59.74ms
step:635/2420 train_time:37932ms step_avg:59.73ms
step:636/2420 train_time:37992ms step_avg:59.74ms
step:637/2420 train_time:38050ms step_avg:59.73ms
step:638/2420 train_time:38110ms step_avg:59.73ms
step:639/2420 train_time:38169ms step_avg:59.73ms
step:640/2420 train_time:38229ms step_avg:59.73ms
step:641/2420 train_time:38287ms step_avg:59.73ms
step:642/2420 train_time:38348ms step_avg:59.73ms
step:643/2420 train_time:38406ms step_avg:59.73ms
step:644/2420 train_time:38466ms step_avg:59.73ms
step:645/2420 train_time:38524ms step_avg:59.73ms
step:646/2420 train_time:38584ms step_avg:59.73ms
step:647/2420 train_time:38642ms step_avg:59.73ms
step:648/2420 train_time:38703ms step_avg:59.73ms
step:649/2420 train_time:38762ms step_avg:59.73ms
step:650/2420 train_time:38823ms step_avg:59.73ms
step:651/2420 train_time:38881ms step_avg:59.73ms
step:652/2420 train_time:38941ms step_avg:59.73ms
step:653/2420 train_time:39000ms step_avg:59.72ms
step:654/2420 train_time:39061ms step_avg:59.73ms
step:655/2420 train_time:39120ms step_avg:59.73ms
step:656/2420 train_time:39181ms step_avg:59.73ms
step:657/2420 train_time:39239ms step_avg:59.73ms
step:658/2420 train_time:39300ms step_avg:59.73ms
step:659/2420 train_time:39358ms step_avg:59.72ms
step:660/2420 train_time:39419ms step_avg:59.73ms
step:661/2420 train_time:39478ms step_avg:59.72ms
step:662/2420 train_time:39538ms step_avg:59.73ms
step:663/2420 train_time:39597ms step_avg:59.72ms
step:664/2420 train_time:39657ms step_avg:59.72ms
step:665/2420 train_time:39716ms step_avg:59.72ms
step:666/2420 train_time:39776ms step_avg:59.72ms
step:667/2420 train_time:39835ms step_avg:59.72ms
step:668/2420 train_time:39895ms step_avg:59.72ms
step:669/2420 train_time:39954ms step_avg:59.72ms
step:670/2420 train_time:40015ms step_avg:59.72ms
step:671/2420 train_time:40074ms step_avg:59.72ms
step:672/2420 train_time:40134ms step_avg:59.72ms
step:673/2420 train_time:40194ms step_avg:59.72ms
step:674/2420 train_time:40254ms step_avg:59.72ms
step:675/2420 train_time:40313ms step_avg:59.72ms
step:676/2420 train_time:40373ms step_avg:59.72ms
step:677/2420 train_time:40432ms step_avg:59.72ms
step:678/2420 train_time:40492ms step_avg:59.72ms
step:679/2420 train_time:40550ms step_avg:59.72ms
step:680/2420 train_time:40611ms step_avg:59.72ms
step:681/2420 train_time:40670ms step_avg:59.72ms
step:682/2420 train_time:40730ms step_avg:59.72ms
step:683/2420 train_time:40788ms step_avg:59.72ms
step:684/2420 train_time:40849ms step_avg:59.72ms
step:685/2420 train_time:40907ms step_avg:59.72ms
step:686/2420 train_time:40968ms step_avg:59.72ms
step:687/2420 train_time:41026ms step_avg:59.72ms
step:688/2420 train_time:41087ms step_avg:59.72ms
step:689/2420 train_time:41145ms step_avg:59.72ms
step:690/2420 train_time:41205ms step_avg:59.72ms
step:691/2420 train_time:41264ms step_avg:59.72ms
step:692/2420 train_time:41324ms step_avg:59.72ms
step:693/2420 train_time:41383ms step_avg:59.72ms
step:694/2420 train_time:41443ms step_avg:59.72ms
step:695/2420 train_time:41502ms step_avg:59.72ms
step:696/2420 train_time:41563ms step_avg:59.72ms
step:697/2420 train_time:41621ms step_avg:59.72ms
step:698/2420 train_time:41682ms step_avg:59.72ms
step:699/2420 train_time:41740ms step_avg:59.71ms
step:700/2420 train_time:41801ms step_avg:59.72ms
step:701/2420 train_time:41859ms step_avg:59.71ms
step:702/2420 train_time:41920ms step_avg:59.71ms
step:703/2420 train_time:41978ms step_avg:59.71ms
step:704/2420 train_time:42039ms step_avg:59.71ms
step:705/2420 train_time:42097ms step_avg:59.71ms
step:706/2420 train_time:42158ms step_avg:59.71ms
step:707/2420 train_time:42217ms step_avg:59.71ms
step:708/2420 train_time:42277ms step_avg:59.71ms
step:709/2420 train_time:42336ms step_avg:59.71ms
step:710/2420 train_time:42397ms step_avg:59.71ms
step:711/2420 train_time:42455ms step_avg:59.71ms
step:712/2420 train_time:42516ms step_avg:59.71ms
step:713/2420 train_time:42575ms step_avg:59.71ms
step:714/2420 train_time:42636ms step_avg:59.71ms
step:715/2420 train_time:42694ms step_avg:59.71ms
step:716/2420 train_time:42755ms step_avg:59.71ms
step:717/2420 train_time:42814ms step_avg:59.71ms
step:718/2420 train_time:42875ms step_avg:59.71ms
step:719/2420 train_time:42933ms step_avg:59.71ms
step:720/2420 train_time:42994ms step_avg:59.71ms
step:721/2420 train_time:43053ms step_avg:59.71ms
step:722/2420 train_time:43113ms step_avg:59.71ms
step:723/2420 train_time:43172ms step_avg:59.71ms
step:724/2420 train_time:43232ms step_avg:59.71ms
step:725/2420 train_time:43291ms step_avg:59.71ms
step:726/2420 train_time:43352ms step_avg:59.71ms
step:727/2420 train_time:43411ms step_avg:59.71ms
step:728/2420 train_time:43472ms step_avg:59.71ms
step:729/2420 train_time:43531ms step_avg:59.71ms
step:730/2420 train_time:43591ms step_avg:59.71ms
step:731/2420 train_time:43650ms step_avg:59.71ms
step:732/2420 train_time:43710ms step_avg:59.71ms
step:733/2420 train_time:43768ms step_avg:59.71ms
step:734/2420 train_time:43828ms step_avg:59.71ms
step:735/2420 train_time:43886ms step_avg:59.71ms
step:736/2420 train_time:43947ms step_avg:59.71ms
step:737/2420 train_time:44004ms step_avg:59.71ms
step:738/2420 train_time:44065ms step_avg:59.71ms
step:739/2420 train_time:44123ms step_avg:59.71ms
step:740/2420 train_time:44184ms step_avg:59.71ms
step:741/2420 train_time:44242ms step_avg:59.71ms
step:742/2420 train_time:44303ms step_avg:59.71ms
step:743/2420 train_time:44362ms step_avg:59.71ms
step:744/2420 train_time:44423ms step_avg:59.71ms
step:745/2420 train_time:44482ms step_avg:59.71ms
step:746/2420 train_time:44542ms step_avg:59.71ms
step:747/2420 train_time:44601ms step_avg:59.71ms
step:748/2420 train_time:44662ms step_avg:59.71ms
step:749/2420 train_time:44720ms step_avg:59.71ms
step:750/2420 train_time:44780ms step_avg:59.71ms
step:750/2420 val_loss:3.6914 train_time:44842ms step_avg:59.79ms
step:751/2420 train_time:44863ms step_avg:59.74ms
step:752/2420 train_time:44901ms step_avg:59.71ms
step:753/2420 train_time:44962ms step_avg:59.71ms
step:754/2420 train_time:45026ms step_avg:59.72ms
step:755/2420 train_time:45085ms step_avg:59.72ms
step:756/2420 train_time:45146ms step_avg:59.72ms
step:757/2420 train_time:45204ms step_avg:59.72ms
step:758/2420 train_time:45264ms step_avg:59.72ms
step:759/2420 train_time:45323ms step_avg:59.71ms
step:760/2420 train_time:45383ms step_avg:59.71ms
step:761/2420 train_time:45441ms step_avg:59.71ms
step:762/2420 train_time:45501ms step_avg:59.71ms
step:763/2420 train_time:45559ms step_avg:59.71ms
step:764/2420 train_time:45619ms step_avg:59.71ms
step:765/2420 train_time:45677ms step_avg:59.71ms
step:766/2420 train_time:45737ms step_avg:59.71ms
step:767/2420 train_time:45797ms step_avg:59.71ms
step:768/2420 train_time:45858ms step_avg:59.71ms
step:769/2420 train_time:45918ms step_avg:59.71ms
step:770/2420 train_time:45981ms step_avg:59.72ms
step:771/2420 train_time:46041ms step_avg:59.72ms
step:772/2420 train_time:46102ms step_avg:59.72ms
step:773/2420 train_time:46162ms step_avg:59.72ms
step:774/2420 train_time:46222ms step_avg:59.72ms
step:775/2420 train_time:46280ms step_avg:59.72ms
step:776/2420 train_time:46340ms step_avg:59.72ms
step:777/2420 train_time:46398ms step_avg:59.71ms
step:778/2420 train_time:46458ms step_avg:59.71ms
step:779/2420 train_time:46516ms step_avg:59.71ms
step:780/2420 train_time:46576ms step_avg:59.71ms
step:781/2420 train_time:46633ms step_avg:59.71ms
step:782/2420 train_time:46693ms step_avg:59.71ms
step:783/2420 train_time:46752ms step_avg:59.71ms
step:784/2420 train_time:46812ms step_avg:59.71ms
step:785/2420 train_time:46871ms step_avg:59.71ms
step:786/2420 train_time:46932ms step_avg:59.71ms
step:787/2420 train_time:46992ms step_avg:59.71ms
step:788/2420 train_time:47052ms step_avg:59.71ms
step:789/2420 train_time:47111ms step_avg:59.71ms
step:790/2420 train_time:47171ms step_avg:59.71ms
step:791/2420 train_time:47230ms step_avg:59.71ms
step:792/2420 train_time:47290ms step_avg:59.71ms
step:793/2420 train_time:47348ms step_avg:59.71ms
step:794/2420 train_time:47408ms step_avg:59.71ms
step:795/2420 train_time:47467ms step_avg:59.71ms
step:796/2420 train_time:47527ms step_avg:59.71ms
step:797/2420 train_time:47586ms step_avg:59.71ms
step:798/2420 train_time:47647ms step_avg:59.71ms
step:799/2420 train_time:47707ms step_avg:59.71ms
step:800/2420 train_time:47768ms step_avg:59.71ms
step:801/2420 train_time:47828ms step_avg:59.71ms
step:802/2420 train_time:47889ms step_avg:59.71ms
step:803/2420 train_time:47949ms step_avg:59.71ms
step:804/2420 train_time:48012ms step_avg:59.72ms
step:805/2420 train_time:48071ms step_avg:59.72ms
step:806/2420 train_time:48131ms step_avg:59.72ms
step:807/2420 train_time:48191ms step_avg:59.72ms
step:808/2420 train_time:48252ms step_avg:59.72ms
step:809/2420 train_time:48311ms step_avg:59.72ms
step:810/2420 train_time:48372ms step_avg:59.72ms
step:811/2420 train_time:48430ms step_avg:59.72ms
step:812/2420 train_time:48491ms step_avg:59.72ms
step:813/2420 train_time:48550ms step_avg:59.72ms
step:814/2420 train_time:48611ms step_avg:59.72ms
step:815/2420 train_time:48670ms step_avg:59.72ms
step:816/2420 train_time:48731ms step_avg:59.72ms
step:817/2420 train_time:48791ms step_avg:59.72ms
step:818/2420 train_time:48851ms step_avg:59.72ms
step:819/2420 train_time:48911ms step_avg:59.72ms
step:820/2420 train_time:48972ms step_avg:59.72ms
step:821/2420 train_time:49031ms step_avg:59.72ms
step:822/2420 train_time:49092ms step_avg:59.72ms
step:823/2420 train_time:49151ms step_avg:59.72ms
step:824/2420 train_time:49212ms step_avg:59.72ms
step:825/2420 train_time:49272ms step_avg:59.72ms
step:826/2420 train_time:49332ms step_avg:59.72ms
step:827/2420 train_time:49391ms step_avg:59.72ms
step:828/2420 train_time:49452ms step_avg:59.72ms
step:829/2420 train_time:49511ms step_avg:59.72ms
step:830/2420 train_time:49572ms step_avg:59.73ms
step:831/2420 train_time:49631ms step_avg:59.72ms
step:832/2420 train_time:49692ms step_avg:59.73ms
step:833/2420 train_time:49751ms step_avg:59.73ms
step:834/2420 train_time:49812ms step_avg:59.73ms
step:835/2420 train_time:49871ms step_avg:59.73ms
step:836/2420 train_time:49932ms step_avg:59.73ms
step:837/2420 train_time:49991ms step_avg:59.73ms
step:838/2420 train_time:50052ms step_avg:59.73ms
step:839/2420 train_time:50112ms step_avg:59.73ms
step:840/2420 train_time:50172ms step_avg:59.73ms
step:841/2420 train_time:50232ms step_avg:59.73ms
step:842/2420 train_time:50292ms step_avg:59.73ms
step:843/2420 train_time:50351ms step_avg:59.73ms
step:844/2420 train_time:50412ms step_avg:59.73ms
step:845/2420 train_time:50472ms step_avg:59.73ms
step:846/2420 train_time:50533ms step_avg:59.73ms
step:847/2420 train_time:50593ms step_avg:59.73ms
step:848/2420 train_time:50654ms step_avg:59.73ms
step:849/2420 train_time:50713ms step_avg:59.73ms
step:850/2420 train_time:50774ms step_avg:59.73ms
step:851/2420 train_time:50834ms step_avg:59.73ms
step:852/2420 train_time:50895ms step_avg:59.74ms
step:853/2420 train_time:50954ms step_avg:59.74ms
step:854/2420 train_time:51016ms step_avg:59.74ms
step:855/2420 train_time:51075ms step_avg:59.74ms
step:856/2420 train_time:51137ms step_avg:59.74ms
step:857/2420 train_time:51196ms step_avg:59.74ms
step:858/2420 train_time:51257ms step_avg:59.74ms
step:859/2420 train_time:51316ms step_avg:59.74ms
step:860/2420 train_time:51378ms step_avg:59.74ms
step:861/2420 train_time:51437ms step_avg:59.74ms
step:862/2420 train_time:51498ms step_avg:59.74ms
step:863/2420 train_time:51558ms step_avg:59.74ms
step:864/2420 train_time:51619ms step_avg:59.74ms
step:865/2420 train_time:51679ms step_avg:59.74ms
step:866/2420 train_time:51741ms step_avg:59.75ms
step:867/2420 train_time:51800ms step_avg:59.75ms
step:868/2420 train_time:51862ms step_avg:59.75ms
step:869/2420 train_time:51921ms step_avg:59.75ms
step:870/2420 train_time:51982ms step_avg:59.75ms
step:871/2420 train_time:52041ms step_avg:59.75ms
step:872/2420 train_time:52102ms step_avg:59.75ms
step:873/2420 train_time:52162ms step_avg:59.75ms
step:874/2420 train_time:52224ms step_avg:59.75ms
step:875/2420 train_time:52284ms step_avg:59.75ms
step:876/2420 train_time:52345ms step_avg:59.75ms
step:877/2420 train_time:52405ms step_avg:59.75ms
step:878/2420 train_time:52466ms step_avg:59.76ms
step:879/2420 train_time:52526ms step_avg:59.76ms
step:880/2420 train_time:52587ms step_avg:59.76ms
step:881/2420 train_time:52647ms step_avg:59.76ms
step:882/2420 train_time:52708ms step_avg:59.76ms
step:883/2420 train_time:52767ms step_avg:59.76ms
step:884/2420 train_time:52829ms step_avg:59.76ms
step:885/2420 train_time:52888ms step_avg:59.76ms
step:886/2420 train_time:52948ms step_avg:59.76ms
step:887/2420 train_time:53008ms step_avg:59.76ms
step:888/2420 train_time:53069ms step_avg:59.76ms
step:889/2420 train_time:53128ms step_avg:59.76ms
step:890/2420 train_time:53189ms step_avg:59.76ms
step:891/2420 train_time:53249ms step_avg:59.76ms
step:892/2420 train_time:53311ms step_avg:59.77ms
step:893/2420 train_time:53370ms step_avg:59.76ms
step:894/2420 train_time:53430ms step_avg:59.77ms
step:895/2420 train_time:53489ms step_avg:59.76ms
step:896/2420 train_time:53550ms step_avg:59.77ms
step:897/2420 train_time:53609ms step_avg:59.77ms
step:898/2420 train_time:53670ms step_avg:59.77ms
step:899/2420 train_time:53729ms step_avg:59.77ms
step:900/2420 train_time:53790ms step_avg:59.77ms
step:901/2420 train_time:53849ms step_avg:59.77ms
step:902/2420 train_time:53910ms step_avg:59.77ms
step:903/2420 train_time:53968ms step_avg:59.77ms
step:904/2420 train_time:54029ms step_avg:59.77ms
step:905/2420 train_time:54088ms step_avg:59.77ms
step:906/2420 train_time:54150ms step_avg:59.77ms
step:907/2420 train_time:54209ms step_avg:59.77ms
step:908/2420 train_time:54270ms step_avg:59.77ms
step:909/2420 train_time:54329ms step_avg:59.77ms
step:910/2420 train_time:54390ms step_avg:59.77ms
step:911/2420 train_time:54449ms step_avg:59.77ms
step:912/2420 train_time:54511ms step_avg:59.77ms
step:913/2420 train_time:54569ms step_avg:59.77ms
step:914/2420 train_time:54630ms step_avg:59.77ms
step:915/2420 train_time:54689ms step_avg:59.77ms
step:916/2420 train_time:54750ms step_avg:59.77ms
step:917/2420 train_time:54809ms step_avg:59.77ms
step:918/2420 train_time:54870ms step_avg:59.77ms
step:919/2420 train_time:54929ms step_avg:59.77ms
step:920/2420 train_time:54990ms step_avg:59.77ms
step:921/2420 train_time:55049ms step_avg:59.77ms
step:922/2420 train_time:55111ms step_avg:59.77ms
step:923/2420 train_time:55170ms step_avg:59.77ms
step:924/2420 train_time:55230ms step_avg:59.77ms
step:925/2420 train_time:55290ms step_avg:59.77ms
step:926/2420 train_time:55351ms step_avg:59.77ms
step:927/2420 train_time:55411ms step_avg:59.77ms
step:928/2420 train_time:55472ms step_avg:59.78ms
step:929/2420 train_time:55531ms step_avg:59.77ms
step:930/2420 train_time:55591ms step_avg:59.78ms
step:931/2420 train_time:55650ms step_avg:59.77ms
step:932/2420 train_time:55711ms step_avg:59.78ms
step:933/2420 train_time:55770ms step_avg:59.77ms
step:934/2420 train_time:55830ms step_avg:59.78ms
step:935/2420 train_time:55889ms step_avg:59.77ms
step:936/2420 train_time:55950ms step_avg:59.78ms
step:937/2420 train_time:56009ms step_avg:59.78ms
step:938/2420 train_time:56070ms step_avg:59.78ms
step:939/2420 train_time:56130ms step_avg:59.78ms
step:940/2420 train_time:56191ms step_avg:59.78ms
step:941/2420 train_time:56250ms step_avg:59.78ms
step:942/2420 train_time:56311ms step_avg:59.78ms
step:943/2420 train_time:56370ms step_avg:59.78ms
step:944/2420 train_time:56431ms step_avg:59.78ms
step:945/2420 train_time:56490ms step_avg:59.78ms
step:946/2420 train_time:56551ms step_avg:59.78ms
step:947/2420 train_time:56610ms step_avg:59.78ms
step:948/2420 train_time:56670ms step_avg:59.78ms
step:949/2420 train_time:56729ms step_avg:59.78ms
step:950/2420 train_time:56790ms step_avg:59.78ms
step:951/2420 train_time:56849ms step_avg:59.78ms
step:952/2420 train_time:56911ms step_avg:59.78ms
step:953/2420 train_time:56970ms step_avg:59.78ms
step:954/2420 train_time:57030ms step_avg:59.78ms
step:955/2420 train_time:57089ms step_avg:59.78ms
step:956/2420 train_time:57150ms step_avg:59.78ms
step:957/2420 train_time:57210ms step_avg:59.78ms
step:958/2420 train_time:57271ms step_avg:59.78ms
step:959/2420 train_time:57330ms step_avg:59.78ms
step:960/2420 train_time:57390ms step_avg:59.78ms
step:961/2420 train_time:57450ms step_avg:59.78ms
step:962/2420 train_time:57511ms step_avg:59.78ms
step:963/2420 train_time:57570ms step_avg:59.78ms
step:964/2420 train_time:57630ms step_avg:59.78ms
step:965/2420 train_time:57690ms step_avg:59.78ms
step:966/2420 train_time:57751ms step_avg:59.78ms
step:967/2420 train_time:57810ms step_avg:59.78ms
step:968/2420 train_time:57871ms step_avg:59.78ms
step:969/2420 train_time:57930ms step_avg:59.78ms
step:970/2420 train_time:57991ms step_avg:59.78ms
step:971/2420 train_time:58051ms step_avg:59.78ms
step:972/2420 train_time:58112ms step_avg:59.79ms
step:973/2420 train_time:58171ms step_avg:59.78ms
step:974/2420 train_time:58232ms step_avg:59.79ms
step:975/2420 train_time:58291ms step_avg:59.79ms
step:976/2420 train_time:58352ms step_avg:59.79ms
step:977/2420 train_time:58411ms step_avg:59.79ms
step:978/2420 train_time:58472ms step_avg:59.79ms
step:979/2420 train_time:58531ms step_avg:59.79ms
step:980/2420 train_time:58592ms step_avg:59.79ms
step:981/2420 train_time:58652ms step_avg:59.79ms
step:982/2420 train_time:58713ms step_avg:59.79ms
step:983/2420 train_time:58772ms step_avg:59.79ms
step:984/2420 train_time:58833ms step_avg:59.79ms
step:985/2420 train_time:58892ms step_avg:59.79ms
step:986/2420 train_time:58953ms step_avg:59.79ms
step:987/2420 train_time:59013ms step_avg:59.79ms
step:988/2420 train_time:59074ms step_avg:59.79ms
step:989/2420 train_time:59133ms step_avg:59.79ms
step:990/2420 train_time:59195ms step_avg:59.79ms
step:991/2420 train_time:59254ms step_avg:59.79ms
step:992/2420 train_time:59315ms step_avg:59.79ms
step:993/2420 train_time:59374ms step_avg:59.79ms
step:994/2420 train_time:59436ms step_avg:59.79ms
step:995/2420 train_time:59494ms step_avg:59.79ms
step:996/2420 train_time:59555ms step_avg:59.79ms
step:997/2420 train_time:59615ms step_avg:59.79ms
step:998/2420 train_time:59676ms step_avg:59.80ms
step:999/2420 train_time:59735ms step_avg:59.80ms
step:1000/2420 train_time:59797ms step_avg:59.80ms
step:1000/2420 val_loss:3.5801 train_time:59860ms step_avg:59.86ms
step:1001/2420 train_time:59881ms step_avg:59.82ms
step:1002/2420 train_time:59919ms step_avg:59.80ms
step:1003/2420 train_time:59978ms step_avg:59.80ms
step:1004/2420 train_time:60040ms step_avg:59.80ms
step:1005/2420 train_time:60101ms step_avg:59.80ms
step:1006/2420 train_time:60163ms step_avg:59.80ms
step:1007/2420 train_time:60223ms step_avg:59.80ms
step:1008/2420 train_time:60284ms step_avg:59.81ms
step:1009/2420 train_time:60342ms step_avg:59.80ms
step:1010/2420 train_time:60403ms step_avg:59.81ms
step:1011/2420 train_time:60462ms step_avg:59.80ms
step:1012/2420 train_time:60522ms step_avg:59.80ms
step:1013/2420 train_time:60581ms step_avg:59.80ms
step:1014/2420 train_time:60642ms step_avg:59.80ms
step:1015/2420 train_time:60700ms step_avg:59.80ms
step:1016/2420 train_time:60761ms step_avg:59.80ms
step:1017/2420 train_time:60824ms step_avg:59.81ms
step:1018/2420 train_time:60887ms step_avg:59.81ms
step:1019/2420 train_time:60947ms step_avg:59.81ms
step:1020/2420 train_time:61009ms step_avg:59.81ms
step:1021/2420 train_time:61069ms step_avg:59.81ms
step:1022/2420 train_time:61132ms step_avg:59.82ms
step:1023/2420 train_time:61192ms step_avg:59.82ms
step:1024/2420 train_time:61253ms step_avg:59.82ms
step:1025/2420 train_time:61312ms step_avg:59.82ms
step:1026/2420 train_time:61374ms step_avg:59.82ms
step:1027/2420 train_time:61433ms step_avg:59.82ms
step:1028/2420 train_time:61494ms step_avg:59.82ms
step:1029/2420 train_time:61553ms step_avg:59.82ms
step:1030/2420 train_time:61614ms step_avg:59.82ms
step:1031/2420 train_time:61673ms step_avg:59.82ms
step:1032/2420 train_time:61734ms step_avg:59.82ms
step:1033/2420 train_time:61794ms step_avg:59.82ms
step:1034/2420 train_time:61855ms step_avg:59.82ms
step:1035/2420 train_time:61914ms step_avg:59.82ms
step:1036/2420 train_time:61976ms step_avg:59.82ms
step:1037/2420 train_time:62035ms step_avg:59.82ms
step:1038/2420 train_time:62096ms step_avg:59.82ms
step:1039/2420 train_time:62156ms step_avg:59.82ms
step:1040/2420 train_time:62217ms step_avg:59.82ms
step:1041/2420 train_time:62276ms step_avg:59.82ms
step:1042/2420 train_time:62337ms step_avg:59.82ms
step:1043/2420 train_time:62396ms step_avg:59.82ms
step:1044/2420 train_time:62457ms step_avg:59.82ms
step:1045/2420 train_time:62515ms step_avg:59.82ms
step:1046/2420 train_time:62576ms step_avg:59.82ms
step:1047/2420 train_time:62635ms step_avg:59.82ms
step:1048/2420 train_time:62696ms step_avg:59.82ms
step:1049/2420 train_time:62754ms step_avg:59.82ms
step:1050/2420 train_time:62815ms step_avg:59.82ms
step:1051/2420 train_time:62875ms step_avg:59.82ms
step:1052/2420 train_time:62936ms step_avg:59.82ms
step:1053/2420 train_time:62995ms step_avg:59.82ms
step:1054/2420 train_time:63056ms step_avg:59.83ms
step:1055/2420 train_time:63115ms step_avg:59.83ms
step:1056/2420 train_time:63177ms step_avg:59.83ms
step:1057/2420 train_time:63236ms step_avg:59.83ms
step:1058/2420 train_time:63296ms step_avg:59.83ms
step:1059/2420 train_time:63355ms step_avg:59.83ms
step:1060/2420 train_time:63417ms step_avg:59.83ms
step:1061/2420 train_time:63476ms step_avg:59.83ms
step:1062/2420 train_time:63536ms step_avg:59.83ms
step:1063/2420 train_time:63595ms step_avg:59.83ms
step:1064/2420 train_time:63656ms step_avg:59.83ms
step:1065/2420 train_time:63715ms step_avg:59.83ms
step:1066/2420 train_time:63776ms step_avg:59.83ms
step:1067/2420 train_time:63835ms step_avg:59.83ms
step:1068/2420 train_time:63897ms step_avg:59.83ms
step:1069/2420 train_time:63956ms step_avg:59.83ms
step:1070/2420 train_time:64016ms step_avg:59.83ms
step:1071/2420 train_time:64076ms step_avg:59.83ms
step:1072/2420 train_time:64137ms step_avg:59.83ms
step:1073/2420 train_time:64196ms step_avg:59.83ms
step:1074/2420 train_time:64257ms step_avg:59.83ms
step:1075/2420 train_time:64316ms step_avg:59.83ms
step:1076/2420 train_time:64377ms step_avg:59.83ms
step:1077/2420 train_time:64436ms step_avg:59.83ms
step:1078/2420 train_time:64497ms step_avg:59.83ms
step:1079/2420 train_time:64556ms step_avg:59.83ms
step:1080/2420 train_time:64616ms step_avg:59.83ms
step:1081/2420 train_time:64676ms step_avg:59.83ms
step:1082/2420 train_time:64736ms step_avg:59.83ms
step:1083/2420 train_time:64795ms step_avg:59.83ms
step:1084/2420 train_time:64855ms step_avg:59.83ms
step:1085/2420 train_time:64915ms step_avg:59.83ms
step:1086/2420 train_time:64976ms step_avg:59.83ms
step:1087/2420 train_time:65036ms step_avg:59.83ms
step:1088/2420 train_time:65096ms step_avg:59.83ms
step:1089/2420 train_time:65155ms step_avg:59.83ms
step:1090/2420 train_time:65216ms step_avg:59.83ms
step:1091/2420 train_time:65276ms step_avg:59.83ms
step:1092/2420 train_time:65336ms step_avg:59.83ms
step:1093/2420 train_time:65395ms step_avg:59.83ms
step:1094/2420 train_time:65456ms step_avg:59.83ms
step:1095/2420 train_time:65516ms step_avg:59.83ms
step:1096/2420 train_time:65576ms step_avg:59.83ms
step:1097/2420 train_time:65635ms step_avg:59.83ms
step:1098/2420 train_time:65696ms step_avg:59.83ms
step:1099/2420 train_time:65756ms step_avg:59.83ms
step:1100/2420 train_time:65816ms step_avg:59.83ms
step:1101/2420 train_time:65876ms step_avg:59.83ms
step:1102/2420 train_time:65937ms step_avg:59.83ms
step:1103/2420 train_time:65996ms step_avg:59.83ms
step:1104/2420 train_time:66057ms step_avg:59.83ms
step:1105/2420 train_time:66116ms step_avg:59.83ms
step:1106/2420 train_time:66177ms step_avg:59.83ms
step:1107/2420 train_time:66236ms step_avg:59.83ms
step:1108/2420 train_time:66297ms step_avg:59.83ms
step:1109/2420 train_time:66356ms step_avg:59.83ms
step:1110/2420 train_time:66417ms step_avg:59.83ms
step:1111/2420 train_time:66476ms step_avg:59.83ms
step:1112/2420 train_time:66537ms step_avg:59.84ms
step:1113/2420 train_time:66596ms step_avg:59.83ms
step:1114/2420 train_time:66657ms step_avg:59.84ms
step:1115/2420 train_time:66716ms step_avg:59.83ms
step:1116/2420 train_time:66777ms step_avg:59.84ms
step:1117/2420 train_time:66836ms step_avg:59.84ms
step:1118/2420 train_time:66897ms step_avg:59.84ms
step:1119/2420 train_time:66956ms step_avg:59.84ms
step:1120/2420 train_time:67018ms step_avg:59.84ms
step:1121/2420 train_time:67077ms step_avg:59.84ms
step:1122/2420 train_time:67138ms step_avg:59.84ms
step:1123/2420 train_time:67197ms step_avg:59.84ms
step:1124/2420 train_time:67258ms step_avg:59.84ms
step:1125/2420 train_time:67317ms step_avg:59.84ms
step:1126/2420 train_time:67378ms step_avg:59.84ms
step:1127/2420 train_time:67437ms step_avg:59.84ms
step:1128/2420 train_time:67498ms step_avg:59.84ms
step:1129/2420 train_time:67557ms step_avg:59.84ms
step:1130/2420 train_time:67618ms step_avg:59.84ms
step:1131/2420 train_time:67677ms step_avg:59.84ms
step:1132/2420 train_time:67738ms step_avg:59.84ms
step:1133/2420 train_time:67797ms step_avg:59.84ms
step:1134/2420 train_time:67858ms step_avg:59.84ms
step:1135/2420 train_time:67918ms step_avg:59.84ms
step:1136/2420 train_time:67979ms step_avg:59.84ms
step:1137/2420 train_time:68038ms step_avg:59.84ms
step:1138/2420 train_time:68099ms step_avg:59.84ms
step:1139/2420 train_time:68158ms step_avg:59.84ms
step:1140/2420 train_time:68219ms step_avg:59.84ms
step:1141/2420 train_time:68278ms step_avg:59.84ms
step:1142/2420 train_time:68339ms step_avg:59.84ms
step:1143/2420 train_time:68398ms step_avg:59.84ms
step:1144/2420 train_time:68459ms step_avg:59.84ms
step:1145/2420 train_time:68519ms step_avg:59.84ms
step:1146/2420 train_time:68579ms step_avg:59.84ms
step:1147/2420 train_time:68639ms step_avg:59.84ms
step:1148/2420 train_time:68700ms step_avg:59.84ms
step:1149/2420 train_time:68759ms step_avg:59.84ms
step:1150/2420 train_time:68820ms step_avg:59.84ms
step:1151/2420 train_time:68879ms step_avg:59.84ms
step:1152/2420 train_time:68940ms step_avg:59.84ms
step:1153/2420 train_time:68999ms step_avg:59.84ms
step:1154/2420 train_time:69060ms step_avg:59.84ms
step:1155/2420 train_time:69120ms step_avg:59.84ms
step:1156/2420 train_time:69181ms step_avg:59.85ms
step:1157/2420 train_time:69240ms step_avg:59.84ms
step:1158/2420 train_time:69301ms step_avg:59.85ms
step:1159/2420 train_time:69360ms step_avg:59.84ms
step:1160/2420 train_time:69422ms step_avg:59.85ms
step:1161/2420 train_time:69481ms step_avg:59.85ms
step:1162/2420 train_time:69542ms step_avg:59.85ms
step:1163/2420 train_time:69602ms step_avg:59.85ms
step:1164/2420 train_time:69663ms step_avg:59.85ms
step:1165/2420 train_time:69722ms step_avg:59.85ms
step:1166/2420 train_time:69784ms step_avg:59.85ms
step:1167/2420 train_time:69844ms step_avg:59.85ms
step:1168/2420 train_time:69905ms step_avg:59.85ms
step:1169/2420 train_time:69964ms step_avg:59.85ms
step:1170/2420 train_time:70025ms step_avg:59.85ms
step:1171/2420 train_time:70084ms step_avg:59.85ms
step:1172/2420 train_time:70146ms step_avg:59.85ms
step:1173/2420 train_time:70206ms step_avg:59.85ms
step:1174/2420 train_time:70267ms step_avg:59.85ms
step:1175/2420 train_time:70326ms step_avg:59.85ms
step:1176/2420 train_time:70388ms step_avg:59.85ms
step:1177/2420 train_time:70447ms step_avg:59.85ms
step:1178/2420 train_time:70509ms step_avg:59.85ms
step:1179/2420 train_time:70569ms step_avg:59.85ms
step:1180/2420 train_time:70631ms step_avg:59.86ms
step:1181/2420 train_time:70690ms step_avg:59.86ms
step:1182/2420 train_time:70751ms step_avg:59.86ms
step:1183/2420 train_time:70811ms step_avg:59.86ms
step:1184/2420 train_time:70872ms step_avg:59.86ms
step:1185/2420 train_time:70932ms step_avg:59.86ms
step:1186/2420 train_time:70994ms step_avg:59.86ms
step:1187/2420 train_time:71053ms step_avg:59.86ms
step:1188/2420 train_time:71114ms step_avg:59.86ms
step:1189/2420 train_time:71174ms step_avg:59.86ms
step:1190/2420 train_time:71234ms step_avg:59.86ms
step:1191/2420 train_time:71294ms step_avg:59.86ms
step:1192/2420 train_time:71355ms step_avg:59.86ms
step:1193/2420 train_time:71415ms step_avg:59.86ms
step:1194/2420 train_time:71476ms step_avg:59.86ms
step:1195/2420 train_time:71535ms step_avg:59.86ms
step:1196/2420 train_time:71596ms step_avg:59.86ms
step:1197/2420 train_time:71654ms step_avg:59.86ms
step:1198/2420 train_time:71715ms step_avg:59.86ms
step:1199/2420 train_time:71775ms step_avg:59.86ms
step:1200/2420 train_time:71836ms step_avg:59.86ms
step:1201/2420 train_time:71895ms step_avg:59.86ms
step:1202/2420 train_time:71957ms step_avg:59.86ms
step:1203/2420 train_time:72016ms step_avg:59.86ms
step:1204/2420 train_time:72078ms step_avg:59.87ms
step:1205/2420 train_time:72137ms step_avg:59.86ms
step:1206/2420 train_time:72198ms step_avg:59.87ms
step:1207/2420 train_time:72257ms step_avg:59.86ms
step:1208/2420 train_time:72317ms step_avg:59.87ms
step:1209/2420 train_time:72377ms step_avg:59.86ms
step:1210/2420 train_time:72437ms step_avg:59.87ms
step:1211/2420 train_time:72496ms step_avg:59.86ms
step:1212/2420 train_time:72557ms step_avg:59.87ms
step:1213/2420 train_time:72617ms step_avg:59.87ms
step:1214/2420 train_time:72677ms step_avg:59.87ms
step:1215/2420 train_time:72736ms step_avg:59.87ms
step:1216/2420 train_time:72797ms step_avg:59.87ms
step:1217/2420 train_time:72856ms step_avg:59.87ms
step:1218/2420 train_time:72917ms step_avg:59.87ms
step:1219/2420 train_time:72977ms step_avg:59.87ms
step:1220/2420 train_time:73037ms step_avg:59.87ms
step:1221/2420 train_time:73096ms step_avg:59.87ms
step:1222/2420 train_time:73157ms step_avg:59.87ms
step:1223/2420 train_time:73217ms step_avg:59.87ms
step:1224/2420 train_time:73278ms step_avg:59.87ms
step:1225/2420 train_time:73337ms step_avg:59.87ms
step:1226/2420 train_time:73397ms step_avg:59.87ms
step:1227/2420 train_time:73456ms step_avg:59.87ms
step:1228/2420 train_time:73517ms step_avg:59.87ms
step:1229/2420 train_time:73576ms step_avg:59.87ms
step:1230/2420 train_time:73636ms step_avg:59.87ms
step:1231/2420 train_time:73696ms step_avg:59.87ms
step:1232/2420 train_time:73757ms step_avg:59.87ms
step:1233/2420 train_time:73815ms step_avg:59.87ms
step:1234/2420 train_time:73877ms step_avg:59.87ms
step:1235/2420 train_time:73936ms step_avg:59.87ms
step:1236/2420 train_time:73997ms step_avg:59.87ms
step:1237/2420 train_time:74055ms step_avg:59.87ms
step:1238/2420 train_time:74116ms step_avg:59.87ms
step:1239/2420 train_time:74176ms step_avg:59.87ms
step:1240/2420 train_time:74236ms step_avg:59.87ms
step:1241/2420 train_time:74296ms step_avg:59.87ms
step:1242/2420 train_time:74357ms step_avg:59.87ms
step:1243/2420 train_time:74416ms step_avg:59.87ms
step:1244/2420 train_time:74477ms step_avg:59.87ms
step:1245/2420 train_time:74536ms step_avg:59.87ms
step:1246/2420 train_time:74597ms step_avg:59.87ms
step:1247/2420 train_time:74656ms step_avg:59.87ms
step:1248/2420 train_time:74717ms step_avg:59.87ms
step:1249/2420 train_time:74777ms step_avg:59.87ms
step:1250/2420 train_time:74837ms step_avg:59.87ms
step:1250/2420 val_loss:3.5219 train_time:74900ms step_avg:59.92ms
step:1251/2420 train_time:74921ms step_avg:59.89ms
step:1252/2420 train_time:74961ms step_avg:59.87ms
step:1253/2420 train_time:75023ms step_avg:59.87ms
step:1254/2420 train_time:75088ms step_avg:59.88ms
step:1255/2420 train_time:75148ms step_avg:59.88ms
step:1256/2420 train_time:75210ms step_avg:59.88ms
step:1257/2420 train_time:75269ms step_avg:59.88ms
step:1258/2420 train_time:75330ms step_avg:59.88ms
step:1259/2420 train_time:75390ms step_avg:59.88ms
step:1260/2420 train_time:75451ms step_avg:59.88ms
step:1261/2420 train_time:75509ms step_avg:59.88ms
step:1262/2420 train_time:75570ms step_avg:59.88ms
step:1263/2420 train_time:75629ms step_avg:59.88ms
step:1264/2420 train_time:75689ms step_avg:59.88ms
step:1265/2420 train_time:75748ms step_avg:59.88ms
step:1266/2420 train_time:75808ms step_avg:59.88ms
step:1267/2420 train_time:75868ms step_avg:59.88ms
step:1268/2420 train_time:75931ms step_avg:59.88ms
step:1269/2420 train_time:75993ms step_avg:59.88ms
step:1270/2420 train_time:76055ms step_avg:59.89ms
step:1271/2420 train_time:76115ms step_avg:59.89ms
step:1272/2420 train_time:76177ms step_avg:59.89ms
step:1273/2420 train_time:76236ms step_avg:59.89ms
step:1274/2420 train_time:76298ms step_avg:59.89ms
step:1275/2420 train_time:76357ms step_avg:59.89ms
step:1276/2420 train_time:76417ms step_avg:59.89ms
step:1277/2420 train_time:76476ms step_avg:59.89ms
step:1278/2420 train_time:76537ms step_avg:59.89ms
step:1279/2420 train_time:76596ms step_avg:59.89ms
step:1280/2420 train_time:76657ms step_avg:59.89ms
step:1281/2420 train_time:76715ms step_avg:59.89ms
step:1282/2420 train_time:76776ms step_avg:59.89ms
step:1283/2420 train_time:76835ms step_avg:59.89ms
step:1284/2420 train_time:76896ms step_avg:59.89ms
step:1285/2420 train_time:76957ms step_avg:59.89ms
step:1286/2420 train_time:77019ms step_avg:59.89ms
step:1287/2420 train_time:77078ms step_avg:59.89ms
step:1288/2420 train_time:77139ms step_avg:59.89ms
step:1289/2420 train_time:77198ms step_avg:59.89ms
step:1290/2420 train_time:77259ms step_avg:59.89ms
step:1291/2420 train_time:77318ms step_avg:59.89ms
step:1292/2420 train_time:77379ms step_avg:59.89ms
step:1293/2420 train_time:77438ms step_avg:59.89ms
step:1294/2420 train_time:77499ms step_avg:59.89ms
step:1295/2420 train_time:77558ms step_avg:59.89ms
step:1296/2420 train_time:77619ms step_avg:59.89ms
step:1297/2420 train_time:77678ms step_avg:59.89ms
step:1298/2420 train_time:77739ms step_avg:59.89ms
step:1299/2420 train_time:77797ms step_avg:59.89ms
step:1300/2420 train_time:77858ms step_avg:59.89ms
step:1301/2420 train_time:77917ms step_avg:59.89ms
step:1302/2420 train_time:77978ms step_avg:59.89ms
step:1303/2420 train_time:78037ms step_avg:59.89ms
step:1304/2420 train_time:78099ms step_avg:59.89ms
step:1305/2420 train_time:78158ms step_avg:59.89ms
step:1306/2420 train_time:78219ms step_avg:59.89ms
step:1307/2420 train_time:78278ms step_avg:59.89ms
step:1308/2420 train_time:78340ms step_avg:59.89ms
step:1309/2420 train_time:78399ms step_avg:59.89ms
step:1310/2420 train_time:78460ms step_avg:59.89ms
step:1311/2420 train_time:78519ms step_avg:59.89ms
step:1312/2420 train_time:78580ms step_avg:59.89ms
step:1313/2420 train_time:78639ms step_avg:59.89ms
step:1314/2420 train_time:78700ms step_avg:59.89ms
step:1315/2420 train_time:78759ms step_avg:59.89ms
step:1316/2420 train_time:78821ms step_avg:59.89ms
step:1317/2420 train_time:78880ms step_avg:59.89ms
step:1318/2420 train_time:78941ms step_avg:59.89ms
step:1319/2420 train_time:79000ms step_avg:59.89ms
step:1320/2420 train_time:79062ms step_avg:59.90ms
step:1321/2420 train_time:79121ms step_avg:59.90ms
step:1322/2420 train_time:79182ms step_avg:59.90ms
step:1323/2420 train_time:79241ms step_avg:59.90ms
step:1324/2420 train_time:79303ms step_avg:59.90ms
step:1325/2420 train_time:79363ms step_avg:59.90ms
step:1326/2420 train_time:79424ms step_avg:59.90ms
step:1327/2420 train_time:79483ms step_avg:59.90ms
step:1328/2420 train_time:79545ms step_avg:59.90ms
step:1329/2420 train_time:79604ms step_avg:59.90ms
step:1330/2420 train_time:79665ms step_avg:59.90ms
step:1331/2420 train_time:79724ms step_avg:59.90ms
step:1332/2420 train_time:79786ms step_avg:59.90ms
step:1333/2420 train_time:79846ms step_avg:59.90ms
step:1334/2420 train_time:79907ms step_avg:59.90ms
step:1335/2420 train_time:79966ms step_avg:59.90ms
step:1336/2420 train_time:80027ms step_avg:59.90ms
step:1337/2420 train_time:80087ms step_avg:59.90ms
step:1338/2420 train_time:80148ms step_avg:59.90ms
step:1339/2420 train_time:80208ms step_avg:59.90ms
step:1340/2420 train_time:80269ms step_avg:59.90ms
step:1341/2420 train_time:80329ms step_avg:59.90ms
step:1342/2420 train_time:80390ms step_avg:59.90ms
step:1343/2420 train_time:80450ms step_avg:59.90ms
step:1344/2420 train_time:80512ms step_avg:59.90ms
step:1345/2420 train_time:80571ms step_avg:59.90ms
step:1346/2420 train_time:80633ms step_avg:59.91ms
step:1347/2420 train_time:80692ms step_avg:59.91ms
step:1348/2420 train_time:80753ms step_avg:59.91ms
step:1349/2420 train_time:80813ms step_avg:59.91ms
step:1350/2420 train_time:80875ms step_avg:59.91ms
step:1351/2420 train_time:80934ms step_avg:59.91ms
step:1352/2420 train_time:80994ms step_avg:59.91ms
step:1353/2420 train_time:81053ms step_avg:59.91ms
step:1354/2420 train_time:81115ms step_avg:59.91ms
step:1355/2420 train_time:81174ms step_avg:59.91ms
step:1356/2420 train_time:81235ms step_avg:59.91ms
step:1357/2420 train_time:81295ms step_avg:59.91ms
step:1358/2420 train_time:81356ms step_avg:59.91ms
step:1359/2420 train_time:81415ms step_avg:59.91ms
step:1360/2420 train_time:81476ms step_avg:59.91ms
step:1361/2420 train_time:81536ms step_avg:59.91ms
step:1362/2420 train_time:81597ms step_avg:59.91ms
step:1363/2420 train_time:81656ms step_avg:59.91ms
step:1364/2420 train_time:81717ms step_avg:59.91ms
step:1365/2420 train_time:81776ms step_avg:59.91ms
step:1366/2420 train_time:81837ms step_avg:59.91ms
step:1367/2420 train_time:81896ms step_avg:59.91ms
step:1368/2420 train_time:81957ms step_avg:59.91ms
step:1369/2420 train_time:82016ms step_avg:59.91ms
step:1370/2420 train_time:82077ms step_avg:59.91ms
step:1371/2420 train_time:82136ms step_avg:59.91ms
step:1372/2420 train_time:82197ms step_avg:59.91ms
step:1373/2420 train_time:82257ms step_avg:59.91ms
step:1374/2420 train_time:82318ms step_avg:59.91ms
step:1375/2420 train_time:82376ms step_avg:59.91ms
step:1376/2420 train_time:82438ms step_avg:59.91ms
step:1377/2420 train_time:82497ms step_avg:59.91ms
step:1378/2420 train_time:82558ms step_avg:59.91ms
step:1379/2420 train_time:82617ms step_avg:59.91ms
step:1380/2420 train_time:82678ms step_avg:59.91ms
step:1381/2420 train_time:82736ms step_avg:59.91ms
step:1382/2420 train_time:82797ms step_avg:59.91ms
step:1383/2420 train_time:82856ms step_avg:59.91ms
step:1384/2420 train_time:82917ms step_avg:59.91ms
step:1385/2420 train_time:82976ms step_avg:59.91ms
step:1386/2420 train_time:83037ms step_avg:59.91ms
step:1387/2420 train_time:83096ms step_avg:59.91ms
step:1388/2420 train_time:83157ms step_avg:59.91ms
step:1389/2420 train_time:83217ms step_avg:59.91ms
step:1390/2420 train_time:83278ms step_avg:59.91ms
step:1391/2420 train_time:83337ms step_avg:59.91ms
step:1392/2420 train_time:83397ms step_avg:59.91ms
step:1393/2420 train_time:83456ms step_avg:59.91ms
step:1394/2420 train_time:83517ms step_avg:59.91ms
step:1395/2420 train_time:83576ms step_avg:59.91ms
step:1396/2420 train_time:83637ms step_avg:59.91ms
step:1397/2420 train_time:83696ms step_avg:59.91ms
step:1398/2420 train_time:83756ms step_avg:59.91ms
step:1399/2420 train_time:83816ms step_avg:59.91ms
step:1400/2420 train_time:83876ms step_avg:59.91ms
step:1401/2420 train_time:83935ms step_avg:59.91ms
step:1402/2420 train_time:83996ms step_avg:59.91ms
step:1403/2420 train_time:84055ms step_avg:59.91ms
step:1404/2420 train_time:84116ms step_avg:59.91ms
step:1405/2420 train_time:84175ms step_avg:59.91ms
step:1406/2420 train_time:84236ms step_avg:59.91ms
step:1407/2420 train_time:84295ms step_avg:59.91ms
step:1408/2420 train_time:84357ms step_avg:59.91ms
step:1409/2420 train_time:84416ms step_avg:59.91ms
step:1410/2420 train_time:84476ms step_avg:59.91ms
step:1411/2420 train_time:84536ms step_avg:59.91ms
step:1412/2420 train_time:84597ms step_avg:59.91ms
step:1413/2420 train_time:84656ms step_avg:59.91ms
step:1414/2420 train_time:84717ms step_avg:59.91ms
step:1415/2420 train_time:84776ms step_avg:59.91ms
step:1416/2420 train_time:84837ms step_avg:59.91ms
step:1417/2420 train_time:84896ms step_avg:59.91ms
step:1418/2420 train_time:84957ms step_avg:59.91ms
step:1419/2420 train_time:85017ms step_avg:59.91ms
step:1420/2420 train_time:85078ms step_avg:59.91ms
step:1421/2420 train_time:85137ms step_avg:59.91ms
step:1422/2420 train_time:85197ms step_avg:59.91ms
step:1423/2420 train_time:85256ms step_avg:59.91ms
step:1424/2420 train_time:85318ms step_avg:59.91ms
step:1425/2420 train_time:85377ms step_avg:59.91ms
step:1426/2420 train_time:85438ms step_avg:59.91ms
step:1427/2420 train_time:85497ms step_avg:59.91ms
step:1428/2420 train_time:85558ms step_avg:59.91ms
step:1429/2420 train_time:85617ms step_avg:59.91ms
step:1430/2420 train_time:85678ms step_avg:59.91ms
step:1431/2420 train_time:85736ms step_avg:59.91ms
step:1432/2420 train_time:85797ms step_avg:59.91ms
step:1433/2420 train_time:85856ms step_avg:59.91ms
step:1434/2420 train_time:85917ms step_avg:59.91ms
step:1435/2420 train_time:85976ms step_avg:59.91ms
step:1436/2420 train_time:86037ms step_avg:59.91ms
step:1437/2420 train_time:86096ms step_avg:59.91ms
step:1438/2420 train_time:86157ms step_avg:59.91ms
step:1439/2420 train_time:86216ms step_avg:59.91ms
step:1440/2420 train_time:86277ms step_avg:59.91ms
step:1441/2420 train_time:86336ms step_avg:59.91ms
step:1442/2420 train_time:86397ms step_avg:59.91ms
step:1443/2420 train_time:86456ms step_avg:59.91ms
step:1444/2420 train_time:86517ms step_avg:59.91ms
step:1445/2420 train_time:86575ms step_avg:59.91ms
step:1446/2420 train_time:86636ms step_avg:59.91ms
step:1447/2420 train_time:86695ms step_avg:59.91ms
step:1448/2420 train_time:86757ms step_avg:59.91ms
step:1449/2420 train_time:86817ms step_avg:59.91ms
step:1450/2420 train_time:86877ms step_avg:59.92ms
step:1451/2420 train_time:86936ms step_avg:59.91ms
step:1452/2420 train_time:86996ms step_avg:59.91ms
step:1453/2420 train_time:87055ms step_avg:59.91ms
step:1454/2420 train_time:87116ms step_avg:59.92ms
step:1455/2420 train_time:87176ms step_avg:59.91ms
step:1456/2420 train_time:87237ms step_avg:59.92ms
step:1457/2420 train_time:87296ms step_avg:59.91ms
step:1458/2420 train_time:87356ms step_avg:59.92ms
step:1459/2420 train_time:87416ms step_avg:59.91ms
step:1460/2420 train_time:87476ms step_avg:59.92ms
step:1461/2420 train_time:87536ms step_avg:59.92ms
step:1462/2420 train_time:87597ms step_avg:59.92ms
step:1463/2420 train_time:87656ms step_avg:59.92ms
step:1464/2420 train_time:87718ms step_avg:59.92ms
step:1465/2420 train_time:87777ms step_avg:59.92ms
step:1466/2420 train_time:87838ms step_avg:59.92ms
step:1467/2420 train_time:87896ms step_avg:59.92ms
step:1468/2420 train_time:87957ms step_avg:59.92ms
step:1469/2420 train_time:88016ms step_avg:59.92ms
step:1470/2420 train_time:88077ms step_avg:59.92ms
step:1471/2420 train_time:88136ms step_avg:59.92ms
step:1472/2420 train_time:88197ms step_avg:59.92ms
step:1473/2420 train_time:88256ms step_avg:59.92ms
step:1474/2420 train_time:88317ms step_avg:59.92ms
step:1475/2420 train_time:88376ms step_avg:59.92ms
step:1476/2420 train_time:88437ms step_avg:59.92ms
step:1477/2420 train_time:88496ms step_avg:59.92ms
step:1478/2420 train_time:88557ms step_avg:59.92ms
step:1479/2420 train_time:88616ms step_avg:59.92ms
step:1480/2420 train_time:88677ms step_avg:59.92ms
step:1481/2420 train_time:88736ms step_avg:59.92ms
step:1482/2420 train_time:88797ms step_avg:59.92ms
step:1483/2420 train_time:88856ms step_avg:59.92ms
step:1484/2420 train_time:88917ms step_avg:59.92ms
step:1485/2420 train_time:88976ms step_avg:59.92ms
step:1486/2420 train_time:89037ms step_avg:59.92ms
step:1487/2420 train_time:89096ms step_avg:59.92ms
step:1488/2420 train_time:89157ms step_avg:59.92ms
step:1489/2420 train_time:89216ms step_avg:59.92ms
step:1490/2420 train_time:89277ms step_avg:59.92ms
step:1491/2420 train_time:89336ms step_avg:59.92ms
step:1492/2420 train_time:89396ms step_avg:59.92ms
step:1493/2420 train_time:89456ms step_avg:59.92ms
step:1494/2420 train_time:89517ms step_avg:59.92ms
step:1495/2420 train_time:89576ms step_avg:59.92ms
step:1496/2420 train_time:89638ms step_avg:59.92ms
step:1497/2420 train_time:89697ms step_avg:59.92ms
step:1498/2420 train_time:89758ms step_avg:59.92ms
step:1499/2420 train_time:89817ms step_avg:59.92ms
step:1500/2420 train_time:89878ms step_avg:59.92ms
step:1500/2420 val_loss:3.4739 train_time:89941ms step_avg:59.96ms
step:1501/2420 train_time:89962ms step_avg:59.93ms
step:1502/2420 train_time:90003ms step_avg:59.92ms
step:1503/2420 train_time:90066ms step_avg:59.92ms
step:1504/2420 train_time:90131ms step_avg:59.93ms
step:1505/2420 train_time:90191ms step_avg:59.93ms
step:1506/2420 train_time:90253ms step_avg:59.93ms
step:1507/2420 train_time:90312ms step_avg:59.93ms
step:1508/2420 train_time:90372ms step_avg:59.93ms
step:1509/2420 train_time:90431ms step_avg:59.93ms
step:1510/2420 train_time:90491ms step_avg:59.93ms
step:1511/2420 train_time:90551ms step_avg:59.93ms
step:1512/2420 train_time:90612ms step_avg:59.93ms
step:1513/2420 train_time:90670ms step_avg:59.93ms
step:1514/2420 train_time:90731ms step_avg:59.93ms
step:1515/2420 train_time:90790ms step_avg:59.93ms
step:1516/2420 train_time:90853ms step_avg:59.93ms
step:1517/2420 train_time:90915ms step_avg:59.93ms
step:1518/2420 train_time:90977ms step_avg:59.93ms
step:1519/2420 train_time:91038ms step_avg:59.93ms
step:1520/2420 train_time:91100ms step_avg:59.93ms
step:1521/2420 train_time:91161ms step_avg:59.93ms
step:1522/2420 train_time:91221ms step_avg:59.94ms
step:1523/2420 train_time:91280ms step_avg:59.93ms
step:1524/2420 train_time:91342ms step_avg:59.94ms
step:1525/2420 train_time:91400ms step_avg:59.93ms
step:1526/2420 train_time:91461ms step_avg:59.94ms
step:1527/2420 train_time:91521ms step_avg:59.94ms
step:1528/2420 train_time:91581ms step_avg:59.94ms
step:1529/2420 train_time:91640ms step_avg:59.93ms
step:1530/2420 train_time:91701ms step_avg:59.94ms
step:1531/2420 train_time:91760ms step_avg:59.93ms
step:1532/2420 train_time:91821ms step_avg:59.94ms
step:1533/2420 train_time:91881ms step_avg:59.94ms
step:1534/2420 train_time:91942ms step_avg:59.94ms
step:1535/2420 train_time:92002ms step_avg:59.94ms
step:1536/2420 train_time:92065ms step_avg:59.94ms
step:1537/2420 train_time:92124ms step_avg:59.94ms
step:1538/2420 train_time:92186ms step_avg:59.94ms
step:1539/2420 train_time:92245ms step_avg:59.94ms
step:1540/2420 train_time:92307ms step_avg:59.94ms
step:1541/2420 train_time:92366ms step_avg:59.94ms
step:1542/2420 train_time:92427ms step_avg:59.94ms
step:1543/2420 train_time:92486ms step_avg:59.94ms
step:1544/2420 train_time:92547ms step_avg:59.94ms
step:1545/2420 train_time:92606ms step_avg:59.94ms
step:1546/2420 train_time:92667ms step_avg:59.94ms
step:1547/2420 train_time:92726ms step_avg:59.94ms
step:1548/2420 train_time:92788ms step_avg:59.94ms
step:1549/2420 train_time:92847ms step_avg:59.94ms
step:1550/2420 train_time:92909ms step_avg:59.94ms
step:1551/2420 train_time:92969ms step_avg:59.94ms
step:1552/2420 train_time:93030ms step_avg:59.94ms
step:1553/2420 train_time:93090ms step_avg:59.94ms
step:1554/2420 train_time:93152ms step_avg:59.94ms
step:1555/2420 train_time:93212ms step_avg:59.94ms
step:1556/2420 train_time:93274ms step_avg:59.94ms
step:1557/2420 train_time:93333ms step_avg:59.94ms
step:1558/2420 train_time:93395ms step_avg:59.95ms
step:1559/2420 train_time:93454ms step_avg:59.94ms
step:1560/2420 train_time:93515ms step_avg:59.95ms
step:1561/2420 train_time:93574ms step_avg:59.95ms
step:1562/2420 train_time:93635ms step_avg:59.95ms
step:1563/2420 train_time:93694ms step_avg:59.95ms
step:1564/2420 train_time:93756ms step_avg:59.95ms
step:1565/2420 train_time:93815ms step_avg:59.95ms
step:1566/2420 train_time:93876ms step_avg:59.95ms
step:1567/2420 train_time:93936ms step_avg:59.95ms
step:1568/2420 train_time:93997ms step_avg:59.95ms
step:1569/2420 train_time:94057ms step_avg:59.95ms
step:1570/2420 train_time:94118ms step_avg:59.95ms
step:1571/2420 train_time:94177ms step_avg:59.95ms
step:1572/2420 train_time:94238ms step_avg:59.95ms
step:1573/2420 train_time:94297ms step_avg:59.95ms
step:1574/2420 train_time:94358ms step_avg:59.95ms
step:1575/2420 train_time:94417ms step_avg:59.95ms
step:1576/2420 train_time:94479ms step_avg:59.95ms
step:1577/2420 train_time:94538ms step_avg:59.95ms
step:1578/2420 train_time:94599ms step_avg:59.95ms
step:1579/2420 train_time:94658ms step_avg:59.95ms
step:1580/2420 train_time:94720ms step_avg:59.95ms
step:1581/2420 train_time:94779ms step_avg:59.95ms
step:1582/2420 train_time:94840ms step_avg:59.95ms
step:1583/2420 train_time:94899ms step_avg:59.95ms
step:1584/2420 train_time:94961ms step_avg:59.95ms
step:1585/2420 train_time:95020ms step_avg:59.95ms
step:1586/2420 train_time:95081ms step_avg:59.95ms
step:1587/2420 train_time:95140ms step_avg:59.95ms
step:1588/2420 train_time:95202ms step_avg:59.95ms
step:1589/2420 train_time:95261ms step_avg:59.95ms
step:1590/2420 train_time:95323ms step_avg:59.95ms
step:1591/2420 train_time:95382ms step_avg:59.95ms
step:1592/2420 train_time:95443ms step_avg:59.95ms
step:1593/2420 train_time:95503ms step_avg:59.95ms
step:1594/2420 train_time:95565ms step_avg:59.95ms
step:1595/2420 train_time:95626ms step_avg:59.95ms
step:1596/2420 train_time:95688ms step_avg:59.95ms
step:1597/2420 train_time:95747ms step_avg:59.95ms
step:1598/2420 train_time:95809ms step_avg:59.96ms
step:1599/2420 train_time:95869ms step_avg:59.96ms
step:1600/2420 train_time:95931ms step_avg:59.96ms
step:1601/2420 train_time:95991ms step_avg:59.96ms
step:1602/2420 train_time:96054ms step_avg:59.96ms
step:1603/2420 train_time:96114ms step_avg:59.96ms
step:1604/2420 train_time:96176ms step_avg:59.96ms
step:1605/2420 train_time:96235ms step_avg:59.96ms
step:1606/2420 train_time:96297ms step_avg:59.96ms
step:1607/2420 train_time:96357ms step_avg:59.96ms
step:1608/2420 train_time:96418ms step_avg:59.96ms
step:1609/2420 train_time:96477ms step_avg:59.96ms
step:1610/2420 train_time:96538ms step_avg:59.96ms
step:1611/2420 train_time:96598ms step_avg:59.96ms
step:1612/2420 train_time:96659ms step_avg:59.96ms
step:1613/2420 train_time:96718ms step_avg:59.96ms
step:1614/2420 train_time:96780ms step_avg:59.96ms
step:1615/2420 train_time:96840ms step_avg:59.96ms
step:1616/2420 train_time:96902ms step_avg:59.96ms
step:1617/2420 train_time:96962ms step_avg:59.96ms
step:1618/2420 train_time:97024ms step_avg:59.97ms
step:1619/2420 train_time:97084ms step_avg:59.97ms
step:1620/2420 train_time:97146ms step_avg:59.97ms
step:1621/2420 train_time:97206ms step_avg:59.97ms
step:1622/2420 train_time:97268ms step_avg:59.97ms
step:1623/2420 train_time:97328ms step_avg:59.97ms
step:1624/2420 train_time:97390ms step_avg:59.97ms
step:1625/2420 train_time:97450ms step_avg:59.97ms
step:1626/2420 train_time:97512ms step_avg:59.97ms
step:1627/2420 train_time:97573ms step_avg:59.97ms
step:1628/2420 train_time:97635ms step_avg:59.97ms
step:1629/2420 train_time:97694ms step_avg:59.97ms
step:1630/2420 train_time:97756ms step_avg:59.97ms
step:1631/2420 train_time:97816ms step_avg:59.97ms
step:1632/2420 train_time:97877ms step_avg:59.97ms
step:1633/2420 train_time:97936ms step_avg:59.97ms
step:1634/2420 train_time:97998ms step_avg:59.97ms
step:1635/2420 train_time:98057ms step_avg:59.97ms
step:1636/2420 train_time:98119ms step_avg:59.97ms
step:1637/2420 train_time:98178ms step_avg:59.97ms
step:1638/2420 train_time:98239ms step_avg:59.98ms
step:1639/2420 train_time:98299ms step_avg:59.97ms
step:1640/2420 train_time:98360ms step_avg:59.98ms
step:1641/2420 train_time:98420ms step_avg:59.98ms
step:1642/2420 train_time:98482ms step_avg:59.98ms
step:1643/2420 train_time:98542ms step_avg:59.98ms
step:1644/2420 train_time:98605ms step_avg:59.98ms
step:1645/2420 train_time:98665ms step_avg:59.98ms
step:1646/2420 train_time:98726ms step_avg:59.98ms
step:1647/2420 train_time:98787ms step_avg:59.98ms
step:1648/2420 train_time:98849ms step_avg:59.98ms
step:1649/2420 train_time:98908ms step_avg:59.98ms
step:1650/2420 train_time:98970ms step_avg:59.98ms
step:1651/2420 train_time:99031ms step_avg:59.98ms
step:1652/2420 train_time:99093ms step_avg:59.98ms
step:1653/2420 train_time:99154ms step_avg:59.98ms
step:1654/2420 train_time:99215ms step_avg:59.99ms
step:1655/2420 train_time:99275ms step_avg:59.98ms
step:1656/2420 train_time:99336ms step_avg:59.99ms
step:1657/2420 train_time:99397ms step_avg:59.99ms
step:1658/2420 train_time:99458ms step_avg:59.99ms
step:1659/2420 train_time:99517ms step_avg:59.99ms
step:1660/2420 train_time:99579ms step_avg:59.99ms
step:1661/2420 train_time:99638ms step_avg:59.99ms
step:1662/2420 train_time:99699ms step_avg:59.99ms
step:1663/2420 train_time:99759ms step_avg:59.99ms
step:1664/2420 train_time:99820ms step_avg:59.99ms
step:1665/2420 train_time:99880ms step_avg:59.99ms
step:1666/2420 train_time:99942ms step_avg:59.99ms
step:1667/2420 train_time:100002ms step_avg:59.99ms
step:1668/2420 train_time:100064ms step_avg:59.99ms
step:1669/2420 train_time:100124ms step_avg:59.99ms
step:1670/2420 train_time:100186ms step_avg:59.99ms
step:1671/2420 train_time:100246ms step_avg:59.99ms
step:1672/2420 train_time:100308ms step_avg:59.99ms
step:1673/2420 train_time:100368ms step_avg:59.99ms
step:1674/2420 train_time:100430ms step_avg:59.99ms
step:1675/2420 train_time:100491ms step_avg:59.99ms
step:1676/2420 train_time:100553ms step_avg:60.00ms
step:1677/2420 train_time:100613ms step_avg:60.00ms
step:1678/2420 train_time:100675ms step_avg:60.00ms
step:1679/2420 train_time:100734ms step_avg:60.00ms
step:1680/2420 train_time:100796ms step_avg:60.00ms
step:1681/2420 train_time:100856ms step_avg:60.00ms
step:1682/2420 train_time:100917ms step_avg:60.00ms
step:1683/2420 train_time:100977ms step_avg:60.00ms
step:1684/2420 train_time:101038ms step_avg:60.00ms
step:1685/2420 train_time:101097ms step_avg:60.00ms
step:1686/2420 train_time:101159ms step_avg:60.00ms
step:1687/2420 train_time:101219ms step_avg:60.00ms
step:1688/2420 train_time:101280ms step_avg:60.00ms
step:1689/2420 train_time:101340ms step_avg:60.00ms
step:1690/2420 train_time:101402ms step_avg:60.00ms
step:1691/2420 train_time:101462ms step_avg:60.00ms
step:1692/2420 train_time:101524ms step_avg:60.00ms
step:1693/2420 train_time:101583ms step_avg:60.00ms
step:1694/2420 train_time:101645ms step_avg:60.00ms
step:1695/2420 train_time:101705ms step_avg:60.00ms
step:1696/2420 train_time:101767ms step_avg:60.00ms
step:1697/2420 train_time:101827ms step_avg:60.00ms
step:1698/2420 train_time:101889ms step_avg:60.01ms
step:1699/2420 train_time:101949ms step_avg:60.01ms
step:1700/2420 train_time:102012ms step_avg:60.01ms
step:1701/2420 train_time:102071ms step_avg:60.01ms
step:1702/2420 train_time:102133ms step_avg:60.01ms
step:1703/2420 train_time:102193ms step_avg:60.01ms
step:1704/2420 train_time:102255ms step_avg:60.01ms
step:1705/2420 train_time:102315ms step_avg:60.01ms
step:1706/2420 train_time:102376ms step_avg:60.01ms
step:1707/2420 train_time:102436ms step_avg:60.01ms
step:1708/2420 train_time:102497ms step_avg:60.01ms
step:1709/2420 train_time:102557ms step_avg:60.01ms
step:1710/2420 train_time:102618ms step_avg:60.01ms
step:1711/2420 train_time:102677ms step_avg:60.01ms
step:1712/2420 train_time:102739ms step_avg:60.01ms
step:1713/2420 train_time:102798ms step_avg:60.01ms
step:1714/2420 train_time:102860ms step_avg:60.01ms
step:1715/2420 train_time:102920ms step_avg:60.01ms
step:1716/2420 train_time:102981ms step_avg:60.01ms
step:1717/2420 train_time:103041ms step_avg:60.01ms
step:1718/2420 train_time:103102ms step_avg:60.01ms
step:1719/2420 train_time:103162ms step_avg:60.01ms
step:1720/2420 train_time:103224ms step_avg:60.01ms
step:1721/2420 train_time:103284ms step_avg:60.01ms
step:1722/2420 train_time:103345ms step_avg:60.01ms
step:1723/2420 train_time:103405ms step_avg:60.01ms
step:1724/2420 train_time:103467ms step_avg:60.02ms
step:1725/2420 train_time:103527ms step_avg:60.02ms
step:1726/2420 train_time:103590ms step_avg:60.02ms
step:1727/2420 train_time:103650ms step_avg:60.02ms
step:1728/2420 train_time:103712ms step_avg:60.02ms
step:1729/2420 train_time:103772ms step_avg:60.02ms
step:1730/2420 train_time:103834ms step_avg:60.02ms
step:1731/2420 train_time:103894ms step_avg:60.02ms
step:1732/2420 train_time:103956ms step_avg:60.02ms
step:1733/2420 train_time:104016ms step_avg:60.02ms
step:1734/2420 train_time:104078ms step_avg:60.02ms
step:1735/2420 train_time:104137ms step_avg:60.02ms
step:1736/2420 train_time:104199ms step_avg:60.02ms
step:1737/2420 train_time:104258ms step_avg:60.02ms
step:1738/2420 train_time:104320ms step_avg:60.02ms
step:1739/2420 train_time:104379ms step_avg:60.02ms
step:1740/2420 train_time:104441ms step_avg:60.02ms
step:1741/2420 train_time:104501ms step_avg:60.02ms
step:1742/2420 train_time:104563ms step_avg:60.02ms
step:1743/2420 train_time:104623ms step_avg:60.02ms
step:1744/2420 train_time:104685ms step_avg:60.03ms
step:1745/2420 train_time:104745ms step_avg:60.03ms
step:1746/2420 train_time:104807ms step_avg:60.03ms
step:1747/2420 train_time:104867ms step_avg:60.03ms
step:1748/2420 train_time:104929ms step_avg:60.03ms
step:1749/2420 train_time:104989ms step_avg:60.03ms
step:1750/2420 train_time:105051ms step_avg:60.03ms
step:1750/2420 val_loss:3.4020 train_time:105115ms step_avg:60.07ms
step:1751/2420 train_time:105135ms step_avg:60.04ms
step:1752/2420 train_time:105175ms step_avg:60.03ms
step:1753/2420 train_time:105234ms step_avg:60.03ms
step:1754/2420 train_time:105297ms step_avg:60.03ms
step:1755/2420 train_time:105358ms step_avg:60.03ms
step:1756/2420 train_time:105421ms step_avg:60.03ms
step:1757/2420 train_time:105479ms step_avg:60.03ms
step:1758/2420 train_time:105540ms step_avg:60.03ms
step:1759/2420 train_time:105599ms step_avg:60.03ms
step:1760/2420 train_time:105660ms step_avg:60.03ms
step:1761/2420 train_time:105719ms step_avg:60.03ms
step:1762/2420 train_time:105779ms step_avg:60.03ms
step:1763/2420 train_time:105838ms step_avg:60.03ms
step:1764/2420 train_time:105898ms step_avg:60.03ms
step:1765/2420 train_time:105957ms step_avg:60.03ms
step:1766/2420 train_time:106022ms step_avg:60.04ms
step:1767/2420 train_time:106086ms step_avg:60.04ms
step:1768/2420 train_time:106148ms step_avg:60.04ms
step:1769/2420 train_time:106208ms step_avg:60.04ms
step:1770/2420 train_time:106270ms step_avg:60.04ms
step:1771/2420 train_time:106331ms step_avg:60.04ms
step:1772/2420 train_time:106393ms step_avg:60.04ms
step:1773/2420 train_time:106453ms step_avg:60.04ms
step:1774/2420 train_time:106515ms step_avg:60.04ms
step:1775/2420 train_time:106575ms step_avg:60.04ms
step:1776/2420 train_time:106636ms step_avg:60.04ms
step:1777/2420 train_time:106696ms step_avg:60.04ms
step:1778/2420 train_time:106757ms step_avg:60.04ms
step:1779/2420 train_time:106816ms step_avg:60.04ms
step:1780/2420 train_time:106876ms step_avg:60.04ms
step:1781/2420 train_time:106935ms step_avg:60.04ms
step:1782/2420 train_time:106997ms step_avg:60.04ms
step:1783/2420 train_time:107057ms step_avg:60.04ms
step:1784/2420 train_time:107120ms step_avg:60.04ms
step:1785/2420 train_time:107180ms step_avg:60.05ms
step:1786/2420 train_time:107242ms step_avg:60.05ms
step:1787/2420 train_time:107302ms step_avg:60.05ms
step:1788/2420 train_time:107365ms step_avg:60.05ms
step:1789/2420 train_time:107424ms step_avg:60.05ms
step:1790/2420 train_time:107486ms step_avg:60.05ms
step:1791/2420 train_time:107546ms step_avg:60.05ms
step:1792/2420 train_time:107607ms step_avg:60.05ms
step:1793/2420 train_time:107667ms step_avg:60.05ms
step:1794/2420 train_time:107729ms step_avg:60.05ms
step:1795/2420 train_time:107789ms step_avg:60.05ms
step:1796/2420 train_time:107850ms step_avg:60.05ms
step:1797/2420 train_time:107910ms step_avg:60.05ms
step:1798/2420 train_time:107972ms step_avg:60.05ms
step:1799/2420 train_time:108033ms step_avg:60.05ms
step:1800/2420 train_time:108095ms step_avg:60.05ms
step:1801/2420 train_time:108155ms step_avg:60.05ms
step:1802/2420 train_time:108217ms step_avg:60.05ms
step:1803/2420 train_time:108277ms step_avg:60.05ms
step:1804/2420 train_time:108339ms step_avg:60.05ms
step:1805/2420 train_time:108398ms step_avg:60.05ms
step:1806/2420 train_time:108459ms step_avg:60.06ms
step:1807/2420 train_time:108519ms step_avg:60.05ms
step:1808/2420 train_time:108581ms step_avg:60.06ms
step:1809/2420 train_time:108640ms step_avg:60.06ms
step:1810/2420 train_time:108702ms step_avg:60.06ms
step:1811/2420 train_time:108762ms step_avg:60.06ms
step:1812/2420 train_time:108823ms step_avg:60.06ms
step:1813/2420 train_time:108884ms step_avg:60.06ms
step:1814/2420 train_time:108945ms step_avg:60.06ms
step:1815/2420 train_time:109006ms step_avg:60.06ms
step:1816/2420 train_time:109067ms step_avg:60.06ms
step:1817/2420 train_time:109128ms step_avg:60.06ms
step:1818/2420 train_time:109190ms step_avg:60.06ms
step:1819/2420 train_time:109250ms step_avg:60.06ms
step:1820/2420 train_time:109312ms step_avg:60.06ms
step:1821/2420 train_time:109372ms step_avg:60.06ms
step:1822/2420 train_time:109434ms step_avg:60.06ms
step:1823/2420 train_time:109494ms step_avg:60.06ms
step:1824/2420 train_time:109556ms step_avg:60.06ms
step:1825/2420 train_time:109615ms step_avg:60.06ms
step:1826/2420 train_time:109677ms step_avg:60.06ms
step:1827/2420 train_time:109737ms step_avg:60.06ms
step:1828/2420 train_time:109798ms step_avg:60.06ms
step:1829/2420 train_time:109857ms step_avg:60.06ms
step:1830/2420 train_time:109918ms step_avg:60.06ms
step:1831/2420 train_time:109978ms step_avg:60.06ms
step:1832/2420 train_time:110039ms step_avg:60.07ms
step:1833/2420 train_time:110099ms step_avg:60.06ms
step:1834/2420 train_time:110161ms step_avg:60.07ms
step:1835/2420 train_time:110220ms step_avg:60.07ms
step:1836/2420 train_time:110282ms step_avg:60.07ms
step:1837/2420 train_time:110342ms step_avg:60.07ms
step:1838/2420 train_time:110404ms step_avg:60.07ms
step:1839/2420 train_time:110463ms step_avg:60.07ms
step:1840/2420 train_time:110525ms step_avg:60.07ms
step:1841/2420 train_time:110584ms step_avg:60.07ms
step:1842/2420 train_time:110647ms step_avg:60.07ms
step:1843/2420 train_time:110707ms step_avg:60.07ms
step:1844/2420 train_time:110769ms step_avg:60.07ms
step:1845/2420 train_time:110829ms step_avg:60.07ms
step:1846/2420 train_time:110891ms step_avg:60.07ms
step:1847/2420 train_time:110951ms step_avg:60.07ms
step:1848/2420 train_time:111012ms step_avg:60.07ms
step:1849/2420 train_time:111072ms step_avg:60.07ms
step:1850/2420 train_time:111134ms step_avg:60.07ms
step:1851/2420 train_time:111195ms step_avg:60.07ms
step:1852/2420 train_time:111257ms step_avg:60.07ms
step:1853/2420 train_time:111316ms step_avg:60.07ms
step:1854/2420 train_time:111378ms step_avg:60.07ms
step:1855/2420 train_time:111438ms step_avg:60.07ms
step:1856/2420 train_time:111499ms step_avg:60.07ms
step:1857/2420 train_time:111558ms step_avg:60.07ms
step:1858/2420 train_time:111619ms step_avg:60.07ms
step:1859/2420 train_time:111678ms step_avg:60.07ms
step:1860/2420 train_time:111740ms step_avg:60.08ms
step:1861/2420 train_time:111799ms step_avg:60.07ms
step:1862/2420 train_time:111860ms step_avg:60.08ms
step:1863/2420 train_time:111920ms step_avg:60.08ms
step:1864/2420 train_time:111982ms step_avg:60.08ms
step:1865/2420 train_time:112041ms step_avg:60.08ms
step:1866/2420 train_time:112103ms step_avg:60.08ms
step:1867/2420 train_time:112163ms step_avg:60.08ms
step:1868/2420 train_time:112224ms step_avg:60.08ms
step:1869/2420 train_time:112284ms step_avg:60.08ms
step:1870/2420 train_time:112346ms step_avg:60.08ms
step:1871/2420 train_time:112406ms step_avg:60.08ms
step:1872/2420 train_time:112468ms step_avg:60.08ms
step:1873/2420 train_time:112528ms step_avg:60.08ms
step:1874/2420 train_time:112590ms step_avg:60.08ms
step:1875/2420 train_time:112649ms step_avg:60.08ms
step:1876/2420 train_time:112711ms step_avg:60.08ms
step:1877/2420 train_time:112772ms step_avg:60.08ms
step:1878/2420 train_time:112834ms step_avg:60.08ms
step:1879/2420 train_time:112894ms step_avg:60.08ms
step:1880/2420 train_time:112956ms step_avg:60.08ms
step:1881/2420 train_time:113015ms step_avg:60.08ms
step:1882/2420 train_time:113077ms step_avg:60.08ms
step:1883/2420 train_time:113136ms step_avg:60.08ms
step:1884/2420 train_time:113198ms step_avg:60.08ms
step:1885/2420 train_time:113258ms step_avg:60.08ms
step:1886/2420 train_time:113320ms step_avg:60.08ms
step:1887/2420 train_time:113380ms step_avg:60.08ms
step:1888/2420 train_time:113441ms step_avg:60.09ms
step:1889/2420 train_time:113500ms step_avg:60.08ms
step:1890/2420 train_time:113562ms step_avg:60.09ms
step:1891/2420 train_time:113622ms step_avg:60.09ms
step:1892/2420 train_time:113683ms step_avg:60.09ms
step:1893/2420 train_time:113743ms step_avg:60.09ms
step:1894/2420 train_time:113805ms step_avg:60.09ms
step:1895/2420 train_time:113865ms step_avg:60.09ms
step:1896/2420 train_time:113927ms step_avg:60.09ms
step:1897/2420 train_time:113987ms step_avg:60.09ms
step:1898/2420 train_time:114049ms step_avg:60.09ms
step:1899/2420 train_time:114110ms step_avg:60.09ms
step:1900/2420 train_time:114171ms step_avg:60.09ms
step:1901/2420 train_time:114231ms step_avg:60.09ms
step:1902/2420 train_time:114294ms step_avg:60.09ms
step:1903/2420 train_time:114354ms step_avg:60.09ms
step:1904/2420 train_time:114415ms step_avg:60.09ms
step:1905/2420 train_time:114475ms step_avg:60.09ms
step:1906/2420 train_time:114537ms step_avg:60.09ms
step:1907/2420 train_time:114596ms step_avg:60.09ms
step:1908/2420 train_time:114657ms step_avg:60.09ms
step:1909/2420 train_time:114716ms step_avg:60.09ms
step:1910/2420 train_time:114778ms step_avg:60.09ms
step:1911/2420 train_time:114838ms step_avg:60.09ms
step:1912/2420 train_time:114899ms step_avg:60.09ms
step:1913/2420 train_time:114960ms step_avg:60.09ms
step:1914/2420 train_time:115022ms step_avg:60.10ms
step:1915/2420 train_time:115082ms step_avg:60.09ms
step:1916/2420 train_time:115143ms step_avg:60.10ms
step:1917/2420 train_time:115203ms step_avg:60.10ms
step:1918/2420 train_time:115265ms step_avg:60.10ms
step:1919/2420 train_time:115325ms step_avg:60.10ms
step:1920/2420 train_time:115386ms step_avg:60.10ms
step:1921/2420 train_time:115446ms step_avg:60.10ms
step:1922/2420 train_time:115508ms step_avg:60.10ms
step:1923/2420 train_time:115568ms step_avg:60.10ms
step:1924/2420 train_time:115630ms step_avg:60.10ms
step:1925/2420 train_time:115690ms step_avg:60.10ms
step:1926/2420 train_time:115751ms step_avg:60.10ms
step:1927/2420 train_time:115811ms step_avg:60.10ms
step:1928/2420 train_time:115873ms step_avg:60.10ms
step:1929/2420 train_time:115933ms step_avg:60.10ms
step:1930/2420 train_time:115995ms step_avg:60.10ms
step:1931/2420 train_time:116055ms step_avg:60.10ms
step:1932/2420 train_time:116116ms step_avg:60.10ms
step:1933/2420 train_time:116176ms step_avg:60.10ms
step:1934/2420 train_time:116237ms step_avg:60.10ms
step:1935/2420 train_time:116296ms step_avg:60.10ms
step:1936/2420 train_time:116358ms step_avg:60.10ms
step:1937/2420 train_time:116417ms step_avg:60.10ms
step:1938/2420 train_time:116478ms step_avg:60.10ms
step:1939/2420 train_time:116538ms step_avg:60.10ms
step:1940/2420 train_time:116601ms step_avg:60.10ms
step:1941/2420 train_time:116660ms step_avg:60.10ms
step:1942/2420 train_time:116722ms step_avg:60.10ms
step:1943/2420 train_time:116781ms step_avg:60.10ms
step:1944/2420 train_time:116842ms step_avg:60.10ms
step:1945/2420 train_time:116902ms step_avg:60.10ms
step:1946/2420 train_time:116964ms step_avg:60.10ms
step:1947/2420 train_time:117024ms step_avg:60.10ms
step:1948/2420 train_time:117086ms step_avg:60.11ms
step:1949/2420 train_time:117146ms step_avg:60.11ms
step:1950/2420 train_time:117207ms step_avg:60.11ms
step:1951/2420 train_time:117267ms step_avg:60.11ms
step:1952/2420 train_time:117329ms step_avg:60.11ms
step:1953/2420 train_time:117389ms step_avg:60.11ms
step:1954/2420 train_time:117451ms step_avg:60.11ms
step:1955/2420 train_time:117511ms step_avg:60.11ms
step:1956/2420 train_time:117573ms step_avg:60.11ms
step:1957/2420 train_time:117633ms step_avg:60.11ms
step:1958/2420 train_time:117695ms step_avg:60.11ms
step:1959/2420 train_time:117756ms step_avg:60.11ms
step:1960/2420 train_time:117817ms step_avg:60.11ms
step:1961/2420 train_time:117877ms step_avg:60.11ms
step:1962/2420 train_time:117939ms step_avg:60.11ms
step:1963/2420 train_time:117999ms step_avg:60.11ms
step:1964/2420 train_time:118060ms step_avg:60.11ms
step:1965/2420 train_time:118118ms step_avg:60.11ms
step:1966/2420 train_time:118180ms step_avg:60.11ms
step:1967/2420 train_time:118239ms step_avg:60.11ms
step:1968/2420 train_time:118301ms step_avg:60.11ms
step:1969/2420 train_time:118361ms step_avg:60.11ms
step:1970/2420 train_time:118424ms step_avg:60.11ms
step:1971/2420 train_time:118483ms step_avg:60.11ms
step:1972/2420 train_time:118545ms step_avg:60.11ms
step:1973/2420 train_time:118605ms step_avg:60.11ms
step:1974/2420 train_time:118667ms step_avg:60.11ms
step:1975/2420 train_time:118727ms step_avg:60.11ms
step:1976/2420 train_time:118789ms step_avg:60.12ms
step:1977/2420 train_time:118849ms step_avg:60.12ms
step:1978/2420 train_time:118911ms step_avg:60.12ms
step:1979/2420 train_time:118971ms step_avg:60.12ms
step:1980/2420 train_time:119033ms step_avg:60.12ms
step:1981/2420 train_time:119093ms step_avg:60.12ms
step:1982/2420 train_time:119155ms step_avg:60.12ms
step:1983/2420 train_time:119216ms step_avg:60.12ms
step:1984/2420 train_time:119277ms step_avg:60.12ms
step:1985/2420 train_time:119337ms step_avg:60.12ms
step:1986/2420 train_time:119399ms step_avg:60.12ms
step:1987/2420 train_time:119458ms step_avg:60.12ms
step:1988/2420 train_time:119519ms step_avg:60.12ms
step:1989/2420 train_time:119579ms step_avg:60.12ms
step:1990/2420 train_time:119640ms step_avg:60.12ms
step:1991/2420 train_time:119700ms step_avg:60.12ms
step:1992/2420 train_time:119762ms step_avg:60.12ms
step:1993/2420 train_time:119822ms step_avg:60.12ms
step:1994/2420 train_time:119883ms step_avg:60.12ms
step:1995/2420 train_time:119943ms step_avg:60.12ms
step:1996/2420 train_time:120005ms step_avg:60.12ms
step:1997/2420 train_time:120066ms step_avg:60.12ms
step:1998/2420 train_time:120128ms step_avg:60.12ms
step:1999/2420 train_time:120188ms step_avg:60.12ms
step:2000/2420 train_time:120250ms step_avg:60.12ms
step:2000/2420 val_loss:3.3474 train_time:120315ms step_avg:60.16ms
step:2001/2420 train_time:120335ms step_avg:60.14ms
step:2002/2420 train_time:120376ms step_avg:60.13ms
step:2003/2420 train_time:120440ms step_avg:60.13ms
step:2004/2420 train_time:120503ms step_avg:60.13ms
step:2005/2420 train_time:120563ms step_avg:60.13ms
step:2006/2420 train_time:120624ms step_avg:60.13ms
step:2007/2420 train_time:120684ms step_avg:60.13ms
step:2008/2420 train_time:120745ms step_avg:60.13ms
step:2009/2420 train_time:120804ms step_avg:60.13ms
step:2010/2420 train_time:120865ms step_avg:60.13ms
step:2011/2420 train_time:120924ms step_avg:60.13ms
step:2012/2420 train_time:120985ms step_avg:60.13ms
step:2013/2420 train_time:121044ms step_avg:60.13ms
step:2014/2420 train_time:121105ms step_avg:60.13ms
step:2015/2420 train_time:121164ms step_avg:60.13ms
step:2016/2420 train_time:121225ms step_avg:60.13ms
step:2017/2420 train_time:121285ms step_avg:60.13ms
step:2018/2420 train_time:121348ms step_avg:60.13ms
step:2019/2420 train_time:121410ms step_avg:60.13ms
step:2020/2420 train_time:121472ms step_avg:60.13ms
step:2021/2420 train_time:121533ms step_avg:60.14ms
step:2022/2420 train_time:121595ms step_avg:60.14ms
step:2023/2420 train_time:121655ms step_avg:60.14ms
step:2024/2420 train_time:121717ms step_avg:60.14ms
step:2025/2420 train_time:121776ms step_avg:60.14ms
step:2026/2420 train_time:121838ms step_avg:60.14ms
step:2027/2420 train_time:121897ms step_avg:60.14ms
step:2028/2420 train_time:121958ms step_avg:60.14ms
step:2029/2420 train_time:122018ms step_avg:60.14ms
step:2030/2420 train_time:122079ms step_avg:60.14ms
step:2031/2420 train_time:122139ms step_avg:60.14ms
step:2032/2420 train_time:122200ms step_avg:60.14ms
step:2033/2420 train_time:122261ms step_avg:60.14ms
step:2034/2420 train_time:122323ms step_avg:60.14ms
step:2035/2420 train_time:122383ms step_avg:60.14ms
step:2036/2420 train_time:122445ms step_avg:60.14ms
step:2037/2420 train_time:122505ms step_avg:60.14ms
step:2038/2420 train_time:122566ms step_avg:60.14ms
step:2039/2420 train_time:122626ms step_avg:60.14ms
step:2040/2420 train_time:122688ms step_avg:60.14ms
step:2041/2420 train_time:122748ms step_avg:60.14ms
step:2042/2420 train_time:122809ms step_avg:60.14ms
step:2043/2420 train_time:122869ms step_avg:60.14ms
step:2044/2420 train_time:122931ms step_avg:60.14ms
step:2045/2420 train_time:122990ms step_avg:60.14ms
step:2046/2420 train_time:123052ms step_avg:60.14ms
step:2047/2420 train_time:123112ms step_avg:60.14ms
step:2048/2420 train_time:123174ms step_avg:60.14ms
step:2049/2420 train_time:123233ms step_avg:60.14ms
step:2050/2420 train_time:123295ms step_avg:60.14ms
step:2051/2420 train_time:123356ms step_avg:60.14ms
step:2052/2420 train_time:123419ms step_avg:60.15ms
step:2053/2420 train_time:123479ms step_avg:60.15ms
step:2054/2420 train_time:123541ms step_avg:60.15ms
step:2055/2420 train_time:123601ms step_avg:60.15ms
step:2056/2420 train_time:123663ms step_avg:60.15ms
step:2057/2420 train_time:123723ms step_avg:60.15ms
step:2058/2420 train_time:123784ms step_avg:60.15ms
step:2059/2420 train_time:123844ms step_avg:60.15ms
step:2060/2420 train_time:123906ms step_avg:60.15ms
step:2061/2420 train_time:123965ms step_avg:60.15ms
step:2062/2420 train_time:124026ms step_avg:60.15ms
step:2063/2420 train_time:124085ms step_avg:60.15ms
step:2064/2420 train_time:124147ms step_avg:60.15ms
step:2065/2420 train_time:124206ms step_avg:60.15ms
step:2066/2420 train_time:124268ms step_avg:60.15ms
step:2067/2420 train_time:124328ms step_avg:60.15ms
step:2068/2420 train_time:124390ms step_avg:60.15ms
step:2069/2420 train_time:124450ms step_avg:60.15ms
step:2070/2420 train_time:124513ms step_avg:60.15ms
step:2071/2420 train_time:124573ms step_avg:60.15ms
step:2072/2420 train_time:124634ms step_avg:60.15ms
step:2073/2420 train_time:124694ms step_avg:60.15ms
step:2074/2420 train_time:124756ms step_avg:60.15ms
step:2075/2420 train_time:124817ms step_avg:60.15ms
step:2076/2420 train_time:124879ms step_avg:60.15ms
step:2077/2420 train_time:124938ms step_avg:60.15ms
step:2078/2420 train_time:125000ms step_avg:60.15ms
step:2079/2420 train_time:125060ms step_avg:60.15ms
step:2080/2420 train_time:125122ms step_avg:60.15ms
step:2081/2420 train_time:125182ms step_avg:60.15ms
step:2082/2420 train_time:125243ms step_avg:60.16ms
step:2083/2420 train_time:125303ms step_avg:60.16ms
step:2084/2420 train_time:125365ms step_avg:60.16ms
step:2085/2420 train_time:125424ms step_avg:60.16ms
step:2086/2420 train_time:125485ms step_avg:60.16ms
step:2087/2420 train_time:125545ms step_avg:60.16ms
step:2088/2420 train_time:125606ms step_avg:60.16ms
step:2089/2420 train_time:125666ms step_avg:60.16ms
step:2090/2420 train_time:125727ms step_avg:60.16ms
step:2091/2420 train_time:125787ms step_avg:60.16ms
step:2092/2420 train_time:125849ms step_avg:60.16ms
step:2093/2420 train_time:125909ms step_avg:60.16ms
step:2094/2420 train_time:125971ms step_avg:60.16ms
step:2095/2420 train_time:126030ms step_avg:60.16ms
step:2096/2420 train_time:126092ms step_avg:60.16ms
step:2097/2420 train_time:126152ms step_avg:60.16ms
step:2098/2420 train_time:126213ms step_avg:60.16ms
step:2099/2420 train_time:126273ms step_avg:60.16ms
step:2100/2420 train_time:126335ms step_avg:60.16ms
step:2101/2420 train_time:126395ms step_avg:60.16ms
step:2102/2420 train_time:126457ms step_avg:60.16ms
step:2103/2420 train_time:126517ms step_avg:60.16ms
step:2104/2420 train_time:126579ms step_avg:60.16ms
step:2105/2420 train_time:126639ms step_avg:60.16ms
step:2106/2420 train_time:126700ms step_avg:60.16ms
step:2107/2420 train_time:126761ms step_avg:60.16ms
step:2108/2420 train_time:126823ms step_avg:60.16ms
step:2109/2420 train_time:126882ms step_avg:60.16ms
step:2110/2420 train_time:126944ms step_avg:60.16ms
step:2111/2420 train_time:127004ms step_avg:60.16ms
step:2112/2420 train_time:127065ms step_avg:60.16ms
step:2113/2420 train_time:127125ms step_avg:60.16ms
step:2114/2420 train_time:127187ms step_avg:60.16ms
step:2115/2420 train_time:127246ms step_avg:60.16ms
step:2116/2420 train_time:127308ms step_avg:60.16ms
step:2117/2420 train_time:127367ms step_avg:60.16ms
step:2118/2420 train_time:127429ms step_avg:60.16ms
step:2119/2420 train_time:127488ms step_avg:60.16ms
step:2120/2420 train_time:127550ms step_avg:60.16ms
step:2121/2420 train_time:127610ms step_avg:60.17ms
step:2122/2420 train_time:127673ms step_avg:60.17ms
step:2123/2420 train_time:127733ms step_avg:60.17ms
step:2124/2420 train_time:127794ms step_avg:60.17ms
step:2125/2420 train_time:127854ms step_avg:60.17ms
step:2126/2420 train_time:127916ms step_avg:60.17ms
step:2127/2420 train_time:127977ms step_avg:60.17ms
step:2128/2420 train_time:128039ms step_avg:60.17ms
step:2129/2420 train_time:128099ms step_avg:60.17ms
step:2130/2420 train_time:128162ms step_avg:60.17ms
step:2131/2420 train_time:128222ms step_avg:60.17ms
step:2132/2420 train_time:128283ms step_avg:60.17ms
step:2133/2420 train_time:128343ms step_avg:60.17ms
step:2134/2420 train_time:128404ms step_avg:60.17ms
step:2135/2420 train_time:128463ms step_avg:60.17ms
step:2136/2420 train_time:128525ms step_avg:60.17ms
step:2137/2420 train_time:128585ms step_avg:60.17ms
step:2138/2420 train_time:128647ms step_avg:60.17ms
step:2139/2420 train_time:128706ms step_avg:60.17ms
step:2140/2420 train_time:128768ms step_avg:60.17ms
step:2141/2420 train_time:128827ms step_avg:60.17ms
step:2142/2420 train_time:128889ms step_avg:60.17ms
step:2143/2420 train_time:128949ms step_avg:60.17ms
step:2144/2420 train_time:129011ms step_avg:60.17ms
step:2145/2420 train_time:129071ms step_avg:60.17ms
step:2146/2420 train_time:129133ms step_avg:60.17ms
step:2147/2420 train_time:129193ms step_avg:60.17ms
step:2148/2420 train_time:129256ms step_avg:60.18ms
step:2149/2420 train_time:129316ms step_avg:60.18ms
step:2150/2420 train_time:129379ms step_avg:60.18ms
step:2151/2420 train_time:129439ms step_avg:60.18ms
step:2152/2420 train_time:129502ms step_avg:60.18ms
step:2153/2420 train_time:129561ms step_avg:60.18ms
step:2154/2420 train_time:129623ms step_avg:60.18ms
step:2155/2420 train_time:129682ms step_avg:60.18ms
step:2156/2420 train_time:129744ms step_avg:60.18ms
step:2157/2420 train_time:129803ms step_avg:60.18ms
step:2158/2420 train_time:129865ms step_avg:60.18ms
step:2159/2420 train_time:129925ms step_avg:60.18ms
step:2160/2420 train_time:129987ms step_avg:60.18ms
step:2161/2420 train_time:130046ms step_avg:60.18ms
step:2162/2420 train_time:130108ms step_avg:60.18ms
step:2163/2420 train_time:130168ms step_avg:60.18ms
step:2164/2420 train_time:130230ms step_avg:60.18ms
step:2165/2420 train_time:130289ms step_avg:60.18ms
step:2166/2420 train_time:130351ms step_avg:60.18ms
step:2167/2420 train_time:130411ms step_avg:60.18ms
step:2168/2420 train_time:130473ms step_avg:60.18ms
step:2169/2420 train_time:130534ms step_avg:60.18ms
step:2170/2420 train_time:130596ms step_avg:60.18ms
step:2171/2420 train_time:130656ms step_avg:60.18ms
step:2172/2420 train_time:130718ms step_avg:60.18ms
step:2173/2420 train_time:130778ms step_avg:60.18ms
step:2174/2420 train_time:130840ms step_avg:60.18ms
step:2175/2420 train_time:130900ms step_avg:60.18ms
step:2176/2420 train_time:130962ms step_avg:60.18ms
step:2177/2420 train_time:131023ms step_avg:60.18ms
step:2178/2420 train_time:131084ms step_avg:60.19ms
step:2179/2420 train_time:131144ms step_avg:60.19ms
step:2180/2420 train_time:131206ms step_avg:60.19ms
step:2181/2420 train_time:131265ms step_avg:60.19ms
step:2182/2420 train_time:131327ms step_avg:60.19ms
step:2183/2420 train_time:131387ms step_avg:60.19ms
step:2184/2420 train_time:131449ms step_avg:60.19ms
step:2185/2420 train_time:131508ms step_avg:60.19ms
step:2186/2420 train_time:131570ms step_avg:60.19ms
step:2187/2420 train_time:131630ms step_avg:60.19ms
step:2188/2420 train_time:131692ms step_avg:60.19ms
step:2189/2420 train_time:131752ms step_avg:60.19ms
step:2190/2420 train_time:131814ms step_avg:60.19ms
step:2191/2420 train_time:131874ms step_avg:60.19ms
step:2192/2420 train_time:131936ms step_avg:60.19ms
step:2193/2420 train_time:131996ms step_avg:60.19ms
step:2194/2420 train_time:132058ms step_avg:60.19ms
step:2195/2420 train_time:132119ms step_avg:60.19ms
step:2196/2420 train_time:132181ms step_avg:60.19ms
step:2197/2420 train_time:132241ms step_avg:60.19ms
step:2198/2420 train_time:132303ms step_avg:60.19ms
step:2199/2420 train_time:132363ms step_avg:60.19ms
step:2200/2420 train_time:132424ms step_avg:60.19ms
step:2201/2420 train_time:132484ms step_avg:60.19ms
step:2202/2420 train_time:132545ms step_avg:60.19ms
step:2203/2420 train_time:132605ms step_avg:60.19ms
step:2204/2420 train_time:132667ms step_avg:60.19ms
step:2205/2420 train_time:132727ms step_avg:60.19ms
step:2206/2420 train_time:132788ms step_avg:60.19ms
step:2207/2420 train_time:132848ms step_avg:60.19ms
step:2208/2420 train_time:132910ms step_avg:60.19ms
step:2209/2420 train_time:132970ms step_avg:60.19ms
step:2210/2420 train_time:133032ms step_avg:60.20ms
step:2211/2420 train_time:133093ms step_avg:60.20ms
step:2212/2420 train_time:133155ms step_avg:60.20ms
step:2213/2420 train_time:133215ms step_avg:60.20ms
step:2214/2420 train_time:133277ms step_avg:60.20ms
step:2215/2420 train_time:133337ms step_avg:60.20ms
step:2216/2420 train_time:133399ms step_avg:60.20ms
step:2217/2420 train_time:133460ms step_avg:60.20ms
step:2218/2420 train_time:133522ms step_avg:60.20ms
step:2219/2420 train_time:133582ms step_avg:60.20ms
step:2220/2420 train_time:133643ms step_avg:60.20ms
step:2221/2420 train_time:133703ms step_avg:60.20ms
step:2222/2420 train_time:133764ms step_avg:60.20ms
step:2223/2420 train_time:133823ms step_avg:60.20ms
step:2224/2420 train_time:133885ms step_avg:60.20ms
step:2225/2420 train_time:133945ms step_avg:60.20ms
step:2226/2420 train_time:134006ms step_avg:60.20ms
step:2227/2420 train_time:134066ms step_avg:60.20ms
step:2228/2420 train_time:134128ms step_avg:60.20ms
step:2229/2420 train_time:134188ms step_avg:60.20ms
step:2230/2420 train_time:134250ms step_avg:60.20ms
step:2231/2420 train_time:134310ms step_avg:60.20ms
step:2232/2420 train_time:134372ms step_avg:60.20ms
step:2233/2420 train_time:134432ms step_avg:60.20ms
step:2234/2420 train_time:134494ms step_avg:60.20ms
step:2235/2420 train_time:134555ms step_avg:60.20ms
step:2236/2420 train_time:134617ms step_avg:60.20ms
step:2237/2420 train_time:134677ms step_avg:60.20ms
step:2238/2420 train_time:134738ms step_avg:60.20ms
step:2239/2420 train_time:134798ms step_avg:60.20ms
step:2240/2420 train_time:134860ms step_avg:60.21ms
step:2241/2420 train_time:134921ms step_avg:60.21ms
step:2242/2420 train_time:134982ms step_avg:60.21ms
step:2243/2420 train_time:135042ms step_avg:60.21ms
step:2244/2420 train_time:135104ms step_avg:60.21ms
step:2245/2420 train_time:135164ms step_avg:60.21ms
step:2246/2420 train_time:135225ms step_avg:60.21ms
step:2247/2420 train_time:135285ms step_avg:60.21ms
step:2248/2420 train_time:135346ms step_avg:60.21ms
step:2249/2420 train_time:135406ms step_avg:60.21ms
step:2250/2420 train_time:135468ms step_avg:60.21ms
step:2250/2420 val_loss:3.3030 train_time:135532ms step_avg:60.24ms
step:2251/2420 train_time:135552ms step_avg:60.22ms
step:2252/2420 train_time:135592ms step_avg:60.21ms
step:2253/2420 train_time:135656ms step_avg:60.21ms
step:2254/2420 train_time:135720ms step_avg:60.21ms
step:2255/2420 train_time:135781ms step_avg:60.21ms
step:2256/2420 train_time:135842ms step_avg:60.21ms
step:2257/2420 train_time:135902ms step_avg:60.21ms
step:2258/2420 train_time:135963ms step_avg:60.21ms
step:2259/2420 train_time:136023ms step_avg:60.21ms
step:2260/2420 train_time:136084ms step_avg:60.21ms
step:2261/2420 train_time:136144ms step_avg:60.21ms
step:2262/2420 train_time:136205ms step_avg:60.21ms
step:2263/2420 train_time:136264ms step_avg:60.21ms
step:2264/2420 train_time:136326ms step_avg:60.21ms
step:2265/2420 train_time:136386ms step_avg:60.21ms
step:2266/2420 train_time:136447ms step_avg:60.22ms
step:2267/2420 train_time:136508ms step_avg:60.22ms
step:2268/2420 train_time:136572ms step_avg:60.22ms
step:2269/2420 train_time:136634ms step_avg:60.22ms
step:2270/2420 train_time:136696ms step_avg:60.22ms
step:2271/2420 train_time:136756ms step_avg:60.22ms
step:2272/2420 train_time:136818ms step_avg:60.22ms
step:2273/2420 train_time:136877ms step_avg:60.22ms
step:2274/2420 train_time:136937ms step_avg:60.22ms
step:2275/2420 train_time:136997ms step_avg:60.22ms
step:2276/2420 train_time:137058ms step_avg:60.22ms
step:2277/2420 train_time:137117ms step_avg:60.22ms
step:2278/2420 train_time:137179ms step_avg:60.22ms
step:2279/2420 train_time:137238ms step_avg:60.22ms
step:2280/2420 train_time:137299ms step_avg:60.22ms
step:2281/2420 train_time:137358ms step_avg:60.22ms
step:2282/2420 train_time:137420ms step_avg:60.22ms
step:2283/2420 train_time:137480ms step_avg:60.22ms
step:2284/2420 train_time:137543ms step_avg:60.22ms
step:2285/2420 train_time:137603ms step_avg:60.22ms
step:2286/2420 train_time:137666ms step_avg:60.22ms
step:2287/2420 train_time:137727ms step_avg:60.22ms
step:2288/2420 train_time:137789ms step_avg:60.22ms
step:2289/2420 train_time:137850ms step_avg:60.22ms
step:2290/2420 train_time:137912ms step_avg:60.22ms
step:2291/2420 train_time:137972ms step_avg:60.22ms
step:2292/2420 train_time:138034ms step_avg:60.22ms
step:2293/2420 train_time:138093ms step_avg:60.22ms
step:2294/2420 train_time:138406ms step_avg:60.33ms
step:2295/2420 train_time:138463ms step_avg:60.33ms
step:2296/2420 train_time:138525ms step_avg:60.33ms
step:2297/2420 train_time:138583ms step_avg:60.33ms
step:2298/2420 train_time:138644ms step_avg:60.33ms
step:2299/2420 train_time:138702ms step_avg:60.33ms
step:2300/2420 train_time:138763ms step_avg:60.33ms
step:2301/2420 train_time:138822ms step_avg:60.33ms
step:2302/2420 train_time:138883ms step_avg:60.33ms
step:2303/2420 train_time:138942ms step_avg:60.33ms
step:2304/2420 train_time:139002ms step_avg:60.33ms
step:2305/2420 train_time:139061ms step_avg:60.33ms
step:2306/2420 train_time:139122ms step_avg:60.33ms
step:2307/2420 train_time:139180ms step_avg:60.33ms
step:2308/2420 train_time:139242ms step_avg:60.33ms
step:2309/2420 train_time:139309ms step_avg:60.33ms
step:2310/2420 train_time:139377ms step_avg:60.34ms
step:2311/2420 train_time:139438ms step_avg:60.34ms
step:2312/2420 train_time:139501ms step_avg:60.34ms
step:2313/2420 train_time:139562ms step_avg:60.34ms
step:2314/2420 train_time:139623ms step_avg:60.34ms
step:2315/2420 train_time:139683ms step_avg:60.34ms
step:2316/2420 train_time:139744ms step_avg:60.34ms
step:2317/2420 train_time:139803ms step_avg:60.34ms
step:2318/2420 train_time:139864ms step_avg:60.34ms
step:2319/2420 train_time:139923ms step_avg:60.34ms
step:2320/2420 train_time:139984ms step_avg:60.34ms
step:2321/2420 train_time:140043ms step_avg:60.34ms
step:2322/2420 train_time:140103ms step_avg:60.34ms
step:2323/2420 train_time:140162ms step_avg:60.34ms
step:2324/2420 train_time:140225ms step_avg:60.34ms
step:2325/2420 train_time:140287ms step_avg:60.34ms
step:2326/2420 train_time:140352ms step_avg:60.34ms
step:2327/2420 train_time:140412ms step_avg:60.34ms
step:2328/2420 train_time:140475ms step_avg:60.34ms
step:2329/2420 train_time:140535ms step_avg:60.34ms
step:2330/2420 train_time:140597ms step_avg:60.34ms
step:2331/2420 train_time:140657ms step_avg:60.34ms
step:2332/2420 train_time:140717ms step_avg:60.34ms
step:2333/2420 train_time:140777ms step_avg:60.34ms
step:2334/2420 train_time:140838ms step_avg:60.34ms
step:2335/2420 train_time:140897ms step_avg:60.34ms
step:2336/2420 train_time:140958ms step_avg:60.34ms
step:2337/2420 train_time:141017ms step_avg:60.34ms
step:2338/2420 train_time:141078ms step_avg:60.34ms
step:2339/2420 train_time:141138ms step_avg:60.34ms
step:2340/2420 train_time:141199ms step_avg:60.34ms
step:2341/2420 train_time:141261ms step_avg:60.34ms
step:2342/2420 train_time:141324ms step_avg:60.34ms
step:2343/2420 train_time:141385ms step_avg:60.34ms
step:2344/2420 train_time:141448ms step_avg:60.34ms
step:2345/2420 train_time:141508ms step_avg:60.34ms
step:2346/2420 train_time:141571ms step_avg:60.35ms
step:2347/2420 train_time:141631ms step_avg:60.35ms
step:2348/2420 train_time:141693ms step_avg:60.35ms
step:2349/2420 train_time:141753ms step_avg:60.35ms
step:2350/2420 train_time:141814ms step_avg:60.35ms
step:2351/2420 train_time:141874ms step_avg:60.35ms
step:2352/2420 train_time:141935ms step_avg:60.35ms
step:2353/2420 train_time:141995ms step_avg:60.35ms
step:2354/2420 train_time:142056ms step_avg:60.35ms
step:2355/2420 train_time:142115ms step_avg:60.35ms
step:2356/2420 train_time:142176ms step_avg:60.35ms
step:2357/2420 train_time:142236ms step_avg:60.35ms
step:2358/2420 train_time:142298ms step_avg:60.35ms
step:2359/2420 train_time:142359ms step_avg:60.35ms
step:2360/2420 train_time:142423ms step_avg:60.35ms
step:2361/2420 train_time:142483ms step_avg:60.35ms
step:2362/2420 train_time:142545ms step_avg:60.35ms
step:2363/2420 train_time:142605ms step_avg:60.35ms
step:2364/2420 train_time:142667ms step_avg:60.35ms
step:2365/2420 train_time:142726ms step_avg:60.35ms
step:2366/2420 train_time:142788ms step_avg:60.35ms
step:2367/2420 train_time:142848ms step_avg:60.35ms
step:2368/2420 train_time:142910ms step_avg:60.35ms
step:2369/2420 train_time:142970ms step_avg:60.35ms
step:2370/2420 train_time:143032ms step_avg:60.35ms
step:2371/2420 train_time:143092ms step_avg:60.35ms
step:2372/2420 train_time:143154ms step_avg:60.35ms
step:2373/2420 train_time:143214ms step_avg:60.35ms
step:2374/2420 train_time:143275ms step_avg:60.35ms
step:2375/2420 train_time:143335ms step_avg:60.35ms
step:2376/2420 train_time:143397ms step_avg:60.35ms
step:2377/2420 train_time:143457ms step_avg:60.35ms
step:2378/2420 train_time:143518ms step_avg:60.35ms
step:2379/2420 train_time:143578ms step_avg:60.35ms
step:2380/2420 train_time:143640ms step_avg:60.35ms
step:2381/2420 train_time:143700ms step_avg:60.35ms
step:2382/2420 train_time:143762ms step_avg:60.35ms
step:2383/2420 train_time:144126ms step_avg:60.48ms
step:2384/2420 train_time:144149ms step_avg:60.47ms
step:2385/2420 train_time:144229ms step_avg:60.47ms
step:2386/2420 train_time:144535ms step_avg:60.58ms
step:2387/2420 train_time:144592ms step_avg:60.57ms
step:2388/2420 train_time:144936ms step_avg:60.69ms
step:2389/2420 train_time:144994ms step_avg:60.69ms
step:2390/2420 train_time:145055ms step_avg:60.69ms
step:2391/2420 train_time:145113ms step_avg:60.69ms
step:2392/2420 train_time:145173ms step_avg:60.69ms
step:2393/2420 train_time:145232ms step_avg:60.69ms
step:2394/2420 train_time:145292ms step_avg:60.69ms
step:2395/2420 train_time:145351ms step_avg:60.69ms
step:2396/2420 train_time:145412ms step_avg:60.69ms
step:2397/2420 train_time:145470ms step_avg:60.69ms
step:2398/2420 train_time:145531ms step_avg:60.69ms
step:2399/2420 train_time:145590ms step_avg:60.69ms
step:2400/2420 train_time:145651ms step_avg:60.69ms
step:2401/2420 train_time:145709ms step_avg:60.69ms
step:2402/2420 train_time:145771ms step_avg:60.69ms
step:2403/2420 train_time:145838ms step_avg:60.69ms
step:2404/2420 train_time:145905ms step_avg:60.69ms
step:2405/2420 train_time:145967ms step_avg:60.69ms
step:2406/2420 train_time:146029ms step_avg:60.69ms
step:2407/2420 train_time:146089ms step_avg:60.69ms
step:2408/2420 train_time:146151ms step_avg:60.69ms
step:2409/2420 train_time:146210ms step_avg:60.69ms
step:2410/2420 train_time:146271ms step_avg:60.69ms
step:2411/2420 train_time:146330ms step_avg:60.69ms
step:2412/2420 train_time:146391ms step_avg:60.69ms
step:2413/2420 train_time:146451ms step_avg:60.69ms
step:2414/2420 train_time:146512ms step_avg:60.69ms
step:2415/2420 train_time:146571ms step_avg:60.69ms
step:2416/2420 train_time:146631ms step_avg:60.69ms
step:2417/2420 train_time:146691ms step_avg:60.69ms
step:2418/2420 train_time:146754ms step_avg:60.69ms
step:2419/2420 train_time:146816ms step_avg:60.69ms
step:2420/2420 train_time:146879ms step_avg:60.69ms
step:2420/2420 val_loss:3.2771 train_time:146943ms step_avg:60.72ms
peak memory allocated: 29512 MiB reserved: 44036 MiB

import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:31:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   43C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            129W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           49695      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           49696      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49697      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49698      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49699      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49700      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49701      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49702      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           49696      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           49697      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           49698      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           49699      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           49700      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           49701      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           49702      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:91ms step_avg:90.85ms
step:2/2160 train_time:117ms step_avg:58.27ms
step:3/2160 train_time:139ms step_avg:46.31ms
step:4/2160 train_time:163ms step_avg:40.65ms
step:5/2160 train_time:195ms step_avg:38.93ms
step:6/2160 train_time:334ms step_avg:55.73ms
step:7/2160 train_time:369ms step_avg:52.68ms
step:8/2160 train_time:401ms step_avg:50.15ms
step:9/2160 train_time:434ms step_avg:48.24ms
step:10/2160 train_time:466ms step_avg:46.64ms
step:11/2160 train_time:500ms step_avg:45.41ms
step:12/2160 train_time:532ms step_avg:44.35ms
step:13/2160 train_time:565ms step_avg:43.49ms
step:14/2160 train_time:598ms step_avg:42.70ms
step:15/2160 train_time:631ms step_avg:42.05ms
step:16/2160 train_time:663ms step_avg:41.44ms
step:17/2160 train_time:696ms step_avg:40.97ms
step:18/2160 train_time:729ms step_avg:40.48ms
step:19/2160 train_time:762ms step_avg:40.09ms
step:20/2160 train_time:794ms step_avg:39.71ms
step:21/2160 train_time:827ms step_avg:39.40ms
step:22/2160 train_time:860ms step_avg:39.08ms
step:23/2160 train_time:893ms step_avg:38.81ms
step:24/2160 train_time:925ms step_avg:38.55ms
step:25/2160 train_time:958ms step_avg:38.32ms
step:26/2160 train_time:990ms step_avg:38.09ms
step:27/2160 train_time:1024ms step_avg:37.92ms
step:28/2160 train_time:1056ms step_avg:37.73ms
step:29/2160 train_time:1090ms step_avg:37.57ms
step:30/2160 train_time:1122ms step_avg:37.40ms
step:31/2160 train_time:1155ms step_avg:37.26ms
step:32/2160 train_time:1187ms step_avg:37.11ms
step:33/2160 train_time:1220ms step_avg:36.98ms
step:34/2160 train_time:1253ms step_avg:36.86ms
step:35/2160 train_time:1288ms step_avg:36.80ms
step:36/2160 train_time:1321ms step_avg:36.69ms
step:37/2160 train_time:1355ms step_avg:36.61ms
step:38/2160 train_time:1387ms step_avg:36.51ms
step:39/2160 train_time:1421ms step_avg:36.44ms
step:40/2160 train_time:1453ms step_avg:36.34ms
step:41/2160 train_time:1487ms step_avg:36.28ms
step:42/2160 train_time:1520ms step_avg:36.19ms
step:43/2160 train_time:1553ms step_avg:36.12ms
step:44/2160 train_time:1585ms step_avg:36.03ms
step:45/2160 train_time:1619ms step_avg:35.98ms
step:46/2160 train_time:1652ms step_avg:35.92ms
step:47/2160 train_time:1686ms step_avg:35.86ms
step:48/2160 train_time:1718ms step_avg:35.80ms
step:49/2160 train_time:1752ms step_avg:35.75ms
step:50/2160 train_time:1785ms step_avg:35.69ms
step:51/2160 train_time:1817ms step_avg:35.63ms
step:52/2160 train_time:1850ms step_avg:35.57ms
step:53/2160 train_time:1883ms step_avg:35.52ms
step:54/2160 train_time:1915ms step_avg:35.47ms
step:55/2160 train_time:1949ms step_avg:35.43ms
step:56/2160 train_time:1981ms step_avg:35.38ms
step:57/2160 train_time:2014ms step_avg:35.34ms
step:58/2160 train_time:2047ms step_avg:35.28ms
step:59/2160 train_time:2080ms step_avg:35.25ms
step:60/2160 train_time:2112ms step_avg:35.20ms
step:61/2160 train_time:2145ms step_avg:35.17ms
step:62/2160 train_time:2178ms step_avg:35.13ms
step:63/2160 train_time:2211ms step_avg:35.10ms
step:64/2160 train_time:2244ms step_avg:35.06ms
step:65/2160 train_time:2277ms step_avg:35.04ms
step:66/2160 train_time:2310ms step_avg:35.00ms
step:67/2160 train_time:2343ms step_avg:34.98ms
step:68/2160 train_time:2376ms step_avg:34.94ms
step:69/2160 train_time:2409ms step_avg:34.92ms
step:70/2160 train_time:2442ms step_avg:34.88ms
step:71/2160 train_time:2475ms step_avg:34.86ms
step:72/2160 train_time:2508ms step_avg:34.83ms
step:73/2160 train_time:2542ms step_avg:34.82ms
step:74/2160 train_time:2574ms step_avg:34.78ms
step:75/2160 train_time:2608ms step_avg:34.77ms
step:76/2160 train_time:2640ms step_avg:34.74ms
step:77/2160 train_time:2673ms step_avg:34.72ms
step:78/2160 train_time:2706ms step_avg:34.69ms
step:79/2160 train_time:2739ms step_avg:34.67ms
step:80/2160 train_time:2772ms step_avg:34.65ms
step:81/2160 train_time:2805ms step_avg:34.63ms
step:82/2160 train_time:2837ms step_avg:34.60ms
step:83/2160 train_time:2871ms step_avg:34.59ms
step:84/2160 train_time:2903ms step_avg:34.56ms
step:85/2160 train_time:2936ms step_avg:34.54ms
step:86/2160 train_time:2969ms step_avg:34.52ms
step:87/2160 train_time:3002ms step_avg:34.51ms
step:88/2160 train_time:3034ms step_avg:34.48ms
step:89/2160 train_time:3068ms step_avg:34.47ms
step:90/2160 train_time:3100ms step_avg:34.44ms
step:91/2160 train_time:3133ms step_avg:34.43ms
step:92/2160 train_time:3165ms step_avg:34.41ms
step:93/2160 train_time:3199ms step_avg:34.40ms
step:94/2160 train_time:3231ms step_avg:34.38ms
step:95/2160 train_time:3264ms step_avg:34.36ms
step:96/2160 train_time:3297ms step_avg:34.34ms
step:97/2160 train_time:3330ms step_avg:34.33ms
step:98/2160 train_time:3363ms step_avg:34.31ms
step:99/2160 train_time:3396ms step_avg:34.30ms
step:100/2160 train_time:3429ms step_avg:34.29ms
step:101/2160 train_time:3462ms step_avg:34.28ms
step:102/2160 train_time:3495ms step_avg:34.26ms
step:103/2160 train_time:3528ms step_avg:34.25ms
step:104/2160 train_time:3560ms step_avg:34.24ms
step:105/2160 train_time:3594ms step_avg:34.23ms
step:106/2160 train_time:3626ms step_avg:34.21ms
step:107/2160 train_time:3659ms step_avg:34.20ms
step:108/2160 train_time:3692ms step_avg:34.19ms
step:109/2160 train_time:3726ms step_avg:34.18ms
step:110/2160 train_time:3758ms step_avg:34.17ms
step:111/2160 train_time:3792ms step_avg:34.16ms
step:112/2160 train_time:3824ms step_avg:34.14ms
step:113/2160 train_time:3857ms step_avg:34.13ms
step:114/2160 train_time:3889ms step_avg:34.11ms
step:115/2160 train_time:3922ms step_avg:34.11ms
step:116/2160 train_time:3955ms step_avg:34.09ms
step:117/2160 train_time:3988ms step_avg:34.09ms
step:118/2160 train_time:4020ms step_avg:34.07ms
step:119/2160 train_time:4053ms step_avg:34.06ms
step:120/2160 train_time:4086ms step_avg:34.05ms
step:121/2160 train_time:4119ms step_avg:34.04ms
step:122/2160 train_time:4151ms step_avg:34.02ms
step:123/2160 train_time:4185ms step_avg:34.02ms
step:124/2160 train_time:4217ms step_avg:34.01ms
step:125/2160 train_time:4250ms step_avg:34.00ms
step:126/2160 train_time:4283ms step_avg:33.99ms
step:127/2160 train_time:4316ms step_avg:33.99ms
step:128/2160 train_time:4349ms step_avg:33.97ms
step:129/2160 train_time:4382ms step_avg:33.97ms
step:130/2160 train_time:4415ms step_avg:33.96ms
step:131/2160 train_time:4448ms step_avg:33.95ms
step:132/2160 train_time:4480ms step_avg:33.94ms
step:133/2160 train_time:4513ms step_avg:33.94ms
step:134/2160 train_time:4546ms step_avg:33.92ms
step:135/2160 train_time:4579ms step_avg:33.92ms
step:136/2160 train_time:4611ms step_avg:33.91ms
step:137/2160 train_time:4645ms step_avg:33.90ms
step:138/2160 train_time:4677ms step_avg:33.89ms
step:139/2160 train_time:4710ms step_avg:33.89ms
step:140/2160 train_time:4743ms step_avg:33.88ms
step:141/2160 train_time:4776ms step_avg:33.87ms
step:142/2160 train_time:4808ms step_avg:33.86ms
step:143/2160 train_time:4841ms step_avg:33.86ms
step:144/2160 train_time:4874ms step_avg:33.85ms
step:145/2160 train_time:4907ms step_avg:33.84ms
step:146/2160 train_time:4939ms step_avg:33.83ms
step:147/2160 train_time:4972ms step_avg:33.83ms
step:148/2160 train_time:5005ms step_avg:33.81ms
step:149/2160 train_time:5038ms step_avg:33.81ms
step:150/2160 train_time:5070ms step_avg:33.80ms
step:151/2160 train_time:5103ms step_avg:33.80ms
step:152/2160 train_time:5136ms step_avg:33.79ms
step:153/2160 train_time:5169ms step_avg:33.78ms
step:154/2160 train_time:5201ms step_avg:33.77ms
step:155/2160 train_time:5234ms step_avg:33.77ms
step:156/2160 train_time:5267ms step_avg:33.76ms
step:157/2160 train_time:5300ms step_avg:33.75ms
step:158/2160 train_time:5332ms step_avg:33.74ms
step:159/2160 train_time:5365ms step_avg:33.74ms
step:160/2160 train_time:5398ms step_avg:33.73ms
step:161/2160 train_time:5431ms step_avg:33.73ms
step:162/2160 train_time:5463ms step_avg:33.72ms
step:163/2160 train_time:5496ms step_avg:33.72ms
step:164/2160 train_time:5529ms step_avg:33.71ms
step:165/2160 train_time:5562ms step_avg:33.71ms
step:166/2160 train_time:5594ms step_avg:33.70ms
step:167/2160 train_time:5628ms step_avg:33.70ms
step:168/2160 train_time:5660ms step_avg:33.69ms
step:169/2160 train_time:5693ms step_avg:33.69ms
step:170/2160 train_time:5726ms step_avg:33.68ms
step:171/2160 train_time:5758ms step_avg:33.68ms
step:172/2160 train_time:5791ms step_avg:33.67ms
step:173/2160 train_time:5824ms step_avg:33.67ms
step:174/2160 train_time:5857ms step_avg:33.66ms
step:175/2160 train_time:5890ms step_avg:33.66ms
step:176/2160 train_time:5922ms step_avg:33.65ms
step:177/2160 train_time:5955ms step_avg:33.64ms
step:178/2160 train_time:5987ms step_avg:33.64ms
step:179/2160 train_time:6021ms step_avg:33.64ms
step:180/2160 train_time:6053ms step_avg:33.63ms
step:181/2160 train_time:6086ms step_avg:33.63ms
step:182/2160 train_time:6119ms step_avg:33.62ms
step:183/2160 train_time:6151ms step_avg:33.61ms
step:184/2160 train_time:6184ms step_avg:33.61ms
step:185/2160 train_time:6216ms step_avg:33.60ms
step:186/2160 train_time:6248ms step_avg:33.59ms
step:187/2160 train_time:6282ms step_avg:33.59ms
step:188/2160 train_time:6314ms step_avg:33.59ms
step:189/2160 train_time:6347ms step_avg:33.58ms
step:190/2160 train_time:6380ms step_avg:33.58ms
step:191/2160 train_time:6412ms step_avg:33.57ms
step:192/2160 train_time:6445ms step_avg:33.57ms
step:193/2160 train_time:6478ms step_avg:33.56ms
step:194/2160 train_time:6510ms step_avg:33.56ms
step:195/2160 train_time:6543ms step_avg:33.55ms
step:196/2160 train_time:6576ms step_avg:33.55ms
step:197/2160 train_time:6609ms step_avg:33.55ms
step:198/2160 train_time:6641ms step_avg:33.54ms
step:199/2160 train_time:6674ms step_avg:33.54ms
step:200/2160 train_time:6706ms step_avg:33.53ms
step:201/2160 train_time:6739ms step_avg:33.53ms
step:202/2160 train_time:6772ms step_avg:33.52ms
step:203/2160 train_time:6805ms step_avg:33.52ms
step:204/2160 train_time:6837ms step_avg:33.51ms
step:205/2160 train_time:6870ms step_avg:33.51ms
step:206/2160 train_time:6902ms step_avg:33.50ms
step:207/2160 train_time:6935ms step_avg:33.50ms
step:208/2160 train_time:6967ms step_avg:33.50ms
step:209/2160 train_time:7001ms step_avg:33.50ms
step:210/2160 train_time:7033ms step_avg:33.49ms
step:211/2160 train_time:7066ms step_avg:33.49ms
step:212/2160 train_time:7099ms step_avg:33.48ms
step:213/2160 train_time:7131ms step_avg:33.48ms
step:214/2160 train_time:7164ms step_avg:33.47ms
step:215/2160 train_time:7197ms step_avg:33.47ms
step:216/2160 train_time:7229ms step_avg:33.47ms
step:217/2160 train_time:7263ms step_avg:33.47ms
step:218/2160 train_time:7295ms step_avg:33.46ms
step:219/2160 train_time:7328ms step_avg:33.46ms
step:220/2160 train_time:7360ms step_avg:33.46ms
step:221/2160 train_time:7393ms step_avg:33.45ms
step:222/2160 train_time:7426ms step_avg:33.45ms
step:223/2160 train_time:7459ms step_avg:33.45ms
step:224/2160 train_time:7491ms step_avg:33.44ms
step:225/2160 train_time:7524ms step_avg:33.44ms
step:226/2160 train_time:7557ms step_avg:33.44ms
step:227/2160 train_time:7590ms step_avg:33.44ms
step:228/2160 train_time:7622ms step_avg:33.43ms
step:229/2160 train_time:7655ms step_avg:33.43ms
step:230/2160 train_time:7687ms step_avg:33.42ms
step:231/2160 train_time:7720ms step_avg:33.42ms
step:232/2160 train_time:7753ms step_avg:33.42ms
step:233/2160 train_time:7786ms step_avg:33.42ms
step:234/2160 train_time:7818ms step_avg:33.41ms
step:235/2160 train_time:7852ms step_avg:33.41ms
step:236/2160 train_time:7884ms step_avg:33.41ms
step:237/2160 train_time:7917ms step_avg:33.40ms
step:238/2160 train_time:7950ms step_avg:33.40ms
step:239/2160 train_time:7983ms step_avg:33.40ms
step:240/2160 train_time:8015ms step_avg:33.40ms
step:241/2160 train_time:8048ms step_avg:33.40ms
step:242/2160 train_time:8081ms step_avg:33.39ms
step:243/2160 train_time:8113ms step_avg:33.39ms
step:244/2160 train_time:8146ms step_avg:33.38ms
step:245/2160 train_time:8179ms step_avg:33.38ms
step:246/2160 train_time:8212ms step_avg:33.38ms
step:247/2160 train_time:8245ms step_avg:33.38ms
step:248/2160 train_time:8277ms step_avg:33.38ms
step:249/2160 train_time:8310ms step_avg:33.37ms
step:250/2160 train_time:8342ms step_avg:33.37ms
step:250/2160 val_loss:4.3112 train_time:8378ms step_avg:33.51ms
step:251/2160 train_time:8399ms step_avg:33.46ms
step:252/2160 train_time:8421ms step_avg:33.42ms
step:253/2160 train_time:8444ms step_avg:33.38ms
step:254/2160 train_time:8476ms step_avg:33.37ms
step:255/2160 train_time:8512ms step_avg:33.38ms
step:256/2160 train_time:8547ms step_avg:33.39ms
step:257/2160 train_time:8581ms step_avg:33.39ms
step:258/2160 train_time:8614ms step_avg:33.39ms
step:259/2160 train_time:8648ms step_avg:33.39ms
step:260/2160 train_time:8681ms step_avg:33.39ms
step:261/2160 train_time:8714ms step_avg:33.39ms
step:262/2160 train_time:8746ms step_avg:33.38ms
step:263/2160 train_time:8779ms step_avg:33.38ms
step:264/2160 train_time:8811ms step_avg:33.37ms
step:265/2160 train_time:8844ms step_avg:33.37ms
step:266/2160 train_time:8876ms step_avg:33.37ms
step:267/2160 train_time:8909ms step_avg:33.37ms
step:268/2160 train_time:8942ms step_avg:33.36ms
step:269/2160 train_time:8974ms step_avg:33.36ms
step:270/2160 train_time:9006ms step_avg:33.36ms
step:271/2160 train_time:9039ms step_avg:33.36ms
step:272/2160 train_time:9071ms step_avg:33.35ms
step:273/2160 train_time:9104ms step_avg:33.35ms
step:274/2160 train_time:9137ms step_avg:33.34ms
step:275/2160 train_time:9170ms step_avg:33.34ms
step:276/2160 train_time:9202ms step_avg:33.34ms
step:277/2160 train_time:9235ms step_avg:33.34ms
step:278/2160 train_time:9267ms step_avg:33.34ms
step:279/2160 train_time:9300ms step_avg:33.33ms
step:280/2160 train_time:9332ms step_avg:33.33ms
step:281/2160 train_time:9365ms step_avg:33.33ms
step:282/2160 train_time:9397ms step_avg:33.32ms
step:283/2160 train_time:9431ms step_avg:33.32ms
step:284/2160 train_time:9463ms step_avg:33.32ms
step:285/2160 train_time:9496ms step_avg:33.32ms
step:286/2160 train_time:9529ms step_avg:33.32ms
step:287/2160 train_time:9562ms step_avg:33.32ms
step:288/2160 train_time:9595ms step_avg:33.32ms
step:289/2160 train_time:9628ms step_avg:33.32ms
step:290/2160 train_time:9661ms step_avg:33.31ms
step:291/2160 train_time:9694ms step_avg:33.31ms
step:292/2160 train_time:9726ms step_avg:33.31ms
step:293/2160 train_time:9759ms step_avg:33.31ms
step:294/2160 train_time:9791ms step_avg:33.30ms
step:295/2160 train_time:9825ms step_avg:33.30ms
step:296/2160 train_time:9857ms step_avg:33.30ms
step:297/2160 train_time:9890ms step_avg:33.30ms
step:298/2160 train_time:9923ms step_avg:33.30ms
step:299/2160 train_time:9955ms step_avg:33.30ms
step:300/2160 train_time:9988ms step_avg:33.29ms
step:301/2160 train_time:10020ms step_avg:33.29ms
step:302/2160 train_time:10053ms step_avg:33.29ms
step:303/2160 train_time:10086ms step_avg:33.29ms
step:304/2160 train_time:10118ms step_avg:33.28ms
step:305/2160 train_time:10151ms step_avg:33.28ms
step:306/2160 train_time:10183ms step_avg:33.28ms
step:307/2160 train_time:10216ms step_avg:33.28ms
step:308/2160 train_time:10248ms step_avg:33.27ms
step:309/2160 train_time:10281ms step_avg:33.27ms
step:310/2160 train_time:10314ms step_avg:33.27ms
step:311/2160 train_time:10347ms step_avg:33.27ms
step:312/2160 train_time:10379ms step_avg:33.27ms
step:313/2160 train_time:10412ms step_avg:33.26ms
step:314/2160 train_time:10444ms step_avg:33.26ms
step:315/2160 train_time:10477ms step_avg:33.26ms
step:316/2160 train_time:10509ms step_avg:33.26ms
step:317/2160 train_time:10543ms step_avg:33.26ms
step:318/2160 train_time:10575ms step_avg:33.26ms
step:319/2160 train_time:10609ms step_avg:33.26ms
step:320/2160 train_time:10641ms step_avg:33.25ms
step:321/2160 train_time:10674ms step_avg:33.25ms
step:322/2160 train_time:10706ms step_avg:33.25ms
step:323/2160 train_time:10740ms step_avg:33.25ms
step:324/2160 train_time:10772ms step_avg:33.25ms
step:325/2160 train_time:10805ms step_avg:33.25ms
step:326/2160 train_time:10838ms step_avg:33.24ms
step:327/2160 train_time:10870ms step_avg:33.24ms
step:328/2160 train_time:10903ms step_avg:33.24ms
step:329/2160 train_time:10935ms step_avg:33.24ms
step:330/2160 train_time:10968ms step_avg:33.24ms
step:331/2160 train_time:11001ms step_avg:33.23ms
step:332/2160 train_time:11033ms step_avg:33.23ms
step:333/2160 train_time:11066ms step_avg:33.23ms
step:334/2160 train_time:11099ms step_avg:33.23ms
step:335/2160 train_time:11131ms step_avg:33.23ms
step:336/2160 train_time:11164ms step_avg:33.22ms
step:337/2160 train_time:11197ms step_avg:33.22ms
step:338/2160 train_time:11229ms step_avg:33.22ms
step:339/2160 train_time:11262ms step_avg:33.22ms
step:340/2160 train_time:11294ms step_avg:33.22ms
step:341/2160 train_time:11327ms step_avg:33.22ms
step:342/2160 train_time:11360ms step_avg:33.22ms
step:343/2160 train_time:11392ms step_avg:33.21ms
step:344/2160 train_time:11425ms step_avg:33.21ms
step:345/2160 train_time:11458ms step_avg:33.21ms
step:346/2160 train_time:11490ms step_avg:33.21ms
step:347/2160 train_time:11523ms step_avg:33.21ms
step:348/2160 train_time:11555ms step_avg:33.21ms
step:349/2160 train_time:11589ms step_avg:33.21ms
step:350/2160 train_time:11621ms step_avg:33.20ms
step:351/2160 train_time:11654ms step_avg:33.20ms
step:352/2160 train_time:11687ms step_avg:33.20ms
step:353/2160 train_time:11720ms step_avg:33.20ms
step:354/2160 train_time:11752ms step_avg:33.20ms
step:355/2160 train_time:11786ms step_avg:33.20ms
step:356/2160 train_time:11818ms step_avg:33.20ms
step:357/2160 train_time:11851ms step_avg:33.20ms
step:358/2160 train_time:11883ms step_avg:33.19ms
step:359/2160 train_time:11916ms step_avg:33.19ms
step:360/2160 train_time:11949ms step_avg:33.19ms
step:361/2160 train_time:11982ms step_avg:33.19ms
step:362/2160 train_time:12014ms step_avg:33.19ms
step:363/2160 train_time:12047ms step_avg:33.19ms
step:364/2160 train_time:12080ms step_avg:33.19ms
step:365/2160 train_time:12113ms step_avg:33.19ms
step:366/2160 train_time:12145ms step_avg:33.18ms
step:367/2160 train_time:12178ms step_avg:33.18ms
step:368/2160 train_time:12210ms step_avg:33.18ms
step:369/2160 train_time:12243ms step_avg:33.18ms
step:370/2160 train_time:12276ms step_avg:33.18ms
step:371/2160 train_time:12309ms step_avg:33.18ms
step:372/2160 train_time:12341ms step_avg:33.17ms
step:373/2160 train_time:12374ms step_avg:33.17ms
step:374/2160 train_time:12407ms step_avg:33.17ms
step:375/2160 train_time:12439ms step_avg:33.17ms
step:376/2160 train_time:12471ms step_avg:33.17ms
step:377/2160 train_time:12505ms step_avg:33.17ms
step:378/2160 train_time:12538ms step_avg:33.17ms
step:379/2160 train_time:12571ms step_avg:33.17ms
step:380/2160 train_time:12603ms step_avg:33.17ms
step:381/2160 train_time:12636ms step_avg:33.17ms
step:382/2160 train_time:12668ms step_avg:33.16ms
step:383/2160 train_time:12701ms step_avg:33.16ms
step:384/2160 train_time:12734ms step_avg:33.16ms
step:385/2160 train_time:12767ms step_avg:33.16ms
step:386/2160 train_time:12800ms step_avg:33.16ms
step:387/2160 train_time:12833ms step_avg:33.16ms
step:388/2160 train_time:12865ms step_avg:33.16ms
step:389/2160 train_time:12898ms step_avg:33.16ms
step:390/2160 train_time:12930ms step_avg:33.15ms
step:391/2160 train_time:12964ms step_avg:33.16ms
step:392/2160 train_time:12996ms step_avg:33.15ms
step:393/2160 train_time:13029ms step_avg:33.15ms
step:394/2160 train_time:13061ms step_avg:33.15ms
step:395/2160 train_time:13094ms step_avg:33.15ms
step:396/2160 train_time:13126ms step_avg:33.15ms
step:397/2160 train_time:13160ms step_avg:33.15ms
step:398/2160 train_time:13192ms step_avg:33.15ms
step:399/2160 train_time:13225ms step_avg:33.15ms
step:400/2160 train_time:13257ms step_avg:33.14ms
step:401/2160 train_time:13291ms step_avg:33.14ms
step:402/2160 train_time:13323ms step_avg:33.14ms
step:403/2160 train_time:13356ms step_avg:33.14ms
step:404/2160 train_time:13388ms step_avg:33.14ms
step:405/2160 train_time:13421ms step_avg:33.14ms
step:406/2160 train_time:13454ms step_avg:33.14ms
step:407/2160 train_time:13487ms step_avg:33.14ms
step:408/2160 train_time:13519ms step_avg:33.14ms
step:409/2160 train_time:13552ms step_avg:33.14ms
step:410/2160 train_time:13585ms step_avg:33.13ms
step:411/2160 train_time:13618ms step_avg:33.13ms
step:412/2160 train_time:13650ms step_avg:33.13ms
step:413/2160 train_time:13683ms step_avg:33.13ms
step:414/2160 train_time:13716ms step_avg:33.13ms
step:415/2160 train_time:13749ms step_avg:33.13ms
step:416/2160 train_time:13781ms step_avg:33.13ms
step:417/2160 train_time:13814ms step_avg:33.13ms
step:418/2160 train_time:13847ms step_avg:33.13ms
step:419/2160 train_time:13880ms step_avg:33.13ms
step:420/2160 train_time:13913ms step_avg:33.13ms
step:421/2160 train_time:13946ms step_avg:33.13ms
step:422/2160 train_time:13978ms step_avg:33.12ms
step:423/2160 train_time:14011ms step_avg:33.12ms
step:424/2160 train_time:14044ms step_avg:33.12ms
step:425/2160 train_time:14076ms step_avg:33.12ms
step:426/2160 train_time:14109ms step_avg:33.12ms
step:427/2160 train_time:14142ms step_avg:33.12ms
step:428/2160 train_time:14174ms step_avg:33.12ms
step:429/2160 train_time:14208ms step_avg:33.12ms
step:430/2160 train_time:14240ms step_avg:33.12ms
step:431/2160 train_time:14273ms step_avg:33.12ms
step:432/2160 train_time:14305ms step_avg:33.11ms
step:433/2160 train_time:14338ms step_avg:33.11ms
step:434/2160 train_time:14370ms step_avg:33.11ms
step:435/2160 train_time:14404ms step_avg:33.11ms
step:436/2160 train_time:14436ms step_avg:33.11ms
step:437/2160 train_time:14470ms step_avg:33.11ms
step:438/2160 train_time:14502ms step_avg:33.11ms
step:439/2160 train_time:14535ms step_avg:33.11ms
step:440/2160 train_time:14567ms step_avg:33.11ms
step:441/2160 train_time:14600ms step_avg:33.11ms
step:442/2160 train_time:14633ms step_avg:33.11ms
step:443/2160 train_time:14666ms step_avg:33.11ms
step:444/2160 train_time:14698ms step_avg:33.10ms
step:445/2160 train_time:14731ms step_avg:33.10ms
step:446/2160 train_time:14763ms step_avg:33.10ms
step:447/2160 train_time:14796ms step_avg:33.10ms
step:448/2160 train_time:14829ms step_avg:33.10ms
step:449/2160 train_time:14862ms step_avg:33.10ms
step:450/2160 train_time:14894ms step_avg:33.10ms
step:451/2160 train_time:14927ms step_avg:33.10ms
step:452/2160 train_time:14960ms step_avg:33.10ms
step:453/2160 train_time:14993ms step_avg:33.10ms
step:454/2160 train_time:15025ms step_avg:33.09ms
step:455/2160 train_time:15058ms step_avg:33.10ms
step:456/2160 train_time:15091ms step_avg:33.09ms
step:457/2160 train_time:15124ms step_avg:33.09ms
step:458/2160 train_time:15156ms step_avg:33.09ms
step:459/2160 train_time:15189ms step_avg:33.09ms
step:460/2160 train_time:15222ms step_avg:33.09ms
step:461/2160 train_time:15254ms step_avg:33.09ms
step:462/2160 train_time:15287ms step_avg:33.09ms
step:463/2160 train_time:15320ms step_avg:33.09ms
step:464/2160 train_time:15353ms step_avg:33.09ms
step:465/2160 train_time:15386ms step_avg:33.09ms
step:466/2160 train_time:15419ms step_avg:33.09ms
step:467/2160 train_time:15452ms step_avg:33.09ms
step:468/2160 train_time:15484ms step_avg:33.09ms
step:469/2160 train_time:15517ms step_avg:33.09ms
step:470/2160 train_time:15550ms step_avg:33.08ms
step:471/2160 train_time:15583ms step_avg:33.09ms
step:472/2160 train_time:15616ms step_avg:33.08ms
step:473/2160 train_time:15649ms step_avg:33.08ms
step:474/2160 train_time:15681ms step_avg:33.08ms
step:475/2160 train_time:15715ms step_avg:33.08ms
step:476/2160 train_time:15747ms step_avg:33.08ms
step:477/2160 train_time:15780ms step_avg:33.08ms
step:478/2160 train_time:15812ms step_avg:33.08ms
step:479/2160 train_time:15846ms step_avg:33.08ms
step:480/2160 train_time:15878ms step_avg:33.08ms
step:481/2160 train_time:15911ms step_avg:33.08ms
step:482/2160 train_time:15943ms step_avg:33.08ms
step:483/2160 train_time:15976ms step_avg:33.08ms
step:484/2160 train_time:16009ms step_avg:33.08ms
step:485/2160 train_time:16042ms step_avg:33.08ms
step:486/2160 train_time:16074ms step_avg:33.07ms
step:487/2160 train_time:16108ms step_avg:33.08ms
step:488/2160 train_time:16140ms step_avg:33.07ms
step:489/2160 train_time:16173ms step_avg:33.07ms
step:490/2160 train_time:16206ms step_avg:33.07ms
step:491/2160 train_time:16239ms step_avg:33.07ms
step:492/2160 train_time:16271ms step_avg:33.07ms
step:493/2160 train_time:16305ms step_avg:33.07ms
step:494/2160 train_time:16337ms step_avg:33.07ms
step:495/2160 train_time:16370ms step_avg:33.07ms
step:496/2160 train_time:16403ms step_avg:33.07ms
step:497/2160 train_time:16436ms step_avg:33.07ms
step:498/2160 train_time:16468ms step_avg:33.07ms
step:499/2160 train_time:16502ms step_avg:33.07ms
step:500/2160 train_time:16534ms step_avg:33.07ms
step:500/2160 val_loss:4.0216 train_time:16570ms step_avg:33.14ms
step:501/2160 train_time:16591ms step_avg:33.12ms
step:502/2160 train_time:16613ms step_avg:33.09ms
step:503/2160 train_time:16636ms step_avg:33.07ms
step:504/2160 train_time:16670ms step_avg:33.08ms
step:505/2160 train_time:16706ms step_avg:33.08ms
step:506/2160 train_time:16738ms step_avg:33.08ms
step:507/2160 train_time:16772ms step_avg:33.08ms
step:508/2160 train_time:16804ms step_avg:33.08ms
step:509/2160 train_time:16838ms step_avg:33.08ms
step:510/2160 train_time:16870ms step_avg:33.08ms
step:511/2160 train_time:16903ms step_avg:33.08ms
step:512/2160 train_time:16936ms step_avg:33.08ms
step:513/2160 train_time:16968ms step_avg:33.08ms
step:514/2160 train_time:17001ms step_avg:33.07ms
step:515/2160 train_time:17033ms step_avg:33.07ms
step:516/2160 train_time:17066ms step_avg:33.07ms
step:517/2160 train_time:17099ms step_avg:33.07ms
step:518/2160 train_time:17131ms step_avg:33.07ms
step:519/2160 train_time:17163ms step_avg:33.07ms
step:520/2160 train_time:17195ms step_avg:33.07ms
step:521/2160 train_time:17228ms step_avg:33.07ms
step:522/2160 train_time:17260ms step_avg:33.07ms
step:523/2160 train_time:17293ms step_avg:33.07ms
step:524/2160 train_time:17325ms step_avg:33.06ms
step:525/2160 train_time:17358ms step_avg:33.06ms
step:526/2160 train_time:17390ms step_avg:33.06ms
step:527/2160 train_time:17423ms step_avg:33.06ms
step:528/2160 train_time:17456ms step_avg:33.06ms
step:529/2160 train_time:17488ms step_avg:33.06ms
step:530/2160 train_time:17521ms step_avg:33.06ms
step:531/2160 train_time:17554ms step_avg:33.06ms
step:532/2160 train_time:17587ms step_avg:33.06ms
step:533/2160 train_time:17620ms step_avg:33.06ms
step:534/2160 train_time:17653ms step_avg:33.06ms
step:535/2160 train_time:17686ms step_avg:33.06ms
step:536/2160 train_time:17719ms step_avg:33.06ms
step:537/2160 train_time:17752ms step_avg:33.06ms
step:538/2160 train_time:17785ms step_avg:33.06ms
step:539/2160 train_time:17819ms step_avg:33.06ms
step:540/2160 train_time:17851ms step_avg:33.06ms
step:541/2160 train_time:17884ms step_avg:33.06ms
step:542/2160 train_time:17916ms step_avg:33.06ms
step:543/2160 train_time:17950ms step_avg:33.06ms
step:544/2160 train_time:17983ms step_avg:33.06ms
step:545/2160 train_time:18016ms step_avg:33.06ms
step:546/2160 train_time:18048ms step_avg:33.05ms
step:547/2160 train_time:18081ms step_avg:33.06ms
step:548/2160 train_time:18113ms step_avg:33.05ms
step:549/2160 train_time:18146ms step_avg:33.05ms
step:550/2160 train_time:18178ms step_avg:33.05ms
step:551/2160 train_time:18211ms step_avg:33.05ms
step:552/2160 train_time:18243ms step_avg:33.05ms
step:553/2160 train_time:18277ms step_avg:33.05ms
step:554/2160 train_time:18309ms step_avg:33.05ms
step:555/2160 train_time:18342ms step_avg:33.05ms
step:556/2160 train_time:18374ms step_avg:33.05ms
step:557/2160 train_time:18407ms step_avg:33.05ms
step:558/2160 train_time:18439ms step_avg:33.04ms
step:559/2160 train_time:18472ms step_avg:33.04ms
step:560/2160 train_time:18504ms step_avg:33.04ms
step:561/2160 train_time:18537ms step_avg:33.04ms
step:562/2160 train_time:18570ms step_avg:33.04ms
step:563/2160 train_time:18603ms step_avg:33.04ms
step:564/2160 train_time:18635ms step_avg:33.04ms
step:565/2160 train_time:18668ms step_avg:33.04ms
step:566/2160 train_time:18701ms step_avg:33.04ms
step:567/2160 train_time:18735ms step_avg:33.04ms
step:568/2160 train_time:18767ms step_avg:33.04ms
step:569/2160 train_time:18800ms step_avg:33.04ms
step:570/2160 train_time:18833ms step_avg:33.04ms
step:571/2160 train_time:18866ms step_avg:33.04ms
step:572/2160 train_time:18898ms step_avg:33.04ms
step:573/2160 train_time:18931ms step_avg:33.04ms
step:574/2160 train_time:18964ms step_avg:33.04ms
step:575/2160 train_time:18997ms step_avg:33.04ms
step:576/2160 train_time:19029ms step_avg:33.04ms
step:577/2160 train_time:19062ms step_avg:33.04ms
step:578/2160 train_time:19095ms step_avg:33.04ms
step:579/2160 train_time:19127ms step_avg:33.04ms
step:580/2160 train_time:19160ms step_avg:33.03ms
step:581/2160 train_time:19193ms step_avg:33.03ms
step:582/2160 train_time:19225ms step_avg:33.03ms
step:583/2160 train_time:19258ms step_avg:33.03ms
step:584/2160 train_time:19290ms step_avg:33.03ms
step:585/2160 train_time:19323ms step_avg:33.03ms
step:586/2160 train_time:19355ms step_avg:33.03ms
step:587/2160 train_time:19388ms step_avg:33.03ms
step:588/2160 train_time:19421ms step_avg:33.03ms
step:589/2160 train_time:19454ms step_avg:33.03ms
step:590/2160 train_time:19486ms step_avg:33.03ms
step:591/2160 train_time:19519ms step_avg:33.03ms
step:592/2160 train_time:19551ms step_avg:33.03ms
step:593/2160 train_time:19584ms step_avg:33.03ms
step:594/2160 train_time:19616ms step_avg:33.02ms
step:595/2160 train_time:19650ms step_avg:33.02ms
step:596/2160 train_time:19682ms step_avg:33.02ms
step:597/2160 train_time:19716ms step_avg:33.02ms
step:598/2160 train_time:19748ms step_avg:33.02ms
step:599/2160 train_time:19781ms step_avg:33.02ms
step:600/2160 train_time:19814ms step_avg:33.02ms
step:601/2160 train_time:19847ms step_avg:33.02ms
step:602/2160 train_time:19879ms step_avg:33.02ms
step:603/2160 train_time:19913ms step_avg:33.02ms
step:604/2160 train_time:19945ms step_avg:33.02ms
step:605/2160 train_time:19978ms step_avg:33.02ms
step:606/2160 train_time:20011ms step_avg:33.02ms
step:607/2160 train_time:20044ms step_avg:33.02ms
step:608/2160 train_time:20076ms step_avg:33.02ms
step:609/2160 train_time:20109ms step_avg:33.02ms
step:610/2160 train_time:20141ms step_avg:33.02ms
step:611/2160 train_time:20175ms step_avg:33.02ms
step:612/2160 train_time:20207ms step_avg:33.02ms
step:613/2160 train_time:20240ms step_avg:33.02ms
step:614/2160 train_time:20272ms step_avg:33.02ms
step:615/2160 train_time:20305ms step_avg:33.02ms
step:616/2160 train_time:20337ms step_avg:33.02ms
step:617/2160 train_time:20371ms step_avg:33.02ms
step:618/2160 train_time:20403ms step_avg:33.01ms
step:619/2160 train_time:20436ms step_avg:33.01ms
step:620/2160 train_time:20468ms step_avg:33.01ms
step:621/2160 train_time:20501ms step_avg:33.01ms
step:622/2160 train_time:20534ms step_avg:33.01ms
step:623/2160 train_time:20567ms step_avg:33.01ms
step:624/2160 train_time:20599ms step_avg:33.01ms
step:625/2160 train_time:20633ms step_avg:33.01ms
step:626/2160 train_time:20665ms step_avg:33.01ms
step:627/2160 train_time:20699ms step_avg:33.01ms
step:628/2160 train_time:20731ms step_avg:33.01ms
step:629/2160 train_time:20764ms step_avg:33.01ms
step:630/2160 train_time:20797ms step_avg:33.01ms
step:631/2160 train_time:20830ms step_avg:33.01ms
step:632/2160 train_time:20862ms step_avg:33.01ms
step:633/2160 train_time:20896ms step_avg:33.01ms
step:634/2160 train_time:20929ms step_avg:33.01ms
step:635/2160 train_time:20962ms step_avg:33.01ms
step:636/2160 train_time:20994ms step_avg:33.01ms
step:637/2160 train_time:21027ms step_avg:33.01ms
step:638/2160 train_time:21060ms step_avg:33.01ms
step:639/2160 train_time:21093ms step_avg:33.01ms
step:640/2160 train_time:21125ms step_avg:33.01ms
step:641/2160 train_time:21159ms step_avg:33.01ms
step:642/2160 train_time:21191ms step_avg:33.01ms
step:643/2160 train_time:21224ms step_avg:33.01ms
step:644/2160 train_time:21256ms step_avg:33.01ms
step:645/2160 train_time:21290ms step_avg:33.01ms
step:646/2160 train_time:21322ms step_avg:33.01ms
step:647/2160 train_time:21355ms step_avg:33.01ms
step:648/2160 train_time:21388ms step_avg:33.01ms
step:649/2160 train_time:21421ms step_avg:33.01ms
step:650/2160 train_time:21453ms step_avg:33.00ms
step:651/2160 train_time:21486ms step_avg:33.00ms
step:652/2160 train_time:21518ms step_avg:33.00ms
step:653/2160 train_time:21551ms step_avg:33.00ms
step:654/2160 train_time:21584ms step_avg:33.00ms
step:655/2160 train_time:21617ms step_avg:33.00ms
step:656/2160 train_time:21650ms step_avg:33.00ms
step:657/2160 train_time:21683ms step_avg:33.00ms
step:658/2160 train_time:21715ms step_avg:33.00ms
step:659/2160 train_time:21748ms step_avg:33.00ms
step:660/2160 train_time:21780ms step_avg:33.00ms
step:661/2160 train_time:21814ms step_avg:33.00ms
step:662/2160 train_time:21846ms step_avg:33.00ms
step:663/2160 train_time:21880ms step_avg:33.00ms
step:664/2160 train_time:21912ms step_avg:33.00ms
step:665/2160 train_time:21945ms step_avg:33.00ms
step:666/2160 train_time:21977ms step_avg:33.00ms
step:667/2160 train_time:22010ms step_avg:33.00ms
step:668/2160 train_time:22043ms step_avg:33.00ms
step:669/2160 train_time:22076ms step_avg:33.00ms
step:670/2160 train_time:22109ms step_avg:33.00ms
step:671/2160 train_time:22142ms step_avg:33.00ms
step:672/2160 train_time:22174ms step_avg:33.00ms
step:673/2160 train_time:22207ms step_avg:33.00ms
step:674/2160 train_time:22240ms step_avg:33.00ms
step:675/2160 train_time:22273ms step_avg:33.00ms
step:676/2160 train_time:22305ms step_avg:33.00ms
step:677/2160 train_time:22339ms step_avg:33.00ms
step:678/2160 train_time:22371ms step_avg:33.00ms
step:679/2160 train_time:22404ms step_avg:33.00ms
step:680/2160 train_time:22437ms step_avg:32.99ms
step:681/2160 train_time:22469ms step_avg:32.99ms
step:682/2160 train_time:22502ms step_avg:32.99ms
step:683/2160 train_time:22535ms step_avg:32.99ms
step:684/2160 train_time:22568ms step_avg:32.99ms
step:685/2160 train_time:22600ms step_avg:32.99ms
step:686/2160 train_time:22633ms step_avg:32.99ms
step:687/2160 train_time:22666ms step_avg:32.99ms
step:688/2160 train_time:22698ms step_avg:32.99ms
step:689/2160 train_time:22731ms step_avg:32.99ms
step:690/2160 train_time:22764ms step_avg:32.99ms
step:691/2160 train_time:22797ms step_avg:32.99ms
step:692/2160 train_time:22829ms step_avg:32.99ms
step:693/2160 train_time:22862ms step_avg:32.99ms
step:694/2160 train_time:22894ms step_avg:32.99ms
step:695/2160 train_time:22927ms step_avg:32.99ms
step:696/2160 train_time:22960ms step_avg:32.99ms
step:697/2160 train_time:22993ms step_avg:32.99ms
step:698/2160 train_time:23025ms step_avg:32.99ms
step:699/2160 train_time:23059ms step_avg:32.99ms
step:700/2160 train_time:23091ms step_avg:32.99ms
step:701/2160 train_time:23124ms step_avg:32.99ms
step:702/2160 train_time:23157ms step_avg:32.99ms
step:703/2160 train_time:23190ms step_avg:32.99ms
step:704/2160 train_time:23222ms step_avg:32.99ms
step:705/2160 train_time:23255ms step_avg:32.99ms
step:706/2160 train_time:23288ms step_avg:32.99ms
step:707/2160 train_time:23321ms step_avg:32.99ms
step:708/2160 train_time:23354ms step_avg:32.99ms
step:709/2160 train_time:23412ms step_avg:33.02ms
step:710/2160 train_time:23470ms step_avg:33.06ms
step:711/2160 train_time:23530ms step_avg:33.09ms
step:712/2160 train_time:23590ms step_avg:33.13ms
step:713/2160 train_time:23650ms step_avg:33.17ms
step:714/2160 train_time:23709ms step_avg:33.21ms
step:715/2160 train_time:23769ms step_avg:33.24ms
step:716/2160 train_time:23828ms step_avg:33.28ms
step:717/2160 train_time:23889ms step_avg:33.32ms
step:718/2160 train_time:23948ms step_avg:33.35ms
step:719/2160 train_time:24008ms step_avg:33.39ms
step:720/2160 train_time:24067ms step_avg:33.43ms
step:721/2160 train_time:24127ms step_avg:33.46ms
step:722/2160 train_time:24186ms step_avg:33.50ms
step:723/2160 train_time:24247ms step_avg:33.54ms
step:724/2160 train_time:24305ms step_avg:33.57ms
step:725/2160 train_time:24366ms step_avg:33.61ms
step:726/2160 train_time:24424ms step_avg:33.64ms
step:727/2160 train_time:24484ms step_avg:33.68ms
step:728/2160 train_time:24543ms step_avg:33.71ms
step:729/2160 train_time:24603ms step_avg:33.75ms
step:730/2160 train_time:24662ms step_avg:33.78ms
step:731/2160 train_time:24722ms step_avg:33.82ms
step:732/2160 train_time:24780ms step_avg:33.85ms
step:733/2160 train_time:24840ms step_avg:33.89ms
step:734/2160 train_time:24899ms step_avg:33.92ms
step:735/2160 train_time:24959ms step_avg:33.96ms
step:736/2160 train_time:25018ms step_avg:33.99ms
step:737/2160 train_time:25078ms step_avg:34.03ms
step:738/2160 train_time:25136ms step_avg:34.06ms
step:739/2160 train_time:25196ms step_avg:34.09ms
step:740/2160 train_time:25254ms step_avg:34.13ms
step:741/2160 train_time:25314ms step_avg:34.16ms
step:742/2160 train_time:25373ms step_avg:34.20ms
step:743/2160 train_time:25433ms step_avg:34.23ms
step:744/2160 train_time:25492ms step_avg:34.26ms
step:745/2160 train_time:25552ms step_avg:34.30ms
step:746/2160 train_time:25612ms step_avg:34.33ms
step:747/2160 train_time:25672ms step_avg:34.37ms
step:748/2160 train_time:25731ms step_avg:34.40ms
step:749/2160 train_time:25791ms step_avg:34.43ms
step:750/2160 train_time:25851ms step_avg:34.47ms
step:750/2160 val_loss:3.8655 train_time:25913ms step_avg:34.55ms
step:751/2160 train_time:25936ms step_avg:34.53ms
step:752/2160 train_time:25975ms step_avg:34.54ms
step:753/2160 train_time:26038ms step_avg:34.58ms
step:754/2160 train_time:26103ms step_avg:34.62ms
step:755/2160 train_time:26164ms step_avg:34.65ms
step:756/2160 train_time:26222ms step_avg:34.69ms
step:757/2160 train_time:26282ms step_avg:34.72ms
step:758/2160 train_time:26339ms step_avg:34.75ms
step:759/2160 train_time:26399ms step_avg:34.78ms
step:760/2160 train_time:26457ms step_avg:34.81ms
step:761/2160 train_time:26516ms step_avg:34.84ms
step:762/2160 train_time:26574ms step_avg:34.87ms
step:763/2160 train_time:26633ms step_avg:34.91ms
step:764/2160 train_time:26691ms step_avg:34.94ms
step:765/2160 train_time:26750ms step_avg:34.97ms
step:766/2160 train_time:26809ms step_avg:35.00ms
step:767/2160 train_time:26870ms step_avg:35.03ms
step:768/2160 train_time:26930ms step_avg:35.07ms
step:769/2160 train_time:26992ms step_avg:35.10ms
step:770/2160 train_time:27054ms step_avg:35.14ms
step:771/2160 train_time:27117ms step_avg:35.17ms
step:772/2160 train_time:27177ms step_avg:35.20ms
step:773/2160 train_time:27238ms step_avg:35.24ms
step:774/2160 train_time:27297ms step_avg:35.27ms
step:775/2160 train_time:27356ms step_avg:35.30ms
step:776/2160 train_time:27415ms step_avg:35.33ms
step:777/2160 train_time:27474ms step_avg:35.36ms
step:778/2160 train_time:27532ms step_avg:35.39ms
step:779/2160 train_time:27591ms step_avg:35.42ms
step:780/2160 train_time:27650ms step_avg:35.45ms
step:781/2160 train_time:27709ms step_avg:35.48ms
step:782/2160 train_time:27767ms step_avg:35.51ms
step:783/2160 train_time:27827ms step_avg:35.54ms
step:784/2160 train_time:27886ms step_avg:35.57ms
step:785/2160 train_time:27948ms step_avg:35.60ms
step:786/2160 train_time:28008ms step_avg:35.63ms
step:787/2160 train_time:28069ms step_avg:35.67ms
step:788/2160 train_time:28130ms step_avg:35.70ms
step:789/2160 train_time:28191ms step_avg:35.73ms
step:790/2160 train_time:28251ms step_avg:35.76ms
step:791/2160 train_time:28311ms step_avg:35.79ms
step:792/2160 train_time:28370ms step_avg:35.82ms
step:793/2160 train_time:28430ms step_avg:35.85ms
step:794/2160 train_time:28489ms step_avg:35.88ms
step:795/2160 train_time:28548ms step_avg:35.91ms
step:796/2160 train_time:28607ms step_avg:35.94ms
step:797/2160 train_time:28665ms step_avg:35.97ms
step:798/2160 train_time:28723ms step_avg:35.99ms
step:799/2160 train_time:28783ms step_avg:36.02ms
step:800/2160 train_time:28842ms step_avg:36.05ms
step:801/2160 train_time:28902ms step_avg:36.08ms
step:802/2160 train_time:28960ms step_avg:36.11ms
step:803/2160 train_time:29021ms step_avg:36.14ms
step:804/2160 train_time:29080ms step_avg:36.17ms
step:805/2160 train_time:29141ms step_avg:36.20ms
step:806/2160 train_time:29199ms step_avg:36.23ms
step:807/2160 train_time:29259ms step_avg:36.26ms
step:808/2160 train_time:29318ms step_avg:36.28ms
step:809/2160 train_time:29378ms step_avg:36.31ms
step:810/2160 train_time:29436ms step_avg:36.34ms
step:811/2160 train_time:29497ms step_avg:36.37ms
step:812/2160 train_time:29556ms step_avg:36.40ms
step:813/2160 train_time:29615ms step_avg:36.43ms
step:814/2160 train_time:29674ms step_avg:36.45ms
step:815/2160 train_time:29734ms step_avg:36.48ms
step:816/2160 train_time:29793ms step_avg:36.51ms
step:817/2160 train_time:29854ms step_avg:36.54ms
step:818/2160 train_time:29913ms step_avg:36.57ms
step:819/2160 train_time:29975ms step_avg:36.60ms
step:820/2160 train_time:30034ms step_avg:36.63ms
step:821/2160 train_time:30096ms step_avg:36.66ms
step:822/2160 train_time:30156ms step_avg:36.69ms
step:823/2160 train_time:30216ms step_avg:36.71ms
step:824/2160 train_time:30276ms step_avg:36.74ms
step:825/2160 train_time:30338ms step_avg:36.77ms
step:826/2160 train_time:30396ms step_avg:36.80ms
step:827/2160 train_time:30456ms step_avg:36.83ms
step:828/2160 train_time:30515ms step_avg:36.85ms
step:829/2160 train_time:30574ms step_avg:36.88ms
step:830/2160 train_time:30633ms step_avg:36.91ms
step:831/2160 train_time:30693ms step_avg:36.93ms
step:832/2160 train_time:30751ms step_avg:36.96ms
step:833/2160 train_time:30811ms step_avg:36.99ms
step:834/2160 train_time:30871ms step_avg:37.02ms
step:835/2160 train_time:30931ms step_avg:37.04ms
step:836/2160 train_time:30991ms step_avg:37.07ms
step:837/2160 train_time:31052ms step_avg:37.10ms
step:838/2160 train_time:31112ms step_avg:37.13ms
step:839/2160 train_time:31173ms step_avg:37.15ms
step:840/2160 train_time:31233ms step_avg:37.18ms
step:841/2160 train_time:31294ms step_avg:37.21ms
step:842/2160 train_time:31354ms step_avg:37.24ms
step:843/2160 train_time:31414ms step_avg:37.26ms
step:844/2160 train_time:31473ms step_avg:37.29ms
step:845/2160 train_time:31533ms step_avg:37.32ms
step:846/2160 train_time:31592ms step_avg:37.34ms
step:847/2160 train_time:31652ms step_avg:37.37ms
step:848/2160 train_time:31711ms step_avg:37.39ms
step:849/2160 train_time:31771ms step_avg:37.42ms
step:850/2160 train_time:31830ms step_avg:37.45ms
step:851/2160 train_time:31891ms step_avg:37.47ms
step:852/2160 train_time:31951ms step_avg:37.50ms
step:853/2160 train_time:32012ms step_avg:37.53ms
step:854/2160 train_time:32072ms step_avg:37.55ms
step:855/2160 train_time:32133ms step_avg:37.58ms
step:856/2160 train_time:32193ms step_avg:37.61ms
step:857/2160 train_time:32254ms step_avg:37.64ms
step:858/2160 train_time:32313ms step_avg:37.66ms
step:859/2160 train_time:32374ms step_avg:37.69ms
step:860/2160 train_time:32433ms step_avg:37.71ms
step:861/2160 train_time:32493ms step_avg:37.74ms
step:862/2160 train_time:32551ms step_avg:37.76ms
step:863/2160 train_time:32611ms step_avg:37.79ms
step:864/2160 train_time:32670ms step_avg:37.81ms
step:865/2160 train_time:32730ms step_avg:37.84ms
step:866/2160 train_time:32789ms step_avg:37.86ms
step:867/2160 train_time:32850ms step_avg:37.89ms
step:868/2160 train_time:32909ms step_avg:37.91ms
step:869/2160 train_time:32970ms step_avg:37.94ms
step:870/2160 train_time:33030ms step_avg:37.97ms
step:871/2160 train_time:33091ms step_avg:37.99ms
step:872/2160 train_time:33150ms step_avg:38.02ms
step:873/2160 train_time:33211ms step_avg:38.04ms
step:874/2160 train_time:33271ms step_avg:38.07ms
step:875/2160 train_time:33331ms step_avg:38.09ms
step:876/2160 train_time:33391ms step_avg:38.12ms
step:877/2160 train_time:33451ms step_avg:38.14ms
step:878/2160 train_time:33509ms step_avg:38.17ms
step:879/2160 train_time:33570ms step_avg:38.19ms
step:880/2160 train_time:33629ms step_avg:38.21ms
step:881/2160 train_time:33690ms step_avg:38.24ms
step:882/2160 train_time:33748ms step_avg:38.26ms
step:883/2160 train_time:33808ms step_avg:38.29ms
step:884/2160 train_time:33867ms step_avg:38.31ms
step:885/2160 train_time:33927ms step_avg:38.34ms
step:886/2160 train_time:33986ms step_avg:38.36ms
step:887/2160 train_time:34047ms step_avg:38.38ms
step:888/2160 train_time:34106ms step_avg:38.41ms
step:889/2160 train_time:34166ms step_avg:38.43ms
step:890/2160 train_time:34225ms step_avg:38.46ms
step:891/2160 train_time:34285ms step_avg:38.48ms
step:892/2160 train_time:34344ms step_avg:38.50ms
step:893/2160 train_time:34404ms step_avg:38.53ms
step:894/2160 train_time:34463ms step_avg:38.55ms
step:895/2160 train_time:34523ms step_avg:38.57ms
step:896/2160 train_time:34581ms step_avg:38.60ms
step:897/2160 train_time:34642ms step_avg:38.62ms
step:898/2160 train_time:34700ms step_avg:38.64ms
step:899/2160 train_time:34760ms step_avg:38.67ms
step:900/2160 train_time:34818ms step_avg:38.69ms
step:901/2160 train_time:34878ms step_avg:38.71ms
step:902/2160 train_time:34937ms step_avg:38.73ms
step:903/2160 train_time:34998ms step_avg:38.76ms
step:904/2160 train_time:35057ms step_avg:38.78ms
step:905/2160 train_time:35118ms step_avg:38.80ms
step:906/2160 train_time:35177ms step_avg:38.83ms
step:907/2160 train_time:35238ms step_avg:38.85ms
step:908/2160 train_time:35297ms step_avg:38.87ms
step:909/2160 train_time:35358ms step_avg:38.90ms
step:910/2160 train_time:35416ms step_avg:38.92ms
step:911/2160 train_time:35477ms step_avg:38.94ms
step:912/2160 train_time:35535ms step_avg:38.96ms
step:913/2160 train_time:35596ms step_avg:38.99ms
step:914/2160 train_time:35656ms step_avg:39.01ms
step:915/2160 train_time:35716ms step_avg:39.03ms
step:916/2160 train_time:35774ms step_avg:39.05ms
step:917/2160 train_time:35835ms step_avg:39.08ms
step:918/2160 train_time:35894ms step_avg:39.10ms
step:919/2160 train_time:35955ms step_avg:39.12ms
step:920/2160 train_time:36014ms step_avg:39.15ms
step:921/2160 train_time:36075ms step_avg:39.17ms
step:922/2160 train_time:36135ms step_avg:39.19ms
step:923/2160 train_time:36197ms step_avg:39.22ms
step:924/2160 train_time:36256ms step_avg:39.24ms
step:925/2160 train_time:36316ms step_avg:39.26ms
step:926/2160 train_time:36376ms step_avg:39.28ms
step:927/2160 train_time:36436ms step_avg:39.31ms
step:928/2160 train_time:36495ms step_avg:39.33ms
step:929/2160 train_time:36555ms step_avg:39.35ms
step:930/2160 train_time:36614ms step_avg:39.37ms
step:931/2160 train_time:36675ms step_avg:39.39ms
step:932/2160 train_time:36734ms step_avg:39.41ms
step:933/2160 train_time:36794ms step_avg:39.44ms
step:934/2160 train_time:36853ms step_avg:39.46ms
step:935/2160 train_time:36913ms step_avg:39.48ms
step:936/2160 train_time:36973ms step_avg:39.50ms
step:937/2160 train_time:37033ms step_avg:39.52ms
step:938/2160 train_time:37093ms step_avg:39.54ms
step:939/2160 train_time:37155ms step_avg:39.57ms
step:940/2160 train_time:37214ms step_avg:39.59ms
step:941/2160 train_time:37274ms step_avg:39.61ms
step:942/2160 train_time:37333ms step_avg:39.63ms
step:943/2160 train_time:37394ms step_avg:39.65ms
step:944/2160 train_time:37453ms step_avg:39.68ms
step:945/2160 train_time:37514ms step_avg:39.70ms
step:946/2160 train_time:37573ms step_avg:39.72ms
step:947/2160 train_time:37633ms step_avg:39.74ms
step:948/2160 train_time:37692ms step_avg:39.76ms
step:949/2160 train_time:37752ms step_avg:39.78ms
step:950/2160 train_time:37811ms step_avg:39.80ms
step:951/2160 train_time:37872ms step_avg:39.82ms
step:952/2160 train_time:37931ms step_avg:39.84ms
step:953/2160 train_time:37992ms step_avg:39.87ms
step:954/2160 train_time:38052ms step_avg:39.89ms
step:955/2160 train_time:38113ms step_avg:39.91ms
step:956/2160 train_time:38173ms step_avg:39.93ms
step:957/2160 train_time:38234ms step_avg:39.95ms
step:958/2160 train_time:38293ms step_avg:39.97ms
step:959/2160 train_time:38353ms step_avg:39.99ms
step:960/2160 train_time:38412ms step_avg:40.01ms
step:961/2160 train_time:38473ms step_avg:40.03ms
step:962/2160 train_time:38533ms step_avg:40.05ms
step:963/2160 train_time:38593ms step_avg:40.08ms
step:964/2160 train_time:38652ms step_avg:40.10ms
step:965/2160 train_time:38713ms step_avg:40.12ms
step:966/2160 train_time:38772ms step_avg:40.14ms
step:967/2160 train_time:38832ms step_avg:40.16ms
step:968/2160 train_time:38892ms step_avg:40.18ms
step:969/2160 train_time:38952ms step_avg:40.20ms
step:970/2160 train_time:39011ms step_avg:40.22ms
step:971/2160 train_time:39072ms step_avg:40.24ms
step:972/2160 train_time:39131ms step_avg:40.26ms
step:973/2160 train_time:39192ms step_avg:40.28ms
step:974/2160 train_time:39251ms step_avg:40.30ms
step:975/2160 train_time:39312ms step_avg:40.32ms
step:976/2160 train_time:39372ms step_avg:40.34ms
step:977/2160 train_time:39432ms step_avg:40.36ms
step:978/2160 train_time:39492ms step_avg:40.38ms
step:979/2160 train_time:39553ms step_avg:40.40ms
step:980/2160 train_time:39612ms step_avg:40.42ms
step:981/2160 train_time:39673ms step_avg:40.44ms
step:982/2160 train_time:39733ms step_avg:40.46ms
step:983/2160 train_time:39793ms step_avg:40.48ms
step:984/2160 train_time:39853ms step_avg:40.50ms
step:985/2160 train_time:39912ms step_avg:40.52ms
step:986/2160 train_time:39971ms step_avg:40.54ms
step:987/2160 train_time:40032ms step_avg:40.56ms
step:988/2160 train_time:40091ms step_avg:40.58ms
step:989/2160 train_time:40153ms step_avg:40.60ms
step:990/2160 train_time:40212ms step_avg:40.62ms
step:991/2160 train_time:40273ms step_avg:40.64ms
step:992/2160 train_time:40332ms step_avg:40.66ms
step:993/2160 train_time:40393ms step_avg:40.68ms
step:994/2160 train_time:40453ms step_avg:40.70ms
step:995/2160 train_time:40514ms step_avg:40.72ms
step:996/2160 train_time:40573ms step_avg:40.74ms
step:997/2160 train_time:40635ms step_avg:40.76ms
step:998/2160 train_time:40694ms step_avg:40.78ms
step:999/2160 train_time:40754ms step_avg:40.80ms
step:1000/2160 train_time:40813ms step_avg:40.81ms
step:1000/2160 val_loss:3.7108 train_time:40875ms step_avg:40.87ms
step:1001/2160 train_time:40897ms step_avg:40.86ms
step:1002/2160 train_time:40936ms step_avg:40.85ms
step:1003/2160 train_time:41000ms step_avg:40.88ms
step:1004/2160 train_time:41062ms step_avg:40.90ms
step:1005/2160 train_time:41122ms step_avg:40.92ms
step:1006/2160 train_time:41181ms step_avg:40.94ms
step:1007/2160 train_time:41241ms step_avg:40.95ms
step:1008/2160 train_time:41299ms step_avg:40.97ms
step:1009/2160 train_time:41359ms step_avg:40.99ms
step:1010/2160 train_time:41417ms step_avg:41.01ms
step:1011/2160 train_time:41478ms step_avg:41.03ms
step:1012/2160 train_time:41537ms step_avg:41.04ms
step:1013/2160 train_time:41596ms step_avg:41.06ms
step:1014/2160 train_time:41655ms step_avg:41.08ms
step:1015/2160 train_time:41715ms step_avg:41.10ms
step:1016/2160 train_time:41774ms step_avg:41.12ms
step:1017/2160 train_time:41836ms step_avg:41.14ms
step:1018/2160 train_time:41897ms step_avg:41.16ms
step:1019/2160 train_time:41960ms step_avg:41.18ms
step:1020/2160 train_time:42021ms step_avg:41.20ms
step:1021/2160 train_time:42082ms step_avg:41.22ms
step:1022/2160 train_time:42141ms step_avg:41.23ms
step:1023/2160 train_time:42201ms step_avg:41.25ms
step:1024/2160 train_time:42260ms step_avg:41.27ms
step:1025/2160 train_time:42320ms step_avg:41.29ms
step:1026/2160 train_time:42378ms step_avg:41.30ms
step:1027/2160 train_time:42439ms step_avg:41.32ms
step:1028/2160 train_time:42497ms step_avg:41.34ms
step:1029/2160 train_time:42557ms step_avg:41.36ms
step:1030/2160 train_time:42615ms step_avg:41.37ms
step:1031/2160 train_time:42675ms step_avg:41.39ms
step:1032/2160 train_time:42734ms step_avg:41.41ms
step:1033/2160 train_time:42795ms step_avg:41.43ms
step:1034/2160 train_time:42856ms step_avg:41.45ms
step:1035/2160 train_time:42918ms step_avg:41.47ms
step:1036/2160 train_time:42979ms step_avg:41.49ms
step:1037/2160 train_time:43040ms step_avg:41.50ms
step:1038/2160 train_time:43100ms step_avg:41.52ms
step:1039/2160 train_time:43160ms step_avg:41.54ms
step:1040/2160 train_time:43219ms step_avg:41.56ms
step:1041/2160 train_time:43279ms step_avg:41.57ms
step:1042/2160 train_time:43337ms step_avg:41.59ms
step:1043/2160 train_time:43398ms step_avg:41.61ms
step:1044/2160 train_time:43456ms step_avg:41.62ms
step:1045/2160 train_time:43517ms step_avg:41.64ms
step:1046/2160 train_time:43575ms step_avg:41.66ms
step:1047/2160 train_time:43636ms step_avg:41.68ms
step:1048/2160 train_time:43695ms step_avg:41.69ms
step:1049/2160 train_time:43755ms step_avg:41.71ms
step:1050/2160 train_time:43814ms step_avg:41.73ms
step:1051/2160 train_time:43876ms step_avg:41.75ms
step:1052/2160 train_time:43937ms step_avg:41.76ms
step:1053/2160 train_time:43998ms step_avg:41.78ms
step:1054/2160 train_time:44059ms step_avg:41.80ms
step:1055/2160 train_time:44120ms step_avg:41.82ms
step:1056/2160 train_time:44179ms step_avg:41.84ms
step:1057/2160 train_time:44240ms step_avg:41.85ms
step:1058/2160 train_time:44300ms step_avg:41.87ms
step:1059/2160 train_time:44359ms step_avg:41.89ms
step:1060/2160 train_time:44418ms step_avg:41.90ms
step:1061/2160 train_time:44478ms step_avg:41.92ms
step:1062/2160 train_time:44536ms step_avg:41.94ms
step:1063/2160 train_time:44597ms step_avg:41.95ms
step:1064/2160 train_time:44655ms step_avg:41.97ms
step:1065/2160 train_time:44715ms step_avg:41.99ms
step:1066/2160 train_time:44774ms step_avg:42.00ms
step:1067/2160 train_time:44835ms step_avg:42.02ms
step:1068/2160 train_time:44895ms step_avg:42.04ms
step:1069/2160 train_time:44956ms step_avg:42.05ms
step:1070/2160 train_time:45017ms step_avg:42.07ms
step:1071/2160 train_time:45080ms step_avg:42.09ms
step:1072/2160 train_time:45139ms step_avg:42.11ms
step:1073/2160 train_time:45199ms step_avg:42.12ms
step:1074/2160 train_time:45259ms step_avg:42.14ms
step:1075/2160 train_time:45319ms step_avg:42.16ms
step:1076/2160 train_time:45378ms step_avg:42.17ms
step:1077/2160 train_time:45438ms step_avg:42.19ms
step:1078/2160 train_time:45498ms step_avg:42.21ms
step:1079/2160 train_time:45557ms step_avg:42.22ms
step:1080/2160 train_time:45616ms step_avg:42.24ms
step:1081/2160 train_time:45677ms step_avg:42.25ms
step:1082/2160 train_time:45736ms step_avg:42.27ms
step:1083/2160 train_time:45796ms step_avg:42.29ms
step:1084/2160 train_time:45855ms step_avg:42.30ms
step:1085/2160 train_time:45916ms step_avg:42.32ms
step:1086/2160 train_time:45976ms step_avg:42.34ms
step:1087/2160 train_time:46038ms step_avg:42.35ms
step:1088/2160 train_time:46098ms step_avg:42.37ms
step:1089/2160 train_time:46158ms step_avg:42.39ms
step:1090/2160 train_time:46218ms step_avg:42.40ms
step:1091/2160 train_time:46279ms step_avg:42.42ms
step:1092/2160 train_time:46339ms step_avg:42.43ms
step:1093/2160 train_time:46398ms step_avg:42.45ms
step:1094/2160 train_time:46457ms step_avg:42.47ms
step:1095/2160 train_time:46518ms step_avg:42.48ms
step:1096/2160 train_time:46578ms step_avg:42.50ms
step:1097/2160 train_time:46638ms step_avg:42.51ms
step:1098/2160 train_time:46697ms step_avg:42.53ms
step:1099/2160 train_time:46757ms step_avg:42.55ms
step:1100/2160 train_time:46817ms step_avg:42.56ms
step:1101/2160 train_time:46877ms step_avg:42.58ms
step:1102/2160 train_time:46937ms step_avg:42.59ms
step:1103/2160 train_time:46998ms step_avg:42.61ms
step:1104/2160 train_time:47057ms step_avg:42.62ms
step:1105/2160 train_time:47118ms step_avg:42.64ms
step:1106/2160 train_time:47178ms step_avg:42.66ms
step:1107/2160 train_time:47239ms step_avg:42.67ms
step:1108/2160 train_time:47298ms step_avg:42.69ms
step:1109/2160 train_time:47358ms step_avg:42.70ms
step:1110/2160 train_time:47418ms step_avg:42.72ms
step:1111/2160 train_time:47478ms step_avg:42.73ms
step:1112/2160 train_time:47537ms step_avg:42.75ms
step:1113/2160 train_time:47598ms step_avg:42.77ms
step:1114/2160 train_time:47657ms step_avg:42.78ms
step:1115/2160 train_time:47717ms step_avg:42.80ms
step:1116/2160 train_time:47776ms step_avg:42.81ms
step:1117/2160 train_time:47837ms step_avg:42.83ms
step:1118/2160 train_time:47897ms step_avg:42.84ms
step:1119/2160 train_time:47957ms step_avg:42.86ms
step:1120/2160 train_time:48017ms step_avg:42.87ms
step:1121/2160 train_time:48078ms step_avg:42.89ms
step:1122/2160 train_time:48137ms step_avg:42.90ms
step:1123/2160 train_time:48198ms step_avg:42.92ms
step:1124/2160 train_time:48257ms step_avg:42.93ms
step:1125/2160 train_time:48317ms step_avg:42.95ms
step:1126/2160 train_time:48377ms step_avg:42.96ms
step:1127/2160 train_time:48438ms step_avg:42.98ms
step:1128/2160 train_time:48497ms step_avg:42.99ms
step:1129/2160 train_time:48557ms step_avg:43.01ms
step:1130/2160 train_time:48617ms step_avg:43.02ms
step:1131/2160 train_time:48677ms step_avg:43.04ms
step:1132/2160 train_time:48737ms step_avg:43.05ms
step:1133/2160 train_time:48798ms step_avg:43.07ms
step:1134/2160 train_time:48858ms step_avg:43.08ms
step:1135/2160 train_time:48918ms step_avg:43.10ms
step:1136/2160 train_time:48978ms step_avg:43.11ms
step:1137/2160 train_time:49039ms step_avg:43.13ms
step:1138/2160 train_time:49099ms step_avg:43.14ms
step:1139/2160 train_time:49158ms step_avg:43.16ms
step:1140/2160 train_time:49218ms step_avg:43.17ms
step:1141/2160 train_time:49279ms step_avg:43.19ms
step:1142/2160 train_time:49338ms step_avg:43.20ms
step:1143/2160 train_time:49399ms step_avg:43.22ms
step:1144/2160 train_time:49459ms step_avg:43.23ms
step:1145/2160 train_time:49518ms step_avg:43.25ms
step:1146/2160 train_time:49578ms step_avg:43.26ms
step:1147/2160 train_time:49638ms step_avg:43.28ms
step:1148/2160 train_time:49697ms step_avg:43.29ms
step:1149/2160 train_time:49757ms step_avg:43.30ms
step:1150/2160 train_time:49816ms step_avg:43.32ms
step:1151/2160 train_time:49877ms step_avg:43.33ms
step:1152/2160 train_time:49937ms step_avg:43.35ms
step:1153/2160 train_time:49998ms step_avg:43.36ms
step:1154/2160 train_time:50057ms step_avg:43.38ms
step:1155/2160 train_time:50117ms step_avg:43.39ms
step:1156/2160 train_time:50177ms step_avg:43.41ms
step:1157/2160 train_time:50238ms step_avg:43.42ms
step:1158/2160 train_time:50297ms step_avg:43.43ms
step:1159/2160 train_time:50357ms step_avg:43.45ms
step:1160/2160 train_time:50417ms step_avg:43.46ms
step:1161/2160 train_time:50477ms step_avg:43.48ms
step:1162/2160 train_time:50536ms step_avg:43.49ms
step:1163/2160 train_time:50596ms step_avg:43.51ms
step:1164/2160 train_time:50656ms step_avg:43.52ms
step:1165/2160 train_time:50717ms step_avg:43.53ms
step:1166/2160 train_time:50777ms step_avg:43.55ms
step:1167/2160 train_time:50839ms step_avg:43.56ms
step:1168/2160 train_time:50899ms step_avg:43.58ms
step:1169/2160 train_time:50959ms step_avg:43.59ms
step:1170/2160 train_time:51018ms step_avg:43.61ms
step:1171/2160 train_time:51079ms step_avg:43.62ms
step:1172/2160 train_time:51139ms step_avg:43.63ms
step:1173/2160 train_time:51199ms step_avg:43.65ms
step:1174/2160 train_time:51258ms step_avg:43.66ms
step:1175/2160 train_time:51319ms step_avg:43.68ms
step:1176/2160 train_time:51379ms step_avg:43.69ms
step:1177/2160 train_time:51439ms step_avg:43.70ms
step:1178/2160 train_time:51499ms step_avg:43.72ms
step:1179/2160 train_time:51559ms step_avg:43.73ms
step:1180/2160 train_time:51617ms step_avg:43.74ms
step:1181/2160 train_time:51678ms step_avg:43.76ms
step:1182/2160 train_time:51737ms step_avg:43.77ms
step:1183/2160 train_time:51798ms step_avg:43.78ms
step:1184/2160 train_time:51857ms step_avg:43.80ms
step:1185/2160 train_time:51917ms step_avg:43.81ms
step:1186/2160 train_time:51977ms step_avg:43.83ms
step:1187/2160 train_time:52038ms step_avg:43.84ms
step:1188/2160 train_time:52099ms step_avg:43.85ms
step:1189/2160 train_time:52159ms step_avg:43.87ms
step:1190/2160 train_time:52218ms step_avg:43.88ms
step:1191/2160 train_time:52280ms step_avg:43.90ms
step:1192/2160 train_time:52339ms step_avg:43.91ms
step:1193/2160 train_time:52399ms step_avg:43.92ms
step:1194/2160 train_time:52459ms step_avg:43.94ms
step:1195/2160 train_time:52519ms step_avg:43.95ms
step:1196/2160 train_time:52579ms step_avg:43.96ms
step:1197/2160 train_time:52640ms step_avg:43.98ms
step:1198/2160 train_time:52698ms step_avg:43.99ms
step:1199/2160 train_time:52759ms step_avg:44.00ms
step:1200/2160 train_time:52818ms step_avg:44.01ms
step:1201/2160 train_time:52879ms step_avg:44.03ms
step:1202/2160 train_time:52938ms step_avg:44.04ms
step:1203/2160 train_time:52999ms step_avg:44.06ms
step:1204/2160 train_time:53057ms step_avg:44.07ms
step:1205/2160 train_time:53118ms step_avg:44.08ms
step:1206/2160 train_time:53177ms step_avg:44.09ms
step:1207/2160 train_time:53239ms step_avg:44.11ms
step:1208/2160 train_time:53299ms step_avg:44.12ms
step:1209/2160 train_time:53360ms step_avg:44.14ms
step:1210/2160 train_time:53419ms step_avg:44.15ms
step:1211/2160 train_time:53480ms step_avg:44.16ms
step:1212/2160 train_time:53539ms step_avg:44.17ms
step:1213/2160 train_time:53599ms step_avg:44.19ms
step:1214/2160 train_time:53659ms step_avg:44.20ms
step:1215/2160 train_time:53719ms step_avg:44.21ms
step:1216/2160 train_time:53778ms step_avg:44.23ms
step:1217/2160 train_time:53839ms step_avg:44.24ms
step:1218/2160 train_time:53899ms step_avg:44.25ms
step:1219/2160 train_time:53958ms step_avg:44.26ms
step:1220/2160 train_time:54018ms step_avg:44.28ms
step:1221/2160 train_time:54079ms step_avg:44.29ms
step:1222/2160 train_time:54138ms step_avg:44.30ms
step:1223/2160 train_time:54199ms step_avg:44.32ms
step:1224/2160 train_time:54258ms step_avg:44.33ms
step:1225/2160 train_time:54319ms step_avg:44.34ms
step:1226/2160 train_time:54378ms step_avg:44.35ms
step:1227/2160 train_time:54439ms step_avg:44.37ms
step:1228/2160 train_time:54499ms step_avg:44.38ms
step:1229/2160 train_time:54559ms step_avg:44.39ms
step:1230/2160 train_time:54618ms step_avg:44.41ms
step:1231/2160 train_time:54679ms step_avg:44.42ms
step:1232/2160 train_time:54739ms step_avg:44.43ms
step:1233/2160 train_time:54799ms step_avg:44.44ms
step:1234/2160 train_time:54858ms step_avg:44.46ms
step:1235/2160 train_time:54919ms step_avg:44.47ms
step:1236/2160 train_time:54978ms step_avg:44.48ms
step:1237/2160 train_time:55039ms step_avg:44.49ms
step:1238/2160 train_time:55100ms step_avg:44.51ms
step:1239/2160 train_time:55159ms step_avg:44.52ms
step:1240/2160 train_time:55219ms step_avg:44.53ms
step:1241/2160 train_time:55279ms step_avg:44.54ms
step:1242/2160 train_time:55339ms step_avg:44.56ms
step:1243/2160 train_time:55400ms step_avg:44.57ms
step:1244/2160 train_time:55459ms step_avg:44.58ms
step:1245/2160 train_time:55519ms step_avg:44.59ms
step:1246/2160 train_time:55578ms step_avg:44.61ms
step:1247/2160 train_time:55639ms step_avg:44.62ms
step:1248/2160 train_time:55699ms step_avg:44.63ms
step:1249/2160 train_time:55759ms step_avg:44.64ms
step:1250/2160 train_time:55818ms step_avg:44.65ms
step:1250/2160 val_loss:3.5938 train_time:55881ms step_avg:44.71ms
step:1251/2160 train_time:55904ms step_avg:44.69ms
step:1252/2160 train_time:55941ms step_avg:44.68ms
step:1253/2160 train_time:56005ms step_avg:44.70ms
step:1254/2160 train_time:56065ms step_avg:44.71ms
step:1255/2160 train_time:56125ms step_avg:44.72ms
step:1256/2160 train_time:56185ms step_avg:44.73ms
step:1257/2160 train_time:56244ms step_avg:44.74ms
step:1258/2160 train_time:56303ms step_avg:44.76ms
step:1259/2160 train_time:56364ms step_avg:44.77ms
step:1260/2160 train_time:56423ms step_avg:44.78ms
step:1261/2160 train_time:56484ms step_avg:44.79ms
step:1262/2160 train_time:56544ms step_avg:44.80ms
step:1263/2160 train_time:56604ms step_avg:44.82ms
step:1264/2160 train_time:56664ms step_avg:44.83ms
step:1265/2160 train_time:56724ms step_avg:44.84ms
step:1266/2160 train_time:56785ms step_avg:44.85ms
step:1267/2160 train_time:56848ms step_avg:44.87ms
step:1268/2160 train_time:56910ms step_avg:44.88ms
step:1269/2160 train_time:56971ms step_avg:44.89ms
step:1270/2160 train_time:57030ms step_avg:44.91ms
step:1271/2160 train_time:57090ms step_avg:44.92ms
step:1272/2160 train_time:57149ms step_avg:44.93ms
step:1273/2160 train_time:57209ms step_avg:44.94ms
step:1274/2160 train_time:57267ms step_avg:44.95ms
step:1275/2160 train_time:57326ms step_avg:44.96ms
step:1276/2160 train_time:57385ms step_avg:44.97ms
step:1277/2160 train_time:57446ms step_avg:44.98ms
step:1278/2160 train_time:57504ms step_avg:45.00ms
step:1279/2160 train_time:57565ms step_avg:45.01ms
step:1280/2160 train_time:57624ms step_avg:45.02ms
step:1281/2160 train_time:57684ms step_avg:45.03ms
step:1282/2160 train_time:57743ms step_avg:45.04ms
step:1283/2160 train_time:57806ms step_avg:45.06ms
step:1284/2160 train_time:57867ms step_avg:45.07ms
step:1285/2160 train_time:57928ms step_avg:45.08ms
step:1286/2160 train_time:57988ms step_avg:45.09ms
step:1287/2160 train_time:58047ms step_avg:45.10ms
step:1288/2160 train_time:58106ms step_avg:45.11ms
step:1289/2160 train_time:58166ms step_avg:45.13ms
step:1290/2160 train_time:58226ms step_avg:45.14ms
step:1291/2160 train_time:58286ms step_avg:45.15ms
step:1292/2160 train_time:58345ms step_avg:45.16ms
step:1293/2160 train_time:58405ms step_avg:45.17ms
step:1294/2160 train_time:58465ms step_avg:45.18ms
step:1295/2160 train_time:58524ms step_avg:45.19ms
step:1296/2160 train_time:58583ms step_avg:45.20ms
step:1297/2160 train_time:58643ms step_avg:45.21ms
step:1298/2160 train_time:58702ms step_avg:45.23ms
step:1299/2160 train_time:58764ms step_avg:45.24ms
step:1300/2160 train_time:58825ms step_avg:45.25ms
step:1301/2160 train_time:58886ms step_avg:45.26ms
step:1302/2160 train_time:58945ms step_avg:45.27ms
step:1303/2160 train_time:59006ms step_avg:45.29ms
step:1304/2160 train_time:59066ms step_avg:45.30ms
step:1305/2160 train_time:59127ms step_avg:45.31ms
step:1306/2160 train_time:59186ms step_avg:45.32ms
step:1307/2160 train_time:59246ms step_avg:45.33ms
step:1308/2160 train_time:59305ms step_avg:45.34ms
step:1309/2160 train_time:59365ms step_avg:45.35ms
step:1310/2160 train_time:59424ms step_avg:45.36ms
step:1311/2160 train_time:59484ms step_avg:45.37ms
step:1312/2160 train_time:59543ms step_avg:45.38ms
step:1313/2160 train_time:59604ms step_avg:45.40ms
step:1314/2160 train_time:59663ms step_avg:45.41ms
step:1315/2160 train_time:59724ms step_avg:45.42ms
step:1316/2160 train_time:59783ms step_avg:45.43ms
step:1317/2160 train_time:59844ms step_avg:45.44ms
step:1318/2160 train_time:59904ms step_avg:45.45ms
step:1319/2160 train_time:59965ms step_avg:45.46ms
step:1320/2160 train_time:60025ms step_avg:45.47ms
step:1321/2160 train_time:60085ms step_avg:45.48ms
step:1322/2160 train_time:60145ms step_avg:45.50ms
step:1323/2160 train_time:60205ms step_avg:45.51ms
step:1324/2160 train_time:60265ms step_avg:45.52ms
step:1325/2160 train_time:60325ms step_avg:45.53ms
step:1326/2160 train_time:60383ms step_avg:45.54ms
step:1327/2160 train_time:60444ms step_avg:45.55ms
step:1328/2160 train_time:60502ms step_avg:45.56ms
step:1329/2160 train_time:60563ms step_avg:45.57ms
step:1330/2160 train_time:60623ms step_avg:45.58ms
step:1331/2160 train_time:60684ms step_avg:45.59ms
step:1332/2160 train_time:60743ms step_avg:45.60ms
step:1333/2160 train_time:60805ms step_avg:45.61ms
step:1334/2160 train_time:60864ms step_avg:45.63ms
step:1335/2160 train_time:60927ms step_avg:45.64ms
step:1336/2160 train_time:60986ms step_avg:45.65ms
step:1337/2160 train_time:61046ms step_avg:45.66ms
step:1338/2160 train_time:61105ms step_avg:45.67ms
step:1339/2160 train_time:61166ms step_avg:45.68ms
step:1340/2160 train_time:61225ms step_avg:45.69ms
step:1341/2160 train_time:61285ms step_avg:45.70ms
step:1342/2160 train_time:61344ms step_avg:45.71ms
step:1343/2160 train_time:61405ms step_avg:45.72ms
step:1344/2160 train_time:61464ms step_avg:45.73ms
step:1345/2160 train_time:61525ms step_avg:45.74ms
step:1346/2160 train_time:61584ms step_avg:45.75ms
step:1347/2160 train_time:61645ms step_avg:45.76ms
step:1348/2160 train_time:61705ms step_avg:45.78ms
step:1349/2160 train_time:61766ms step_avg:45.79ms
step:1350/2160 train_time:61825ms step_avg:45.80ms
step:1351/2160 train_time:61886ms step_avg:45.81ms
step:1352/2160 train_time:61945ms step_avg:45.82ms
step:1353/2160 train_time:62006ms step_avg:45.83ms
step:1354/2160 train_time:62066ms step_avg:45.84ms
step:1355/2160 train_time:62126ms step_avg:45.85ms
step:1356/2160 train_time:62186ms step_avg:45.86ms
step:1357/2160 train_time:62246ms step_avg:45.87ms
step:1358/2160 train_time:62305ms step_avg:45.88ms
step:1359/2160 train_time:62365ms step_avg:45.89ms
step:1360/2160 train_time:62424ms step_avg:45.90ms
step:1361/2160 train_time:62484ms step_avg:45.91ms
step:1362/2160 train_time:62543ms step_avg:45.92ms
step:1363/2160 train_time:62605ms step_avg:45.93ms
step:1364/2160 train_time:62665ms step_avg:45.94ms
step:1365/2160 train_time:62726ms step_avg:45.95ms
step:1366/2160 train_time:62786ms step_avg:45.96ms
step:1367/2160 train_time:62846ms step_avg:45.97ms
step:1368/2160 train_time:62906ms step_avg:45.98ms
step:1369/2160 train_time:62966ms step_avg:45.99ms
step:1370/2160 train_time:63026ms step_avg:46.00ms
step:1371/2160 train_time:63086ms step_avg:46.01ms
step:1372/2160 train_time:63145ms step_avg:46.02ms
step:1373/2160 train_time:63206ms step_avg:46.04ms
step:1374/2160 train_time:63266ms step_avg:46.05ms
step:1375/2160 train_time:63326ms step_avg:46.06ms
step:1376/2160 train_time:63384ms step_avg:46.06ms
step:1377/2160 train_time:63445ms step_avg:46.08ms
step:1378/2160 train_time:63505ms step_avg:46.08ms
step:1379/2160 train_time:63566ms step_avg:46.10ms
step:1380/2160 train_time:63625ms step_avg:46.11ms
step:1381/2160 train_time:63686ms step_avg:46.12ms
step:1382/2160 train_time:63745ms step_avg:46.13ms
step:1383/2160 train_time:63806ms step_avg:46.14ms
step:1384/2160 train_time:63866ms step_avg:46.15ms
step:1385/2160 train_time:63926ms step_avg:46.16ms
step:1386/2160 train_time:63986ms step_avg:46.17ms
step:1387/2160 train_time:64047ms step_avg:46.18ms
step:1388/2160 train_time:64106ms step_avg:46.19ms
step:1389/2160 train_time:64166ms step_avg:46.20ms
step:1390/2160 train_time:64227ms step_avg:46.21ms
step:1391/2160 train_time:64287ms step_avg:46.22ms
step:1392/2160 train_time:64346ms step_avg:46.23ms
step:1393/2160 train_time:64406ms step_avg:46.24ms
step:1394/2160 train_time:64465ms step_avg:46.24ms
step:1395/2160 train_time:64526ms step_avg:46.25ms
step:1396/2160 train_time:64584ms step_avg:46.26ms
step:1397/2160 train_time:64645ms step_avg:46.27ms
step:1398/2160 train_time:64705ms step_avg:46.28ms
step:1399/2160 train_time:64766ms step_avg:46.29ms
step:1400/2160 train_time:64825ms step_avg:46.30ms
step:1401/2160 train_time:64886ms step_avg:46.31ms
step:1402/2160 train_time:64946ms step_avg:46.32ms
step:1403/2160 train_time:65007ms step_avg:46.33ms
step:1404/2160 train_time:65066ms step_avg:46.34ms
step:1405/2160 train_time:65126ms step_avg:46.35ms
step:1406/2160 train_time:65185ms step_avg:46.36ms
step:1407/2160 train_time:65246ms step_avg:46.37ms
step:1408/2160 train_time:65306ms step_avg:46.38ms
step:1409/2160 train_time:65366ms step_avg:46.39ms
step:1410/2160 train_time:65426ms step_avg:46.40ms
step:1411/2160 train_time:65486ms step_avg:46.41ms
step:1412/2160 train_time:65545ms step_avg:46.42ms
step:1413/2160 train_time:65606ms step_avg:46.43ms
step:1414/2160 train_time:65666ms step_avg:46.44ms
step:1415/2160 train_time:65727ms step_avg:46.45ms
step:1416/2160 train_time:65814ms step_avg:46.48ms
step:1417/2160 train_time:65902ms step_avg:46.51ms
step:1418/2160 train_time:65989ms step_avg:46.54ms
step:1419/2160 train_time:66078ms step_avg:46.57ms
step:1420/2160 train_time:66165ms step_avg:46.59ms
step:1421/2160 train_time:66254ms step_avg:46.62ms
step:1422/2160 train_time:66340ms step_avg:46.65ms
step:1423/2160 train_time:66428ms step_avg:46.68ms
step:1424/2160 train_time:66514ms step_avg:46.71ms
step:1425/2160 train_time:66603ms step_avg:46.74ms
step:1426/2160 train_time:66690ms step_avg:46.77ms
step:1427/2160 train_time:66779ms step_avg:46.80ms
step:1428/2160 train_time:66865ms step_avg:46.82ms
step:1429/2160 train_time:66954ms step_avg:46.85ms
step:1430/2160 train_time:67040ms step_avg:46.88ms
step:1431/2160 train_time:67129ms step_avg:46.91ms
step:1432/2160 train_time:67217ms step_avg:46.94ms
step:1433/2160 train_time:67306ms step_avg:46.97ms
step:1434/2160 train_time:67393ms step_avg:47.00ms
step:1435/2160 train_time:67481ms step_avg:47.03ms
step:1436/2160 train_time:67568ms step_avg:47.05ms
step:1437/2160 train_time:67657ms step_avg:47.08ms
step:1438/2160 train_time:67742ms step_avg:47.11ms
step:1439/2160 train_time:67831ms step_avg:47.14ms
step:1440/2160 train_time:67918ms step_avg:47.17ms
step:1441/2160 train_time:68006ms step_avg:47.19ms
step:1442/2160 train_time:68093ms step_avg:47.22ms
step:1443/2160 train_time:68182ms step_avg:47.25ms
step:1444/2160 train_time:68269ms step_avg:47.28ms
step:1445/2160 train_time:68359ms step_avg:47.31ms
step:1446/2160 train_time:68445ms step_avg:47.33ms
step:1447/2160 train_time:68534ms step_avg:47.36ms
step:1448/2160 train_time:68620ms step_avg:47.39ms
step:1449/2160 train_time:68708ms step_avg:47.42ms
step:1450/2160 train_time:68796ms step_avg:47.45ms
step:1451/2160 train_time:68885ms step_avg:47.47ms
step:1452/2160 train_time:68972ms step_avg:47.50ms
step:1453/2160 train_time:69061ms step_avg:47.53ms
step:1454/2160 train_time:69148ms step_avg:47.56ms
step:1455/2160 train_time:69237ms step_avg:47.59ms
step:1456/2160 train_time:69323ms step_avg:47.61ms
step:1457/2160 train_time:69413ms step_avg:47.64ms
step:1458/2160 train_time:69500ms step_avg:47.67ms
step:1459/2160 train_time:69587ms step_avg:47.70ms
step:1460/2160 train_time:69674ms step_avg:47.72ms
step:1461/2160 train_time:69762ms step_avg:47.75ms
step:1462/2160 train_time:69849ms step_avg:47.78ms
step:1463/2160 train_time:69939ms step_avg:47.80ms
step:1464/2160 train_time:70025ms step_avg:47.83ms
step:1465/2160 train_time:70114ms step_avg:47.86ms
step:1466/2160 train_time:70201ms step_avg:47.89ms
step:1467/2160 train_time:70288ms step_avg:47.91ms
step:1468/2160 train_time:70376ms step_avg:47.94ms
step:1469/2160 train_time:70464ms step_avg:47.97ms
step:1470/2160 train_time:70551ms step_avg:47.99ms
step:1471/2160 train_time:70639ms step_avg:48.02ms
step:1472/2160 train_time:70727ms step_avg:48.05ms
step:1473/2160 train_time:70816ms step_avg:48.08ms
step:1474/2160 train_time:70903ms step_avg:48.10ms
step:1475/2160 train_time:70992ms step_avg:48.13ms
step:1476/2160 train_time:71079ms step_avg:48.16ms
step:1477/2160 train_time:71167ms step_avg:48.18ms
step:1478/2160 train_time:71255ms step_avg:48.21ms
step:1479/2160 train_time:71343ms step_avg:48.24ms
step:1480/2160 train_time:71429ms step_avg:48.26ms
step:1481/2160 train_time:71518ms step_avg:48.29ms
step:1482/2160 train_time:71604ms step_avg:48.32ms
step:1483/2160 train_time:71692ms step_avg:48.34ms
step:1484/2160 train_time:71779ms step_avg:48.37ms
step:1485/2160 train_time:71867ms step_avg:48.40ms
step:1486/2160 train_time:71954ms step_avg:48.42ms
step:1487/2160 train_time:72042ms step_avg:48.45ms
step:1488/2160 train_time:72129ms step_avg:48.47ms
step:1489/2160 train_time:72217ms step_avg:48.50ms
step:1490/2160 train_time:72303ms step_avg:48.53ms
step:1491/2160 train_time:72392ms step_avg:48.55ms
step:1492/2160 train_time:72480ms step_avg:48.58ms
step:1493/2160 train_time:72567ms step_avg:48.60ms
step:1494/2160 train_time:72654ms step_avg:48.63ms
step:1495/2160 train_time:72741ms step_avg:48.66ms
step:1496/2160 train_time:72829ms step_avg:48.68ms
step:1497/2160 train_time:72917ms step_avg:48.71ms
step:1498/2160 train_time:73004ms step_avg:48.73ms
step:1499/2160 train_time:73092ms step_avg:48.76ms
step:1500/2160 train_time:73179ms step_avg:48.79ms
step:1500/2160 val_loss:3.4903 train_time:73269ms step_avg:48.85ms
step:1501/2160 train_time:73292ms step_avg:48.83ms
step:1502/2160 train_time:73360ms step_avg:48.84ms
step:1503/2160 train_time:73451ms step_avg:48.87ms
step:1504/2160 train_time:73539ms step_avg:48.90ms
step:1505/2160 train_time:73626ms step_avg:48.92ms
step:1506/2160 train_time:73711ms step_avg:48.95ms
step:1507/2160 train_time:73800ms step_avg:48.97ms
step:1508/2160 train_time:73886ms step_avg:49.00ms
step:1509/2160 train_time:73973ms step_avg:49.02ms
step:1510/2160 train_time:74060ms step_avg:49.05ms
step:1511/2160 train_time:74147ms step_avg:49.07ms
step:1512/2160 train_time:74235ms step_avg:49.10ms
step:1513/2160 train_time:74327ms step_avg:49.13ms
step:1514/2160 train_time:74416ms step_avg:49.15ms
step:1515/2160 train_time:74506ms step_avg:49.18ms
step:1516/2160 train_time:74593ms step_avg:49.20ms
step:1517/2160 train_time:74680ms step_avg:49.23ms
step:1518/2160 train_time:74766ms step_avg:49.25ms
step:1519/2160 train_time:74853ms step_avg:49.28ms
step:1520/2160 train_time:74939ms step_avg:49.30ms
step:1521/2160 train_time:75026ms step_avg:49.33ms
step:1522/2160 train_time:75112ms step_avg:49.35ms
step:1523/2160 train_time:75203ms step_avg:49.38ms
step:1524/2160 train_time:75291ms step_avg:49.40ms
step:1525/2160 train_time:75383ms step_avg:49.43ms
step:1526/2160 train_time:75471ms step_avg:49.46ms
step:1527/2160 train_time:75559ms step_avg:49.48ms
step:1528/2160 train_time:75645ms step_avg:49.51ms
step:1529/2160 train_time:75733ms step_avg:49.53ms
step:1530/2160 train_time:75819ms step_avg:49.55ms
step:1531/2160 train_time:75906ms step_avg:49.58ms
step:1532/2160 train_time:75992ms step_avg:49.60ms
step:1533/2160 train_time:76081ms step_avg:49.63ms
step:1534/2160 train_time:76168ms step_avg:49.65ms
step:1535/2160 train_time:76255ms step_avg:49.68ms
step:1536/2160 train_time:76344ms step_avg:49.70ms
step:1537/2160 train_time:76434ms step_avg:49.73ms
step:1538/2160 train_time:76521ms step_avg:49.75ms
step:1539/2160 train_time:76609ms step_avg:49.78ms
step:1540/2160 train_time:76696ms step_avg:49.80ms
step:1541/2160 train_time:76783ms step_avg:49.83ms
step:1542/2160 train_time:76869ms step_avg:49.85ms
step:1543/2160 train_time:76958ms step_avg:49.88ms
step:1544/2160 train_time:77044ms step_avg:49.90ms
step:1545/2160 train_time:77131ms step_avg:49.92ms
step:1546/2160 train_time:77220ms step_avg:49.95ms
step:1547/2160 train_time:77309ms step_avg:49.97ms
step:1548/2160 train_time:77396ms step_avg:50.00ms
step:1549/2160 train_time:77486ms step_avg:50.02ms
step:1550/2160 train_time:77573ms step_avg:50.05ms
step:1551/2160 train_time:77661ms step_avg:50.07ms
step:1552/2160 train_time:77747ms step_avg:50.09ms
step:1553/2160 train_time:77835ms step_avg:50.12ms
step:1554/2160 train_time:77922ms step_avg:50.14ms
step:1555/2160 train_time:78009ms step_avg:50.17ms
step:1556/2160 train_time:78096ms step_avg:50.19ms
step:1557/2160 train_time:78186ms step_avg:50.22ms
step:1558/2160 train_time:78274ms step_avg:50.24ms
step:1559/2160 train_time:78365ms step_avg:50.27ms
step:1560/2160 train_time:78453ms step_avg:50.29ms
step:1561/2160 train_time:78542ms step_avg:50.31ms
step:1562/2160 train_time:78627ms step_avg:50.34ms
step:1563/2160 train_time:78715ms step_avg:50.36ms
step:1564/2160 train_time:78802ms step_avg:50.38ms
step:1565/2160 train_time:78889ms step_avg:50.41ms
step:1566/2160 train_time:78975ms step_avg:50.43ms
step:1567/2160 train_time:79064ms step_avg:50.46ms
step:1568/2160 train_time:79151ms step_avg:50.48ms
step:1569/2160 train_time:79240ms step_avg:50.50ms
step:1570/2160 train_time:79327ms step_avg:50.53ms
step:1571/2160 train_time:79417ms step_avg:50.55ms
step:1572/2160 train_time:79504ms step_avg:50.58ms
step:1573/2160 train_time:79593ms step_avg:50.60ms
step:1574/2160 train_time:79679ms step_avg:50.62ms
step:1575/2160 train_time:79767ms step_avg:50.65ms
step:1576/2160 train_time:79853ms step_avg:50.67ms
step:1577/2160 train_time:79941ms step_avg:50.69ms
step:1578/2160 train_time:80027ms step_avg:50.71ms
step:1579/2160 train_time:80114ms step_avg:50.74ms
step:1580/2160 train_time:80201ms step_avg:50.76ms
step:1581/2160 train_time:80290ms step_avg:50.78ms
step:1582/2160 train_time:80377ms step_avg:50.81ms
step:1583/2160 train_time:80467ms step_avg:50.83ms
step:1584/2160 train_time:80554ms step_avg:50.85ms
step:1585/2160 train_time:80644ms step_avg:50.88ms
step:1586/2160 train_time:80730ms step_avg:50.90ms
step:1587/2160 train_time:80819ms step_avg:50.93ms
step:1588/2160 train_time:80906ms step_avg:50.95ms
step:1589/2160 train_time:80994ms step_avg:50.97ms
step:1590/2160 train_time:81081ms step_avg:50.99ms
step:1591/2160 train_time:81170ms step_avg:51.02ms
step:1592/2160 train_time:81257ms step_avg:51.04ms
step:1593/2160 train_time:81346ms step_avg:51.06ms
step:1594/2160 train_time:81433ms step_avg:51.09ms
step:1595/2160 train_time:81522ms step_avg:51.11ms
step:1596/2160 train_time:81608ms step_avg:51.13ms
step:1597/2160 train_time:81697ms step_avg:51.16ms
step:1598/2160 train_time:81784ms step_avg:51.18ms
step:1599/2160 train_time:81873ms step_avg:51.20ms
step:1600/2160 train_time:81959ms step_avg:51.22ms
step:1601/2160 train_time:82047ms step_avg:51.25ms
step:1602/2160 train_time:82134ms step_avg:51.27ms
step:1603/2160 train_time:82222ms step_avg:51.29ms
step:1604/2160 train_time:82308ms step_avg:51.31ms
step:1605/2160 train_time:82398ms step_avg:51.34ms
step:1606/2160 train_time:82485ms step_avg:51.36ms
step:1607/2160 train_time:82573ms step_avg:51.38ms
step:1608/2160 train_time:82660ms step_avg:51.41ms
step:1609/2160 train_time:82749ms step_avg:51.43ms
step:1610/2160 train_time:82835ms step_avg:51.45ms
step:1611/2160 train_time:82924ms step_avg:51.47ms
step:1612/2160 train_time:83010ms step_avg:51.50ms
step:1613/2160 train_time:83099ms step_avg:51.52ms
step:1614/2160 train_time:83186ms step_avg:51.54ms
step:1615/2160 train_time:83273ms step_avg:51.56ms
step:1616/2160 train_time:83361ms step_avg:51.58ms
step:1617/2160 train_time:83449ms step_avg:51.61ms
step:1618/2160 train_time:83537ms step_avg:51.63ms
step:1619/2160 train_time:83626ms step_avg:51.65ms
step:1620/2160 train_time:83712ms step_avg:51.67ms
step:1621/2160 train_time:83801ms step_avg:51.70ms
step:1622/2160 train_time:83887ms step_avg:51.72ms
step:1623/2160 train_time:83975ms step_avg:51.74ms
step:1624/2160 train_time:84063ms step_avg:51.76ms
step:1625/2160 train_time:84152ms step_avg:51.79ms
step:1626/2160 train_time:84239ms step_avg:51.81ms
step:1627/2160 train_time:84327ms step_avg:51.83ms
step:1628/2160 train_time:84414ms step_avg:51.85ms
step:1629/2160 train_time:84503ms step_avg:51.87ms
step:1630/2160 train_time:84589ms step_avg:51.89ms
step:1631/2160 train_time:84677ms step_avg:51.92ms
step:1632/2160 train_time:84764ms step_avg:51.94ms
step:1633/2160 train_time:84851ms step_avg:51.96ms
step:1634/2160 train_time:84938ms step_avg:51.98ms
step:1635/2160 train_time:85027ms step_avg:52.00ms
step:1636/2160 train_time:85113ms step_avg:52.03ms
step:1637/2160 train_time:85203ms step_avg:52.05ms
step:1638/2160 train_time:85289ms step_avg:52.07ms
step:1639/2160 train_time:85378ms step_avg:52.09ms
step:1640/2160 train_time:85465ms step_avg:52.11ms
step:1641/2160 train_time:85554ms step_avg:52.14ms
step:1642/2160 train_time:85640ms step_avg:52.16ms
step:1643/2160 train_time:85727ms step_avg:52.18ms
step:1644/2160 train_time:85815ms step_avg:52.20ms
step:1645/2160 train_time:85905ms step_avg:52.22ms
step:1646/2160 train_time:85992ms step_avg:52.24ms
step:1647/2160 train_time:86081ms step_avg:52.27ms
step:1648/2160 train_time:86167ms step_avg:52.29ms
step:1649/2160 train_time:86255ms step_avg:52.31ms
step:1650/2160 train_time:86342ms step_avg:52.33ms
step:1651/2160 train_time:86431ms step_avg:52.35ms
step:1652/2160 train_time:86518ms step_avg:52.37ms
step:1653/2160 train_time:86606ms step_avg:52.39ms
step:1654/2160 train_time:86693ms step_avg:52.41ms
step:1655/2160 train_time:86782ms step_avg:52.44ms
step:1656/2160 train_time:86869ms step_avg:52.46ms
step:1657/2160 train_time:86958ms step_avg:52.48ms
step:1658/2160 train_time:87044ms step_avg:52.50ms
step:1659/2160 train_time:87132ms step_avg:52.52ms
step:1660/2160 train_time:87219ms step_avg:52.54ms
step:1661/2160 train_time:87308ms step_avg:52.56ms
step:1662/2160 train_time:87395ms step_avg:52.58ms
step:1663/2160 train_time:87483ms step_avg:52.61ms
step:1664/2160 train_time:87568ms step_avg:52.63ms
step:1665/2160 train_time:87657ms step_avg:52.65ms
step:1666/2160 train_time:87744ms step_avg:52.67ms
step:1667/2160 train_time:87832ms step_avg:52.69ms
step:1668/2160 train_time:87920ms step_avg:52.71ms
step:1669/2160 train_time:88008ms step_avg:52.73ms
step:1670/2160 train_time:88094ms step_avg:52.75ms
step:1671/2160 train_time:88185ms step_avg:52.77ms
step:1672/2160 train_time:88272ms step_avg:52.79ms
step:1673/2160 train_time:88360ms step_avg:52.82ms
step:1674/2160 train_time:88446ms step_avg:52.84ms
step:1675/2160 train_time:88533ms step_avg:52.86ms
step:1676/2160 train_time:88620ms step_avg:52.88ms
step:1677/2160 train_time:88708ms step_avg:52.90ms
step:1678/2160 train_time:88796ms step_avg:52.92ms
step:1679/2160 train_time:88885ms step_avg:52.94ms
step:1680/2160 train_time:88972ms step_avg:52.96ms
step:1681/2160 train_time:89061ms step_avg:52.98ms
step:1682/2160 train_time:89147ms step_avg:53.00ms
step:1683/2160 train_time:89237ms step_avg:53.02ms
step:1684/2160 train_time:89325ms step_avg:53.04ms
step:1685/2160 train_time:89413ms step_avg:53.06ms
step:1686/2160 train_time:89501ms step_avg:53.08ms
step:1687/2160 train_time:89589ms step_avg:53.11ms
step:1688/2160 train_time:89676ms step_avg:53.13ms
step:1689/2160 train_time:89766ms step_avg:53.15ms
step:1690/2160 train_time:89852ms step_avg:53.17ms
step:1691/2160 train_time:89940ms step_avg:53.19ms
step:1692/2160 train_time:90028ms step_avg:53.21ms
step:1693/2160 train_time:90116ms step_avg:53.23ms
step:1694/2160 train_time:90203ms step_avg:53.25ms
step:1695/2160 train_time:90292ms step_avg:53.27ms
step:1696/2160 train_time:90379ms step_avg:53.29ms
step:1697/2160 train_time:90468ms step_avg:53.31ms
step:1698/2160 train_time:90554ms step_avg:53.33ms
step:1699/2160 train_time:90644ms step_avg:53.35ms
step:1700/2160 train_time:90730ms step_avg:53.37ms
step:1701/2160 train_time:90818ms step_avg:53.39ms
step:1702/2160 train_time:90906ms step_avg:53.41ms
step:1703/2160 train_time:90995ms step_avg:53.43ms
step:1704/2160 train_time:91083ms step_avg:53.45ms
step:1705/2160 train_time:91171ms step_avg:53.47ms
step:1706/2160 train_time:91257ms step_avg:53.49ms
step:1707/2160 train_time:91346ms step_avg:53.51ms
step:1708/2160 train_time:91433ms step_avg:53.53ms
step:1709/2160 train_time:91522ms step_avg:53.55ms
step:1710/2160 train_time:91607ms step_avg:53.57ms
step:1711/2160 train_time:91697ms step_avg:53.59ms
step:1712/2160 train_time:91784ms step_avg:53.61ms
step:1713/2160 train_time:91872ms step_avg:53.63ms
step:1714/2160 train_time:91960ms step_avg:53.65ms
step:1715/2160 train_time:92048ms step_avg:53.67ms
step:1716/2160 train_time:92135ms step_avg:53.69ms
step:1717/2160 train_time:92224ms step_avg:53.71ms
step:1718/2160 train_time:92310ms step_avg:53.73ms
step:1719/2160 train_time:92399ms step_avg:53.75ms
step:1720/2160 train_time:92486ms step_avg:53.77ms
step:1721/2160 train_time:92574ms step_avg:53.79ms
step:1722/2160 train_time:92661ms step_avg:53.81ms
step:1723/2160 train_time:92749ms step_avg:53.83ms
step:1724/2160 train_time:92837ms step_avg:53.85ms
step:1725/2160 train_time:92926ms step_avg:53.87ms
step:1726/2160 train_time:93012ms step_avg:53.89ms
step:1727/2160 train_time:93102ms step_avg:53.91ms
step:1728/2160 train_time:93188ms step_avg:53.93ms
step:1729/2160 train_time:93276ms step_avg:53.95ms
step:1730/2160 train_time:93363ms step_avg:53.97ms
step:1731/2160 train_time:93452ms step_avg:53.99ms
step:1732/2160 train_time:93539ms step_avg:54.01ms
step:1733/2160 train_time:93628ms step_avg:54.03ms
step:1734/2160 train_time:93715ms step_avg:54.05ms
step:1735/2160 train_time:93804ms step_avg:54.07ms
step:1736/2160 train_time:93891ms step_avg:54.08ms
step:1737/2160 train_time:93980ms step_avg:54.10ms
step:1738/2160 train_time:94066ms step_avg:54.12ms
step:1739/2160 train_time:94155ms step_avg:54.14ms
step:1740/2160 train_time:94242ms step_avg:54.16ms
step:1741/2160 train_time:94330ms step_avg:54.18ms
step:1742/2160 train_time:94417ms step_avg:54.20ms
step:1743/2160 train_time:94506ms step_avg:54.22ms
step:1744/2160 train_time:94593ms step_avg:54.24ms
step:1745/2160 train_time:94682ms step_avg:54.26ms
step:1746/2160 train_time:94769ms step_avg:54.28ms
step:1747/2160 train_time:94859ms step_avg:54.30ms
step:1748/2160 train_time:94946ms step_avg:54.32ms
step:1749/2160 train_time:95034ms step_avg:54.34ms
step:1750/2160 train_time:95121ms step_avg:54.35ms
step:1750/2160 val_loss:3.3927 train_time:95210ms step_avg:54.41ms
step:1751/2160 train_time:95233ms step_avg:54.39ms
step:1752/2160 train_time:95299ms step_avg:54.39ms
step:1753/2160 train_time:95393ms step_avg:54.42ms
step:1754/2160 train_time:95479ms step_avg:54.43ms
step:1755/2160 train_time:95567ms step_avg:54.45ms
step:1756/2160 train_time:95654ms step_avg:54.47ms
step:1757/2160 train_time:95740ms step_avg:54.49ms
step:1758/2160 train_time:95826ms step_avg:54.51ms
step:1759/2160 train_time:95913ms step_avg:54.53ms
step:1760/2160 train_time:96000ms step_avg:54.55ms
step:1761/2160 train_time:96088ms step_avg:54.56ms
step:1762/2160 train_time:96176ms step_avg:54.58ms
step:1763/2160 train_time:96267ms step_avg:54.60ms
step:1764/2160 train_time:96356ms step_avg:54.62ms
step:1765/2160 train_time:96445ms step_avg:54.64ms
step:1766/2160 train_time:96531ms step_avg:54.66ms
step:1767/2160 train_time:96619ms step_avg:54.68ms
step:1768/2160 train_time:96706ms step_avg:54.70ms
step:1769/2160 train_time:96795ms step_avg:54.72ms
step:1770/2160 train_time:96880ms step_avg:54.73ms
step:1771/2160 train_time:96969ms step_avg:54.75ms
step:1772/2160 train_time:97054ms step_avg:54.77ms
step:1773/2160 train_time:97144ms step_avg:54.79ms
step:1774/2160 train_time:97234ms step_avg:54.81ms
step:1775/2160 train_time:97324ms step_avg:54.83ms
step:1776/2160 train_time:97412ms step_avg:54.85ms
step:1777/2160 train_time:97501ms step_avg:54.87ms
step:1778/2160 train_time:97588ms step_avg:54.89ms
step:1779/2160 train_time:97675ms step_avg:54.90ms
step:1780/2160 train_time:97761ms step_avg:54.92ms
step:1781/2160 train_time:97849ms step_avg:54.94ms
step:1782/2160 train_time:97935ms step_avg:54.96ms
step:1783/2160 train_time:98023ms step_avg:54.98ms
step:1784/2160 train_time:98109ms step_avg:54.99ms
step:1785/2160 train_time:98199ms step_avg:55.01ms
step:1786/2160 train_time:98287ms step_avg:55.03ms
step:1787/2160 train_time:98378ms step_avg:55.05ms
step:1788/2160 train_time:98465ms step_avg:55.07ms
step:1789/2160 train_time:98553ms step_avg:55.09ms
step:1790/2160 train_time:98639ms step_avg:55.11ms
step:1791/2160 train_time:98727ms step_avg:55.12ms
step:1792/2160 train_time:98814ms step_avg:55.14ms
step:1793/2160 train_time:98901ms step_avg:55.16ms
step:1794/2160 train_time:98988ms step_avg:55.18ms
step:1795/2160 train_time:99076ms step_avg:55.20ms
step:1796/2160 train_time:99162ms step_avg:55.21ms
step:1797/2160 train_time:99251ms step_avg:55.23ms
step:1798/2160 train_time:99338ms step_avg:55.25ms
step:1799/2160 train_time:99428ms step_avg:55.27ms
step:1800/2160 train_time:99515ms step_avg:55.29ms
step:1801/2160 train_time:99602ms step_avg:55.30ms
step:1802/2160 train_time:99688ms step_avg:55.32ms
step:1803/2160 train_time:99776ms step_avg:55.34ms
step:1804/2160 train_time:99863ms step_avg:55.36ms
step:1805/2160 train_time:99952ms step_avg:55.38ms
step:1806/2160 train_time:100038ms step_avg:55.39ms
step:1807/2160 train_time:100126ms step_avg:55.41ms
step:1808/2160 train_time:100214ms step_avg:55.43ms
step:1809/2160 train_time:100304ms step_avg:55.45ms
step:1810/2160 train_time:100391ms step_avg:55.46ms
step:1811/2160 train_time:100479ms step_avg:55.48ms
step:1812/2160 train_time:100566ms step_avg:55.50ms
step:1813/2160 train_time:100654ms step_avg:55.52ms
step:1814/2160 train_time:100739ms step_avg:55.53ms
step:1815/2160 train_time:100828ms step_avg:55.55ms
step:1816/2160 train_time:100914ms step_avg:55.57ms
step:1817/2160 train_time:101002ms step_avg:55.59ms
step:1818/2160 train_time:101089ms step_avg:55.60ms
step:1819/2160 train_time:101177ms step_avg:55.62ms
step:1820/2160 train_time:101264ms step_avg:55.64ms
step:1821/2160 train_time:101354ms step_avg:55.66ms
step:1822/2160 train_time:101440ms step_avg:55.68ms
step:1823/2160 train_time:101529ms step_avg:55.69ms
step:1824/2160 train_time:101616ms step_avg:55.71ms
step:1825/2160 train_time:101703ms step_avg:55.73ms
step:1826/2160 train_time:101790ms step_avg:55.74ms
step:1827/2160 train_time:101878ms step_avg:55.76ms
step:1828/2160 train_time:101965ms step_avg:55.78ms
step:1829/2160 train_time:102053ms step_avg:55.80ms
step:1830/2160 train_time:102139ms step_avg:55.81ms
step:1831/2160 train_time:102228ms step_avg:55.83ms
step:1832/2160 train_time:102316ms step_avg:55.85ms
step:1833/2160 train_time:102405ms step_avg:55.87ms
step:1834/2160 train_time:102493ms step_avg:55.88ms
step:1835/2160 train_time:102581ms step_avg:55.90ms
step:1836/2160 train_time:102668ms step_avg:55.92ms
step:1837/2160 train_time:102756ms step_avg:55.94ms
step:1838/2160 train_time:102842ms step_avg:55.95ms
step:1839/2160 train_time:102931ms step_avg:55.97ms
step:1840/2160 train_time:103016ms step_avg:55.99ms
step:1841/2160 train_time:103104ms step_avg:56.00ms
step:1842/2160 train_time:103193ms step_avg:56.02ms
step:1843/2160 train_time:103282ms step_avg:56.04ms
step:1844/2160 train_time:103369ms step_avg:56.06ms
step:1845/2160 train_time:103458ms step_avg:56.07ms
step:1846/2160 train_time:103545ms step_avg:56.09ms
step:1847/2160 train_time:103635ms step_avg:56.11ms
step:1848/2160 train_time:103722ms step_avg:56.13ms
step:1849/2160 train_time:103810ms step_avg:56.14ms
step:1850/2160 train_time:103896ms step_avg:56.16ms
step:1851/2160 train_time:103984ms step_avg:56.18ms
step:1852/2160 train_time:104071ms step_avg:56.19ms
step:1853/2160 train_time:104160ms step_avg:56.21ms
step:1854/2160 train_time:104247ms step_avg:56.23ms
step:1855/2160 train_time:104337ms step_avg:56.25ms
step:1856/2160 train_time:104424ms step_avg:56.26ms
step:1857/2160 train_time:104514ms step_avg:56.28ms
step:1858/2160 train_time:104601ms step_avg:56.30ms
step:1859/2160 train_time:104689ms step_avg:56.31ms
step:1860/2160 train_time:104775ms step_avg:56.33ms
step:1861/2160 train_time:104864ms step_avg:56.35ms
step:1862/2160 train_time:104950ms step_avg:56.36ms
step:1863/2160 train_time:105038ms step_avg:56.38ms
step:1864/2160 train_time:105125ms step_avg:56.40ms
step:1865/2160 train_time:105214ms step_avg:56.41ms
step:1866/2160 train_time:105301ms step_avg:56.43ms
step:1867/2160 train_time:105390ms step_avg:56.45ms
step:1868/2160 train_time:105476ms step_avg:56.46ms
step:1869/2160 train_time:105564ms step_avg:56.48ms
step:1870/2160 train_time:105651ms step_avg:56.50ms
step:1871/2160 train_time:105739ms step_avg:56.51ms
step:1872/2160 train_time:105826ms step_avg:56.53ms
step:1873/2160 train_time:105914ms step_avg:56.55ms
step:1874/2160 train_time:106000ms step_avg:56.56ms
step:1875/2160 train_time:106088ms step_avg:56.58ms
step:1876/2160 train_time:106175ms step_avg:56.60ms
step:1877/2160 train_time:106264ms step_avg:56.61ms
step:1878/2160 train_time:106351ms step_avg:56.63ms
step:1879/2160 train_time:106439ms step_avg:56.65ms
step:1880/2160 train_time:106526ms step_avg:56.66ms
step:1881/2160 train_time:106615ms step_avg:56.68ms
step:1882/2160 train_time:106702ms step_avg:56.70ms
step:1883/2160 train_time:106791ms step_avg:56.71ms
step:1884/2160 train_time:106876ms step_avg:56.73ms
step:1885/2160 train_time:106965ms step_avg:56.75ms
step:1886/2160 train_time:107051ms step_avg:56.76ms
step:1887/2160 train_time:107139ms step_avg:56.78ms
step:1888/2160 train_time:107226ms step_avg:56.79ms
step:1889/2160 train_time:107315ms step_avg:56.81ms
step:1890/2160 train_time:107402ms step_avg:56.83ms
step:1891/2160 train_time:107491ms step_avg:56.84ms
step:1892/2160 train_time:107578ms step_avg:56.86ms
step:1893/2160 train_time:107666ms step_avg:56.88ms
step:1894/2160 train_time:107753ms step_avg:56.89ms
step:1895/2160 train_time:107841ms step_avg:56.91ms
step:1896/2160 train_time:107928ms step_avg:56.92ms
step:1897/2160 train_time:108017ms step_avg:56.94ms
step:1898/2160 train_time:108104ms step_avg:56.96ms
step:1899/2160 train_time:108193ms step_avg:56.97ms
step:1900/2160 train_time:108279ms step_avg:56.99ms
step:1901/2160 train_time:108368ms step_avg:57.01ms
step:1902/2160 train_time:108454ms step_avg:57.02ms
step:1903/2160 train_time:108542ms step_avg:57.04ms
step:1904/2160 train_time:108629ms step_avg:57.05ms
step:1905/2160 train_time:108718ms step_avg:57.07ms
step:1906/2160 train_time:108805ms step_avg:57.09ms
step:1907/2160 train_time:108894ms step_avg:57.10ms
step:1908/2160 train_time:108981ms step_avg:57.12ms
step:1909/2160 train_time:109069ms step_avg:57.13ms
step:1910/2160 train_time:109156ms step_avg:57.15ms
step:1911/2160 train_time:109244ms step_avg:57.17ms
step:1912/2160 train_time:109332ms step_avg:57.18ms
step:1913/2160 train_time:109419ms step_avg:57.20ms
step:1914/2160 train_time:109505ms step_avg:57.21ms
step:1915/2160 train_time:109594ms step_avg:57.23ms
step:1916/2160 train_time:109680ms step_avg:57.24ms
step:1917/2160 train_time:109769ms step_avg:57.26ms
step:1918/2160 train_time:109856ms step_avg:57.28ms
step:1919/2160 train_time:109944ms step_avg:57.29ms
step:1920/2160 train_time:110031ms step_avg:57.31ms
step:1921/2160 train_time:110120ms step_avg:57.32ms
step:1922/2160 train_time:110206ms step_avg:57.34ms
step:1923/2160 train_time:110296ms step_avg:57.36ms
step:1924/2160 train_time:110383ms step_avg:57.37ms
step:1925/2160 train_time:110471ms step_avg:57.39ms
step:1926/2160 train_time:110557ms step_avg:57.40ms
step:1927/2160 train_time:110646ms step_avg:57.42ms
step:1928/2160 train_time:110733ms step_avg:57.43ms
step:1929/2160 train_time:110822ms step_avg:57.45ms
step:1930/2160 train_time:110909ms step_avg:57.47ms
step:1931/2160 train_time:110998ms step_avg:57.48ms
step:1932/2160 train_time:111084ms step_avg:57.50ms
step:1933/2160 train_time:111173ms step_avg:57.51ms
step:1934/2160 train_time:111259ms step_avg:57.53ms
step:1935/2160 train_time:111347ms step_avg:57.54ms
step:1936/2160 train_time:111435ms step_avg:57.56ms
step:1937/2160 train_time:111524ms step_avg:57.58ms
step:1938/2160 train_time:111612ms step_avg:57.59ms
step:1939/2160 train_time:111699ms step_avg:57.61ms
step:1940/2160 train_time:111785ms step_avg:57.62ms
step:1941/2160 train_time:111875ms step_avg:57.64ms
step:1942/2160 train_time:111963ms step_avg:57.65ms
step:1943/2160 train_time:112052ms step_avg:57.67ms
step:1944/2160 train_time:112137ms step_avg:57.68ms
step:1945/2160 train_time:112226ms step_avg:57.70ms
step:1946/2160 train_time:112313ms step_avg:57.71ms
step:1947/2160 train_time:112400ms step_avg:57.73ms
step:1948/2160 train_time:112486ms step_avg:57.74ms
step:1949/2160 train_time:112575ms step_avg:57.76ms
step:1950/2160 train_time:112661ms step_avg:57.78ms
step:1951/2160 train_time:112750ms step_avg:57.79ms
step:1952/2160 train_time:112836ms step_avg:57.81ms
step:1953/2160 train_time:112924ms step_avg:57.82ms
step:1954/2160 train_time:113012ms step_avg:57.84ms
step:1955/2160 train_time:113098ms step_avg:57.85ms
step:1956/2160 train_time:113185ms step_avg:57.87ms
step:1957/2160 train_time:113273ms step_avg:57.88ms
step:1958/2160 train_time:113359ms step_avg:57.90ms
step:1959/2160 train_time:113448ms step_avg:57.91ms
step:1960/2160 train_time:113534ms step_avg:57.93ms
step:1961/2160 train_time:113622ms step_avg:57.94ms
step:1962/2160 train_time:113709ms step_avg:57.96ms
step:1963/2160 train_time:113797ms step_avg:57.97ms
step:1964/2160 train_time:113885ms step_avg:57.99ms
step:1965/2160 train_time:113974ms step_avg:58.00ms
step:1966/2160 train_time:114061ms step_avg:58.02ms
step:1967/2160 train_time:114149ms step_avg:58.03ms
step:1968/2160 train_time:114236ms step_avg:58.05ms
step:1969/2160 train_time:114324ms step_avg:58.06ms
step:1970/2160 train_time:114411ms step_avg:58.08ms
step:1971/2160 train_time:114500ms step_avg:58.09ms
step:1972/2160 train_time:114587ms step_avg:58.11ms
step:1973/2160 train_time:114676ms step_avg:58.12ms
step:1974/2160 train_time:114762ms step_avg:58.14ms
step:1975/2160 train_time:114852ms step_avg:58.15ms
step:1976/2160 train_time:114938ms step_avg:58.17ms
step:1977/2160 train_time:115025ms step_avg:58.18ms
step:1978/2160 train_time:115113ms step_avg:58.20ms
step:1979/2160 train_time:115201ms step_avg:58.21ms
step:1980/2160 train_time:115288ms step_avg:58.23ms
step:1981/2160 train_time:115377ms step_avg:58.24ms
step:1982/2160 train_time:115463ms step_avg:58.26ms
step:1983/2160 train_time:115552ms step_avg:58.27ms
step:1984/2160 train_time:115638ms step_avg:58.29ms
step:1985/2160 train_time:115726ms step_avg:58.30ms
step:1986/2160 train_time:115813ms step_avg:58.31ms
step:1987/2160 train_time:115901ms step_avg:58.33ms
step:1988/2160 train_time:115988ms step_avg:58.34ms
step:1989/2160 train_time:116076ms step_avg:58.36ms
step:1990/2160 train_time:116164ms step_avg:58.37ms
step:1991/2160 train_time:116252ms step_avg:58.39ms
step:1992/2160 train_time:116338ms step_avg:58.40ms
step:1993/2160 train_time:116426ms step_avg:58.42ms
step:1994/2160 train_time:116513ms step_avg:58.43ms
step:1995/2160 train_time:116602ms step_avg:58.45ms
step:1996/2160 train_time:116688ms step_avg:58.46ms
step:1997/2160 train_time:116776ms step_avg:58.48ms
step:1998/2160 train_time:116862ms step_avg:58.49ms
step:1999/2160 train_time:116951ms step_avg:58.50ms
step:2000/2160 train_time:117038ms step_avg:58.52ms
step:2000/2160 val_loss:3.3150 train_time:117127ms step_avg:58.56ms
step:2001/2160 train_time:117150ms step_avg:58.55ms
step:2002/2160 train_time:117215ms step_avg:58.55ms
step:2003/2160 train_time:117311ms step_avg:58.57ms
step:2004/2160 train_time:117399ms step_avg:58.58ms
step:2005/2160 train_time:117488ms step_avg:58.60ms
step:2006/2160 train_time:117573ms step_avg:58.61ms
step:2007/2160 train_time:117659ms step_avg:58.62ms
step:2008/2160 train_time:117745ms step_avg:58.64ms
step:2009/2160 train_time:117831ms step_avg:58.65ms
step:2010/2160 train_time:117916ms step_avg:58.66ms
step:2011/2160 train_time:118004ms step_avg:58.68ms
step:2012/2160 train_time:118091ms step_avg:58.69ms
step:2013/2160 train_time:118181ms step_avg:58.71ms
step:2014/2160 train_time:118270ms step_avg:58.72ms
step:2015/2160 train_time:118361ms step_avg:58.74ms
step:2016/2160 train_time:118449ms step_avg:58.75ms
step:2017/2160 train_time:118535ms step_avg:58.77ms
step:2018/2160 train_time:118621ms step_avg:58.78ms
step:2019/2160 train_time:118708ms step_avg:58.80ms
step:2020/2160 train_time:118793ms step_avg:58.81ms
step:2021/2160 train_time:118880ms step_avg:58.82ms
step:2022/2160 train_time:118967ms step_avg:58.84ms
step:2023/2160 train_time:119054ms step_avg:58.85ms
step:2024/2160 train_time:119144ms step_avg:58.87ms
step:2025/2160 train_time:119235ms step_avg:58.88ms
step:2026/2160 train_time:119323ms step_avg:58.90ms
step:2027/2160 train_time:119411ms step_avg:58.91ms
step:2028/2160 train_time:119497ms step_avg:58.92ms
step:2029/2160 train_time:119586ms step_avg:58.94ms
step:2030/2160 train_time:119672ms step_avg:58.95ms
step:2031/2160 train_time:119760ms step_avg:58.97ms
step:2032/2160 train_time:119845ms step_avg:58.98ms
step:2033/2160 train_time:119932ms step_avg:58.99ms
step:2034/2160 train_time:120018ms step_avg:59.01ms
step:2035/2160 train_time:120107ms step_avg:59.02ms
step:2036/2160 train_time:120196ms step_avg:59.04ms
step:2037/2160 train_time:120287ms step_avg:59.05ms
step:2038/2160 train_time:120374ms step_avg:59.06ms
step:2039/2160 train_time:120464ms step_avg:59.08ms
step:2040/2160 train_time:120550ms step_avg:59.09ms
step:2041/2160 train_time:120637ms step_avg:59.11ms
step:2042/2160 train_time:120724ms step_avg:59.12ms
step:2043/2160 train_time:120811ms step_avg:59.13ms
step:2044/2160 train_time:120897ms step_avg:59.15ms
step:2045/2160 train_time:120985ms step_avg:59.16ms
step:2046/2160 train_time:121071ms step_avg:59.17ms
step:2047/2160 train_time:121161ms step_avg:59.19ms
step:2048/2160 train_time:121249ms step_avg:59.20ms
step:2049/2160 train_time:121338ms step_avg:59.22ms
step:2050/2160 train_time:121426ms step_avg:59.23ms
step:2051/2160 train_time:121514ms step_avg:59.25ms
step:2052/2160 train_time:121601ms step_avg:59.26ms
step:2053/2160 train_time:121689ms step_avg:59.27ms
step:2054/2160 train_time:121774ms step_avg:59.29ms
step:2055/2160 train_time:121862ms step_avg:59.30ms
step:2056/2160 train_time:121948ms step_avg:59.31ms
step:2057/2160 train_time:122037ms step_avg:59.33ms
step:2058/2160 train_time:122125ms step_avg:59.34ms
step:2059/2160 train_time:122214ms step_avg:59.36ms
step:2060/2160 train_time:122301ms step_avg:59.37ms
step:2061/2160 train_time:122389ms step_avg:59.38ms
step:2062/2160 train_time:122477ms step_avg:59.40ms
step:2063/2160 train_time:122566ms step_avg:59.41ms
step:2064/2160 train_time:122652ms step_avg:59.42ms
step:2065/2160 train_time:122741ms step_avg:59.44ms
step:2066/2160 train_time:122826ms step_avg:59.45ms
step:2067/2160 train_time:122914ms step_avg:59.46ms
step:2068/2160 train_time:123000ms step_avg:59.48ms
step:2069/2160 train_time:123089ms step_avg:59.49ms
step:2070/2160 train_time:123175ms step_avg:59.50ms
step:2071/2160 train_time:123265ms step_avg:59.52ms
step:2072/2160 train_time:123351ms step_avg:59.53ms
step:2073/2160 train_time:123440ms step_avg:59.55ms
step:2074/2160 train_time:123528ms step_avg:59.56ms
step:2075/2160 train_time:123616ms step_avg:59.57ms
step:2076/2160 train_time:123702ms step_avg:59.59ms
step:2077/2160 train_time:123790ms step_avg:59.60ms
step:2078/2160 train_time:123876ms step_avg:59.61ms
step:2079/2160 train_time:123965ms step_avg:59.63ms
step:2080/2160 train_time:124050ms step_avg:59.64ms
step:2081/2160 train_time:124140ms step_avg:59.65ms
step:2082/2160 train_time:124227ms step_avg:59.67ms
step:2083/2160 train_time:124316ms step_avg:59.68ms
step:2084/2160 train_time:124403ms step_avg:59.69ms
step:2085/2160 train_time:124491ms step_avg:59.71ms
step:2086/2160 train_time:124577ms step_avg:59.72ms
step:2087/2160 train_time:124666ms step_avg:59.73ms
step:2088/2160 train_time:124752ms step_avg:59.75ms
step:2089/2160 train_time:124840ms step_avg:59.76ms
step:2090/2160 train_time:124925ms step_avg:59.77ms
step:2091/2160 train_time:125013ms step_avg:59.79ms
step:2092/2160 train_time:125100ms step_avg:59.80ms
step:2093/2160 train_time:125189ms step_avg:59.81ms
step:2094/2160 train_time:125275ms step_avg:59.83ms
step:2095/2160 train_time:125364ms step_avg:59.84ms
step:2096/2160 train_time:125451ms step_avg:59.85ms
step:2097/2160 train_time:125538ms step_avg:59.87ms
step:2098/2160 train_time:125625ms step_avg:59.88ms
step:2099/2160 train_time:125713ms step_avg:59.89ms
step:2100/2160 train_time:125800ms step_avg:59.90ms
step:2101/2160 train_time:125888ms step_avg:59.92ms
step:2102/2160 train_time:125974ms step_avg:59.93ms
step:2103/2160 train_time:126063ms step_avg:59.94ms
step:2104/2160 train_time:126149ms step_avg:59.96ms
step:2105/2160 train_time:126239ms step_avg:59.97ms
step:2106/2160 train_time:126326ms step_avg:59.98ms
step:2107/2160 train_time:126415ms step_avg:60.00ms
step:2108/2160 train_time:126502ms step_avg:60.01ms
step:2109/2160 train_time:126591ms step_avg:60.02ms
step:2110/2160 train_time:126677ms step_avg:60.04ms
step:2111/2160 train_time:126766ms step_avg:60.05ms
step:2112/2160 train_time:126852ms step_avg:60.06ms
step:2113/2160 train_time:126942ms step_avg:60.08ms
step:2114/2160 train_time:127028ms step_avg:60.09ms
step:2115/2160 train_time:127116ms step_avg:60.10ms
step:2116/2160 train_time:127204ms step_avg:60.12ms
step:2117/2160 train_time:127292ms step_avg:60.13ms
step:2118/2160 train_time:127378ms step_avg:60.14ms
step:2119/2160 train_time:127467ms step_avg:60.15ms
step:2120/2160 train_time:127553ms step_avg:60.17ms
step:2121/2160 train_time:127642ms step_avg:60.18ms
step:2122/2160 train_time:127728ms step_avg:60.19ms
step:2123/2160 train_time:127817ms step_avg:60.21ms
step:2124/2160 train_time:127904ms step_avg:60.22ms
step:2125/2160 train_time:127993ms step_avg:60.23ms
step:2126/2160 train_time:128079ms step_avg:60.24ms
step:2127/2160 train_time:128169ms step_avg:60.26ms
step:2128/2160 train_time:128255ms step_avg:60.27ms
step:2129/2160 train_time:128344ms step_avg:60.28ms
step:2130/2160 train_time:128431ms step_avg:60.30ms
step:2131/2160 train_time:128519ms step_avg:60.31ms
step:2132/2160 train_time:128607ms step_avg:60.32ms
step:2133/2160 train_time:128694ms step_avg:60.33ms
step:2134/2160 train_time:128781ms step_avg:60.35ms
step:2135/2160 train_time:128870ms step_avg:60.36ms
step:2136/2160 train_time:128957ms step_avg:60.37ms
step:2137/2160 train_time:129047ms step_avg:60.39ms
step:2138/2160 train_time:129134ms step_avg:60.40ms
step:2139/2160 train_time:129224ms step_avg:60.41ms
step:2140/2160 train_time:129310ms step_avg:60.43ms
step:2141/2160 train_time:129398ms step_avg:60.44ms
step:2142/2160 train_time:129485ms step_avg:60.45ms
step:2143/2160 train_time:129574ms step_avg:60.46ms
step:2144/2160 train_time:129660ms step_avg:60.48ms
step:2145/2160 train_time:129749ms step_avg:60.49ms
step:2146/2160 train_time:129837ms step_avg:60.50ms
step:2147/2160 train_time:129927ms step_avg:60.52ms
step:2148/2160 train_time:130013ms step_avg:60.53ms
step:2149/2160 train_time:130102ms step_avg:60.54ms
step:2150/2160 train_time:130189ms step_avg:60.55ms
step:2151/2160 train_time:130276ms step_avg:60.57ms
step:2152/2160 train_time:130363ms step_avg:60.58ms
step:2153/2160 train_time:130451ms step_avg:60.59ms
step:2154/2160 train_time:130538ms step_avg:60.60ms
step:2155/2160 train_time:130627ms step_avg:60.62ms
step:2156/2160 train_time:130714ms step_avg:60.63ms
step:2157/2160 train_time:130802ms step_avg:60.64ms
step:2158/2160 train_time:130889ms step_avg:60.65ms
step:2159/2160 train_time:130978ms step_avg:60.67ms
step:2160/2160 train_time:131067ms step_avg:60.68ms
step:2160/2160 val_loss:3.2785 train_time:131157ms step_avg:60.72ms
peak memory allocated: 29975 MiB reserved: 44136 MiB

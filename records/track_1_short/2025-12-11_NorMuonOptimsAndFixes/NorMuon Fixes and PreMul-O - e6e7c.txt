import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:27:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           47606      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           47607      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           47608      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           47609      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           47610      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           47611      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           47612      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           47613      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           47607      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           47608      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           47609      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           47610      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           47611      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           47612      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           47613      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:91ms step_avg:91.30ms
step:2/2160 train_time:117ms step_avg:58.43ms
step:3/2160 train_time:140ms step_avg:46.56ms
step:4/2160 train_time:164ms step_avg:40.89ms
step:5/2160 train_time:195ms step_avg:38.96ms
step:6/2160 train_time:327ms step_avg:54.46ms
step:7/2160 train_time:466ms step_avg:66.60ms
step:8/2160 train_time:498ms step_avg:62.31ms
step:9/2160 train_time:531ms step_avg:59.02ms
step:10/2160 train_time:563ms step_avg:56.34ms
step:11/2160 train_time:597ms step_avg:54.27ms
step:12/2160 train_time:629ms step_avg:52.43ms
step:13/2160 train_time:663ms step_avg:50.97ms
step:14/2160 train_time:695ms step_avg:49.64ms
step:15/2160 train_time:728ms step_avg:48.55ms
step:16/2160 train_time:761ms step_avg:47.54ms
step:17/2160 train_time:794ms step_avg:46.69ms
step:18/2160 train_time:826ms step_avg:45.89ms
step:19/2160 train_time:860ms step_avg:45.24ms
step:20/2160 train_time:892ms step_avg:44.59ms
step:21/2160 train_time:925ms step_avg:44.06ms
step:22/2160 train_time:958ms step_avg:43.52ms
step:23/2160 train_time:991ms step_avg:43.07ms
step:24/2160 train_time:1023ms step_avg:42.63ms
step:25/2160 train_time:1056ms step_avg:42.25ms
step:26/2160 train_time:1089ms step_avg:41.88ms
step:27/2160 train_time:1122ms step_avg:41.56ms
step:28/2160 train_time:1154ms step_avg:41.23ms
step:29/2160 train_time:1187ms step_avg:40.95ms
step:30/2160 train_time:1220ms step_avg:40.66ms
step:31/2160 train_time:1253ms step_avg:40.43ms
step:32/2160 train_time:1286ms step_avg:40.18ms
step:33/2160 train_time:1319ms step_avg:39.97ms
step:34/2160 train_time:1352ms step_avg:39.75ms
step:35/2160 train_time:1386ms step_avg:39.60ms
step:36/2160 train_time:1419ms step_avg:39.41ms
step:37/2160 train_time:1454ms step_avg:39.29ms
step:38/2160 train_time:1487ms step_avg:39.13ms
step:39/2160 train_time:1520ms step_avg:38.98ms
step:40/2160 train_time:1553ms step_avg:38.82ms
step:41/2160 train_time:1587ms step_avg:38.70ms
step:42/2160 train_time:1619ms step_avg:38.55ms
step:43/2160 train_time:1653ms step_avg:38.43ms
step:44/2160 train_time:1685ms step_avg:38.31ms
step:45/2160 train_time:1719ms step_avg:38.19ms
step:46/2160 train_time:1751ms step_avg:38.07ms
step:47/2160 train_time:1784ms step_avg:37.97ms
step:48/2160 train_time:1817ms step_avg:37.85ms
step:49/2160 train_time:1850ms step_avg:37.76ms
step:50/2160 train_time:1882ms step_avg:37.65ms
step:51/2160 train_time:1916ms step_avg:37.58ms
step:52/2160 train_time:1949ms step_avg:37.47ms
step:53/2160 train_time:1982ms step_avg:37.40ms
step:54/2160 train_time:2014ms step_avg:37.30ms
step:55/2160 train_time:2047ms step_avg:37.22ms
step:56/2160 train_time:2080ms step_avg:37.14ms
step:57/2160 train_time:2113ms step_avg:37.07ms
step:58/2160 train_time:2146ms step_avg:36.99ms
step:59/2160 train_time:2179ms step_avg:36.93ms
step:60/2160 train_time:2211ms step_avg:36.86ms
step:61/2160 train_time:2244ms step_avg:36.79ms
step:62/2160 train_time:2277ms step_avg:36.72ms
step:63/2160 train_time:2310ms step_avg:36.67ms
step:64/2160 train_time:2342ms step_avg:36.60ms
step:65/2160 train_time:2376ms step_avg:36.56ms
step:66/2160 train_time:2409ms step_avg:36.50ms
step:67/2160 train_time:2444ms step_avg:36.47ms
step:68/2160 train_time:2475ms step_avg:36.40ms
step:69/2160 train_time:2508ms step_avg:36.35ms
step:70/2160 train_time:2541ms step_avg:36.30ms
step:71/2160 train_time:2575ms step_avg:36.27ms
step:72/2160 train_time:2607ms step_avg:36.21ms
step:73/2160 train_time:2641ms step_avg:36.18ms
step:74/2160 train_time:2674ms step_avg:36.14ms
step:75/2160 train_time:2707ms step_avg:36.10ms
step:76/2160 train_time:2740ms step_avg:36.05ms
step:77/2160 train_time:2773ms step_avg:36.01ms
step:78/2160 train_time:2805ms step_avg:35.97ms
step:79/2160 train_time:2839ms step_avg:35.93ms
step:80/2160 train_time:2871ms step_avg:35.89ms
step:81/2160 train_time:2905ms step_avg:35.86ms
step:82/2160 train_time:2937ms step_avg:35.81ms
step:83/2160 train_time:2970ms step_avg:35.78ms
step:84/2160 train_time:3002ms step_avg:35.74ms
step:85/2160 train_time:3035ms step_avg:35.71ms
step:86/2160 train_time:3068ms step_avg:35.67ms
step:87/2160 train_time:3101ms step_avg:35.65ms
step:88/2160 train_time:3134ms step_avg:35.61ms
step:89/2160 train_time:3167ms step_avg:35.58ms
step:90/2160 train_time:3199ms step_avg:35.55ms
step:91/2160 train_time:3232ms step_avg:35.52ms
step:92/2160 train_time:3265ms step_avg:35.49ms
step:93/2160 train_time:3298ms step_avg:35.47ms
step:94/2160 train_time:3331ms step_avg:35.43ms
step:95/2160 train_time:3364ms step_avg:35.41ms
step:96/2160 train_time:3396ms step_avg:35.38ms
step:97/2160 train_time:3429ms step_avg:35.36ms
step:98/2160 train_time:3462ms step_avg:35.33ms
step:99/2160 train_time:3496ms step_avg:35.31ms
step:100/2160 train_time:3528ms step_avg:35.28ms
step:101/2160 train_time:3562ms step_avg:35.26ms
step:102/2160 train_time:3594ms step_avg:35.23ms
step:103/2160 train_time:3627ms step_avg:35.22ms
step:104/2160 train_time:3660ms step_avg:35.19ms
step:105/2160 train_time:3693ms step_avg:35.17ms
step:106/2160 train_time:3725ms step_avg:35.15ms
step:107/2160 train_time:3760ms step_avg:35.14ms
step:108/2160 train_time:3792ms step_avg:35.11ms
step:109/2160 train_time:3826ms step_avg:35.10ms
step:110/2160 train_time:3858ms step_avg:35.07ms
step:111/2160 train_time:3891ms step_avg:35.05ms
step:112/2160 train_time:3923ms step_avg:35.03ms
step:113/2160 train_time:3957ms step_avg:35.01ms
step:114/2160 train_time:3989ms step_avg:34.99ms
step:115/2160 train_time:4022ms step_avg:34.98ms
step:116/2160 train_time:4055ms step_avg:34.95ms
step:117/2160 train_time:4088ms step_avg:34.94ms
step:118/2160 train_time:4120ms step_avg:34.91ms
step:119/2160 train_time:4153ms step_avg:34.90ms
step:120/2160 train_time:4185ms step_avg:34.88ms
step:121/2160 train_time:4219ms step_avg:34.86ms
step:122/2160 train_time:4251ms step_avg:34.84ms
step:123/2160 train_time:4284ms step_avg:34.83ms
step:124/2160 train_time:4317ms step_avg:34.81ms
step:125/2160 train_time:4350ms step_avg:34.80ms
step:126/2160 train_time:4382ms step_avg:34.78ms
step:127/2160 train_time:4415ms step_avg:34.77ms
step:128/2160 train_time:4448ms step_avg:34.75ms
step:129/2160 train_time:4481ms step_avg:34.74ms
step:130/2160 train_time:4514ms step_avg:34.72ms
step:131/2160 train_time:4547ms step_avg:34.71ms
step:132/2160 train_time:4579ms step_avg:34.69ms
step:133/2160 train_time:4612ms step_avg:34.68ms
step:134/2160 train_time:4645ms step_avg:34.66ms
step:135/2160 train_time:4678ms step_avg:34.65ms
step:136/2160 train_time:4711ms step_avg:34.64ms
step:137/2160 train_time:4744ms step_avg:34.63ms
step:138/2160 train_time:4776ms step_avg:34.61ms
step:139/2160 train_time:4809ms step_avg:34.60ms
step:140/2160 train_time:4841ms step_avg:34.58ms
step:141/2160 train_time:4875ms step_avg:34.58ms
step:142/2160 train_time:4907ms step_avg:34.56ms
step:143/2160 train_time:4941ms step_avg:34.55ms
step:144/2160 train_time:4973ms step_avg:34.54ms
step:145/2160 train_time:5006ms step_avg:34.53ms
step:146/2160 train_time:5038ms step_avg:34.51ms
step:147/2160 train_time:5072ms step_avg:34.50ms
step:148/2160 train_time:5104ms step_avg:34.49ms
step:149/2160 train_time:5137ms step_avg:34.48ms
step:150/2160 train_time:5170ms step_avg:34.46ms
step:151/2160 train_time:5203ms step_avg:34.46ms
step:152/2160 train_time:5235ms step_avg:34.44ms
step:153/2160 train_time:5268ms step_avg:34.43ms
step:154/2160 train_time:5300ms step_avg:34.42ms
step:155/2160 train_time:5334ms step_avg:34.41ms
step:156/2160 train_time:5366ms step_avg:34.40ms
step:157/2160 train_time:5399ms step_avg:34.39ms
step:158/2160 train_time:5432ms step_avg:34.38ms
step:159/2160 train_time:5465ms step_avg:34.37ms
step:160/2160 train_time:5497ms step_avg:34.36ms
step:161/2160 train_time:5530ms step_avg:34.35ms
step:162/2160 train_time:5563ms step_avg:34.34ms
step:163/2160 train_time:5596ms step_avg:34.33ms
step:164/2160 train_time:5628ms step_avg:34.32ms
step:165/2160 train_time:5661ms step_avg:34.31ms
step:166/2160 train_time:5694ms step_avg:34.30ms
step:167/2160 train_time:5727ms step_avg:34.29ms
step:168/2160 train_time:5759ms step_avg:34.28ms
step:169/2160 train_time:5792ms step_avg:34.27ms
step:170/2160 train_time:5825ms step_avg:34.26ms
step:171/2160 train_time:5858ms step_avg:34.26ms
step:172/2160 train_time:5891ms step_avg:34.25ms
step:173/2160 train_time:5924ms step_avg:34.24ms
step:174/2160 train_time:5956ms step_avg:34.23ms
step:175/2160 train_time:5989ms step_avg:34.22ms
step:176/2160 train_time:6022ms step_avg:34.21ms
step:177/2160 train_time:6055ms step_avg:34.21ms
step:178/2160 train_time:6087ms step_avg:34.20ms
step:179/2160 train_time:6120ms step_avg:34.19ms
step:180/2160 train_time:6152ms step_avg:34.18ms
step:181/2160 train_time:6186ms step_avg:34.18ms
step:182/2160 train_time:6218ms step_avg:34.17ms
step:183/2160 train_time:6252ms step_avg:34.16ms
step:184/2160 train_time:6284ms step_avg:34.15ms
step:185/2160 train_time:6317ms step_avg:34.15ms
step:186/2160 train_time:6350ms step_avg:34.14ms
step:187/2160 train_time:6383ms step_avg:34.13ms
step:188/2160 train_time:6415ms step_avg:34.12ms
step:189/2160 train_time:6448ms step_avg:34.12ms
step:190/2160 train_time:6480ms step_avg:34.11ms
step:191/2160 train_time:6514ms step_avg:34.10ms
step:192/2160 train_time:6546ms step_avg:34.09ms
step:193/2160 train_time:6580ms step_avg:34.09ms
step:194/2160 train_time:6612ms step_avg:34.08ms
step:195/2160 train_time:6645ms step_avg:34.08ms
step:196/2160 train_time:6677ms step_avg:34.07ms
step:197/2160 train_time:6711ms step_avg:34.07ms
step:198/2160 train_time:6743ms step_avg:34.06ms
step:199/2160 train_time:6777ms step_avg:34.05ms
step:200/2160 train_time:6809ms step_avg:34.04ms
step:201/2160 train_time:6842ms step_avg:34.04ms
step:202/2160 train_time:6875ms step_avg:34.03ms
step:203/2160 train_time:6908ms step_avg:34.03ms
step:204/2160 train_time:6940ms step_avg:34.02ms
step:205/2160 train_time:6973ms step_avg:34.01ms
step:206/2160 train_time:7006ms step_avg:34.01ms
step:207/2160 train_time:7039ms step_avg:34.01ms
step:208/2160 train_time:7071ms step_avg:34.00ms
step:209/2160 train_time:7105ms step_avg:33.99ms
step:210/2160 train_time:7137ms step_avg:33.99ms
step:211/2160 train_time:7170ms step_avg:33.98ms
step:212/2160 train_time:7203ms step_avg:33.97ms
step:213/2160 train_time:7235ms step_avg:33.97ms
step:214/2160 train_time:7268ms step_avg:33.96ms
step:215/2160 train_time:7301ms step_avg:33.96ms
step:216/2160 train_time:7333ms step_avg:33.95ms
step:217/2160 train_time:7367ms step_avg:33.95ms
step:218/2160 train_time:7399ms step_avg:33.94ms
step:219/2160 train_time:7432ms step_avg:33.93ms
step:220/2160 train_time:7464ms step_avg:33.93ms
step:221/2160 train_time:7497ms step_avg:33.92ms
step:222/2160 train_time:7529ms step_avg:33.92ms
step:223/2160 train_time:7563ms step_avg:33.91ms
step:224/2160 train_time:7595ms step_avg:33.91ms
step:225/2160 train_time:7628ms step_avg:33.90ms
step:226/2160 train_time:7661ms step_avg:33.90ms
step:227/2160 train_time:7694ms step_avg:33.89ms
step:228/2160 train_time:7726ms step_avg:33.89ms
step:229/2160 train_time:7760ms step_avg:33.88ms
step:230/2160 train_time:7792ms step_avg:33.88ms
step:231/2160 train_time:7825ms step_avg:33.87ms
step:232/2160 train_time:7857ms step_avg:33.87ms
step:233/2160 train_time:7890ms step_avg:33.86ms
step:234/2160 train_time:7923ms step_avg:33.86ms
step:235/2160 train_time:7956ms step_avg:33.85ms
step:236/2160 train_time:7988ms step_avg:33.85ms
step:237/2160 train_time:8022ms step_avg:33.85ms
step:238/2160 train_time:8054ms step_avg:33.84ms
step:239/2160 train_time:8087ms step_avg:33.84ms
step:240/2160 train_time:8120ms step_avg:33.83ms
step:241/2160 train_time:8153ms step_avg:33.83ms
step:242/2160 train_time:8185ms step_avg:33.82ms
step:243/2160 train_time:8219ms step_avg:33.82ms
step:244/2160 train_time:8251ms step_avg:33.82ms
step:245/2160 train_time:8284ms step_avg:33.81ms
step:246/2160 train_time:8317ms step_avg:33.81ms
step:247/2160 train_time:8350ms step_avg:33.80ms
step:248/2160 train_time:8382ms step_avg:33.80ms
step:249/2160 train_time:8415ms step_avg:33.79ms
step:250/2160 train_time:8447ms step_avg:33.79ms
step:250/2160 val_loss:4.3110 train_time:8483ms step_avg:33.93ms
step:251/2160 train_time:8506ms step_avg:33.89ms
step:252/2160 train_time:8528ms step_avg:33.84ms
step:253/2160 train_time:8549ms step_avg:33.79ms
step:254/2160 train_time:8581ms step_avg:33.78ms
step:255/2160 train_time:8616ms step_avg:33.79ms
step:256/2160 train_time:8649ms step_avg:33.79ms
step:257/2160 train_time:8685ms step_avg:33.79ms
step:258/2160 train_time:8717ms step_avg:33.79ms
step:259/2160 train_time:8751ms step_avg:33.79ms
step:260/2160 train_time:8783ms step_avg:33.78ms
step:261/2160 train_time:8816ms step_avg:33.78ms
step:262/2160 train_time:8849ms step_avg:33.77ms
step:263/2160 train_time:8882ms step_avg:33.77ms
step:264/2160 train_time:8914ms step_avg:33.76ms
step:265/2160 train_time:8947ms step_avg:33.76ms
step:266/2160 train_time:8979ms step_avg:33.76ms
step:267/2160 train_time:9012ms step_avg:33.75ms
step:268/2160 train_time:9044ms step_avg:33.75ms
step:269/2160 train_time:9077ms step_avg:33.74ms
step:270/2160 train_time:9109ms step_avg:33.74ms
step:271/2160 train_time:9142ms step_avg:33.74ms
step:272/2160 train_time:9174ms step_avg:33.73ms
step:273/2160 train_time:9207ms step_avg:33.73ms
step:274/2160 train_time:9240ms step_avg:33.72ms
step:275/2160 train_time:9272ms step_avg:33.72ms
step:276/2160 train_time:9304ms step_avg:33.71ms
step:277/2160 train_time:9337ms step_avg:33.71ms
step:278/2160 train_time:9370ms step_avg:33.70ms
step:279/2160 train_time:9402ms step_avg:33.70ms
step:280/2160 train_time:9435ms step_avg:33.69ms
step:281/2160 train_time:9468ms step_avg:33.69ms
step:282/2160 train_time:9500ms step_avg:33.69ms
step:283/2160 train_time:9533ms step_avg:33.68ms
step:284/2160 train_time:9565ms step_avg:33.68ms
step:285/2160 train_time:9599ms step_avg:33.68ms
step:286/2160 train_time:9631ms step_avg:33.68ms
step:287/2160 train_time:9665ms step_avg:33.68ms
step:288/2160 train_time:9697ms step_avg:33.67ms
step:289/2160 train_time:9731ms step_avg:33.67ms
step:290/2160 train_time:9763ms step_avg:33.67ms
step:291/2160 train_time:9797ms step_avg:33.67ms
step:292/2160 train_time:9829ms step_avg:33.66ms
step:293/2160 train_time:9863ms step_avg:33.66ms
step:294/2160 train_time:9895ms step_avg:33.66ms
step:295/2160 train_time:9928ms step_avg:33.65ms
step:296/2160 train_time:9960ms step_avg:33.65ms
step:297/2160 train_time:9993ms step_avg:33.65ms
step:298/2160 train_time:10025ms step_avg:33.64ms
step:299/2160 train_time:10058ms step_avg:33.64ms
step:300/2160 train_time:10090ms step_avg:33.63ms
step:301/2160 train_time:10124ms step_avg:33.63ms
step:302/2160 train_time:10156ms step_avg:33.63ms
step:303/2160 train_time:10189ms step_avg:33.63ms
step:304/2160 train_time:10221ms step_avg:33.62ms
step:305/2160 train_time:10254ms step_avg:33.62ms
step:306/2160 train_time:10286ms step_avg:33.61ms
step:307/2160 train_time:10319ms step_avg:33.61ms
step:308/2160 train_time:10351ms step_avg:33.61ms
step:309/2160 train_time:10384ms step_avg:33.61ms
step:310/2160 train_time:10416ms step_avg:33.60ms
step:311/2160 train_time:10449ms step_avg:33.60ms
step:312/2160 train_time:10481ms step_avg:33.59ms
step:313/2160 train_time:10515ms step_avg:33.59ms
step:314/2160 train_time:10547ms step_avg:33.59ms
step:315/2160 train_time:10580ms step_avg:33.59ms
step:316/2160 train_time:10612ms step_avg:33.58ms
step:317/2160 train_time:10646ms step_avg:33.58ms
step:318/2160 train_time:10678ms step_avg:33.58ms
step:319/2160 train_time:10711ms step_avg:33.58ms
step:320/2160 train_time:10744ms step_avg:33.57ms
step:321/2160 train_time:10777ms step_avg:33.57ms
step:322/2160 train_time:10809ms step_avg:33.57ms
step:323/2160 train_time:10843ms step_avg:33.57ms
step:324/2160 train_time:10875ms step_avg:33.57ms
step:325/2160 train_time:10908ms step_avg:33.56ms
step:326/2160 train_time:10941ms step_avg:33.56ms
step:327/2160 train_time:10973ms step_avg:33.56ms
step:328/2160 train_time:11006ms step_avg:33.55ms
step:329/2160 train_time:11039ms step_avg:33.55ms
step:330/2160 train_time:11071ms step_avg:33.55ms
step:331/2160 train_time:11105ms step_avg:33.55ms
step:332/2160 train_time:11137ms step_avg:33.55ms
step:333/2160 train_time:11170ms step_avg:33.54ms
step:334/2160 train_time:11202ms step_avg:33.54ms
step:335/2160 train_time:11235ms step_avg:33.54ms
step:336/2160 train_time:11267ms step_avg:33.53ms
step:337/2160 train_time:11301ms step_avg:33.53ms
step:338/2160 train_time:11333ms step_avg:33.53ms
step:339/2160 train_time:11366ms step_avg:33.53ms
step:340/2160 train_time:11398ms step_avg:33.52ms
step:341/2160 train_time:11431ms step_avg:33.52ms
step:342/2160 train_time:11463ms step_avg:33.52ms
step:343/2160 train_time:11496ms step_avg:33.52ms
step:344/2160 train_time:11528ms step_avg:33.51ms
step:345/2160 train_time:11561ms step_avg:33.51ms
step:346/2160 train_time:11593ms step_avg:33.51ms
step:347/2160 train_time:11627ms step_avg:33.51ms
step:348/2160 train_time:11659ms step_avg:33.50ms
step:349/2160 train_time:11692ms step_avg:33.50ms
step:350/2160 train_time:11724ms step_avg:33.50ms
step:351/2160 train_time:11758ms step_avg:33.50ms
step:352/2160 train_time:11790ms step_avg:33.49ms
step:353/2160 train_time:11823ms step_avg:33.49ms
step:354/2160 train_time:11855ms step_avg:33.49ms
step:355/2160 train_time:11889ms step_avg:33.49ms
step:356/2160 train_time:11921ms step_avg:33.49ms
step:357/2160 train_time:11954ms step_avg:33.49ms
step:358/2160 train_time:11986ms step_avg:33.48ms
step:359/2160 train_time:12020ms step_avg:33.48ms
step:360/2160 train_time:12052ms step_avg:33.48ms
step:361/2160 train_time:12085ms step_avg:33.48ms
step:362/2160 train_time:12118ms step_avg:33.47ms
step:363/2160 train_time:12151ms step_avg:33.47ms
step:364/2160 train_time:12183ms step_avg:33.47ms
step:365/2160 train_time:12216ms step_avg:33.47ms
step:366/2160 train_time:12248ms step_avg:33.47ms
step:367/2160 train_time:12282ms step_avg:33.46ms
step:368/2160 train_time:12314ms step_avg:33.46ms
step:369/2160 train_time:12347ms step_avg:33.46ms
step:370/2160 train_time:12379ms step_avg:33.46ms
step:371/2160 train_time:12412ms step_avg:33.46ms
step:372/2160 train_time:12445ms step_avg:33.45ms
step:373/2160 train_time:12478ms step_avg:33.45ms
step:374/2160 train_time:12510ms step_avg:33.45ms
step:375/2160 train_time:12543ms step_avg:33.45ms
step:376/2160 train_time:12575ms step_avg:33.44ms
step:377/2160 train_time:12608ms step_avg:33.44ms
step:378/2160 train_time:12640ms step_avg:33.44ms
step:379/2160 train_time:12673ms step_avg:33.44ms
step:380/2160 train_time:12705ms step_avg:33.44ms
step:381/2160 train_time:12738ms step_avg:33.43ms
step:382/2160 train_time:12771ms step_avg:33.43ms
step:383/2160 train_time:12804ms step_avg:33.43ms
step:384/2160 train_time:12836ms step_avg:33.43ms
step:385/2160 train_time:12869ms step_avg:33.43ms
step:386/2160 train_time:12901ms step_avg:33.42ms
step:387/2160 train_time:12934ms step_avg:33.42ms
step:388/2160 train_time:12966ms step_avg:33.42ms
step:389/2160 train_time:13000ms step_avg:33.42ms
step:390/2160 train_time:13032ms step_avg:33.41ms
step:391/2160 train_time:13065ms step_avg:33.41ms
step:392/2160 train_time:13097ms step_avg:33.41ms
step:393/2160 train_time:13131ms step_avg:33.41ms
step:394/2160 train_time:13163ms step_avg:33.41ms
step:395/2160 train_time:13196ms step_avg:33.41ms
step:396/2160 train_time:13228ms step_avg:33.40ms
step:397/2160 train_time:13262ms step_avg:33.40ms
step:398/2160 train_time:13294ms step_avg:33.40ms
step:399/2160 train_time:13327ms step_avg:33.40ms
step:400/2160 train_time:13359ms step_avg:33.40ms
step:401/2160 train_time:13392ms step_avg:33.40ms
step:402/2160 train_time:13424ms step_avg:33.39ms
step:403/2160 train_time:13457ms step_avg:33.39ms
step:404/2160 train_time:13489ms step_avg:33.39ms
step:405/2160 train_time:13523ms step_avg:33.39ms
step:406/2160 train_time:13555ms step_avg:33.39ms
step:407/2160 train_time:13589ms step_avg:33.39ms
step:408/2160 train_time:13621ms step_avg:33.38ms
step:409/2160 train_time:13654ms step_avg:33.38ms
step:410/2160 train_time:13686ms step_avg:33.38ms
step:411/2160 train_time:13719ms step_avg:33.38ms
step:412/2160 train_time:13752ms step_avg:33.38ms
step:413/2160 train_time:13785ms step_avg:33.38ms
step:414/2160 train_time:13817ms step_avg:33.38ms
step:415/2160 train_time:13851ms step_avg:33.38ms
step:416/2160 train_time:13883ms step_avg:33.37ms
step:417/2160 train_time:13916ms step_avg:33.37ms
step:418/2160 train_time:13948ms step_avg:33.37ms
step:419/2160 train_time:13982ms step_avg:33.37ms
step:420/2160 train_time:14014ms step_avg:33.37ms
step:421/2160 train_time:14047ms step_avg:33.37ms
step:422/2160 train_time:14080ms step_avg:33.36ms
step:423/2160 train_time:14113ms step_avg:33.36ms
step:424/2160 train_time:14145ms step_avg:33.36ms
step:425/2160 train_time:14178ms step_avg:33.36ms
step:426/2160 train_time:14210ms step_avg:33.36ms
step:427/2160 train_time:14244ms step_avg:33.36ms
step:428/2160 train_time:14276ms step_avg:33.36ms
step:429/2160 train_time:14309ms step_avg:33.35ms
step:430/2160 train_time:14341ms step_avg:33.35ms
step:431/2160 train_time:14374ms step_avg:33.35ms
step:432/2160 train_time:14407ms step_avg:33.35ms
step:433/2160 train_time:14440ms step_avg:33.35ms
step:434/2160 train_time:14472ms step_avg:33.35ms
step:435/2160 train_time:14505ms step_avg:33.34ms
step:436/2160 train_time:14537ms step_avg:33.34ms
step:437/2160 train_time:14570ms step_avg:33.34ms
step:438/2160 train_time:14602ms step_avg:33.34ms
step:439/2160 train_time:14636ms step_avg:33.34ms
step:440/2160 train_time:14668ms step_avg:33.34ms
step:441/2160 train_time:14701ms step_avg:33.34ms
step:442/2160 train_time:14733ms step_avg:33.33ms
step:443/2160 train_time:14767ms step_avg:33.33ms
step:444/2160 train_time:14799ms step_avg:33.33ms
step:445/2160 train_time:14832ms step_avg:33.33ms
step:446/2160 train_time:14864ms step_avg:33.33ms
step:447/2160 train_time:14897ms step_avg:33.33ms
step:448/2160 train_time:14930ms step_avg:33.33ms
step:449/2160 train_time:14963ms step_avg:33.33ms
step:450/2160 train_time:14995ms step_avg:33.32ms
step:451/2160 train_time:15029ms step_avg:33.32ms
step:452/2160 train_time:15061ms step_avg:33.32ms
step:453/2160 train_time:15094ms step_avg:33.32ms
step:454/2160 train_time:15126ms step_avg:33.32ms
step:455/2160 train_time:15159ms step_avg:33.32ms
step:456/2160 train_time:15191ms step_avg:33.31ms
step:457/2160 train_time:15225ms step_avg:33.31ms
step:458/2160 train_time:15257ms step_avg:33.31ms
step:459/2160 train_time:15291ms step_avg:33.31ms
step:460/2160 train_time:15323ms step_avg:33.31ms
step:461/2160 train_time:15356ms step_avg:33.31ms
step:462/2160 train_time:15388ms step_avg:33.31ms
step:463/2160 train_time:15421ms step_avg:33.31ms
step:464/2160 train_time:15454ms step_avg:33.31ms
step:465/2160 train_time:15487ms step_avg:33.31ms
step:466/2160 train_time:15519ms step_avg:33.30ms
step:467/2160 train_time:15552ms step_avg:33.30ms
step:468/2160 train_time:15585ms step_avg:33.30ms
step:469/2160 train_time:15618ms step_avg:33.30ms
step:470/2160 train_time:15650ms step_avg:33.30ms
step:471/2160 train_time:15684ms step_avg:33.30ms
step:472/2160 train_time:15716ms step_avg:33.30ms
step:473/2160 train_time:15749ms step_avg:33.30ms
step:474/2160 train_time:15781ms step_avg:33.29ms
step:475/2160 train_time:15814ms step_avg:33.29ms
step:476/2160 train_time:15846ms step_avg:33.29ms
step:477/2160 train_time:15879ms step_avg:33.29ms
step:478/2160 train_time:15912ms step_avg:33.29ms
step:479/2160 train_time:15946ms step_avg:33.29ms
step:480/2160 train_time:15978ms step_avg:33.29ms
step:481/2160 train_time:16011ms step_avg:33.29ms
step:482/2160 train_time:16043ms step_avg:33.28ms
step:483/2160 train_time:16076ms step_avg:33.28ms
step:484/2160 train_time:16109ms step_avg:33.28ms
step:485/2160 train_time:16142ms step_avg:33.28ms
step:486/2160 train_time:16174ms step_avg:33.28ms
step:487/2160 train_time:16207ms step_avg:33.28ms
step:488/2160 train_time:16240ms step_avg:33.28ms
step:489/2160 train_time:16273ms step_avg:33.28ms
step:490/2160 train_time:16305ms step_avg:33.27ms
step:491/2160 train_time:16338ms step_avg:33.28ms
step:492/2160 train_time:16370ms step_avg:33.27ms
step:493/2160 train_time:16404ms step_avg:33.27ms
step:494/2160 train_time:16436ms step_avg:33.27ms
step:495/2160 train_time:16470ms step_avg:33.27ms
step:496/2160 train_time:16502ms step_avg:33.27ms
step:497/2160 train_time:16535ms step_avg:33.27ms
step:498/2160 train_time:16567ms step_avg:33.27ms
step:499/2160 train_time:16600ms step_avg:33.27ms
step:500/2160 train_time:16632ms step_avg:33.26ms
step:500/2160 val_loss:4.0186 train_time:16668ms step_avg:33.34ms
step:501/2160 train_time:16690ms step_avg:33.31ms
step:502/2160 train_time:16712ms step_avg:33.29ms
step:503/2160 train_time:16735ms step_avg:33.27ms
step:504/2160 train_time:16769ms step_avg:33.27ms
step:505/2160 train_time:16803ms step_avg:33.27ms
step:506/2160 train_time:16837ms step_avg:33.27ms
step:507/2160 train_time:16870ms step_avg:33.27ms
step:508/2160 train_time:16903ms step_avg:33.27ms
step:509/2160 train_time:16937ms step_avg:33.27ms
step:510/2160 train_time:16969ms step_avg:33.27ms
step:511/2160 train_time:17002ms step_avg:33.27ms
step:512/2160 train_time:17035ms step_avg:33.27ms
step:513/2160 train_time:17067ms step_avg:33.27ms
step:514/2160 train_time:17100ms step_avg:33.27ms
step:515/2160 train_time:17133ms step_avg:33.27ms
step:516/2160 train_time:17165ms step_avg:33.27ms
step:517/2160 train_time:17198ms step_avg:33.27ms
step:518/2160 train_time:17230ms step_avg:33.26ms
step:519/2160 train_time:17263ms step_avg:33.26ms
step:520/2160 train_time:17296ms step_avg:33.26ms
step:521/2160 train_time:17328ms step_avg:33.26ms
step:522/2160 train_time:17360ms step_avg:33.26ms
step:523/2160 train_time:17393ms step_avg:33.26ms
step:524/2160 train_time:17426ms step_avg:33.25ms
step:525/2160 train_time:17458ms step_avg:33.25ms
step:526/2160 train_time:17490ms step_avg:33.25ms
step:527/2160 train_time:17524ms step_avg:33.25ms
step:528/2160 train_time:17556ms step_avg:33.25ms
step:529/2160 train_time:17589ms step_avg:33.25ms
step:530/2160 train_time:17621ms step_avg:33.25ms
step:531/2160 train_time:17654ms step_avg:33.25ms
step:532/2160 train_time:17687ms step_avg:33.25ms
step:533/2160 train_time:17721ms step_avg:33.25ms
step:534/2160 train_time:17753ms step_avg:33.25ms
step:535/2160 train_time:17787ms step_avg:33.25ms
step:536/2160 train_time:17819ms step_avg:33.24ms
step:537/2160 train_time:17853ms step_avg:33.25ms
step:538/2160 train_time:17886ms step_avg:33.25ms
step:539/2160 train_time:17919ms step_avg:33.25ms
step:540/2160 train_time:17951ms step_avg:33.24ms
step:541/2160 train_time:17985ms step_avg:33.24ms
step:542/2160 train_time:18017ms step_avg:33.24ms
step:543/2160 train_time:18050ms step_avg:33.24ms
step:544/2160 train_time:18083ms step_avg:33.24ms
step:545/2160 train_time:18116ms step_avg:33.24ms
step:546/2160 train_time:18148ms step_avg:33.24ms
step:547/2160 train_time:18181ms step_avg:33.24ms
step:548/2160 train_time:18213ms step_avg:33.24ms
step:549/2160 train_time:18246ms step_avg:33.24ms
step:550/2160 train_time:18279ms step_avg:33.23ms
step:551/2160 train_time:18312ms step_avg:33.23ms
step:552/2160 train_time:18344ms step_avg:33.23ms
step:553/2160 train_time:18377ms step_avg:33.23ms
step:554/2160 train_time:18409ms step_avg:33.23ms
step:555/2160 train_time:18443ms step_avg:33.23ms
step:556/2160 train_time:18475ms step_avg:33.23ms
step:557/2160 train_time:18508ms step_avg:33.23ms
step:558/2160 train_time:18540ms step_avg:33.23ms
step:559/2160 train_time:18573ms step_avg:33.23ms
step:560/2160 train_time:18605ms step_avg:33.22ms
step:561/2160 train_time:18639ms step_avg:33.22ms
step:562/2160 train_time:18671ms step_avg:33.22ms
step:563/2160 train_time:18705ms step_avg:33.22ms
step:564/2160 train_time:18737ms step_avg:33.22ms
step:565/2160 train_time:18770ms step_avg:33.22ms
step:566/2160 train_time:18802ms step_avg:33.22ms
step:567/2160 train_time:18836ms step_avg:33.22ms
step:568/2160 train_time:18868ms step_avg:33.22ms
step:569/2160 train_time:18902ms step_avg:33.22ms
step:570/2160 train_time:18934ms step_avg:33.22ms
step:571/2160 train_time:18967ms step_avg:33.22ms
step:572/2160 train_time:18999ms step_avg:33.22ms
step:573/2160 train_time:19033ms step_avg:33.22ms
step:574/2160 train_time:19065ms step_avg:33.21ms
step:575/2160 train_time:19098ms step_avg:33.21ms
step:576/2160 train_time:19130ms step_avg:33.21ms
step:577/2160 train_time:19164ms step_avg:33.21ms
step:578/2160 train_time:19196ms step_avg:33.21ms
step:579/2160 train_time:19229ms step_avg:33.21ms
step:580/2160 train_time:19261ms step_avg:33.21ms
step:581/2160 train_time:19294ms step_avg:33.21ms
step:582/2160 train_time:19327ms step_avg:33.21ms
step:583/2160 train_time:19360ms step_avg:33.21ms
step:584/2160 train_time:19392ms step_avg:33.21ms
step:585/2160 train_time:19426ms step_avg:33.21ms
step:586/2160 train_time:19458ms step_avg:33.20ms
step:587/2160 train_time:19491ms step_avg:33.20ms
step:588/2160 train_time:19523ms step_avg:33.20ms
step:589/2160 train_time:19556ms step_avg:33.20ms
step:590/2160 train_time:19588ms step_avg:33.20ms
step:591/2160 train_time:19622ms step_avg:33.20ms
step:592/2160 train_time:19654ms step_avg:33.20ms
step:593/2160 train_time:19688ms step_avg:33.20ms
step:594/2160 train_time:19720ms step_avg:33.20ms
step:595/2160 train_time:19754ms step_avg:33.20ms
step:596/2160 train_time:19786ms step_avg:33.20ms
step:597/2160 train_time:19820ms step_avg:33.20ms
step:598/2160 train_time:19852ms step_avg:33.20ms
step:599/2160 train_time:19886ms step_avg:33.20ms
step:600/2160 train_time:19918ms step_avg:33.20ms
step:601/2160 train_time:19951ms step_avg:33.20ms
step:602/2160 train_time:19983ms step_avg:33.19ms
step:603/2160 train_time:20017ms step_avg:33.20ms
step:604/2160 train_time:20050ms step_avg:33.19ms
step:605/2160 train_time:20083ms step_avg:33.19ms
step:606/2160 train_time:20115ms step_avg:33.19ms
step:607/2160 train_time:20148ms step_avg:33.19ms
step:608/2160 train_time:20180ms step_avg:33.19ms
step:609/2160 train_time:20213ms step_avg:33.19ms
step:610/2160 train_time:20246ms step_avg:33.19ms
step:611/2160 train_time:20279ms step_avg:33.19ms
step:612/2160 train_time:20311ms step_avg:33.19ms
step:613/2160 train_time:20345ms step_avg:33.19ms
step:614/2160 train_time:20377ms step_avg:33.19ms
step:615/2160 train_time:20410ms step_avg:33.19ms
step:616/2160 train_time:20442ms step_avg:33.18ms
step:617/2160 train_time:20475ms step_avg:33.19ms
step:618/2160 train_time:20507ms step_avg:33.18ms
step:619/2160 train_time:20541ms step_avg:33.18ms
step:620/2160 train_time:20573ms step_avg:33.18ms
step:621/2160 train_time:20607ms step_avg:33.18ms
step:622/2160 train_time:20639ms step_avg:33.18ms
step:623/2160 train_time:20672ms step_avg:33.18ms
step:624/2160 train_time:20705ms step_avg:33.18ms
step:625/2160 train_time:20738ms step_avg:33.18ms
step:626/2160 train_time:20770ms step_avg:33.18ms
step:627/2160 train_time:20803ms step_avg:33.18ms
step:628/2160 train_time:20836ms step_avg:33.18ms
step:629/2160 train_time:20869ms step_avg:33.18ms
step:630/2160 train_time:20901ms step_avg:33.18ms
step:631/2160 train_time:20934ms step_avg:33.18ms
step:632/2160 train_time:20967ms step_avg:33.18ms
step:633/2160 train_time:21000ms step_avg:33.18ms
step:634/2160 train_time:21033ms step_avg:33.17ms
step:635/2160 train_time:21066ms step_avg:33.17ms
step:636/2160 train_time:21098ms step_avg:33.17ms
step:637/2160 train_time:21131ms step_avg:33.17ms
step:638/2160 train_time:21163ms step_avg:33.17ms
step:639/2160 train_time:21196ms step_avg:33.17ms
step:640/2160 train_time:21229ms step_avg:33.17ms
step:641/2160 train_time:21262ms step_avg:33.17ms
step:642/2160 train_time:21294ms step_avg:33.17ms
step:643/2160 train_time:21327ms step_avg:33.17ms
step:644/2160 train_time:21359ms step_avg:33.17ms
step:645/2160 train_time:21393ms step_avg:33.17ms
step:646/2160 train_time:21425ms step_avg:33.17ms
step:647/2160 train_time:21458ms step_avg:33.17ms
step:648/2160 train_time:21491ms step_avg:33.16ms
step:649/2160 train_time:21524ms step_avg:33.16ms
step:650/2160 train_time:21556ms step_avg:33.16ms
step:651/2160 train_time:21589ms step_avg:33.16ms
step:652/2160 train_time:21621ms step_avg:33.16ms
step:653/2160 train_time:21655ms step_avg:33.16ms
step:654/2160 train_time:21687ms step_avg:33.16ms
step:655/2160 train_time:21721ms step_avg:33.16ms
step:656/2160 train_time:21753ms step_avg:33.16ms
step:657/2160 train_time:21786ms step_avg:33.16ms
step:658/2160 train_time:21819ms step_avg:33.16ms
step:659/2160 train_time:21852ms step_avg:33.16ms
step:660/2160 train_time:21885ms step_avg:33.16ms
step:661/2160 train_time:21918ms step_avg:33.16ms
step:662/2160 train_time:21950ms step_avg:33.16ms
step:663/2160 train_time:21984ms step_avg:33.16ms
step:664/2160 train_time:22016ms step_avg:33.16ms
step:665/2160 train_time:22049ms step_avg:33.16ms
step:666/2160 train_time:22081ms step_avg:33.15ms
step:667/2160 train_time:22115ms step_avg:33.16ms
step:668/2160 train_time:22147ms step_avg:33.15ms
step:669/2160 train_time:22180ms step_avg:33.15ms
step:670/2160 train_time:22213ms step_avg:33.15ms
step:671/2160 train_time:22246ms step_avg:33.15ms
step:672/2160 train_time:22278ms step_avg:33.15ms
step:673/2160 train_time:22312ms step_avg:33.15ms
step:674/2160 train_time:22344ms step_avg:33.15ms
step:675/2160 train_time:22377ms step_avg:33.15ms
step:676/2160 train_time:22409ms step_avg:33.15ms
step:677/2160 train_time:22442ms step_avg:33.15ms
step:678/2160 train_time:22475ms step_avg:33.15ms
step:679/2160 train_time:22508ms step_avg:33.15ms
step:680/2160 train_time:22540ms step_avg:33.15ms
step:681/2160 train_time:22573ms step_avg:33.15ms
step:682/2160 train_time:22605ms step_avg:33.15ms
step:683/2160 train_time:22639ms step_avg:33.15ms
step:684/2160 train_time:22671ms step_avg:33.14ms
step:685/2160 train_time:22704ms step_avg:33.14ms
step:686/2160 train_time:22737ms step_avg:33.14ms
step:687/2160 train_time:22770ms step_avg:33.14ms
step:688/2160 train_time:22802ms step_avg:33.14ms
step:689/2160 train_time:22836ms step_avg:33.14ms
step:690/2160 train_time:22868ms step_avg:33.14ms
step:691/2160 train_time:22902ms step_avg:33.14ms
step:692/2160 train_time:22935ms step_avg:33.14ms
step:693/2160 train_time:22968ms step_avg:33.14ms
step:694/2160 train_time:23000ms step_avg:33.14ms
step:695/2160 train_time:23034ms step_avg:33.14ms
step:696/2160 train_time:23067ms step_avg:33.14ms
step:697/2160 train_time:23100ms step_avg:33.14ms
step:698/2160 train_time:23132ms step_avg:33.14ms
step:699/2160 train_time:23165ms step_avg:33.14ms
step:700/2160 train_time:23198ms step_avg:33.14ms
step:701/2160 train_time:23231ms step_avg:33.14ms
step:702/2160 train_time:23263ms step_avg:33.14ms
step:703/2160 train_time:23296ms step_avg:33.14ms
step:704/2160 train_time:23328ms step_avg:33.14ms
step:705/2160 train_time:23362ms step_avg:33.14ms
step:706/2160 train_time:23394ms step_avg:33.14ms
step:707/2160 train_time:23427ms step_avg:33.14ms
step:708/2160 train_time:23461ms step_avg:33.14ms
step:709/2160 train_time:23519ms step_avg:33.17ms
step:710/2160 train_time:23577ms step_avg:33.21ms
step:711/2160 train_time:23638ms step_avg:33.25ms
step:712/2160 train_time:23696ms step_avg:33.28ms
step:713/2160 train_time:23756ms step_avg:33.32ms
step:714/2160 train_time:23815ms step_avg:33.35ms
step:715/2160 train_time:23876ms step_avg:33.39ms
step:716/2160 train_time:23935ms step_avg:33.43ms
step:717/2160 train_time:23996ms step_avg:33.47ms
step:718/2160 train_time:24056ms step_avg:33.50ms
step:719/2160 train_time:24116ms step_avg:33.54ms
step:720/2160 train_time:24175ms step_avg:33.58ms
step:721/2160 train_time:24235ms step_avg:33.61ms
step:722/2160 train_time:24294ms step_avg:33.65ms
step:723/2160 train_time:24354ms step_avg:33.69ms
step:724/2160 train_time:24413ms step_avg:33.72ms
step:725/2160 train_time:24473ms step_avg:33.76ms
step:726/2160 train_time:24532ms step_avg:33.79ms
step:727/2160 train_time:24592ms step_avg:33.83ms
step:728/2160 train_time:24651ms step_avg:33.86ms
step:729/2160 train_time:24711ms step_avg:33.90ms
step:730/2160 train_time:24770ms step_avg:33.93ms
step:731/2160 train_time:24831ms step_avg:33.97ms
step:732/2160 train_time:24890ms step_avg:34.00ms
step:733/2160 train_time:24951ms step_avg:34.04ms
step:734/2160 train_time:25010ms step_avg:34.07ms
step:735/2160 train_time:25071ms step_avg:34.11ms
step:736/2160 train_time:25130ms step_avg:34.14ms
step:737/2160 train_time:25191ms step_avg:34.18ms
step:738/2160 train_time:25250ms step_avg:34.21ms
step:739/2160 train_time:25311ms step_avg:34.25ms
step:740/2160 train_time:25370ms step_avg:34.28ms
step:741/2160 train_time:25430ms step_avg:34.32ms
step:742/2160 train_time:25488ms step_avg:34.35ms
step:743/2160 train_time:25549ms step_avg:34.39ms
step:744/2160 train_time:25607ms step_avg:34.42ms
step:745/2160 train_time:25668ms step_avg:34.45ms
step:746/2160 train_time:25727ms step_avg:34.49ms
step:747/2160 train_time:25788ms step_avg:34.52ms
step:748/2160 train_time:25848ms step_avg:34.56ms
step:749/2160 train_time:25909ms step_avg:34.59ms
step:750/2160 train_time:25967ms step_avg:34.62ms
step:750/2160 val_loss:3.8891 train_time:26030ms step_avg:34.71ms
step:751/2160 train_time:26052ms step_avg:34.69ms
step:752/2160 train_time:26088ms step_avg:34.69ms
step:753/2160 train_time:26151ms step_avg:34.73ms
step:754/2160 train_time:26214ms step_avg:34.77ms
step:755/2160 train_time:26276ms step_avg:34.80ms
step:756/2160 train_time:26335ms step_avg:34.84ms
step:757/2160 train_time:26396ms step_avg:34.87ms
step:758/2160 train_time:26454ms step_avg:34.90ms
step:759/2160 train_time:26514ms step_avg:34.93ms
step:760/2160 train_time:26573ms step_avg:34.96ms
step:761/2160 train_time:26633ms step_avg:35.00ms
step:762/2160 train_time:26691ms step_avg:35.03ms
step:763/2160 train_time:26750ms step_avg:35.06ms
step:764/2160 train_time:26808ms step_avg:35.09ms
step:765/2160 train_time:26867ms step_avg:35.12ms
step:766/2160 train_time:26926ms step_avg:35.15ms
step:767/2160 train_time:26987ms step_avg:35.18ms
step:768/2160 train_time:27045ms step_avg:35.21ms
step:769/2160 train_time:27107ms step_avg:35.25ms
step:770/2160 train_time:27167ms step_avg:35.28ms
step:771/2160 train_time:27228ms step_avg:35.32ms
step:772/2160 train_time:27287ms step_avg:35.35ms
step:773/2160 train_time:27347ms step_avg:35.38ms
step:774/2160 train_time:27406ms step_avg:35.41ms
step:775/2160 train_time:27466ms step_avg:35.44ms
step:776/2160 train_time:27524ms step_avg:35.47ms
step:777/2160 train_time:27584ms step_avg:35.50ms
step:778/2160 train_time:27643ms step_avg:35.53ms
step:779/2160 train_time:27703ms step_avg:35.56ms
step:780/2160 train_time:27761ms step_avg:35.59ms
step:781/2160 train_time:27821ms step_avg:35.62ms
step:782/2160 train_time:27880ms step_avg:35.65ms
step:783/2160 train_time:27941ms step_avg:35.68ms
step:784/2160 train_time:28000ms step_avg:35.71ms
step:785/2160 train_time:28061ms step_avg:35.75ms
step:786/2160 train_time:28121ms step_avg:35.78ms
step:787/2160 train_time:28184ms step_avg:35.81ms
step:788/2160 train_time:28243ms step_avg:35.84ms
step:789/2160 train_time:28304ms step_avg:35.87ms
step:790/2160 train_time:28362ms step_avg:35.90ms
step:791/2160 train_time:28422ms step_avg:35.93ms
step:792/2160 train_time:28482ms step_avg:35.96ms
step:793/2160 train_time:28542ms step_avg:35.99ms
step:794/2160 train_time:28602ms step_avg:36.02ms
step:795/2160 train_time:28661ms step_avg:36.05ms
step:796/2160 train_time:28720ms step_avg:36.08ms
step:797/2160 train_time:28780ms step_avg:36.11ms
step:798/2160 train_time:28839ms step_avg:36.14ms
step:799/2160 train_time:28900ms step_avg:36.17ms
step:800/2160 train_time:28959ms step_avg:36.20ms
step:801/2160 train_time:29019ms step_avg:36.23ms
step:802/2160 train_time:29080ms step_avg:36.26ms
step:803/2160 train_time:29141ms step_avg:36.29ms
step:804/2160 train_time:29201ms step_avg:36.32ms
step:805/2160 train_time:29262ms step_avg:36.35ms
step:806/2160 train_time:29322ms step_avg:36.38ms
step:807/2160 train_time:29383ms step_avg:36.41ms
step:808/2160 train_time:29442ms step_avg:36.44ms
step:809/2160 train_time:29502ms step_avg:36.47ms
step:810/2160 train_time:29561ms step_avg:36.50ms
step:811/2160 train_time:29621ms step_avg:36.52ms
step:812/2160 train_time:29679ms step_avg:36.55ms
step:813/2160 train_time:29739ms step_avg:36.58ms
step:814/2160 train_time:29798ms step_avg:36.61ms
step:815/2160 train_time:29858ms step_avg:36.64ms
step:816/2160 train_time:29917ms step_avg:36.66ms
step:817/2160 train_time:29978ms step_avg:36.69ms
step:818/2160 train_time:30038ms step_avg:36.72ms
step:819/2160 train_time:30098ms step_avg:36.75ms
step:820/2160 train_time:30158ms step_avg:36.78ms
step:821/2160 train_time:30219ms step_avg:36.81ms
step:822/2160 train_time:30278ms step_avg:36.83ms
step:823/2160 train_time:30340ms step_avg:36.86ms
step:824/2160 train_time:30399ms step_avg:36.89ms
step:825/2160 train_time:30460ms step_avg:36.92ms
step:826/2160 train_time:30520ms step_avg:36.95ms
step:827/2160 train_time:30580ms step_avg:36.98ms
step:828/2160 train_time:30639ms step_avg:37.00ms
step:829/2160 train_time:30699ms step_avg:37.03ms
step:830/2160 train_time:30757ms step_avg:37.06ms
step:831/2160 train_time:30817ms step_avg:37.08ms
step:832/2160 train_time:30876ms step_avg:37.11ms
step:833/2160 train_time:30936ms step_avg:37.14ms
step:834/2160 train_time:30995ms step_avg:37.16ms
step:835/2160 train_time:31056ms step_avg:37.19ms
step:836/2160 train_time:31115ms step_avg:37.22ms
step:837/2160 train_time:31176ms step_avg:37.25ms
step:838/2160 train_time:31236ms step_avg:37.27ms
step:839/2160 train_time:31296ms step_avg:37.30ms
step:840/2160 train_time:31356ms step_avg:37.33ms
step:841/2160 train_time:31417ms step_avg:37.36ms
step:842/2160 train_time:31476ms step_avg:37.38ms
step:843/2160 train_time:31537ms step_avg:37.41ms
step:844/2160 train_time:31596ms step_avg:37.44ms
step:845/2160 train_time:31656ms step_avg:37.46ms
step:846/2160 train_time:31715ms step_avg:37.49ms
step:847/2160 train_time:31775ms step_avg:37.52ms
step:848/2160 train_time:31834ms step_avg:37.54ms
step:849/2160 train_time:31895ms step_avg:37.57ms
step:850/2160 train_time:31953ms step_avg:37.59ms
step:851/2160 train_time:32013ms step_avg:37.62ms
step:852/2160 train_time:32072ms step_avg:37.64ms
step:853/2160 train_time:32132ms step_avg:37.67ms
step:854/2160 train_time:32191ms step_avg:37.69ms
step:855/2160 train_time:32252ms step_avg:37.72ms
step:856/2160 train_time:32311ms step_avg:37.75ms
step:857/2160 train_time:32372ms step_avg:37.77ms
step:858/2160 train_time:32431ms step_avg:37.80ms
step:859/2160 train_time:32491ms step_avg:37.82ms
step:860/2160 train_time:32550ms step_avg:37.85ms
step:861/2160 train_time:32610ms step_avg:37.87ms
step:862/2160 train_time:32669ms step_avg:37.90ms
step:863/2160 train_time:32728ms step_avg:37.92ms
step:864/2160 train_time:32786ms step_avg:37.95ms
step:865/2160 train_time:32846ms step_avg:37.97ms
step:866/2160 train_time:32904ms step_avg:38.00ms
step:867/2160 train_time:32964ms step_avg:38.02ms
step:868/2160 train_time:33023ms step_avg:38.05ms
step:869/2160 train_time:33083ms step_avg:38.07ms
step:870/2160 train_time:33142ms step_avg:38.09ms
step:871/2160 train_time:33203ms step_avg:38.12ms
step:872/2160 train_time:33262ms step_avg:38.14ms
step:873/2160 train_time:33322ms step_avg:38.17ms
step:874/2160 train_time:33382ms step_avg:38.19ms
step:875/2160 train_time:33442ms step_avg:38.22ms
step:876/2160 train_time:33502ms step_avg:38.24ms
step:877/2160 train_time:33562ms step_avg:38.27ms
step:878/2160 train_time:33621ms step_avg:38.29ms
step:879/2160 train_time:33683ms step_avg:38.32ms
step:880/2160 train_time:33741ms step_avg:38.34ms
step:881/2160 train_time:33802ms step_avg:38.37ms
step:882/2160 train_time:33861ms step_avg:38.39ms
step:883/2160 train_time:33921ms step_avg:38.42ms
step:884/2160 train_time:33980ms step_avg:38.44ms
step:885/2160 train_time:34041ms step_avg:38.46ms
step:886/2160 train_time:34100ms step_avg:38.49ms
step:887/2160 train_time:34161ms step_avg:38.51ms
step:888/2160 train_time:34221ms step_avg:38.54ms
step:889/2160 train_time:34281ms step_avg:38.56ms
step:890/2160 train_time:34341ms step_avg:38.59ms
step:891/2160 train_time:34401ms step_avg:38.61ms
step:892/2160 train_time:34460ms step_avg:38.63ms
step:893/2160 train_time:34520ms step_avg:38.66ms
step:894/2160 train_time:34579ms step_avg:38.68ms
step:895/2160 train_time:34640ms step_avg:38.70ms
step:896/2160 train_time:34699ms step_avg:38.73ms
step:897/2160 train_time:34759ms step_avg:38.75ms
step:898/2160 train_time:34819ms step_avg:38.77ms
step:899/2160 train_time:34879ms step_avg:38.80ms
step:900/2160 train_time:34938ms step_avg:38.82ms
step:901/2160 train_time:34998ms step_avg:38.84ms
step:902/2160 train_time:35058ms step_avg:38.87ms
step:903/2160 train_time:35119ms step_avg:38.89ms
step:904/2160 train_time:35178ms step_avg:38.91ms
step:905/2160 train_time:35239ms step_avg:38.94ms
step:906/2160 train_time:35298ms step_avg:38.96ms
step:907/2160 train_time:35359ms step_avg:38.98ms
step:908/2160 train_time:35419ms step_avg:39.01ms
step:909/2160 train_time:35480ms step_avg:39.03ms
step:910/2160 train_time:35539ms step_avg:39.05ms
step:911/2160 train_time:35599ms step_avg:39.08ms
step:912/2160 train_time:35658ms step_avg:39.10ms
step:913/2160 train_time:35719ms step_avg:39.12ms
step:914/2160 train_time:35778ms step_avg:39.14ms
step:915/2160 train_time:35839ms step_avg:39.17ms
step:916/2160 train_time:35898ms step_avg:39.19ms
step:917/2160 train_time:35958ms step_avg:39.21ms
step:918/2160 train_time:36017ms step_avg:39.23ms
step:919/2160 train_time:36078ms step_avg:39.26ms
step:920/2160 train_time:36137ms step_avg:39.28ms
step:921/2160 train_time:36198ms step_avg:39.30ms
step:922/2160 train_time:36257ms step_avg:39.32ms
step:923/2160 train_time:36318ms step_avg:39.35ms
step:924/2160 train_time:36378ms step_avg:39.37ms
step:925/2160 train_time:36439ms step_avg:39.39ms
step:926/2160 train_time:36498ms step_avg:39.41ms
step:927/2160 train_time:36559ms step_avg:39.44ms
step:928/2160 train_time:36618ms step_avg:39.46ms
step:929/2160 train_time:36678ms step_avg:39.48ms
step:930/2160 train_time:36737ms step_avg:39.50ms
step:931/2160 train_time:36798ms step_avg:39.52ms
step:932/2160 train_time:36857ms step_avg:39.55ms
step:933/2160 train_time:36917ms step_avg:39.57ms
step:934/2160 train_time:36976ms step_avg:39.59ms
step:935/2160 train_time:37037ms step_avg:39.61ms
step:936/2160 train_time:37096ms step_avg:39.63ms
step:937/2160 train_time:37156ms step_avg:39.65ms
step:938/2160 train_time:37216ms step_avg:39.68ms
step:939/2160 train_time:37277ms step_avg:39.70ms
step:940/2160 train_time:37336ms step_avg:39.72ms
step:941/2160 train_time:37396ms step_avg:39.74ms
step:942/2160 train_time:37456ms step_avg:39.76ms
step:943/2160 train_time:37517ms step_avg:39.78ms
step:944/2160 train_time:37576ms step_avg:39.81ms
step:945/2160 train_time:37637ms step_avg:39.83ms
step:946/2160 train_time:37695ms step_avg:39.85ms
step:947/2160 train_time:37755ms step_avg:39.87ms
step:948/2160 train_time:37814ms step_avg:39.89ms
step:949/2160 train_time:37875ms step_avg:39.91ms
step:950/2160 train_time:37934ms step_avg:39.93ms
step:951/2160 train_time:37995ms step_avg:39.95ms
step:952/2160 train_time:38054ms step_avg:39.97ms
step:953/2160 train_time:38115ms step_avg:40.00ms
step:954/2160 train_time:38175ms step_avg:40.02ms
step:955/2160 train_time:38236ms step_avg:40.04ms
step:956/2160 train_time:38295ms step_avg:40.06ms
step:957/2160 train_time:38356ms step_avg:40.08ms
step:958/2160 train_time:38415ms step_avg:40.10ms
step:959/2160 train_time:38476ms step_avg:40.12ms
step:960/2160 train_time:38535ms step_avg:40.14ms
step:961/2160 train_time:38596ms step_avg:40.16ms
step:962/2160 train_time:38655ms step_avg:40.18ms
step:963/2160 train_time:38716ms step_avg:40.20ms
step:964/2160 train_time:38775ms step_avg:40.22ms
step:965/2160 train_time:38835ms step_avg:40.24ms
step:966/2160 train_time:38894ms step_avg:40.26ms
step:967/2160 train_time:38955ms step_avg:40.28ms
step:968/2160 train_time:39014ms step_avg:40.30ms
step:969/2160 train_time:39075ms step_avg:40.33ms
step:970/2160 train_time:39135ms step_avg:40.34ms
step:971/2160 train_time:39196ms step_avg:40.37ms
step:972/2160 train_time:39255ms step_avg:40.39ms
step:973/2160 train_time:39315ms step_avg:40.41ms
step:974/2160 train_time:39375ms step_avg:40.43ms
step:975/2160 train_time:39436ms step_avg:40.45ms
step:976/2160 train_time:39495ms step_avg:40.47ms
step:977/2160 train_time:39556ms step_avg:40.49ms
step:978/2160 train_time:39615ms step_avg:40.51ms
step:979/2160 train_time:39675ms step_avg:40.53ms
step:980/2160 train_time:39733ms step_avg:40.54ms
step:981/2160 train_time:39795ms step_avg:40.57ms
step:982/2160 train_time:39854ms step_avg:40.58ms
step:983/2160 train_time:39916ms step_avg:40.61ms
step:984/2160 train_time:39975ms step_avg:40.63ms
step:985/2160 train_time:40036ms step_avg:40.65ms
step:986/2160 train_time:40096ms step_avg:40.66ms
step:987/2160 train_time:40156ms step_avg:40.69ms
step:988/2160 train_time:40215ms step_avg:40.70ms
step:989/2160 train_time:40276ms step_avg:40.72ms
step:990/2160 train_time:40335ms step_avg:40.74ms
step:991/2160 train_time:40397ms step_avg:40.76ms
step:992/2160 train_time:40455ms step_avg:40.78ms
step:993/2160 train_time:40516ms step_avg:40.80ms
step:994/2160 train_time:40575ms step_avg:40.82ms
step:995/2160 train_time:40635ms step_avg:40.84ms
step:996/2160 train_time:40694ms step_avg:40.86ms
step:997/2160 train_time:40755ms step_avg:40.88ms
step:998/2160 train_time:40814ms step_avg:40.90ms
step:999/2160 train_time:40875ms step_avg:40.92ms
step:1000/2160 train_time:40934ms step_avg:40.93ms
step:1000/2160 val_loss:3.7075 train_time:40997ms step_avg:41.00ms
step:1001/2160 train_time:41019ms step_avg:40.98ms
step:1002/2160 train_time:41056ms step_avg:40.97ms
step:1003/2160 train_time:41118ms step_avg:40.99ms
step:1004/2160 train_time:41181ms step_avg:41.02ms
step:1005/2160 train_time:41246ms step_avg:41.04ms
step:1006/2160 train_time:41305ms step_avg:41.06ms
step:1007/2160 train_time:41366ms step_avg:41.08ms
step:1008/2160 train_time:41424ms step_avg:41.10ms
step:1009/2160 train_time:41484ms step_avg:41.11ms
step:1010/2160 train_time:41543ms step_avg:41.13ms
step:1011/2160 train_time:41602ms step_avg:41.15ms
step:1012/2160 train_time:41661ms step_avg:41.17ms
step:1013/2160 train_time:41721ms step_avg:41.19ms
step:1014/2160 train_time:41780ms step_avg:41.20ms
step:1015/2160 train_time:41839ms step_avg:41.22ms
step:1016/2160 train_time:41898ms step_avg:41.24ms
step:1017/2160 train_time:41959ms step_avg:41.26ms
step:1018/2160 train_time:42019ms step_avg:41.28ms
step:1019/2160 train_time:42081ms step_avg:41.30ms
step:1020/2160 train_time:42144ms step_avg:41.32ms
step:1021/2160 train_time:42208ms step_avg:41.34ms
step:1022/2160 train_time:42268ms step_avg:41.36ms
step:1023/2160 train_time:42330ms step_avg:41.38ms
step:1024/2160 train_time:42388ms step_avg:41.39ms
step:1025/2160 train_time:42448ms step_avg:41.41ms
step:1026/2160 train_time:42506ms step_avg:41.43ms
step:1027/2160 train_time:42566ms step_avg:41.45ms
step:1028/2160 train_time:42624ms step_avg:41.46ms
step:1029/2160 train_time:42685ms step_avg:41.48ms
step:1030/2160 train_time:42744ms step_avg:41.50ms
step:1031/2160 train_time:42804ms step_avg:41.52ms
step:1032/2160 train_time:42863ms step_avg:41.53ms
step:1033/2160 train_time:42923ms step_avg:41.55ms
step:1034/2160 train_time:42983ms step_avg:41.57ms
step:1035/2160 train_time:43045ms step_avg:41.59ms
step:1036/2160 train_time:43106ms step_avg:41.61ms
step:1037/2160 train_time:43169ms step_avg:41.63ms
step:1038/2160 train_time:43229ms step_avg:41.65ms
step:1039/2160 train_time:43290ms step_avg:41.67ms
step:1040/2160 train_time:43349ms step_avg:41.68ms
step:1041/2160 train_time:43408ms step_avg:41.70ms
step:1042/2160 train_time:43467ms step_avg:41.72ms
step:1043/2160 train_time:43526ms step_avg:41.73ms
step:1044/2160 train_time:43585ms step_avg:41.75ms
step:1045/2160 train_time:43645ms step_avg:41.77ms
step:1046/2160 train_time:43704ms step_avg:41.78ms
step:1047/2160 train_time:43764ms step_avg:41.80ms
step:1048/2160 train_time:43823ms step_avg:41.82ms
step:1049/2160 train_time:43884ms step_avg:41.83ms
step:1050/2160 train_time:43943ms step_avg:41.85ms
step:1051/2160 train_time:44005ms step_avg:41.87ms
step:1052/2160 train_time:44065ms step_avg:41.89ms
step:1053/2160 train_time:44128ms step_avg:41.91ms
step:1054/2160 train_time:44188ms step_avg:41.92ms
step:1055/2160 train_time:44250ms step_avg:41.94ms
step:1056/2160 train_time:44309ms step_avg:41.96ms
step:1057/2160 train_time:44369ms step_avg:41.98ms
step:1058/2160 train_time:44428ms step_avg:41.99ms
step:1059/2160 train_time:44488ms step_avg:42.01ms
step:1060/2160 train_time:44546ms step_avg:42.02ms
step:1061/2160 train_time:44606ms step_avg:42.04ms
step:1062/2160 train_time:44665ms step_avg:42.06ms
step:1063/2160 train_time:44725ms step_avg:42.07ms
step:1064/2160 train_time:44784ms step_avg:42.09ms
step:1065/2160 train_time:44845ms step_avg:42.11ms
step:1066/2160 train_time:44904ms step_avg:42.12ms
step:1067/2160 train_time:44965ms step_avg:42.14ms
step:1068/2160 train_time:45025ms step_avg:42.16ms
step:1069/2160 train_time:45088ms step_avg:42.18ms
step:1070/2160 train_time:45147ms step_avg:42.19ms
step:1071/2160 train_time:45209ms step_avg:42.21ms
step:1072/2160 train_time:45269ms step_avg:42.23ms
step:1073/2160 train_time:45329ms step_avg:42.25ms
step:1074/2160 train_time:45389ms step_avg:42.26ms
step:1075/2160 train_time:45448ms step_avg:42.28ms
step:1076/2160 train_time:45507ms step_avg:42.29ms
step:1077/2160 train_time:45567ms step_avg:42.31ms
step:1078/2160 train_time:45626ms step_avg:42.32ms
step:1079/2160 train_time:45687ms step_avg:42.34ms
step:1080/2160 train_time:45745ms step_avg:42.36ms
step:1081/2160 train_time:45806ms step_avg:42.37ms
step:1082/2160 train_time:45865ms step_avg:42.39ms
step:1083/2160 train_time:45926ms step_avg:42.41ms
step:1084/2160 train_time:45986ms step_avg:42.42ms
step:1085/2160 train_time:46047ms step_avg:42.44ms
step:1086/2160 train_time:46107ms step_avg:42.46ms
step:1087/2160 train_time:46169ms step_avg:42.47ms
step:1088/2160 train_time:46228ms step_avg:42.49ms
step:1089/2160 train_time:46289ms step_avg:42.51ms
step:1090/2160 train_time:46348ms step_avg:42.52ms
step:1091/2160 train_time:46408ms step_avg:42.54ms
step:1092/2160 train_time:46467ms step_avg:42.55ms
step:1093/2160 train_time:46528ms step_avg:42.57ms
step:1094/2160 train_time:46587ms step_avg:42.58ms
step:1095/2160 train_time:46647ms step_avg:42.60ms
step:1096/2160 train_time:46706ms step_avg:42.61ms
step:1097/2160 train_time:46765ms step_avg:42.63ms
step:1098/2160 train_time:46824ms step_avg:42.65ms
step:1099/2160 train_time:46885ms step_avg:42.66ms
step:1100/2160 train_time:46945ms step_avg:42.68ms
step:1101/2160 train_time:47005ms step_avg:42.69ms
step:1102/2160 train_time:47065ms step_avg:42.71ms
step:1103/2160 train_time:47127ms step_avg:42.73ms
step:1104/2160 train_time:47187ms step_avg:42.74ms
step:1105/2160 train_time:47248ms step_avg:42.76ms
step:1106/2160 train_time:47307ms step_avg:42.77ms
step:1107/2160 train_time:47368ms step_avg:42.79ms
step:1108/2160 train_time:47427ms step_avg:42.80ms
step:1109/2160 train_time:47488ms step_avg:42.82ms
step:1110/2160 train_time:47547ms step_avg:42.84ms
step:1111/2160 train_time:47607ms step_avg:42.85ms
step:1112/2160 train_time:47666ms step_avg:42.86ms
step:1113/2160 train_time:47726ms step_avg:42.88ms
step:1114/2160 train_time:47786ms step_avg:42.90ms
step:1115/2160 train_time:47846ms step_avg:42.91ms
step:1116/2160 train_time:47905ms step_avg:42.93ms
step:1117/2160 train_time:47967ms step_avg:42.94ms
step:1118/2160 train_time:48027ms step_avg:42.96ms
step:1119/2160 train_time:48088ms step_avg:42.97ms
step:1120/2160 train_time:48148ms step_avg:42.99ms
step:1121/2160 train_time:48209ms step_avg:43.01ms
step:1122/2160 train_time:48269ms step_avg:43.02ms
step:1123/2160 train_time:48330ms step_avg:43.04ms
step:1124/2160 train_time:48389ms step_avg:43.05ms
step:1125/2160 train_time:48449ms step_avg:43.07ms
step:1126/2160 train_time:48508ms step_avg:43.08ms
step:1127/2160 train_time:48568ms step_avg:43.10ms
step:1128/2160 train_time:48628ms step_avg:43.11ms
step:1129/2160 train_time:48687ms step_avg:43.12ms
step:1130/2160 train_time:48747ms step_avg:43.14ms
step:1131/2160 train_time:48807ms step_avg:43.15ms
step:1132/2160 train_time:48866ms step_avg:43.17ms
step:1133/2160 train_time:48929ms step_avg:43.18ms
step:1134/2160 train_time:48988ms step_avg:43.20ms
step:1135/2160 train_time:49049ms step_avg:43.21ms
step:1136/2160 train_time:49108ms step_avg:43.23ms
step:1137/2160 train_time:49169ms step_avg:43.24ms
step:1138/2160 train_time:49229ms step_avg:43.26ms
step:1139/2160 train_time:49290ms step_avg:43.27ms
step:1140/2160 train_time:49350ms step_avg:43.29ms
step:1141/2160 train_time:49410ms step_avg:43.30ms
step:1142/2160 train_time:49469ms step_avg:43.32ms
step:1143/2160 train_time:49528ms step_avg:43.33ms
step:1144/2160 train_time:49587ms step_avg:43.35ms
step:1145/2160 train_time:49647ms step_avg:43.36ms
step:1146/2160 train_time:49707ms step_avg:43.37ms
step:1147/2160 train_time:49767ms step_avg:43.39ms
step:1148/2160 train_time:49826ms step_avg:43.40ms
step:1149/2160 train_time:49887ms step_avg:43.42ms
step:1150/2160 train_time:49947ms step_avg:43.43ms
step:1151/2160 train_time:50007ms step_avg:43.45ms
step:1152/2160 train_time:50066ms step_avg:43.46ms
step:1153/2160 train_time:50129ms step_avg:43.48ms
step:1154/2160 train_time:50188ms step_avg:43.49ms
step:1155/2160 train_time:50249ms step_avg:43.51ms
step:1156/2160 train_time:50308ms step_avg:43.52ms
step:1157/2160 train_time:50369ms step_avg:43.53ms
step:1158/2160 train_time:50428ms step_avg:43.55ms
step:1159/2160 train_time:50488ms step_avg:43.56ms
step:1160/2160 train_time:50547ms step_avg:43.58ms
step:1161/2160 train_time:50607ms step_avg:43.59ms
step:1162/2160 train_time:50666ms step_avg:43.60ms
step:1163/2160 train_time:50727ms step_avg:43.62ms
step:1164/2160 train_time:50786ms step_avg:43.63ms
step:1165/2160 train_time:50846ms step_avg:43.64ms
step:1166/2160 train_time:50906ms step_avg:43.66ms
step:1167/2160 train_time:50966ms step_avg:43.67ms
step:1168/2160 train_time:51027ms step_avg:43.69ms
step:1169/2160 train_time:51088ms step_avg:43.70ms
step:1170/2160 train_time:51148ms step_avg:43.72ms
step:1171/2160 train_time:51208ms step_avg:43.73ms
step:1172/2160 train_time:51268ms step_avg:43.74ms
step:1173/2160 train_time:51328ms step_avg:43.76ms
step:1174/2160 train_time:51388ms step_avg:43.77ms
step:1175/2160 train_time:51448ms step_avg:43.79ms
step:1176/2160 train_time:51508ms step_avg:43.80ms
step:1177/2160 train_time:51568ms step_avg:43.81ms
step:1178/2160 train_time:51627ms step_avg:43.83ms
step:1179/2160 train_time:51687ms step_avg:43.84ms
step:1180/2160 train_time:51746ms step_avg:43.85ms
step:1181/2160 train_time:51806ms step_avg:43.87ms
step:1182/2160 train_time:51865ms step_avg:43.88ms
step:1183/2160 train_time:51926ms step_avg:43.89ms
step:1184/2160 train_time:51986ms step_avg:43.91ms
step:1185/2160 train_time:52048ms step_avg:43.92ms
step:1186/2160 train_time:52107ms step_avg:43.93ms
step:1187/2160 train_time:52168ms step_avg:43.95ms
step:1188/2160 train_time:52227ms step_avg:43.96ms
step:1189/2160 train_time:52288ms step_avg:43.98ms
step:1190/2160 train_time:52348ms step_avg:43.99ms
step:1191/2160 train_time:52408ms step_avg:44.00ms
step:1192/2160 train_time:52467ms step_avg:44.02ms
step:1193/2160 train_time:52528ms step_avg:44.03ms
step:1194/2160 train_time:52587ms step_avg:44.04ms
step:1195/2160 train_time:52648ms step_avg:44.06ms
step:1196/2160 train_time:52708ms step_avg:44.07ms
step:1197/2160 train_time:52768ms step_avg:44.08ms
step:1198/2160 train_time:52827ms step_avg:44.10ms
step:1199/2160 train_time:52887ms step_avg:44.11ms
step:1200/2160 train_time:52947ms step_avg:44.12ms
step:1201/2160 train_time:53008ms step_avg:44.14ms
step:1202/2160 train_time:53067ms step_avg:44.15ms
step:1203/2160 train_time:53128ms step_avg:44.16ms
step:1204/2160 train_time:53188ms step_avg:44.18ms
step:1205/2160 train_time:53248ms step_avg:44.19ms
step:1206/2160 train_time:53307ms step_avg:44.20ms
step:1207/2160 train_time:53367ms step_avg:44.21ms
step:1208/2160 train_time:53426ms step_avg:44.23ms
step:1209/2160 train_time:53488ms step_avg:44.24ms
step:1210/2160 train_time:53547ms step_avg:44.25ms
step:1211/2160 train_time:53608ms step_avg:44.27ms
step:1212/2160 train_time:53667ms step_avg:44.28ms
step:1213/2160 train_time:53728ms step_avg:44.29ms
step:1214/2160 train_time:53787ms step_avg:44.31ms
step:1215/2160 train_time:53848ms step_avg:44.32ms
step:1216/2160 train_time:53908ms step_avg:44.33ms
step:1217/2160 train_time:53968ms step_avg:44.35ms
step:1218/2160 train_time:54028ms step_avg:44.36ms
step:1219/2160 train_time:54088ms step_avg:44.37ms
step:1220/2160 train_time:54149ms step_avg:44.38ms
step:1221/2160 train_time:54209ms step_avg:44.40ms
step:1222/2160 train_time:54268ms step_avg:44.41ms
step:1223/2160 train_time:54329ms step_avg:44.42ms
step:1224/2160 train_time:54387ms step_avg:44.43ms
step:1225/2160 train_time:54448ms step_avg:44.45ms
step:1226/2160 train_time:54507ms step_avg:44.46ms
step:1227/2160 train_time:54567ms step_avg:44.47ms
step:1228/2160 train_time:54626ms step_avg:44.48ms
step:1229/2160 train_time:54688ms step_avg:44.50ms
step:1230/2160 train_time:54747ms step_avg:44.51ms
step:1231/2160 train_time:54808ms step_avg:44.52ms
step:1232/2160 train_time:54867ms step_avg:44.53ms
step:1233/2160 train_time:54927ms step_avg:44.55ms
step:1234/2160 train_time:54986ms step_avg:44.56ms
step:1235/2160 train_time:55047ms step_avg:44.57ms
step:1236/2160 train_time:55107ms step_avg:44.58ms
step:1237/2160 train_time:55168ms step_avg:44.60ms
step:1238/2160 train_time:55227ms step_avg:44.61ms
step:1239/2160 train_time:55289ms step_avg:44.62ms
step:1240/2160 train_time:55348ms step_avg:44.64ms
step:1241/2160 train_time:55408ms step_avg:44.65ms
step:1242/2160 train_time:55467ms step_avg:44.66ms
step:1243/2160 train_time:55528ms step_avg:44.67ms
step:1244/2160 train_time:55587ms step_avg:44.68ms
step:1245/2160 train_time:55648ms step_avg:44.70ms
step:1246/2160 train_time:55707ms step_avg:44.71ms
step:1247/2160 train_time:55767ms step_avg:44.72ms
step:1248/2160 train_time:55827ms step_avg:44.73ms
step:1249/2160 train_time:55888ms step_avg:44.75ms
step:1250/2160 train_time:55948ms step_avg:44.76ms
step:1250/2160 val_loss:3.5908 train_time:56011ms step_avg:44.81ms
step:1251/2160 train_time:56033ms step_avg:44.79ms
step:1252/2160 train_time:56071ms step_avg:44.79ms
step:1253/2160 train_time:56135ms step_avg:44.80ms
step:1254/2160 train_time:56196ms step_avg:44.81ms
step:1255/2160 train_time:56256ms step_avg:44.83ms
step:1256/2160 train_time:56314ms step_avg:44.84ms
step:1257/2160 train_time:56374ms step_avg:44.85ms
step:1258/2160 train_time:56431ms step_avg:44.86ms
step:1259/2160 train_time:56491ms step_avg:44.87ms
step:1260/2160 train_time:56549ms step_avg:44.88ms
step:1261/2160 train_time:56609ms step_avg:44.89ms
step:1262/2160 train_time:56667ms step_avg:44.90ms
step:1263/2160 train_time:56726ms step_avg:44.91ms
step:1264/2160 train_time:56785ms step_avg:44.92ms
step:1265/2160 train_time:56845ms step_avg:44.94ms
step:1266/2160 train_time:56904ms step_avg:44.95ms
step:1267/2160 train_time:56966ms step_avg:44.96ms
step:1268/2160 train_time:57027ms step_avg:44.97ms
step:1269/2160 train_time:57090ms step_avg:44.99ms
step:1270/2160 train_time:57152ms step_avg:45.00ms
step:1271/2160 train_time:57212ms step_avg:45.01ms
step:1272/2160 train_time:57271ms step_avg:45.02ms
step:1273/2160 train_time:57331ms step_avg:45.04ms
step:1274/2160 train_time:57390ms step_avg:45.05ms
step:1275/2160 train_time:57449ms step_avg:45.06ms
step:1276/2160 train_time:57507ms step_avg:45.07ms
step:1277/2160 train_time:57568ms step_avg:45.08ms
step:1278/2160 train_time:57626ms step_avg:45.09ms
step:1279/2160 train_time:57685ms step_avg:45.10ms
step:1280/2160 train_time:57743ms step_avg:45.11ms
step:1281/2160 train_time:57804ms step_avg:45.12ms
step:1282/2160 train_time:57864ms step_avg:45.14ms
step:1283/2160 train_time:57924ms step_avg:45.15ms
step:1284/2160 train_time:57984ms step_avg:45.16ms
step:1285/2160 train_time:58048ms step_avg:45.17ms
step:1286/2160 train_time:58109ms step_avg:45.19ms
step:1287/2160 train_time:58170ms step_avg:45.20ms
step:1288/2160 train_time:58230ms step_avg:45.21ms
step:1289/2160 train_time:58290ms step_avg:45.22ms
step:1290/2160 train_time:58349ms step_avg:45.23ms
step:1291/2160 train_time:58408ms step_avg:45.24ms
step:1292/2160 train_time:58467ms step_avg:45.25ms
step:1293/2160 train_time:58527ms step_avg:45.26ms
step:1294/2160 train_time:58586ms step_avg:45.28ms
step:1295/2160 train_time:58646ms step_avg:45.29ms
step:1296/2160 train_time:58705ms step_avg:45.30ms
step:1297/2160 train_time:58764ms step_avg:45.31ms
step:1298/2160 train_time:58824ms step_avg:45.32ms
step:1299/2160 train_time:58885ms step_avg:45.33ms
step:1300/2160 train_time:58944ms step_avg:45.34ms
step:1301/2160 train_time:59005ms step_avg:45.35ms
step:1302/2160 train_time:59067ms step_avg:45.37ms
step:1303/2160 train_time:59129ms step_avg:45.38ms
step:1304/2160 train_time:59188ms step_avg:45.39ms
step:1305/2160 train_time:59249ms step_avg:45.40ms
step:1306/2160 train_time:59308ms step_avg:45.41ms
step:1307/2160 train_time:59369ms step_avg:45.42ms
step:1308/2160 train_time:59427ms step_avg:45.43ms
step:1309/2160 train_time:59487ms step_avg:45.44ms
step:1310/2160 train_time:59546ms step_avg:45.45ms
step:1311/2160 train_time:59606ms step_avg:45.47ms
step:1312/2160 train_time:59664ms step_avg:45.48ms
step:1313/2160 train_time:59724ms step_avg:45.49ms
step:1314/2160 train_time:59783ms step_avg:45.50ms
step:1315/2160 train_time:59843ms step_avg:45.51ms
step:1316/2160 train_time:59903ms step_avg:45.52ms
step:1317/2160 train_time:59966ms step_avg:45.53ms
step:1318/2160 train_time:60026ms step_avg:45.54ms
step:1319/2160 train_time:60088ms step_avg:45.56ms
step:1320/2160 train_time:60147ms step_avg:45.57ms
step:1321/2160 train_time:60208ms step_avg:45.58ms
step:1322/2160 train_time:60267ms step_avg:45.59ms
step:1323/2160 train_time:60329ms step_avg:45.60ms
step:1324/2160 train_time:60388ms step_avg:45.61ms
step:1325/2160 train_time:60448ms step_avg:45.62ms
step:1326/2160 train_time:60507ms step_avg:45.63ms
step:1327/2160 train_time:60567ms step_avg:45.64ms
step:1328/2160 train_time:60625ms step_avg:45.65ms
step:1329/2160 train_time:60687ms step_avg:45.66ms
step:1330/2160 train_time:60745ms step_avg:45.67ms
step:1331/2160 train_time:60806ms step_avg:45.68ms
step:1332/2160 train_time:60865ms step_avg:45.69ms
step:1333/2160 train_time:60926ms step_avg:45.71ms
step:1334/2160 train_time:60986ms step_avg:45.72ms
step:1335/2160 train_time:61048ms step_avg:45.73ms
step:1336/2160 train_time:61108ms step_avg:45.74ms
step:1337/2160 train_time:61170ms step_avg:45.75ms
step:1338/2160 train_time:61229ms step_avg:45.76ms
step:1339/2160 train_time:61289ms step_avg:45.77ms
step:1340/2160 train_time:61348ms step_avg:45.78ms
step:1341/2160 train_time:61408ms step_avg:45.79ms
step:1342/2160 train_time:61467ms step_avg:45.80ms
step:1343/2160 train_time:61527ms step_avg:45.81ms
step:1344/2160 train_time:61586ms step_avg:45.82ms
step:1345/2160 train_time:61646ms step_avg:45.83ms
step:1346/2160 train_time:61705ms step_avg:45.84ms
step:1347/2160 train_time:61766ms step_avg:45.85ms
step:1348/2160 train_time:61826ms step_avg:45.86ms
step:1349/2160 train_time:61887ms step_avg:45.88ms
step:1350/2160 train_time:61946ms step_avg:45.89ms
step:1351/2160 train_time:62007ms step_avg:45.90ms
step:1352/2160 train_time:62067ms step_avg:45.91ms
step:1353/2160 train_time:62128ms step_avg:45.92ms
step:1354/2160 train_time:62188ms step_avg:45.93ms
step:1355/2160 train_time:62249ms step_avg:45.94ms
step:1356/2160 train_time:62308ms step_avg:45.95ms
step:1357/2160 train_time:62368ms step_avg:45.96ms
step:1358/2160 train_time:62428ms step_avg:45.97ms
step:1359/2160 train_time:62487ms step_avg:45.98ms
step:1360/2160 train_time:62545ms step_avg:45.99ms
step:1361/2160 train_time:62606ms step_avg:46.00ms
step:1362/2160 train_time:62665ms step_avg:46.01ms
step:1363/2160 train_time:62726ms step_avg:46.02ms
step:1364/2160 train_time:62785ms step_avg:46.03ms
step:1365/2160 train_time:62846ms step_avg:46.04ms
step:1366/2160 train_time:62905ms step_avg:46.05ms
step:1367/2160 train_time:62966ms step_avg:46.06ms
step:1368/2160 train_time:63025ms step_avg:46.07ms
step:1369/2160 train_time:63086ms step_avg:46.08ms
step:1370/2160 train_time:63146ms step_avg:46.09ms
step:1371/2160 train_time:63207ms step_avg:46.10ms
step:1372/2160 train_time:63267ms step_avg:46.11ms
step:1373/2160 train_time:63328ms step_avg:46.12ms
step:1374/2160 train_time:63388ms step_avg:46.13ms
step:1375/2160 train_time:63448ms step_avg:46.14ms
step:1376/2160 train_time:63507ms step_avg:46.15ms
step:1377/2160 train_time:63567ms step_avg:46.16ms
step:1378/2160 train_time:63626ms step_avg:46.17ms
step:1379/2160 train_time:63686ms step_avg:46.18ms
step:1380/2160 train_time:63745ms step_avg:46.19ms
step:1381/2160 train_time:63806ms step_avg:46.20ms
step:1382/2160 train_time:63866ms step_avg:46.21ms
step:1383/2160 train_time:63927ms step_avg:46.22ms
step:1384/2160 train_time:63986ms step_avg:46.23ms
step:1385/2160 train_time:64047ms step_avg:46.24ms
step:1386/2160 train_time:64106ms step_avg:46.25ms
step:1387/2160 train_time:64167ms step_avg:46.26ms
step:1388/2160 train_time:64226ms step_avg:46.27ms
step:1389/2160 train_time:64287ms step_avg:46.28ms
step:1390/2160 train_time:64347ms step_avg:46.29ms
step:1391/2160 train_time:64408ms step_avg:46.30ms
step:1392/2160 train_time:64467ms step_avg:46.31ms
step:1393/2160 train_time:64528ms step_avg:46.32ms
step:1394/2160 train_time:64587ms step_avg:46.33ms
step:1395/2160 train_time:64646ms step_avg:46.34ms
step:1396/2160 train_time:64705ms step_avg:46.35ms
step:1397/2160 train_time:64767ms step_avg:46.36ms
step:1398/2160 train_time:64826ms step_avg:46.37ms
step:1399/2160 train_time:64886ms step_avg:46.38ms
step:1400/2160 train_time:64945ms step_avg:46.39ms
step:1401/2160 train_time:65006ms step_avg:46.40ms
step:1402/2160 train_time:65065ms step_avg:46.41ms
step:1403/2160 train_time:65127ms step_avg:46.42ms
step:1404/2160 train_time:65186ms step_avg:46.43ms
step:1405/2160 train_time:65247ms step_avg:46.44ms
step:1406/2160 train_time:65306ms step_avg:46.45ms
step:1407/2160 train_time:65367ms step_avg:46.46ms
step:1408/2160 train_time:65427ms step_avg:46.47ms
step:1409/2160 train_time:65488ms step_avg:46.48ms
step:1410/2160 train_time:65547ms step_avg:46.49ms
step:1411/2160 train_time:65607ms step_avg:46.50ms
step:1412/2160 train_time:65667ms step_avg:46.51ms
step:1413/2160 train_time:65727ms step_avg:46.52ms
step:1414/2160 train_time:65786ms step_avg:46.53ms
step:1415/2160 train_time:65847ms step_avg:46.53ms
step:1416/2160 train_time:65935ms step_avg:46.56ms
step:1417/2160 train_time:66023ms step_avg:46.59ms
step:1418/2160 train_time:66110ms step_avg:46.62ms
step:1419/2160 train_time:66198ms step_avg:46.65ms
step:1420/2160 train_time:66285ms step_avg:46.68ms
step:1421/2160 train_time:66375ms step_avg:46.71ms
step:1422/2160 train_time:66461ms step_avg:46.74ms
step:1423/2160 train_time:66549ms step_avg:46.77ms
step:1424/2160 train_time:66635ms step_avg:46.79ms
step:1425/2160 train_time:66723ms step_avg:46.82ms
step:1426/2160 train_time:66810ms step_avg:46.85ms
step:1427/2160 train_time:66900ms step_avg:46.88ms
step:1428/2160 train_time:66986ms step_avg:46.91ms
step:1429/2160 train_time:67076ms step_avg:46.94ms
step:1430/2160 train_time:67162ms step_avg:46.97ms
step:1431/2160 train_time:67249ms step_avg:46.99ms
step:1432/2160 train_time:67336ms step_avg:47.02ms
step:1433/2160 train_time:67424ms step_avg:47.05ms
step:1434/2160 train_time:67512ms step_avg:47.08ms
step:1435/2160 train_time:67602ms step_avg:47.11ms
step:1436/2160 train_time:67688ms step_avg:47.14ms
step:1437/2160 train_time:67777ms step_avg:47.17ms
step:1438/2160 train_time:67863ms step_avg:47.19ms
step:1439/2160 train_time:67953ms step_avg:47.22ms
step:1440/2160 train_time:68039ms step_avg:47.25ms
step:1441/2160 train_time:68128ms step_avg:47.28ms
step:1442/2160 train_time:68215ms step_avg:47.31ms
step:1443/2160 train_time:68302ms step_avg:47.33ms
step:1444/2160 train_time:68390ms step_avg:47.36ms
step:1445/2160 train_time:68478ms step_avg:47.39ms
step:1446/2160 train_time:68565ms step_avg:47.42ms
step:1447/2160 train_time:68654ms step_avg:47.45ms
step:1448/2160 train_time:68741ms step_avg:47.47ms
step:1449/2160 train_time:68829ms step_avg:47.50ms
step:1450/2160 train_time:68917ms step_avg:47.53ms
step:1451/2160 train_time:69005ms step_avg:47.56ms
step:1452/2160 train_time:69093ms step_avg:47.58ms
step:1453/2160 train_time:69182ms step_avg:47.61ms
step:1454/2160 train_time:69268ms step_avg:47.64ms
step:1455/2160 train_time:69357ms step_avg:47.67ms
step:1456/2160 train_time:69444ms step_avg:47.70ms
step:1457/2160 train_time:69531ms step_avg:47.72ms
step:1458/2160 train_time:69619ms step_avg:47.75ms
step:1459/2160 train_time:69706ms step_avg:47.78ms
step:1460/2160 train_time:69794ms step_avg:47.80ms
step:1461/2160 train_time:69882ms step_avg:47.83ms
step:1462/2160 train_time:69968ms step_avg:47.86ms
step:1463/2160 train_time:70057ms step_avg:47.89ms
step:1464/2160 train_time:70144ms step_avg:47.91ms
step:1465/2160 train_time:70232ms step_avg:47.94ms
step:1466/2160 train_time:70319ms step_avg:47.97ms
step:1467/2160 train_time:70408ms step_avg:47.99ms
step:1468/2160 train_time:70495ms step_avg:48.02ms
step:1469/2160 train_time:70583ms step_avg:48.05ms
step:1470/2160 train_time:70669ms step_avg:48.07ms
step:1471/2160 train_time:70759ms step_avg:48.10ms
step:1472/2160 train_time:70845ms step_avg:48.13ms
step:1473/2160 train_time:70934ms step_avg:48.16ms
step:1474/2160 train_time:71021ms step_avg:48.18ms
step:1475/2160 train_time:71110ms step_avg:48.21ms
step:1476/2160 train_time:71197ms step_avg:48.24ms
step:1477/2160 train_time:71285ms step_avg:48.26ms
step:1478/2160 train_time:71372ms step_avg:48.29ms
step:1479/2160 train_time:71461ms step_avg:48.32ms
step:1480/2160 train_time:71547ms step_avg:48.34ms
step:1481/2160 train_time:71636ms step_avg:48.37ms
step:1482/2160 train_time:71722ms step_avg:48.40ms
step:1483/2160 train_time:71811ms step_avg:48.42ms
step:1484/2160 train_time:71898ms step_avg:48.45ms
step:1485/2160 train_time:71986ms step_avg:48.48ms
step:1486/2160 train_time:72074ms step_avg:48.50ms
step:1487/2160 train_time:72163ms step_avg:48.53ms
step:1488/2160 train_time:72250ms step_avg:48.55ms
step:1489/2160 train_time:72339ms step_avg:48.58ms
step:1490/2160 train_time:72425ms step_avg:48.61ms
step:1491/2160 train_time:72512ms step_avg:48.63ms
step:1492/2160 train_time:72600ms step_avg:48.66ms
step:1493/2160 train_time:72687ms step_avg:48.69ms
step:1494/2160 train_time:72774ms step_avg:48.71ms
step:1495/2160 train_time:72863ms step_avg:48.74ms
step:1496/2160 train_time:72950ms step_avg:48.76ms
step:1497/2160 train_time:73039ms step_avg:48.79ms
step:1498/2160 train_time:73126ms step_avg:48.82ms
step:1499/2160 train_time:73215ms step_avg:48.84ms
step:1500/2160 train_time:73302ms step_avg:48.87ms
step:1500/2160 val_loss:3.4866 train_time:73391ms step_avg:48.93ms
step:1501/2160 train_time:73414ms step_avg:48.91ms
step:1502/2160 train_time:73480ms step_avg:48.92ms
step:1503/2160 train_time:73571ms step_avg:48.95ms
step:1504/2160 train_time:73659ms step_avg:48.98ms
step:1505/2160 train_time:73748ms step_avg:49.00ms
step:1506/2160 train_time:73834ms step_avg:49.03ms
step:1507/2160 train_time:73921ms step_avg:49.05ms
step:1508/2160 train_time:74007ms step_avg:49.08ms
step:1509/2160 train_time:74095ms step_avg:49.10ms
step:1510/2160 train_time:74182ms step_avg:49.13ms
step:1511/2160 train_time:74269ms step_avg:49.15ms
step:1512/2160 train_time:74358ms step_avg:49.18ms
step:1513/2160 train_time:74449ms step_avg:49.21ms
step:1514/2160 train_time:74539ms step_avg:49.23ms
step:1515/2160 train_time:74629ms step_avg:49.26ms
step:1516/2160 train_time:74716ms step_avg:49.28ms
step:1517/2160 train_time:74804ms step_avg:49.31ms
step:1518/2160 train_time:74889ms step_avg:49.33ms
step:1519/2160 train_time:74976ms step_avg:49.36ms
step:1520/2160 train_time:75062ms step_avg:49.38ms
step:1521/2160 train_time:75150ms step_avg:49.41ms
step:1522/2160 train_time:75237ms step_avg:49.43ms
step:1523/2160 train_time:75326ms step_avg:49.46ms
step:1524/2160 train_time:75414ms step_avg:49.48ms
step:1525/2160 train_time:75505ms step_avg:49.51ms
step:1526/2160 train_time:75591ms step_avg:49.54ms
step:1527/2160 train_time:75680ms step_avg:49.56ms
step:1528/2160 train_time:75767ms step_avg:49.59ms
step:1529/2160 train_time:75854ms step_avg:49.61ms
step:1530/2160 train_time:75940ms step_avg:49.63ms
step:1531/2160 train_time:76028ms step_avg:49.66ms
step:1532/2160 train_time:76114ms step_avg:49.68ms
step:1533/2160 train_time:76204ms step_avg:49.71ms
step:1534/2160 train_time:76290ms step_avg:49.73ms
step:1535/2160 train_time:76379ms step_avg:49.76ms
step:1536/2160 train_time:76468ms step_avg:49.78ms
step:1537/2160 train_time:76558ms step_avg:49.81ms
step:1538/2160 train_time:76646ms step_avg:49.83ms
step:1539/2160 train_time:76734ms step_avg:49.86ms
step:1540/2160 train_time:76821ms step_avg:49.88ms
step:1541/2160 train_time:76909ms step_avg:49.91ms
step:1542/2160 train_time:76996ms step_avg:49.93ms
step:1543/2160 train_time:77085ms step_avg:49.96ms
step:1544/2160 train_time:77170ms step_avg:49.98ms
step:1545/2160 train_time:77260ms step_avg:50.01ms
step:1546/2160 train_time:77347ms step_avg:50.03ms
step:1547/2160 train_time:77436ms step_avg:50.06ms
step:1548/2160 train_time:77525ms step_avg:50.08ms
step:1549/2160 train_time:77614ms step_avg:50.11ms
step:1550/2160 train_time:77702ms step_avg:50.13ms
step:1551/2160 train_time:77790ms step_avg:50.15ms
step:1552/2160 train_time:77876ms step_avg:50.18ms
step:1553/2160 train_time:77966ms step_avg:50.20ms
step:1554/2160 train_time:78053ms step_avg:50.23ms
step:1555/2160 train_time:78141ms step_avg:50.25ms
step:1556/2160 train_time:78227ms step_avg:50.27ms
step:1557/2160 train_time:78316ms step_avg:50.30ms
step:1558/2160 train_time:78403ms step_avg:50.32ms
step:1559/2160 train_time:78492ms step_avg:50.35ms
step:1560/2160 train_time:78579ms step_avg:50.37ms
step:1561/2160 train_time:78667ms step_avg:50.40ms
step:1562/2160 train_time:78754ms step_avg:50.42ms
step:1563/2160 train_time:78842ms step_avg:50.44ms
step:1564/2160 train_time:78929ms step_avg:50.47ms
step:1565/2160 train_time:79019ms step_avg:50.49ms
step:1566/2160 train_time:79105ms step_avg:50.51ms
step:1567/2160 train_time:79193ms step_avg:50.54ms
step:1568/2160 train_time:79280ms step_avg:50.56ms
step:1569/2160 train_time:79368ms step_avg:50.59ms
step:1570/2160 train_time:79456ms step_avg:50.61ms
step:1571/2160 train_time:79546ms step_avg:50.63ms
step:1572/2160 train_time:79632ms step_avg:50.66ms
step:1573/2160 train_time:79721ms step_avg:50.68ms
step:1574/2160 train_time:79807ms step_avg:50.70ms
step:1575/2160 train_time:79895ms step_avg:50.73ms
step:1576/2160 train_time:79983ms step_avg:50.75ms
step:1577/2160 train_time:80071ms step_avg:50.77ms
step:1578/2160 train_time:80158ms step_avg:50.80ms
step:1579/2160 train_time:80247ms step_avg:50.82ms
step:1580/2160 train_time:80333ms step_avg:50.84ms
step:1581/2160 train_time:80422ms step_avg:50.87ms
step:1582/2160 train_time:80508ms step_avg:50.89ms
step:1583/2160 train_time:80597ms step_avg:50.91ms
step:1584/2160 train_time:80685ms step_avg:50.94ms
step:1585/2160 train_time:80775ms step_avg:50.96ms
step:1586/2160 train_time:80862ms step_avg:50.98ms
step:1587/2160 train_time:80949ms step_avg:51.01ms
step:1588/2160 train_time:81036ms step_avg:51.03ms
step:1589/2160 train_time:81126ms step_avg:51.05ms
step:1590/2160 train_time:81213ms step_avg:51.08ms
step:1591/2160 train_time:81301ms step_avg:51.10ms
step:1592/2160 train_time:81388ms step_avg:51.12ms
step:1593/2160 train_time:81477ms step_avg:51.15ms
step:1594/2160 train_time:81565ms step_avg:51.17ms
step:1595/2160 train_time:81653ms step_avg:51.19ms
step:1596/2160 train_time:81739ms step_avg:51.22ms
step:1597/2160 train_time:81829ms step_avg:51.24ms
step:1598/2160 train_time:81916ms step_avg:51.26ms
step:1599/2160 train_time:82005ms step_avg:51.28ms
step:1600/2160 train_time:82091ms step_avg:51.31ms
step:1601/2160 train_time:82180ms step_avg:51.33ms
step:1602/2160 train_time:82266ms step_avg:51.35ms
step:1603/2160 train_time:82354ms step_avg:51.38ms
step:1604/2160 train_time:82442ms step_avg:51.40ms
step:1605/2160 train_time:82531ms step_avg:51.42ms
step:1606/2160 train_time:82618ms step_avg:51.44ms
step:1607/2160 train_time:82706ms step_avg:51.47ms
step:1608/2160 train_time:82793ms step_avg:51.49ms
step:1609/2160 train_time:82882ms step_avg:51.51ms
step:1610/2160 train_time:82968ms step_avg:51.53ms
step:1611/2160 train_time:83057ms step_avg:51.56ms
step:1612/2160 train_time:83145ms step_avg:51.58ms
step:1613/2160 train_time:83234ms step_avg:51.60ms
step:1614/2160 train_time:83320ms step_avg:51.62ms
step:1615/2160 train_time:83408ms step_avg:51.65ms
step:1616/2160 train_time:83495ms step_avg:51.67ms
step:1617/2160 train_time:83584ms step_avg:51.69ms
step:1618/2160 train_time:83670ms step_avg:51.71ms
step:1619/2160 train_time:83760ms step_avg:51.74ms
step:1620/2160 train_time:83847ms step_avg:51.76ms
step:1621/2160 train_time:83935ms step_avg:51.78ms
step:1622/2160 train_time:84022ms step_avg:51.80ms
step:1623/2160 train_time:84110ms step_avg:51.82ms
step:1624/2160 train_time:84196ms step_avg:51.84ms
step:1625/2160 train_time:84285ms step_avg:51.87ms
step:1626/2160 train_time:84371ms step_avg:51.89ms
step:1627/2160 train_time:84461ms step_avg:51.91ms
step:1628/2160 train_time:84548ms step_avg:51.93ms
step:1629/2160 train_time:84636ms step_avg:51.96ms
step:1630/2160 train_time:84723ms step_avg:51.98ms
step:1631/2160 train_time:84811ms step_avg:52.00ms
step:1632/2160 train_time:84898ms step_avg:52.02ms
step:1633/2160 train_time:84987ms step_avg:52.04ms
step:1634/2160 train_time:85074ms step_avg:52.06ms
step:1635/2160 train_time:85163ms step_avg:52.09ms
step:1636/2160 train_time:85249ms step_avg:52.11ms
step:1637/2160 train_time:85338ms step_avg:52.13ms
step:1638/2160 train_time:85425ms step_avg:52.15ms
step:1639/2160 train_time:85513ms step_avg:52.17ms
step:1640/2160 train_time:85600ms step_avg:52.20ms
step:1641/2160 train_time:85689ms step_avg:52.22ms
step:1642/2160 train_time:85775ms step_avg:52.24ms
step:1643/2160 train_time:85864ms step_avg:52.26ms
step:1644/2160 train_time:85951ms step_avg:52.28ms
step:1645/2160 train_time:86039ms step_avg:52.30ms
step:1646/2160 train_time:86127ms step_avg:52.33ms
step:1647/2160 train_time:86215ms step_avg:52.35ms
step:1648/2160 train_time:86302ms step_avg:52.37ms
step:1649/2160 train_time:86390ms step_avg:52.39ms
step:1650/2160 train_time:86477ms step_avg:52.41ms
step:1651/2160 train_time:86567ms step_avg:52.43ms
step:1652/2160 train_time:86654ms step_avg:52.45ms
step:1653/2160 train_time:86743ms step_avg:52.48ms
step:1654/2160 train_time:86829ms step_avg:52.50ms
step:1655/2160 train_time:86918ms step_avg:52.52ms
step:1656/2160 train_time:87005ms step_avg:52.54ms
step:1657/2160 train_time:87093ms step_avg:52.56ms
step:1658/2160 train_time:87180ms step_avg:52.58ms
step:1659/2160 train_time:87268ms step_avg:52.60ms
step:1660/2160 train_time:87355ms step_avg:52.62ms
step:1661/2160 train_time:87444ms step_avg:52.65ms
step:1662/2160 train_time:87530ms step_avg:52.67ms
step:1663/2160 train_time:87619ms step_avg:52.69ms
step:1664/2160 train_time:87707ms step_avg:52.71ms
step:1665/2160 train_time:87796ms step_avg:52.73ms
step:1666/2160 train_time:87883ms step_avg:52.75ms
step:1667/2160 train_time:87971ms step_avg:52.77ms
step:1668/2160 train_time:88058ms step_avg:52.79ms
step:1669/2160 train_time:88147ms step_avg:52.81ms
step:1670/2160 train_time:88234ms step_avg:52.83ms
step:1671/2160 train_time:88323ms step_avg:52.86ms
step:1672/2160 train_time:88409ms step_avg:52.88ms
step:1673/2160 train_time:88498ms step_avg:52.90ms
step:1674/2160 train_time:88585ms step_avg:52.92ms
step:1675/2160 train_time:88672ms step_avg:52.94ms
step:1676/2160 train_time:88759ms step_avg:52.96ms
step:1677/2160 train_time:88849ms step_avg:52.98ms
step:1678/2160 train_time:88936ms step_avg:53.00ms
step:1679/2160 train_time:89025ms step_avg:53.02ms
step:1680/2160 train_time:89112ms step_avg:53.04ms
step:1681/2160 train_time:89200ms step_avg:53.06ms
step:1682/2160 train_time:89288ms step_avg:53.08ms
step:1683/2160 train_time:89375ms step_avg:53.10ms
step:1684/2160 train_time:89462ms step_avg:53.12ms
step:1685/2160 train_time:89550ms step_avg:53.15ms
step:1686/2160 train_time:89637ms step_avg:53.17ms
step:1687/2160 train_time:89727ms step_avg:53.19ms
step:1688/2160 train_time:89814ms step_avg:53.21ms
step:1689/2160 train_time:89903ms step_avg:53.23ms
step:1690/2160 train_time:89989ms step_avg:53.25ms
step:1691/2160 train_time:90077ms step_avg:53.27ms
step:1692/2160 train_time:90165ms step_avg:53.29ms
step:1693/2160 train_time:90253ms step_avg:53.31ms
step:1694/2160 train_time:90340ms step_avg:53.33ms
step:1695/2160 train_time:90428ms step_avg:53.35ms
step:1696/2160 train_time:90516ms step_avg:53.37ms
step:1697/2160 train_time:90605ms step_avg:53.39ms
step:1698/2160 train_time:90692ms step_avg:53.41ms
step:1699/2160 train_time:90781ms step_avg:53.43ms
step:1700/2160 train_time:90867ms step_avg:53.45ms
step:1701/2160 train_time:90956ms step_avg:53.47ms
step:1702/2160 train_time:91043ms step_avg:53.49ms
step:1703/2160 train_time:91132ms step_avg:53.51ms
step:1704/2160 train_time:91218ms step_avg:53.53ms
step:1705/2160 train_time:91307ms step_avg:53.55ms
step:1706/2160 train_time:91393ms step_avg:53.57ms
step:1707/2160 train_time:91482ms step_avg:53.59ms
step:1708/2160 train_time:91568ms step_avg:53.61ms
step:1709/2160 train_time:91657ms step_avg:53.63ms
step:1710/2160 train_time:91744ms step_avg:53.65ms
step:1711/2160 train_time:91833ms step_avg:53.67ms
step:1712/2160 train_time:91919ms step_avg:53.69ms
step:1713/2160 train_time:92008ms step_avg:53.71ms
step:1714/2160 train_time:92095ms step_avg:53.73ms
step:1715/2160 train_time:92184ms step_avg:53.75ms
step:1716/2160 train_time:92270ms step_avg:53.77ms
step:1717/2160 train_time:92358ms step_avg:53.79ms
step:1718/2160 train_time:92445ms step_avg:53.81ms
step:1719/2160 train_time:92533ms step_avg:53.83ms
step:1720/2160 train_time:92620ms step_avg:53.85ms
step:1721/2160 train_time:92708ms step_avg:53.87ms
step:1722/2160 train_time:92794ms step_avg:53.89ms
step:1723/2160 train_time:92883ms step_avg:53.91ms
step:1724/2160 train_time:92969ms step_avg:53.93ms
step:1725/2160 train_time:93058ms step_avg:53.95ms
step:1726/2160 train_time:93145ms step_avg:53.97ms
step:1727/2160 train_time:93234ms step_avg:53.99ms
step:1728/2160 train_time:93320ms step_avg:54.00ms
step:1729/2160 train_time:93408ms step_avg:54.02ms
step:1730/2160 train_time:93495ms step_avg:54.04ms
step:1731/2160 train_time:93584ms step_avg:54.06ms
step:1732/2160 train_time:93670ms step_avg:54.08ms
step:1733/2160 train_time:93758ms step_avg:54.10ms
step:1734/2160 train_time:93845ms step_avg:54.12ms
step:1735/2160 train_time:93933ms step_avg:54.14ms
step:1736/2160 train_time:94020ms step_avg:54.16ms
step:1737/2160 train_time:94109ms step_avg:54.18ms
step:1738/2160 train_time:94196ms step_avg:54.20ms
step:1739/2160 train_time:94286ms step_avg:54.22ms
step:1740/2160 train_time:94372ms step_avg:54.24ms
step:1741/2160 train_time:94461ms step_avg:54.26ms
step:1742/2160 train_time:94548ms step_avg:54.28ms
step:1743/2160 train_time:94636ms step_avg:54.30ms
step:1744/2160 train_time:94723ms step_avg:54.31ms
step:1745/2160 train_time:94812ms step_avg:54.33ms
step:1746/2160 train_time:94899ms step_avg:54.35ms
step:1747/2160 train_time:94988ms step_avg:54.37ms
step:1748/2160 train_time:95075ms step_avg:54.39ms
step:1749/2160 train_time:95164ms step_avg:54.41ms
step:1750/2160 train_time:95251ms step_avg:54.43ms
step:1750/2160 val_loss:3.3889 train_time:95341ms step_avg:54.48ms
step:1751/2160 train_time:95365ms step_avg:54.46ms
step:1752/2160 train_time:95429ms step_avg:54.47ms
step:1753/2160 train_time:95526ms step_avg:54.49ms
step:1754/2160 train_time:95614ms step_avg:54.51ms
step:1755/2160 train_time:95702ms step_avg:54.53ms
step:1756/2160 train_time:95787ms step_avg:54.55ms
step:1757/2160 train_time:95874ms step_avg:54.57ms
step:1758/2160 train_time:95960ms step_avg:54.58ms
step:1759/2160 train_time:96046ms step_avg:54.60ms
step:1760/2160 train_time:96132ms step_avg:54.62ms
step:1761/2160 train_time:96220ms step_avg:54.64ms
step:1762/2160 train_time:96307ms step_avg:54.66ms
step:1763/2160 train_time:96399ms step_avg:54.68ms
step:1764/2160 train_time:96487ms step_avg:54.70ms
step:1765/2160 train_time:96579ms step_avg:54.72ms
step:1766/2160 train_time:96666ms step_avg:54.74ms
step:1767/2160 train_time:96754ms step_avg:54.76ms
step:1768/2160 train_time:96841ms step_avg:54.77ms
step:1769/2160 train_time:96928ms step_avg:54.79ms
step:1770/2160 train_time:97013ms step_avg:54.81ms
step:1771/2160 train_time:97101ms step_avg:54.83ms
step:1772/2160 train_time:97186ms step_avg:54.85ms
step:1773/2160 train_time:97275ms step_avg:54.86ms
step:1774/2160 train_time:97363ms step_avg:54.88ms
step:1775/2160 train_time:97454ms step_avg:54.90ms
step:1776/2160 train_time:97542ms step_avg:54.92ms
step:1777/2160 train_time:97631ms step_avg:54.94ms
step:1778/2160 train_time:97718ms step_avg:54.96ms
step:1779/2160 train_time:97806ms step_avg:54.98ms
step:1780/2160 train_time:97892ms step_avg:55.00ms
step:1781/2160 train_time:97980ms step_avg:55.01ms
step:1782/2160 train_time:98065ms step_avg:55.03ms
step:1783/2160 train_time:98153ms step_avg:55.05ms
step:1784/2160 train_time:98240ms step_avg:55.07ms
step:1785/2160 train_time:98329ms step_avg:55.09ms
step:1786/2160 train_time:98417ms step_avg:55.10ms
step:1787/2160 train_time:98507ms step_avg:55.12ms
step:1788/2160 train_time:98595ms step_avg:55.14ms
step:1789/2160 train_time:98685ms step_avg:55.16ms
step:1790/2160 train_time:98771ms step_avg:55.18ms
step:1791/2160 train_time:98859ms step_avg:55.20ms
step:1792/2160 train_time:98944ms step_avg:55.21ms
step:1793/2160 train_time:99032ms step_avg:55.23ms
step:1794/2160 train_time:99118ms step_avg:55.25ms
step:1795/2160 train_time:99205ms step_avg:55.27ms
step:1796/2160 train_time:99292ms step_avg:55.29ms
step:1797/2160 train_time:99383ms step_avg:55.30ms
step:1798/2160 train_time:99469ms step_avg:55.32ms
step:1799/2160 train_time:99560ms step_avg:55.34ms
step:1800/2160 train_time:99646ms step_avg:55.36ms
step:1801/2160 train_time:99736ms step_avg:55.38ms
step:1802/2160 train_time:99822ms step_avg:55.40ms
step:1803/2160 train_time:99908ms step_avg:55.41ms
step:1804/2160 train_time:99995ms step_avg:55.43ms
step:1805/2160 train_time:100083ms step_avg:55.45ms
step:1806/2160 train_time:100169ms step_avg:55.46ms
step:1807/2160 train_time:100259ms step_avg:55.48ms
step:1808/2160 train_time:100346ms step_avg:55.50ms
step:1809/2160 train_time:100435ms step_avg:55.52ms
step:1810/2160 train_time:100523ms step_avg:55.54ms
step:1811/2160 train_time:100611ms step_avg:55.56ms
step:1812/2160 train_time:100699ms step_avg:55.57ms
step:1813/2160 train_time:100787ms step_avg:55.59ms
step:1814/2160 train_time:100873ms step_avg:55.61ms
step:1815/2160 train_time:100963ms step_avg:55.63ms
step:1816/2160 train_time:101049ms step_avg:55.64ms
step:1817/2160 train_time:101138ms step_avg:55.66ms
step:1818/2160 train_time:101224ms step_avg:55.68ms
step:1819/2160 train_time:101312ms step_avg:55.70ms
step:1820/2160 train_time:101399ms step_avg:55.71ms
step:1821/2160 train_time:101488ms step_avg:55.73ms
step:1822/2160 train_time:101575ms step_avg:55.75ms
step:1823/2160 train_time:101665ms step_avg:55.77ms
step:1824/2160 train_time:101752ms step_avg:55.78ms
step:1825/2160 train_time:101841ms step_avg:55.80ms
step:1826/2160 train_time:101926ms step_avg:55.82ms
step:1827/2160 train_time:102014ms step_avg:55.84ms
step:1828/2160 train_time:102101ms step_avg:55.85ms
step:1829/2160 train_time:102188ms step_avg:55.87ms
step:1830/2160 train_time:102274ms step_avg:55.89ms
step:1831/2160 train_time:102363ms step_avg:55.91ms
step:1832/2160 train_time:102450ms step_avg:55.92ms
step:1833/2160 train_time:102540ms step_avg:55.94ms
step:1834/2160 train_time:102627ms step_avg:55.96ms
step:1835/2160 train_time:102716ms step_avg:55.98ms
step:1836/2160 train_time:102802ms step_avg:55.99ms
step:1837/2160 train_time:102890ms step_avg:56.01ms
step:1838/2160 train_time:102976ms step_avg:56.03ms
step:1839/2160 train_time:103065ms step_avg:56.04ms
step:1840/2160 train_time:103151ms step_avg:56.06ms
step:1841/2160 train_time:103239ms step_avg:56.08ms
step:1842/2160 train_time:103325ms step_avg:56.09ms
step:1843/2160 train_time:103413ms step_avg:56.11ms
step:1844/2160 train_time:103501ms step_avg:56.13ms
step:1845/2160 train_time:103589ms step_avg:56.15ms
step:1846/2160 train_time:103676ms step_avg:56.16ms
step:1847/2160 train_time:103765ms step_avg:56.18ms
step:1848/2160 train_time:103853ms step_avg:56.20ms
step:1849/2160 train_time:103942ms step_avg:56.22ms
step:1850/2160 train_time:104029ms step_avg:56.23ms
step:1851/2160 train_time:104117ms step_avg:56.25ms
step:1852/2160 train_time:104203ms step_avg:56.27ms
step:1853/2160 train_time:104292ms step_avg:56.28ms
step:1854/2160 train_time:104379ms step_avg:56.30ms
step:1855/2160 train_time:104467ms step_avg:56.32ms
step:1856/2160 train_time:104554ms step_avg:56.33ms
step:1857/2160 train_time:104643ms step_avg:56.35ms
step:1858/2160 train_time:104730ms step_avg:56.37ms
step:1859/2160 train_time:104818ms step_avg:56.38ms
step:1860/2160 train_time:104904ms step_avg:56.40ms
step:1861/2160 train_time:104993ms step_avg:56.42ms
step:1862/2160 train_time:105080ms step_avg:56.43ms
step:1863/2160 train_time:105168ms step_avg:56.45ms
step:1864/2160 train_time:105254ms step_avg:56.47ms
step:1865/2160 train_time:105343ms step_avg:56.48ms
step:1866/2160 train_time:105431ms step_avg:56.50ms
step:1867/2160 train_time:105520ms step_avg:56.52ms
step:1868/2160 train_time:105606ms step_avg:56.53ms
step:1869/2160 train_time:105695ms step_avg:56.55ms
step:1870/2160 train_time:105782ms step_avg:56.57ms
step:1871/2160 train_time:105870ms step_avg:56.58ms
step:1872/2160 train_time:105958ms step_avg:56.60ms
step:1873/2160 train_time:106046ms step_avg:56.62ms
step:1874/2160 train_time:106133ms step_avg:56.63ms
step:1875/2160 train_time:106222ms step_avg:56.65ms
step:1876/2160 train_time:106308ms step_avg:56.67ms
step:1877/2160 train_time:106397ms step_avg:56.68ms
step:1878/2160 train_time:106483ms step_avg:56.70ms
step:1879/2160 train_time:106572ms step_avg:56.72ms
step:1880/2160 train_time:106659ms step_avg:56.73ms
step:1881/2160 train_time:106748ms step_avg:56.75ms
step:1882/2160 train_time:106834ms step_avg:56.77ms
step:1883/2160 train_time:106923ms step_avg:56.78ms
step:1884/2160 train_time:107009ms step_avg:56.80ms
step:1885/2160 train_time:107099ms step_avg:56.82ms
step:1886/2160 train_time:107185ms step_avg:56.83ms
step:1887/2160 train_time:107274ms step_avg:56.85ms
step:1888/2160 train_time:107361ms step_avg:56.86ms
step:1889/2160 train_time:107450ms step_avg:56.88ms
step:1890/2160 train_time:107537ms step_avg:56.90ms
step:1891/2160 train_time:107625ms step_avg:56.91ms
step:1892/2160 train_time:107712ms step_avg:56.93ms
step:1893/2160 train_time:107800ms step_avg:56.95ms
step:1894/2160 train_time:107886ms step_avg:56.96ms
step:1895/2160 train_time:107975ms step_avg:56.98ms
step:1896/2160 train_time:108061ms step_avg:56.99ms
step:1897/2160 train_time:108148ms step_avg:57.01ms
step:1898/2160 train_time:108235ms step_avg:57.03ms
step:1899/2160 train_time:108323ms step_avg:57.04ms
step:1900/2160 train_time:108409ms step_avg:57.06ms
step:1901/2160 train_time:108500ms step_avg:57.08ms
step:1902/2160 train_time:108585ms step_avg:57.09ms
step:1903/2160 train_time:108673ms step_avg:57.11ms
step:1904/2160 train_time:108760ms step_avg:57.12ms
step:1905/2160 train_time:108848ms step_avg:57.14ms
step:1906/2160 train_time:108935ms step_avg:57.15ms
step:1907/2160 train_time:109024ms step_avg:57.17ms
step:1908/2160 train_time:109110ms step_avg:57.19ms
step:1909/2160 train_time:109199ms step_avg:57.20ms
step:1910/2160 train_time:109285ms step_avg:57.22ms
step:1911/2160 train_time:109374ms step_avg:57.23ms
step:1912/2160 train_time:109460ms step_avg:57.25ms
step:1913/2160 train_time:109548ms step_avg:57.27ms
step:1914/2160 train_time:109635ms step_avg:57.28ms
step:1915/2160 train_time:109724ms step_avg:57.30ms
step:1916/2160 train_time:109810ms step_avg:57.31ms
step:1917/2160 train_time:109899ms step_avg:57.33ms
step:1918/2160 train_time:109986ms step_avg:57.34ms
step:1919/2160 train_time:110074ms step_avg:57.36ms
step:1920/2160 train_time:110161ms step_avg:57.38ms
step:1921/2160 train_time:110249ms step_avg:57.39ms
step:1922/2160 train_time:110335ms step_avg:57.41ms
step:1923/2160 train_time:110424ms step_avg:57.42ms
step:1924/2160 train_time:110510ms step_avg:57.44ms
step:1925/2160 train_time:110599ms step_avg:57.45ms
step:1926/2160 train_time:110685ms step_avg:57.47ms
step:1927/2160 train_time:110773ms step_avg:57.48ms
step:1928/2160 train_time:110860ms step_avg:57.50ms
step:1929/2160 train_time:110947ms step_avg:57.52ms
step:1930/2160 train_time:111035ms step_avg:57.53ms
step:1931/2160 train_time:111124ms step_avg:57.55ms
step:1932/2160 train_time:111210ms step_avg:57.56ms
step:1933/2160 train_time:111298ms step_avg:57.58ms
step:1934/2160 train_time:111384ms step_avg:57.59ms
step:1935/2160 train_time:111472ms step_avg:57.61ms
step:1936/2160 train_time:111560ms step_avg:57.62ms
step:1937/2160 train_time:111648ms step_avg:57.64ms
step:1938/2160 train_time:111735ms step_avg:57.65ms
step:1939/2160 train_time:111824ms step_avg:57.67ms
step:1940/2160 train_time:111910ms step_avg:57.69ms
step:1941/2160 train_time:111999ms step_avg:57.70ms
step:1942/2160 train_time:112085ms step_avg:57.72ms
step:1943/2160 train_time:112173ms step_avg:57.73ms
step:1944/2160 train_time:112260ms step_avg:57.75ms
step:1945/2160 train_time:112348ms step_avg:57.76ms
step:1946/2160 train_time:112435ms step_avg:57.78ms
step:1947/2160 train_time:112524ms step_avg:57.79ms
step:1948/2160 train_time:112611ms step_avg:57.81ms
step:1949/2160 train_time:112698ms step_avg:57.82ms
step:1950/2160 train_time:112784ms step_avg:57.84ms
step:1951/2160 train_time:112872ms step_avg:57.85ms
step:1952/2160 train_time:112959ms step_avg:57.87ms
step:1953/2160 train_time:113046ms step_avg:57.88ms
step:1954/2160 train_time:113133ms step_avg:57.90ms
step:1955/2160 train_time:113222ms step_avg:57.91ms
step:1956/2160 train_time:113308ms step_avg:57.93ms
step:1957/2160 train_time:113398ms step_avg:57.94ms
step:1958/2160 train_time:113485ms step_avg:57.96ms
step:1959/2160 train_time:113572ms step_avg:57.97ms
step:1960/2160 train_time:113659ms step_avg:57.99ms
step:1961/2160 train_time:113747ms step_avg:58.00ms
step:1962/2160 train_time:113834ms step_avg:58.02ms
step:1963/2160 train_time:113923ms step_avg:58.04ms
step:1964/2160 train_time:114009ms step_avg:58.05ms
step:1965/2160 train_time:114099ms step_avg:58.07ms
step:1966/2160 train_time:114185ms step_avg:58.08ms
step:1967/2160 train_time:114273ms step_avg:58.10ms
step:1968/2160 train_time:114361ms step_avg:58.11ms
step:1969/2160 train_time:114448ms step_avg:58.13ms
step:1970/2160 train_time:114535ms step_avg:58.14ms
step:1971/2160 train_time:114623ms step_avg:58.15ms
step:1972/2160 train_time:114710ms step_avg:58.17ms
step:1973/2160 train_time:114799ms step_avg:58.19ms
step:1974/2160 train_time:114885ms step_avg:58.20ms
step:1975/2160 train_time:114974ms step_avg:58.21ms
step:1976/2160 train_time:115060ms step_avg:58.23ms
step:1977/2160 train_time:115149ms step_avg:58.24ms
step:1978/2160 train_time:115235ms step_avg:58.26ms
step:1979/2160 train_time:115324ms step_avg:58.27ms
step:1980/2160 train_time:115410ms step_avg:58.29ms
step:1981/2160 train_time:115498ms step_avg:58.30ms
step:1982/2160 train_time:115585ms step_avg:58.32ms
step:1983/2160 train_time:115672ms step_avg:58.33ms
step:1984/2160 train_time:115759ms step_avg:58.35ms
step:1985/2160 train_time:115847ms step_avg:58.36ms
step:1986/2160 train_time:115935ms step_avg:58.38ms
step:1987/2160 train_time:116023ms step_avg:58.39ms
step:1988/2160 train_time:116110ms step_avg:58.41ms
step:1989/2160 train_time:116198ms step_avg:58.42ms
step:1990/2160 train_time:116285ms step_avg:58.43ms
step:1991/2160 train_time:116373ms step_avg:58.45ms
step:1992/2160 train_time:116461ms step_avg:58.46ms
step:1993/2160 train_time:116549ms step_avg:58.48ms
step:1994/2160 train_time:116636ms step_avg:58.49ms
step:1995/2160 train_time:116725ms step_avg:58.51ms
step:1996/2160 train_time:116811ms step_avg:58.52ms
step:1997/2160 train_time:116901ms step_avg:58.54ms
step:1998/2160 train_time:116987ms step_avg:58.55ms
step:1999/2160 train_time:117075ms step_avg:58.57ms
step:2000/2160 train_time:117162ms step_avg:58.58ms
step:2000/2160 val_loss:3.3116 train_time:117252ms step_avg:58.63ms
step:2001/2160 train_time:117274ms step_avg:58.61ms
step:2002/2160 train_time:117342ms step_avg:58.61ms
step:2003/2160 train_time:117434ms step_avg:58.63ms
step:2004/2160 train_time:117520ms step_avg:58.64ms
step:2005/2160 train_time:117609ms step_avg:58.66ms
step:2006/2160 train_time:117695ms step_avg:58.67ms
step:2007/2160 train_time:117782ms step_avg:58.69ms
step:2008/2160 train_time:117867ms step_avg:58.70ms
step:2009/2160 train_time:117954ms step_avg:58.71ms
step:2010/2160 train_time:118039ms step_avg:58.73ms
step:2011/2160 train_time:118126ms step_avg:58.74ms
step:2012/2160 train_time:118214ms step_avg:58.75ms
step:2013/2160 train_time:118304ms step_avg:58.77ms
step:2014/2160 train_time:118395ms step_avg:58.79ms
step:2015/2160 train_time:118485ms step_avg:58.80ms
step:2016/2160 train_time:118572ms step_avg:58.82ms
step:2017/2160 train_time:118659ms step_avg:58.83ms
step:2018/2160 train_time:118744ms step_avg:58.84ms
step:2019/2160 train_time:118832ms step_avg:58.86ms
step:2020/2160 train_time:118917ms step_avg:58.87ms
step:2021/2160 train_time:119004ms step_avg:58.88ms
step:2022/2160 train_time:119090ms step_avg:58.90ms
step:2023/2160 train_time:119178ms step_avg:58.91ms
step:2024/2160 train_time:119266ms step_avg:58.93ms
step:2025/2160 train_time:119357ms step_avg:58.94ms
step:2026/2160 train_time:119445ms step_avg:58.96ms
step:2027/2160 train_time:119534ms step_avg:58.97ms
step:2028/2160 train_time:119621ms step_avg:58.98ms
step:2029/2160 train_time:119709ms step_avg:59.00ms
step:2030/2160 train_time:119796ms step_avg:59.01ms
step:2031/2160 train_time:119882ms step_avg:59.03ms
step:2032/2160 train_time:119967ms step_avg:59.04ms
step:2033/2160 train_time:120055ms step_avg:59.05ms
step:2034/2160 train_time:120140ms step_avg:59.07ms
step:2035/2160 train_time:120230ms step_avg:59.08ms
step:2036/2160 train_time:120318ms step_avg:59.10ms
step:2037/2160 train_time:120407ms step_avg:59.11ms
step:2038/2160 train_time:120495ms step_avg:59.12ms
step:2039/2160 train_time:120583ms step_avg:59.14ms
step:2040/2160 train_time:120670ms step_avg:59.15ms
step:2041/2160 train_time:120758ms step_avg:59.17ms
step:2042/2160 train_time:120844ms step_avg:59.18ms
step:2043/2160 train_time:120932ms step_avg:59.19ms
step:2044/2160 train_time:121017ms step_avg:59.21ms
step:2045/2160 train_time:121105ms step_avg:59.22ms
step:2046/2160 train_time:121192ms step_avg:59.23ms
step:2047/2160 train_time:121280ms step_avg:59.25ms
step:2048/2160 train_time:121368ms step_avg:59.26ms
step:2049/2160 train_time:121457ms step_avg:59.28ms
step:2050/2160 train_time:121544ms step_avg:59.29ms
step:2051/2160 train_time:121635ms step_avg:59.31ms
step:2052/2160 train_time:121721ms step_avg:59.32ms
step:2053/2160 train_time:121809ms step_avg:59.33ms
step:2054/2160 train_time:121894ms step_avg:59.34ms
step:2055/2160 train_time:121982ms step_avg:59.36ms
step:2056/2160 train_time:122067ms step_avg:59.37ms
step:2057/2160 train_time:122156ms step_avg:59.39ms
step:2058/2160 train_time:122242ms step_avg:59.40ms
step:2059/2160 train_time:122332ms step_avg:59.41ms
step:2060/2160 train_time:122418ms step_avg:59.43ms
step:2061/2160 train_time:122508ms step_avg:59.44ms
step:2062/2160 train_time:122595ms step_avg:59.45ms
step:2063/2160 train_time:122683ms step_avg:59.47ms
step:2064/2160 train_time:122770ms step_avg:59.48ms
step:2065/2160 train_time:122858ms step_avg:59.50ms
step:2066/2160 train_time:122944ms step_avg:59.51ms
step:2067/2160 train_time:123032ms step_avg:59.52ms
step:2068/2160 train_time:123119ms step_avg:59.54ms
step:2069/2160 train_time:123206ms step_avg:59.55ms
step:2070/2160 train_time:123293ms step_avg:59.56ms
step:2071/2160 train_time:123381ms step_avg:59.58ms
step:2072/2160 train_time:123469ms step_avg:59.59ms
step:2073/2160 train_time:123558ms step_avg:59.60ms
step:2074/2160 train_time:123644ms step_avg:59.62ms
step:2075/2160 train_time:123734ms step_avg:59.63ms
step:2076/2160 train_time:123819ms step_avg:59.64ms
step:2077/2160 train_time:123906ms step_avg:59.66ms
step:2078/2160 train_time:123993ms step_avg:59.67ms
step:2079/2160 train_time:124081ms step_avg:59.68ms
step:2080/2160 train_time:124167ms step_avg:59.70ms
step:2081/2160 train_time:124256ms step_avg:59.71ms
step:2082/2160 train_time:124343ms step_avg:59.72ms
step:2083/2160 train_time:124432ms step_avg:59.74ms
step:2084/2160 train_time:124518ms step_avg:59.75ms
step:2085/2160 train_time:124607ms step_avg:59.76ms
step:2086/2160 train_time:124694ms step_avg:59.78ms
step:2087/2160 train_time:124782ms step_avg:59.79ms
step:2088/2160 train_time:124868ms step_avg:59.80ms
step:2089/2160 train_time:124956ms step_avg:59.82ms
step:2090/2160 train_time:125042ms step_avg:59.83ms
step:2091/2160 train_time:125130ms step_avg:59.84ms
step:2092/2160 train_time:125218ms step_avg:59.86ms
step:2093/2160 train_time:125305ms step_avg:59.87ms
step:2094/2160 train_time:125394ms step_avg:59.88ms
step:2095/2160 train_time:125482ms step_avg:59.90ms
step:2096/2160 train_time:125568ms step_avg:59.91ms
step:2097/2160 train_time:125657ms step_avg:59.92ms
step:2098/2160 train_time:125743ms step_avg:59.93ms
step:2099/2160 train_time:125832ms step_avg:59.95ms
step:2100/2160 train_time:125918ms step_avg:59.96ms
step:2101/2160 train_time:126007ms step_avg:59.97ms
step:2102/2160 train_time:126094ms step_avg:59.99ms
step:2103/2160 train_time:126182ms step_avg:60.00ms
step:2104/2160 train_time:126268ms step_avg:60.01ms
step:2105/2160 train_time:126357ms step_avg:60.03ms
step:2106/2160 train_time:126443ms step_avg:60.04ms
step:2107/2160 train_time:126532ms step_avg:60.05ms
step:2108/2160 train_time:126619ms step_avg:60.07ms
step:2109/2160 train_time:126706ms step_avg:60.08ms
step:2110/2160 train_time:126793ms step_avg:60.09ms
step:2111/2160 train_time:126882ms step_avg:60.10ms
step:2112/2160 train_time:126969ms step_avg:60.12ms
step:2113/2160 train_time:127057ms step_avg:60.13ms
step:2114/2160 train_time:127144ms step_avg:60.14ms
step:2115/2160 train_time:127232ms step_avg:60.16ms
step:2116/2160 train_time:127318ms step_avg:60.17ms
step:2117/2160 train_time:127406ms step_avg:60.18ms
step:2118/2160 train_time:127492ms step_avg:60.19ms
step:2119/2160 train_time:127580ms step_avg:60.21ms
step:2120/2160 train_time:127666ms step_avg:60.22ms
step:2121/2160 train_time:127756ms step_avg:60.23ms
step:2122/2160 train_time:127842ms step_avg:60.25ms
step:2123/2160 train_time:127931ms step_avg:60.26ms
step:2124/2160 train_time:128017ms step_avg:60.27ms
step:2125/2160 train_time:128106ms step_avg:60.29ms
step:2126/2160 train_time:128194ms step_avg:60.30ms
step:2127/2160 train_time:128282ms step_avg:60.31ms
step:2128/2160 train_time:128369ms step_avg:60.32ms
step:2129/2160 train_time:128458ms step_avg:60.34ms
step:2130/2160 train_time:128544ms step_avg:60.35ms
step:2131/2160 train_time:128633ms step_avg:60.36ms
step:2132/2160 train_time:128719ms step_avg:60.37ms
step:2133/2160 train_time:128808ms step_avg:60.39ms
step:2134/2160 train_time:128896ms step_avg:60.40ms
step:2135/2160 train_time:128984ms step_avg:60.41ms
step:2136/2160 train_time:129071ms step_avg:60.43ms
step:2137/2160 train_time:129159ms step_avg:60.44ms
step:2138/2160 train_time:129245ms step_avg:60.45ms
step:2139/2160 train_time:129334ms step_avg:60.46ms
step:2140/2160 train_time:129421ms step_avg:60.48ms
step:2141/2160 train_time:129509ms step_avg:60.49ms
step:2142/2160 train_time:129596ms step_avg:60.50ms
step:2143/2160 train_time:129684ms step_avg:60.52ms
step:2144/2160 train_time:129771ms step_avg:60.53ms
step:2145/2160 train_time:129859ms step_avg:60.54ms
step:2146/2160 train_time:129946ms step_avg:60.55ms
step:2147/2160 train_time:130036ms step_avg:60.57ms
step:2148/2160 train_time:130123ms step_avg:60.58ms
step:2149/2160 train_time:130212ms step_avg:60.59ms
step:2150/2160 train_time:130298ms step_avg:60.60ms
step:2151/2160 train_time:130386ms step_avg:60.62ms
step:2152/2160 train_time:130473ms step_avg:60.63ms
step:2153/2160 train_time:130562ms step_avg:60.64ms
step:2154/2160 train_time:130649ms step_avg:60.65ms
step:2155/2160 train_time:130737ms step_avg:60.67ms
step:2156/2160 train_time:130824ms step_avg:60.68ms
step:2157/2160 train_time:130913ms step_avg:60.69ms
step:2158/2160 train_time:131000ms step_avg:60.70ms
step:2159/2160 train_time:131088ms step_avg:60.72ms
step:2160/2160 train_time:131175ms step_avg:60.73ms
step:2160/2160 val_loss:3.2748 train_time:131265ms step_avg:60.77ms
peak memory allocated: 29892 MiB reserved: 45096 MiB

import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:35:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   45C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   45C    P0            132W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           51784      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           51785      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           51786      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           51787      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           51788      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           51789      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           51790      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           51791      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           51785      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           51786      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           51787      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           51788      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           51789      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           51790      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           51791      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2160 train_time:98ms step_avg:98.43ms
step:2/2160 train_time:129ms step_avg:64.58ms
step:3/2160 train_time:156ms step_avg:51.90ms
step:4/2160 train_time:184ms step_avg:46.06ms
step:5/2160 train_time:210ms step_avg:42.00ms
step:6/2160 train_time:333ms step_avg:55.45ms
step:7/2160 train_time:364ms step_avg:51.95ms
step:8/2160 train_time:396ms step_avg:49.49ms
step:9/2160 train_time:429ms step_avg:47.65ms
step:10/2160 train_time:461ms step_avg:46.12ms
step:11/2160 train_time:495ms step_avg:44.96ms
step:12/2160 train_time:527ms step_avg:43.92ms
step:13/2160 train_time:560ms step_avg:43.10ms
step:14/2160 train_time:593ms step_avg:42.34ms
step:15/2160 train_time:626ms step_avg:41.74ms
step:16/2160 train_time:659ms step_avg:41.17ms
step:17/2160 train_time:692ms step_avg:40.69ms
step:18/2160 train_time:724ms step_avg:40.23ms
step:19/2160 train_time:758ms step_avg:39.87ms
step:20/2160 train_time:790ms step_avg:39.50ms
step:21/2160 train_time:823ms step_avg:39.20ms
step:22/2160 train_time:856ms step_avg:38.90ms
step:23/2160 train_time:889ms step_avg:38.65ms
step:24/2160 train_time:922ms step_avg:38.40ms
step:25/2160 train_time:955ms step_avg:38.20ms
step:26/2160 train_time:987ms step_avg:37.98ms
step:27/2160 train_time:1021ms step_avg:37.80ms
step:28/2160 train_time:1053ms step_avg:37.61ms
step:29/2160 train_time:1086ms step_avg:37.45ms
step:30/2160 train_time:1118ms step_avg:37.28ms
step:31/2160 train_time:1152ms step_avg:37.15ms
step:32/2160 train_time:1184ms step_avg:37.00ms
step:33/2160 train_time:1218ms step_avg:36.90ms
step:34/2160 train_time:1250ms step_avg:36.77ms
step:35/2160 train_time:1285ms step_avg:36.71ms
step:36/2160 train_time:1318ms step_avg:36.60ms
step:37/2160 train_time:1352ms step_avg:36.55ms
step:38/2160 train_time:1385ms step_avg:36.45ms
step:39/2160 train_time:1419ms step_avg:36.39ms
step:40/2160 train_time:1452ms step_avg:36.30ms
step:41/2160 train_time:1486ms step_avg:36.23ms
step:42/2160 train_time:1518ms step_avg:36.14ms
step:43/2160 train_time:1552ms step_avg:36.08ms
step:44/2160 train_time:1584ms step_avg:36.00ms
step:45/2160 train_time:1618ms step_avg:35.95ms
step:46/2160 train_time:1651ms step_avg:35.88ms
step:47/2160 train_time:1684ms step_avg:35.83ms
step:48/2160 train_time:1717ms step_avg:35.76ms
step:49/2160 train_time:1750ms step_avg:35.71ms
step:50/2160 train_time:1782ms step_avg:35.64ms
step:51/2160 train_time:1816ms step_avg:35.60ms
step:52/2160 train_time:1848ms step_avg:35.54ms
step:53/2160 train_time:1882ms step_avg:35.50ms
step:54/2160 train_time:1914ms step_avg:35.44ms
step:55/2160 train_time:1947ms step_avg:35.40ms
step:56/2160 train_time:1980ms step_avg:35.35ms
step:57/2160 train_time:2013ms step_avg:35.32ms
step:58/2160 train_time:2046ms step_avg:35.27ms
step:59/2160 train_time:2079ms step_avg:35.23ms
step:60/2160 train_time:2111ms step_avg:35.18ms
step:61/2160 train_time:2144ms step_avg:35.15ms
step:62/2160 train_time:2177ms step_avg:35.11ms
step:63/2160 train_time:2210ms step_avg:35.08ms
step:64/2160 train_time:2243ms step_avg:35.04ms
step:65/2160 train_time:2277ms step_avg:35.03ms
step:66/2160 train_time:2310ms step_avg:34.99ms
step:67/2160 train_time:2343ms step_avg:34.97ms
step:68/2160 train_time:2376ms step_avg:34.94ms
step:69/2160 train_time:2409ms step_avg:34.91ms
step:70/2160 train_time:2442ms step_avg:34.88ms
step:71/2160 train_time:2475ms step_avg:34.86ms
step:72/2160 train_time:2508ms step_avg:34.83ms
step:73/2160 train_time:2542ms step_avg:34.82ms
step:74/2160 train_time:2574ms step_avg:34.79ms
step:75/2160 train_time:2608ms step_avg:34.77ms
step:76/2160 train_time:2640ms step_avg:34.74ms
step:77/2160 train_time:2674ms step_avg:34.73ms
step:78/2160 train_time:2707ms step_avg:34.70ms
step:79/2160 train_time:2740ms step_avg:34.69ms
step:80/2160 train_time:2773ms step_avg:34.66ms
step:81/2160 train_time:2806ms step_avg:34.64ms
step:82/2160 train_time:2838ms step_avg:34.61ms
step:83/2160 train_time:2872ms step_avg:34.60ms
step:84/2160 train_time:2904ms step_avg:34.57ms
step:85/2160 train_time:2937ms step_avg:34.56ms
step:86/2160 train_time:2970ms step_avg:34.53ms
step:87/2160 train_time:3003ms step_avg:34.52ms
step:88/2160 train_time:3035ms step_avg:34.49ms
step:89/2160 train_time:3068ms step_avg:34.48ms
step:90/2160 train_time:3101ms step_avg:34.45ms
step:91/2160 train_time:3134ms step_avg:34.44ms
step:92/2160 train_time:3166ms step_avg:34.41ms
step:93/2160 train_time:3199ms step_avg:34.40ms
step:94/2160 train_time:3232ms step_avg:34.38ms
step:95/2160 train_time:3265ms step_avg:34.37ms
step:96/2160 train_time:3297ms step_avg:34.35ms
step:97/2160 train_time:3331ms step_avg:34.34ms
step:98/2160 train_time:3364ms step_avg:34.32ms
step:99/2160 train_time:3397ms step_avg:34.32ms
step:100/2160 train_time:3430ms step_avg:34.30ms
step:101/2160 train_time:3463ms step_avg:34.29ms
step:102/2160 train_time:3496ms step_avg:34.27ms
step:103/2160 train_time:3529ms step_avg:34.26ms
step:104/2160 train_time:3562ms step_avg:34.25ms
step:105/2160 train_time:3595ms step_avg:34.24ms
step:106/2160 train_time:3628ms step_avg:34.22ms
step:107/2160 train_time:3661ms step_avg:34.22ms
step:108/2160 train_time:3694ms step_avg:34.20ms
step:109/2160 train_time:3727ms step_avg:34.19ms
step:110/2160 train_time:3759ms step_avg:34.18ms
step:111/2160 train_time:3793ms step_avg:34.17ms
step:112/2160 train_time:3825ms step_avg:34.15ms
step:113/2160 train_time:3859ms step_avg:34.15ms
step:114/2160 train_time:3891ms step_avg:34.14ms
step:115/2160 train_time:3924ms step_avg:34.13ms
step:116/2160 train_time:3957ms step_avg:34.11ms
step:117/2160 train_time:3990ms step_avg:34.10ms
step:118/2160 train_time:4022ms step_avg:34.09ms
step:119/2160 train_time:4055ms step_avg:34.08ms
step:120/2160 train_time:4087ms step_avg:34.06ms
step:121/2160 train_time:4121ms step_avg:34.06ms
step:122/2160 train_time:4153ms step_avg:34.04ms
step:123/2160 train_time:4186ms step_avg:34.04ms
step:124/2160 train_time:4219ms step_avg:34.02ms
step:125/2160 train_time:4252ms step_avg:34.02ms
step:126/2160 train_time:4285ms step_avg:34.01ms
step:127/2160 train_time:4319ms step_avg:34.00ms
step:128/2160 train_time:4351ms step_avg:33.99ms
step:129/2160 train_time:4385ms step_avg:33.99ms
step:130/2160 train_time:4417ms step_avg:33.98ms
step:131/2160 train_time:4450ms step_avg:33.97ms
step:132/2160 train_time:4483ms step_avg:33.96ms
step:133/2160 train_time:4516ms step_avg:33.96ms
step:134/2160 train_time:4549ms step_avg:33.95ms
step:135/2160 train_time:4582ms step_avg:33.94ms
step:136/2160 train_time:4615ms step_avg:33.93ms
step:137/2160 train_time:4648ms step_avg:33.93ms
step:138/2160 train_time:4680ms step_avg:33.92ms
step:139/2160 train_time:4714ms step_avg:33.91ms
step:140/2160 train_time:4746ms step_avg:33.90ms
step:141/2160 train_time:4780ms step_avg:33.90ms
step:142/2160 train_time:4812ms step_avg:33.89ms
step:143/2160 train_time:4846ms step_avg:33.89ms
step:144/2160 train_time:4878ms step_avg:33.87ms
step:145/2160 train_time:4911ms step_avg:33.87ms
step:146/2160 train_time:4944ms step_avg:33.86ms
step:147/2160 train_time:4977ms step_avg:33.86ms
step:148/2160 train_time:5010ms step_avg:33.85ms
step:149/2160 train_time:5043ms step_avg:33.85ms
step:150/2160 train_time:5076ms step_avg:33.84ms
step:151/2160 train_time:5109ms step_avg:33.83ms
step:152/2160 train_time:5141ms step_avg:33.83ms
step:153/2160 train_time:5174ms step_avg:33.82ms
step:154/2160 train_time:5207ms step_avg:33.81ms
step:155/2160 train_time:5240ms step_avg:33.81ms
step:156/2160 train_time:5272ms step_avg:33.80ms
step:157/2160 train_time:5306ms step_avg:33.80ms
step:158/2160 train_time:5338ms step_avg:33.79ms
step:159/2160 train_time:5371ms step_avg:33.78ms
step:160/2160 train_time:5404ms step_avg:33.77ms
step:161/2160 train_time:5438ms step_avg:33.77ms
step:162/2160 train_time:5470ms step_avg:33.77ms
step:163/2160 train_time:5503ms step_avg:33.76ms
step:164/2160 train_time:5536ms step_avg:33.75ms
step:165/2160 train_time:5569ms step_avg:33.75ms
step:166/2160 train_time:5601ms step_avg:33.74ms
step:167/2160 train_time:5634ms step_avg:33.74ms
step:168/2160 train_time:5666ms step_avg:33.73ms
step:169/2160 train_time:5700ms step_avg:33.73ms
step:170/2160 train_time:5732ms step_avg:33.72ms
step:171/2160 train_time:5766ms step_avg:33.72ms
step:172/2160 train_time:5798ms step_avg:33.71ms
step:173/2160 train_time:5831ms step_avg:33.71ms
step:174/2160 train_time:5864ms step_avg:33.70ms
step:175/2160 train_time:5897ms step_avg:33.70ms
step:176/2160 train_time:5930ms step_avg:33.69ms
step:177/2160 train_time:5963ms step_avg:33.69ms
step:178/2160 train_time:5996ms step_avg:33.68ms
step:179/2160 train_time:6029ms step_avg:33.68ms
step:180/2160 train_time:6061ms step_avg:33.67ms
step:181/2160 train_time:6094ms step_avg:33.67ms
step:182/2160 train_time:6126ms step_avg:33.66ms
step:183/2160 train_time:6160ms step_avg:33.66ms
step:184/2160 train_time:6192ms step_avg:33.65ms
step:185/2160 train_time:6225ms step_avg:33.65ms
step:186/2160 train_time:6258ms step_avg:33.64ms
step:187/2160 train_time:6291ms step_avg:33.64ms
step:188/2160 train_time:6323ms step_avg:33.63ms
step:189/2160 train_time:6356ms step_avg:33.63ms
step:190/2160 train_time:6389ms step_avg:33.63ms
step:191/2160 train_time:6422ms step_avg:33.62ms
step:192/2160 train_time:6455ms step_avg:33.62ms
step:193/2160 train_time:6488ms step_avg:33.62ms
step:194/2160 train_time:6520ms step_avg:33.61ms
step:195/2160 train_time:6553ms step_avg:33.61ms
step:196/2160 train_time:6585ms step_avg:33.60ms
step:197/2160 train_time:6619ms step_avg:33.60ms
step:198/2160 train_time:6652ms step_avg:33.59ms
step:199/2160 train_time:6685ms step_avg:33.59ms
step:200/2160 train_time:6717ms step_avg:33.58ms
step:201/2160 train_time:6750ms step_avg:33.58ms
step:202/2160 train_time:6782ms step_avg:33.58ms
step:203/2160 train_time:6816ms step_avg:33.58ms
step:204/2160 train_time:6848ms step_avg:33.57ms
step:205/2160 train_time:6883ms step_avg:33.57ms
step:206/2160 train_time:6915ms step_avg:33.57ms
step:207/2160 train_time:6948ms step_avg:33.57ms
step:208/2160 train_time:6981ms step_avg:33.56ms
step:209/2160 train_time:7014ms step_avg:33.56ms
step:210/2160 train_time:7046ms step_avg:33.55ms
step:211/2160 train_time:7080ms step_avg:33.55ms
step:212/2160 train_time:7112ms step_avg:33.55ms
step:213/2160 train_time:7145ms step_avg:33.55ms
step:214/2160 train_time:7177ms step_avg:33.54ms
step:215/2160 train_time:7211ms step_avg:33.54ms
step:216/2160 train_time:7243ms step_avg:33.53ms
step:217/2160 train_time:7277ms step_avg:33.53ms
step:218/2160 train_time:7309ms step_avg:33.53ms
step:219/2160 train_time:7342ms step_avg:33.53ms
step:220/2160 train_time:7375ms step_avg:33.52ms
step:221/2160 train_time:7408ms step_avg:33.52ms
step:222/2160 train_time:7440ms step_avg:33.51ms
step:223/2160 train_time:7473ms step_avg:33.51ms
step:224/2160 train_time:7506ms step_avg:33.51ms
step:225/2160 train_time:7539ms step_avg:33.51ms
step:226/2160 train_time:7571ms step_avg:33.50ms
step:227/2160 train_time:7604ms step_avg:33.50ms
step:228/2160 train_time:7637ms step_avg:33.49ms
step:229/2160 train_time:7670ms step_avg:33.49ms
step:230/2160 train_time:7702ms step_avg:33.49ms
step:231/2160 train_time:7736ms step_avg:33.49ms
step:232/2160 train_time:7768ms step_avg:33.48ms
step:233/2160 train_time:7802ms step_avg:33.48ms
step:234/2160 train_time:7834ms step_avg:33.48ms
step:235/2160 train_time:7867ms step_avg:33.48ms
step:236/2160 train_time:7899ms step_avg:33.47ms
step:237/2160 train_time:7933ms step_avg:33.47ms
step:238/2160 train_time:7965ms step_avg:33.47ms
step:239/2160 train_time:7999ms step_avg:33.47ms
step:240/2160 train_time:8031ms step_avg:33.46ms
step:241/2160 train_time:8065ms step_avg:33.47ms
step:242/2160 train_time:8098ms step_avg:33.46ms
step:243/2160 train_time:8131ms step_avg:33.46ms
step:244/2160 train_time:8163ms step_avg:33.46ms
step:245/2160 train_time:8197ms step_avg:33.46ms
step:246/2160 train_time:8229ms step_avg:33.45ms
step:247/2160 train_time:8263ms step_avg:33.45ms
step:248/2160 train_time:8295ms step_avg:33.45ms
step:249/2160 train_time:8328ms step_avg:33.45ms
step:250/2160 train_time:8360ms step_avg:33.44ms
step:250/2160 val_loss:4.3119 train_time:8396ms step_avg:33.58ms
step:251/2160 train_time:8417ms step_avg:33.54ms
step:252/2160 train_time:8439ms step_avg:33.49ms
step:253/2160 train_time:8463ms step_avg:33.45ms
step:254/2160 train_time:8495ms step_avg:33.45ms
step:255/2160 train_time:8530ms step_avg:33.45ms
step:256/2160 train_time:8564ms step_avg:33.45ms
step:257/2160 train_time:8599ms step_avg:33.46ms
step:258/2160 train_time:8631ms step_avg:33.45ms
step:259/2160 train_time:8664ms step_avg:33.45ms
step:260/2160 train_time:8697ms step_avg:33.45ms
step:261/2160 train_time:8730ms step_avg:33.45ms
step:262/2160 train_time:8763ms step_avg:33.45ms
step:263/2160 train_time:8795ms step_avg:33.44ms
step:264/2160 train_time:8828ms step_avg:33.44ms
step:265/2160 train_time:8861ms step_avg:33.44ms
step:266/2160 train_time:8893ms step_avg:33.43ms
step:267/2160 train_time:8926ms step_avg:33.43ms
step:268/2160 train_time:8958ms step_avg:33.43ms
step:269/2160 train_time:8991ms step_avg:33.42ms
step:270/2160 train_time:9023ms step_avg:33.42ms
step:271/2160 train_time:9057ms step_avg:33.42ms
step:272/2160 train_time:9089ms step_avg:33.42ms
step:273/2160 train_time:9122ms step_avg:33.41ms
step:274/2160 train_time:9154ms step_avg:33.41ms
step:275/2160 train_time:9187ms step_avg:33.41ms
step:276/2160 train_time:9219ms step_avg:33.40ms
step:277/2160 train_time:9252ms step_avg:33.40ms
step:278/2160 train_time:9285ms step_avg:33.40ms
step:279/2160 train_time:9318ms step_avg:33.40ms
step:280/2160 train_time:9350ms step_avg:33.39ms
step:281/2160 train_time:9383ms step_avg:33.39ms
step:282/2160 train_time:9416ms step_avg:33.39ms
step:283/2160 train_time:9449ms step_avg:33.39ms
step:284/2160 train_time:9481ms step_avg:33.39ms
step:285/2160 train_time:9515ms step_avg:33.39ms
step:286/2160 train_time:9548ms step_avg:33.38ms
step:287/2160 train_time:9582ms step_avg:33.39ms
step:288/2160 train_time:9615ms step_avg:33.38ms
step:289/2160 train_time:9648ms step_avg:33.38ms
step:290/2160 train_time:9680ms step_avg:33.38ms
step:291/2160 train_time:9714ms step_avg:33.38ms
step:292/2160 train_time:9746ms step_avg:33.38ms
step:293/2160 train_time:9779ms step_avg:33.38ms
step:294/2160 train_time:9812ms step_avg:33.37ms
step:295/2160 train_time:9845ms step_avg:33.37ms
step:296/2160 train_time:9877ms step_avg:33.37ms
step:297/2160 train_time:9910ms step_avg:33.37ms
step:298/2160 train_time:9942ms step_avg:33.36ms
step:299/2160 train_time:9976ms step_avg:33.36ms
step:300/2160 train_time:10008ms step_avg:33.36ms
step:301/2160 train_time:10041ms step_avg:33.36ms
step:302/2160 train_time:10073ms step_avg:33.36ms
step:303/2160 train_time:10106ms step_avg:33.35ms
step:304/2160 train_time:10138ms step_avg:33.35ms
step:305/2160 train_time:10171ms step_avg:33.35ms
step:306/2160 train_time:10203ms step_avg:33.34ms
step:307/2160 train_time:10237ms step_avg:33.34ms
step:308/2160 train_time:10269ms step_avg:33.34ms
step:309/2160 train_time:10302ms step_avg:33.34ms
step:310/2160 train_time:10334ms step_avg:33.34ms
step:311/2160 train_time:10367ms step_avg:33.34ms
step:312/2160 train_time:10399ms step_avg:33.33ms
step:313/2160 train_time:10433ms step_avg:33.33ms
step:314/2160 train_time:10465ms step_avg:33.33ms
step:315/2160 train_time:10499ms step_avg:33.33ms
step:316/2160 train_time:10531ms step_avg:33.33ms
step:317/2160 train_time:10565ms step_avg:33.33ms
step:318/2160 train_time:10597ms step_avg:33.32ms
step:319/2160 train_time:10630ms step_avg:33.32ms
step:320/2160 train_time:10663ms step_avg:33.32ms
step:321/2160 train_time:10696ms step_avg:33.32ms
step:322/2160 train_time:10729ms step_avg:33.32ms
step:323/2160 train_time:10762ms step_avg:33.32ms
step:324/2160 train_time:10795ms step_avg:33.32ms
step:325/2160 train_time:10828ms step_avg:33.32ms
step:326/2160 train_time:10860ms step_avg:33.31ms
step:327/2160 train_time:10894ms step_avg:33.31ms
step:328/2160 train_time:10926ms step_avg:33.31ms
step:329/2160 train_time:10959ms step_avg:33.31ms
step:330/2160 train_time:10992ms step_avg:33.31ms
step:331/2160 train_time:11024ms step_avg:33.31ms
step:332/2160 train_time:11057ms step_avg:33.30ms
step:333/2160 train_time:11090ms step_avg:33.30ms
step:334/2160 train_time:11122ms step_avg:33.30ms
step:335/2160 train_time:11155ms step_avg:33.30ms
step:336/2160 train_time:11187ms step_avg:33.30ms
step:337/2160 train_time:11220ms step_avg:33.29ms
step:338/2160 train_time:11252ms step_avg:33.29ms
step:339/2160 train_time:11285ms step_avg:33.29ms
step:340/2160 train_time:11318ms step_avg:33.29ms
step:341/2160 train_time:11351ms step_avg:33.29ms
step:342/2160 train_time:11383ms step_avg:33.28ms
step:343/2160 train_time:11416ms step_avg:33.28ms
step:344/2160 train_time:11449ms step_avg:33.28ms
step:345/2160 train_time:11482ms step_avg:33.28ms
step:346/2160 train_time:11514ms step_avg:33.28ms
step:347/2160 train_time:11547ms step_avg:33.28ms
step:348/2160 train_time:11579ms step_avg:33.27ms
step:349/2160 train_time:11613ms step_avg:33.27ms
step:350/2160 train_time:11645ms step_avg:33.27ms
step:351/2160 train_time:11679ms step_avg:33.27ms
step:352/2160 train_time:11711ms step_avg:33.27ms
step:353/2160 train_time:11744ms step_avg:33.27ms
step:354/2160 train_time:11776ms step_avg:33.27ms
step:355/2160 train_time:11810ms step_avg:33.27ms
step:356/2160 train_time:11842ms step_avg:33.27ms
step:357/2160 train_time:11876ms step_avg:33.27ms
step:358/2160 train_time:11908ms step_avg:33.26ms
step:359/2160 train_time:11941ms step_avg:33.26ms
step:360/2160 train_time:11974ms step_avg:33.26ms
step:361/2160 train_time:12006ms step_avg:33.26ms
step:362/2160 train_time:12039ms step_avg:33.26ms
step:363/2160 train_time:12072ms step_avg:33.25ms
step:364/2160 train_time:12104ms step_avg:33.25ms
step:365/2160 train_time:12137ms step_avg:33.25ms
step:366/2160 train_time:12169ms step_avg:33.25ms
step:367/2160 train_time:12202ms step_avg:33.25ms
step:368/2160 train_time:12235ms step_avg:33.25ms
step:369/2160 train_time:12268ms step_avg:33.25ms
step:370/2160 train_time:12300ms step_avg:33.24ms
step:371/2160 train_time:12333ms step_avg:33.24ms
step:372/2160 train_time:12365ms step_avg:33.24ms
step:373/2160 train_time:12399ms step_avg:33.24ms
step:374/2160 train_time:12431ms step_avg:33.24ms
step:375/2160 train_time:12464ms step_avg:33.24ms
step:376/2160 train_time:12497ms step_avg:33.24ms
step:377/2160 train_time:12529ms step_avg:33.23ms
step:378/2160 train_time:12562ms step_avg:33.23ms
step:379/2160 train_time:12595ms step_avg:33.23ms
step:380/2160 train_time:12627ms step_avg:33.23ms
step:381/2160 train_time:12662ms step_avg:33.23ms
step:382/2160 train_time:12694ms step_avg:33.23ms
step:383/2160 train_time:12727ms step_avg:33.23ms
step:384/2160 train_time:12759ms step_avg:33.23ms
step:385/2160 train_time:12793ms step_avg:33.23ms
step:386/2160 train_time:12825ms step_avg:33.23ms
step:387/2160 train_time:12859ms step_avg:33.23ms
step:388/2160 train_time:12891ms step_avg:33.22ms
step:389/2160 train_time:12925ms step_avg:33.23ms
step:390/2160 train_time:12957ms step_avg:33.22ms
step:391/2160 train_time:12990ms step_avg:33.22ms
step:392/2160 train_time:13023ms step_avg:33.22ms
step:393/2160 train_time:13056ms step_avg:33.22ms
step:394/2160 train_time:13088ms step_avg:33.22ms
step:395/2160 train_time:13122ms step_avg:33.22ms
step:396/2160 train_time:13154ms step_avg:33.22ms
step:397/2160 train_time:13187ms step_avg:33.22ms
step:398/2160 train_time:13219ms step_avg:33.21ms
step:399/2160 train_time:13253ms step_avg:33.21ms
step:400/2160 train_time:13285ms step_avg:33.21ms
step:401/2160 train_time:13318ms step_avg:33.21ms
step:402/2160 train_time:13350ms step_avg:33.21ms
step:403/2160 train_time:13383ms step_avg:33.21ms
step:404/2160 train_time:13415ms step_avg:33.21ms
step:405/2160 train_time:13448ms step_avg:33.21ms
step:406/2160 train_time:13481ms step_avg:33.20ms
step:407/2160 train_time:13514ms step_avg:33.20ms
step:408/2160 train_time:13546ms step_avg:33.20ms
step:409/2160 train_time:13579ms step_avg:33.20ms
step:410/2160 train_time:13612ms step_avg:33.20ms
step:411/2160 train_time:13645ms step_avg:33.20ms
step:412/2160 train_time:13677ms step_avg:33.20ms
step:413/2160 train_time:13710ms step_avg:33.20ms
step:414/2160 train_time:13743ms step_avg:33.19ms
step:415/2160 train_time:13776ms step_avg:33.20ms
step:416/2160 train_time:13809ms step_avg:33.19ms
step:417/2160 train_time:13842ms step_avg:33.19ms
step:418/2160 train_time:13875ms step_avg:33.19ms
step:419/2160 train_time:13908ms step_avg:33.19ms
step:420/2160 train_time:13940ms step_avg:33.19ms
step:421/2160 train_time:13973ms step_avg:33.19ms
step:422/2160 train_time:14006ms step_avg:33.19ms
step:423/2160 train_time:14039ms step_avg:33.19ms
step:424/2160 train_time:14071ms step_avg:33.19ms
step:425/2160 train_time:14105ms step_avg:33.19ms
step:426/2160 train_time:14137ms step_avg:33.19ms
step:427/2160 train_time:14170ms step_avg:33.19ms
step:428/2160 train_time:14203ms step_avg:33.18ms
step:429/2160 train_time:14236ms step_avg:33.18ms
step:430/2160 train_time:14268ms step_avg:33.18ms
step:431/2160 train_time:14302ms step_avg:33.18ms
step:432/2160 train_time:14334ms step_avg:33.18ms
step:433/2160 train_time:14367ms step_avg:33.18ms
step:434/2160 train_time:14399ms step_avg:33.18ms
step:435/2160 train_time:14432ms step_avg:33.18ms
step:436/2160 train_time:14464ms step_avg:33.18ms
step:437/2160 train_time:14498ms step_avg:33.18ms
step:438/2160 train_time:14530ms step_avg:33.17ms
step:439/2160 train_time:14563ms step_avg:33.17ms
step:440/2160 train_time:14595ms step_avg:33.17ms
step:441/2160 train_time:14629ms step_avg:33.17ms
step:442/2160 train_time:14661ms step_avg:33.17ms
step:443/2160 train_time:14694ms step_avg:33.17ms
step:444/2160 train_time:14726ms step_avg:33.17ms
step:445/2160 train_time:14760ms step_avg:33.17ms
step:446/2160 train_time:14793ms step_avg:33.17ms
step:447/2160 train_time:14826ms step_avg:33.17ms
step:448/2160 train_time:14858ms step_avg:33.17ms
step:449/2160 train_time:14892ms step_avg:33.17ms
step:450/2160 train_time:14924ms step_avg:33.16ms
step:451/2160 train_time:14957ms step_avg:33.16ms
step:452/2160 train_time:14989ms step_avg:33.16ms
step:453/2160 train_time:15023ms step_avg:33.16ms
step:454/2160 train_time:15055ms step_avg:33.16ms
step:455/2160 train_time:15088ms step_avg:33.16ms
step:456/2160 train_time:15121ms step_avg:33.16ms
step:457/2160 train_time:15154ms step_avg:33.16ms
step:458/2160 train_time:15187ms step_avg:33.16ms
step:459/2160 train_time:15220ms step_avg:33.16ms
step:460/2160 train_time:15252ms step_avg:33.16ms
step:461/2160 train_time:15285ms step_avg:33.16ms
step:462/2160 train_time:15318ms step_avg:33.16ms
step:463/2160 train_time:15351ms step_avg:33.16ms
step:464/2160 train_time:15383ms step_avg:33.15ms
step:465/2160 train_time:15416ms step_avg:33.15ms
step:466/2160 train_time:15449ms step_avg:33.15ms
step:467/2160 train_time:15482ms step_avg:33.15ms
step:468/2160 train_time:15514ms step_avg:33.15ms
step:469/2160 train_time:15548ms step_avg:33.15ms
step:470/2160 train_time:15580ms step_avg:33.15ms
step:471/2160 train_time:15613ms step_avg:33.15ms
step:472/2160 train_time:15645ms step_avg:33.15ms
step:473/2160 train_time:15679ms step_avg:33.15ms
step:474/2160 train_time:15712ms step_avg:33.15ms
step:475/2160 train_time:15745ms step_avg:33.15ms
step:476/2160 train_time:15777ms step_avg:33.15ms
step:477/2160 train_time:15811ms step_avg:33.15ms
step:478/2160 train_time:15843ms step_avg:33.14ms
step:479/2160 train_time:15876ms step_avg:33.14ms
step:480/2160 train_time:15908ms step_avg:33.14ms
step:481/2160 train_time:15942ms step_avg:33.14ms
step:482/2160 train_time:15975ms step_avg:33.14ms
step:483/2160 train_time:16008ms step_avg:33.14ms
step:484/2160 train_time:16040ms step_avg:33.14ms
step:485/2160 train_time:16073ms step_avg:33.14ms
step:486/2160 train_time:16105ms step_avg:33.14ms
step:487/2160 train_time:16139ms step_avg:33.14ms
step:488/2160 train_time:16171ms step_avg:33.14ms
step:489/2160 train_time:16204ms step_avg:33.14ms
step:490/2160 train_time:16236ms step_avg:33.14ms
step:491/2160 train_time:16270ms step_avg:33.14ms
step:492/2160 train_time:16302ms step_avg:33.13ms
step:493/2160 train_time:16335ms step_avg:33.13ms
step:494/2160 train_time:16368ms step_avg:33.13ms
step:495/2160 train_time:16401ms step_avg:33.13ms
step:496/2160 train_time:16434ms step_avg:33.13ms
step:497/2160 train_time:16467ms step_avg:33.13ms
step:498/2160 train_time:16499ms step_avg:33.13ms
step:499/2160 train_time:16532ms step_avg:33.13ms
step:500/2160 train_time:16564ms step_avg:33.13ms
step:500/2160 val_loss:4.0236 train_time:16600ms step_avg:33.20ms
step:501/2160 train_time:16621ms step_avg:33.18ms
step:502/2160 train_time:16643ms step_avg:33.15ms
step:503/2160 train_time:16666ms step_avg:33.13ms
step:504/2160 train_time:16699ms step_avg:33.13ms
step:505/2160 train_time:16733ms step_avg:33.14ms
step:506/2160 train_time:16767ms step_avg:33.14ms
step:507/2160 train_time:16801ms step_avg:33.14ms
step:508/2160 train_time:16833ms step_avg:33.14ms
step:509/2160 train_time:16866ms step_avg:33.14ms
step:510/2160 train_time:16901ms step_avg:33.14ms
step:511/2160 train_time:16932ms step_avg:33.13ms
step:512/2160 train_time:16964ms step_avg:33.13ms
step:513/2160 train_time:16997ms step_avg:33.13ms
step:514/2160 train_time:17029ms step_avg:33.13ms
step:515/2160 train_time:17062ms step_avg:33.13ms
step:516/2160 train_time:17094ms step_avg:33.13ms
step:517/2160 train_time:17127ms step_avg:33.13ms
step:518/2160 train_time:17160ms step_avg:33.13ms
step:519/2160 train_time:17193ms step_avg:33.13ms
step:520/2160 train_time:17225ms step_avg:33.12ms
step:521/2160 train_time:17258ms step_avg:33.12ms
step:522/2160 train_time:17290ms step_avg:33.12ms
step:523/2160 train_time:17323ms step_avg:33.12ms
step:524/2160 train_time:17355ms step_avg:33.12ms
step:525/2160 train_time:17388ms step_avg:33.12ms
step:526/2160 train_time:17420ms step_avg:33.12ms
step:527/2160 train_time:17453ms step_avg:33.12ms
step:528/2160 train_time:17485ms step_avg:33.12ms
step:529/2160 train_time:17519ms step_avg:33.12ms
step:530/2160 train_time:17551ms step_avg:33.12ms
step:531/2160 train_time:17585ms step_avg:33.12ms
step:532/2160 train_time:17618ms step_avg:33.12ms
step:533/2160 train_time:17652ms step_avg:33.12ms
step:534/2160 train_time:17684ms step_avg:33.12ms
step:535/2160 train_time:17718ms step_avg:33.12ms
step:536/2160 train_time:17750ms step_avg:33.12ms
step:537/2160 train_time:17785ms step_avg:33.12ms
step:538/2160 train_time:17817ms step_avg:33.12ms
step:539/2160 train_time:17851ms step_avg:33.12ms
step:540/2160 train_time:17884ms step_avg:33.12ms
step:541/2160 train_time:17917ms step_avg:33.12ms
step:542/2160 train_time:17949ms step_avg:33.12ms
step:543/2160 train_time:17982ms step_avg:33.12ms
step:544/2160 train_time:18015ms step_avg:33.12ms
step:545/2160 train_time:18048ms step_avg:33.12ms
step:546/2160 train_time:18081ms step_avg:33.11ms
step:547/2160 train_time:18114ms step_avg:33.11ms
step:548/2160 train_time:18146ms step_avg:33.11ms
step:549/2160 train_time:18179ms step_avg:33.11ms
step:550/2160 train_time:18211ms step_avg:33.11ms
step:551/2160 train_time:18244ms step_avg:33.11ms
step:552/2160 train_time:18276ms step_avg:33.11ms
step:553/2160 train_time:18310ms step_avg:33.11ms
step:554/2160 train_time:18342ms step_avg:33.11ms
step:555/2160 train_time:18375ms step_avg:33.11ms
step:556/2160 train_time:18407ms step_avg:33.11ms
step:557/2160 train_time:18440ms step_avg:33.11ms
step:558/2160 train_time:18473ms step_avg:33.10ms
step:559/2160 train_time:18506ms step_avg:33.11ms
step:560/2160 train_time:18539ms step_avg:33.10ms
step:561/2160 train_time:18572ms step_avg:33.11ms
step:562/2160 train_time:18605ms step_avg:33.10ms
step:563/2160 train_time:18639ms step_avg:33.11ms
step:564/2160 train_time:18671ms step_avg:33.10ms
step:565/2160 train_time:18705ms step_avg:33.11ms
step:566/2160 train_time:18737ms step_avg:33.10ms
step:567/2160 train_time:18771ms step_avg:33.11ms
step:568/2160 train_time:18804ms step_avg:33.11ms
step:569/2160 train_time:18837ms step_avg:33.11ms
step:570/2160 train_time:18869ms step_avg:33.10ms
step:571/2160 train_time:18903ms step_avg:33.10ms
step:572/2160 train_time:18935ms step_avg:33.10ms
step:573/2160 train_time:18969ms step_avg:33.10ms
step:574/2160 train_time:19001ms step_avg:33.10ms
step:575/2160 train_time:19034ms step_avg:33.10ms
step:576/2160 train_time:19066ms step_avg:33.10ms
step:577/2160 train_time:19099ms step_avg:33.10ms
step:578/2160 train_time:19132ms step_avg:33.10ms
step:579/2160 train_time:19165ms step_avg:33.10ms
step:580/2160 train_time:19197ms step_avg:33.10ms
step:581/2160 train_time:19230ms step_avg:33.10ms
step:582/2160 train_time:19263ms step_avg:33.10ms
step:583/2160 train_time:19296ms step_avg:33.10ms
step:584/2160 train_time:19328ms step_avg:33.10ms
step:585/2160 train_time:19361ms step_avg:33.10ms
step:586/2160 train_time:19393ms step_avg:33.09ms
step:587/2160 train_time:19427ms step_avg:33.09ms
step:588/2160 train_time:19459ms step_avg:33.09ms
step:589/2160 train_time:19492ms step_avg:33.09ms
step:590/2160 train_time:19524ms step_avg:33.09ms
step:591/2160 train_time:19558ms step_avg:33.09ms
step:592/2160 train_time:19590ms step_avg:33.09ms
step:593/2160 train_time:19624ms step_avg:33.09ms
step:594/2160 train_time:19656ms step_avg:33.09ms
step:595/2160 train_time:19689ms step_avg:33.09ms
step:596/2160 train_time:19722ms step_avg:33.09ms
step:597/2160 train_time:19755ms step_avg:33.09ms
step:598/2160 train_time:19787ms step_avg:33.09ms
step:599/2160 train_time:19821ms step_avg:33.09ms
step:600/2160 train_time:19853ms step_avg:33.09ms
step:601/2160 train_time:19886ms step_avg:33.09ms
step:602/2160 train_time:19918ms step_avg:33.09ms
step:603/2160 train_time:19952ms step_avg:33.09ms
step:604/2160 train_time:19985ms step_avg:33.09ms
step:605/2160 train_time:20019ms step_avg:33.09ms
step:606/2160 train_time:20051ms step_avg:33.09ms
step:607/2160 train_time:20084ms step_avg:33.09ms
step:608/2160 train_time:20117ms step_avg:33.09ms
step:609/2160 train_time:20150ms step_avg:33.09ms
step:610/2160 train_time:20183ms step_avg:33.09ms
step:611/2160 train_time:20216ms step_avg:33.09ms
step:612/2160 train_time:20248ms step_avg:33.08ms
step:613/2160 train_time:20281ms step_avg:33.08ms
step:614/2160 train_time:20313ms step_avg:33.08ms
step:615/2160 train_time:20347ms step_avg:33.08ms
step:616/2160 train_time:20379ms step_avg:33.08ms
step:617/2160 train_time:20412ms step_avg:33.08ms
step:618/2160 train_time:20444ms step_avg:33.08ms
step:619/2160 train_time:20478ms step_avg:33.08ms
step:620/2160 train_time:20510ms step_avg:33.08ms
step:621/2160 train_time:20543ms step_avg:33.08ms
step:622/2160 train_time:20576ms step_avg:33.08ms
step:623/2160 train_time:20609ms step_avg:33.08ms
step:624/2160 train_time:20641ms step_avg:33.08ms
step:625/2160 train_time:20675ms step_avg:33.08ms
step:626/2160 train_time:20707ms step_avg:33.08ms
step:627/2160 train_time:20740ms step_avg:33.08ms
step:628/2160 train_time:20773ms step_avg:33.08ms
step:629/2160 train_time:20806ms step_avg:33.08ms
step:630/2160 train_time:20838ms step_avg:33.08ms
step:631/2160 train_time:20872ms step_avg:33.08ms
step:632/2160 train_time:20904ms step_avg:33.08ms
step:633/2160 train_time:20938ms step_avg:33.08ms
step:634/2160 train_time:20970ms step_avg:33.08ms
step:635/2160 train_time:21004ms step_avg:33.08ms
step:636/2160 train_time:21036ms step_avg:33.08ms
step:637/2160 train_time:21069ms step_avg:33.08ms
step:638/2160 train_time:21102ms step_avg:33.07ms
step:639/2160 train_time:21135ms step_avg:33.07ms
step:640/2160 train_time:21167ms step_avg:33.07ms
step:641/2160 train_time:21200ms step_avg:33.07ms
step:642/2160 train_time:21232ms step_avg:33.07ms
step:643/2160 train_time:21266ms step_avg:33.07ms
step:644/2160 train_time:21298ms step_avg:33.07ms
step:645/2160 train_time:21331ms step_avg:33.07ms
step:646/2160 train_time:21364ms step_avg:33.07ms
step:647/2160 train_time:21397ms step_avg:33.07ms
step:648/2160 train_time:21429ms step_avg:33.07ms
step:649/2160 train_time:21462ms step_avg:33.07ms
step:650/2160 train_time:21495ms step_avg:33.07ms
step:651/2160 train_time:21528ms step_avg:33.07ms
step:652/2160 train_time:21561ms step_avg:33.07ms
step:653/2160 train_time:21594ms step_avg:33.07ms
step:654/2160 train_time:21626ms step_avg:33.07ms
step:655/2160 train_time:21659ms step_avg:33.07ms
step:656/2160 train_time:21692ms step_avg:33.07ms
step:657/2160 train_time:21725ms step_avg:33.07ms
step:658/2160 train_time:21758ms step_avg:33.07ms
step:659/2160 train_time:21791ms step_avg:33.07ms
step:660/2160 train_time:21823ms step_avg:33.07ms
step:661/2160 train_time:21856ms step_avg:33.07ms
step:662/2160 train_time:21889ms step_avg:33.06ms
step:663/2160 train_time:21922ms step_avg:33.07ms
step:664/2160 train_time:21954ms step_avg:33.06ms
step:665/2160 train_time:21988ms step_avg:33.06ms
step:666/2160 train_time:22021ms step_avg:33.06ms
step:667/2160 train_time:22054ms step_avg:33.06ms
step:668/2160 train_time:22086ms step_avg:33.06ms
step:669/2160 train_time:22120ms step_avg:33.06ms
step:670/2160 train_time:22152ms step_avg:33.06ms
step:671/2160 train_time:22185ms step_avg:33.06ms
step:672/2160 train_time:22218ms step_avg:33.06ms
step:673/2160 train_time:22251ms step_avg:33.06ms
step:674/2160 train_time:22284ms step_avg:33.06ms
step:675/2160 train_time:22317ms step_avg:33.06ms
step:676/2160 train_time:22349ms step_avg:33.06ms
step:677/2160 train_time:22383ms step_avg:33.06ms
step:678/2160 train_time:22415ms step_avg:33.06ms
step:679/2160 train_time:22449ms step_avg:33.06ms
step:680/2160 train_time:22481ms step_avg:33.06ms
step:681/2160 train_time:22514ms step_avg:33.06ms
step:682/2160 train_time:22547ms step_avg:33.06ms
step:683/2160 train_time:22580ms step_avg:33.06ms
step:684/2160 train_time:22612ms step_avg:33.06ms
step:685/2160 train_time:22645ms step_avg:33.06ms
step:686/2160 train_time:22678ms step_avg:33.06ms
step:687/2160 train_time:22711ms step_avg:33.06ms
step:688/2160 train_time:22744ms step_avg:33.06ms
step:689/2160 train_time:22777ms step_avg:33.06ms
step:690/2160 train_time:22810ms step_avg:33.06ms
step:691/2160 train_time:22843ms step_avg:33.06ms
step:692/2160 train_time:22875ms step_avg:33.06ms
step:693/2160 train_time:22909ms step_avg:33.06ms
step:694/2160 train_time:22941ms step_avg:33.06ms
step:695/2160 train_time:22974ms step_avg:33.06ms
step:696/2160 train_time:23007ms step_avg:33.06ms
step:697/2160 train_time:23040ms step_avg:33.06ms
step:698/2160 train_time:23073ms step_avg:33.06ms
step:699/2160 train_time:23106ms step_avg:33.06ms
step:700/2160 train_time:23138ms step_avg:33.05ms
step:701/2160 train_time:23172ms step_avg:33.06ms
step:702/2160 train_time:23204ms step_avg:33.05ms
step:703/2160 train_time:23238ms step_avg:33.05ms
step:704/2160 train_time:23270ms step_avg:33.05ms
step:705/2160 train_time:23303ms step_avg:33.05ms
step:706/2160 train_time:23335ms step_avg:33.05ms
step:707/2160 train_time:23369ms step_avg:33.05ms
step:708/2160 train_time:23403ms step_avg:33.05ms
step:709/2160 train_time:23461ms step_avg:33.09ms
step:710/2160 train_time:23520ms step_avg:33.13ms
step:711/2160 train_time:23581ms step_avg:33.17ms
step:712/2160 train_time:23640ms step_avg:33.20ms
step:713/2160 train_time:23701ms step_avg:33.24ms
step:714/2160 train_time:23760ms step_avg:33.28ms
step:715/2160 train_time:23820ms step_avg:33.31ms
step:716/2160 train_time:23879ms step_avg:33.35ms
step:717/2160 train_time:23940ms step_avg:33.39ms
step:718/2160 train_time:23999ms step_avg:33.42ms
step:719/2160 train_time:24059ms step_avg:33.46ms
step:720/2160 train_time:24119ms step_avg:33.50ms
step:721/2160 train_time:24179ms step_avg:33.54ms
step:722/2160 train_time:24238ms step_avg:33.57ms
step:723/2160 train_time:24299ms step_avg:33.61ms
step:724/2160 train_time:24358ms step_avg:33.64ms
step:725/2160 train_time:24418ms step_avg:33.68ms
step:726/2160 train_time:24477ms step_avg:33.72ms
step:727/2160 train_time:24538ms step_avg:33.75ms
step:728/2160 train_time:24597ms step_avg:33.79ms
step:729/2160 train_time:24658ms step_avg:33.82ms
step:730/2160 train_time:24716ms step_avg:33.86ms
step:731/2160 train_time:24777ms step_avg:33.89ms
step:732/2160 train_time:24836ms step_avg:33.93ms
step:733/2160 train_time:24897ms step_avg:33.97ms
step:734/2160 train_time:24955ms step_avg:34.00ms
step:735/2160 train_time:25016ms step_avg:34.04ms
step:736/2160 train_time:25075ms step_avg:34.07ms
step:737/2160 train_time:25136ms step_avg:34.11ms
step:738/2160 train_time:25194ms step_avg:34.14ms
step:739/2160 train_time:25255ms step_avg:34.17ms
step:740/2160 train_time:25313ms step_avg:34.21ms
step:741/2160 train_time:25374ms step_avg:34.24ms
step:742/2160 train_time:25432ms step_avg:34.28ms
step:743/2160 train_time:25493ms step_avg:34.31ms
step:744/2160 train_time:25551ms step_avg:34.34ms
step:745/2160 train_time:25611ms step_avg:34.38ms
step:746/2160 train_time:25669ms step_avg:34.41ms
step:747/2160 train_time:25730ms step_avg:34.44ms
step:748/2160 train_time:25788ms step_avg:34.48ms
step:749/2160 train_time:25848ms step_avg:34.51ms
step:750/2160 train_time:25907ms step_avg:34.54ms
step:750/2160 val_loss:3.8875 train_time:25969ms step_avg:34.63ms
step:751/2160 train_time:25992ms step_avg:34.61ms
step:752/2160 train_time:26027ms step_avg:34.61ms
step:753/2160 train_time:26092ms step_avg:34.65ms
step:754/2160 train_time:26156ms step_avg:34.69ms
step:755/2160 train_time:26216ms step_avg:34.72ms
step:756/2160 train_time:26274ms step_avg:34.75ms
step:757/2160 train_time:26334ms step_avg:34.79ms
step:758/2160 train_time:26392ms step_avg:34.82ms
step:759/2160 train_time:26452ms step_avg:34.85ms
step:760/2160 train_time:26510ms step_avg:34.88ms
step:761/2160 train_time:26569ms step_avg:34.91ms
step:762/2160 train_time:26628ms step_avg:34.94ms
step:763/2160 train_time:26687ms step_avg:34.98ms
step:764/2160 train_time:26745ms step_avg:35.01ms
step:765/2160 train_time:26806ms step_avg:35.04ms
step:766/2160 train_time:26864ms step_avg:35.07ms
step:767/2160 train_time:26925ms step_avg:35.10ms
step:768/2160 train_time:26986ms step_avg:35.14ms
step:769/2160 train_time:27049ms step_avg:35.17ms
step:770/2160 train_time:27111ms step_avg:35.21ms
step:771/2160 train_time:27171ms step_avg:35.24ms
step:772/2160 train_time:27231ms step_avg:35.27ms
step:773/2160 train_time:27291ms step_avg:35.30ms
step:774/2160 train_time:27350ms step_avg:35.34ms
step:775/2160 train_time:27410ms step_avg:35.37ms
step:776/2160 train_time:27469ms step_avg:35.40ms
step:777/2160 train_time:27528ms step_avg:35.43ms
step:778/2160 train_time:27587ms step_avg:35.46ms
step:779/2160 train_time:27647ms step_avg:35.49ms
step:780/2160 train_time:27706ms step_avg:35.52ms
step:781/2160 train_time:27766ms step_avg:35.55ms
step:782/2160 train_time:27824ms step_avg:35.58ms
step:783/2160 train_time:27884ms step_avg:35.61ms
step:784/2160 train_time:27944ms step_avg:35.64ms
step:785/2160 train_time:28006ms step_avg:35.68ms
step:786/2160 train_time:28067ms step_avg:35.71ms
step:787/2160 train_time:28129ms step_avg:35.74ms
step:788/2160 train_time:28188ms step_avg:35.77ms
step:789/2160 train_time:28249ms step_avg:35.80ms
step:790/2160 train_time:28308ms step_avg:35.83ms
step:791/2160 train_time:28369ms step_avg:35.86ms
step:792/2160 train_time:28428ms step_avg:35.89ms
step:793/2160 train_time:28487ms step_avg:35.92ms
step:794/2160 train_time:28546ms step_avg:35.95ms
step:795/2160 train_time:28606ms step_avg:35.98ms
step:796/2160 train_time:28665ms step_avg:36.01ms
step:797/2160 train_time:28724ms step_avg:36.04ms
step:798/2160 train_time:28783ms step_avg:36.07ms
step:799/2160 train_time:28843ms step_avg:36.10ms
step:800/2160 train_time:28902ms step_avg:36.13ms
step:801/2160 train_time:28963ms step_avg:36.16ms
step:802/2160 train_time:29022ms step_avg:36.19ms
step:803/2160 train_time:29085ms step_avg:36.22ms
step:804/2160 train_time:29145ms step_avg:36.25ms
step:805/2160 train_time:29207ms step_avg:36.28ms
step:806/2160 train_time:29267ms step_avg:36.31ms
step:807/2160 train_time:29329ms step_avg:36.34ms
step:808/2160 train_time:29388ms step_avg:36.37ms
step:809/2160 train_time:29448ms step_avg:36.40ms
step:810/2160 train_time:29507ms step_avg:36.43ms
step:811/2160 train_time:29566ms step_avg:36.46ms
step:812/2160 train_time:29625ms step_avg:36.48ms
step:813/2160 train_time:29685ms step_avg:36.51ms
step:814/2160 train_time:29743ms step_avg:36.54ms
step:815/2160 train_time:29804ms step_avg:36.57ms
step:816/2160 train_time:29863ms step_avg:36.60ms
step:817/2160 train_time:29923ms step_avg:36.63ms
step:818/2160 train_time:29982ms step_avg:36.65ms
step:819/2160 train_time:30043ms step_avg:36.68ms
step:820/2160 train_time:30104ms step_avg:36.71ms
step:821/2160 train_time:30165ms step_avg:36.74ms
step:822/2160 train_time:30225ms step_avg:36.77ms
step:823/2160 train_time:30286ms step_avg:36.80ms
step:824/2160 train_time:30346ms step_avg:36.83ms
step:825/2160 train_time:30407ms step_avg:36.86ms
step:826/2160 train_time:30467ms step_avg:36.88ms
step:827/2160 train_time:30526ms step_avg:36.91ms
step:828/2160 train_time:30585ms step_avg:36.94ms
step:829/2160 train_time:30645ms step_avg:36.97ms
step:830/2160 train_time:30704ms step_avg:36.99ms
step:831/2160 train_time:30764ms step_avg:37.02ms
step:832/2160 train_time:30823ms step_avg:37.05ms
step:833/2160 train_time:30883ms step_avg:37.07ms
step:834/2160 train_time:30942ms step_avg:37.10ms
step:835/2160 train_time:31003ms step_avg:37.13ms
step:836/2160 train_time:31063ms step_avg:37.16ms
step:837/2160 train_time:31124ms step_avg:37.19ms
step:838/2160 train_time:31184ms step_avg:37.21ms
step:839/2160 train_time:31245ms step_avg:37.24ms
step:840/2160 train_time:31304ms step_avg:37.27ms
step:841/2160 train_time:31365ms step_avg:37.29ms
step:842/2160 train_time:31425ms step_avg:37.32ms
step:843/2160 train_time:31485ms step_avg:37.35ms
step:844/2160 train_time:31544ms step_avg:37.37ms
step:845/2160 train_time:31605ms step_avg:37.40ms
step:846/2160 train_time:31664ms step_avg:37.43ms
step:847/2160 train_time:31724ms step_avg:37.45ms
step:848/2160 train_time:31783ms step_avg:37.48ms
step:849/2160 train_time:31843ms step_avg:37.51ms
step:850/2160 train_time:31902ms step_avg:37.53ms
step:851/2160 train_time:31963ms step_avg:37.56ms
step:852/2160 train_time:32023ms step_avg:37.59ms
step:853/2160 train_time:32084ms step_avg:37.61ms
step:854/2160 train_time:32143ms step_avg:37.64ms
step:855/2160 train_time:32204ms step_avg:37.67ms
step:856/2160 train_time:32264ms step_avg:37.69ms
step:857/2160 train_time:32325ms step_avg:37.72ms
step:858/2160 train_time:32384ms step_avg:37.74ms
step:859/2160 train_time:32445ms step_avg:37.77ms
step:860/2160 train_time:32504ms step_avg:37.80ms
step:861/2160 train_time:32565ms step_avg:37.82ms
step:862/2160 train_time:32624ms step_avg:37.85ms
step:863/2160 train_time:32684ms step_avg:37.87ms
step:864/2160 train_time:32743ms step_avg:37.90ms
step:865/2160 train_time:32804ms step_avg:37.92ms
step:866/2160 train_time:32862ms step_avg:37.95ms
step:867/2160 train_time:32923ms step_avg:37.97ms
step:868/2160 train_time:32982ms step_avg:38.00ms
step:869/2160 train_time:33043ms step_avg:38.02ms
step:870/2160 train_time:33103ms step_avg:38.05ms
step:871/2160 train_time:33163ms step_avg:38.08ms
step:872/2160 train_time:33223ms step_avg:38.10ms
step:873/2160 train_time:33284ms step_avg:38.13ms
step:874/2160 train_time:33343ms step_avg:38.15ms
step:875/2160 train_time:33404ms step_avg:38.18ms
step:876/2160 train_time:33463ms step_avg:38.20ms
step:877/2160 train_time:33525ms step_avg:38.23ms
step:878/2160 train_time:33584ms step_avg:38.25ms
step:879/2160 train_time:33645ms step_avg:38.28ms
step:880/2160 train_time:33704ms step_avg:38.30ms
step:881/2160 train_time:33765ms step_avg:38.33ms
step:882/2160 train_time:33824ms step_avg:38.35ms
step:883/2160 train_time:33884ms step_avg:38.37ms
step:884/2160 train_time:33943ms step_avg:38.40ms
step:885/2160 train_time:34004ms step_avg:38.42ms
step:886/2160 train_time:34063ms step_avg:38.45ms
step:887/2160 train_time:34124ms step_avg:38.47ms
step:888/2160 train_time:34184ms step_avg:38.50ms
step:889/2160 train_time:34245ms step_avg:38.52ms
step:890/2160 train_time:34303ms step_avg:38.54ms
step:891/2160 train_time:34365ms step_avg:38.57ms
step:892/2160 train_time:34424ms step_avg:38.59ms
step:893/2160 train_time:34485ms step_avg:38.62ms
step:894/2160 train_time:34545ms step_avg:38.64ms
step:895/2160 train_time:34606ms step_avg:38.67ms
step:896/2160 train_time:34665ms step_avg:38.69ms
step:897/2160 train_time:34726ms step_avg:38.71ms
step:898/2160 train_time:34785ms step_avg:38.74ms
step:899/2160 train_time:34846ms step_avg:38.76ms
step:900/2160 train_time:34905ms step_avg:38.78ms
step:901/2160 train_time:34966ms step_avg:38.81ms
step:902/2160 train_time:35025ms step_avg:38.83ms
step:903/2160 train_time:35086ms step_avg:38.85ms
step:904/2160 train_time:35145ms step_avg:38.88ms
step:905/2160 train_time:35205ms step_avg:38.90ms
step:906/2160 train_time:35264ms step_avg:38.92ms
step:907/2160 train_time:35325ms step_avg:38.95ms
step:908/2160 train_time:35384ms step_avg:38.97ms
step:909/2160 train_time:35446ms step_avg:38.99ms
step:910/2160 train_time:35505ms step_avg:39.02ms
step:911/2160 train_time:35566ms step_avg:39.04ms
step:912/2160 train_time:35626ms step_avg:39.06ms
step:913/2160 train_time:35686ms step_avg:39.09ms
step:914/2160 train_time:35746ms step_avg:39.11ms
step:915/2160 train_time:35808ms step_avg:39.13ms
step:916/2160 train_time:35867ms step_avg:39.16ms
step:917/2160 train_time:35928ms step_avg:39.18ms
step:918/2160 train_time:35987ms step_avg:39.20ms
step:919/2160 train_time:36049ms step_avg:39.23ms
step:920/2160 train_time:36108ms step_avg:39.25ms
step:921/2160 train_time:36169ms step_avg:39.27ms
step:922/2160 train_time:36229ms step_avg:39.29ms
step:923/2160 train_time:36288ms step_avg:39.32ms
step:924/2160 train_time:36347ms step_avg:39.34ms
step:925/2160 train_time:36408ms step_avg:39.36ms
step:926/2160 train_time:36467ms step_avg:39.38ms
step:927/2160 train_time:36528ms step_avg:39.40ms
step:928/2160 train_time:36587ms step_avg:39.43ms
step:929/2160 train_time:36648ms step_avg:39.45ms
step:930/2160 train_time:36707ms step_avg:39.47ms
step:931/2160 train_time:36768ms step_avg:39.49ms
step:932/2160 train_time:36829ms step_avg:39.52ms
step:933/2160 train_time:36888ms step_avg:39.54ms
step:934/2160 train_time:36948ms step_avg:39.56ms
step:935/2160 train_time:37008ms step_avg:39.58ms
step:936/2160 train_time:37068ms step_avg:39.60ms
step:937/2160 train_time:37129ms step_avg:39.63ms
step:938/2160 train_time:37188ms step_avg:39.65ms
step:939/2160 train_time:37249ms step_avg:39.67ms
step:940/2160 train_time:37308ms step_avg:39.69ms
step:941/2160 train_time:37369ms step_avg:39.71ms
step:942/2160 train_time:37428ms step_avg:39.73ms
step:943/2160 train_time:37488ms step_avg:39.75ms
step:944/2160 train_time:37547ms step_avg:39.77ms
step:945/2160 train_time:37608ms step_avg:39.80ms
step:946/2160 train_time:37668ms step_avg:39.82ms
step:947/2160 train_time:37728ms step_avg:39.84ms
step:948/2160 train_time:37787ms step_avg:39.86ms
step:949/2160 train_time:37848ms step_avg:39.88ms
step:950/2160 train_time:37907ms step_avg:39.90ms
step:951/2160 train_time:37967ms step_avg:39.92ms
step:952/2160 train_time:38028ms step_avg:39.94ms
step:953/2160 train_time:38088ms step_avg:39.97ms
step:954/2160 train_time:38147ms step_avg:39.99ms
step:955/2160 train_time:38208ms step_avg:40.01ms
step:956/2160 train_time:38267ms step_avg:40.03ms
step:957/2160 train_time:38328ms step_avg:40.05ms
step:958/2160 train_time:38387ms step_avg:40.07ms
step:959/2160 train_time:38447ms step_avg:40.09ms
step:960/2160 train_time:38507ms step_avg:40.11ms
step:961/2160 train_time:38568ms step_avg:40.13ms
step:962/2160 train_time:38628ms step_avg:40.15ms
step:963/2160 train_time:38688ms step_avg:40.17ms
step:964/2160 train_time:38747ms step_avg:40.19ms
step:965/2160 train_time:38809ms step_avg:40.22ms
step:966/2160 train_time:38868ms step_avg:40.24ms
step:967/2160 train_time:38929ms step_avg:40.26ms
step:968/2160 train_time:38988ms step_avg:40.28ms
step:969/2160 train_time:39049ms step_avg:40.30ms
step:970/2160 train_time:39108ms step_avg:40.32ms
step:971/2160 train_time:39169ms step_avg:40.34ms
step:972/2160 train_time:39229ms step_avg:40.36ms
step:973/2160 train_time:39289ms step_avg:40.38ms
step:974/2160 train_time:39348ms step_avg:40.40ms
step:975/2160 train_time:39409ms step_avg:40.42ms
step:976/2160 train_time:39468ms step_avg:40.44ms
step:977/2160 train_time:39529ms step_avg:40.46ms
step:978/2160 train_time:39587ms step_avg:40.48ms
step:979/2160 train_time:39648ms step_avg:40.50ms
step:980/2160 train_time:39708ms step_avg:40.52ms
step:981/2160 train_time:39768ms step_avg:40.54ms
step:982/2160 train_time:39828ms step_avg:40.56ms
step:983/2160 train_time:39888ms step_avg:40.58ms
step:984/2160 train_time:39948ms step_avg:40.60ms
step:985/2160 train_time:40009ms step_avg:40.62ms
step:986/2160 train_time:40068ms step_avg:40.64ms
step:987/2160 train_time:40129ms step_avg:40.66ms
step:988/2160 train_time:40188ms step_avg:40.68ms
step:989/2160 train_time:40249ms step_avg:40.70ms
step:990/2160 train_time:40308ms step_avg:40.72ms
step:991/2160 train_time:40370ms step_avg:40.74ms
step:992/2160 train_time:40429ms step_avg:40.76ms
step:993/2160 train_time:40489ms step_avg:40.77ms
step:994/2160 train_time:40548ms step_avg:40.79ms
step:995/2160 train_time:40609ms step_avg:40.81ms
step:996/2160 train_time:40668ms step_avg:40.83ms
step:997/2160 train_time:40729ms step_avg:40.85ms
step:998/2160 train_time:40789ms step_avg:40.87ms
step:999/2160 train_time:40849ms step_avg:40.89ms
step:1000/2160 train_time:40908ms step_avg:40.91ms
step:1000/2160 val_loss:3.7124 train_time:40971ms step_avg:40.97ms
step:1001/2160 train_time:40993ms step_avg:40.95ms
step:1002/2160 train_time:41032ms step_avg:40.95ms
step:1003/2160 train_time:41098ms step_avg:40.97ms
step:1004/2160 train_time:41159ms step_avg:40.99ms
step:1005/2160 train_time:41219ms step_avg:41.01ms
step:1006/2160 train_time:41279ms step_avg:41.03ms
step:1007/2160 train_time:41339ms step_avg:41.05ms
step:1008/2160 train_time:41398ms step_avg:41.07ms
step:1009/2160 train_time:41458ms step_avg:41.09ms
step:1010/2160 train_time:41516ms step_avg:41.11ms
step:1011/2160 train_time:41577ms step_avg:41.12ms
step:1012/2160 train_time:41635ms step_avg:41.14ms
step:1013/2160 train_time:41695ms step_avg:41.16ms
step:1014/2160 train_time:41754ms step_avg:41.18ms
step:1015/2160 train_time:41815ms step_avg:41.20ms
step:1016/2160 train_time:41874ms step_avg:41.21ms
step:1017/2160 train_time:41936ms step_avg:41.24ms
step:1018/2160 train_time:41997ms step_avg:41.25ms
step:1019/2160 train_time:42061ms step_avg:41.28ms
step:1020/2160 train_time:42122ms step_avg:41.30ms
step:1021/2160 train_time:42184ms step_avg:41.32ms
step:1022/2160 train_time:42243ms step_avg:41.33ms
step:1023/2160 train_time:42304ms step_avg:41.35ms
step:1024/2160 train_time:42363ms step_avg:41.37ms
step:1025/2160 train_time:42423ms step_avg:41.39ms
step:1026/2160 train_time:42482ms step_avg:41.41ms
step:1027/2160 train_time:42541ms step_avg:41.42ms
step:1028/2160 train_time:42600ms step_avg:41.44ms
step:1029/2160 train_time:42660ms step_avg:41.46ms
step:1030/2160 train_time:42719ms step_avg:41.47ms
step:1031/2160 train_time:42779ms step_avg:41.49ms
step:1032/2160 train_time:42838ms step_avg:41.51ms
step:1033/2160 train_time:42900ms step_avg:41.53ms
step:1034/2160 train_time:42960ms step_avg:41.55ms
step:1035/2160 train_time:43023ms step_avg:41.57ms
step:1036/2160 train_time:43083ms step_avg:41.59ms
step:1037/2160 train_time:43145ms step_avg:41.61ms
step:1038/2160 train_time:43204ms step_avg:41.62ms
step:1039/2160 train_time:43264ms step_avg:41.64ms
step:1040/2160 train_time:43324ms step_avg:41.66ms
step:1041/2160 train_time:43384ms step_avg:41.68ms
step:1042/2160 train_time:43443ms step_avg:41.69ms
step:1043/2160 train_time:43502ms step_avg:41.71ms
step:1044/2160 train_time:43561ms step_avg:41.73ms
step:1045/2160 train_time:43622ms step_avg:41.74ms
step:1046/2160 train_time:43681ms step_avg:41.76ms
step:1047/2160 train_time:43742ms step_avg:41.78ms
step:1048/2160 train_time:43801ms step_avg:41.80ms
step:1049/2160 train_time:43861ms step_avg:41.81ms
step:1050/2160 train_time:43921ms step_avg:41.83ms
step:1051/2160 train_time:43983ms step_avg:41.85ms
step:1052/2160 train_time:44043ms step_avg:41.87ms
step:1053/2160 train_time:44106ms step_avg:41.89ms
step:1054/2160 train_time:44165ms step_avg:41.90ms
step:1055/2160 train_time:44225ms step_avg:41.92ms
step:1056/2160 train_time:44284ms step_avg:41.94ms
step:1057/2160 train_time:44345ms step_avg:41.95ms
step:1058/2160 train_time:44404ms step_avg:41.97ms
step:1059/2160 train_time:44463ms step_avg:41.99ms
step:1060/2160 train_time:44522ms step_avg:42.00ms
step:1061/2160 train_time:44584ms step_avg:42.02ms
step:1062/2160 train_time:44643ms step_avg:42.04ms
step:1063/2160 train_time:44703ms step_avg:42.05ms
step:1064/2160 train_time:44762ms step_avg:42.07ms
step:1065/2160 train_time:44822ms step_avg:42.09ms
step:1066/2160 train_time:44881ms step_avg:42.10ms
step:1067/2160 train_time:44943ms step_avg:42.12ms
step:1068/2160 train_time:45003ms step_avg:42.14ms
step:1069/2160 train_time:45064ms step_avg:42.16ms
step:1070/2160 train_time:45123ms step_avg:42.17ms
step:1071/2160 train_time:45184ms step_avg:42.19ms
step:1072/2160 train_time:45244ms step_avg:42.21ms
step:1073/2160 train_time:45305ms step_avg:42.22ms
step:1074/2160 train_time:45364ms step_avg:42.24ms
step:1075/2160 train_time:45424ms step_avg:42.25ms
step:1076/2160 train_time:45483ms step_avg:42.27ms
step:1077/2160 train_time:45543ms step_avg:42.29ms
step:1078/2160 train_time:45603ms step_avg:42.30ms
step:1079/2160 train_time:45663ms step_avg:42.32ms
step:1080/2160 train_time:45723ms step_avg:42.34ms
step:1081/2160 train_time:45783ms step_avg:42.35ms
step:1082/2160 train_time:45843ms step_avg:42.37ms
step:1083/2160 train_time:45904ms step_avg:42.39ms
step:1084/2160 train_time:45964ms step_avg:42.40ms
step:1085/2160 train_time:46025ms step_avg:42.42ms
step:1086/2160 train_time:46084ms step_avg:42.43ms
step:1087/2160 train_time:46145ms step_avg:42.45ms
step:1088/2160 train_time:46205ms step_avg:42.47ms
step:1089/2160 train_time:46266ms step_avg:42.48ms
step:1090/2160 train_time:46325ms step_avg:42.50ms
step:1091/2160 train_time:46385ms step_avg:42.52ms
step:1092/2160 train_time:46444ms step_avg:42.53ms
step:1093/2160 train_time:46505ms step_avg:42.55ms
step:1094/2160 train_time:46564ms step_avg:42.56ms
step:1095/2160 train_time:46624ms step_avg:42.58ms
step:1096/2160 train_time:46683ms step_avg:42.59ms
step:1097/2160 train_time:46744ms step_avg:42.61ms
step:1098/2160 train_time:46804ms step_avg:42.63ms
step:1099/2160 train_time:46865ms step_avg:42.64ms
step:1100/2160 train_time:46925ms step_avg:42.66ms
step:1101/2160 train_time:46985ms step_avg:42.67ms
step:1102/2160 train_time:47044ms step_avg:42.69ms
step:1103/2160 train_time:47105ms step_avg:42.71ms
step:1104/2160 train_time:47165ms step_avg:42.72ms
step:1105/2160 train_time:47225ms step_avg:42.74ms
step:1106/2160 train_time:47284ms step_avg:42.75ms
step:1107/2160 train_time:47345ms step_avg:42.77ms
step:1108/2160 train_time:47403ms step_avg:42.78ms
step:1109/2160 train_time:47463ms step_avg:42.80ms
step:1110/2160 train_time:47523ms step_avg:42.81ms
step:1111/2160 train_time:47583ms step_avg:42.83ms
step:1112/2160 train_time:47642ms step_avg:42.84ms
step:1113/2160 train_time:47703ms step_avg:42.86ms
step:1114/2160 train_time:47762ms step_avg:42.87ms
step:1115/2160 train_time:47822ms step_avg:42.89ms
step:1116/2160 train_time:47882ms step_avg:42.90ms
step:1117/2160 train_time:47944ms step_avg:42.92ms
step:1118/2160 train_time:48003ms step_avg:42.94ms
step:1119/2160 train_time:48064ms step_avg:42.95ms
step:1120/2160 train_time:48124ms step_avg:42.97ms
step:1121/2160 train_time:48184ms step_avg:42.98ms
step:1122/2160 train_time:48243ms step_avg:43.00ms
step:1123/2160 train_time:48304ms step_avg:43.01ms
step:1124/2160 train_time:48364ms step_avg:43.03ms
step:1125/2160 train_time:48424ms step_avg:43.04ms
step:1126/2160 train_time:48484ms step_avg:43.06ms
step:1127/2160 train_time:48544ms step_avg:43.07ms
step:1128/2160 train_time:48603ms step_avg:43.09ms
step:1129/2160 train_time:48664ms step_avg:43.10ms
step:1130/2160 train_time:48723ms step_avg:43.12ms
step:1131/2160 train_time:48784ms step_avg:43.13ms
step:1132/2160 train_time:48843ms step_avg:43.15ms
step:1133/2160 train_time:48905ms step_avg:43.16ms
step:1134/2160 train_time:48965ms step_avg:43.18ms
step:1135/2160 train_time:49025ms step_avg:43.19ms
step:1136/2160 train_time:49085ms step_avg:43.21ms
step:1137/2160 train_time:49145ms step_avg:43.22ms
step:1138/2160 train_time:49204ms step_avg:43.24ms
step:1139/2160 train_time:49265ms step_avg:43.25ms
step:1140/2160 train_time:49324ms step_avg:43.27ms
step:1141/2160 train_time:49384ms step_avg:43.28ms
step:1142/2160 train_time:49443ms step_avg:43.30ms
step:1143/2160 train_time:49505ms step_avg:43.31ms
step:1144/2160 train_time:49565ms step_avg:43.33ms
step:1145/2160 train_time:49624ms step_avg:43.34ms
step:1146/2160 train_time:49683ms step_avg:43.35ms
step:1147/2160 train_time:49744ms step_avg:43.37ms
step:1148/2160 train_time:49803ms step_avg:43.38ms
step:1149/2160 train_time:49864ms step_avg:43.40ms
step:1150/2160 train_time:49924ms step_avg:43.41ms
step:1151/2160 train_time:49984ms step_avg:43.43ms
step:1152/2160 train_time:50044ms step_avg:43.44ms
step:1153/2160 train_time:50104ms step_avg:43.46ms
step:1154/2160 train_time:50164ms step_avg:43.47ms
step:1155/2160 train_time:50225ms step_avg:43.48ms
step:1156/2160 train_time:50284ms step_avg:43.50ms
step:1157/2160 train_time:50344ms step_avg:43.51ms
step:1158/2160 train_time:50404ms step_avg:43.53ms
step:1159/2160 train_time:50464ms step_avg:43.54ms
step:1160/2160 train_time:50524ms step_avg:43.55ms
step:1161/2160 train_time:50584ms step_avg:43.57ms
step:1162/2160 train_time:50643ms step_avg:43.58ms
step:1163/2160 train_time:50704ms step_avg:43.60ms
step:1164/2160 train_time:50764ms step_avg:43.61ms
step:1165/2160 train_time:50824ms step_avg:43.63ms
step:1166/2160 train_time:50883ms step_avg:43.64ms
step:1167/2160 train_time:50945ms step_avg:43.65ms
step:1168/2160 train_time:51004ms step_avg:43.67ms
step:1169/2160 train_time:51064ms step_avg:43.68ms
step:1170/2160 train_time:51124ms step_avg:43.70ms
step:1171/2160 train_time:51185ms step_avg:43.71ms
step:1172/2160 train_time:51244ms step_avg:43.72ms
step:1173/2160 train_time:51305ms step_avg:43.74ms
step:1174/2160 train_time:51365ms step_avg:43.75ms
step:1175/2160 train_time:51425ms step_avg:43.77ms
step:1176/2160 train_time:51485ms step_avg:43.78ms
step:1177/2160 train_time:51545ms step_avg:43.79ms
step:1178/2160 train_time:51604ms step_avg:43.81ms
step:1179/2160 train_time:51665ms step_avg:43.82ms
step:1180/2160 train_time:51724ms step_avg:43.83ms
step:1181/2160 train_time:51785ms step_avg:43.85ms
step:1182/2160 train_time:51844ms step_avg:43.86ms
step:1183/2160 train_time:51905ms step_avg:43.88ms
step:1184/2160 train_time:51965ms step_avg:43.89ms
step:1185/2160 train_time:52025ms step_avg:43.90ms
step:1186/2160 train_time:52084ms step_avg:43.92ms
step:1187/2160 train_time:52146ms step_avg:43.93ms
step:1188/2160 train_time:52205ms step_avg:43.94ms
step:1189/2160 train_time:52265ms step_avg:43.96ms
step:1190/2160 train_time:52325ms step_avg:43.97ms
step:1191/2160 train_time:52385ms step_avg:43.98ms
step:1192/2160 train_time:52444ms step_avg:44.00ms
step:1193/2160 train_time:52504ms step_avg:44.01ms
step:1194/2160 train_time:52565ms step_avg:44.02ms
step:1195/2160 train_time:52625ms step_avg:44.04ms
step:1196/2160 train_time:52684ms step_avg:44.05ms
step:1197/2160 train_time:52745ms step_avg:44.06ms
step:1198/2160 train_time:52804ms step_avg:44.08ms
step:1199/2160 train_time:52865ms step_avg:44.09ms
step:1200/2160 train_time:52924ms step_avg:44.10ms
step:1201/2160 train_time:52984ms step_avg:44.12ms
step:1202/2160 train_time:53044ms step_avg:44.13ms
step:1203/2160 train_time:53105ms step_avg:44.14ms
step:1204/2160 train_time:53164ms step_avg:44.16ms
step:1205/2160 train_time:53224ms step_avg:44.17ms
step:1206/2160 train_time:53284ms step_avg:44.18ms
step:1207/2160 train_time:53344ms step_avg:44.20ms
step:1208/2160 train_time:53403ms step_avg:44.21ms
step:1209/2160 train_time:53463ms step_avg:44.22ms
step:1210/2160 train_time:53523ms step_avg:44.23ms
step:1211/2160 train_time:53583ms step_avg:44.25ms
step:1212/2160 train_time:53643ms step_avg:44.26ms
step:1213/2160 train_time:53704ms step_avg:44.27ms
step:1214/2160 train_time:53763ms step_avg:44.29ms
step:1215/2160 train_time:53824ms step_avg:44.30ms
step:1216/2160 train_time:53883ms step_avg:44.31ms
step:1217/2160 train_time:53944ms step_avg:44.33ms
step:1218/2160 train_time:54003ms step_avg:44.34ms
step:1219/2160 train_time:54064ms step_avg:44.35ms
step:1220/2160 train_time:54124ms step_avg:44.36ms
step:1221/2160 train_time:54185ms step_avg:44.38ms
step:1222/2160 train_time:54244ms step_avg:44.39ms
step:1223/2160 train_time:54304ms step_avg:44.40ms
step:1224/2160 train_time:54364ms step_avg:44.42ms
step:1225/2160 train_time:54424ms step_avg:44.43ms
step:1226/2160 train_time:54484ms step_avg:44.44ms
step:1227/2160 train_time:54544ms step_avg:44.45ms
step:1228/2160 train_time:54604ms step_avg:44.47ms
step:1229/2160 train_time:54664ms step_avg:44.48ms
step:1230/2160 train_time:54723ms step_avg:44.49ms
step:1231/2160 train_time:54784ms step_avg:44.50ms
step:1232/2160 train_time:54843ms step_avg:44.52ms
step:1233/2160 train_time:54904ms step_avg:44.53ms
step:1234/2160 train_time:54964ms step_avg:44.54ms
step:1235/2160 train_time:55025ms step_avg:44.55ms
step:1236/2160 train_time:55084ms step_avg:44.57ms
step:1237/2160 train_time:55145ms step_avg:44.58ms
step:1238/2160 train_time:55204ms step_avg:44.59ms
step:1239/2160 train_time:55266ms step_avg:44.61ms
step:1240/2160 train_time:55325ms step_avg:44.62ms
step:1241/2160 train_time:55385ms step_avg:44.63ms
step:1242/2160 train_time:55444ms step_avg:44.64ms
step:1243/2160 train_time:55505ms step_avg:44.65ms
step:1244/2160 train_time:55565ms step_avg:44.67ms
step:1245/2160 train_time:55624ms step_avg:44.68ms
step:1246/2160 train_time:55683ms step_avg:44.69ms
step:1247/2160 train_time:55744ms step_avg:44.70ms
step:1248/2160 train_time:55804ms step_avg:44.71ms
step:1249/2160 train_time:55865ms step_avg:44.73ms
step:1250/2160 train_time:55924ms step_avg:44.74ms
step:1250/2160 val_loss:3.5996 train_time:55987ms step_avg:44.79ms
step:1251/2160 train_time:56011ms step_avg:44.77ms
step:1252/2160 train_time:56046ms step_avg:44.77ms
step:1253/2160 train_time:56111ms step_avg:44.78ms
step:1254/2160 train_time:56172ms step_avg:44.79ms
step:1255/2160 train_time:56233ms step_avg:44.81ms
step:1256/2160 train_time:56293ms step_avg:44.82ms
step:1257/2160 train_time:56353ms step_avg:44.83ms
step:1258/2160 train_time:56412ms step_avg:44.84ms
step:1259/2160 train_time:56472ms step_avg:44.85ms
step:1260/2160 train_time:56530ms step_avg:44.87ms
step:1261/2160 train_time:56590ms step_avg:44.88ms
step:1262/2160 train_time:56649ms step_avg:44.89ms
step:1263/2160 train_time:56708ms step_avg:44.90ms
step:1264/2160 train_time:56767ms step_avg:44.91ms
step:1265/2160 train_time:56826ms step_avg:44.92ms
step:1266/2160 train_time:56885ms step_avg:44.93ms
step:1267/2160 train_time:56949ms step_avg:44.95ms
step:1268/2160 train_time:57010ms step_avg:44.96ms
step:1269/2160 train_time:57073ms step_avg:44.97ms
step:1270/2160 train_time:57134ms step_avg:44.99ms
step:1271/2160 train_time:57197ms step_avg:45.00ms
step:1272/2160 train_time:57256ms step_avg:45.01ms
step:1273/2160 train_time:57317ms step_avg:45.03ms
step:1274/2160 train_time:57376ms step_avg:45.04ms
step:1275/2160 train_time:57437ms step_avg:45.05ms
step:1276/2160 train_time:57496ms step_avg:45.06ms
step:1277/2160 train_time:57557ms step_avg:45.07ms
step:1278/2160 train_time:57616ms step_avg:45.08ms
step:1279/2160 train_time:57676ms step_avg:45.09ms
step:1280/2160 train_time:57735ms step_avg:45.11ms
step:1281/2160 train_time:57796ms step_avg:45.12ms
step:1282/2160 train_time:57856ms step_avg:45.13ms
step:1283/2160 train_time:57917ms step_avg:45.14ms
step:1284/2160 train_time:57978ms step_avg:45.15ms
step:1285/2160 train_time:58040ms step_avg:45.17ms
step:1286/2160 train_time:58100ms step_avg:45.18ms
step:1287/2160 train_time:58161ms step_avg:45.19ms
step:1288/2160 train_time:58221ms step_avg:45.20ms
step:1289/2160 train_time:58282ms step_avg:45.21ms
step:1290/2160 train_time:58340ms step_avg:45.23ms
step:1291/2160 train_time:58402ms step_avg:45.24ms
step:1292/2160 train_time:58460ms step_avg:45.25ms
step:1293/2160 train_time:58520ms step_avg:45.26ms
step:1294/2160 train_time:58579ms step_avg:45.27ms
step:1295/2160 train_time:58639ms step_avg:45.28ms
step:1296/2160 train_time:58699ms step_avg:45.29ms
step:1297/2160 train_time:58759ms step_avg:45.30ms
step:1298/2160 train_time:58818ms step_avg:45.31ms
step:1299/2160 train_time:58879ms step_avg:45.33ms
step:1300/2160 train_time:58939ms step_avg:45.34ms
step:1301/2160 train_time:59000ms step_avg:45.35ms
step:1302/2160 train_time:59060ms step_avg:45.36ms
step:1303/2160 train_time:59120ms step_avg:45.37ms
step:1304/2160 train_time:59180ms step_avg:45.38ms
step:1305/2160 train_time:59241ms step_avg:45.40ms
step:1306/2160 train_time:59301ms step_avg:45.41ms
step:1307/2160 train_time:59361ms step_avg:45.42ms
step:1308/2160 train_time:59420ms step_avg:45.43ms
step:1309/2160 train_time:59480ms step_avg:45.44ms
step:1310/2160 train_time:59539ms step_avg:45.45ms
step:1311/2160 train_time:59600ms step_avg:45.46ms
step:1312/2160 train_time:59659ms step_avg:45.47ms
step:1313/2160 train_time:59719ms step_avg:45.48ms
step:1314/2160 train_time:59779ms step_avg:45.49ms
step:1315/2160 train_time:59839ms step_avg:45.51ms
step:1316/2160 train_time:59899ms step_avg:45.52ms
step:1317/2160 train_time:59960ms step_avg:45.53ms
step:1318/2160 train_time:60020ms step_avg:45.54ms
step:1319/2160 train_time:60080ms step_avg:45.55ms
step:1320/2160 train_time:60140ms step_avg:45.56ms
step:1321/2160 train_time:60202ms step_avg:45.57ms
step:1322/2160 train_time:60261ms step_avg:45.58ms
step:1323/2160 train_time:60322ms step_avg:45.59ms
step:1324/2160 train_time:60380ms step_avg:45.60ms
step:1325/2160 train_time:60441ms step_avg:45.62ms
step:1326/2160 train_time:60500ms step_avg:45.63ms
step:1327/2160 train_time:60559ms step_avg:45.64ms
step:1328/2160 train_time:60618ms step_avg:45.65ms
step:1329/2160 train_time:60679ms step_avg:45.66ms
step:1330/2160 train_time:60738ms step_avg:45.67ms
step:1331/2160 train_time:60799ms step_avg:45.68ms
step:1332/2160 train_time:60858ms step_avg:45.69ms
step:1333/2160 train_time:60919ms step_avg:45.70ms
step:1334/2160 train_time:60979ms step_avg:45.71ms
step:1335/2160 train_time:61040ms step_avg:45.72ms
step:1336/2160 train_time:61100ms step_avg:45.73ms
step:1337/2160 train_time:61161ms step_avg:45.74ms
step:1338/2160 train_time:61220ms step_avg:45.75ms
step:1339/2160 train_time:61281ms step_avg:45.77ms
step:1340/2160 train_time:61340ms step_avg:45.78ms
step:1341/2160 train_time:61401ms step_avg:45.79ms
step:1342/2160 train_time:61460ms step_avg:45.80ms
step:1343/2160 train_time:61520ms step_avg:45.81ms
step:1344/2160 train_time:61579ms step_avg:45.82ms
step:1345/2160 train_time:61639ms step_avg:45.83ms
step:1346/2160 train_time:61698ms step_avg:45.84ms
step:1347/2160 train_time:61759ms step_avg:45.85ms
step:1348/2160 train_time:61818ms step_avg:45.86ms
step:1349/2160 train_time:61879ms step_avg:45.87ms
step:1350/2160 train_time:61939ms step_avg:45.88ms
step:1351/2160 train_time:62000ms step_avg:45.89ms
step:1352/2160 train_time:62060ms step_avg:45.90ms
step:1353/2160 train_time:62121ms step_avg:45.91ms
step:1354/2160 train_time:62180ms step_avg:45.92ms
step:1355/2160 train_time:62242ms step_avg:45.93ms
step:1356/2160 train_time:62301ms step_avg:45.94ms
step:1357/2160 train_time:62361ms step_avg:45.96ms
step:1358/2160 train_time:62420ms step_avg:45.96ms
step:1359/2160 train_time:62481ms step_avg:45.98ms
step:1360/2160 train_time:62539ms step_avg:45.98ms
step:1361/2160 train_time:62600ms step_avg:46.00ms
step:1362/2160 train_time:62658ms step_avg:46.00ms
step:1363/2160 train_time:62719ms step_avg:46.02ms
step:1364/2160 train_time:62779ms step_avg:46.03ms
step:1365/2160 train_time:62840ms step_avg:46.04ms
step:1366/2160 train_time:62900ms step_avg:46.05ms
step:1367/2160 train_time:62960ms step_avg:46.06ms
step:1368/2160 train_time:63019ms step_avg:46.07ms
step:1369/2160 train_time:63081ms step_avg:46.08ms
step:1370/2160 train_time:63140ms step_avg:46.09ms
step:1371/2160 train_time:63201ms step_avg:46.10ms
step:1372/2160 train_time:63261ms step_avg:46.11ms
step:1373/2160 train_time:63321ms step_avg:46.12ms
step:1374/2160 train_time:63380ms step_avg:46.13ms
step:1375/2160 train_time:63440ms step_avg:46.14ms
step:1376/2160 train_time:63499ms step_avg:46.15ms
step:1377/2160 train_time:63559ms step_avg:46.16ms
step:1378/2160 train_time:63618ms step_avg:46.17ms
step:1379/2160 train_time:63679ms step_avg:46.18ms
step:1380/2160 train_time:63738ms step_avg:46.19ms
step:1381/2160 train_time:63798ms step_avg:46.20ms
step:1382/2160 train_time:63859ms step_avg:46.21ms
step:1383/2160 train_time:63919ms step_avg:46.22ms
step:1384/2160 train_time:63980ms step_avg:46.23ms
step:1385/2160 train_time:64041ms step_avg:46.24ms
step:1386/2160 train_time:64101ms step_avg:46.25ms
step:1387/2160 train_time:64161ms step_avg:46.26ms
step:1388/2160 train_time:64220ms step_avg:46.27ms
step:1389/2160 train_time:64281ms step_avg:46.28ms
step:1390/2160 train_time:64340ms step_avg:46.29ms
step:1391/2160 train_time:64400ms step_avg:46.30ms
step:1392/2160 train_time:64459ms step_avg:46.31ms
step:1393/2160 train_time:64518ms step_avg:46.32ms
step:1394/2160 train_time:64578ms step_avg:46.33ms
step:1395/2160 train_time:64640ms step_avg:46.34ms
step:1396/2160 train_time:64699ms step_avg:46.35ms
step:1397/2160 train_time:64759ms step_avg:46.36ms
step:1398/2160 train_time:64819ms step_avg:46.37ms
step:1399/2160 train_time:64880ms step_avg:46.38ms
step:1400/2160 train_time:64939ms step_avg:46.39ms
step:1401/2160 train_time:65001ms step_avg:46.40ms
step:1402/2160 train_time:65060ms step_avg:46.41ms
step:1403/2160 train_time:65121ms step_avg:46.42ms
step:1404/2160 train_time:65180ms step_avg:46.42ms
step:1405/2160 train_time:65242ms step_avg:46.44ms
step:1406/2160 train_time:65301ms step_avg:46.44ms
step:1407/2160 train_time:65361ms step_avg:46.45ms
step:1408/2160 train_time:65420ms step_avg:46.46ms
step:1409/2160 train_time:65479ms step_avg:46.47ms
step:1410/2160 train_time:65538ms step_avg:46.48ms
step:1411/2160 train_time:65599ms step_avg:46.49ms
step:1412/2160 train_time:65659ms step_avg:46.50ms
step:1413/2160 train_time:65719ms step_avg:46.51ms
step:1414/2160 train_time:65778ms step_avg:46.52ms
step:1415/2160 train_time:65839ms step_avg:46.53ms
step:1416/2160 train_time:65928ms step_avg:46.56ms
step:1417/2160 train_time:66017ms step_avg:46.59ms
step:1418/2160 train_time:66103ms step_avg:46.62ms
step:1419/2160 train_time:66193ms step_avg:46.65ms
step:1420/2160 train_time:66281ms step_avg:46.68ms
step:1421/2160 train_time:66370ms step_avg:46.71ms
step:1422/2160 train_time:66455ms step_avg:46.73ms
step:1423/2160 train_time:66543ms step_avg:46.76ms
step:1424/2160 train_time:66631ms step_avg:46.79ms
step:1425/2160 train_time:66719ms step_avg:46.82ms
step:1426/2160 train_time:66806ms step_avg:46.85ms
step:1427/2160 train_time:66895ms step_avg:46.88ms
step:1428/2160 train_time:66982ms step_avg:46.91ms
step:1429/2160 train_time:67070ms step_avg:46.94ms
step:1430/2160 train_time:67156ms step_avg:46.96ms
step:1431/2160 train_time:67246ms step_avg:46.99ms
step:1432/2160 train_time:67333ms step_avg:47.02ms
step:1433/2160 train_time:67421ms step_avg:47.05ms
step:1434/2160 train_time:67509ms step_avg:47.08ms
step:1435/2160 train_time:67596ms step_avg:47.11ms
step:1436/2160 train_time:67683ms step_avg:47.13ms
step:1437/2160 train_time:67773ms step_avg:47.16ms
step:1438/2160 train_time:67859ms step_avg:47.19ms
step:1439/2160 train_time:67948ms step_avg:47.22ms
step:1440/2160 train_time:68036ms step_avg:47.25ms
step:1441/2160 train_time:68123ms step_avg:47.27ms
step:1442/2160 train_time:68211ms step_avg:47.30ms
step:1443/2160 train_time:68299ms step_avg:47.33ms
step:1444/2160 train_time:68386ms step_avg:47.36ms
step:1445/2160 train_time:68475ms step_avg:47.39ms
step:1446/2160 train_time:68561ms step_avg:47.41ms
step:1447/2160 train_time:68650ms step_avg:47.44ms
step:1448/2160 train_time:68736ms step_avg:47.47ms
step:1449/2160 train_time:68824ms step_avg:47.50ms
step:1450/2160 train_time:68912ms step_avg:47.53ms
step:1451/2160 train_time:69000ms step_avg:47.55ms
step:1452/2160 train_time:69086ms step_avg:47.58ms
step:1453/2160 train_time:69175ms step_avg:47.61ms
step:1454/2160 train_time:69262ms step_avg:47.64ms
step:1455/2160 train_time:69351ms step_avg:47.66ms
step:1456/2160 train_time:69437ms step_avg:47.69ms
step:1457/2160 train_time:69526ms step_avg:47.72ms
step:1458/2160 train_time:69613ms step_avg:47.75ms
step:1459/2160 train_time:69701ms step_avg:47.77ms
step:1460/2160 train_time:69788ms step_avg:47.80ms
step:1461/2160 train_time:69876ms step_avg:47.83ms
step:1462/2160 train_time:69962ms step_avg:47.85ms
step:1463/2160 train_time:70052ms step_avg:47.88ms
step:1464/2160 train_time:70139ms step_avg:47.91ms
step:1465/2160 train_time:70227ms step_avg:47.94ms
step:1466/2160 train_time:70313ms step_avg:47.96ms
step:1467/2160 train_time:70401ms step_avg:47.99ms
step:1468/2160 train_time:70488ms step_avg:48.02ms
step:1469/2160 train_time:70577ms step_avg:48.04ms
step:1470/2160 train_time:70664ms step_avg:48.07ms
step:1471/2160 train_time:70753ms step_avg:48.10ms
step:1472/2160 train_time:70841ms step_avg:48.13ms
step:1473/2160 train_time:70930ms step_avg:48.15ms
step:1474/2160 train_time:71016ms step_avg:48.18ms
step:1475/2160 train_time:71106ms step_avg:48.21ms
step:1476/2160 train_time:71194ms step_avg:48.23ms
step:1477/2160 train_time:71283ms step_avg:48.26ms
step:1478/2160 train_time:71370ms step_avg:48.29ms
step:1479/2160 train_time:71458ms step_avg:48.32ms
step:1480/2160 train_time:71545ms step_avg:48.34ms
step:1481/2160 train_time:71634ms step_avg:48.37ms
step:1482/2160 train_time:71721ms step_avg:48.39ms
step:1483/2160 train_time:71811ms step_avg:48.42ms
step:1484/2160 train_time:71897ms step_avg:48.45ms
step:1485/2160 train_time:71985ms step_avg:48.47ms
step:1486/2160 train_time:72072ms step_avg:48.50ms
step:1487/2160 train_time:72160ms step_avg:48.53ms
step:1488/2160 train_time:72248ms step_avg:48.55ms
step:1489/2160 train_time:72336ms step_avg:48.58ms
step:1490/2160 train_time:72423ms step_avg:48.61ms
step:1491/2160 train_time:72512ms step_avg:48.63ms
step:1492/2160 train_time:72598ms step_avg:48.66ms
step:1493/2160 train_time:72687ms step_avg:48.69ms
step:1494/2160 train_time:72774ms step_avg:48.71ms
step:1495/2160 train_time:72862ms step_avg:48.74ms
step:1496/2160 train_time:72949ms step_avg:48.76ms
step:1497/2160 train_time:73037ms step_avg:48.79ms
step:1498/2160 train_time:73124ms step_avg:48.81ms
step:1499/2160 train_time:73214ms step_avg:48.84ms
step:1500/2160 train_time:73301ms step_avg:48.87ms
step:1500/2160 val_loss:3.4919 train_time:73391ms step_avg:48.93ms
step:1501/2160 train_time:73414ms step_avg:48.91ms
step:1502/2160 train_time:73480ms step_avg:48.92ms
step:1503/2160 train_time:73570ms step_avg:48.95ms
step:1504/2160 train_time:73657ms step_avg:48.97ms
step:1505/2160 train_time:73745ms step_avg:49.00ms
step:1506/2160 train_time:73831ms step_avg:49.02ms
step:1507/2160 train_time:73918ms step_avg:49.05ms
step:1508/2160 train_time:74005ms step_avg:49.07ms
step:1509/2160 train_time:74093ms step_avg:49.10ms
step:1510/2160 train_time:74180ms step_avg:49.13ms
step:1511/2160 train_time:74269ms step_avg:49.15ms
step:1512/2160 train_time:74358ms step_avg:49.18ms
step:1513/2160 train_time:74447ms step_avg:49.20ms
step:1514/2160 train_time:74536ms step_avg:49.23ms
step:1515/2160 train_time:74625ms step_avg:49.26ms
step:1516/2160 train_time:74712ms step_avg:49.28ms
step:1517/2160 train_time:74800ms step_avg:49.31ms
step:1518/2160 train_time:74886ms step_avg:49.33ms
step:1519/2160 train_time:74975ms step_avg:49.36ms
step:1520/2160 train_time:75061ms step_avg:49.38ms
step:1521/2160 train_time:75149ms step_avg:49.41ms
step:1522/2160 train_time:75237ms step_avg:49.43ms
step:1523/2160 train_time:75325ms step_avg:49.46ms
step:1524/2160 train_time:75413ms step_avg:49.48ms
step:1525/2160 train_time:75503ms step_avg:49.51ms
step:1526/2160 train_time:75590ms step_avg:49.53ms
step:1527/2160 train_time:75679ms step_avg:49.56ms
step:1528/2160 train_time:75765ms step_avg:49.58ms
step:1529/2160 train_time:75853ms step_avg:49.61ms
step:1530/2160 train_time:75940ms step_avg:49.63ms
step:1531/2160 train_time:76028ms step_avg:49.66ms
step:1532/2160 train_time:76114ms step_avg:49.68ms
step:1533/2160 train_time:76203ms step_avg:49.71ms
step:1534/2160 train_time:76290ms step_avg:49.73ms
step:1535/2160 train_time:76380ms step_avg:49.76ms
step:1536/2160 train_time:76468ms step_avg:49.78ms
step:1537/2160 train_time:76557ms step_avg:49.81ms
step:1538/2160 train_time:76644ms step_avg:49.83ms
step:1539/2160 train_time:76731ms step_avg:49.86ms
step:1540/2160 train_time:76819ms step_avg:49.88ms
step:1541/2160 train_time:76907ms step_avg:49.91ms
step:1542/2160 train_time:76993ms step_avg:49.93ms
step:1543/2160 train_time:77081ms step_avg:49.96ms
step:1544/2160 train_time:77168ms step_avg:49.98ms
step:1545/2160 train_time:77257ms step_avg:50.00ms
step:1546/2160 train_time:77344ms step_avg:50.03ms
step:1547/2160 train_time:77433ms step_avg:50.05ms
step:1548/2160 train_time:77520ms step_avg:50.08ms
step:1549/2160 train_time:77609ms step_avg:50.10ms
step:1550/2160 train_time:77696ms step_avg:50.13ms
step:1551/2160 train_time:77784ms step_avg:50.15ms
step:1552/2160 train_time:77871ms step_avg:50.17ms
step:1553/2160 train_time:77960ms step_avg:50.20ms
step:1554/2160 train_time:78047ms step_avg:50.22ms
step:1555/2160 train_time:78136ms step_avg:50.25ms
step:1556/2160 train_time:78222ms step_avg:50.27ms
step:1557/2160 train_time:78312ms step_avg:50.30ms
step:1558/2160 train_time:78400ms step_avg:50.32ms
step:1559/2160 train_time:78490ms step_avg:50.35ms
step:1560/2160 train_time:78577ms step_avg:50.37ms
step:1561/2160 train_time:78665ms step_avg:50.39ms
step:1562/2160 train_time:78752ms step_avg:50.42ms
step:1563/2160 train_time:78841ms step_avg:50.44ms
step:1564/2160 train_time:78927ms step_avg:50.46ms
step:1565/2160 train_time:79016ms step_avg:50.49ms
step:1566/2160 train_time:79102ms step_avg:50.51ms
step:1567/2160 train_time:79190ms step_avg:50.54ms
step:1568/2160 train_time:79277ms step_avg:50.56ms
step:1569/2160 train_time:79365ms step_avg:50.58ms
step:1570/2160 train_time:79453ms step_avg:50.61ms
step:1571/2160 train_time:79542ms step_avg:50.63ms
step:1572/2160 train_time:79629ms step_avg:50.65ms
step:1573/2160 train_time:79718ms step_avg:50.68ms
step:1574/2160 train_time:79804ms step_avg:50.70ms
step:1575/2160 train_time:79893ms step_avg:50.73ms
step:1576/2160 train_time:79979ms step_avg:50.75ms
step:1577/2160 train_time:80068ms step_avg:50.77ms
step:1578/2160 train_time:80155ms step_avg:50.80ms
step:1579/2160 train_time:80243ms step_avg:50.82ms
step:1580/2160 train_time:80329ms step_avg:50.84ms
step:1581/2160 train_time:80418ms step_avg:50.87ms
step:1582/2160 train_time:80506ms step_avg:50.89ms
step:1583/2160 train_time:80595ms step_avg:50.91ms
step:1584/2160 train_time:80682ms step_avg:50.94ms
step:1585/2160 train_time:80771ms step_avg:50.96ms
step:1586/2160 train_time:80858ms step_avg:50.98ms
step:1587/2160 train_time:80947ms step_avg:51.01ms
step:1588/2160 train_time:81035ms step_avg:51.03ms
step:1589/2160 train_time:81123ms step_avg:51.05ms
step:1590/2160 train_time:81210ms step_avg:51.08ms
step:1591/2160 train_time:81300ms step_avg:51.10ms
step:1592/2160 train_time:81387ms step_avg:51.12ms
step:1593/2160 train_time:81476ms step_avg:51.15ms
step:1594/2160 train_time:81563ms step_avg:51.17ms
step:1595/2160 train_time:81651ms step_avg:51.19ms
step:1596/2160 train_time:81739ms step_avg:51.22ms
step:1597/2160 train_time:81829ms step_avg:51.24ms
step:1598/2160 train_time:81916ms step_avg:51.26ms
step:1599/2160 train_time:82004ms step_avg:51.28ms
step:1600/2160 train_time:82091ms step_avg:51.31ms
step:1601/2160 train_time:82180ms step_avg:51.33ms
step:1602/2160 train_time:82267ms step_avg:51.35ms
step:1603/2160 train_time:82355ms step_avg:51.38ms
step:1604/2160 train_time:82442ms step_avg:51.40ms
step:1605/2160 train_time:82530ms step_avg:51.42ms
step:1606/2160 train_time:82618ms step_avg:51.44ms
step:1607/2160 train_time:82707ms step_avg:51.47ms
step:1608/2160 train_time:82793ms step_avg:51.49ms
step:1609/2160 train_time:82882ms step_avg:51.51ms
step:1610/2160 train_time:82969ms step_avg:51.53ms
step:1611/2160 train_time:83057ms step_avg:51.56ms
step:1612/2160 train_time:83144ms step_avg:51.58ms
step:1613/2160 train_time:83233ms step_avg:51.60ms
step:1614/2160 train_time:83319ms step_avg:51.62ms
step:1615/2160 train_time:83407ms step_avg:51.65ms
step:1616/2160 train_time:83494ms step_avg:51.67ms
step:1617/2160 train_time:83583ms step_avg:51.69ms
step:1618/2160 train_time:83670ms step_avg:51.71ms
step:1619/2160 train_time:83758ms step_avg:51.73ms
step:1620/2160 train_time:83845ms step_avg:51.76ms
step:1621/2160 train_time:83934ms step_avg:51.78ms
step:1622/2160 train_time:84020ms step_avg:51.80ms
step:1623/2160 train_time:84107ms step_avg:51.82ms
step:1624/2160 train_time:84195ms step_avg:51.84ms
step:1625/2160 train_time:84283ms step_avg:51.87ms
step:1626/2160 train_time:84370ms step_avg:51.89ms
step:1627/2160 train_time:84459ms step_avg:51.91ms
step:1628/2160 train_time:84545ms step_avg:51.93ms
step:1629/2160 train_time:84635ms step_avg:51.95ms
step:1630/2160 train_time:84721ms step_avg:51.98ms
step:1631/2160 train_time:84810ms step_avg:52.00ms
step:1632/2160 train_time:84897ms step_avg:52.02ms
step:1633/2160 train_time:84985ms step_avg:52.04ms
step:1634/2160 train_time:85072ms step_avg:52.06ms
step:1635/2160 train_time:85161ms step_avg:52.09ms
step:1636/2160 train_time:85248ms step_avg:52.11ms
step:1637/2160 train_time:85337ms step_avg:52.13ms
step:1638/2160 train_time:85423ms step_avg:52.15ms
step:1639/2160 train_time:85512ms step_avg:52.17ms
step:1640/2160 train_time:85598ms step_avg:52.19ms
step:1641/2160 train_time:85687ms step_avg:52.22ms
step:1642/2160 train_time:85774ms step_avg:52.24ms
step:1643/2160 train_time:85861ms step_avg:52.26ms
step:1644/2160 train_time:85948ms step_avg:52.28ms
step:1645/2160 train_time:86038ms step_avg:52.30ms
step:1646/2160 train_time:86124ms step_avg:52.32ms
step:1647/2160 train_time:86213ms step_avg:52.35ms
step:1648/2160 train_time:86299ms step_avg:52.37ms
step:1649/2160 train_time:86387ms step_avg:52.39ms
step:1650/2160 train_time:86474ms step_avg:52.41ms
step:1651/2160 train_time:86563ms step_avg:52.43ms
step:1652/2160 train_time:86651ms step_avg:52.45ms
step:1653/2160 train_time:86741ms step_avg:52.48ms
step:1654/2160 train_time:86828ms step_avg:52.50ms
step:1655/2160 train_time:86916ms step_avg:52.52ms
step:1656/2160 train_time:87003ms step_avg:52.54ms
step:1657/2160 train_time:87091ms step_avg:52.56ms
step:1658/2160 train_time:87178ms step_avg:52.58ms
step:1659/2160 train_time:87266ms step_avg:52.60ms
step:1660/2160 train_time:87353ms step_avg:52.62ms
step:1661/2160 train_time:87442ms step_avg:52.64ms
step:1662/2160 train_time:87529ms step_avg:52.66ms
step:1663/2160 train_time:87618ms step_avg:52.69ms
step:1664/2160 train_time:87705ms step_avg:52.71ms
step:1665/2160 train_time:87793ms step_avg:52.73ms
step:1666/2160 train_time:87880ms step_avg:52.75ms
step:1667/2160 train_time:87968ms step_avg:52.77ms
step:1668/2160 train_time:88056ms step_avg:52.79ms
step:1669/2160 train_time:88143ms step_avg:52.81ms
step:1670/2160 train_time:88230ms step_avg:52.83ms
step:1671/2160 train_time:88319ms step_avg:52.85ms
step:1672/2160 train_time:88405ms step_avg:52.87ms
step:1673/2160 train_time:88494ms step_avg:52.90ms
step:1674/2160 train_time:88581ms step_avg:52.92ms
step:1675/2160 train_time:88669ms step_avg:52.94ms
step:1676/2160 train_time:88756ms step_avg:52.96ms
step:1677/2160 train_time:88844ms step_avg:52.98ms
step:1678/2160 train_time:88931ms step_avg:53.00ms
step:1679/2160 train_time:89020ms step_avg:53.02ms
step:1680/2160 train_time:89106ms step_avg:53.04ms
step:1681/2160 train_time:89195ms step_avg:53.06ms
step:1682/2160 train_time:89282ms step_avg:53.08ms
step:1683/2160 train_time:89371ms step_avg:53.10ms
step:1684/2160 train_time:89458ms step_avg:53.12ms
step:1685/2160 train_time:89547ms step_avg:53.14ms
step:1686/2160 train_time:89634ms step_avg:53.16ms
step:1687/2160 train_time:89722ms step_avg:53.18ms
step:1688/2160 train_time:89808ms step_avg:53.20ms
step:1689/2160 train_time:89897ms step_avg:53.23ms
step:1690/2160 train_time:89983ms step_avg:53.24ms
step:1691/2160 train_time:90071ms step_avg:53.27ms
step:1692/2160 train_time:90158ms step_avg:53.29ms
step:1693/2160 train_time:90246ms step_avg:53.31ms
step:1694/2160 train_time:90333ms step_avg:53.33ms
step:1695/2160 train_time:90422ms step_avg:53.35ms
step:1696/2160 train_time:90509ms step_avg:53.37ms
step:1697/2160 train_time:90599ms step_avg:53.39ms
step:1698/2160 train_time:90685ms step_avg:53.41ms
step:1699/2160 train_time:90773ms step_avg:53.43ms
step:1700/2160 train_time:90860ms step_avg:53.45ms
step:1701/2160 train_time:90949ms step_avg:53.47ms
step:1702/2160 train_time:91037ms step_avg:53.49ms
step:1703/2160 train_time:91125ms step_avg:53.51ms
step:1704/2160 train_time:91211ms step_avg:53.53ms
step:1705/2160 train_time:91300ms step_avg:53.55ms
step:1706/2160 train_time:91387ms step_avg:53.57ms
step:1707/2160 train_time:91477ms step_avg:53.59ms
step:1708/2160 train_time:91562ms step_avg:53.61ms
step:1709/2160 train_time:91651ms step_avg:53.63ms
step:1710/2160 train_time:91738ms step_avg:53.65ms
step:1711/2160 train_time:91826ms step_avg:53.67ms
step:1712/2160 train_time:91913ms step_avg:53.69ms
step:1713/2160 train_time:92002ms step_avg:53.71ms
step:1714/2160 train_time:92089ms step_avg:53.73ms
step:1715/2160 train_time:92177ms step_avg:53.75ms
step:1716/2160 train_time:92264ms step_avg:53.77ms
step:1717/2160 train_time:92352ms step_avg:53.79ms
step:1718/2160 train_time:92439ms step_avg:53.81ms
step:1719/2160 train_time:92527ms step_avg:53.83ms
step:1720/2160 train_time:92613ms step_avg:53.84ms
step:1721/2160 train_time:92702ms step_avg:53.87ms
step:1722/2160 train_time:92789ms step_avg:53.88ms
step:1723/2160 train_time:92879ms step_avg:53.91ms
step:1724/2160 train_time:92965ms step_avg:53.92ms
step:1725/2160 train_time:93053ms step_avg:53.94ms
step:1726/2160 train_time:93140ms step_avg:53.96ms
step:1727/2160 train_time:93229ms step_avg:53.98ms
step:1728/2160 train_time:93315ms step_avg:54.00ms
step:1729/2160 train_time:93404ms step_avg:54.02ms
step:1730/2160 train_time:93490ms step_avg:54.04ms
step:1731/2160 train_time:93579ms step_avg:54.06ms
step:1732/2160 train_time:93665ms step_avg:54.08ms
step:1733/2160 train_time:93754ms step_avg:54.10ms
step:1734/2160 train_time:93840ms step_avg:54.12ms
step:1735/2160 train_time:93928ms step_avg:54.14ms
step:1736/2160 train_time:94015ms step_avg:54.16ms
step:1737/2160 train_time:94104ms step_avg:54.18ms
step:1738/2160 train_time:94191ms step_avg:54.19ms
step:1739/2160 train_time:94281ms step_avg:54.22ms
step:1740/2160 train_time:94369ms step_avg:54.23ms
step:1741/2160 train_time:94458ms step_avg:54.26ms
step:1742/2160 train_time:94545ms step_avg:54.27ms
step:1743/2160 train_time:94634ms step_avg:54.29ms
step:1744/2160 train_time:94720ms step_avg:54.31ms
step:1745/2160 train_time:94809ms step_avg:54.33ms
step:1746/2160 train_time:94896ms step_avg:54.35ms
step:1747/2160 train_time:94984ms step_avg:54.37ms
step:1748/2160 train_time:95070ms step_avg:54.39ms
step:1749/2160 train_time:95159ms step_avg:54.41ms
step:1750/2160 train_time:95245ms step_avg:54.43ms
step:1750/2160 val_loss:3.3945 train_time:95336ms step_avg:54.48ms
step:1751/2160 train_time:95358ms step_avg:54.46ms
step:1752/2160 train_time:95425ms step_avg:54.47ms
step:1753/2160 train_time:95518ms step_avg:54.49ms
step:1754/2160 train_time:95606ms step_avg:54.51ms
step:1755/2160 train_time:95694ms step_avg:54.53ms
step:1756/2160 train_time:95780ms step_avg:54.54ms
step:1757/2160 train_time:95869ms step_avg:54.56ms
step:1758/2160 train_time:95954ms step_avg:54.58ms
step:1759/2160 train_time:96042ms step_avg:54.60ms
step:1760/2160 train_time:96129ms step_avg:54.62ms
step:1761/2160 train_time:96216ms step_avg:54.64ms
step:1762/2160 train_time:96304ms step_avg:54.66ms
step:1763/2160 train_time:96395ms step_avg:54.68ms
step:1764/2160 train_time:96485ms step_avg:54.70ms
step:1765/2160 train_time:96574ms step_avg:54.72ms
step:1766/2160 train_time:96662ms step_avg:54.74ms
step:1767/2160 train_time:96752ms step_avg:54.75ms
step:1768/2160 train_time:96838ms step_avg:54.77ms
step:1769/2160 train_time:96925ms step_avg:54.79ms
step:1770/2160 train_time:97011ms step_avg:54.81ms
step:1771/2160 train_time:97099ms step_avg:54.83ms
step:1772/2160 train_time:97185ms step_avg:54.84ms
step:1773/2160 train_time:97274ms step_avg:54.86ms
step:1774/2160 train_time:97362ms step_avg:54.88ms
step:1775/2160 train_time:97453ms step_avg:54.90ms
step:1776/2160 train_time:97541ms step_avg:54.92ms
step:1777/2160 train_time:97631ms step_avg:54.94ms
step:1778/2160 train_time:97717ms step_avg:54.96ms
step:1779/2160 train_time:97806ms step_avg:54.98ms
step:1780/2160 train_time:97892ms step_avg:55.00ms
step:1781/2160 train_time:97979ms step_avg:55.01ms
step:1782/2160 train_time:98065ms step_avg:55.03ms
step:1783/2160 train_time:98153ms step_avg:55.05ms
step:1784/2160 train_time:98240ms step_avg:55.07ms
step:1785/2160 train_time:98330ms step_avg:55.09ms
step:1786/2160 train_time:98417ms step_avg:55.10ms
step:1787/2160 train_time:98507ms step_avg:55.12ms
step:1788/2160 train_time:98594ms step_avg:55.14ms
step:1789/2160 train_time:98683ms step_avg:55.16ms
step:1790/2160 train_time:98770ms step_avg:55.18ms
step:1791/2160 train_time:98857ms step_avg:55.20ms
step:1792/2160 train_time:98943ms step_avg:55.21ms
step:1793/2160 train_time:99032ms step_avg:55.23ms
step:1794/2160 train_time:99117ms step_avg:55.25ms
step:1795/2160 train_time:99205ms step_avg:55.27ms
step:1796/2160 train_time:99292ms step_avg:55.29ms
step:1797/2160 train_time:99380ms step_avg:55.30ms
step:1798/2160 train_time:99469ms step_avg:55.32ms
step:1799/2160 train_time:99557ms step_avg:55.34ms
step:1800/2160 train_time:99645ms step_avg:55.36ms
step:1801/2160 train_time:99734ms step_avg:55.38ms
step:1802/2160 train_time:99820ms step_avg:55.39ms
step:1803/2160 train_time:99908ms step_avg:55.41ms
step:1804/2160 train_time:99994ms step_avg:55.43ms
step:1805/2160 train_time:100082ms step_avg:55.45ms
step:1806/2160 train_time:100169ms step_avg:55.46ms
step:1807/2160 train_time:100258ms step_avg:55.48ms
step:1808/2160 train_time:100345ms step_avg:55.50ms
step:1809/2160 train_time:100434ms step_avg:55.52ms
step:1810/2160 train_time:100521ms step_avg:55.54ms
step:1811/2160 train_time:100611ms step_avg:55.56ms
step:1812/2160 train_time:100697ms step_avg:55.57ms
step:1813/2160 train_time:100786ms step_avg:55.59ms
step:1814/2160 train_time:100873ms step_avg:55.61ms
step:1815/2160 train_time:100961ms step_avg:55.63ms
step:1816/2160 train_time:101048ms step_avg:55.64ms
step:1817/2160 train_time:101135ms step_avg:55.66ms
step:1818/2160 train_time:101222ms step_avg:55.68ms
step:1819/2160 train_time:101312ms step_avg:55.70ms
step:1820/2160 train_time:101398ms step_avg:55.71ms
step:1821/2160 train_time:101487ms step_avg:55.73ms
step:1822/2160 train_time:101574ms step_avg:55.75ms
step:1823/2160 train_time:101662ms step_avg:55.77ms
step:1824/2160 train_time:101749ms step_avg:55.78ms
step:1825/2160 train_time:101837ms step_avg:55.80ms
step:1826/2160 train_time:101923ms step_avg:55.82ms
step:1827/2160 train_time:102013ms step_avg:55.84ms
step:1828/2160 train_time:102099ms step_avg:55.85ms
step:1829/2160 train_time:102187ms step_avg:55.87ms
step:1830/2160 train_time:102274ms step_avg:55.89ms
step:1831/2160 train_time:102362ms step_avg:55.90ms
step:1832/2160 train_time:102449ms step_avg:55.92ms
step:1833/2160 train_time:102538ms step_avg:55.94ms
step:1834/2160 train_time:102625ms step_avg:55.96ms
step:1835/2160 train_time:102714ms step_avg:55.98ms
step:1836/2160 train_time:102801ms step_avg:55.99ms
step:1837/2160 train_time:102889ms step_avg:56.01ms
step:1838/2160 train_time:102976ms step_avg:56.03ms
step:1839/2160 train_time:103064ms step_avg:56.04ms
step:1840/2160 train_time:103151ms step_avg:56.06ms
step:1841/2160 train_time:103238ms step_avg:56.08ms
step:1842/2160 train_time:103325ms step_avg:56.09ms
step:1843/2160 train_time:103414ms step_avg:56.11ms
step:1844/2160 train_time:103501ms step_avg:56.13ms
step:1845/2160 train_time:103591ms step_avg:56.15ms
step:1846/2160 train_time:103678ms step_avg:56.16ms
step:1847/2160 train_time:103767ms step_avg:56.18ms
step:1848/2160 train_time:103853ms step_avg:56.20ms
step:1849/2160 train_time:103941ms step_avg:56.21ms
step:1850/2160 train_time:104028ms step_avg:56.23ms
step:1851/2160 train_time:104116ms step_avg:56.25ms
step:1852/2160 train_time:104203ms step_avg:56.27ms
step:1853/2160 train_time:104293ms step_avg:56.28ms
step:1854/2160 train_time:104380ms step_avg:56.30ms
step:1855/2160 train_time:104468ms step_avg:56.32ms
step:1856/2160 train_time:104554ms step_avg:56.33ms
step:1857/2160 train_time:104643ms step_avg:56.35ms
step:1858/2160 train_time:104731ms step_avg:56.37ms
step:1859/2160 train_time:104819ms step_avg:56.38ms
step:1860/2160 train_time:104906ms step_avg:56.40ms
step:1861/2160 train_time:104994ms step_avg:56.42ms
step:1862/2160 train_time:105080ms step_avg:56.43ms
step:1863/2160 train_time:105170ms step_avg:56.45ms
step:1864/2160 train_time:105255ms step_avg:56.47ms
step:1865/2160 train_time:105344ms step_avg:56.48ms
step:1866/2160 train_time:105431ms step_avg:56.50ms
step:1867/2160 train_time:105519ms step_avg:56.52ms
step:1868/2160 train_time:105606ms step_avg:56.53ms
step:1869/2160 train_time:105695ms step_avg:56.55ms
step:1870/2160 train_time:105782ms step_avg:56.57ms
step:1871/2160 train_time:105872ms step_avg:56.59ms
step:1872/2160 train_time:105958ms step_avg:56.60ms
step:1873/2160 train_time:106046ms step_avg:56.62ms
step:1874/2160 train_time:106134ms step_avg:56.63ms
step:1875/2160 train_time:106221ms step_avg:56.65ms
step:1876/2160 train_time:106308ms step_avg:56.67ms
step:1877/2160 train_time:106397ms step_avg:56.68ms
step:1878/2160 train_time:106483ms step_avg:56.70ms
step:1879/2160 train_time:106573ms step_avg:56.72ms
step:1880/2160 train_time:106660ms step_avg:56.73ms
step:1881/2160 train_time:106749ms step_avg:56.75ms
step:1882/2160 train_time:106835ms step_avg:56.77ms
step:1883/2160 train_time:106923ms step_avg:56.78ms
step:1884/2160 train_time:107010ms step_avg:56.80ms
step:1885/2160 train_time:107099ms step_avg:56.82ms
step:1886/2160 train_time:107186ms step_avg:56.83ms
step:1887/2160 train_time:107274ms step_avg:56.85ms
step:1888/2160 train_time:107362ms step_avg:56.87ms
step:1889/2160 train_time:107452ms step_avg:56.88ms
step:1890/2160 train_time:107539ms step_avg:56.90ms
step:1891/2160 train_time:107627ms step_avg:56.92ms
step:1892/2160 train_time:107714ms step_avg:56.93ms
step:1893/2160 train_time:107801ms step_avg:56.95ms
step:1894/2160 train_time:107888ms step_avg:56.96ms
step:1895/2160 train_time:107976ms step_avg:56.98ms
step:1896/2160 train_time:108063ms step_avg:57.00ms
step:1897/2160 train_time:108154ms step_avg:57.01ms
step:1898/2160 train_time:108240ms step_avg:57.03ms
step:1899/2160 train_time:108328ms step_avg:57.05ms
step:1900/2160 train_time:108415ms step_avg:57.06ms
step:1901/2160 train_time:108504ms step_avg:57.08ms
step:1902/2160 train_time:108591ms step_avg:57.09ms
step:1903/2160 train_time:108679ms step_avg:57.11ms
step:1904/2160 train_time:108766ms step_avg:57.12ms
step:1905/2160 train_time:108854ms step_avg:57.14ms
step:1906/2160 train_time:108940ms step_avg:57.16ms
step:1907/2160 train_time:109030ms step_avg:57.17ms
step:1908/2160 train_time:109116ms step_avg:57.19ms
step:1909/2160 train_time:109204ms step_avg:57.20ms
step:1910/2160 train_time:109291ms step_avg:57.22ms
step:1911/2160 train_time:109379ms step_avg:57.24ms
step:1912/2160 train_time:109467ms step_avg:57.25ms
step:1913/2160 train_time:109555ms step_avg:57.27ms
step:1914/2160 train_time:109642ms step_avg:57.28ms
step:1915/2160 train_time:109731ms step_avg:57.30ms
step:1916/2160 train_time:109817ms step_avg:57.32ms
step:1917/2160 train_time:109906ms step_avg:57.33ms
step:1918/2160 train_time:109993ms step_avg:57.35ms
step:1919/2160 train_time:110082ms step_avg:57.36ms
step:1920/2160 train_time:110170ms step_avg:57.38ms
step:1921/2160 train_time:110258ms step_avg:57.40ms
step:1922/2160 train_time:110345ms step_avg:57.41ms
step:1923/2160 train_time:110433ms step_avg:57.43ms
step:1924/2160 train_time:110519ms step_avg:57.44ms
step:1925/2160 train_time:110607ms step_avg:57.46ms
step:1926/2160 train_time:110694ms step_avg:57.47ms
step:1927/2160 train_time:110782ms step_avg:57.49ms
step:1928/2160 train_time:110870ms step_avg:57.50ms
step:1929/2160 train_time:110956ms step_avg:57.52ms
step:1930/2160 train_time:111044ms step_avg:57.54ms
step:1931/2160 train_time:111133ms step_avg:57.55ms
step:1932/2160 train_time:111219ms step_avg:57.57ms
step:1933/2160 train_time:111307ms step_avg:57.58ms
step:1934/2160 train_time:111394ms step_avg:57.60ms
step:1935/2160 train_time:111482ms step_avg:57.61ms
step:1936/2160 train_time:111569ms step_avg:57.63ms
step:1937/2160 train_time:111657ms step_avg:57.64ms
step:1938/2160 train_time:111744ms step_avg:57.66ms
step:1939/2160 train_time:111833ms step_avg:57.68ms
step:1940/2160 train_time:111919ms step_avg:57.69ms
step:1941/2160 train_time:112008ms step_avg:57.71ms
step:1942/2160 train_time:112095ms step_avg:57.72ms
step:1943/2160 train_time:112185ms step_avg:57.74ms
step:1944/2160 train_time:112272ms step_avg:57.75ms
step:1945/2160 train_time:112360ms step_avg:57.77ms
step:1946/2160 train_time:112448ms step_avg:57.78ms
step:1947/2160 train_time:112535ms step_avg:57.80ms
step:1948/2160 train_time:112621ms step_avg:57.81ms
step:1949/2160 train_time:112710ms step_avg:57.83ms
step:1950/2160 train_time:112797ms step_avg:57.84ms
step:1951/2160 train_time:112884ms step_avg:57.86ms
step:1952/2160 train_time:112971ms step_avg:57.87ms
step:1953/2160 train_time:113059ms step_avg:57.89ms
step:1954/2160 train_time:113146ms step_avg:57.90ms
step:1955/2160 train_time:113234ms step_avg:57.92ms
step:1956/2160 train_time:113321ms step_avg:57.94ms
step:1957/2160 train_time:113410ms step_avg:57.95ms
step:1958/2160 train_time:113496ms step_avg:57.97ms
step:1959/2160 train_time:113584ms step_avg:57.98ms
step:1960/2160 train_time:113672ms step_avg:58.00ms
step:1961/2160 train_time:113760ms step_avg:58.01ms
step:1962/2160 train_time:113846ms step_avg:58.03ms
step:1963/2160 train_time:113935ms step_avg:58.04ms
step:1964/2160 train_time:114022ms step_avg:58.06ms
step:1965/2160 train_time:114110ms step_avg:58.07ms
step:1966/2160 train_time:114196ms step_avg:58.09ms
step:1967/2160 train_time:114285ms step_avg:58.10ms
step:1968/2160 train_time:114373ms step_avg:58.12ms
step:1969/2160 train_time:114461ms step_avg:58.13ms
step:1970/2160 train_time:114547ms step_avg:58.15ms
step:1971/2160 train_time:114636ms step_avg:58.16ms
step:1972/2160 train_time:114723ms step_avg:58.18ms
step:1973/2160 train_time:114812ms step_avg:58.19ms
step:1974/2160 train_time:114898ms step_avg:58.21ms
step:1975/2160 train_time:114987ms step_avg:58.22ms
step:1976/2160 train_time:115073ms step_avg:58.24ms
step:1977/2160 train_time:115160ms step_avg:58.25ms
step:1978/2160 train_time:115247ms step_avg:58.26ms
step:1979/2160 train_time:115336ms step_avg:58.28ms
step:1980/2160 train_time:115422ms step_avg:58.29ms
step:1981/2160 train_time:115511ms step_avg:58.31ms
step:1982/2160 train_time:115597ms step_avg:58.32ms
step:1983/2160 train_time:115686ms step_avg:58.34ms
step:1984/2160 train_time:115773ms step_avg:58.35ms
step:1985/2160 train_time:115861ms step_avg:58.37ms
step:1986/2160 train_time:115948ms step_avg:58.38ms
step:1987/2160 train_time:116036ms step_avg:58.40ms
step:1988/2160 train_time:116123ms step_avg:58.41ms
step:1989/2160 train_time:116213ms step_avg:58.43ms
step:1990/2160 train_time:116301ms step_avg:58.44ms
step:1991/2160 train_time:116389ms step_avg:58.46ms
step:1992/2160 train_time:116475ms step_avg:58.47ms
step:1993/2160 train_time:116563ms step_avg:58.49ms
step:1994/2160 train_time:116650ms step_avg:58.50ms
step:1995/2160 train_time:116738ms step_avg:58.52ms
step:1996/2160 train_time:116825ms step_avg:58.53ms
step:1997/2160 train_time:116915ms step_avg:58.55ms
step:1998/2160 train_time:117001ms step_avg:58.56ms
step:1999/2160 train_time:117089ms step_avg:58.57ms
step:2000/2160 train_time:117175ms step_avg:58.59ms
step:2000/2160 val_loss:3.3177 train_time:117266ms step_avg:58.63ms
step:2001/2160 train_time:117289ms step_avg:58.62ms
step:2002/2160 train_time:117354ms step_avg:58.62ms
step:2003/2160 train_time:117449ms step_avg:58.64ms
step:2004/2160 train_time:117537ms step_avg:58.65ms
step:2005/2160 train_time:117626ms step_avg:58.67ms
step:2006/2160 train_time:117711ms step_avg:58.68ms
step:2007/2160 train_time:117798ms step_avg:58.69ms
step:2008/2160 train_time:117884ms step_avg:58.71ms
step:2009/2160 train_time:117971ms step_avg:58.72ms
step:2010/2160 train_time:118057ms step_avg:58.74ms
step:2011/2160 train_time:118144ms step_avg:58.75ms
step:2012/2160 train_time:118232ms step_avg:58.76ms
step:2013/2160 train_time:118323ms step_avg:58.78ms
step:2014/2160 train_time:118413ms step_avg:58.79ms
step:2015/2160 train_time:118504ms step_avg:58.81ms
step:2016/2160 train_time:118591ms step_avg:58.82ms
step:2017/2160 train_time:118679ms step_avg:58.84ms
step:2018/2160 train_time:118765ms step_avg:58.85ms
step:2019/2160 train_time:118851ms step_avg:58.87ms
step:2020/2160 train_time:118936ms step_avg:58.88ms
step:2021/2160 train_time:119024ms step_avg:58.89ms
step:2022/2160 train_time:119110ms step_avg:58.91ms
step:2023/2160 train_time:119200ms step_avg:58.92ms
step:2024/2160 train_time:119287ms step_avg:58.94ms
step:2025/2160 train_time:119378ms step_avg:58.95ms
step:2026/2160 train_time:119466ms step_avg:58.97ms
step:2027/2160 train_time:119555ms step_avg:58.98ms
step:2028/2160 train_time:119642ms step_avg:58.99ms
step:2029/2160 train_time:119730ms step_avg:59.01ms
step:2030/2160 train_time:119816ms step_avg:59.02ms
step:2031/2160 train_time:119904ms step_avg:59.04ms
step:2032/2160 train_time:119989ms step_avg:59.05ms
step:2033/2160 train_time:120077ms step_avg:59.06ms
step:2034/2160 train_time:120164ms step_avg:59.08ms
step:2035/2160 train_time:120252ms step_avg:59.09ms
step:2036/2160 train_time:120340ms step_avg:59.11ms
step:2037/2160 train_time:120429ms step_avg:59.12ms
step:2038/2160 train_time:120517ms step_avg:59.13ms
step:2039/2160 train_time:120607ms step_avg:59.15ms
step:2040/2160 train_time:120693ms step_avg:59.16ms
step:2041/2160 train_time:120781ms step_avg:59.18ms
step:2042/2160 train_time:120866ms step_avg:59.19ms
step:2043/2160 train_time:120953ms step_avg:59.20ms
step:2044/2160 train_time:121040ms step_avg:59.22ms
step:2045/2160 train_time:121128ms step_avg:59.23ms
step:2046/2160 train_time:121215ms step_avg:59.24ms
step:2047/2160 train_time:121305ms step_avg:59.26ms
step:2048/2160 train_time:121392ms step_avg:59.27ms
step:2049/2160 train_time:121482ms step_avg:59.29ms
step:2050/2160 train_time:121569ms step_avg:59.30ms
step:2051/2160 train_time:121658ms step_avg:59.32ms
step:2052/2160 train_time:121745ms step_avg:59.33ms
step:2053/2160 train_time:121831ms step_avg:59.34ms
step:2054/2160 train_time:121918ms step_avg:59.36ms
step:2055/2160 train_time:122006ms step_avg:59.37ms
step:2056/2160 train_time:122092ms step_avg:59.38ms
step:2057/2160 train_time:122180ms step_avg:59.40ms
step:2058/2160 train_time:122266ms step_avg:59.41ms
step:2059/2160 train_time:122354ms step_avg:59.42ms
step:2060/2160 train_time:122442ms step_avg:59.44ms
step:2061/2160 train_time:122531ms step_avg:59.45ms
step:2062/2160 train_time:122619ms step_avg:59.47ms
step:2063/2160 train_time:122707ms step_avg:59.48ms
step:2064/2160 train_time:122793ms step_avg:59.49ms
step:2065/2160 train_time:122881ms step_avg:59.51ms
step:2066/2160 train_time:122967ms step_avg:59.52ms
step:2067/2160 train_time:123055ms step_avg:59.53ms
step:2068/2160 train_time:123141ms step_avg:59.55ms
step:2069/2160 train_time:123230ms step_avg:59.56ms
step:2070/2160 train_time:123317ms step_avg:59.57ms
step:2071/2160 train_time:123406ms step_avg:59.59ms
step:2072/2160 train_time:123493ms step_avg:59.60ms
step:2073/2160 train_time:123582ms step_avg:59.61ms
step:2074/2160 train_time:123668ms step_avg:59.63ms
step:2075/2160 train_time:123756ms step_avg:59.64ms
step:2076/2160 train_time:123843ms step_avg:59.65ms
step:2077/2160 train_time:123930ms step_avg:59.67ms
step:2078/2160 train_time:124016ms step_avg:59.68ms
step:2079/2160 train_time:124106ms step_avg:59.69ms
step:2080/2160 train_time:124192ms step_avg:59.71ms
step:2081/2160 train_time:124281ms step_avg:59.72ms
step:2082/2160 train_time:124368ms step_avg:59.73ms
step:2083/2160 train_time:124456ms step_avg:59.75ms
step:2084/2160 train_time:124544ms step_avg:59.76ms
step:2085/2160 train_time:124632ms step_avg:59.78ms
step:2086/2160 train_time:124718ms step_avg:59.79ms
step:2087/2160 train_time:124806ms step_avg:59.80ms
step:2088/2160 train_time:124893ms step_avg:59.81ms
step:2089/2160 train_time:124981ms step_avg:59.83ms
step:2090/2160 train_time:125066ms step_avg:59.84ms
step:2091/2160 train_time:125155ms step_avg:59.85ms
step:2092/2160 train_time:125242ms step_avg:59.87ms
step:2093/2160 train_time:125330ms step_avg:59.88ms
step:2094/2160 train_time:125417ms step_avg:59.89ms
step:2095/2160 train_time:125505ms step_avg:59.91ms
step:2096/2160 train_time:125591ms step_avg:59.92ms
step:2097/2160 train_time:125679ms step_avg:59.93ms
step:2098/2160 train_time:125766ms step_avg:59.95ms
step:2099/2160 train_time:125853ms step_avg:59.96ms
step:2100/2160 train_time:125940ms step_avg:59.97ms
step:2101/2160 train_time:126027ms step_avg:59.98ms
step:2102/2160 train_time:126115ms step_avg:60.00ms
step:2103/2160 train_time:126205ms step_avg:60.01ms
step:2104/2160 train_time:126292ms step_avg:60.02ms
step:2105/2160 train_time:126380ms step_avg:60.04ms
step:2106/2160 train_time:126466ms step_avg:60.05ms
step:2107/2160 train_time:126554ms step_avg:60.06ms
step:2108/2160 train_time:126642ms step_avg:60.08ms
step:2109/2160 train_time:126730ms step_avg:60.09ms
step:2110/2160 train_time:126816ms step_avg:60.10ms
step:2111/2160 train_time:126905ms step_avg:60.12ms
step:2112/2160 train_time:126991ms step_avg:60.13ms
step:2113/2160 train_time:127079ms step_avg:60.14ms
step:2114/2160 train_time:127166ms step_avg:60.15ms
step:2115/2160 train_time:127254ms step_avg:60.17ms
step:2116/2160 train_time:127342ms step_avg:60.18ms
step:2117/2160 train_time:127429ms step_avg:60.19ms
step:2118/2160 train_time:127516ms step_avg:60.21ms
step:2119/2160 train_time:127605ms step_avg:60.22ms
step:2120/2160 train_time:127692ms step_avg:60.23ms
step:2121/2160 train_time:127781ms step_avg:60.25ms
step:2122/2160 train_time:127867ms step_avg:60.26ms
step:2123/2160 train_time:127954ms step_avg:60.27ms
step:2124/2160 train_time:128041ms step_avg:60.28ms
step:2125/2160 train_time:128130ms step_avg:60.30ms
step:2126/2160 train_time:128217ms step_avg:60.31ms
step:2127/2160 train_time:128307ms step_avg:60.32ms
step:2128/2160 train_time:128394ms step_avg:60.34ms
step:2129/2160 train_time:128483ms step_avg:60.35ms
step:2130/2160 train_time:128569ms step_avg:60.36ms
step:2131/2160 train_time:128658ms step_avg:60.37ms
step:2132/2160 train_time:128745ms step_avg:60.39ms
step:2133/2160 train_time:128833ms step_avg:60.40ms
step:2134/2160 train_time:128920ms step_avg:60.41ms
step:2135/2160 train_time:129008ms step_avg:60.43ms
step:2136/2160 train_time:129095ms step_avg:60.44ms
step:2137/2160 train_time:129184ms step_avg:60.45ms
step:2138/2160 train_time:129271ms step_avg:60.46ms
step:2139/2160 train_time:129360ms step_avg:60.48ms
step:2140/2160 train_time:129447ms step_avg:60.49ms
step:2141/2160 train_time:129536ms step_avg:60.50ms
step:2142/2160 train_time:129623ms step_avg:60.52ms
step:2143/2160 train_time:129712ms step_avg:60.53ms
step:2144/2160 train_time:129799ms step_avg:60.54ms
step:2145/2160 train_time:129887ms step_avg:60.55ms
step:2146/2160 train_time:129973ms step_avg:60.57ms
step:2147/2160 train_time:130062ms step_avg:60.58ms
step:2148/2160 train_time:130148ms step_avg:60.59ms
step:2149/2160 train_time:130236ms step_avg:60.60ms
step:2150/2160 train_time:130324ms step_avg:60.62ms
step:2151/2160 train_time:130411ms step_avg:60.63ms
step:2152/2160 train_time:130498ms step_avg:60.64ms
step:2153/2160 train_time:130586ms step_avg:60.65ms
step:2154/2160 train_time:130673ms step_avg:60.67ms
step:2155/2160 train_time:130763ms step_avg:60.68ms
step:2156/2160 train_time:130848ms step_avg:60.69ms
step:2157/2160 train_time:130937ms step_avg:60.70ms
step:2158/2160 train_time:131025ms step_avg:60.72ms
step:2159/2160 train_time:131113ms step_avg:60.73ms
step:2160/2160 train_time:131201ms step_avg:60.74ms
step:2160/2160 val_loss:3.2807 train_time:131291ms step_avg:60.78ms
peak memory allocated: 29975 MiB reserved: 44696 MiB

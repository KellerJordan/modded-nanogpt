import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 12:01:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           15439      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           15440      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           15441      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           15442      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           15443      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           15444      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           15445      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           15446      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           15440      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           15441      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           15442      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           15443      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           15444      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           15445      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           15446      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2160 train_time:96ms step_avg:95.69ms
step:2/2160 train_time:123ms step_avg:61.55ms
step:3/2160 train_time:147ms step_avg:49.07ms
step:4/2160 train_time:173ms step_avg:43.18ms
step:5/2160 train_time:196ms step_avg:39.22ms
step:6/2160 train_time:338ms step_avg:56.30ms
step:7/2160 train_time:387ms step_avg:55.22ms
step:8/2160 train_time:419ms step_avg:52.34ms
step:9/2160 train_time:452ms step_avg:50.18ms
step:10/2160 train_time:484ms step_avg:48.39ms
step:11/2160 train_time:517ms step_avg:47.01ms
step:12/2160 train_time:550ms step_avg:45.80ms
step:13/2160 train_time:583ms step_avg:44.81ms
step:14/2160 train_time:615ms step_avg:43.94ms
step:15/2160 train_time:648ms step_avg:43.22ms
step:16/2160 train_time:681ms step_avg:42.55ms
step:17/2160 train_time:714ms step_avg:42.00ms
step:18/2160 train_time:747ms step_avg:41.48ms
step:19/2160 train_time:780ms step_avg:41.04ms
step:20/2160 train_time:812ms step_avg:40.60ms
step:21/2160 train_time:845ms step_avg:40.26ms
step:22/2160 train_time:878ms step_avg:39.90ms
step:23/2160 train_time:911ms step_avg:39.63ms
step:24/2160 train_time:944ms step_avg:39.33ms
step:25/2160 train_time:977ms step_avg:39.09ms
step:26/2160 train_time:1010ms step_avg:38.83ms
step:27/2160 train_time:1043ms step_avg:38.62ms
step:28/2160 train_time:1075ms step_avg:38.39ms
step:29/2160 train_time:1108ms step_avg:38.22ms
step:30/2160 train_time:1141ms step_avg:38.02ms
step:31/2160 train_time:1174ms step_avg:37.87ms
step:32/2160 train_time:1206ms step_avg:37.70ms
step:33/2160 train_time:1240ms step_avg:37.57ms
step:34/2160 train_time:1273ms step_avg:37.43ms
step:35/2160 train_time:1307ms step_avg:37.33ms
step:36/2160 train_time:1340ms step_avg:37.21ms
step:37/2160 train_time:1374ms step_avg:37.14ms
step:38/2160 train_time:1407ms step_avg:37.03ms
step:39/2160 train_time:1441ms step_avg:36.94ms
step:40/2160 train_time:1473ms step_avg:36.83ms
step:41/2160 train_time:1507ms step_avg:36.75ms
step:42/2160 train_time:1539ms step_avg:36.65ms
step:43/2160 train_time:1573ms step_avg:36.58ms
step:44/2160 train_time:1606ms step_avg:36.49ms
step:45/2160 train_time:1639ms step_avg:36.43ms
step:46/2160 train_time:1672ms step_avg:36.35ms
step:47/2160 train_time:1705ms step_avg:36.28ms
step:48/2160 train_time:1738ms step_avg:36.21ms
step:49/2160 train_time:1771ms step_avg:36.15ms
step:50/2160 train_time:1804ms step_avg:36.07ms
step:51/2160 train_time:1837ms step_avg:36.01ms
step:52/2160 train_time:1869ms step_avg:35.94ms
step:53/2160 train_time:1902ms step_avg:35.89ms
step:54/2160 train_time:1935ms step_avg:35.83ms
step:55/2160 train_time:1968ms step_avg:35.78ms
step:56/2160 train_time:2001ms step_avg:35.73ms
step:57/2160 train_time:2034ms step_avg:35.68ms
step:58/2160 train_time:2067ms step_avg:35.63ms
step:59/2160 train_time:2100ms step_avg:35.59ms
step:60/2160 train_time:2132ms step_avg:35.53ms
step:61/2160 train_time:2165ms step_avg:35.49ms
step:62/2160 train_time:2198ms step_avg:35.45ms
step:63/2160 train_time:2231ms step_avg:35.41ms
step:64/2160 train_time:2263ms step_avg:35.36ms
step:65/2160 train_time:2297ms step_avg:35.34ms
step:66/2160 train_time:2330ms step_avg:35.30ms
step:67/2160 train_time:2363ms step_avg:35.27ms
step:68/2160 train_time:2396ms step_avg:35.23ms
step:69/2160 train_time:2429ms step_avg:35.21ms
step:70/2160 train_time:2462ms step_avg:35.17ms
step:71/2160 train_time:2496ms step_avg:35.15ms
step:72/2160 train_time:2528ms step_avg:35.11ms
step:73/2160 train_time:2562ms step_avg:35.09ms
step:74/2160 train_time:2594ms step_avg:35.06ms
step:75/2160 train_time:2627ms step_avg:35.03ms
step:76/2160 train_time:2660ms step_avg:35.00ms
step:77/2160 train_time:2693ms step_avg:34.98ms
step:78/2160 train_time:2726ms step_avg:34.95ms
step:79/2160 train_time:2759ms step_avg:34.93ms
step:80/2160 train_time:2791ms step_avg:34.89ms
step:81/2160 train_time:2825ms step_avg:34.87ms
step:82/2160 train_time:2857ms step_avg:34.84ms
step:83/2160 train_time:2891ms step_avg:34.83ms
step:84/2160 train_time:2923ms step_avg:34.80ms
step:85/2160 train_time:2956ms step_avg:34.78ms
step:86/2160 train_time:2989ms step_avg:34.75ms
step:87/2160 train_time:3022ms step_avg:34.73ms
step:88/2160 train_time:3054ms step_avg:34.71ms
step:89/2160 train_time:3087ms step_avg:34.69ms
step:90/2160 train_time:3120ms step_avg:34.66ms
step:91/2160 train_time:3153ms step_avg:34.64ms
step:92/2160 train_time:3185ms step_avg:34.62ms
step:93/2160 train_time:3218ms step_avg:34.61ms
step:94/2160 train_time:3251ms step_avg:34.58ms
step:95/2160 train_time:3284ms step_avg:34.57ms
step:96/2160 train_time:3316ms step_avg:34.54ms
step:97/2160 train_time:3350ms step_avg:34.54ms
step:98/2160 train_time:3382ms step_avg:34.51ms
step:99/2160 train_time:3416ms step_avg:34.50ms
step:100/2160 train_time:3449ms step_avg:34.49ms
step:101/2160 train_time:3482ms step_avg:34.47ms
step:102/2160 train_time:3514ms step_avg:34.45ms
step:103/2160 train_time:3548ms step_avg:34.44ms
step:104/2160 train_time:3580ms step_avg:34.42ms
step:105/2160 train_time:3614ms step_avg:34.41ms
step:106/2160 train_time:3646ms step_avg:34.40ms
step:107/2160 train_time:3679ms step_avg:34.39ms
step:108/2160 train_time:3712ms step_avg:34.37ms
step:109/2160 train_time:3745ms step_avg:34.36ms
step:110/2160 train_time:3777ms step_avg:34.34ms
step:111/2160 train_time:3811ms step_avg:34.33ms
step:112/2160 train_time:3843ms step_avg:34.31ms
step:113/2160 train_time:3877ms step_avg:34.31ms
step:114/2160 train_time:3909ms step_avg:34.29ms
step:115/2160 train_time:3942ms step_avg:34.28ms
step:116/2160 train_time:3975ms step_avg:34.26ms
step:117/2160 train_time:4008ms step_avg:34.26ms
step:118/2160 train_time:4040ms step_avg:34.24ms
step:119/2160 train_time:4074ms step_avg:34.23ms
step:120/2160 train_time:4106ms step_avg:34.22ms
step:121/2160 train_time:4139ms step_avg:34.21ms
step:122/2160 train_time:4172ms step_avg:34.19ms
step:123/2160 train_time:4205ms step_avg:34.18ms
step:124/2160 train_time:4237ms step_avg:34.17ms
step:125/2160 train_time:4270ms step_avg:34.16ms
step:126/2160 train_time:4303ms step_avg:34.15ms
step:127/2160 train_time:4336ms step_avg:34.14ms
step:128/2160 train_time:4369ms step_avg:34.13ms
step:129/2160 train_time:4402ms step_avg:34.12ms
step:130/2160 train_time:4434ms step_avg:34.11ms
step:131/2160 train_time:4468ms step_avg:34.10ms
step:132/2160 train_time:4500ms step_avg:34.09ms
step:133/2160 train_time:4533ms step_avg:34.08ms
step:134/2160 train_time:4566ms step_avg:34.07ms
step:135/2160 train_time:4599ms step_avg:34.06ms
step:136/2160 train_time:4631ms step_avg:34.05ms
step:137/2160 train_time:4664ms step_avg:34.04ms
step:138/2160 train_time:4696ms step_avg:34.03ms
step:139/2160 train_time:4729ms step_avg:34.02ms
step:140/2160 train_time:4762ms step_avg:34.01ms
step:141/2160 train_time:4795ms step_avg:34.01ms
step:142/2160 train_time:4827ms step_avg:34.00ms
step:143/2160 train_time:4861ms step_avg:33.99ms
step:144/2160 train_time:4893ms step_avg:33.98ms
step:145/2160 train_time:4926ms step_avg:33.97ms
step:146/2160 train_time:4958ms step_avg:33.96ms
step:147/2160 train_time:4992ms step_avg:33.96ms
step:148/2160 train_time:5024ms step_avg:33.95ms
step:149/2160 train_time:5057ms step_avg:33.94ms
step:150/2160 train_time:5090ms step_avg:33.93ms
step:151/2160 train_time:5123ms step_avg:33.93ms
step:152/2160 train_time:5155ms step_avg:33.91ms
step:153/2160 train_time:5188ms step_avg:33.91ms
step:154/2160 train_time:5220ms step_avg:33.90ms
step:155/2160 train_time:5253ms step_avg:33.89ms
step:156/2160 train_time:5286ms step_avg:33.88ms
step:157/2160 train_time:5319ms step_avg:33.88ms
step:158/2160 train_time:5351ms step_avg:33.87ms
step:159/2160 train_time:5384ms step_avg:33.86ms
step:160/2160 train_time:5417ms step_avg:33.86ms
step:161/2160 train_time:5450ms step_avg:33.85ms
step:162/2160 train_time:5483ms step_avg:33.84ms
step:163/2160 train_time:5516ms step_avg:33.84ms
step:164/2160 train_time:5548ms step_avg:33.83ms
step:165/2160 train_time:5581ms step_avg:33.83ms
step:166/2160 train_time:5614ms step_avg:33.82ms
step:167/2160 train_time:5647ms step_avg:33.81ms
step:168/2160 train_time:5679ms step_avg:33.80ms
step:169/2160 train_time:5712ms step_avg:33.80ms
step:170/2160 train_time:5745ms step_avg:33.79ms
step:171/2160 train_time:5778ms step_avg:33.79ms
step:172/2160 train_time:5811ms step_avg:33.78ms
step:173/2160 train_time:5844ms step_avg:33.78ms
step:174/2160 train_time:5876ms step_avg:33.77ms
step:175/2160 train_time:5909ms step_avg:33.77ms
step:176/2160 train_time:5941ms step_avg:33.76ms
step:177/2160 train_time:5974ms step_avg:33.75ms
step:178/2160 train_time:6007ms step_avg:33.75ms
step:179/2160 train_time:6040ms step_avg:33.74ms
step:180/2160 train_time:6072ms step_avg:33.73ms
step:181/2160 train_time:6105ms step_avg:33.73ms
step:182/2160 train_time:6138ms step_avg:33.72ms
step:183/2160 train_time:6171ms step_avg:33.72ms
step:184/2160 train_time:6203ms step_avg:33.71ms
step:185/2160 train_time:6236ms step_avg:33.71ms
step:186/2160 train_time:6269ms step_avg:33.70ms
step:187/2160 train_time:6301ms step_avg:33.70ms
step:188/2160 train_time:6334ms step_avg:33.69ms
step:189/2160 train_time:6367ms step_avg:33.69ms
step:190/2160 train_time:6399ms step_avg:33.68ms
step:191/2160 train_time:6432ms step_avg:33.68ms
step:192/2160 train_time:6465ms step_avg:33.67ms
step:193/2160 train_time:6498ms step_avg:33.67ms
step:194/2160 train_time:6530ms step_avg:33.66ms
step:195/2160 train_time:6563ms step_avg:33.66ms
step:196/2160 train_time:6596ms step_avg:33.65ms
step:197/2160 train_time:6628ms step_avg:33.65ms
step:198/2160 train_time:6661ms step_avg:33.64ms
step:199/2160 train_time:6694ms step_avg:33.64ms
step:200/2160 train_time:6727ms step_avg:33.63ms
step:201/2160 train_time:6760ms step_avg:33.63ms
step:202/2160 train_time:6792ms step_avg:33.62ms
step:203/2160 train_time:6825ms step_avg:33.62ms
step:204/2160 train_time:6858ms step_avg:33.62ms
step:205/2160 train_time:6891ms step_avg:33.61ms
step:206/2160 train_time:6923ms step_avg:33.61ms
step:207/2160 train_time:6956ms step_avg:33.60ms
step:208/2160 train_time:6988ms step_avg:33.60ms
step:209/2160 train_time:7022ms step_avg:33.60ms
step:210/2160 train_time:7054ms step_avg:33.59ms
step:211/2160 train_time:7087ms step_avg:33.59ms
step:212/2160 train_time:7120ms step_avg:33.58ms
step:213/2160 train_time:7153ms step_avg:33.58ms
step:214/2160 train_time:7185ms step_avg:33.57ms
step:215/2160 train_time:7218ms step_avg:33.57ms
step:216/2160 train_time:7251ms step_avg:33.57ms
step:217/2160 train_time:7284ms step_avg:33.57ms
step:218/2160 train_time:7316ms step_avg:33.56ms
step:219/2160 train_time:7349ms step_avg:33.56ms
step:220/2160 train_time:7382ms step_avg:33.55ms
step:221/2160 train_time:7415ms step_avg:33.55ms
step:222/2160 train_time:7447ms step_avg:33.55ms
step:223/2160 train_time:7480ms step_avg:33.54ms
step:224/2160 train_time:7513ms step_avg:33.54ms
step:225/2160 train_time:7546ms step_avg:33.54ms
step:226/2160 train_time:7578ms step_avg:33.53ms
step:227/2160 train_time:7611ms step_avg:33.53ms
step:228/2160 train_time:7644ms step_avg:33.53ms
step:229/2160 train_time:7677ms step_avg:33.52ms
step:230/2160 train_time:7709ms step_avg:33.52ms
step:231/2160 train_time:7742ms step_avg:33.52ms
step:232/2160 train_time:7774ms step_avg:33.51ms
step:233/2160 train_time:7807ms step_avg:33.51ms
step:234/2160 train_time:7840ms step_avg:33.50ms
step:235/2160 train_time:7873ms step_avg:33.50ms
step:236/2160 train_time:7905ms step_avg:33.50ms
step:237/2160 train_time:7938ms step_avg:33.50ms
step:238/2160 train_time:7970ms step_avg:33.49ms
step:239/2160 train_time:8003ms step_avg:33.49ms
step:240/2160 train_time:8036ms step_avg:33.48ms
step:241/2160 train_time:8069ms step_avg:33.48ms
step:242/2160 train_time:8101ms step_avg:33.48ms
step:243/2160 train_time:8134ms step_avg:33.47ms
step:244/2160 train_time:8167ms step_avg:33.47ms
step:245/2160 train_time:8200ms step_avg:33.47ms
step:246/2160 train_time:8232ms step_avg:33.46ms
step:247/2160 train_time:8265ms step_avg:33.46ms
step:248/2160 train_time:8297ms step_avg:33.46ms
step:249/2160 train_time:8330ms step_avg:33.45ms
step:250/2160 train_time:8363ms step_avg:33.45ms
step:250/2160 val_loss:4.3265 train_time:8398ms step_avg:33.59ms
step:251/2160 train_time:8421ms step_avg:33.55ms
step:252/2160 train_time:8444ms step_avg:33.51ms
step:253/2160 train_time:8466ms step_avg:33.46ms
step:254/2160 train_time:8498ms step_avg:33.46ms
step:255/2160 train_time:8533ms step_avg:33.46ms
step:256/2160 train_time:8567ms step_avg:33.46ms
step:257/2160 train_time:8602ms step_avg:33.47ms
step:258/2160 train_time:8635ms step_avg:33.47ms
step:259/2160 train_time:8669ms step_avg:33.47ms
step:260/2160 train_time:8701ms step_avg:33.47ms
step:261/2160 train_time:8734ms step_avg:33.46ms
step:262/2160 train_time:8767ms step_avg:33.46ms
step:263/2160 train_time:8800ms step_avg:33.46ms
step:264/2160 train_time:8832ms step_avg:33.45ms
step:265/2160 train_time:8865ms step_avg:33.45ms
step:266/2160 train_time:8897ms step_avg:33.45ms
step:267/2160 train_time:8930ms step_avg:33.44ms
step:268/2160 train_time:8962ms step_avg:33.44ms
step:269/2160 train_time:8995ms step_avg:33.44ms
step:270/2160 train_time:9027ms step_avg:33.43ms
step:271/2160 train_time:9060ms step_avg:33.43ms
step:272/2160 train_time:9092ms step_avg:33.43ms
step:273/2160 train_time:9125ms step_avg:33.42ms
step:274/2160 train_time:9157ms step_avg:33.42ms
step:275/2160 train_time:9190ms step_avg:33.42ms
step:276/2160 train_time:9222ms step_avg:33.41ms
step:277/2160 train_time:9255ms step_avg:33.41ms
step:278/2160 train_time:9288ms step_avg:33.41ms
step:279/2160 train_time:9320ms step_avg:33.41ms
step:280/2160 train_time:9353ms step_avg:33.40ms
step:281/2160 train_time:9386ms step_avg:33.40ms
step:282/2160 train_time:9418ms step_avg:33.40ms
step:283/2160 train_time:9451ms step_avg:33.39ms
step:284/2160 train_time:9483ms step_avg:33.39ms
step:285/2160 train_time:9516ms step_avg:33.39ms
step:286/2160 train_time:9550ms step_avg:33.39ms
step:287/2160 train_time:9583ms step_avg:33.39ms
step:288/2160 train_time:9615ms step_avg:33.39ms
step:289/2160 train_time:9649ms step_avg:33.39ms
step:290/2160 train_time:9681ms step_avg:33.38ms
step:291/2160 train_time:9714ms step_avg:33.38ms
step:292/2160 train_time:9747ms step_avg:33.38ms
step:293/2160 train_time:9780ms step_avg:33.38ms
step:294/2160 train_time:9813ms step_avg:33.38ms
step:295/2160 train_time:9846ms step_avg:33.37ms
step:296/2160 train_time:9878ms step_avg:33.37ms
step:297/2160 train_time:9911ms step_avg:33.37ms
step:298/2160 train_time:9943ms step_avg:33.37ms
step:299/2160 train_time:9976ms step_avg:33.36ms
step:300/2160 train_time:10009ms step_avg:33.36ms
step:301/2160 train_time:10041ms step_avg:33.36ms
step:302/2160 train_time:10074ms step_avg:33.36ms
step:303/2160 train_time:10107ms step_avg:33.36ms
step:304/2160 train_time:10139ms step_avg:33.35ms
step:305/2160 train_time:10172ms step_avg:33.35ms
step:306/2160 train_time:10204ms step_avg:33.35ms
step:307/2160 train_time:10237ms step_avg:33.34ms
step:308/2160 train_time:10269ms step_avg:33.34ms
step:309/2160 train_time:10303ms step_avg:33.34ms
step:310/2160 train_time:10335ms step_avg:33.34ms
step:311/2160 train_time:10368ms step_avg:33.34ms
step:312/2160 train_time:10400ms step_avg:33.33ms
step:313/2160 train_time:10433ms step_avg:33.33ms
step:314/2160 train_time:10465ms step_avg:33.33ms
step:315/2160 train_time:10499ms step_avg:33.33ms
step:316/2160 train_time:10531ms step_avg:33.33ms
step:317/2160 train_time:10565ms step_avg:33.33ms
step:318/2160 train_time:10597ms step_avg:33.32ms
step:319/2160 train_time:10631ms step_avg:33.33ms
step:320/2160 train_time:10664ms step_avg:33.32ms
step:321/2160 train_time:10696ms step_avg:33.32ms
step:322/2160 train_time:10729ms step_avg:33.32ms
step:323/2160 train_time:10762ms step_avg:33.32ms
step:324/2160 train_time:10794ms step_avg:33.32ms
step:325/2160 train_time:10827ms step_avg:33.31ms
step:326/2160 train_time:10860ms step_avg:33.31ms
step:327/2160 train_time:10893ms step_avg:33.31ms
step:328/2160 train_time:10925ms step_avg:33.31ms
step:329/2160 train_time:10958ms step_avg:33.31ms
step:330/2160 train_time:10991ms step_avg:33.31ms
step:331/2160 train_time:11024ms step_avg:33.30ms
step:332/2160 train_time:11056ms step_avg:33.30ms
step:333/2160 train_time:11089ms step_avg:33.30ms
step:334/2160 train_time:11121ms step_avg:33.30ms
step:335/2160 train_time:11154ms step_avg:33.30ms
step:336/2160 train_time:11187ms step_avg:33.29ms
step:337/2160 train_time:11220ms step_avg:33.29ms
step:338/2160 train_time:11252ms step_avg:33.29ms
step:339/2160 train_time:11285ms step_avg:33.29ms
step:340/2160 train_time:11317ms step_avg:33.28ms
step:341/2160 train_time:11350ms step_avg:33.28ms
step:342/2160 train_time:11382ms step_avg:33.28ms
step:343/2160 train_time:11415ms step_avg:33.28ms
step:344/2160 train_time:11448ms step_avg:33.28ms
step:345/2160 train_time:11481ms step_avg:33.28ms
step:346/2160 train_time:11513ms step_avg:33.28ms
step:347/2160 train_time:11546ms step_avg:33.27ms
step:348/2160 train_time:11579ms step_avg:33.27ms
step:349/2160 train_time:11612ms step_avg:33.27ms
step:350/2160 train_time:11645ms step_avg:33.27ms
step:351/2160 train_time:11678ms step_avg:33.27ms
step:352/2160 train_time:11710ms step_avg:33.27ms
step:353/2160 train_time:11743ms step_avg:33.27ms
step:354/2160 train_time:11775ms step_avg:33.26ms
step:355/2160 train_time:11808ms step_avg:33.26ms
step:356/2160 train_time:11840ms step_avg:33.26ms
step:357/2160 train_time:11874ms step_avg:33.26ms
step:358/2160 train_time:11906ms step_avg:33.26ms
step:359/2160 train_time:11940ms step_avg:33.26ms
step:360/2160 train_time:11972ms step_avg:33.26ms
step:361/2160 train_time:12005ms step_avg:33.26ms
step:362/2160 train_time:12038ms step_avg:33.25ms
step:363/2160 train_time:12071ms step_avg:33.25ms
step:364/2160 train_time:12103ms step_avg:33.25ms
step:365/2160 train_time:12136ms step_avg:33.25ms
step:366/2160 train_time:12168ms step_avg:33.25ms
step:367/2160 train_time:12201ms step_avg:33.25ms
step:368/2160 train_time:12233ms step_avg:33.24ms
step:369/2160 train_time:12266ms step_avg:33.24ms
step:370/2160 train_time:12298ms step_avg:33.24ms
step:371/2160 train_time:12331ms step_avg:33.24ms
step:372/2160 train_time:12364ms step_avg:33.24ms
step:373/2160 train_time:12397ms step_avg:33.24ms
step:374/2160 train_time:12429ms step_avg:33.23ms
step:375/2160 train_time:12462ms step_avg:33.23ms
step:376/2160 train_time:12495ms step_avg:33.23ms
step:377/2160 train_time:12528ms step_avg:33.23ms
step:378/2160 train_time:12560ms step_avg:33.23ms
step:379/2160 train_time:12593ms step_avg:33.23ms
step:380/2160 train_time:12626ms step_avg:33.23ms
step:381/2160 train_time:12659ms step_avg:33.23ms
step:382/2160 train_time:12691ms step_avg:33.22ms
step:383/2160 train_time:12724ms step_avg:33.22ms
step:384/2160 train_time:12756ms step_avg:33.22ms
step:385/2160 train_time:12789ms step_avg:33.22ms
step:386/2160 train_time:12822ms step_avg:33.22ms
step:387/2160 train_time:12855ms step_avg:33.22ms
step:388/2160 train_time:12887ms step_avg:33.21ms
step:389/2160 train_time:12920ms step_avg:33.21ms
step:390/2160 train_time:12952ms step_avg:33.21ms
step:391/2160 train_time:12986ms step_avg:33.21ms
step:392/2160 train_time:13018ms step_avg:33.21ms
step:393/2160 train_time:13051ms step_avg:33.21ms
step:394/2160 train_time:13084ms step_avg:33.21ms
step:395/2160 train_time:13117ms step_avg:33.21ms
step:396/2160 train_time:13149ms step_avg:33.21ms
step:397/2160 train_time:13183ms step_avg:33.21ms
step:398/2160 train_time:13215ms step_avg:33.20ms
step:399/2160 train_time:13248ms step_avg:33.20ms
step:400/2160 train_time:13280ms step_avg:33.20ms
step:401/2160 train_time:13313ms step_avg:33.20ms
step:402/2160 train_time:13345ms step_avg:33.20ms
step:403/2160 train_time:13378ms step_avg:33.20ms
step:404/2160 train_time:13411ms step_avg:33.20ms
step:405/2160 train_time:13444ms step_avg:33.19ms
step:406/2160 train_time:13476ms step_avg:33.19ms
step:407/2160 train_time:13509ms step_avg:33.19ms
step:408/2160 train_time:13542ms step_avg:33.19ms
step:409/2160 train_time:13575ms step_avg:33.19ms
step:410/2160 train_time:13607ms step_avg:33.19ms
step:411/2160 train_time:13640ms step_avg:33.19ms
step:412/2160 train_time:13673ms step_avg:33.19ms
step:413/2160 train_time:13706ms step_avg:33.19ms
step:414/2160 train_time:13738ms step_avg:33.18ms
step:415/2160 train_time:13771ms step_avg:33.18ms
step:416/2160 train_time:13803ms step_avg:33.18ms
step:417/2160 train_time:13836ms step_avg:33.18ms
step:418/2160 train_time:13869ms step_avg:33.18ms
step:419/2160 train_time:13902ms step_avg:33.18ms
step:420/2160 train_time:13934ms step_avg:33.18ms
step:421/2160 train_time:13968ms step_avg:33.18ms
step:422/2160 train_time:14000ms step_avg:33.18ms
step:423/2160 train_time:14033ms step_avg:33.18ms
step:424/2160 train_time:14066ms step_avg:33.17ms
step:425/2160 train_time:14099ms step_avg:33.17ms
step:426/2160 train_time:14131ms step_avg:33.17ms
step:427/2160 train_time:14165ms step_avg:33.17ms
step:428/2160 train_time:14197ms step_avg:33.17ms
step:429/2160 train_time:14230ms step_avg:33.17ms
step:430/2160 train_time:14262ms step_avg:33.17ms
step:431/2160 train_time:14295ms step_avg:33.17ms
step:432/2160 train_time:14327ms step_avg:33.17ms
step:433/2160 train_time:14360ms step_avg:33.16ms
step:434/2160 train_time:14393ms step_avg:33.16ms
step:435/2160 train_time:14426ms step_avg:33.16ms
step:436/2160 train_time:14458ms step_avg:33.16ms
step:437/2160 train_time:14491ms step_avg:33.16ms
step:438/2160 train_time:14524ms step_avg:33.16ms
step:439/2160 train_time:14557ms step_avg:33.16ms
step:440/2160 train_time:14589ms step_avg:33.16ms
step:441/2160 train_time:14622ms step_avg:33.16ms
step:442/2160 train_time:14655ms step_avg:33.16ms
step:443/2160 train_time:14688ms step_avg:33.16ms
step:444/2160 train_time:14720ms step_avg:33.15ms
step:445/2160 train_time:14753ms step_avg:33.15ms
step:446/2160 train_time:14786ms step_avg:33.15ms
step:447/2160 train_time:14819ms step_avg:33.15ms
step:448/2160 train_time:14851ms step_avg:33.15ms
step:449/2160 train_time:14885ms step_avg:33.15ms
step:450/2160 train_time:14917ms step_avg:33.15ms
step:451/2160 train_time:14950ms step_avg:33.15ms
step:452/2160 train_time:14982ms step_avg:33.15ms
step:453/2160 train_time:15015ms step_avg:33.15ms
step:454/2160 train_time:15048ms step_avg:33.15ms
step:455/2160 train_time:15081ms step_avg:33.15ms
step:456/2160 train_time:15114ms step_avg:33.14ms
step:457/2160 train_time:15147ms step_avg:33.14ms
step:458/2160 train_time:15179ms step_avg:33.14ms
step:459/2160 train_time:15212ms step_avg:33.14ms
step:460/2160 train_time:15245ms step_avg:33.14ms
step:461/2160 train_time:15278ms step_avg:33.14ms
step:462/2160 train_time:15311ms step_avg:33.14ms
step:463/2160 train_time:15344ms step_avg:33.14ms
step:464/2160 train_time:15376ms step_avg:33.14ms
step:465/2160 train_time:15409ms step_avg:33.14ms
step:466/2160 train_time:15441ms step_avg:33.14ms
step:467/2160 train_time:15474ms step_avg:33.14ms
step:468/2160 train_time:15507ms step_avg:33.13ms
step:469/2160 train_time:15540ms step_avg:33.14ms
step:470/2160 train_time:15573ms step_avg:33.13ms
step:471/2160 train_time:15606ms step_avg:33.13ms
step:472/2160 train_time:15638ms step_avg:33.13ms
step:473/2160 train_time:15671ms step_avg:33.13ms
step:474/2160 train_time:15704ms step_avg:33.13ms
step:475/2160 train_time:15737ms step_avg:33.13ms
step:476/2160 train_time:15769ms step_avg:33.13ms
step:477/2160 train_time:15802ms step_avg:33.13ms
step:478/2160 train_time:15835ms step_avg:33.13ms
step:479/2160 train_time:15868ms step_avg:33.13ms
step:480/2160 train_time:15900ms step_avg:33.13ms
step:481/2160 train_time:15933ms step_avg:33.12ms
step:482/2160 train_time:15966ms step_avg:33.12ms
step:483/2160 train_time:15999ms step_avg:33.12ms
step:484/2160 train_time:16031ms step_avg:33.12ms
step:485/2160 train_time:16064ms step_avg:33.12ms
step:486/2160 train_time:16097ms step_avg:33.12ms
step:487/2160 train_time:16130ms step_avg:33.12ms
step:488/2160 train_time:16163ms step_avg:33.12ms
step:489/2160 train_time:16195ms step_avg:33.12ms
step:490/2160 train_time:16228ms step_avg:33.12ms
step:491/2160 train_time:16261ms step_avg:33.12ms
step:492/2160 train_time:16293ms step_avg:33.12ms
step:493/2160 train_time:16326ms step_avg:33.12ms
step:494/2160 train_time:16359ms step_avg:33.11ms
step:495/2160 train_time:16392ms step_avg:33.12ms
step:496/2160 train_time:16424ms step_avg:33.11ms
step:497/2160 train_time:16457ms step_avg:33.11ms
step:498/2160 train_time:16490ms step_avg:33.11ms
step:499/2160 train_time:16523ms step_avg:33.11ms
step:500/2160 train_time:16555ms step_avg:33.11ms
step:500/2160 val_loss:4.0207 train_time:16591ms step_avg:33.18ms
step:501/2160 train_time:16615ms step_avg:33.16ms
step:502/2160 train_time:16638ms step_avg:33.14ms
step:503/2160 train_time:16659ms step_avg:33.12ms
step:504/2160 train_time:16691ms step_avg:33.12ms
step:505/2160 train_time:16726ms step_avg:33.12ms
step:506/2160 train_time:16759ms step_avg:33.12ms
step:507/2160 train_time:16793ms step_avg:33.12ms
step:508/2160 train_time:16826ms step_avg:33.12ms
step:509/2160 train_time:16859ms step_avg:33.12ms
step:510/2160 train_time:16891ms step_avg:33.12ms
step:511/2160 train_time:16924ms step_avg:33.12ms
step:512/2160 train_time:16956ms step_avg:33.12ms
step:513/2160 train_time:16989ms step_avg:33.12ms
step:514/2160 train_time:17021ms step_avg:33.11ms
step:515/2160 train_time:17054ms step_avg:33.11ms
step:516/2160 train_time:17086ms step_avg:33.11ms
step:517/2160 train_time:17119ms step_avg:33.11ms
step:518/2160 train_time:17151ms step_avg:33.11ms
step:519/2160 train_time:17184ms step_avg:33.11ms
step:520/2160 train_time:17216ms step_avg:33.11ms
step:521/2160 train_time:17249ms step_avg:33.11ms
step:522/2160 train_time:17281ms step_avg:33.10ms
step:523/2160 train_time:17314ms step_avg:33.10ms
step:524/2160 train_time:17346ms step_avg:33.10ms
step:525/2160 train_time:17379ms step_avg:33.10ms
step:526/2160 train_time:17411ms step_avg:33.10ms
step:527/2160 train_time:17444ms step_avg:33.10ms
step:528/2160 train_time:17476ms step_avg:33.10ms
step:529/2160 train_time:17509ms step_avg:33.10ms
step:530/2160 train_time:17541ms step_avg:33.10ms
step:531/2160 train_time:17575ms step_avg:33.10ms
step:532/2160 train_time:17607ms step_avg:33.10ms
step:533/2160 train_time:17641ms step_avg:33.10ms
step:534/2160 train_time:17674ms step_avg:33.10ms
step:535/2160 train_time:17708ms step_avg:33.10ms
step:536/2160 train_time:17741ms step_avg:33.10ms
step:537/2160 train_time:17774ms step_avg:33.10ms
step:538/2160 train_time:17806ms step_avg:33.10ms
step:539/2160 train_time:17840ms step_avg:33.10ms
step:540/2160 train_time:17872ms step_avg:33.10ms
step:541/2160 train_time:17906ms step_avg:33.10ms
step:542/2160 train_time:17938ms step_avg:33.10ms
step:543/2160 train_time:17971ms step_avg:33.10ms
step:544/2160 train_time:18003ms step_avg:33.09ms
step:545/2160 train_time:18037ms step_avg:33.09ms
step:546/2160 train_time:18069ms step_avg:33.09ms
step:547/2160 train_time:18102ms step_avg:33.09ms
step:548/2160 train_time:18134ms step_avg:33.09ms
step:549/2160 train_time:18167ms step_avg:33.09ms
step:550/2160 train_time:18199ms step_avg:33.09ms
step:551/2160 train_time:18232ms step_avg:33.09ms
step:552/2160 train_time:18264ms step_avg:33.09ms
step:553/2160 train_time:18297ms step_avg:33.09ms
step:554/2160 train_time:18329ms step_avg:33.09ms
step:555/2160 train_time:18362ms step_avg:33.09ms
step:556/2160 train_time:18395ms step_avg:33.08ms
step:557/2160 train_time:18428ms step_avg:33.08ms
step:558/2160 train_time:18460ms step_avg:33.08ms
step:559/2160 train_time:18493ms step_avg:33.08ms
step:560/2160 train_time:18525ms step_avg:33.08ms
step:561/2160 train_time:18558ms step_avg:33.08ms
step:562/2160 train_time:18591ms step_avg:33.08ms
step:563/2160 train_time:18625ms step_avg:33.08ms
step:564/2160 train_time:18657ms step_avg:33.08ms
step:565/2160 train_time:18691ms step_avg:33.08ms
step:566/2160 train_time:18723ms step_avg:33.08ms
step:567/2160 train_time:18757ms step_avg:33.08ms
step:568/2160 train_time:18789ms step_avg:33.08ms
step:569/2160 train_time:18823ms step_avg:33.08ms
step:570/2160 train_time:18855ms step_avg:33.08ms
step:571/2160 train_time:18888ms step_avg:33.08ms
step:572/2160 train_time:18920ms step_avg:33.08ms
step:573/2160 train_time:18953ms step_avg:33.08ms
step:574/2160 train_time:18986ms step_avg:33.08ms
step:575/2160 train_time:19019ms step_avg:33.08ms
step:576/2160 train_time:19052ms step_avg:33.08ms
step:577/2160 train_time:19085ms step_avg:33.08ms
step:578/2160 train_time:19117ms step_avg:33.07ms
step:579/2160 train_time:19150ms step_avg:33.07ms
step:580/2160 train_time:19183ms step_avg:33.07ms
step:581/2160 train_time:19216ms step_avg:33.07ms
step:582/2160 train_time:19248ms step_avg:33.07ms
step:583/2160 train_time:19281ms step_avg:33.07ms
step:584/2160 train_time:19313ms step_avg:33.07ms
step:585/2160 train_time:19346ms step_avg:33.07ms
step:586/2160 train_time:19379ms step_avg:33.07ms
step:587/2160 train_time:19412ms step_avg:33.07ms
step:588/2160 train_time:19444ms step_avg:33.07ms
step:589/2160 train_time:19477ms step_avg:33.07ms
step:590/2160 train_time:19509ms step_avg:33.07ms
step:591/2160 train_time:19543ms step_avg:33.07ms
step:592/2160 train_time:19575ms step_avg:33.07ms
step:593/2160 train_time:19608ms step_avg:33.07ms
step:594/2160 train_time:19640ms step_avg:33.06ms
step:595/2160 train_time:19673ms step_avg:33.06ms
step:596/2160 train_time:19706ms step_avg:33.06ms
step:597/2160 train_time:19739ms step_avg:33.06ms
step:598/2160 train_time:19772ms step_avg:33.06ms
step:599/2160 train_time:19805ms step_avg:33.06ms
step:600/2160 train_time:19837ms step_avg:33.06ms
step:601/2160 train_time:19871ms step_avg:33.06ms
step:602/2160 train_time:19903ms step_avg:33.06ms
step:603/2160 train_time:19936ms step_avg:33.06ms
step:604/2160 train_time:19969ms step_avg:33.06ms
step:605/2160 train_time:20002ms step_avg:33.06ms
step:606/2160 train_time:20034ms step_avg:33.06ms
step:607/2160 train_time:20067ms step_avg:33.06ms
step:608/2160 train_time:20100ms step_avg:33.06ms
step:609/2160 train_time:20133ms step_avg:33.06ms
step:610/2160 train_time:20165ms step_avg:33.06ms
step:611/2160 train_time:20198ms step_avg:33.06ms
step:612/2160 train_time:20231ms step_avg:33.06ms
step:613/2160 train_time:20264ms step_avg:33.06ms
step:614/2160 train_time:20296ms step_avg:33.06ms
step:615/2160 train_time:20329ms step_avg:33.06ms
step:616/2160 train_time:20361ms step_avg:33.05ms
step:617/2160 train_time:20394ms step_avg:33.05ms
step:618/2160 train_time:20427ms step_avg:33.05ms
step:619/2160 train_time:20460ms step_avg:33.05ms
step:620/2160 train_time:20492ms step_avg:33.05ms
step:621/2160 train_time:20526ms step_avg:33.05ms
step:622/2160 train_time:20558ms step_avg:33.05ms
step:623/2160 train_time:20592ms step_avg:33.05ms
step:624/2160 train_time:20624ms step_avg:33.05ms
step:625/2160 train_time:20657ms step_avg:33.05ms
step:626/2160 train_time:20690ms step_avg:33.05ms
step:627/2160 train_time:20723ms step_avg:33.05ms
step:628/2160 train_time:20755ms step_avg:33.05ms
step:629/2160 train_time:20789ms step_avg:33.05ms
step:630/2160 train_time:20821ms step_avg:33.05ms
step:631/2160 train_time:20854ms step_avg:33.05ms
step:632/2160 train_time:20887ms step_avg:33.05ms
step:633/2160 train_time:20920ms step_avg:33.05ms
step:634/2160 train_time:20952ms step_avg:33.05ms
step:635/2160 train_time:20986ms step_avg:33.05ms
step:636/2160 train_time:21018ms step_avg:33.05ms
step:637/2160 train_time:21051ms step_avg:33.05ms
step:638/2160 train_time:21083ms step_avg:33.05ms
step:639/2160 train_time:21117ms step_avg:33.05ms
step:640/2160 train_time:21149ms step_avg:33.05ms
step:641/2160 train_time:21182ms step_avg:33.05ms
step:642/2160 train_time:21215ms step_avg:33.04ms
step:643/2160 train_time:21248ms step_avg:33.04ms
step:644/2160 train_time:21280ms step_avg:33.04ms
step:645/2160 train_time:21313ms step_avg:33.04ms
step:646/2160 train_time:21345ms step_avg:33.04ms
step:647/2160 train_time:21378ms step_avg:33.04ms
step:648/2160 train_time:21411ms step_avg:33.04ms
step:649/2160 train_time:21444ms step_avg:33.04ms
step:650/2160 train_time:21476ms step_avg:33.04ms
step:651/2160 train_time:21509ms step_avg:33.04ms
step:652/2160 train_time:21542ms step_avg:33.04ms
step:653/2160 train_time:21575ms step_avg:33.04ms
step:654/2160 train_time:21607ms step_avg:33.04ms
step:655/2160 train_time:21641ms step_avg:33.04ms
step:656/2160 train_time:21673ms step_avg:33.04ms
step:657/2160 train_time:21706ms step_avg:33.04ms
step:658/2160 train_time:21738ms step_avg:33.04ms
step:659/2160 train_time:21771ms step_avg:33.04ms
step:660/2160 train_time:21804ms step_avg:33.04ms
step:661/2160 train_time:21837ms step_avg:33.04ms
step:662/2160 train_time:21870ms step_avg:33.04ms
step:663/2160 train_time:21903ms step_avg:33.04ms
step:664/2160 train_time:21935ms step_avg:33.03ms
step:665/2160 train_time:21968ms step_avg:33.04ms
step:666/2160 train_time:22001ms step_avg:33.03ms
step:667/2160 train_time:22034ms step_avg:33.03ms
step:668/2160 train_time:22066ms step_avg:33.03ms
step:669/2160 train_time:22100ms step_avg:33.03ms
step:670/2160 train_time:22132ms step_avg:33.03ms
step:671/2160 train_time:22166ms step_avg:33.03ms
step:672/2160 train_time:22198ms step_avg:33.03ms
step:673/2160 train_time:22231ms step_avg:33.03ms
step:674/2160 train_time:22263ms step_avg:33.03ms
step:675/2160 train_time:22296ms step_avg:33.03ms
step:676/2160 train_time:22329ms step_avg:33.03ms
step:677/2160 train_time:22362ms step_avg:33.03ms
step:678/2160 train_time:22394ms step_avg:33.03ms
step:679/2160 train_time:22427ms step_avg:33.03ms
step:680/2160 train_time:22460ms step_avg:33.03ms
step:681/2160 train_time:22492ms step_avg:33.03ms
step:682/2160 train_time:22525ms step_avg:33.03ms
step:683/2160 train_time:22558ms step_avg:33.03ms
step:684/2160 train_time:22591ms step_avg:33.03ms
step:685/2160 train_time:22624ms step_avg:33.03ms
step:686/2160 train_time:22657ms step_avg:33.03ms
step:687/2160 train_time:22690ms step_avg:33.03ms
step:688/2160 train_time:22723ms step_avg:33.03ms
step:689/2160 train_time:22756ms step_avg:33.03ms
step:690/2160 train_time:22788ms step_avg:33.03ms
step:691/2160 train_time:22822ms step_avg:33.03ms
step:692/2160 train_time:22854ms step_avg:33.03ms
step:693/2160 train_time:22887ms step_avg:33.03ms
step:694/2160 train_time:22920ms step_avg:33.03ms
step:695/2160 train_time:22953ms step_avg:33.03ms
step:696/2160 train_time:22985ms step_avg:33.02ms
step:697/2160 train_time:23018ms step_avg:33.02ms
step:698/2160 train_time:23051ms step_avg:33.02ms
step:699/2160 train_time:23084ms step_avg:33.02ms
step:700/2160 train_time:23116ms step_avg:33.02ms
step:701/2160 train_time:23149ms step_avg:33.02ms
step:702/2160 train_time:23182ms step_avg:33.02ms
step:703/2160 train_time:23215ms step_avg:33.02ms
step:704/2160 train_time:23247ms step_avg:33.02ms
step:705/2160 train_time:23280ms step_avg:33.02ms
step:706/2160 train_time:23312ms step_avg:33.02ms
step:707/2160 train_time:23346ms step_avg:33.02ms
step:708/2160 train_time:23379ms step_avg:33.02ms
step:709/2160 train_time:23437ms step_avg:33.06ms
step:710/2160 train_time:23495ms step_avg:33.09ms
step:711/2160 train_time:23555ms step_avg:33.13ms
step:712/2160 train_time:23614ms step_avg:33.17ms
step:713/2160 train_time:23674ms step_avg:33.20ms
step:714/2160 train_time:23733ms step_avg:33.24ms
step:715/2160 train_time:23794ms step_avg:33.28ms
step:716/2160 train_time:23854ms step_avg:33.32ms
step:717/2160 train_time:23914ms step_avg:33.35ms
step:718/2160 train_time:23973ms step_avg:33.39ms
step:719/2160 train_time:24033ms step_avg:33.43ms
step:720/2160 train_time:24092ms step_avg:33.46ms
step:721/2160 train_time:24153ms step_avg:33.50ms
step:722/2160 train_time:24212ms step_avg:33.53ms
step:723/2160 train_time:24272ms step_avg:33.57ms
step:724/2160 train_time:24331ms step_avg:33.61ms
step:725/2160 train_time:24390ms step_avg:33.64ms
step:726/2160 train_time:24449ms step_avg:33.68ms
step:727/2160 train_time:24510ms step_avg:33.71ms
step:728/2160 train_time:24569ms step_avg:33.75ms
step:729/2160 train_time:24629ms step_avg:33.78ms
step:730/2160 train_time:24687ms step_avg:33.82ms
step:731/2160 train_time:24748ms step_avg:33.86ms
step:732/2160 train_time:24807ms step_avg:33.89ms
step:733/2160 train_time:24868ms step_avg:33.93ms
step:734/2160 train_time:24927ms step_avg:33.96ms
step:735/2160 train_time:24988ms step_avg:34.00ms
step:736/2160 train_time:25047ms step_avg:34.03ms
step:737/2160 train_time:25108ms step_avg:34.07ms
step:738/2160 train_time:25167ms step_avg:34.10ms
step:739/2160 train_time:25227ms step_avg:34.14ms
step:740/2160 train_time:25287ms step_avg:34.17ms
step:741/2160 train_time:25346ms step_avg:34.21ms
step:742/2160 train_time:25405ms step_avg:34.24ms
step:743/2160 train_time:25465ms step_avg:34.27ms
step:744/2160 train_time:25524ms step_avg:34.31ms
step:745/2160 train_time:25584ms step_avg:34.34ms
step:746/2160 train_time:25643ms step_avg:34.37ms
step:747/2160 train_time:25703ms step_avg:34.41ms
step:748/2160 train_time:25762ms step_avg:34.44ms
step:749/2160 train_time:25823ms step_avg:34.48ms
step:750/2160 train_time:25881ms step_avg:34.51ms
step:750/2160 val_loss:3.8656 train_time:25944ms step_avg:34.59ms
step:751/2160 train_time:25968ms step_avg:34.58ms
step:752/2160 train_time:26002ms step_avg:34.58ms
step:753/2160 train_time:26064ms step_avg:34.61ms
step:754/2160 train_time:26126ms step_avg:34.65ms
step:755/2160 train_time:26187ms step_avg:34.68ms
step:756/2160 train_time:26245ms step_avg:34.72ms
step:757/2160 train_time:26305ms step_avg:34.75ms
step:758/2160 train_time:26363ms step_avg:34.78ms
step:759/2160 train_time:26423ms step_avg:34.81ms
step:760/2160 train_time:26481ms step_avg:34.84ms
step:761/2160 train_time:26540ms step_avg:34.88ms
step:762/2160 train_time:26598ms step_avg:34.91ms
step:763/2160 train_time:26657ms step_avg:34.94ms
step:764/2160 train_time:26715ms step_avg:34.97ms
step:765/2160 train_time:26774ms step_avg:35.00ms
step:766/2160 train_time:26833ms step_avg:35.03ms
step:767/2160 train_time:26894ms step_avg:35.06ms
step:768/2160 train_time:26954ms step_avg:35.10ms
step:769/2160 train_time:27017ms step_avg:35.13ms
step:770/2160 train_time:27078ms step_avg:35.17ms
step:771/2160 train_time:27139ms step_avg:35.20ms
step:772/2160 train_time:27199ms step_avg:35.23ms
step:773/2160 train_time:27259ms step_avg:35.26ms
step:774/2160 train_time:27318ms step_avg:35.29ms
step:775/2160 train_time:27378ms step_avg:35.33ms
step:776/2160 train_time:27437ms step_avg:35.36ms
step:777/2160 train_time:27497ms step_avg:35.39ms
step:778/2160 train_time:27555ms step_avg:35.42ms
step:779/2160 train_time:27615ms step_avg:35.45ms
step:780/2160 train_time:27673ms step_avg:35.48ms
step:781/2160 train_time:27732ms step_avg:35.51ms
step:782/2160 train_time:27791ms step_avg:35.54ms
step:783/2160 train_time:27852ms step_avg:35.57ms
step:784/2160 train_time:27911ms step_avg:35.60ms
step:785/2160 train_time:27974ms step_avg:35.64ms
step:786/2160 train_time:28034ms step_avg:35.67ms
step:787/2160 train_time:28097ms step_avg:35.70ms
step:788/2160 train_time:28156ms step_avg:35.73ms
step:789/2160 train_time:28218ms step_avg:35.76ms
step:790/2160 train_time:28277ms step_avg:35.79ms
step:791/2160 train_time:28337ms step_avg:35.82ms
step:792/2160 train_time:28397ms step_avg:35.85ms
step:793/2160 train_time:28456ms step_avg:35.88ms
step:794/2160 train_time:28515ms step_avg:35.91ms
step:795/2160 train_time:28574ms step_avg:35.94ms
step:796/2160 train_time:28633ms step_avg:35.97ms
step:797/2160 train_time:28692ms step_avg:36.00ms
step:798/2160 train_time:28751ms step_avg:36.03ms
step:799/2160 train_time:28810ms step_avg:36.06ms
step:800/2160 train_time:28869ms step_avg:36.09ms
step:801/2160 train_time:28930ms step_avg:36.12ms
step:802/2160 train_time:28990ms step_avg:36.15ms
step:803/2160 train_time:29052ms step_avg:36.18ms
step:804/2160 train_time:29112ms step_avg:36.21ms
step:805/2160 train_time:29174ms step_avg:36.24ms
step:806/2160 train_time:29234ms step_avg:36.27ms
step:807/2160 train_time:29296ms step_avg:36.30ms
step:808/2160 train_time:29355ms step_avg:36.33ms
step:809/2160 train_time:29415ms step_avg:36.36ms
step:810/2160 train_time:29474ms step_avg:36.39ms
step:811/2160 train_time:29534ms step_avg:36.42ms
step:812/2160 train_time:29594ms step_avg:36.45ms
step:813/2160 train_time:29653ms step_avg:36.47ms
step:814/2160 train_time:29712ms step_avg:36.50ms
step:815/2160 train_time:29772ms step_avg:36.53ms
step:816/2160 train_time:29831ms step_avg:36.56ms
step:817/2160 train_time:29891ms step_avg:36.59ms
step:818/2160 train_time:29950ms step_avg:36.61ms
step:819/2160 train_time:30012ms step_avg:36.64ms
step:820/2160 train_time:30072ms step_avg:36.67ms
step:821/2160 train_time:30133ms step_avg:36.70ms
step:822/2160 train_time:30193ms step_avg:36.73ms
step:823/2160 train_time:30254ms step_avg:36.76ms
step:824/2160 train_time:30314ms step_avg:36.79ms
step:825/2160 train_time:30374ms step_avg:36.82ms
step:826/2160 train_time:30434ms step_avg:36.84ms
step:827/2160 train_time:30494ms step_avg:36.87ms
step:828/2160 train_time:30553ms step_avg:36.90ms
step:829/2160 train_time:30613ms step_avg:36.93ms
step:830/2160 train_time:30672ms step_avg:36.95ms
step:831/2160 train_time:30732ms step_avg:36.98ms
step:832/2160 train_time:30791ms step_avg:37.01ms
step:833/2160 train_time:30851ms step_avg:37.04ms
step:834/2160 train_time:30909ms step_avg:37.06ms
step:835/2160 train_time:30970ms step_avg:37.09ms
step:836/2160 train_time:31030ms step_avg:37.12ms
step:837/2160 train_time:31092ms step_avg:37.15ms
step:838/2160 train_time:31151ms step_avg:37.17ms
step:839/2160 train_time:31212ms step_avg:37.20ms
step:840/2160 train_time:31271ms step_avg:37.23ms
step:841/2160 train_time:31332ms step_avg:37.26ms
step:842/2160 train_time:31392ms step_avg:37.28ms
step:843/2160 train_time:31453ms step_avg:37.31ms
step:844/2160 train_time:31512ms step_avg:37.34ms
step:845/2160 train_time:31572ms step_avg:37.36ms
step:846/2160 train_time:31631ms step_avg:37.39ms
step:847/2160 train_time:31691ms step_avg:37.42ms
step:848/2160 train_time:31749ms step_avg:37.44ms
step:849/2160 train_time:31810ms step_avg:37.47ms
step:850/2160 train_time:31869ms step_avg:37.49ms
step:851/2160 train_time:31929ms step_avg:37.52ms
step:852/2160 train_time:31988ms step_avg:37.55ms
step:853/2160 train_time:32049ms step_avg:37.57ms
step:854/2160 train_time:32109ms step_avg:37.60ms
step:855/2160 train_time:32170ms step_avg:37.63ms
step:856/2160 train_time:32230ms step_avg:37.65ms
step:857/2160 train_time:32291ms step_avg:37.68ms
step:858/2160 train_time:32350ms step_avg:37.70ms
step:859/2160 train_time:32411ms step_avg:37.73ms
step:860/2160 train_time:32471ms step_avg:37.76ms
step:861/2160 train_time:32532ms step_avg:37.78ms
step:862/2160 train_time:32590ms step_avg:37.81ms
step:863/2160 train_time:32650ms step_avg:37.83ms
step:864/2160 train_time:32709ms step_avg:37.86ms
step:865/2160 train_time:32770ms step_avg:37.88ms
step:866/2160 train_time:32829ms step_avg:37.91ms
step:867/2160 train_time:32889ms step_avg:37.93ms
step:868/2160 train_time:32948ms step_avg:37.96ms
step:869/2160 train_time:33008ms step_avg:37.98ms
step:870/2160 train_time:33067ms step_avg:38.01ms
step:871/2160 train_time:33128ms step_avg:38.03ms
step:872/2160 train_time:33187ms step_avg:38.06ms
step:873/2160 train_time:33248ms step_avg:38.09ms
step:874/2160 train_time:33307ms step_avg:38.11ms
step:875/2160 train_time:33367ms step_avg:38.13ms
step:876/2160 train_time:33427ms step_avg:38.16ms
step:877/2160 train_time:33488ms step_avg:38.18ms
step:878/2160 train_time:33546ms step_avg:38.21ms
step:879/2160 train_time:33607ms step_avg:38.23ms
step:880/2160 train_time:33666ms step_avg:38.26ms
step:881/2160 train_time:33726ms step_avg:38.28ms
step:882/2160 train_time:33784ms step_avg:38.30ms
step:883/2160 train_time:33844ms step_avg:38.33ms
step:884/2160 train_time:33902ms step_avg:38.35ms
step:885/2160 train_time:33962ms step_avg:38.37ms
step:886/2160 train_time:34020ms step_avg:38.40ms
step:887/2160 train_time:34080ms step_avg:38.42ms
step:888/2160 train_time:34139ms step_avg:38.44ms
step:889/2160 train_time:34199ms step_avg:38.47ms
step:890/2160 train_time:34258ms step_avg:38.49ms
step:891/2160 train_time:34318ms step_avg:38.52ms
step:892/2160 train_time:34377ms step_avg:38.54ms
step:893/2160 train_time:34437ms step_avg:38.56ms
step:894/2160 train_time:34497ms step_avg:38.59ms
step:895/2160 train_time:34558ms step_avg:38.61ms
step:896/2160 train_time:34616ms step_avg:38.63ms
step:897/2160 train_time:34677ms step_avg:38.66ms
step:898/2160 train_time:34736ms step_avg:38.68ms
step:899/2160 train_time:34796ms step_avg:38.71ms
step:900/2160 train_time:34855ms step_avg:38.73ms
step:901/2160 train_time:34915ms step_avg:38.75ms
step:902/2160 train_time:34974ms step_avg:38.77ms
step:903/2160 train_time:35035ms step_avg:38.80ms
step:904/2160 train_time:35094ms step_avg:38.82ms
step:905/2160 train_time:35155ms step_avg:38.84ms
step:906/2160 train_time:35214ms step_avg:38.87ms
step:907/2160 train_time:35275ms step_avg:38.89ms
step:908/2160 train_time:35334ms step_avg:38.91ms
step:909/2160 train_time:35395ms step_avg:38.94ms
step:910/2160 train_time:35454ms step_avg:38.96ms
step:911/2160 train_time:35515ms step_avg:38.98ms
step:912/2160 train_time:35575ms step_avg:39.01ms
step:913/2160 train_time:35636ms step_avg:39.03ms
step:914/2160 train_time:35695ms step_avg:39.05ms
step:915/2160 train_time:35755ms step_avg:39.08ms
step:916/2160 train_time:35815ms step_avg:39.10ms
step:917/2160 train_time:35876ms step_avg:39.12ms
step:918/2160 train_time:35935ms step_avg:39.14ms
step:919/2160 train_time:35995ms step_avg:39.17ms
step:920/2160 train_time:36055ms step_avg:39.19ms
step:921/2160 train_time:36115ms step_avg:39.21ms
step:922/2160 train_time:36175ms step_avg:39.24ms
step:923/2160 train_time:36236ms step_avg:39.26ms
step:924/2160 train_time:36295ms step_avg:39.28ms
step:925/2160 train_time:36355ms step_avg:39.30ms
step:926/2160 train_time:36414ms step_avg:39.32ms
step:927/2160 train_time:36475ms step_avg:39.35ms
step:928/2160 train_time:36535ms step_avg:39.37ms
step:929/2160 train_time:36595ms step_avg:39.39ms
step:930/2160 train_time:36655ms step_avg:39.41ms
step:931/2160 train_time:36715ms step_avg:39.44ms
step:932/2160 train_time:36775ms step_avg:39.46ms
step:933/2160 train_time:36836ms step_avg:39.48ms
step:934/2160 train_time:36896ms step_avg:39.50ms
step:935/2160 train_time:36956ms step_avg:39.52ms
step:936/2160 train_time:37015ms step_avg:39.55ms
step:937/2160 train_time:37075ms step_avg:39.57ms
step:938/2160 train_time:37135ms step_avg:39.59ms
step:939/2160 train_time:37195ms step_avg:39.61ms
step:940/2160 train_time:37254ms step_avg:39.63ms
step:941/2160 train_time:37315ms step_avg:39.65ms
step:942/2160 train_time:37374ms step_avg:39.68ms
step:943/2160 train_time:37436ms step_avg:39.70ms
step:944/2160 train_time:37495ms step_avg:39.72ms
step:945/2160 train_time:37556ms step_avg:39.74ms
step:946/2160 train_time:37615ms step_avg:39.76ms
step:947/2160 train_time:37676ms step_avg:39.78ms
step:948/2160 train_time:37736ms step_avg:39.81ms
step:949/2160 train_time:37797ms step_avg:39.83ms
step:950/2160 train_time:37856ms step_avg:39.85ms
step:951/2160 train_time:37916ms step_avg:39.87ms
step:952/2160 train_time:37976ms step_avg:39.89ms
step:953/2160 train_time:38037ms step_avg:39.91ms
step:954/2160 train_time:38096ms step_avg:39.93ms
step:955/2160 train_time:38157ms step_avg:39.95ms
step:956/2160 train_time:38216ms step_avg:39.97ms
step:957/2160 train_time:38276ms step_avg:40.00ms
step:958/2160 train_time:38335ms step_avg:40.02ms
step:959/2160 train_time:38395ms step_avg:40.04ms
step:960/2160 train_time:38455ms step_avg:40.06ms
step:961/2160 train_time:38516ms step_avg:40.08ms
step:962/2160 train_time:38575ms step_avg:40.10ms
step:963/2160 train_time:38636ms step_avg:40.12ms
step:964/2160 train_time:38695ms step_avg:40.14ms
step:965/2160 train_time:38755ms step_avg:40.16ms
step:966/2160 train_time:38814ms step_avg:40.18ms
step:967/2160 train_time:38877ms step_avg:40.20ms
step:968/2160 train_time:38936ms step_avg:40.22ms
step:969/2160 train_time:38997ms step_avg:40.24ms
step:970/2160 train_time:39056ms step_avg:40.26ms
step:971/2160 train_time:39116ms step_avg:40.28ms
step:972/2160 train_time:39175ms step_avg:40.30ms
step:973/2160 train_time:39236ms step_avg:40.32ms
step:974/2160 train_time:39296ms step_avg:40.35ms
step:975/2160 train_time:39356ms step_avg:40.36ms
step:976/2160 train_time:39415ms step_avg:40.38ms
step:977/2160 train_time:39475ms step_avg:40.40ms
step:978/2160 train_time:39534ms step_avg:40.42ms
step:979/2160 train_time:39596ms step_avg:40.45ms
step:980/2160 train_time:39655ms step_avg:40.46ms
step:981/2160 train_time:39715ms step_avg:40.48ms
step:982/2160 train_time:39775ms step_avg:40.50ms
step:983/2160 train_time:39836ms step_avg:40.53ms
step:984/2160 train_time:39896ms step_avg:40.54ms
step:985/2160 train_time:39956ms step_avg:40.56ms
step:986/2160 train_time:40016ms step_avg:40.58ms
step:987/2160 train_time:40077ms step_avg:40.60ms
step:988/2160 train_time:40136ms step_avg:40.62ms
step:989/2160 train_time:40196ms step_avg:40.64ms
step:990/2160 train_time:40255ms step_avg:40.66ms
step:991/2160 train_time:40316ms step_avg:40.68ms
step:992/2160 train_time:40376ms step_avg:40.70ms
step:993/2160 train_time:40436ms step_avg:40.72ms
step:994/2160 train_time:40496ms step_avg:40.74ms
step:995/2160 train_time:40556ms step_avg:40.76ms
step:996/2160 train_time:40615ms step_avg:40.78ms
step:997/2160 train_time:40676ms step_avg:40.80ms
step:998/2160 train_time:40736ms step_avg:40.82ms
step:999/2160 train_time:40797ms step_avg:40.84ms
step:1000/2160 train_time:40856ms step_avg:40.86ms
step:1000/2160 val_loss:3.7096 train_time:40918ms step_avg:40.92ms
step:1001/2160 train_time:40942ms step_avg:40.90ms
step:1002/2160 train_time:40977ms step_avg:40.89ms
step:1003/2160 train_time:41042ms step_avg:40.92ms
step:1004/2160 train_time:41104ms step_avg:40.94ms
step:1005/2160 train_time:41164ms step_avg:40.96ms
step:1006/2160 train_time:41223ms step_avg:40.98ms
step:1007/2160 train_time:41282ms step_avg:41.00ms
step:1008/2160 train_time:41340ms step_avg:41.01ms
step:1009/2160 train_time:41399ms step_avg:41.03ms
step:1010/2160 train_time:41457ms step_avg:41.05ms
step:1011/2160 train_time:41517ms step_avg:41.06ms
step:1012/2160 train_time:41575ms step_avg:41.08ms
step:1013/2160 train_time:41634ms step_avg:41.10ms
step:1014/2160 train_time:41693ms step_avg:41.12ms
step:1015/2160 train_time:41753ms step_avg:41.14ms
step:1016/2160 train_time:41812ms step_avg:41.15ms
step:1017/2160 train_time:41875ms step_avg:41.17ms
step:1018/2160 train_time:41936ms step_avg:41.19ms
step:1019/2160 train_time:41999ms step_avg:41.22ms
step:1020/2160 train_time:42059ms step_avg:41.23ms
step:1021/2160 train_time:42120ms step_avg:41.25ms
step:1022/2160 train_time:42179ms step_avg:41.27ms
step:1023/2160 train_time:42240ms step_avg:41.29ms
step:1024/2160 train_time:42298ms step_avg:41.31ms
step:1025/2160 train_time:42358ms step_avg:41.32ms
step:1026/2160 train_time:42416ms step_avg:41.34ms
step:1027/2160 train_time:42476ms step_avg:41.36ms
step:1028/2160 train_time:42535ms step_avg:41.38ms
step:1029/2160 train_time:42594ms step_avg:41.39ms
step:1030/2160 train_time:42652ms step_avg:41.41ms
step:1031/2160 train_time:42712ms step_avg:41.43ms
step:1032/2160 train_time:42771ms step_avg:41.44ms
step:1033/2160 train_time:42832ms step_avg:41.46ms
step:1034/2160 train_time:42892ms step_avg:41.48ms
step:1035/2160 train_time:42957ms step_avg:41.50ms
step:1036/2160 train_time:43018ms step_avg:41.52ms
step:1037/2160 train_time:43079ms step_avg:41.54ms
step:1038/2160 train_time:43138ms step_avg:41.56ms
step:1039/2160 train_time:43199ms step_avg:41.58ms
step:1040/2160 train_time:43258ms step_avg:41.59ms
step:1041/2160 train_time:43318ms step_avg:41.61ms
step:1042/2160 train_time:43376ms step_avg:41.63ms
step:1043/2160 train_time:43436ms step_avg:41.65ms
step:1044/2160 train_time:43494ms step_avg:41.66ms
step:1045/2160 train_time:43554ms step_avg:41.68ms
step:1046/2160 train_time:43613ms step_avg:41.70ms
step:1047/2160 train_time:43673ms step_avg:41.71ms
step:1048/2160 train_time:43732ms step_avg:41.73ms
step:1049/2160 train_time:43792ms step_avg:41.75ms
step:1050/2160 train_time:43852ms step_avg:41.76ms
step:1051/2160 train_time:43915ms step_avg:41.78ms
step:1052/2160 train_time:43976ms step_avg:41.80ms
step:1053/2160 train_time:44038ms step_avg:41.82ms
step:1054/2160 train_time:44097ms step_avg:41.84ms
step:1055/2160 train_time:44158ms step_avg:41.86ms
step:1056/2160 train_time:44217ms step_avg:41.87ms
step:1057/2160 train_time:44277ms step_avg:41.89ms
step:1058/2160 train_time:44336ms step_avg:41.91ms
step:1059/2160 train_time:44396ms step_avg:41.92ms
step:1060/2160 train_time:44456ms step_avg:41.94ms
step:1061/2160 train_time:44516ms step_avg:41.96ms
step:1062/2160 train_time:44574ms step_avg:41.97ms
step:1063/2160 train_time:44634ms step_avg:41.99ms
step:1064/2160 train_time:44693ms step_avg:42.00ms
step:1065/2160 train_time:44754ms step_avg:42.02ms
step:1066/2160 train_time:44814ms step_avg:42.04ms
step:1067/2160 train_time:44875ms step_avg:42.06ms
step:1068/2160 train_time:44936ms step_avg:42.07ms
step:1069/2160 train_time:44997ms step_avg:42.09ms
step:1070/2160 train_time:45056ms step_avg:42.11ms
step:1071/2160 train_time:45117ms step_avg:42.13ms
step:1072/2160 train_time:45177ms step_avg:42.14ms
step:1073/2160 train_time:45237ms step_avg:42.16ms
step:1074/2160 train_time:45296ms step_avg:42.18ms
step:1075/2160 train_time:45357ms step_avg:42.19ms
step:1076/2160 train_time:45416ms step_avg:42.21ms
step:1077/2160 train_time:45477ms step_avg:42.23ms
step:1078/2160 train_time:45536ms step_avg:42.24ms
step:1079/2160 train_time:45595ms step_avg:42.26ms
step:1080/2160 train_time:45654ms step_avg:42.27ms
step:1081/2160 train_time:45716ms step_avg:42.29ms
step:1082/2160 train_time:45775ms step_avg:42.31ms
step:1083/2160 train_time:45836ms step_avg:42.32ms
step:1084/2160 train_time:45896ms step_avg:42.34ms
step:1085/2160 train_time:45957ms step_avg:42.36ms
step:1086/2160 train_time:46017ms step_avg:42.37ms
step:1087/2160 train_time:46078ms step_avg:42.39ms
step:1088/2160 train_time:46137ms step_avg:42.41ms
step:1089/2160 train_time:46197ms step_avg:42.42ms
step:1090/2160 train_time:46256ms step_avg:42.44ms
step:1091/2160 train_time:46317ms step_avg:42.45ms
step:1092/2160 train_time:46376ms step_avg:42.47ms
step:1093/2160 train_time:46436ms step_avg:42.48ms
step:1094/2160 train_time:46494ms step_avg:42.50ms
step:1095/2160 train_time:46554ms step_avg:42.52ms
step:1096/2160 train_time:46613ms step_avg:42.53ms
step:1097/2160 train_time:46674ms step_avg:42.55ms
step:1098/2160 train_time:46733ms step_avg:42.56ms
step:1099/2160 train_time:46794ms step_avg:42.58ms
step:1100/2160 train_time:46854ms step_avg:42.59ms
step:1101/2160 train_time:46915ms step_avg:42.61ms
step:1102/2160 train_time:46975ms step_avg:42.63ms
step:1103/2160 train_time:47036ms step_avg:42.64ms
step:1104/2160 train_time:47096ms step_avg:42.66ms
step:1105/2160 train_time:47157ms step_avg:42.68ms
step:1106/2160 train_time:47216ms step_avg:42.69ms
step:1107/2160 train_time:47277ms step_avg:42.71ms
step:1108/2160 train_time:47336ms step_avg:42.72ms
step:1109/2160 train_time:47396ms step_avg:42.74ms
step:1110/2160 train_time:47455ms step_avg:42.75ms
step:1111/2160 train_time:47515ms step_avg:42.77ms
step:1112/2160 train_time:47574ms step_avg:42.78ms
step:1113/2160 train_time:47635ms step_avg:42.80ms
step:1114/2160 train_time:47693ms step_avg:42.81ms
step:1115/2160 train_time:47753ms step_avg:42.83ms
step:1116/2160 train_time:47813ms step_avg:42.84ms
step:1117/2160 train_time:47875ms step_avg:42.86ms
step:1118/2160 train_time:47935ms step_avg:42.88ms
step:1119/2160 train_time:47996ms step_avg:42.89ms
step:1120/2160 train_time:48056ms step_avg:42.91ms
step:1121/2160 train_time:48117ms step_avg:42.92ms
step:1122/2160 train_time:48176ms step_avg:42.94ms
step:1123/2160 train_time:48237ms step_avg:42.95ms
step:1124/2160 train_time:48296ms step_avg:42.97ms
step:1125/2160 train_time:48356ms step_avg:42.98ms
step:1126/2160 train_time:48416ms step_avg:43.00ms
step:1127/2160 train_time:48477ms step_avg:43.01ms
step:1128/2160 train_time:48536ms step_avg:43.03ms
step:1129/2160 train_time:48596ms step_avg:43.04ms
step:1130/2160 train_time:48655ms step_avg:43.06ms
step:1131/2160 train_time:48716ms step_avg:43.07ms
step:1132/2160 train_time:48775ms step_avg:43.09ms
step:1133/2160 train_time:48836ms step_avg:43.10ms
step:1134/2160 train_time:48896ms step_avg:43.12ms
step:1135/2160 train_time:48957ms step_avg:43.13ms
step:1136/2160 train_time:49017ms step_avg:43.15ms
step:1137/2160 train_time:49078ms step_avg:43.16ms
step:1138/2160 train_time:49137ms step_avg:43.18ms
step:1139/2160 train_time:49197ms step_avg:43.19ms
step:1140/2160 train_time:49257ms step_avg:43.21ms
step:1141/2160 train_time:49318ms step_avg:43.22ms
step:1142/2160 train_time:49377ms step_avg:43.24ms
step:1143/2160 train_time:49437ms step_avg:43.25ms
step:1144/2160 train_time:49496ms step_avg:43.27ms
step:1145/2160 train_time:49557ms step_avg:43.28ms
step:1146/2160 train_time:49616ms step_avg:43.29ms
step:1147/2160 train_time:49677ms step_avg:43.31ms
step:1148/2160 train_time:49736ms step_avg:43.32ms
step:1149/2160 train_time:49796ms step_avg:43.34ms
step:1150/2160 train_time:49855ms step_avg:43.35ms
step:1151/2160 train_time:49916ms step_avg:43.37ms
step:1152/2160 train_time:49975ms step_avg:43.38ms
step:1153/2160 train_time:50036ms step_avg:43.40ms
step:1154/2160 train_time:50095ms step_avg:43.41ms
step:1155/2160 train_time:50156ms step_avg:43.43ms
step:1156/2160 train_time:50216ms step_avg:43.44ms
step:1157/2160 train_time:50277ms step_avg:43.45ms
step:1158/2160 train_time:50336ms step_avg:43.47ms
step:1159/2160 train_time:50396ms step_avg:43.48ms
step:1160/2160 train_time:50455ms step_avg:43.50ms
step:1161/2160 train_time:50516ms step_avg:43.51ms
step:1162/2160 train_time:50576ms step_avg:43.52ms
step:1163/2160 train_time:50637ms step_avg:43.54ms
step:1164/2160 train_time:50695ms step_avg:43.55ms
step:1165/2160 train_time:50757ms step_avg:43.57ms
step:1166/2160 train_time:50816ms step_avg:43.58ms
step:1167/2160 train_time:50877ms step_avg:43.60ms
step:1168/2160 train_time:50936ms step_avg:43.61ms
step:1169/2160 train_time:50997ms step_avg:43.62ms
step:1170/2160 train_time:51056ms step_avg:43.64ms
step:1171/2160 train_time:51117ms step_avg:43.65ms
step:1172/2160 train_time:51176ms step_avg:43.67ms
step:1173/2160 train_time:51236ms step_avg:43.68ms
step:1174/2160 train_time:51296ms step_avg:43.69ms
step:1175/2160 train_time:51357ms step_avg:43.71ms
step:1176/2160 train_time:51416ms step_avg:43.72ms
step:1177/2160 train_time:51477ms step_avg:43.74ms
step:1178/2160 train_time:51537ms step_avg:43.75ms
step:1179/2160 train_time:51597ms step_avg:43.76ms
step:1180/2160 train_time:51656ms step_avg:43.78ms
step:1181/2160 train_time:51717ms step_avg:43.79ms
step:1182/2160 train_time:51776ms step_avg:43.80ms
step:1183/2160 train_time:51837ms step_avg:43.82ms
step:1184/2160 train_time:51896ms step_avg:43.83ms
step:1185/2160 train_time:51957ms step_avg:43.85ms
step:1186/2160 train_time:52017ms step_avg:43.86ms
step:1187/2160 train_time:52077ms step_avg:43.87ms
step:1188/2160 train_time:52136ms step_avg:43.89ms
step:1189/2160 train_time:52197ms step_avg:43.90ms
step:1190/2160 train_time:52256ms step_avg:43.91ms
step:1191/2160 train_time:52317ms step_avg:43.93ms
step:1192/2160 train_time:52377ms step_avg:43.94ms
step:1193/2160 train_time:52437ms step_avg:43.95ms
step:1194/2160 train_time:52496ms step_avg:43.97ms
step:1195/2160 train_time:52556ms step_avg:43.98ms
step:1196/2160 train_time:52616ms step_avg:43.99ms
step:1197/2160 train_time:52677ms step_avg:44.01ms
step:1198/2160 train_time:52736ms step_avg:44.02ms
step:1199/2160 train_time:52796ms step_avg:44.03ms
step:1200/2160 train_time:52855ms step_avg:44.05ms
step:1201/2160 train_time:52917ms step_avg:44.06ms
step:1202/2160 train_time:52976ms step_avg:44.07ms
step:1203/2160 train_time:53036ms step_avg:44.09ms
step:1204/2160 train_time:53095ms step_avg:44.10ms
step:1205/2160 train_time:53156ms step_avg:44.11ms
step:1206/2160 train_time:53215ms step_avg:44.13ms
step:1207/2160 train_time:53276ms step_avg:44.14ms
step:1208/2160 train_time:53335ms step_avg:44.15ms
step:1209/2160 train_time:53396ms step_avg:44.17ms
step:1210/2160 train_time:53456ms step_avg:44.18ms
step:1211/2160 train_time:53516ms step_avg:44.19ms
step:1212/2160 train_time:53576ms step_avg:44.20ms
step:1213/2160 train_time:53637ms step_avg:44.22ms
step:1214/2160 train_time:53696ms step_avg:44.23ms
step:1215/2160 train_time:53756ms step_avg:44.24ms
step:1216/2160 train_time:53816ms step_avg:44.26ms
step:1217/2160 train_time:53877ms step_avg:44.27ms
step:1218/2160 train_time:53936ms step_avg:44.28ms
step:1219/2160 train_time:53997ms step_avg:44.30ms
step:1220/2160 train_time:54056ms step_avg:44.31ms
step:1221/2160 train_time:54117ms step_avg:44.32ms
step:1222/2160 train_time:54176ms step_avg:44.33ms
step:1223/2160 train_time:54237ms step_avg:44.35ms
step:1224/2160 train_time:54296ms step_avg:44.36ms
step:1225/2160 train_time:54357ms step_avg:44.37ms
step:1226/2160 train_time:54416ms step_avg:44.39ms
step:1227/2160 train_time:54477ms step_avg:44.40ms
step:1228/2160 train_time:54537ms step_avg:44.41ms
step:1229/2160 train_time:54596ms step_avg:44.42ms
step:1230/2160 train_time:54655ms step_avg:44.44ms
step:1231/2160 train_time:54716ms step_avg:44.45ms
step:1232/2160 train_time:54776ms step_avg:44.46ms
step:1233/2160 train_time:54836ms step_avg:44.47ms
step:1234/2160 train_time:54895ms step_avg:44.49ms
step:1235/2160 train_time:54956ms step_avg:44.50ms
step:1236/2160 train_time:55016ms step_avg:44.51ms
step:1237/2160 train_time:55076ms step_avg:44.52ms
step:1238/2160 train_time:55136ms step_avg:44.54ms
step:1239/2160 train_time:55196ms step_avg:44.55ms
step:1240/2160 train_time:55255ms step_avg:44.56ms
step:1241/2160 train_time:55317ms step_avg:44.57ms
step:1242/2160 train_time:55376ms step_avg:44.59ms
step:1243/2160 train_time:55437ms step_avg:44.60ms
step:1244/2160 train_time:55496ms step_avg:44.61ms
step:1245/2160 train_time:55556ms step_avg:44.62ms
step:1246/2160 train_time:55615ms step_avg:44.63ms
step:1247/2160 train_time:55676ms step_avg:44.65ms
step:1248/2160 train_time:55736ms step_avg:44.66ms
step:1249/2160 train_time:55796ms step_avg:44.67ms
step:1250/2160 train_time:55855ms step_avg:44.68ms
step:1250/2160 val_loss:3.5942 train_time:55917ms step_avg:44.73ms
step:1251/2160 train_time:55942ms step_avg:44.72ms
step:1252/2160 train_time:55980ms step_avg:44.71ms
step:1253/2160 train_time:56044ms step_avg:44.73ms
step:1254/2160 train_time:56104ms step_avg:44.74ms
step:1255/2160 train_time:56165ms step_avg:44.75ms
step:1256/2160 train_time:56224ms step_avg:44.76ms
step:1257/2160 train_time:56284ms step_avg:44.78ms
step:1258/2160 train_time:56343ms step_avg:44.79ms
step:1259/2160 train_time:56403ms step_avg:44.80ms
step:1260/2160 train_time:56461ms step_avg:44.81ms
step:1261/2160 train_time:56522ms step_avg:44.82ms
step:1262/2160 train_time:56580ms step_avg:44.83ms
step:1263/2160 train_time:56640ms step_avg:44.85ms
step:1264/2160 train_time:56699ms step_avg:44.86ms
step:1265/2160 train_time:56759ms step_avg:44.87ms
step:1266/2160 train_time:56818ms step_avg:44.88ms
step:1267/2160 train_time:56880ms step_avg:44.89ms
step:1268/2160 train_time:56941ms step_avg:44.91ms
step:1269/2160 train_time:57004ms step_avg:44.92ms
step:1270/2160 train_time:57065ms step_avg:44.93ms
step:1271/2160 train_time:57126ms step_avg:44.95ms
step:1272/2160 train_time:57186ms step_avg:44.96ms
step:1273/2160 train_time:57247ms step_avg:44.97ms
step:1274/2160 train_time:57307ms step_avg:44.98ms
step:1275/2160 train_time:57367ms step_avg:44.99ms
step:1276/2160 train_time:57425ms step_avg:45.00ms
step:1277/2160 train_time:57486ms step_avg:45.02ms
step:1278/2160 train_time:57545ms step_avg:45.03ms
step:1279/2160 train_time:57605ms step_avg:45.04ms
step:1280/2160 train_time:57664ms step_avg:45.05ms
step:1281/2160 train_time:57724ms step_avg:45.06ms
step:1282/2160 train_time:57784ms step_avg:45.07ms
step:1283/2160 train_time:57846ms step_avg:45.09ms
step:1284/2160 train_time:57906ms step_avg:45.10ms
step:1285/2160 train_time:57968ms step_avg:45.11ms
step:1286/2160 train_time:58029ms step_avg:45.12ms
step:1287/2160 train_time:58090ms step_avg:45.14ms
step:1288/2160 train_time:58150ms step_avg:45.15ms
step:1289/2160 train_time:58210ms step_avg:45.16ms
step:1290/2160 train_time:58269ms step_avg:45.17ms
step:1291/2160 train_time:58329ms step_avg:45.18ms
step:1292/2160 train_time:58388ms step_avg:45.19ms
step:1293/2160 train_time:58447ms step_avg:45.20ms
step:1294/2160 train_time:58506ms step_avg:45.21ms
step:1295/2160 train_time:58566ms step_avg:45.22ms
step:1296/2160 train_time:58625ms step_avg:45.24ms
step:1297/2160 train_time:58685ms step_avg:45.25ms
step:1298/2160 train_time:58744ms step_avg:45.26ms
step:1299/2160 train_time:58806ms step_avg:45.27ms
step:1300/2160 train_time:58866ms step_avg:45.28ms
step:1301/2160 train_time:58927ms step_avg:45.29ms
step:1302/2160 train_time:58988ms step_avg:45.31ms
step:1303/2160 train_time:59050ms step_avg:45.32ms
step:1304/2160 train_time:59110ms step_avg:45.33ms
step:1305/2160 train_time:59172ms step_avg:45.34ms
step:1306/2160 train_time:59231ms step_avg:45.35ms
step:1307/2160 train_time:59292ms step_avg:45.36ms
step:1308/2160 train_time:59350ms step_avg:45.37ms
step:1309/2160 train_time:59410ms step_avg:45.39ms
step:1310/2160 train_time:59469ms step_avg:45.40ms
step:1311/2160 train_time:59529ms step_avg:45.41ms
step:1312/2160 train_time:59588ms step_avg:45.42ms
step:1313/2160 train_time:59648ms step_avg:45.43ms
step:1314/2160 train_time:59707ms step_avg:45.44ms
step:1315/2160 train_time:59768ms step_avg:45.45ms
step:1316/2160 train_time:59827ms step_avg:45.46ms
step:1317/2160 train_time:59890ms step_avg:45.47ms
step:1318/2160 train_time:59950ms step_avg:45.49ms
step:1319/2160 train_time:60011ms step_avg:45.50ms
step:1320/2160 train_time:60071ms step_avg:45.51ms
step:1321/2160 train_time:60132ms step_avg:45.52ms
step:1322/2160 train_time:60191ms step_avg:45.53ms
step:1323/2160 train_time:60253ms step_avg:45.54ms
step:1324/2160 train_time:60311ms step_avg:45.55ms
step:1325/2160 train_time:60372ms step_avg:45.56ms
step:1326/2160 train_time:60430ms step_avg:45.57ms
step:1327/2160 train_time:60490ms step_avg:45.58ms
step:1328/2160 train_time:60549ms step_avg:45.59ms
step:1329/2160 train_time:60609ms step_avg:45.60ms
step:1330/2160 train_time:60668ms step_avg:45.62ms
step:1331/2160 train_time:60728ms step_avg:45.63ms
step:1332/2160 train_time:60788ms step_avg:45.64ms
step:1333/2160 train_time:60849ms step_avg:45.65ms
step:1334/2160 train_time:60908ms step_avg:45.66ms
step:1335/2160 train_time:60970ms step_avg:45.67ms
step:1336/2160 train_time:61030ms step_avg:45.68ms
step:1337/2160 train_time:61091ms step_avg:45.69ms
step:1338/2160 train_time:61151ms step_avg:45.70ms
step:1339/2160 train_time:61212ms step_avg:45.71ms
step:1340/2160 train_time:61271ms step_avg:45.72ms
step:1341/2160 train_time:61331ms step_avg:45.74ms
step:1342/2160 train_time:61390ms step_avg:45.74ms
step:1343/2160 train_time:61450ms step_avg:45.76ms
step:1344/2160 train_time:61509ms step_avg:45.77ms
step:1345/2160 train_time:61569ms step_avg:45.78ms
step:1346/2160 train_time:61628ms step_avg:45.79ms
step:1347/2160 train_time:61689ms step_avg:45.80ms
step:1348/2160 train_time:61749ms step_avg:45.81ms
step:1349/2160 train_time:61810ms step_avg:45.82ms
step:1350/2160 train_time:61870ms step_avg:45.83ms
step:1351/2160 train_time:61930ms step_avg:45.84ms
step:1352/2160 train_time:61990ms step_avg:45.85ms
step:1353/2160 train_time:62051ms step_avg:45.86ms
step:1354/2160 train_time:62110ms step_avg:45.87ms
step:1355/2160 train_time:62171ms step_avg:45.88ms
step:1356/2160 train_time:62230ms step_avg:45.89ms
step:1357/2160 train_time:62291ms step_avg:45.90ms
step:1358/2160 train_time:62350ms step_avg:45.91ms
step:1359/2160 train_time:62411ms step_avg:45.92ms
step:1360/2160 train_time:62470ms step_avg:45.93ms
step:1361/2160 train_time:62530ms step_avg:45.94ms
step:1362/2160 train_time:62589ms step_avg:45.95ms
step:1363/2160 train_time:62650ms step_avg:45.96ms
step:1364/2160 train_time:62709ms step_avg:45.97ms
step:1365/2160 train_time:62769ms step_avg:45.98ms
step:1366/2160 train_time:62829ms step_avg:45.99ms
step:1367/2160 train_time:62889ms step_avg:46.01ms
step:1368/2160 train_time:62949ms step_avg:46.02ms
step:1369/2160 train_time:63011ms step_avg:46.03ms
step:1370/2160 train_time:63070ms step_avg:46.04ms
step:1371/2160 train_time:63131ms step_avg:46.05ms
step:1372/2160 train_time:63191ms step_avg:46.06ms
step:1373/2160 train_time:63251ms step_avg:46.07ms
step:1374/2160 train_time:63310ms step_avg:46.08ms
step:1375/2160 train_time:63371ms step_avg:46.09ms
step:1376/2160 train_time:63430ms step_avg:46.10ms
step:1377/2160 train_time:63490ms step_avg:46.11ms
step:1378/2160 train_time:63549ms step_avg:46.12ms
step:1379/2160 train_time:63610ms step_avg:46.13ms
step:1380/2160 train_time:63670ms step_avg:46.14ms
step:1381/2160 train_time:63729ms step_avg:46.15ms
step:1382/2160 train_time:63788ms step_avg:46.16ms
step:1383/2160 train_time:63849ms step_avg:46.17ms
step:1384/2160 train_time:63909ms step_avg:46.18ms
step:1385/2160 train_time:63970ms step_avg:46.19ms
step:1386/2160 train_time:64030ms step_avg:46.20ms
step:1387/2160 train_time:64091ms step_avg:46.21ms
step:1388/2160 train_time:64150ms step_avg:46.22ms
step:1389/2160 train_time:64211ms step_avg:46.23ms
step:1390/2160 train_time:64271ms step_avg:46.24ms
step:1391/2160 train_time:64332ms step_avg:46.25ms
step:1392/2160 train_time:64390ms step_avg:46.26ms
step:1393/2160 train_time:64451ms step_avg:46.27ms
step:1394/2160 train_time:64510ms step_avg:46.28ms
step:1395/2160 train_time:64570ms step_avg:46.29ms
step:1396/2160 train_time:64629ms step_avg:46.30ms
step:1397/2160 train_time:64690ms step_avg:46.31ms
step:1398/2160 train_time:64749ms step_avg:46.32ms
step:1399/2160 train_time:64810ms step_avg:46.33ms
step:1400/2160 train_time:64869ms step_avg:46.34ms
step:1401/2160 train_time:64930ms step_avg:46.35ms
step:1402/2160 train_time:64989ms step_avg:46.35ms
step:1403/2160 train_time:65050ms step_avg:46.37ms
step:1404/2160 train_time:65110ms step_avg:46.37ms
step:1405/2160 train_time:65171ms step_avg:46.39ms
step:1406/2160 train_time:65230ms step_avg:46.39ms
step:1407/2160 train_time:65291ms step_avg:46.40ms
step:1408/2160 train_time:65351ms step_avg:46.41ms
step:1409/2160 train_time:65412ms step_avg:46.42ms
step:1410/2160 train_time:65471ms step_avg:46.43ms
step:1411/2160 train_time:65531ms step_avg:46.44ms
step:1412/2160 train_time:65590ms step_avg:46.45ms
step:1413/2160 train_time:65651ms step_avg:46.46ms
step:1414/2160 train_time:65710ms step_avg:46.47ms
step:1415/2160 train_time:65771ms step_avg:46.48ms
step:1416/2160 train_time:65859ms step_avg:46.51ms
step:1417/2160 train_time:65947ms step_avg:46.54ms
step:1418/2160 train_time:66033ms step_avg:46.57ms
step:1419/2160 train_time:66124ms step_avg:46.60ms
step:1420/2160 train_time:66211ms step_avg:46.63ms
step:1421/2160 train_time:66300ms step_avg:46.66ms
step:1422/2160 train_time:66386ms step_avg:46.69ms
step:1423/2160 train_time:66475ms step_avg:46.71ms
step:1424/2160 train_time:66561ms step_avg:46.74ms
step:1425/2160 train_time:66649ms step_avg:46.77ms
step:1426/2160 train_time:66737ms step_avg:46.80ms
step:1427/2160 train_time:66826ms step_avg:46.83ms
step:1428/2160 train_time:66913ms step_avg:46.86ms
step:1429/2160 train_time:67001ms step_avg:46.89ms
step:1430/2160 train_time:67088ms step_avg:46.91ms
step:1431/2160 train_time:67178ms step_avg:46.94ms
step:1432/2160 train_time:67264ms step_avg:46.97ms
step:1433/2160 train_time:67352ms step_avg:47.00ms
step:1434/2160 train_time:67440ms step_avg:47.03ms
step:1435/2160 train_time:67528ms step_avg:47.06ms
step:1436/2160 train_time:67615ms step_avg:47.09ms
step:1437/2160 train_time:67704ms step_avg:47.11ms
step:1438/2160 train_time:67791ms step_avg:47.14ms
step:1439/2160 train_time:67880ms step_avg:47.17ms
step:1440/2160 train_time:67966ms step_avg:47.20ms
step:1441/2160 train_time:68054ms step_avg:47.23ms
step:1442/2160 train_time:68142ms step_avg:47.26ms
step:1443/2160 train_time:68230ms step_avg:47.28ms
step:1444/2160 train_time:68318ms step_avg:47.31ms
step:1445/2160 train_time:68406ms step_avg:47.34ms
step:1446/2160 train_time:68493ms step_avg:47.37ms
step:1447/2160 train_time:68583ms step_avg:47.40ms
step:1448/2160 train_time:68669ms step_avg:47.42ms
step:1449/2160 train_time:68757ms step_avg:47.45ms
step:1450/2160 train_time:68844ms step_avg:47.48ms
step:1451/2160 train_time:68932ms step_avg:47.51ms
step:1452/2160 train_time:69019ms step_avg:47.53ms
step:1453/2160 train_time:69108ms step_avg:47.56ms
step:1454/2160 train_time:69195ms step_avg:47.59ms
step:1455/2160 train_time:69284ms step_avg:47.62ms
step:1456/2160 train_time:69371ms step_avg:47.64ms
step:1457/2160 train_time:69459ms step_avg:47.67ms
step:1458/2160 train_time:69546ms step_avg:47.70ms
step:1459/2160 train_time:69633ms step_avg:47.73ms
step:1460/2160 train_time:69721ms step_avg:47.75ms
step:1461/2160 train_time:69809ms step_avg:47.78ms
step:1462/2160 train_time:69896ms step_avg:47.81ms
step:1463/2160 train_time:69985ms step_avg:47.84ms
step:1464/2160 train_time:70072ms step_avg:47.86ms
step:1465/2160 train_time:70160ms step_avg:47.89ms
step:1466/2160 train_time:70246ms step_avg:47.92ms
step:1467/2160 train_time:70334ms step_avg:47.94ms
step:1468/2160 train_time:70421ms step_avg:47.97ms
step:1469/2160 train_time:70509ms step_avg:48.00ms
step:1470/2160 train_time:70597ms step_avg:48.02ms
step:1471/2160 train_time:70686ms step_avg:48.05ms
step:1472/2160 train_time:70773ms step_avg:48.08ms
step:1473/2160 train_time:70861ms step_avg:48.11ms
step:1474/2160 train_time:70948ms step_avg:48.13ms
step:1475/2160 train_time:71037ms step_avg:48.16ms
step:1476/2160 train_time:71124ms step_avg:48.19ms
step:1477/2160 train_time:71213ms step_avg:48.21ms
step:1478/2160 train_time:71301ms step_avg:48.24ms
step:1479/2160 train_time:71390ms step_avg:48.27ms
step:1480/2160 train_time:71477ms step_avg:48.30ms
step:1481/2160 train_time:71566ms step_avg:48.32ms
step:1482/2160 train_time:71652ms step_avg:48.35ms
step:1483/2160 train_time:71741ms step_avg:48.38ms
step:1484/2160 train_time:71827ms step_avg:48.40ms
step:1485/2160 train_time:71915ms step_avg:48.43ms
step:1486/2160 train_time:72002ms step_avg:48.45ms
step:1487/2160 train_time:72091ms step_avg:48.48ms
step:1488/2160 train_time:72179ms step_avg:48.51ms
step:1489/2160 train_time:72267ms step_avg:48.53ms
step:1490/2160 train_time:72354ms step_avg:48.56ms
step:1491/2160 train_time:72442ms step_avg:48.59ms
step:1492/2160 train_time:72529ms step_avg:48.61ms
step:1493/2160 train_time:72617ms step_avg:48.64ms
step:1494/2160 train_time:72704ms step_avg:48.66ms
step:1495/2160 train_time:72792ms step_avg:48.69ms
step:1496/2160 train_time:72879ms step_avg:48.72ms
step:1497/2160 train_time:72967ms step_avg:48.74ms
step:1498/2160 train_time:73053ms step_avg:48.77ms
step:1499/2160 train_time:73142ms step_avg:48.79ms
step:1500/2160 train_time:73229ms step_avg:48.82ms
step:1500/2160 val_loss:3.4878 train_time:73318ms step_avg:48.88ms
step:1501/2160 train_time:73343ms step_avg:48.86ms
step:1502/2160 train_time:73408ms step_avg:48.87ms
step:1503/2160 train_time:73503ms step_avg:48.90ms
step:1504/2160 train_time:73590ms step_avg:48.93ms
step:1505/2160 train_time:73677ms step_avg:48.96ms
step:1506/2160 train_time:73763ms step_avg:48.98ms
step:1507/2160 train_time:73850ms step_avg:49.00ms
step:1508/2160 train_time:73936ms step_avg:49.03ms
step:1509/2160 train_time:74022ms step_avg:49.05ms
step:1510/2160 train_time:74108ms step_avg:49.08ms
step:1511/2160 train_time:74197ms step_avg:49.10ms
step:1512/2160 train_time:74285ms step_avg:49.13ms
step:1513/2160 train_time:74378ms step_avg:49.16ms
step:1514/2160 train_time:74467ms step_avg:49.19ms
step:1515/2160 train_time:74558ms step_avg:49.21ms
step:1516/2160 train_time:74644ms step_avg:49.24ms
step:1517/2160 train_time:74731ms step_avg:49.26ms
step:1518/2160 train_time:74817ms step_avg:49.29ms
step:1519/2160 train_time:74904ms step_avg:49.31ms
step:1520/2160 train_time:74989ms step_avg:49.33ms
step:1521/2160 train_time:75076ms step_avg:49.36ms
step:1522/2160 train_time:75162ms step_avg:49.38ms
step:1523/2160 train_time:75252ms step_avg:49.41ms
step:1524/2160 train_time:75340ms step_avg:49.44ms
step:1525/2160 train_time:75430ms step_avg:49.46ms
step:1526/2160 train_time:75518ms step_avg:49.49ms
step:1527/2160 train_time:75607ms step_avg:49.51ms
step:1528/2160 train_time:75694ms step_avg:49.54ms
step:1529/2160 train_time:75781ms step_avg:49.56ms
step:1530/2160 train_time:75867ms step_avg:49.59ms
step:1531/2160 train_time:75954ms step_avg:49.61ms
step:1532/2160 train_time:76040ms step_avg:49.63ms
step:1533/2160 train_time:76128ms step_avg:49.66ms
step:1534/2160 train_time:76215ms step_avg:49.68ms
step:1535/2160 train_time:76304ms step_avg:49.71ms
step:1536/2160 train_time:76393ms step_avg:49.74ms
step:1537/2160 train_time:76483ms step_avg:49.76ms
step:1538/2160 train_time:76571ms step_avg:49.79ms
step:1539/2160 train_time:76659ms step_avg:49.81ms
step:1540/2160 train_time:76747ms step_avg:49.84ms
step:1541/2160 train_time:76836ms step_avg:49.86ms
step:1542/2160 train_time:76922ms step_avg:49.88ms
step:1543/2160 train_time:77010ms step_avg:49.91ms
step:1544/2160 train_time:77096ms step_avg:49.93ms
step:1545/2160 train_time:77184ms step_avg:49.96ms
step:1546/2160 train_time:77271ms step_avg:49.98ms
step:1547/2160 train_time:77361ms step_avg:50.01ms
step:1548/2160 train_time:77449ms step_avg:50.03ms
step:1549/2160 train_time:77538ms step_avg:50.06ms
step:1550/2160 train_time:77625ms step_avg:50.08ms
step:1551/2160 train_time:77713ms step_avg:50.10ms
step:1552/2160 train_time:77799ms step_avg:50.13ms
step:1553/2160 train_time:77887ms step_avg:50.15ms
step:1554/2160 train_time:77974ms step_avg:50.18ms
step:1555/2160 train_time:78062ms step_avg:50.20ms
step:1556/2160 train_time:78149ms step_avg:50.22ms
step:1557/2160 train_time:78238ms step_avg:50.25ms
step:1558/2160 train_time:78325ms step_avg:50.27ms
step:1559/2160 train_time:78414ms step_avg:50.30ms
step:1560/2160 train_time:78500ms step_avg:50.32ms
step:1561/2160 train_time:78589ms step_avg:50.35ms
step:1562/2160 train_time:78676ms step_avg:50.37ms
step:1563/2160 train_time:78763ms step_avg:50.39ms
step:1564/2160 train_time:78850ms step_avg:50.42ms
step:1565/2160 train_time:78938ms step_avg:50.44ms
step:1566/2160 train_time:79024ms step_avg:50.46ms
step:1567/2160 train_time:79112ms step_avg:50.49ms
step:1568/2160 train_time:79199ms step_avg:50.51ms
step:1569/2160 train_time:79287ms step_avg:50.53ms
step:1570/2160 train_time:79374ms step_avg:50.56ms
step:1571/2160 train_time:79463ms step_avg:50.58ms
step:1572/2160 train_time:79550ms step_avg:50.60ms
step:1573/2160 train_time:79639ms step_avg:50.63ms
step:1574/2160 train_time:79726ms step_avg:50.65ms
step:1575/2160 train_time:79816ms step_avg:50.68ms
step:1576/2160 train_time:79901ms step_avg:50.70ms
step:1577/2160 train_time:79990ms step_avg:50.72ms
step:1578/2160 train_time:80076ms step_avg:50.75ms
step:1579/2160 train_time:80163ms step_avg:50.77ms
step:1580/2160 train_time:80250ms step_avg:50.79ms
step:1581/2160 train_time:80339ms step_avg:50.82ms
step:1582/2160 train_time:80426ms step_avg:50.84ms
step:1583/2160 train_time:80516ms step_avg:50.86ms
step:1584/2160 train_time:80603ms step_avg:50.89ms
step:1585/2160 train_time:80693ms step_avg:50.91ms
step:1586/2160 train_time:80778ms step_avg:50.93ms
step:1587/2160 train_time:80867ms step_avg:50.96ms
step:1588/2160 train_time:80954ms step_avg:50.98ms
step:1589/2160 train_time:81042ms step_avg:51.00ms
step:1590/2160 train_time:81129ms step_avg:51.02ms
step:1591/2160 train_time:81218ms step_avg:51.05ms
step:1592/2160 train_time:81305ms step_avg:51.07ms
step:1593/2160 train_time:81394ms step_avg:51.09ms
step:1594/2160 train_time:81480ms step_avg:51.12ms
step:1595/2160 train_time:81569ms step_avg:51.14ms
step:1596/2160 train_time:81656ms step_avg:51.16ms
step:1597/2160 train_time:81744ms step_avg:51.19ms
step:1598/2160 train_time:81831ms step_avg:51.21ms
step:1599/2160 train_time:81920ms step_avg:51.23ms
step:1600/2160 train_time:82006ms step_avg:51.25ms
step:1601/2160 train_time:82095ms step_avg:51.28ms
step:1602/2160 train_time:82181ms step_avg:51.30ms
step:1603/2160 train_time:82269ms step_avg:51.32ms
step:1604/2160 train_time:82357ms step_avg:51.34ms
step:1605/2160 train_time:82446ms step_avg:51.37ms
step:1606/2160 train_time:82533ms step_avg:51.39ms
step:1607/2160 train_time:82621ms step_avg:51.41ms
step:1608/2160 train_time:82708ms step_avg:51.44ms
step:1609/2160 train_time:82797ms step_avg:51.46ms
step:1610/2160 train_time:82884ms step_avg:51.48ms
step:1611/2160 train_time:82973ms step_avg:51.50ms
step:1612/2160 train_time:83059ms step_avg:51.53ms
step:1613/2160 train_time:83148ms step_avg:51.55ms
step:1614/2160 train_time:83235ms step_avg:51.57ms
step:1615/2160 train_time:83324ms step_avg:51.59ms
step:1616/2160 train_time:83411ms step_avg:51.62ms
step:1617/2160 train_time:83499ms step_avg:51.64ms
step:1618/2160 train_time:83586ms step_avg:51.66ms
step:1619/2160 train_time:83676ms step_avg:51.68ms
step:1620/2160 train_time:83761ms step_avg:51.70ms
step:1621/2160 train_time:83850ms step_avg:51.73ms
step:1622/2160 train_time:83937ms step_avg:51.75ms
step:1623/2160 train_time:84025ms step_avg:51.77ms
step:1624/2160 train_time:84112ms step_avg:51.79ms
step:1625/2160 train_time:84200ms step_avg:51.82ms
step:1626/2160 train_time:84287ms step_avg:51.84ms
step:1627/2160 train_time:84377ms step_avg:51.86ms
step:1628/2160 train_time:84464ms step_avg:51.88ms
step:1629/2160 train_time:84553ms step_avg:51.90ms
step:1630/2160 train_time:84639ms step_avg:51.93ms
step:1631/2160 train_time:84727ms step_avg:51.95ms
step:1632/2160 train_time:84814ms step_avg:51.97ms
step:1633/2160 train_time:84902ms step_avg:51.99ms
step:1634/2160 train_time:84989ms step_avg:52.01ms
step:1635/2160 train_time:85079ms step_avg:52.04ms
step:1636/2160 train_time:85165ms step_avg:52.06ms
step:1637/2160 train_time:85254ms step_avg:52.08ms
step:1638/2160 train_time:85340ms step_avg:52.10ms
step:1639/2160 train_time:85429ms step_avg:52.12ms
step:1640/2160 train_time:85516ms step_avg:52.14ms
step:1641/2160 train_time:85605ms step_avg:52.17ms
step:1642/2160 train_time:85691ms step_avg:52.19ms
step:1643/2160 train_time:85779ms step_avg:52.21ms
step:1644/2160 train_time:85866ms step_avg:52.23ms
step:1645/2160 train_time:85955ms step_avg:52.25ms
step:1646/2160 train_time:86042ms step_avg:52.27ms
step:1647/2160 train_time:86130ms step_avg:52.30ms
step:1648/2160 train_time:86217ms step_avg:52.32ms
step:1649/2160 train_time:86305ms step_avg:52.34ms
step:1650/2160 train_time:86392ms step_avg:52.36ms
step:1651/2160 train_time:86480ms step_avg:52.38ms
step:1652/2160 train_time:86568ms step_avg:52.40ms
step:1653/2160 train_time:86657ms step_avg:52.42ms
step:1654/2160 train_time:86744ms step_avg:52.45ms
step:1655/2160 train_time:86832ms step_avg:52.47ms
step:1656/2160 train_time:86918ms step_avg:52.49ms
step:1657/2160 train_time:87006ms step_avg:52.51ms
step:1658/2160 train_time:87093ms step_avg:52.53ms
step:1659/2160 train_time:87181ms step_avg:52.55ms
step:1660/2160 train_time:87268ms step_avg:52.57ms
step:1661/2160 train_time:87357ms step_avg:52.59ms
step:1662/2160 train_time:87443ms step_avg:52.61ms
step:1663/2160 train_time:87531ms step_avg:52.63ms
step:1664/2160 train_time:87618ms step_avg:52.66ms
step:1665/2160 train_time:87707ms step_avg:52.68ms
step:1666/2160 train_time:87794ms step_avg:52.70ms
step:1667/2160 train_time:87882ms step_avg:52.72ms
step:1668/2160 train_time:87970ms step_avg:52.74ms
step:1669/2160 train_time:88057ms step_avg:52.76ms
step:1670/2160 train_time:88144ms step_avg:52.78ms
step:1671/2160 train_time:88233ms step_avg:52.80ms
step:1672/2160 train_time:88319ms step_avg:52.82ms
step:1673/2160 train_time:88407ms step_avg:52.84ms
step:1674/2160 train_time:88494ms step_avg:52.86ms
step:1675/2160 train_time:88581ms step_avg:52.88ms
step:1676/2160 train_time:88668ms step_avg:52.90ms
step:1677/2160 train_time:88758ms step_avg:52.93ms
step:1678/2160 train_time:88845ms step_avg:52.95ms
step:1679/2160 train_time:88934ms step_avg:52.97ms
step:1680/2160 train_time:89019ms step_avg:52.99ms
step:1681/2160 train_time:89108ms step_avg:53.01ms
step:1682/2160 train_time:89195ms step_avg:53.03ms
step:1683/2160 train_time:89284ms step_avg:53.05ms
step:1684/2160 train_time:89371ms step_avg:53.07ms
step:1685/2160 train_time:89459ms step_avg:53.09ms
step:1686/2160 train_time:89545ms step_avg:53.11ms
step:1687/2160 train_time:89634ms step_avg:53.13ms
step:1688/2160 train_time:89721ms step_avg:53.15ms
step:1689/2160 train_time:89810ms step_avg:53.17ms
step:1690/2160 train_time:89897ms step_avg:53.19ms
step:1691/2160 train_time:89985ms step_avg:53.21ms
step:1692/2160 train_time:90072ms step_avg:53.23ms
step:1693/2160 train_time:90160ms step_avg:53.25ms
step:1694/2160 train_time:90247ms step_avg:53.27ms
step:1695/2160 train_time:90336ms step_avg:53.30ms
step:1696/2160 train_time:90423ms step_avg:53.32ms
step:1697/2160 train_time:90511ms step_avg:53.34ms
step:1698/2160 train_time:90597ms step_avg:53.36ms
step:1699/2160 train_time:90686ms step_avg:53.38ms
step:1700/2160 train_time:90773ms step_avg:53.40ms
step:1701/2160 train_time:90861ms step_avg:53.42ms
step:1702/2160 train_time:90948ms step_avg:53.44ms
step:1703/2160 train_time:91037ms step_avg:53.46ms
step:1704/2160 train_time:91124ms step_avg:53.48ms
step:1705/2160 train_time:91213ms step_avg:53.50ms
step:1706/2160 train_time:91299ms step_avg:53.52ms
step:1707/2160 train_time:91387ms step_avg:53.54ms
step:1708/2160 train_time:91474ms step_avg:53.56ms
step:1709/2160 train_time:91561ms step_avg:53.58ms
step:1710/2160 train_time:91648ms step_avg:53.60ms
step:1711/2160 train_time:91737ms step_avg:53.62ms
step:1712/2160 train_time:91824ms step_avg:53.64ms
step:1713/2160 train_time:91913ms step_avg:53.66ms
step:1714/2160 train_time:91999ms step_avg:53.68ms
step:1715/2160 train_time:92087ms step_avg:53.70ms
step:1716/2160 train_time:92175ms step_avg:53.72ms
step:1717/2160 train_time:92262ms step_avg:53.73ms
step:1718/2160 train_time:92349ms step_avg:53.75ms
step:1719/2160 train_time:92437ms step_avg:53.77ms
step:1720/2160 train_time:92524ms step_avg:53.79ms
step:1721/2160 train_time:92613ms step_avg:53.81ms
step:1722/2160 train_time:92699ms step_avg:53.83ms
step:1723/2160 train_time:92788ms step_avg:53.85ms
step:1724/2160 train_time:92875ms step_avg:53.87ms
step:1725/2160 train_time:92963ms step_avg:53.89ms
step:1726/2160 train_time:93049ms step_avg:53.91ms
step:1727/2160 train_time:93139ms step_avg:53.93ms
step:1728/2160 train_time:93225ms step_avg:53.95ms
step:1729/2160 train_time:93314ms step_avg:53.97ms
step:1730/2160 train_time:93400ms step_avg:53.99ms
step:1731/2160 train_time:93489ms step_avg:54.01ms
step:1732/2160 train_time:93576ms step_avg:54.03ms
step:1733/2160 train_time:93664ms step_avg:54.05ms
step:1734/2160 train_time:93751ms step_avg:54.07ms
step:1735/2160 train_time:93839ms step_avg:54.09ms
step:1736/2160 train_time:93926ms step_avg:54.10ms
step:1737/2160 train_time:94015ms step_avg:54.13ms
step:1738/2160 train_time:94101ms step_avg:54.14ms
step:1739/2160 train_time:94190ms step_avg:54.16ms
step:1740/2160 train_time:94277ms step_avg:54.18ms
step:1741/2160 train_time:94364ms step_avg:54.20ms
step:1742/2160 train_time:94451ms step_avg:54.22ms
step:1743/2160 train_time:94540ms step_avg:54.24ms
step:1744/2160 train_time:94627ms step_avg:54.26ms
step:1745/2160 train_time:94716ms step_avg:54.28ms
step:1746/2160 train_time:94802ms step_avg:54.30ms
step:1747/2160 train_time:94890ms step_avg:54.32ms
step:1748/2160 train_time:94977ms step_avg:54.33ms
step:1749/2160 train_time:95065ms step_avg:54.35ms
step:1750/2160 train_time:95152ms step_avg:54.37ms
step:1750/2160 val_loss:3.3898 train_time:95241ms step_avg:54.42ms
step:1751/2160 train_time:95266ms step_avg:54.41ms
step:1752/2160 train_time:95330ms step_avg:54.41ms
step:1753/2160 train_time:95421ms step_avg:54.43ms
step:1754/2160 train_time:95507ms step_avg:54.45ms
step:1755/2160 train_time:95595ms step_avg:54.47ms
step:1756/2160 train_time:95682ms step_avg:54.49ms
step:1757/2160 train_time:95768ms step_avg:54.51ms
step:1758/2160 train_time:95854ms step_avg:54.52ms
step:1759/2160 train_time:95941ms step_avg:54.54ms
step:1760/2160 train_time:96028ms step_avg:54.56ms
step:1761/2160 train_time:96116ms step_avg:54.58ms
step:1762/2160 train_time:96204ms step_avg:54.60ms
step:1763/2160 train_time:96295ms step_avg:54.62ms
step:1764/2160 train_time:96383ms step_avg:54.64ms
step:1765/2160 train_time:96473ms step_avg:54.66ms
step:1766/2160 train_time:96560ms step_avg:54.68ms
step:1767/2160 train_time:96648ms step_avg:54.70ms
step:1768/2160 train_time:96735ms step_avg:54.71ms
step:1769/2160 train_time:96822ms step_avg:54.73ms
step:1770/2160 train_time:96908ms step_avg:54.75ms
step:1771/2160 train_time:96996ms step_avg:54.77ms
step:1772/2160 train_time:97082ms step_avg:54.79ms
step:1773/2160 train_time:97171ms step_avg:54.81ms
step:1774/2160 train_time:97260ms step_avg:54.83ms
step:1775/2160 train_time:97350ms step_avg:54.85ms
step:1776/2160 train_time:97439ms step_avg:54.86ms
step:1777/2160 train_time:97527ms step_avg:54.88ms
step:1778/2160 train_time:97614ms step_avg:54.90ms
step:1779/2160 train_time:97702ms step_avg:54.92ms
step:1780/2160 train_time:97788ms step_avg:54.94ms
step:1781/2160 train_time:97876ms step_avg:54.96ms
step:1782/2160 train_time:97962ms step_avg:54.97ms
step:1783/2160 train_time:98050ms step_avg:54.99ms
step:1784/2160 train_time:98137ms step_avg:55.01ms
step:1785/2160 train_time:98226ms step_avg:55.03ms
step:1786/2160 train_time:98313ms step_avg:55.05ms
step:1787/2160 train_time:98404ms step_avg:55.07ms
step:1788/2160 train_time:98490ms step_avg:55.08ms
step:1789/2160 train_time:98580ms step_avg:55.10ms
step:1790/2160 train_time:98665ms step_avg:55.12ms
step:1791/2160 train_time:98753ms step_avg:55.14ms
step:1792/2160 train_time:98839ms step_avg:55.16ms
step:1793/2160 train_time:98926ms step_avg:55.17ms
step:1794/2160 train_time:99013ms step_avg:55.19ms
step:1795/2160 train_time:99102ms step_avg:55.21ms
step:1796/2160 train_time:99188ms step_avg:55.23ms
step:1797/2160 train_time:99278ms step_avg:55.25ms
step:1798/2160 train_time:99364ms step_avg:55.26ms
step:1799/2160 train_time:99453ms step_avg:55.28ms
step:1800/2160 train_time:99541ms step_avg:55.30ms
step:1801/2160 train_time:99628ms step_avg:55.32ms
step:1802/2160 train_time:99714ms step_avg:55.34ms
step:1803/2160 train_time:99802ms step_avg:55.35ms
step:1804/2160 train_time:99888ms step_avg:55.37ms
step:1805/2160 train_time:99976ms step_avg:55.39ms
step:1806/2160 train_time:100063ms step_avg:55.41ms
step:1807/2160 train_time:100151ms step_avg:55.42ms
step:1808/2160 train_time:100239ms step_avg:55.44ms
step:1809/2160 train_time:100327ms step_avg:55.46ms
step:1810/2160 train_time:100414ms step_avg:55.48ms
step:1811/2160 train_time:100503ms step_avg:55.50ms
step:1812/2160 train_time:100590ms step_avg:55.51ms
step:1813/2160 train_time:100679ms step_avg:55.53ms
step:1814/2160 train_time:100765ms step_avg:55.55ms
step:1815/2160 train_time:100853ms step_avg:55.57ms
step:1816/2160 train_time:100940ms step_avg:55.58ms
step:1817/2160 train_time:101028ms step_avg:55.60ms
step:1818/2160 train_time:101114ms step_avg:55.62ms
step:1819/2160 train_time:101202ms step_avg:55.64ms
step:1820/2160 train_time:101289ms step_avg:55.65ms
step:1821/2160 train_time:101379ms step_avg:55.67ms
step:1822/2160 train_time:101465ms step_avg:55.69ms
step:1823/2160 train_time:101553ms step_avg:55.71ms
step:1824/2160 train_time:101641ms step_avg:55.72ms
step:1825/2160 train_time:101729ms step_avg:55.74ms
step:1826/2160 train_time:101815ms step_avg:55.76ms
step:1827/2160 train_time:101904ms step_avg:55.78ms
step:1828/2160 train_time:101990ms step_avg:55.79ms
step:1829/2160 train_time:102079ms step_avg:55.81ms
step:1830/2160 train_time:102165ms step_avg:55.83ms
step:1831/2160 train_time:102255ms step_avg:55.85ms
step:1832/2160 train_time:102343ms step_avg:55.86ms
step:1833/2160 train_time:102430ms step_avg:55.88ms
step:1834/2160 train_time:102518ms step_avg:55.90ms
step:1835/2160 train_time:102606ms step_avg:55.92ms
step:1836/2160 train_time:102692ms step_avg:55.93ms
step:1837/2160 train_time:102780ms step_avg:55.95ms
step:1838/2160 train_time:102866ms step_avg:55.97ms
step:1839/2160 train_time:102954ms step_avg:55.98ms
step:1840/2160 train_time:103041ms step_avg:56.00ms
step:1841/2160 train_time:103130ms step_avg:56.02ms
step:1842/2160 train_time:103217ms step_avg:56.04ms
step:1843/2160 train_time:103306ms step_avg:56.05ms
step:1844/2160 train_time:103393ms step_avg:56.07ms
step:1845/2160 train_time:103482ms step_avg:56.09ms
step:1846/2160 train_time:103569ms step_avg:56.10ms
step:1847/2160 train_time:103658ms step_avg:56.12ms
step:1848/2160 train_time:103744ms step_avg:56.14ms
step:1849/2160 train_time:103832ms step_avg:56.16ms
step:1850/2160 train_time:103919ms step_avg:56.17ms
step:1851/2160 train_time:104007ms step_avg:56.19ms
step:1852/2160 train_time:104093ms step_avg:56.21ms
step:1853/2160 train_time:104182ms step_avg:56.22ms
step:1854/2160 train_time:104268ms step_avg:56.24ms
step:1855/2160 train_time:104356ms step_avg:56.26ms
step:1856/2160 train_time:104443ms step_avg:56.27ms
step:1857/2160 train_time:104532ms step_avg:56.29ms
step:1858/2160 train_time:104618ms step_avg:56.31ms
step:1859/2160 train_time:104706ms step_avg:56.32ms
step:1860/2160 train_time:104793ms step_avg:56.34ms
step:1861/2160 train_time:104882ms step_avg:56.36ms
step:1862/2160 train_time:104968ms step_avg:56.37ms
step:1863/2160 train_time:105056ms step_avg:56.39ms
step:1864/2160 train_time:105143ms step_avg:56.41ms
step:1865/2160 train_time:105231ms step_avg:56.42ms
step:1866/2160 train_time:105319ms step_avg:56.44ms
step:1867/2160 train_time:105406ms step_avg:56.46ms
step:1868/2160 train_time:105494ms step_avg:56.47ms
step:1869/2160 train_time:105582ms step_avg:56.49ms
step:1870/2160 train_time:105668ms step_avg:56.51ms
step:1871/2160 train_time:105757ms step_avg:56.52ms
step:1872/2160 train_time:105844ms step_avg:56.54ms
step:1873/2160 train_time:105932ms step_avg:56.56ms
step:1874/2160 train_time:106019ms step_avg:56.57ms
step:1875/2160 train_time:106107ms step_avg:56.59ms
step:1876/2160 train_time:106193ms step_avg:56.61ms
step:1877/2160 train_time:106283ms step_avg:56.62ms
step:1878/2160 train_time:106370ms step_avg:56.64ms
step:1879/2160 train_time:106459ms step_avg:56.66ms
step:1880/2160 train_time:106545ms step_avg:56.67ms
step:1881/2160 train_time:106633ms step_avg:56.69ms
step:1882/2160 train_time:106720ms step_avg:56.71ms
step:1883/2160 train_time:106808ms step_avg:56.72ms
step:1884/2160 train_time:106895ms step_avg:56.74ms
step:1885/2160 train_time:106984ms step_avg:56.76ms
step:1886/2160 train_time:107071ms step_avg:56.77ms
step:1887/2160 train_time:107159ms step_avg:56.79ms
step:1888/2160 train_time:107245ms step_avg:56.80ms
step:1889/2160 train_time:107334ms step_avg:56.82ms
step:1890/2160 train_time:107421ms step_avg:56.84ms
step:1891/2160 train_time:107510ms step_avg:56.85ms
step:1892/2160 train_time:107596ms step_avg:56.87ms
step:1893/2160 train_time:107684ms step_avg:56.89ms
step:1894/2160 train_time:107771ms step_avg:56.90ms
step:1895/2160 train_time:107859ms step_avg:56.92ms
step:1896/2160 train_time:107945ms step_avg:56.93ms
step:1897/2160 train_time:108034ms step_avg:56.95ms
step:1898/2160 train_time:108120ms step_avg:56.97ms
step:1899/2160 train_time:108208ms step_avg:56.98ms
step:1900/2160 train_time:108295ms step_avg:57.00ms
step:1901/2160 train_time:108383ms step_avg:57.01ms
step:1902/2160 train_time:108470ms step_avg:57.03ms
step:1903/2160 train_time:108558ms step_avg:57.05ms
step:1904/2160 train_time:108645ms step_avg:57.06ms
step:1905/2160 train_time:108734ms step_avg:57.08ms
step:1906/2160 train_time:108821ms step_avg:57.09ms
step:1907/2160 train_time:108908ms step_avg:57.11ms
step:1908/2160 train_time:108995ms step_avg:57.13ms
step:1909/2160 train_time:109083ms step_avg:57.14ms
step:1910/2160 train_time:109170ms step_avg:57.16ms
step:1911/2160 train_time:109259ms step_avg:57.17ms
step:1912/2160 train_time:109345ms step_avg:57.19ms
step:1913/2160 train_time:109434ms step_avg:57.21ms
step:1914/2160 train_time:109520ms step_avg:57.22ms
step:1915/2160 train_time:109608ms step_avg:57.24ms
step:1916/2160 train_time:109695ms step_avg:57.25ms
step:1917/2160 train_time:109784ms step_avg:57.27ms
step:1918/2160 train_time:109871ms step_avg:57.28ms
step:1919/2160 train_time:109961ms step_avg:57.30ms
step:1920/2160 train_time:110047ms step_avg:57.32ms
step:1921/2160 train_time:110135ms step_avg:57.33ms
step:1922/2160 train_time:110222ms step_avg:57.35ms
step:1923/2160 train_time:110309ms step_avg:57.36ms
step:1924/2160 train_time:110396ms step_avg:57.38ms
step:1925/2160 train_time:110485ms step_avg:57.39ms
step:1926/2160 train_time:110571ms step_avg:57.41ms
step:1927/2160 train_time:110660ms step_avg:57.43ms
step:1928/2160 train_time:110746ms step_avg:57.44ms
step:1929/2160 train_time:110835ms step_avg:57.46ms
step:1930/2160 train_time:110921ms step_avg:57.47ms
step:1931/2160 train_time:111010ms step_avg:57.49ms
step:1932/2160 train_time:111096ms step_avg:57.50ms
step:1933/2160 train_time:111184ms step_avg:57.52ms
step:1934/2160 train_time:111270ms step_avg:57.53ms
step:1935/2160 train_time:111358ms step_avg:57.55ms
step:1936/2160 train_time:111444ms step_avg:57.56ms
step:1937/2160 train_time:111532ms step_avg:57.58ms
step:1938/2160 train_time:111620ms step_avg:57.60ms
step:1939/2160 train_time:111708ms step_avg:57.61ms
step:1940/2160 train_time:111795ms step_avg:57.63ms
step:1941/2160 train_time:111883ms step_avg:57.64ms
step:1942/2160 train_time:111969ms step_avg:57.66ms
step:1943/2160 train_time:112058ms step_avg:57.67ms
step:1944/2160 train_time:112144ms step_avg:57.69ms
step:1945/2160 train_time:112233ms step_avg:57.70ms
step:1946/2160 train_time:112321ms step_avg:57.72ms
step:1947/2160 train_time:112409ms step_avg:57.73ms
step:1948/2160 train_time:112495ms step_avg:57.75ms
step:1949/2160 train_time:112583ms step_avg:57.76ms
step:1950/2160 train_time:112669ms step_avg:57.78ms
step:1951/2160 train_time:112758ms step_avg:57.79ms
step:1952/2160 train_time:112844ms step_avg:57.81ms
step:1953/2160 train_time:112933ms step_avg:57.83ms
step:1954/2160 train_time:113021ms step_avg:57.84ms
step:1955/2160 train_time:113109ms step_avg:57.86ms
step:1956/2160 train_time:113195ms step_avg:57.87ms
step:1957/2160 train_time:113284ms step_avg:57.89ms
step:1958/2160 train_time:113371ms step_avg:57.90ms
step:1959/2160 train_time:113459ms step_avg:57.92ms
step:1960/2160 train_time:113545ms step_avg:57.93ms
step:1961/2160 train_time:113633ms step_avg:57.95ms
step:1962/2160 train_time:113720ms step_avg:57.96ms
step:1963/2160 train_time:113807ms step_avg:57.98ms
step:1964/2160 train_time:113894ms step_avg:57.99ms
step:1965/2160 train_time:113985ms step_avg:58.01ms
step:1966/2160 train_time:114071ms step_avg:58.02ms
step:1967/2160 train_time:114159ms step_avg:58.04ms
step:1968/2160 train_time:114246ms step_avg:58.05ms
step:1969/2160 train_time:114334ms step_avg:58.07ms
step:1970/2160 train_time:114420ms step_avg:58.08ms
step:1971/2160 train_time:114508ms step_avg:58.10ms
step:1972/2160 train_time:114595ms step_avg:58.11ms
step:1973/2160 train_time:114684ms step_avg:58.13ms
step:1974/2160 train_time:114770ms step_avg:58.14ms
step:1975/2160 train_time:114859ms step_avg:58.16ms
step:1976/2160 train_time:114945ms step_avg:58.17ms
step:1977/2160 train_time:115035ms step_avg:58.19ms
step:1978/2160 train_time:115121ms step_avg:58.20ms
step:1979/2160 train_time:115209ms step_avg:58.22ms
step:1980/2160 train_time:115295ms step_avg:58.23ms
step:1981/2160 train_time:115384ms step_avg:58.25ms
step:1982/2160 train_time:115471ms step_avg:58.26ms
step:1983/2160 train_time:115560ms step_avg:58.28ms
step:1984/2160 train_time:115646ms step_avg:58.29ms
step:1985/2160 train_time:115735ms step_avg:58.30ms
step:1986/2160 train_time:115822ms step_avg:58.32ms
step:1987/2160 train_time:115911ms step_avg:58.33ms
step:1988/2160 train_time:115998ms step_avg:58.35ms
step:1989/2160 train_time:116085ms step_avg:58.36ms
step:1990/2160 train_time:116173ms step_avg:58.38ms
step:1991/2160 train_time:116263ms step_avg:58.39ms
step:1992/2160 train_time:116349ms step_avg:58.41ms
step:1993/2160 train_time:116438ms step_avg:58.42ms
step:1994/2160 train_time:116524ms step_avg:58.44ms
step:1995/2160 train_time:116612ms step_avg:58.45ms
step:1996/2160 train_time:116699ms step_avg:58.47ms
step:1997/2160 train_time:116787ms step_avg:58.48ms
step:1998/2160 train_time:116874ms step_avg:58.50ms
step:1999/2160 train_time:116962ms step_avg:58.51ms
step:2000/2160 train_time:117049ms step_avg:58.52ms
step:2000/2160 val_loss:3.3128 train_time:117139ms step_avg:58.57ms
step:2001/2160 train_time:117163ms step_avg:58.55ms
step:2002/2160 train_time:117228ms step_avg:58.56ms
step:2003/2160 train_time:117321ms step_avg:58.57ms
step:2004/2160 train_time:117408ms step_avg:58.59ms
step:2005/2160 train_time:117498ms step_avg:58.60ms
step:2006/2160 train_time:117584ms step_avg:58.62ms
step:2007/2160 train_time:117671ms step_avg:58.63ms
step:2008/2160 train_time:117756ms step_avg:58.64ms
step:2009/2160 train_time:117843ms step_avg:58.66ms
step:2010/2160 train_time:117928ms step_avg:58.67ms
step:2011/2160 train_time:118016ms step_avg:58.69ms
step:2012/2160 train_time:118102ms step_avg:58.70ms
step:2013/2160 train_time:118193ms step_avg:58.71ms
step:2014/2160 train_time:118281ms step_avg:58.73ms
step:2015/2160 train_time:118373ms step_avg:58.75ms
step:2016/2160 train_time:118459ms step_avg:58.76ms
step:2017/2160 train_time:118548ms step_avg:58.77ms
step:2018/2160 train_time:118634ms step_avg:58.79ms
step:2019/2160 train_time:118721ms step_avg:58.80ms
step:2020/2160 train_time:118807ms step_avg:58.82ms
step:2021/2160 train_time:118897ms step_avg:58.83ms
step:2022/2160 train_time:118983ms step_avg:58.84ms
step:2023/2160 train_time:119071ms step_avg:58.86ms
step:2024/2160 train_time:119159ms step_avg:58.87ms
step:2025/2160 train_time:119249ms step_avg:58.89ms
step:2026/2160 train_time:119337ms step_avg:58.90ms
step:2027/2160 train_time:119427ms step_avg:58.92ms
step:2028/2160 train_time:119514ms step_avg:58.93ms
step:2029/2160 train_time:119601ms step_avg:58.95ms
step:2030/2160 train_time:119688ms step_avg:58.96ms
step:2031/2160 train_time:119777ms step_avg:58.97ms
step:2032/2160 train_time:119863ms step_avg:58.99ms
step:2033/2160 train_time:119950ms step_avg:59.00ms
step:2034/2160 train_time:120036ms step_avg:59.01ms
step:2035/2160 train_time:120123ms step_avg:59.03ms
step:2036/2160 train_time:120211ms step_avg:59.04ms
step:2037/2160 train_time:120300ms step_avg:59.06ms
step:2038/2160 train_time:120388ms step_avg:59.07ms
step:2039/2160 train_time:120477ms step_avg:59.09ms
step:2040/2160 train_time:120563ms step_avg:59.10ms
step:2041/2160 train_time:120652ms step_avg:59.11ms
step:2042/2160 train_time:120738ms step_avg:59.13ms
step:2043/2160 train_time:120825ms step_avg:59.14ms
step:2044/2160 train_time:120911ms step_avg:59.15ms
step:2045/2160 train_time:120998ms step_avg:59.17ms
step:2046/2160 train_time:121085ms step_avg:59.18ms
step:2047/2160 train_time:121174ms step_avg:59.20ms
step:2048/2160 train_time:121260ms step_avg:59.21ms
step:2049/2160 train_time:121350ms step_avg:59.22ms
step:2050/2160 train_time:121437ms step_avg:59.24ms
step:2051/2160 train_time:121525ms step_avg:59.25ms
step:2052/2160 train_time:121614ms step_avg:59.27ms
step:2053/2160 train_time:121702ms step_avg:59.28ms
step:2054/2160 train_time:121788ms step_avg:59.29ms
step:2055/2160 train_time:121876ms step_avg:59.31ms
step:2056/2160 train_time:121962ms step_avg:59.32ms
step:2057/2160 train_time:122050ms step_avg:59.33ms
step:2058/2160 train_time:122137ms step_avg:59.35ms
step:2059/2160 train_time:122224ms step_avg:59.36ms
step:2060/2160 train_time:122311ms step_avg:59.37ms
step:2061/2160 train_time:122399ms step_avg:59.39ms
step:2062/2160 train_time:122487ms step_avg:59.40ms
step:2063/2160 train_time:122576ms step_avg:59.42ms
step:2064/2160 train_time:122662ms step_avg:59.43ms
step:2065/2160 train_time:122751ms step_avg:59.44ms
step:2066/2160 train_time:122837ms step_avg:59.46ms
step:2067/2160 train_time:122924ms step_avg:59.47ms
step:2068/2160 train_time:123011ms step_avg:59.48ms
step:2069/2160 train_time:123098ms step_avg:59.50ms
step:2070/2160 train_time:123185ms step_avg:59.51ms
step:2071/2160 train_time:123273ms step_avg:59.52ms
step:2072/2160 train_time:123359ms step_avg:59.54ms
step:2073/2160 train_time:123448ms step_avg:59.55ms
step:2074/2160 train_time:123535ms step_avg:59.56ms
step:2075/2160 train_time:123623ms step_avg:59.58ms
step:2076/2160 train_time:123710ms step_avg:59.59ms
step:2077/2160 train_time:123798ms step_avg:59.60ms
step:2078/2160 train_time:123884ms step_avg:59.62ms
step:2079/2160 train_time:123972ms step_avg:59.63ms
step:2080/2160 train_time:124058ms step_avg:59.64ms
step:2081/2160 train_time:124146ms step_avg:59.66ms
step:2082/2160 train_time:124233ms step_avg:59.67ms
step:2083/2160 train_time:124321ms step_avg:59.68ms
step:2084/2160 train_time:124409ms step_avg:59.70ms
step:2085/2160 train_time:124497ms step_avg:59.71ms
step:2086/2160 train_time:124584ms step_avg:59.72ms
step:2087/2160 train_time:124672ms step_avg:59.74ms
step:2088/2160 train_time:124758ms step_avg:59.75ms
step:2089/2160 train_time:124846ms step_avg:59.76ms
step:2090/2160 train_time:124933ms step_avg:59.78ms
step:2091/2160 train_time:125021ms step_avg:59.79ms
step:2092/2160 train_time:125108ms step_avg:59.80ms
step:2093/2160 train_time:125196ms step_avg:59.82ms
step:2094/2160 train_time:125282ms step_avg:59.83ms
step:2095/2160 train_time:125371ms step_avg:59.84ms
step:2096/2160 train_time:125457ms step_avg:59.86ms
step:2097/2160 train_time:125546ms step_avg:59.87ms
step:2098/2160 train_time:125633ms step_avg:59.88ms
step:2099/2160 train_time:125720ms step_avg:59.90ms
step:2100/2160 train_time:125807ms step_avg:59.91ms
step:2101/2160 train_time:125895ms step_avg:59.92ms
step:2102/2160 train_time:125982ms step_avg:59.93ms
step:2103/2160 train_time:126070ms step_avg:59.95ms
step:2104/2160 train_time:126156ms step_avg:59.96ms
step:2105/2160 train_time:126245ms step_avg:59.97ms
step:2106/2160 train_time:126332ms step_avg:59.99ms
step:2107/2160 train_time:126419ms step_avg:60.00ms
step:2108/2160 train_time:126506ms step_avg:60.01ms
step:2109/2160 train_time:126595ms step_avg:60.03ms
step:2110/2160 train_time:126681ms step_avg:60.04ms
step:2111/2160 train_time:126770ms step_avg:60.05ms
step:2112/2160 train_time:126857ms step_avg:60.07ms
step:2113/2160 train_time:126945ms step_avg:60.08ms
step:2114/2160 train_time:127033ms step_avg:60.09ms
step:2115/2160 train_time:127120ms step_avg:60.10ms
step:2116/2160 train_time:127206ms step_avg:60.12ms
step:2117/2160 train_time:127296ms step_avg:60.13ms
step:2118/2160 train_time:127383ms step_avg:60.14ms
step:2119/2160 train_time:127471ms step_avg:60.16ms
step:2120/2160 train_time:127558ms step_avg:60.17ms
step:2121/2160 train_time:127646ms step_avg:60.18ms
step:2122/2160 train_time:127733ms step_avg:60.19ms
step:2123/2160 train_time:127821ms step_avg:60.21ms
step:2124/2160 train_time:127908ms step_avg:60.22ms
step:2125/2160 train_time:127997ms step_avg:60.23ms
step:2126/2160 train_time:128083ms step_avg:60.25ms
step:2127/2160 train_time:128172ms step_avg:60.26ms
step:2128/2160 train_time:128258ms step_avg:60.27ms
step:2129/2160 train_time:128348ms step_avg:60.29ms
step:2130/2160 train_time:128435ms step_avg:60.30ms
step:2131/2160 train_time:128523ms step_avg:60.31ms
step:2132/2160 train_time:128610ms step_avg:60.32ms
step:2133/2160 train_time:128699ms step_avg:60.34ms
step:2134/2160 train_time:128786ms step_avg:60.35ms
step:2135/2160 train_time:128876ms step_avg:60.36ms
step:2136/2160 train_time:128961ms step_avg:60.38ms
step:2137/2160 train_time:129050ms step_avg:60.39ms
step:2138/2160 train_time:129137ms step_avg:60.40ms
step:2139/2160 train_time:129224ms step_avg:60.41ms
step:2140/2160 train_time:129313ms step_avg:60.43ms
step:2141/2160 train_time:129401ms step_avg:60.44ms
step:2142/2160 train_time:129487ms step_avg:60.45ms
step:2143/2160 train_time:129577ms step_avg:60.47ms
step:2144/2160 train_time:129663ms step_avg:60.48ms
step:2145/2160 train_time:129752ms step_avg:60.49ms
step:2146/2160 train_time:129838ms step_avg:60.50ms
step:2147/2160 train_time:129926ms step_avg:60.52ms
step:2148/2160 train_time:130014ms step_avg:60.53ms
step:2149/2160 train_time:130102ms step_avg:60.54ms
step:2150/2160 train_time:130189ms step_avg:60.55ms
step:2151/2160 train_time:130278ms step_avg:60.57ms
step:2152/2160 train_time:130365ms step_avg:60.58ms
step:2153/2160 train_time:130454ms step_avg:60.59ms
step:2154/2160 train_time:130539ms step_avg:60.60ms
step:2155/2160 train_time:130628ms step_avg:60.62ms
step:2156/2160 train_time:130715ms step_avg:60.63ms
step:2157/2160 train_time:130803ms step_avg:60.64ms
step:2158/2160 train_time:130890ms step_avg:60.65ms
step:2159/2160 train_time:130979ms step_avg:60.67ms
step:2160/2160 train_time:131067ms step_avg:60.68ms
step:2160/2160 val_loss:3.2757 train_time:131157ms step_avg:60.72ms
peak memory allocated: 29816 MiB reserved: 44176 MiB

import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:18:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   43C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            129W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           38671      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           38672      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           38673      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           38674      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           38675      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           38676      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           38677      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           38678      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           38672      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           38673      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           38674      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           38675      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           38676      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           38677      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           38678      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:96ms step_avg:95.60ms
step:2/2160 train_time:122ms step_avg:61.20ms
step:3/2160 train_time:146ms step_avg:48.56ms
step:4/2160 train_time:170ms step_avg:42.61ms
step:5/2160 train_time:201ms step_avg:40.28ms
step:6/2160 train_time:331ms step_avg:55.15ms
step:7/2160 train_time:384ms step_avg:54.79ms
step:8/2160 train_time:416ms step_avg:52.00ms
step:9/2160 train_time:449ms step_avg:49.86ms
step:10/2160 train_time:481ms step_avg:48.11ms
step:11/2160 train_time:514ms step_avg:46.74ms
step:12/2160 train_time:547ms step_avg:45.56ms
step:13/2160 train_time:580ms step_avg:44.58ms
step:14/2160 train_time:612ms step_avg:43.71ms
step:15/2160 train_time:645ms step_avg:43.01ms
step:16/2160 train_time:678ms step_avg:42.36ms
step:17/2160 train_time:711ms step_avg:41.84ms
step:18/2160 train_time:744ms step_avg:41.34ms
step:19/2160 train_time:777ms step_avg:40.91ms
step:20/2160 train_time:810ms step_avg:40.50ms
step:21/2160 train_time:843ms step_avg:40.14ms
step:22/2160 train_time:875ms step_avg:39.79ms
step:23/2160 train_time:909ms step_avg:39.51ms
step:24/2160 train_time:941ms step_avg:39.22ms
step:25/2160 train_time:975ms step_avg:38.99ms
step:26/2160 train_time:1007ms step_avg:38.74ms
step:27/2160 train_time:1040ms step_avg:38.53ms
step:28/2160 train_time:1073ms step_avg:38.31ms
step:29/2160 train_time:1106ms step_avg:38.14ms
step:30/2160 train_time:1139ms step_avg:37.96ms
step:31/2160 train_time:1172ms step_avg:37.80ms
step:32/2160 train_time:1204ms step_avg:37.64ms
step:33/2160 train_time:1238ms step_avg:37.51ms
step:34/2160 train_time:1270ms step_avg:37.36ms
step:35/2160 train_time:1305ms step_avg:37.29ms
step:36/2160 train_time:1338ms step_avg:37.17ms
step:37/2160 train_time:1373ms step_avg:37.10ms
step:38/2160 train_time:1405ms step_avg:36.98ms
step:39/2160 train_time:1439ms step_avg:36.91ms
step:40/2160 train_time:1472ms step_avg:36.79ms
step:41/2160 train_time:1506ms step_avg:36.72ms
step:42/2160 train_time:1539ms step_avg:36.63ms
step:43/2160 train_time:1572ms step_avg:36.55ms
step:44/2160 train_time:1605ms step_avg:36.47ms
step:45/2160 train_time:1638ms step_avg:36.40ms
step:46/2160 train_time:1670ms step_avg:36.32ms
step:47/2160 train_time:1704ms step_avg:36.25ms
step:48/2160 train_time:1737ms step_avg:36.18ms
step:49/2160 train_time:1770ms step_avg:36.12ms
step:50/2160 train_time:1803ms step_avg:36.05ms
step:51/2160 train_time:1836ms step_avg:35.99ms
step:52/2160 train_time:1868ms step_avg:35.93ms
step:53/2160 train_time:1901ms step_avg:35.88ms
step:54/2160 train_time:1934ms step_avg:35.82ms
step:55/2160 train_time:1967ms step_avg:35.76ms
step:56/2160 train_time:1999ms step_avg:35.70ms
step:57/2160 train_time:2033ms step_avg:35.67ms
step:58/2160 train_time:2066ms step_avg:35.61ms
step:59/2160 train_time:2099ms step_avg:35.57ms
step:60/2160 train_time:2131ms step_avg:35.51ms
step:61/2160 train_time:2164ms step_avg:35.48ms
step:62/2160 train_time:2197ms step_avg:35.43ms
step:63/2160 train_time:2231ms step_avg:35.42ms
step:64/2160 train_time:2264ms step_avg:35.37ms
step:65/2160 train_time:2298ms step_avg:35.35ms
step:66/2160 train_time:2331ms step_avg:35.31ms
step:67/2160 train_time:2365ms step_avg:35.29ms
step:68/2160 train_time:2397ms step_avg:35.25ms
step:69/2160 train_time:2430ms step_avg:35.22ms
step:70/2160 train_time:2463ms step_avg:35.18ms
step:71/2160 train_time:2497ms step_avg:35.17ms
step:72/2160 train_time:2530ms step_avg:35.13ms
step:73/2160 train_time:2563ms step_avg:35.11ms
step:74/2160 train_time:2596ms step_avg:35.08ms
step:75/2160 train_time:2629ms step_avg:35.05ms
step:76/2160 train_time:2661ms step_avg:35.02ms
step:77/2160 train_time:2694ms step_avg:34.99ms
step:78/2160 train_time:2727ms step_avg:34.96ms
step:79/2160 train_time:2760ms step_avg:34.94ms
step:80/2160 train_time:2792ms step_avg:34.90ms
step:81/2160 train_time:2826ms step_avg:34.88ms
step:82/2160 train_time:2858ms step_avg:34.86ms
step:83/2160 train_time:2891ms step_avg:34.84ms
step:84/2160 train_time:2924ms step_avg:34.81ms
step:85/2160 train_time:2957ms step_avg:34.79ms
step:86/2160 train_time:2990ms step_avg:34.76ms
step:87/2160 train_time:3023ms step_avg:34.74ms
step:88/2160 train_time:3055ms step_avg:34.72ms
step:89/2160 train_time:3088ms step_avg:34.70ms
step:90/2160 train_time:3121ms step_avg:34.68ms
step:91/2160 train_time:3155ms step_avg:34.66ms
step:92/2160 train_time:3187ms step_avg:34.65ms
step:93/2160 train_time:3220ms step_avg:34.63ms
step:94/2160 train_time:3253ms step_avg:34.61ms
step:95/2160 train_time:3286ms step_avg:34.59ms
step:96/2160 train_time:3319ms step_avg:34.57ms
step:97/2160 train_time:3352ms step_avg:34.56ms
step:98/2160 train_time:3385ms step_avg:34.54ms
step:99/2160 train_time:3418ms step_avg:34.53ms
step:100/2160 train_time:3451ms step_avg:34.51ms
step:101/2160 train_time:3484ms step_avg:34.50ms
step:102/2160 train_time:3517ms step_avg:34.48ms
step:103/2160 train_time:3551ms step_avg:34.48ms
step:104/2160 train_time:3584ms step_avg:34.46ms
step:105/2160 train_time:3617ms step_avg:34.45ms
step:106/2160 train_time:3650ms step_avg:34.43ms
step:107/2160 train_time:3683ms step_avg:34.42ms
step:108/2160 train_time:3715ms step_avg:34.40ms
step:109/2160 train_time:3748ms step_avg:34.39ms
step:110/2160 train_time:3781ms step_avg:34.37ms
step:111/2160 train_time:3814ms step_avg:34.36ms
step:112/2160 train_time:3847ms step_avg:34.35ms
step:113/2160 train_time:3880ms step_avg:34.34ms
step:114/2160 train_time:3913ms step_avg:34.32ms
step:115/2160 train_time:3946ms step_avg:34.32ms
step:116/2160 train_time:3979ms step_avg:34.30ms
step:117/2160 train_time:4013ms step_avg:34.29ms
step:118/2160 train_time:4045ms step_avg:34.28ms
step:119/2160 train_time:4079ms step_avg:34.27ms
step:120/2160 train_time:4111ms step_avg:34.26ms
step:121/2160 train_time:4144ms step_avg:34.25ms
step:122/2160 train_time:4177ms step_avg:34.23ms
step:123/2160 train_time:4210ms step_avg:34.23ms
step:124/2160 train_time:4243ms step_avg:34.22ms
step:125/2160 train_time:4276ms step_avg:34.21ms
step:126/2160 train_time:4309ms step_avg:34.20ms
step:127/2160 train_time:4342ms step_avg:34.19ms
step:128/2160 train_time:4375ms step_avg:34.18ms
step:129/2160 train_time:4407ms step_avg:34.17ms
step:130/2160 train_time:4440ms step_avg:34.15ms
step:131/2160 train_time:4473ms step_avg:34.15ms
step:132/2160 train_time:4505ms step_avg:34.13ms
step:133/2160 train_time:4539ms step_avg:34.13ms
step:134/2160 train_time:4572ms step_avg:34.12ms
step:135/2160 train_time:4605ms step_avg:34.11ms
step:136/2160 train_time:4638ms step_avg:34.10ms
step:137/2160 train_time:4671ms step_avg:34.09ms
step:138/2160 train_time:4703ms step_avg:34.08ms
step:139/2160 train_time:4736ms step_avg:34.07ms
step:140/2160 train_time:4769ms step_avg:34.06ms
step:141/2160 train_time:4802ms step_avg:34.06ms
step:142/2160 train_time:4834ms step_avg:34.04ms
step:143/2160 train_time:4867ms step_avg:34.04ms
step:144/2160 train_time:4900ms step_avg:34.03ms
step:145/2160 train_time:4933ms step_avg:34.02ms
step:146/2160 train_time:4965ms step_avg:34.01ms
step:147/2160 train_time:4999ms step_avg:34.00ms
step:148/2160 train_time:5031ms step_avg:33.99ms
step:149/2160 train_time:5064ms step_avg:33.99ms
step:150/2160 train_time:5097ms step_avg:33.98ms
step:151/2160 train_time:5130ms step_avg:33.97ms
step:152/2160 train_time:5162ms step_avg:33.96ms
step:153/2160 train_time:5196ms step_avg:33.96ms
step:154/2160 train_time:5228ms step_avg:33.95ms
step:155/2160 train_time:5261ms step_avg:33.94ms
step:156/2160 train_time:5294ms step_avg:33.93ms
step:157/2160 train_time:5327ms step_avg:33.93ms
step:158/2160 train_time:5359ms step_avg:33.92ms
step:159/2160 train_time:5393ms step_avg:33.92ms
step:160/2160 train_time:5425ms step_avg:33.91ms
step:161/2160 train_time:5459ms step_avg:33.91ms
step:162/2160 train_time:5491ms step_avg:33.90ms
step:163/2160 train_time:5524ms step_avg:33.89ms
step:164/2160 train_time:5557ms step_avg:33.88ms
step:165/2160 train_time:5590ms step_avg:33.88ms
step:166/2160 train_time:5622ms step_avg:33.87ms
step:167/2160 train_time:5655ms step_avg:33.87ms
step:168/2160 train_time:5688ms step_avg:33.86ms
step:169/2160 train_time:5721ms step_avg:33.85ms
step:170/2160 train_time:5753ms step_avg:33.84ms
step:171/2160 train_time:5786ms step_avg:33.84ms
step:172/2160 train_time:5818ms step_avg:33.83ms
step:173/2160 train_time:5851ms step_avg:33.82ms
step:174/2160 train_time:5884ms step_avg:33.81ms
step:175/2160 train_time:5917ms step_avg:33.81ms
step:176/2160 train_time:5949ms step_avg:33.80ms
step:177/2160 train_time:5982ms step_avg:33.80ms
step:178/2160 train_time:6014ms step_avg:33.79ms
step:179/2160 train_time:6048ms step_avg:33.79ms
step:180/2160 train_time:6080ms step_avg:33.78ms
step:181/2160 train_time:6113ms step_avg:33.78ms
step:182/2160 train_time:6146ms step_avg:33.77ms
step:183/2160 train_time:6179ms step_avg:33.77ms
step:184/2160 train_time:6211ms step_avg:33.76ms
step:185/2160 train_time:6245ms step_avg:33.75ms
step:186/2160 train_time:6277ms step_avg:33.75ms
step:187/2160 train_time:6310ms step_avg:33.75ms
step:188/2160 train_time:6343ms step_avg:33.74ms
step:189/2160 train_time:6376ms step_avg:33.74ms
step:190/2160 train_time:6408ms step_avg:33.73ms
step:191/2160 train_time:6441ms step_avg:33.72ms
step:192/2160 train_time:6474ms step_avg:33.72ms
step:193/2160 train_time:6507ms step_avg:33.72ms
step:194/2160 train_time:6539ms step_avg:33.71ms
step:195/2160 train_time:6573ms step_avg:33.71ms
step:196/2160 train_time:6605ms step_avg:33.70ms
step:197/2160 train_time:6639ms step_avg:33.70ms
step:198/2160 train_time:6671ms step_avg:33.69ms
step:199/2160 train_time:6704ms step_avg:33.69ms
step:200/2160 train_time:6737ms step_avg:33.68ms
step:201/2160 train_time:6770ms step_avg:33.68ms
step:202/2160 train_time:6802ms step_avg:33.67ms
step:203/2160 train_time:6835ms step_avg:33.67ms
step:204/2160 train_time:6868ms step_avg:33.66ms
step:205/2160 train_time:6901ms step_avg:33.66ms
step:206/2160 train_time:6933ms step_avg:33.66ms
step:207/2160 train_time:6967ms step_avg:33.66ms
step:208/2160 train_time:6999ms step_avg:33.65ms
step:209/2160 train_time:7032ms step_avg:33.65ms
step:210/2160 train_time:7065ms step_avg:33.64ms
step:211/2160 train_time:7098ms step_avg:33.64ms
step:212/2160 train_time:7131ms step_avg:33.64ms
step:213/2160 train_time:7164ms step_avg:33.63ms
step:214/2160 train_time:7196ms step_avg:33.63ms
step:215/2160 train_time:7229ms step_avg:33.62ms
step:216/2160 train_time:7261ms step_avg:33.62ms
step:217/2160 train_time:7295ms step_avg:33.62ms
step:218/2160 train_time:7327ms step_avg:33.61ms
step:219/2160 train_time:7360ms step_avg:33.61ms
step:220/2160 train_time:7393ms step_avg:33.60ms
step:221/2160 train_time:7426ms step_avg:33.60ms
step:222/2160 train_time:7458ms step_avg:33.59ms
step:223/2160 train_time:7491ms step_avg:33.59ms
step:224/2160 train_time:7523ms step_avg:33.59ms
step:225/2160 train_time:7557ms step_avg:33.59ms
step:226/2160 train_time:7589ms step_avg:33.58ms
step:227/2160 train_time:7622ms step_avg:33.58ms
step:228/2160 train_time:7654ms step_avg:33.57ms
step:229/2160 train_time:7687ms step_avg:33.57ms
step:230/2160 train_time:7720ms step_avg:33.56ms
step:231/2160 train_time:7753ms step_avg:33.56ms
step:232/2160 train_time:7786ms step_avg:33.56ms
step:233/2160 train_time:7819ms step_avg:33.56ms
step:234/2160 train_time:7851ms step_avg:33.55ms
step:235/2160 train_time:7884ms step_avg:33.55ms
step:236/2160 train_time:7917ms step_avg:33.55ms
step:237/2160 train_time:7950ms step_avg:33.55ms
step:238/2160 train_time:7982ms step_avg:33.54ms
step:239/2160 train_time:8016ms step_avg:33.54ms
step:240/2160 train_time:8048ms step_avg:33.54ms
step:241/2160 train_time:8081ms step_avg:33.53ms
step:242/2160 train_time:8114ms step_avg:33.53ms
step:243/2160 train_time:8147ms step_avg:33.53ms
step:244/2160 train_time:8179ms step_avg:33.52ms
step:245/2160 train_time:8213ms step_avg:33.52ms
step:246/2160 train_time:8245ms step_avg:33.52ms
step:247/2160 train_time:8278ms step_avg:33.52ms
step:248/2160 train_time:8310ms step_avg:33.51ms
step:249/2160 train_time:8344ms step_avg:33.51ms
step:250/2160 train_time:8376ms step_avg:33.50ms
step:250/2160 val_loss:4.3133 train_time:8411ms step_avg:33.64ms
step:251/2160 train_time:8434ms step_avg:33.60ms
step:252/2160 train_time:8457ms step_avg:33.56ms
step:253/2160 train_time:8480ms step_avg:33.52ms
step:254/2160 train_time:8512ms step_avg:33.51ms
step:255/2160 train_time:8548ms step_avg:33.52ms
step:256/2160 train_time:8581ms step_avg:33.52ms
step:257/2160 train_time:8616ms step_avg:33.53ms
step:258/2160 train_time:8649ms step_avg:33.52ms
step:259/2160 train_time:8682ms step_avg:33.52ms
step:260/2160 train_time:8714ms step_avg:33.52ms
step:261/2160 train_time:8748ms step_avg:33.52ms
step:262/2160 train_time:8781ms step_avg:33.51ms
step:263/2160 train_time:8813ms step_avg:33.51ms
step:264/2160 train_time:8846ms step_avg:33.51ms
step:265/2160 train_time:8879ms step_avg:33.50ms
step:266/2160 train_time:8911ms step_avg:33.50ms
step:267/2160 train_time:8944ms step_avg:33.50ms
step:268/2160 train_time:8976ms step_avg:33.49ms
step:269/2160 train_time:9009ms step_avg:33.49ms
step:270/2160 train_time:9041ms step_avg:33.49ms
step:271/2160 train_time:9074ms step_avg:33.48ms
step:272/2160 train_time:9106ms step_avg:33.48ms
step:273/2160 train_time:9139ms step_avg:33.48ms
step:274/2160 train_time:9171ms step_avg:33.47ms
step:275/2160 train_time:9204ms step_avg:33.47ms
step:276/2160 train_time:9236ms step_avg:33.46ms
step:277/2160 train_time:9269ms step_avg:33.46ms
step:278/2160 train_time:9301ms step_avg:33.46ms
step:279/2160 train_time:9334ms step_avg:33.46ms
step:280/2160 train_time:9367ms step_avg:33.45ms
step:281/2160 train_time:9400ms step_avg:33.45ms
step:282/2160 train_time:9432ms step_avg:33.45ms
step:283/2160 train_time:9465ms step_avg:33.45ms
step:284/2160 train_time:9498ms step_avg:33.44ms
step:285/2160 train_time:9531ms step_avg:33.44ms
step:286/2160 train_time:9564ms step_avg:33.44ms
step:287/2160 train_time:9598ms step_avg:33.44ms
step:288/2160 train_time:9630ms step_avg:33.44ms
step:289/2160 train_time:9664ms step_avg:33.44ms
step:290/2160 train_time:9696ms step_avg:33.43ms
step:291/2160 train_time:9730ms step_avg:33.44ms
step:292/2160 train_time:9762ms step_avg:33.43ms
step:293/2160 train_time:9796ms step_avg:33.43ms
step:294/2160 train_time:9828ms step_avg:33.43ms
step:295/2160 train_time:9861ms step_avg:33.43ms
step:296/2160 train_time:9893ms step_avg:33.42ms
step:297/2160 train_time:9926ms step_avg:33.42ms
step:298/2160 train_time:9959ms step_avg:33.42ms
step:299/2160 train_time:9992ms step_avg:33.42ms
step:300/2160 train_time:10024ms step_avg:33.41ms
step:301/2160 train_time:10057ms step_avg:33.41ms
step:302/2160 train_time:10090ms step_avg:33.41ms
step:303/2160 train_time:10123ms step_avg:33.41ms
step:304/2160 train_time:10155ms step_avg:33.40ms
step:305/2160 train_time:10188ms step_avg:33.40ms
step:306/2160 train_time:10220ms step_avg:33.40ms
step:307/2160 train_time:10253ms step_avg:33.40ms
step:308/2160 train_time:10285ms step_avg:33.39ms
step:309/2160 train_time:10318ms step_avg:33.39ms
step:310/2160 train_time:10351ms step_avg:33.39ms
step:311/2160 train_time:10383ms step_avg:33.39ms
step:312/2160 train_time:10416ms step_avg:33.38ms
step:313/2160 train_time:10449ms step_avg:33.38ms
step:314/2160 train_time:10481ms step_avg:33.38ms
step:315/2160 train_time:10515ms step_avg:33.38ms
step:316/2160 train_time:10547ms step_avg:33.38ms
step:317/2160 train_time:10581ms step_avg:33.38ms
step:318/2160 train_time:10613ms step_avg:33.37ms
step:319/2160 train_time:10647ms step_avg:33.38ms
step:320/2160 train_time:10679ms step_avg:33.37ms
step:321/2160 train_time:10713ms step_avg:33.37ms
step:322/2160 train_time:10745ms step_avg:33.37ms
step:323/2160 train_time:10778ms step_avg:33.37ms
step:324/2160 train_time:10810ms step_avg:33.36ms
step:325/2160 train_time:10843ms step_avg:33.36ms
step:326/2160 train_time:10875ms step_avg:33.36ms
step:327/2160 train_time:10908ms step_avg:33.36ms
step:328/2160 train_time:10941ms step_avg:33.36ms
step:329/2160 train_time:10974ms step_avg:33.36ms
step:330/2160 train_time:11007ms step_avg:33.35ms
step:331/2160 train_time:11040ms step_avg:33.35ms
step:332/2160 train_time:11072ms step_avg:33.35ms
step:333/2160 train_time:11105ms step_avg:33.35ms
step:334/2160 train_time:11137ms step_avg:33.35ms
step:335/2160 train_time:11170ms step_avg:33.34ms
step:336/2160 train_time:11203ms step_avg:33.34ms
step:337/2160 train_time:11236ms step_avg:33.34ms
step:338/2160 train_time:11268ms step_avg:33.34ms
step:339/2160 train_time:11301ms step_avg:33.34ms
step:340/2160 train_time:11333ms step_avg:33.33ms
step:341/2160 train_time:11366ms step_avg:33.33ms
step:342/2160 train_time:11398ms step_avg:33.33ms
step:343/2160 train_time:11432ms step_avg:33.33ms
step:344/2160 train_time:11464ms step_avg:33.33ms
step:345/2160 train_time:11497ms step_avg:33.32ms
step:346/2160 train_time:11529ms step_avg:33.32ms
step:347/2160 train_time:11562ms step_avg:33.32ms
step:348/2160 train_time:11594ms step_avg:33.32ms
step:349/2160 train_time:11628ms step_avg:33.32ms
step:350/2160 train_time:11660ms step_avg:33.32ms
step:351/2160 train_time:11693ms step_avg:33.31ms
step:352/2160 train_time:11726ms step_avg:33.31ms
step:353/2160 train_time:11759ms step_avg:33.31ms
step:354/2160 train_time:11791ms step_avg:33.31ms
step:355/2160 train_time:11824ms step_avg:33.31ms
step:356/2160 train_time:11857ms step_avg:33.30ms
step:357/2160 train_time:11890ms step_avg:33.30ms
step:358/2160 train_time:11922ms step_avg:33.30ms
step:359/2160 train_time:11956ms step_avg:33.30ms
step:360/2160 train_time:11988ms step_avg:33.30ms
step:361/2160 train_time:12022ms step_avg:33.30ms
step:362/2160 train_time:12054ms step_avg:33.30ms
step:363/2160 train_time:12087ms step_avg:33.30ms
step:364/2160 train_time:12119ms step_avg:33.29ms
step:365/2160 train_time:12153ms step_avg:33.30ms
step:366/2160 train_time:12185ms step_avg:33.29ms
step:367/2160 train_time:12218ms step_avg:33.29ms
step:368/2160 train_time:12251ms step_avg:33.29ms
step:369/2160 train_time:12283ms step_avg:33.29ms
step:370/2160 train_time:12316ms step_avg:33.29ms
step:371/2160 train_time:12349ms step_avg:33.29ms
step:372/2160 train_time:12381ms step_avg:33.28ms
step:373/2160 train_time:12415ms step_avg:33.28ms
step:374/2160 train_time:12447ms step_avg:33.28ms
step:375/2160 train_time:12480ms step_avg:33.28ms
step:376/2160 train_time:12513ms step_avg:33.28ms
step:377/2160 train_time:12546ms step_avg:33.28ms
step:378/2160 train_time:12578ms step_avg:33.28ms
step:379/2160 train_time:12611ms step_avg:33.27ms
step:380/2160 train_time:12643ms step_avg:33.27ms
step:381/2160 train_time:12677ms step_avg:33.27ms
step:382/2160 train_time:12709ms step_avg:33.27ms
step:383/2160 train_time:12742ms step_avg:33.27ms
step:384/2160 train_time:12775ms step_avg:33.27ms
step:385/2160 train_time:12808ms step_avg:33.27ms
step:386/2160 train_time:12840ms step_avg:33.26ms
step:387/2160 train_time:12874ms step_avg:33.27ms
step:388/2160 train_time:12906ms step_avg:33.26ms
step:389/2160 train_time:12939ms step_avg:33.26ms
step:390/2160 train_time:12972ms step_avg:33.26ms
step:391/2160 train_time:13005ms step_avg:33.26ms
step:392/2160 train_time:13037ms step_avg:33.26ms
step:393/2160 train_time:13070ms step_avg:33.26ms
step:394/2160 train_time:13103ms step_avg:33.26ms
step:395/2160 train_time:13136ms step_avg:33.26ms
step:396/2160 train_time:13169ms step_avg:33.25ms
step:397/2160 train_time:13202ms step_avg:33.25ms
step:398/2160 train_time:13234ms step_avg:33.25ms
step:399/2160 train_time:13267ms step_avg:33.25ms
step:400/2160 train_time:13300ms step_avg:33.25ms
step:401/2160 train_time:13333ms step_avg:33.25ms
step:402/2160 train_time:13365ms step_avg:33.25ms
step:403/2160 train_time:13399ms step_avg:33.25ms
step:404/2160 train_time:13431ms step_avg:33.24ms
step:405/2160 train_time:13464ms step_avg:33.24ms
step:406/2160 train_time:13496ms step_avg:33.24ms
step:407/2160 train_time:13529ms step_avg:33.24ms
step:408/2160 train_time:13561ms step_avg:33.24ms
step:409/2160 train_time:13595ms step_avg:33.24ms
step:410/2160 train_time:13627ms step_avg:33.24ms
step:411/2160 train_time:13660ms step_avg:33.24ms
step:412/2160 train_time:13693ms step_avg:33.23ms
step:413/2160 train_time:13726ms step_avg:33.23ms
step:414/2160 train_time:13758ms step_avg:33.23ms
step:415/2160 train_time:13791ms step_avg:33.23ms
step:416/2160 train_time:13824ms step_avg:33.23ms
step:417/2160 train_time:13857ms step_avg:33.23ms
step:418/2160 train_time:13889ms step_avg:33.23ms
step:419/2160 train_time:13922ms step_avg:33.23ms
step:420/2160 train_time:13954ms step_avg:33.22ms
step:421/2160 train_time:13988ms step_avg:33.23ms
step:422/2160 train_time:14020ms step_avg:33.22ms
step:423/2160 train_time:14053ms step_avg:33.22ms
step:424/2160 train_time:14086ms step_avg:33.22ms
step:425/2160 train_time:14119ms step_avg:33.22ms
step:426/2160 train_time:14151ms step_avg:33.22ms
step:427/2160 train_time:14185ms step_avg:33.22ms
step:428/2160 train_time:14217ms step_avg:33.22ms
step:429/2160 train_time:14250ms step_avg:33.22ms
step:430/2160 train_time:14283ms step_avg:33.22ms
step:431/2160 train_time:14316ms step_avg:33.22ms
step:432/2160 train_time:14349ms step_avg:33.21ms
step:433/2160 train_time:14382ms step_avg:33.21ms
step:434/2160 train_time:14414ms step_avg:33.21ms
step:435/2160 train_time:14447ms step_avg:33.21ms
step:436/2160 train_time:14480ms step_avg:33.21ms
step:437/2160 train_time:14513ms step_avg:33.21ms
step:438/2160 train_time:14545ms step_avg:33.21ms
step:439/2160 train_time:14579ms step_avg:33.21ms
step:440/2160 train_time:14611ms step_avg:33.21ms
step:441/2160 train_time:14644ms step_avg:33.21ms
step:442/2160 train_time:14676ms step_avg:33.20ms
step:443/2160 train_time:14709ms step_avg:33.20ms
step:444/2160 train_time:14741ms step_avg:33.20ms
step:445/2160 train_time:14775ms step_avg:33.20ms
step:446/2160 train_time:14807ms step_avg:33.20ms
step:447/2160 train_time:14840ms step_avg:33.20ms
step:448/2160 train_time:14873ms step_avg:33.20ms
step:449/2160 train_time:14906ms step_avg:33.20ms
step:450/2160 train_time:14938ms step_avg:33.19ms
step:451/2160 train_time:14971ms step_avg:33.20ms
step:452/2160 train_time:15004ms step_avg:33.19ms
step:453/2160 train_time:15037ms step_avg:33.19ms
step:454/2160 train_time:15069ms step_avg:33.19ms
step:455/2160 train_time:15102ms step_avg:33.19ms
step:456/2160 train_time:15135ms step_avg:33.19ms
step:457/2160 train_time:15168ms step_avg:33.19ms
step:458/2160 train_time:15200ms step_avg:33.19ms
step:459/2160 train_time:15233ms step_avg:33.19ms
step:460/2160 train_time:15265ms step_avg:33.19ms
step:461/2160 train_time:15298ms step_avg:33.19ms
step:462/2160 train_time:15331ms step_avg:33.18ms
step:463/2160 train_time:15363ms step_avg:33.18ms
step:464/2160 train_time:15396ms step_avg:33.18ms
step:465/2160 train_time:15429ms step_avg:33.18ms
step:466/2160 train_time:15461ms step_avg:33.18ms
step:467/2160 train_time:15494ms step_avg:33.18ms
step:468/2160 train_time:15526ms step_avg:33.18ms
step:469/2160 train_time:15560ms step_avg:33.18ms
step:470/2160 train_time:15592ms step_avg:33.17ms
step:471/2160 train_time:15625ms step_avg:33.17ms
step:472/2160 train_time:15657ms step_avg:33.17ms
step:473/2160 train_time:15690ms step_avg:33.17ms
step:474/2160 train_time:15723ms step_avg:33.17ms
step:475/2160 train_time:15756ms step_avg:33.17ms
step:476/2160 train_time:15789ms step_avg:33.17ms
step:477/2160 train_time:15822ms step_avg:33.17ms
step:478/2160 train_time:15854ms step_avg:33.17ms
step:479/2160 train_time:15887ms step_avg:33.17ms
step:480/2160 train_time:15919ms step_avg:33.17ms
step:481/2160 train_time:15953ms step_avg:33.17ms
step:482/2160 train_time:15985ms step_avg:33.16ms
step:483/2160 train_time:16018ms step_avg:33.16ms
step:484/2160 train_time:16051ms step_avg:33.16ms
step:485/2160 train_time:16084ms step_avg:33.16ms
step:486/2160 train_time:16116ms step_avg:33.16ms
step:487/2160 train_time:16149ms step_avg:33.16ms
step:488/2160 train_time:16181ms step_avg:33.16ms
step:489/2160 train_time:16215ms step_avg:33.16ms
step:490/2160 train_time:16247ms step_avg:33.16ms
step:491/2160 train_time:16280ms step_avg:33.16ms
step:492/2160 train_time:16313ms step_avg:33.16ms
step:493/2160 train_time:16346ms step_avg:33.16ms
step:494/2160 train_time:16378ms step_avg:33.15ms
step:495/2160 train_time:16411ms step_avg:33.15ms
step:496/2160 train_time:16443ms step_avg:33.15ms
step:497/2160 train_time:16477ms step_avg:33.15ms
step:498/2160 train_time:16509ms step_avg:33.15ms
step:499/2160 train_time:16542ms step_avg:33.15ms
step:500/2160 train_time:16574ms step_avg:33.15ms
step:500/2160 val_loss:4.0254 train_time:16610ms step_avg:33.22ms
step:501/2160 train_time:16633ms step_avg:33.20ms
step:502/2160 train_time:16655ms step_avg:33.18ms
step:503/2160 train_time:16679ms step_avg:33.16ms
step:504/2160 train_time:16712ms step_avg:33.16ms
step:505/2160 train_time:16749ms step_avg:33.17ms
step:506/2160 train_time:16783ms step_avg:33.17ms
step:507/2160 train_time:16818ms step_avg:33.17ms
step:508/2160 train_time:16850ms step_avg:33.17ms
step:509/2160 train_time:16884ms step_avg:33.17ms
step:510/2160 train_time:16916ms step_avg:33.17ms
step:511/2160 train_time:16949ms step_avg:33.17ms
step:512/2160 train_time:16982ms step_avg:33.17ms
step:513/2160 train_time:17015ms step_avg:33.17ms
step:514/2160 train_time:17047ms step_avg:33.17ms
step:515/2160 train_time:17080ms step_avg:33.16ms
step:516/2160 train_time:17112ms step_avg:33.16ms
step:517/2160 train_time:17145ms step_avg:33.16ms
step:518/2160 train_time:17177ms step_avg:33.16ms
step:519/2160 train_time:17210ms step_avg:33.16ms
step:520/2160 train_time:17242ms step_avg:33.16ms
step:521/2160 train_time:17275ms step_avg:33.16ms
step:522/2160 train_time:17307ms step_avg:33.16ms
step:523/2160 train_time:17340ms step_avg:33.15ms
step:524/2160 train_time:17372ms step_avg:33.15ms
step:525/2160 train_time:17405ms step_avg:33.15ms
step:526/2160 train_time:17437ms step_avg:33.15ms
step:527/2160 train_time:17470ms step_avg:33.15ms
step:528/2160 train_time:17502ms step_avg:33.15ms
step:529/2160 train_time:17535ms step_avg:33.15ms
step:530/2160 train_time:17567ms step_avg:33.15ms
step:531/2160 train_time:17600ms step_avg:33.15ms
step:532/2160 train_time:17633ms step_avg:33.14ms
step:533/2160 train_time:17666ms step_avg:33.14ms
step:534/2160 train_time:17699ms step_avg:33.14ms
step:535/2160 train_time:17733ms step_avg:33.15ms
step:536/2160 train_time:17766ms step_avg:33.14ms
step:537/2160 train_time:17799ms step_avg:33.15ms
step:538/2160 train_time:17832ms step_avg:33.14ms
step:539/2160 train_time:17866ms step_avg:33.15ms
step:540/2160 train_time:17898ms step_avg:33.14ms
step:541/2160 train_time:17931ms step_avg:33.14ms
step:542/2160 train_time:17964ms step_avg:33.14ms
step:543/2160 train_time:17997ms step_avg:33.14ms
step:544/2160 train_time:18030ms step_avg:33.14ms
step:545/2160 train_time:18063ms step_avg:33.14ms
step:546/2160 train_time:18095ms step_avg:33.14ms
step:547/2160 train_time:18128ms step_avg:33.14ms
step:548/2160 train_time:18160ms step_avg:33.14ms
step:549/2160 train_time:18193ms step_avg:33.14ms
step:550/2160 train_time:18226ms step_avg:33.14ms
step:551/2160 train_time:18259ms step_avg:33.14ms
step:552/2160 train_time:18291ms step_avg:33.14ms
step:553/2160 train_time:18324ms step_avg:33.14ms
step:554/2160 train_time:18356ms step_avg:33.13ms
step:555/2160 train_time:18389ms step_avg:33.13ms
step:556/2160 train_time:18421ms step_avg:33.13ms
step:557/2160 train_time:18455ms step_avg:33.13ms
step:558/2160 train_time:18487ms step_avg:33.13ms
step:559/2160 train_time:18520ms step_avg:33.13ms
step:560/2160 train_time:18552ms step_avg:33.13ms
step:561/2160 train_time:18585ms step_avg:33.13ms
step:562/2160 train_time:18618ms step_avg:33.13ms
step:563/2160 train_time:18651ms step_avg:33.13ms
step:564/2160 train_time:18684ms step_avg:33.13ms
step:565/2160 train_time:18717ms step_avg:33.13ms
step:566/2160 train_time:18749ms step_avg:33.13ms
step:567/2160 train_time:18783ms step_avg:33.13ms
step:568/2160 train_time:18816ms step_avg:33.13ms
step:569/2160 train_time:18849ms step_avg:33.13ms
step:570/2160 train_time:18881ms step_avg:33.12ms
step:571/2160 train_time:18915ms step_avg:33.13ms
step:572/2160 train_time:18947ms step_avg:33.12ms
step:573/2160 train_time:18980ms step_avg:33.12ms
step:574/2160 train_time:19012ms step_avg:33.12ms
step:575/2160 train_time:19046ms step_avg:33.12ms
step:576/2160 train_time:19078ms step_avg:33.12ms
step:577/2160 train_time:19111ms step_avg:33.12ms
step:578/2160 train_time:19143ms step_avg:33.12ms
step:579/2160 train_time:19177ms step_avg:33.12ms
step:580/2160 train_time:19209ms step_avg:33.12ms
step:581/2160 train_time:19242ms step_avg:33.12ms
step:582/2160 train_time:19274ms step_avg:33.12ms
step:583/2160 train_time:19307ms step_avg:33.12ms
step:584/2160 train_time:19340ms step_avg:33.12ms
step:585/2160 train_time:19373ms step_avg:33.12ms
step:586/2160 train_time:19405ms step_avg:33.11ms
step:587/2160 train_time:19438ms step_avg:33.11ms
step:588/2160 train_time:19471ms step_avg:33.11ms
step:589/2160 train_time:19503ms step_avg:33.11ms
step:590/2160 train_time:19536ms step_avg:33.11ms
step:591/2160 train_time:19569ms step_avg:33.11ms
step:592/2160 train_time:19602ms step_avg:33.11ms
step:593/2160 train_time:19635ms step_avg:33.11ms
step:594/2160 train_time:19668ms step_avg:33.11ms
step:595/2160 train_time:19701ms step_avg:33.11ms
step:596/2160 train_time:19733ms step_avg:33.11ms
step:597/2160 train_time:19767ms step_avg:33.11ms
step:598/2160 train_time:19800ms step_avg:33.11ms
step:599/2160 train_time:19834ms step_avg:33.11ms
step:600/2160 train_time:19866ms step_avg:33.11ms
step:601/2160 train_time:19900ms step_avg:33.11ms
step:602/2160 train_time:19932ms step_avg:33.11ms
step:603/2160 train_time:19965ms step_avg:33.11ms
step:604/2160 train_time:19997ms step_avg:33.11ms
step:605/2160 train_time:20031ms step_avg:33.11ms
step:606/2160 train_time:20063ms step_avg:33.11ms
step:607/2160 train_time:20096ms step_avg:33.11ms
step:608/2160 train_time:20128ms step_avg:33.11ms
step:609/2160 train_time:20161ms step_avg:33.11ms
step:610/2160 train_time:20194ms step_avg:33.10ms
step:611/2160 train_time:20227ms step_avg:33.10ms
step:612/2160 train_time:20259ms step_avg:33.10ms
step:613/2160 train_time:20292ms step_avg:33.10ms
step:614/2160 train_time:20325ms step_avg:33.10ms
step:615/2160 train_time:20358ms step_avg:33.10ms
step:616/2160 train_time:20390ms step_avg:33.10ms
step:617/2160 train_time:20423ms step_avg:33.10ms
step:618/2160 train_time:20455ms step_avg:33.10ms
step:619/2160 train_time:20488ms step_avg:33.10ms
step:620/2160 train_time:20521ms step_avg:33.10ms
step:621/2160 train_time:20554ms step_avg:33.10ms
step:622/2160 train_time:20587ms step_avg:33.10ms
step:623/2160 train_time:20620ms step_avg:33.10ms
step:624/2160 train_time:20652ms step_avg:33.10ms
step:625/2160 train_time:20685ms step_avg:33.10ms
step:626/2160 train_time:20718ms step_avg:33.10ms
step:627/2160 train_time:20751ms step_avg:33.10ms
step:628/2160 train_time:20783ms step_avg:33.09ms
step:629/2160 train_time:20816ms step_avg:33.09ms
step:630/2160 train_time:20849ms step_avg:33.09ms
step:631/2160 train_time:20882ms step_avg:33.09ms
step:632/2160 train_time:20914ms step_avg:33.09ms
step:633/2160 train_time:20948ms step_avg:33.09ms
step:634/2160 train_time:20980ms step_avg:33.09ms
step:635/2160 train_time:21014ms step_avg:33.09ms
step:636/2160 train_time:21046ms step_avg:33.09ms
step:637/2160 train_time:21079ms step_avg:33.09ms
step:638/2160 train_time:21112ms step_avg:33.09ms
step:639/2160 train_time:21145ms step_avg:33.09ms
step:640/2160 train_time:21177ms step_avg:33.09ms
step:641/2160 train_time:21211ms step_avg:33.09ms
step:642/2160 train_time:21243ms step_avg:33.09ms
step:643/2160 train_time:21276ms step_avg:33.09ms
step:644/2160 train_time:21309ms step_avg:33.09ms
step:645/2160 train_time:21341ms step_avg:33.09ms
step:646/2160 train_time:21374ms step_avg:33.09ms
step:647/2160 train_time:21407ms step_avg:33.09ms
step:648/2160 train_time:21439ms step_avg:33.09ms
step:649/2160 train_time:21472ms step_avg:33.09ms
step:650/2160 train_time:21505ms step_avg:33.08ms
step:651/2160 train_time:21538ms step_avg:33.08ms
step:652/2160 train_time:21570ms step_avg:33.08ms
step:653/2160 train_time:21603ms step_avg:33.08ms
step:654/2160 train_time:21636ms step_avg:33.08ms
step:655/2160 train_time:21669ms step_avg:33.08ms
step:656/2160 train_time:21702ms step_avg:33.08ms
step:657/2160 train_time:21735ms step_avg:33.08ms
step:658/2160 train_time:21768ms step_avg:33.08ms
step:659/2160 train_time:21800ms step_avg:33.08ms
step:660/2160 train_time:21833ms step_avg:33.08ms
step:661/2160 train_time:21866ms step_avg:33.08ms
step:662/2160 train_time:21898ms step_avg:33.08ms
step:663/2160 train_time:21932ms step_avg:33.08ms
step:664/2160 train_time:21964ms step_avg:33.08ms
step:665/2160 train_time:21997ms step_avg:33.08ms
step:666/2160 train_time:22030ms step_avg:33.08ms
step:667/2160 train_time:22063ms step_avg:33.08ms
step:668/2160 train_time:22095ms step_avg:33.08ms
step:669/2160 train_time:22129ms step_avg:33.08ms
step:670/2160 train_time:22161ms step_avg:33.08ms
step:671/2160 train_time:22194ms step_avg:33.08ms
step:672/2160 train_time:22227ms step_avg:33.08ms
step:673/2160 train_time:22260ms step_avg:33.08ms
step:674/2160 train_time:22292ms step_avg:33.07ms
step:675/2160 train_time:22325ms step_avg:33.07ms
step:676/2160 train_time:22357ms step_avg:33.07ms
step:677/2160 train_time:22391ms step_avg:33.07ms
step:678/2160 train_time:22423ms step_avg:33.07ms
step:679/2160 train_time:22456ms step_avg:33.07ms
step:680/2160 train_time:22489ms step_avg:33.07ms
step:681/2160 train_time:22522ms step_avg:33.07ms
step:682/2160 train_time:22554ms step_avg:33.07ms
step:683/2160 train_time:22587ms step_avg:33.07ms
step:684/2160 train_time:22620ms step_avg:33.07ms
step:685/2160 train_time:22653ms step_avg:33.07ms
step:686/2160 train_time:22685ms step_avg:33.07ms
step:687/2160 train_time:22719ms step_avg:33.07ms
step:688/2160 train_time:22751ms step_avg:33.07ms
step:689/2160 train_time:22784ms step_avg:33.07ms
step:690/2160 train_time:22817ms step_avg:33.07ms
step:691/2160 train_time:22850ms step_avg:33.07ms
step:692/2160 train_time:22882ms step_avg:33.07ms
step:693/2160 train_time:22916ms step_avg:33.07ms
step:694/2160 train_time:22949ms step_avg:33.07ms
step:695/2160 train_time:22982ms step_avg:33.07ms
step:696/2160 train_time:23015ms step_avg:33.07ms
step:697/2160 train_time:23048ms step_avg:33.07ms
step:698/2160 train_time:23081ms step_avg:33.07ms
step:699/2160 train_time:23114ms step_avg:33.07ms
step:700/2160 train_time:23147ms step_avg:33.07ms
step:701/2160 train_time:23180ms step_avg:33.07ms
step:702/2160 train_time:23212ms step_avg:33.07ms
step:703/2160 train_time:23245ms step_avg:33.07ms
step:704/2160 train_time:23277ms step_avg:33.06ms
step:705/2160 train_time:23311ms step_avg:33.07ms
step:706/2160 train_time:23343ms step_avg:33.06ms
step:707/2160 train_time:23376ms step_avg:33.06ms
step:708/2160 train_time:23410ms step_avg:33.07ms
step:709/2160 train_time:23468ms step_avg:33.10ms
step:710/2160 train_time:23527ms step_avg:33.14ms
step:711/2160 train_time:23587ms step_avg:33.17ms
step:712/2160 train_time:23646ms step_avg:33.21ms
step:713/2160 train_time:23707ms step_avg:33.25ms
step:714/2160 train_time:23766ms step_avg:33.29ms
step:715/2160 train_time:23827ms step_avg:33.32ms
step:716/2160 train_time:23887ms step_avg:33.36ms
step:717/2160 train_time:23947ms step_avg:33.40ms
step:718/2160 train_time:24007ms step_avg:33.44ms
step:719/2160 train_time:24068ms step_avg:33.47ms
step:720/2160 train_time:24128ms step_avg:33.51ms
step:721/2160 train_time:24189ms step_avg:33.55ms
step:722/2160 train_time:24248ms step_avg:33.58ms
step:723/2160 train_time:24308ms step_avg:33.62ms
step:724/2160 train_time:24367ms step_avg:33.66ms
step:725/2160 train_time:24427ms step_avg:33.69ms
step:726/2160 train_time:24485ms step_avg:33.73ms
step:727/2160 train_time:24545ms step_avg:33.76ms
step:728/2160 train_time:24605ms step_avg:33.80ms
step:729/2160 train_time:24666ms step_avg:33.84ms
step:730/2160 train_time:24725ms step_avg:33.87ms
step:731/2160 train_time:24786ms step_avg:33.91ms
step:732/2160 train_time:24845ms step_avg:33.94ms
step:733/2160 train_time:24905ms step_avg:33.98ms
step:734/2160 train_time:24966ms step_avg:34.01ms
step:735/2160 train_time:25028ms step_avg:34.05ms
step:736/2160 train_time:25087ms step_avg:34.09ms
step:737/2160 train_time:25148ms step_avg:34.12ms
step:738/2160 train_time:25207ms step_avg:34.16ms
step:739/2160 train_time:25269ms step_avg:34.19ms
step:740/2160 train_time:25328ms step_avg:34.23ms
step:741/2160 train_time:25388ms step_avg:34.26ms
step:742/2160 train_time:25447ms step_avg:34.30ms
step:743/2160 train_time:25507ms step_avg:34.33ms
step:744/2160 train_time:25565ms step_avg:34.36ms
step:745/2160 train_time:25626ms step_avg:34.40ms
step:746/2160 train_time:25685ms step_avg:34.43ms
step:747/2160 train_time:25746ms step_avg:34.47ms
step:748/2160 train_time:25805ms step_avg:34.50ms
step:749/2160 train_time:25867ms step_avg:34.54ms
step:750/2160 train_time:25926ms step_avg:34.57ms
step:750/2160 val_loss:3.8735 train_time:25989ms step_avg:34.65ms
step:751/2160 train_time:26012ms step_avg:34.64ms
step:752/2160 train_time:26050ms step_avg:34.64ms
step:753/2160 train_time:26114ms step_avg:34.68ms
step:754/2160 train_time:26174ms step_avg:34.71ms
step:755/2160 train_time:26235ms step_avg:34.75ms
step:756/2160 train_time:26294ms step_avg:34.78ms
step:757/2160 train_time:26353ms step_avg:34.81ms
step:758/2160 train_time:26412ms step_avg:34.84ms
step:759/2160 train_time:26471ms step_avg:34.88ms
step:760/2160 train_time:26529ms step_avg:34.91ms
step:761/2160 train_time:26589ms step_avg:34.94ms
step:762/2160 train_time:26647ms step_avg:34.97ms
step:763/2160 train_time:26707ms step_avg:35.00ms
step:764/2160 train_time:26765ms step_avg:35.03ms
step:765/2160 train_time:26825ms step_avg:35.07ms
step:766/2160 train_time:26883ms step_avg:35.10ms
step:767/2160 train_time:26945ms step_avg:35.13ms
step:768/2160 train_time:27006ms step_avg:35.16ms
step:769/2160 train_time:27069ms step_avg:35.20ms
step:770/2160 train_time:27130ms step_avg:35.23ms
step:771/2160 train_time:27192ms step_avg:35.27ms
step:772/2160 train_time:27252ms step_avg:35.30ms
step:773/2160 train_time:27313ms step_avg:35.33ms
step:774/2160 train_time:27372ms step_avg:35.36ms
step:775/2160 train_time:27432ms step_avg:35.40ms
step:776/2160 train_time:27491ms step_avg:35.43ms
step:777/2160 train_time:27550ms step_avg:35.46ms
step:778/2160 train_time:27609ms step_avg:35.49ms
step:779/2160 train_time:27669ms step_avg:35.52ms
step:780/2160 train_time:27727ms step_avg:35.55ms
step:781/2160 train_time:27787ms step_avg:35.58ms
step:782/2160 train_time:27846ms step_avg:35.61ms
step:783/2160 train_time:27907ms step_avg:35.64ms
step:784/2160 train_time:27967ms step_avg:35.67ms
step:785/2160 train_time:28028ms step_avg:35.70ms
step:786/2160 train_time:28088ms step_avg:35.74ms
step:787/2160 train_time:28150ms step_avg:35.77ms
step:788/2160 train_time:28210ms step_avg:35.80ms
step:789/2160 train_time:28271ms step_avg:35.83ms
step:790/2160 train_time:28332ms step_avg:35.86ms
step:791/2160 train_time:28392ms step_avg:35.89ms
step:792/2160 train_time:28451ms step_avg:35.92ms
step:793/2160 train_time:28510ms step_avg:35.95ms
step:794/2160 train_time:28570ms step_avg:35.98ms
step:795/2160 train_time:28629ms step_avg:36.01ms
step:796/2160 train_time:28688ms step_avg:36.04ms
step:797/2160 train_time:28747ms step_avg:36.07ms
step:798/2160 train_time:28806ms step_avg:36.10ms
step:799/2160 train_time:28866ms step_avg:36.13ms
step:800/2160 train_time:28926ms step_avg:36.16ms
step:801/2160 train_time:28987ms step_avg:36.19ms
step:802/2160 train_time:29047ms step_avg:36.22ms
step:803/2160 train_time:29109ms step_avg:36.25ms
step:804/2160 train_time:29169ms step_avg:36.28ms
step:805/2160 train_time:29231ms step_avg:36.31ms
step:806/2160 train_time:29290ms step_avg:36.34ms
step:807/2160 train_time:29351ms step_avg:36.37ms
step:808/2160 train_time:29410ms step_avg:36.40ms
step:809/2160 train_time:29470ms step_avg:36.43ms
step:810/2160 train_time:29529ms step_avg:36.46ms
step:811/2160 train_time:29589ms step_avg:36.48ms
step:812/2160 train_time:29648ms step_avg:36.51ms
step:813/2160 train_time:29708ms step_avg:36.54ms
step:814/2160 train_time:29767ms step_avg:36.57ms
step:815/2160 train_time:29826ms step_avg:36.60ms
step:816/2160 train_time:29886ms step_avg:36.62ms
step:817/2160 train_time:29946ms step_avg:36.65ms
step:818/2160 train_time:30006ms step_avg:36.68ms
step:819/2160 train_time:30068ms step_avg:36.71ms
step:820/2160 train_time:30128ms step_avg:36.74ms
step:821/2160 train_time:30190ms step_avg:36.77ms
step:822/2160 train_time:30249ms step_avg:36.80ms
step:823/2160 train_time:30310ms step_avg:36.83ms
step:824/2160 train_time:30369ms step_avg:36.86ms
step:825/2160 train_time:30430ms step_avg:36.88ms
step:826/2160 train_time:30489ms step_avg:36.91ms
step:827/2160 train_time:30549ms step_avg:36.94ms
step:828/2160 train_time:30608ms step_avg:36.97ms
step:829/2160 train_time:30668ms step_avg:36.99ms
step:830/2160 train_time:30728ms step_avg:37.02ms
step:831/2160 train_time:30787ms step_avg:37.05ms
step:832/2160 train_time:30846ms step_avg:37.07ms
step:833/2160 train_time:30907ms step_avg:37.10ms
step:834/2160 train_time:30967ms step_avg:37.13ms
step:835/2160 train_time:31028ms step_avg:37.16ms
step:836/2160 train_time:31089ms step_avg:37.19ms
step:837/2160 train_time:31151ms step_avg:37.22ms
step:838/2160 train_time:31210ms step_avg:37.24ms
step:839/2160 train_time:31271ms step_avg:37.27ms
step:840/2160 train_time:31330ms step_avg:37.30ms
step:841/2160 train_time:31390ms step_avg:37.33ms
step:842/2160 train_time:31450ms step_avg:37.35ms
step:843/2160 train_time:31510ms step_avg:37.38ms
step:844/2160 train_time:31569ms step_avg:37.40ms
step:845/2160 train_time:31629ms step_avg:37.43ms
step:846/2160 train_time:31689ms step_avg:37.46ms
step:847/2160 train_time:31748ms step_avg:37.48ms
step:848/2160 train_time:31807ms step_avg:37.51ms
step:849/2160 train_time:31867ms step_avg:37.54ms
step:850/2160 train_time:31927ms step_avg:37.56ms
step:851/2160 train_time:31987ms step_avg:37.59ms
step:852/2160 train_time:32047ms step_avg:37.61ms
step:853/2160 train_time:32110ms step_avg:37.64ms
step:854/2160 train_time:32169ms step_avg:37.67ms
step:855/2160 train_time:32230ms step_avg:37.70ms
step:856/2160 train_time:32290ms step_avg:37.72ms
step:857/2160 train_time:32350ms step_avg:37.75ms
step:858/2160 train_time:32409ms step_avg:37.77ms
step:859/2160 train_time:32470ms step_avg:37.80ms
step:860/2160 train_time:32530ms step_avg:37.83ms
step:861/2160 train_time:32589ms step_avg:37.85ms
step:862/2160 train_time:32649ms step_avg:37.88ms
step:863/2160 train_time:32709ms step_avg:37.90ms
step:864/2160 train_time:32768ms step_avg:37.93ms
step:865/2160 train_time:32828ms step_avg:37.95ms
step:866/2160 train_time:32888ms step_avg:37.98ms
step:867/2160 train_time:32948ms step_avg:38.00ms
step:868/2160 train_time:33007ms step_avg:38.03ms
step:869/2160 train_time:33069ms step_avg:38.05ms
step:870/2160 train_time:33128ms step_avg:38.08ms
step:871/2160 train_time:33189ms step_avg:38.10ms
step:872/2160 train_time:33248ms step_avg:38.13ms
step:873/2160 train_time:33308ms step_avg:38.15ms
step:874/2160 train_time:33368ms step_avg:38.18ms
step:875/2160 train_time:33429ms step_avg:38.20ms
step:876/2160 train_time:33488ms step_avg:38.23ms
step:877/2160 train_time:33549ms step_avg:38.25ms
step:878/2160 train_time:33608ms step_avg:38.28ms
step:879/2160 train_time:33669ms step_avg:38.30ms
step:880/2160 train_time:33728ms step_avg:38.33ms
step:881/2160 train_time:33788ms step_avg:38.35ms
step:882/2160 train_time:33847ms step_avg:38.38ms
step:883/2160 train_time:33908ms step_avg:38.40ms
step:884/2160 train_time:33967ms step_avg:38.42ms
step:885/2160 train_time:34028ms step_avg:38.45ms
step:886/2160 train_time:34088ms step_avg:38.47ms
step:887/2160 train_time:34150ms step_avg:38.50ms
step:888/2160 train_time:34209ms step_avg:38.52ms
step:889/2160 train_time:34270ms step_avg:38.55ms
step:890/2160 train_time:34330ms step_avg:38.57ms
step:891/2160 train_time:34390ms step_avg:38.60ms
step:892/2160 train_time:34449ms step_avg:38.62ms
step:893/2160 train_time:34511ms step_avg:38.65ms
step:894/2160 train_time:34571ms step_avg:38.67ms
step:895/2160 train_time:34630ms step_avg:38.69ms
step:896/2160 train_time:34689ms step_avg:38.72ms
step:897/2160 train_time:34750ms step_avg:38.74ms
step:898/2160 train_time:34809ms step_avg:38.76ms
step:899/2160 train_time:34870ms step_avg:38.79ms
step:900/2160 train_time:34929ms step_avg:38.81ms
step:901/2160 train_time:34990ms step_avg:38.83ms
step:902/2160 train_time:35049ms step_avg:38.86ms
step:903/2160 train_time:35111ms step_avg:38.88ms
step:904/2160 train_time:35170ms step_avg:38.90ms
step:905/2160 train_time:35230ms step_avg:38.93ms
step:906/2160 train_time:35290ms step_avg:38.95ms
step:907/2160 train_time:35350ms step_avg:38.97ms
step:908/2160 train_time:35410ms step_avg:39.00ms
step:909/2160 train_time:35471ms step_avg:39.02ms
step:910/2160 train_time:35530ms step_avg:39.04ms
step:911/2160 train_time:35590ms step_avg:39.07ms
step:912/2160 train_time:35650ms step_avg:39.09ms
step:913/2160 train_time:35710ms step_avg:39.11ms
step:914/2160 train_time:35769ms step_avg:39.13ms
step:915/2160 train_time:35830ms step_avg:39.16ms
step:916/2160 train_time:35889ms step_avg:39.18ms
step:917/2160 train_time:35950ms step_avg:39.20ms
step:918/2160 train_time:36010ms step_avg:39.23ms
step:919/2160 train_time:36071ms step_avg:39.25ms
step:920/2160 train_time:36130ms step_avg:39.27ms
step:921/2160 train_time:36191ms step_avg:39.30ms
step:922/2160 train_time:36251ms step_avg:39.32ms
step:923/2160 train_time:36311ms step_avg:39.34ms
step:924/2160 train_time:36371ms step_avg:39.36ms
step:925/2160 train_time:36431ms step_avg:39.38ms
step:926/2160 train_time:36490ms step_avg:39.41ms
step:927/2160 train_time:36550ms step_avg:39.43ms
step:928/2160 train_time:36609ms step_avg:39.45ms
step:929/2160 train_time:36670ms step_avg:39.47ms
step:930/2160 train_time:36729ms step_avg:39.49ms
step:931/2160 train_time:36789ms step_avg:39.52ms
step:932/2160 train_time:36848ms step_avg:39.54ms
step:933/2160 train_time:36909ms step_avg:39.56ms
step:934/2160 train_time:36969ms step_avg:39.58ms
step:935/2160 train_time:37030ms step_avg:39.60ms
step:936/2160 train_time:37089ms step_avg:39.62ms
step:937/2160 train_time:37149ms step_avg:39.65ms
step:938/2160 train_time:37209ms step_avg:39.67ms
step:939/2160 train_time:37269ms step_avg:39.69ms
step:940/2160 train_time:37330ms step_avg:39.71ms
step:941/2160 train_time:37390ms step_avg:39.73ms
step:942/2160 train_time:37449ms step_avg:39.75ms
step:943/2160 train_time:37510ms step_avg:39.78ms
step:944/2160 train_time:37569ms step_avg:39.80ms
step:945/2160 train_time:37629ms step_avg:39.82ms
step:946/2160 train_time:37689ms step_avg:39.84ms
step:947/2160 train_time:37750ms step_avg:39.86ms
step:948/2160 train_time:37809ms step_avg:39.88ms
step:949/2160 train_time:37870ms step_avg:39.91ms
step:950/2160 train_time:37929ms step_avg:39.93ms
step:951/2160 train_time:37989ms step_avg:39.95ms
step:952/2160 train_time:38049ms step_avg:39.97ms
step:953/2160 train_time:38110ms step_avg:39.99ms
step:954/2160 train_time:38169ms step_avg:40.01ms
step:955/2160 train_time:38230ms step_avg:40.03ms
step:956/2160 train_time:38290ms step_avg:40.05ms
step:957/2160 train_time:38351ms step_avg:40.07ms
step:958/2160 train_time:38410ms step_avg:40.09ms
step:959/2160 train_time:38471ms step_avg:40.12ms
step:960/2160 train_time:38530ms step_avg:40.14ms
step:961/2160 train_time:38590ms step_avg:40.16ms
step:962/2160 train_time:38649ms step_avg:40.18ms
step:963/2160 train_time:38711ms step_avg:40.20ms
step:964/2160 train_time:38770ms step_avg:40.22ms
step:965/2160 train_time:38831ms step_avg:40.24ms
step:966/2160 train_time:38891ms step_avg:40.26ms
step:967/2160 train_time:38951ms step_avg:40.28ms
step:968/2160 train_time:39010ms step_avg:40.30ms
step:969/2160 train_time:39071ms step_avg:40.32ms
step:970/2160 train_time:39130ms step_avg:40.34ms
step:971/2160 train_time:39192ms step_avg:40.36ms
step:972/2160 train_time:39251ms step_avg:40.38ms
step:973/2160 train_time:39312ms step_avg:40.40ms
step:974/2160 train_time:39372ms step_avg:40.42ms
step:975/2160 train_time:39432ms step_avg:40.44ms
step:976/2160 train_time:39492ms step_avg:40.46ms
step:977/2160 train_time:39551ms step_avg:40.48ms
step:978/2160 train_time:39611ms step_avg:40.50ms
step:979/2160 train_time:39671ms step_avg:40.52ms
step:980/2160 train_time:39731ms step_avg:40.54ms
step:981/2160 train_time:39791ms step_avg:40.56ms
step:982/2160 train_time:39851ms step_avg:40.58ms
step:983/2160 train_time:39912ms step_avg:40.60ms
step:984/2160 train_time:39971ms step_avg:40.62ms
step:985/2160 train_time:40031ms step_avg:40.64ms
step:986/2160 train_time:40090ms step_avg:40.66ms
step:987/2160 train_time:40150ms step_avg:40.68ms
step:988/2160 train_time:40210ms step_avg:40.70ms
step:989/2160 train_time:40271ms step_avg:40.72ms
step:990/2160 train_time:40331ms step_avg:40.74ms
step:991/2160 train_time:40391ms step_avg:40.76ms
step:992/2160 train_time:40450ms step_avg:40.78ms
step:993/2160 train_time:40512ms step_avg:40.80ms
step:994/2160 train_time:40571ms step_avg:40.82ms
step:995/2160 train_time:40632ms step_avg:40.84ms
step:996/2160 train_time:40691ms step_avg:40.85ms
step:997/2160 train_time:40751ms step_avg:40.87ms
step:998/2160 train_time:40810ms step_avg:40.89ms
step:999/2160 train_time:40872ms step_avg:40.91ms
step:1000/2160 train_time:40931ms step_avg:40.93ms
step:1000/2160 val_loss:3.7121 train_time:40993ms step_avg:40.99ms
step:1001/2160 train_time:41016ms step_avg:40.98ms
step:1002/2160 train_time:41053ms step_avg:40.97ms
step:1003/2160 train_time:41116ms step_avg:40.99ms
step:1004/2160 train_time:41178ms step_avg:41.01ms
step:1005/2160 train_time:41239ms step_avg:41.03ms
step:1006/2160 train_time:41298ms step_avg:41.05ms
step:1007/2160 train_time:41358ms step_avg:41.07ms
step:1008/2160 train_time:41416ms step_avg:41.09ms
step:1009/2160 train_time:41475ms step_avg:41.11ms
step:1010/2160 train_time:41533ms step_avg:41.12ms
step:1011/2160 train_time:41592ms step_avg:41.14ms
step:1012/2160 train_time:41650ms step_avg:41.16ms
step:1013/2160 train_time:41710ms step_avg:41.17ms
step:1014/2160 train_time:41768ms step_avg:41.19ms
step:1015/2160 train_time:41827ms step_avg:41.21ms
step:1016/2160 train_time:41887ms step_avg:41.23ms
step:1017/2160 train_time:41948ms step_avg:41.25ms
step:1018/2160 train_time:42009ms step_avg:41.27ms
step:1019/2160 train_time:42072ms step_avg:41.29ms
step:1020/2160 train_time:42131ms step_avg:41.31ms
step:1021/2160 train_time:42193ms step_avg:41.32ms
step:1022/2160 train_time:42252ms step_avg:41.34ms
step:1023/2160 train_time:42312ms step_avg:41.36ms
step:1024/2160 train_time:42371ms step_avg:41.38ms
step:1025/2160 train_time:42431ms step_avg:41.40ms
step:1026/2160 train_time:42490ms step_avg:41.41ms
step:1027/2160 train_time:42550ms step_avg:41.43ms
step:1028/2160 train_time:42608ms step_avg:41.45ms
step:1029/2160 train_time:42667ms step_avg:41.46ms
step:1030/2160 train_time:42726ms step_avg:41.48ms
step:1031/2160 train_time:42786ms step_avg:41.50ms
step:1032/2160 train_time:42845ms step_avg:41.52ms
step:1033/2160 train_time:42906ms step_avg:41.54ms
step:1034/2160 train_time:42966ms step_avg:41.55ms
step:1035/2160 train_time:43029ms step_avg:41.57ms
step:1036/2160 train_time:43088ms step_avg:41.59ms
step:1037/2160 train_time:43150ms step_avg:41.61ms
step:1038/2160 train_time:43210ms step_avg:41.63ms
step:1039/2160 train_time:43271ms step_avg:41.65ms
step:1040/2160 train_time:43331ms step_avg:41.66ms
step:1041/2160 train_time:43390ms step_avg:41.68ms
step:1042/2160 train_time:43449ms step_avg:41.70ms
step:1043/2160 train_time:43509ms step_avg:41.72ms
step:1044/2160 train_time:43568ms step_avg:41.73ms
step:1045/2160 train_time:43627ms step_avg:41.75ms
step:1046/2160 train_time:43686ms step_avg:41.76ms
step:1047/2160 train_time:43746ms step_avg:41.78ms
step:1048/2160 train_time:43805ms step_avg:41.80ms
step:1049/2160 train_time:43866ms step_avg:41.82ms
step:1050/2160 train_time:43926ms step_avg:41.83ms
step:1051/2160 train_time:43988ms step_avg:41.85ms
step:1052/2160 train_time:44049ms step_avg:41.87ms
step:1053/2160 train_time:44110ms step_avg:41.89ms
step:1054/2160 train_time:44170ms step_avg:41.91ms
step:1055/2160 train_time:44231ms step_avg:41.92ms
step:1056/2160 train_time:44290ms step_avg:41.94ms
step:1057/2160 train_time:44350ms step_avg:41.96ms
step:1058/2160 train_time:44410ms step_avg:41.98ms
step:1059/2160 train_time:44470ms step_avg:41.99ms
step:1060/2160 train_time:44529ms step_avg:42.01ms
step:1061/2160 train_time:44588ms step_avg:42.02ms
step:1062/2160 train_time:44647ms step_avg:42.04ms
step:1063/2160 train_time:44707ms step_avg:42.06ms
step:1064/2160 train_time:44766ms step_avg:42.07ms
step:1065/2160 train_time:44826ms step_avg:42.09ms
step:1066/2160 train_time:44886ms step_avg:42.11ms
step:1067/2160 train_time:44946ms step_avg:42.12ms
step:1068/2160 train_time:45006ms step_avg:42.14ms
step:1069/2160 train_time:45068ms step_avg:42.16ms
step:1070/2160 train_time:45128ms step_avg:42.18ms
step:1071/2160 train_time:45190ms step_avg:42.19ms
step:1072/2160 train_time:45249ms step_avg:42.21ms
step:1073/2160 train_time:45310ms step_avg:42.23ms
step:1074/2160 train_time:45369ms step_avg:42.24ms
step:1075/2160 train_time:45429ms step_avg:42.26ms
step:1076/2160 train_time:45488ms step_avg:42.28ms
step:1077/2160 train_time:45549ms step_avg:42.29ms
step:1078/2160 train_time:45608ms step_avg:42.31ms
step:1079/2160 train_time:45668ms step_avg:42.32ms
step:1080/2160 train_time:45727ms step_avg:42.34ms
step:1081/2160 train_time:45787ms step_avg:42.36ms
step:1082/2160 train_time:45847ms step_avg:42.37ms
step:1083/2160 train_time:45908ms step_avg:42.39ms
step:1084/2160 train_time:45968ms step_avg:42.41ms
step:1085/2160 train_time:46028ms step_avg:42.42ms
step:1086/2160 train_time:46089ms step_avg:42.44ms
step:1087/2160 train_time:46150ms step_avg:42.46ms
step:1088/2160 train_time:46210ms step_avg:42.47ms
step:1089/2160 train_time:46271ms step_avg:42.49ms
step:1090/2160 train_time:46330ms step_avg:42.50ms
step:1091/2160 train_time:46390ms step_avg:42.52ms
step:1092/2160 train_time:46449ms step_avg:42.54ms
step:1093/2160 train_time:46509ms step_avg:42.55ms
step:1094/2160 train_time:46568ms step_avg:42.57ms
step:1095/2160 train_time:46628ms step_avg:42.58ms
step:1096/2160 train_time:46687ms step_avg:42.60ms
step:1097/2160 train_time:46748ms step_avg:42.61ms
step:1098/2160 train_time:46807ms step_avg:42.63ms
step:1099/2160 train_time:46868ms step_avg:42.65ms
step:1100/2160 train_time:46928ms step_avg:42.66ms
step:1101/2160 train_time:46988ms step_avg:42.68ms
step:1102/2160 train_time:47048ms step_avg:42.69ms
step:1103/2160 train_time:47109ms step_avg:42.71ms
step:1104/2160 train_time:47169ms step_avg:42.73ms
step:1105/2160 train_time:47230ms step_avg:42.74ms
step:1106/2160 train_time:47289ms step_avg:42.76ms
step:1107/2160 train_time:47351ms step_avg:42.77ms
step:1108/2160 train_time:47409ms step_avg:42.79ms
step:1109/2160 train_time:47470ms step_avg:42.80ms
step:1110/2160 train_time:47529ms step_avg:42.82ms
step:1111/2160 train_time:47590ms step_avg:42.83ms
step:1112/2160 train_time:47649ms step_avg:42.85ms
step:1113/2160 train_time:47709ms step_avg:42.87ms
step:1114/2160 train_time:47769ms step_avg:42.88ms
step:1115/2160 train_time:47828ms step_avg:42.90ms
step:1116/2160 train_time:47888ms step_avg:42.91ms
step:1117/2160 train_time:47949ms step_avg:42.93ms
step:1118/2160 train_time:48008ms step_avg:42.94ms
step:1119/2160 train_time:48069ms step_avg:42.96ms
step:1120/2160 train_time:48128ms step_avg:42.97ms
step:1121/2160 train_time:48189ms step_avg:42.99ms
step:1122/2160 train_time:48248ms step_avg:43.00ms
step:1123/2160 train_time:48309ms step_avg:43.02ms
step:1124/2160 train_time:48369ms step_avg:43.03ms
step:1125/2160 train_time:48429ms step_avg:43.05ms
step:1126/2160 train_time:48489ms step_avg:43.06ms
step:1127/2160 train_time:48549ms step_avg:43.08ms
step:1128/2160 train_time:48608ms step_avg:43.09ms
step:1129/2160 train_time:48669ms step_avg:43.11ms
step:1130/2160 train_time:48727ms step_avg:43.12ms
step:1131/2160 train_time:48788ms step_avg:43.14ms
step:1132/2160 train_time:48847ms step_avg:43.15ms
step:1133/2160 train_time:48908ms step_avg:43.17ms
step:1134/2160 train_time:48968ms step_avg:43.18ms
step:1135/2160 train_time:49029ms step_avg:43.20ms
step:1136/2160 train_time:49089ms step_avg:43.21ms
step:1137/2160 train_time:49150ms step_avg:43.23ms
step:1138/2160 train_time:49209ms step_avg:43.24ms
step:1139/2160 train_time:49270ms step_avg:43.26ms
step:1140/2160 train_time:49329ms step_avg:43.27ms
step:1141/2160 train_time:49390ms step_avg:43.29ms
step:1142/2160 train_time:49449ms step_avg:43.30ms
step:1143/2160 train_time:49509ms step_avg:43.32ms
step:1144/2160 train_time:49569ms step_avg:43.33ms
step:1145/2160 train_time:49629ms step_avg:43.34ms
step:1146/2160 train_time:49688ms step_avg:43.36ms
step:1147/2160 train_time:49750ms step_avg:43.37ms
step:1148/2160 train_time:49808ms step_avg:43.39ms
step:1149/2160 train_time:49868ms step_avg:43.40ms
step:1150/2160 train_time:49928ms step_avg:43.42ms
step:1151/2160 train_time:49989ms step_avg:43.43ms
step:1152/2160 train_time:50048ms step_avg:43.44ms
step:1153/2160 train_time:50109ms step_avg:43.46ms
step:1154/2160 train_time:50169ms step_avg:43.47ms
step:1155/2160 train_time:50229ms step_avg:43.49ms
step:1156/2160 train_time:50289ms step_avg:43.50ms
step:1157/2160 train_time:50350ms step_avg:43.52ms
step:1158/2160 train_time:50409ms step_avg:43.53ms
step:1159/2160 train_time:50469ms step_avg:43.55ms
step:1160/2160 train_time:50528ms step_avg:43.56ms
step:1161/2160 train_time:50588ms step_avg:43.57ms
step:1162/2160 train_time:50648ms step_avg:43.59ms
step:1163/2160 train_time:50709ms step_avg:43.60ms
step:1164/2160 train_time:50768ms step_avg:43.62ms
step:1165/2160 train_time:50828ms step_avg:43.63ms
step:1166/2160 train_time:50888ms step_avg:43.64ms
step:1167/2160 train_time:50949ms step_avg:43.66ms
step:1168/2160 train_time:51009ms step_avg:43.67ms
step:1169/2160 train_time:51070ms step_avg:43.69ms
step:1170/2160 train_time:51129ms step_avg:43.70ms
step:1171/2160 train_time:51189ms step_avg:43.71ms
step:1172/2160 train_time:51249ms step_avg:43.73ms
step:1173/2160 train_time:51310ms step_avg:43.74ms
step:1174/2160 train_time:51370ms step_avg:43.76ms
step:1175/2160 train_time:51430ms step_avg:43.77ms
step:1176/2160 train_time:51489ms step_avg:43.78ms
step:1177/2160 train_time:51549ms step_avg:43.80ms
step:1178/2160 train_time:51609ms step_avg:43.81ms
step:1179/2160 train_time:51669ms step_avg:43.82ms
step:1180/2160 train_time:51728ms step_avg:43.84ms
step:1181/2160 train_time:51788ms step_avg:43.85ms
step:1182/2160 train_time:51848ms step_avg:43.86ms
step:1183/2160 train_time:51909ms step_avg:43.88ms
step:1184/2160 train_time:51968ms step_avg:43.89ms
step:1185/2160 train_time:52029ms step_avg:43.91ms
step:1186/2160 train_time:52088ms step_avg:43.92ms
step:1187/2160 train_time:52149ms step_avg:43.93ms
step:1188/2160 train_time:52208ms step_avg:43.95ms
step:1189/2160 train_time:52269ms step_avg:43.96ms
step:1190/2160 train_time:52329ms step_avg:43.97ms
step:1191/2160 train_time:52390ms step_avg:43.99ms
step:1192/2160 train_time:52449ms step_avg:44.00ms
step:1193/2160 train_time:52510ms step_avg:44.01ms
step:1194/2160 train_time:52570ms step_avg:44.03ms
step:1195/2160 train_time:52630ms step_avg:44.04ms
step:1196/2160 train_time:52689ms step_avg:44.05ms
step:1197/2160 train_time:52749ms step_avg:44.07ms
step:1198/2160 train_time:52808ms step_avg:44.08ms
step:1199/2160 train_time:52869ms step_avg:44.09ms
step:1200/2160 train_time:52928ms step_avg:44.11ms
step:1201/2160 train_time:52989ms step_avg:44.12ms
step:1202/2160 train_time:53048ms step_avg:44.13ms
step:1203/2160 train_time:53110ms step_avg:44.15ms
step:1204/2160 train_time:53169ms step_avg:44.16ms
step:1205/2160 train_time:53229ms step_avg:44.17ms
step:1206/2160 train_time:53289ms step_avg:44.19ms
step:1207/2160 train_time:53349ms step_avg:44.20ms
step:1208/2160 train_time:53408ms step_avg:44.21ms
step:1209/2160 train_time:53469ms step_avg:44.23ms
step:1210/2160 train_time:53529ms step_avg:44.24ms
step:1211/2160 train_time:53589ms step_avg:44.25ms
step:1212/2160 train_time:53649ms step_avg:44.26ms
step:1213/2160 train_time:53709ms step_avg:44.28ms
step:1214/2160 train_time:53768ms step_avg:44.29ms
step:1215/2160 train_time:53829ms step_avg:44.30ms
step:1216/2160 train_time:53888ms step_avg:44.32ms
step:1217/2160 train_time:53949ms step_avg:44.33ms
step:1218/2160 train_time:54008ms step_avg:44.34ms
step:1219/2160 train_time:54069ms step_avg:44.36ms
step:1220/2160 train_time:54128ms step_avg:44.37ms
step:1221/2160 train_time:54189ms step_avg:44.38ms
step:1222/2160 train_time:54248ms step_avg:44.39ms
step:1223/2160 train_time:54309ms step_avg:44.41ms
step:1224/2160 train_time:54369ms step_avg:44.42ms
step:1225/2160 train_time:54429ms step_avg:44.43ms
step:1226/2160 train_time:54488ms step_avg:44.44ms
step:1227/2160 train_time:54549ms step_avg:44.46ms
step:1228/2160 train_time:54608ms step_avg:44.47ms
step:1229/2160 train_time:54669ms step_avg:44.48ms
step:1230/2160 train_time:54728ms step_avg:44.49ms
step:1231/2160 train_time:54788ms step_avg:44.51ms
step:1232/2160 train_time:54848ms step_avg:44.52ms
step:1233/2160 train_time:54908ms step_avg:44.53ms
step:1234/2160 train_time:54968ms step_avg:44.54ms
step:1235/2160 train_time:55029ms step_avg:44.56ms
step:1236/2160 train_time:55088ms step_avg:44.57ms
step:1237/2160 train_time:55149ms step_avg:44.58ms
step:1238/2160 train_time:55209ms step_avg:44.59ms
step:1239/2160 train_time:55269ms step_avg:44.61ms
step:1240/2160 train_time:55330ms step_avg:44.62ms
step:1241/2160 train_time:55390ms step_avg:44.63ms
step:1242/2160 train_time:55449ms step_avg:44.65ms
step:1243/2160 train_time:55510ms step_avg:44.66ms
step:1244/2160 train_time:55569ms step_avg:44.67ms
step:1245/2160 train_time:55629ms step_avg:44.68ms
step:1246/2160 train_time:55688ms step_avg:44.69ms
step:1247/2160 train_time:55749ms step_avg:44.71ms
step:1248/2160 train_time:55808ms step_avg:44.72ms
step:1249/2160 train_time:55868ms step_avg:44.73ms
step:1250/2160 train_time:55928ms step_avg:44.74ms
step:1250/2160 val_loss:3.5926 train_time:55991ms step_avg:44.79ms
step:1251/2160 train_time:56014ms step_avg:44.78ms
step:1252/2160 train_time:56050ms step_avg:44.77ms
step:1253/2160 train_time:56113ms step_avg:44.78ms
step:1254/2160 train_time:56174ms step_avg:44.80ms
step:1255/2160 train_time:56236ms step_avg:44.81ms
step:1256/2160 train_time:56297ms step_avg:44.82ms
step:1257/2160 train_time:56358ms step_avg:44.84ms
step:1258/2160 train_time:56417ms step_avg:44.85ms
step:1259/2160 train_time:56478ms step_avg:44.86ms
step:1260/2160 train_time:56536ms step_avg:44.87ms
step:1261/2160 train_time:56596ms step_avg:44.88ms
step:1262/2160 train_time:56655ms step_avg:44.89ms
step:1263/2160 train_time:56714ms step_avg:44.90ms
step:1264/2160 train_time:56773ms step_avg:44.92ms
step:1265/2160 train_time:56833ms step_avg:44.93ms
step:1266/2160 train_time:56892ms step_avg:44.94ms
step:1267/2160 train_time:56953ms step_avg:44.95ms
step:1268/2160 train_time:57013ms step_avg:44.96ms
step:1269/2160 train_time:57075ms step_avg:44.98ms
step:1270/2160 train_time:57135ms step_avg:44.99ms
step:1271/2160 train_time:57197ms step_avg:45.00ms
step:1272/2160 train_time:57256ms step_avg:45.01ms
step:1273/2160 train_time:57317ms step_avg:45.03ms
step:1274/2160 train_time:57377ms step_avg:45.04ms
step:1275/2160 train_time:57438ms step_avg:45.05ms
step:1276/2160 train_time:57498ms step_avg:45.06ms
step:1277/2160 train_time:57558ms step_avg:45.07ms
step:1278/2160 train_time:57616ms step_avg:45.08ms
step:1279/2160 train_time:57676ms step_avg:45.09ms
step:1280/2160 train_time:57735ms step_avg:45.11ms
step:1281/2160 train_time:57797ms step_avg:45.12ms
step:1282/2160 train_time:57855ms step_avg:45.13ms
step:1283/2160 train_time:57916ms step_avg:45.14ms
step:1284/2160 train_time:57976ms step_avg:45.15ms
step:1285/2160 train_time:58038ms step_avg:45.17ms
step:1286/2160 train_time:58098ms step_avg:45.18ms
step:1287/2160 train_time:58161ms step_avg:45.19ms
step:1288/2160 train_time:58221ms step_avg:45.20ms
step:1289/2160 train_time:58282ms step_avg:45.22ms
step:1290/2160 train_time:58342ms step_avg:45.23ms
step:1291/2160 train_time:58403ms step_avg:45.24ms
step:1292/2160 train_time:58462ms step_avg:45.25ms
step:1293/2160 train_time:58523ms step_avg:45.26ms
step:1294/2160 train_time:58582ms step_avg:45.27ms
step:1295/2160 train_time:58642ms step_avg:45.28ms
step:1296/2160 train_time:58702ms step_avg:45.29ms
step:1297/2160 train_time:58762ms step_avg:45.31ms
step:1298/2160 train_time:58821ms step_avg:45.32ms
step:1299/2160 train_time:58883ms step_avg:45.33ms
step:1300/2160 train_time:58942ms step_avg:45.34ms
step:1301/2160 train_time:59004ms step_avg:45.35ms
step:1302/2160 train_time:59064ms step_avg:45.36ms
step:1303/2160 train_time:59125ms step_avg:45.38ms
step:1304/2160 train_time:59185ms step_avg:45.39ms
step:1305/2160 train_time:59246ms step_avg:45.40ms
step:1306/2160 train_time:59306ms step_avg:45.41ms
step:1307/2160 train_time:59367ms step_avg:45.42ms
step:1308/2160 train_time:59426ms step_avg:45.43ms
step:1309/2160 train_time:59487ms step_avg:45.44ms
step:1310/2160 train_time:59545ms step_avg:45.45ms
step:1311/2160 train_time:59605ms step_avg:45.47ms
step:1312/2160 train_time:59665ms step_avg:45.48ms
step:1313/2160 train_time:59724ms step_avg:45.49ms
step:1314/2160 train_time:59784ms step_avg:45.50ms
step:1315/2160 train_time:59844ms step_avg:45.51ms
step:1316/2160 train_time:59904ms step_avg:45.52ms
step:1317/2160 train_time:59965ms step_avg:45.53ms
step:1318/2160 train_time:60024ms step_avg:45.54ms
step:1319/2160 train_time:60086ms step_avg:45.55ms
step:1320/2160 train_time:60145ms step_avg:45.56ms
step:1321/2160 train_time:60206ms step_avg:45.58ms
step:1322/2160 train_time:60266ms step_avg:45.59ms
step:1323/2160 train_time:60326ms step_avg:45.60ms
step:1324/2160 train_time:60386ms step_avg:45.61ms
step:1325/2160 train_time:60446ms step_avg:45.62ms
step:1326/2160 train_time:60506ms step_avg:45.63ms
step:1327/2160 train_time:60566ms step_avg:45.64ms
step:1328/2160 train_time:60625ms step_avg:45.65ms
step:1329/2160 train_time:60685ms step_avg:45.66ms
step:1330/2160 train_time:60744ms step_avg:45.67ms
step:1331/2160 train_time:60804ms step_avg:45.68ms
step:1332/2160 train_time:60863ms step_avg:45.69ms
step:1333/2160 train_time:60924ms step_avg:45.70ms
step:1334/2160 train_time:60984ms step_avg:45.71ms
step:1335/2160 train_time:61046ms step_avg:45.73ms
step:1336/2160 train_time:61105ms step_avg:45.74ms
step:1337/2160 train_time:61167ms step_avg:45.75ms
step:1338/2160 train_time:61226ms step_avg:45.76ms
step:1339/2160 train_time:61287ms step_avg:45.77ms
step:1340/2160 train_time:61346ms step_avg:45.78ms
step:1341/2160 train_time:61406ms step_avg:45.79ms
step:1342/2160 train_time:61465ms step_avg:45.80ms
step:1343/2160 train_time:61525ms step_avg:45.81ms
step:1344/2160 train_time:61584ms step_avg:45.82ms
step:1345/2160 train_time:61645ms step_avg:45.83ms
step:1346/2160 train_time:61704ms step_avg:45.84ms
step:1347/2160 train_time:61764ms step_avg:45.85ms
step:1348/2160 train_time:61824ms step_avg:45.86ms
step:1349/2160 train_time:61885ms step_avg:45.87ms
step:1350/2160 train_time:61944ms step_avg:45.88ms
step:1351/2160 train_time:62005ms step_avg:45.90ms
step:1352/2160 train_time:62065ms step_avg:45.91ms
step:1353/2160 train_time:62126ms step_avg:45.92ms
step:1354/2160 train_time:62186ms step_avg:45.93ms
step:1355/2160 train_time:62247ms step_avg:45.94ms
step:1356/2160 train_time:62307ms step_avg:45.95ms
step:1357/2160 train_time:62367ms step_avg:45.96ms
step:1358/2160 train_time:62425ms step_avg:45.97ms
step:1359/2160 train_time:62485ms step_avg:45.98ms
step:1360/2160 train_time:62544ms step_avg:45.99ms
step:1361/2160 train_time:62605ms step_avg:46.00ms
step:1362/2160 train_time:62664ms step_avg:46.01ms
step:1363/2160 train_time:62724ms step_avg:46.02ms
step:1364/2160 train_time:62784ms step_avg:46.03ms
step:1365/2160 train_time:62845ms step_avg:46.04ms
step:1366/2160 train_time:62904ms step_avg:46.05ms
step:1367/2160 train_time:62964ms step_avg:46.06ms
step:1368/2160 train_time:63024ms step_avg:46.07ms
step:1369/2160 train_time:63086ms step_avg:46.08ms
step:1370/2160 train_time:63145ms step_avg:46.09ms
step:1371/2160 train_time:63206ms step_avg:46.10ms
step:1372/2160 train_time:63266ms step_avg:46.11ms
step:1373/2160 train_time:63326ms step_avg:46.12ms
step:1374/2160 train_time:63386ms step_avg:46.13ms
step:1375/2160 train_time:63445ms step_avg:46.14ms
step:1376/2160 train_time:63505ms step_avg:46.15ms
step:1377/2160 train_time:63564ms step_avg:46.16ms
step:1378/2160 train_time:63624ms step_avg:46.17ms
step:1379/2160 train_time:63684ms step_avg:46.18ms
step:1380/2160 train_time:63744ms step_avg:46.19ms
step:1381/2160 train_time:63805ms step_avg:46.20ms
step:1382/2160 train_time:63864ms step_avg:46.21ms
step:1383/2160 train_time:63925ms step_avg:46.22ms
step:1384/2160 train_time:63985ms step_avg:46.23ms
step:1385/2160 train_time:64046ms step_avg:46.24ms
step:1386/2160 train_time:64106ms step_avg:46.25ms
step:1387/2160 train_time:64167ms step_avg:46.26ms
step:1388/2160 train_time:64227ms step_avg:46.27ms
step:1389/2160 train_time:64287ms step_avg:46.28ms
step:1390/2160 train_time:64346ms step_avg:46.29ms
step:1391/2160 train_time:64406ms step_avg:46.30ms
step:1392/2160 train_time:64465ms step_avg:46.31ms
step:1393/2160 train_time:64525ms step_avg:46.32ms
step:1394/2160 train_time:64584ms step_avg:46.33ms
step:1395/2160 train_time:64645ms step_avg:46.34ms
step:1396/2160 train_time:64704ms step_avg:46.35ms
step:1397/2160 train_time:64765ms step_avg:46.36ms
step:1398/2160 train_time:64824ms step_avg:46.37ms
step:1399/2160 train_time:64885ms step_avg:46.38ms
step:1400/2160 train_time:64944ms step_avg:46.39ms
step:1401/2160 train_time:65005ms step_avg:46.40ms
step:1402/2160 train_time:65065ms step_avg:46.41ms
step:1403/2160 train_time:65125ms step_avg:46.42ms
step:1404/2160 train_time:65185ms step_avg:46.43ms
step:1405/2160 train_time:65246ms step_avg:46.44ms
step:1406/2160 train_time:65306ms step_avg:46.45ms
step:1407/2160 train_time:65365ms step_avg:46.46ms
step:1408/2160 train_time:65424ms step_avg:46.47ms
step:1409/2160 train_time:65485ms step_avg:46.48ms
step:1410/2160 train_time:65544ms step_avg:46.49ms
step:1411/2160 train_time:65604ms step_avg:46.50ms
step:1412/2160 train_time:65664ms step_avg:46.50ms
step:1413/2160 train_time:65725ms step_avg:46.51ms
step:1414/2160 train_time:65784ms step_avg:46.52ms
step:1415/2160 train_time:65845ms step_avg:46.53ms
step:1416/2160 train_time:65933ms step_avg:46.56ms
step:1417/2160 train_time:66021ms step_avg:46.59ms
step:1418/2160 train_time:66108ms step_avg:46.62ms
step:1419/2160 train_time:66197ms step_avg:46.65ms
step:1420/2160 train_time:66284ms step_avg:46.68ms
step:1421/2160 train_time:66373ms step_avg:46.71ms
step:1422/2160 train_time:66459ms step_avg:46.74ms
step:1423/2160 train_time:66548ms step_avg:46.77ms
step:1424/2160 train_time:66635ms step_avg:46.79ms
step:1425/2160 train_time:66723ms step_avg:46.82ms
step:1426/2160 train_time:66809ms step_avg:46.85ms
step:1427/2160 train_time:66898ms step_avg:46.88ms
step:1428/2160 train_time:66985ms step_avg:46.91ms
step:1429/2160 train_time:67074ms step_avg:46.94ms
step:1430/2160 train_time:67160ms step_avg:46.96ms
step:1431/2160 train_time:67250ms step_avg:47.00ms
step:1432/2160 train_time:67337ms step_avg:47.02ms
step:1433/2160 train_time:67426ms step_avg:47.05ms
step:1434/2160 train_time:67513ms step_avg:47.08ms
step:1435/2160 train_time:67601ms step_avg:47.11ms
step:1436/2160 train_time:67688ms step_avg:47.14ms
step:1437/2160 train_time:67777ms step_avg:47.17ms
step:1438/2160 train_time:67864ms step_avg:47.19ms
step:1439/2160 train_time:67953ms step_avg:47.22ms
step:1440/2160 train_time:68041ms step_avg:47.25ms
step:1441/2160 train_time:68129ms step_avg:47.28ms
step:1442/2160 train_time:68216ms step_avg:47.31ms
step:1443/2160 train_time:68304ms step_avg:47.33ms
step:1444/2160 train_time:68391ms step_avg:47.36ms
step:1445/2160 train_time:68480ms step_avg:47.39ms
step:1446/2160 train_time:68567ms step_avg:47.42ms
step:1447/2160 train_time:68657ms step_avg:47.45ms
step:1448/2160 train_time:68744ms step_avg:47.48ms
step:1449/2160 train_time:68833ms step_avg:47.50ms
step:1450/2160 train_time:68920ms step_avg:47.53ms
step:1451/2160 train_time:69007ms step_avg:47.56ms
step:1452/2160 train_time:69094ms step_avg:47.59ms
step:1453/2160 train_time:69182ms step_avg:47.61ms
step:1454/2160 train_time:69269ms step_avg:47.64ms
step:1455/2160 train_time:69359ms step_avg:47.67ms
step:1456/2160 train_time:69446ms step_avg:47.70ms
step:1457/2160 train_time:69534ms step_avg:47.72ms
step:1458/2160 train_time:69621ms step_avg:47.75ms
step:1459/2160 train_time:69711ms step_avg:47.78ms
step:1460/2160 train_time:69798ms step_avg:47.81ms
step:1461/2160 train_time:69886ms step_avg:47.83ms
step:1462/2160 train_time:69974ms step_avg:47.86ms
step:1463/2160 train_time:70061ms step_avg:47.89ms
step:1464/2160 train_time:70148ms step_avg:47.92ms
step:1465/2160 train_time:70237ms step_avg:47.94ms
step:1466/2160 train_time:70324ms step_avg:47.97ms
step:1467/2160 train_time:70413ms step_avg:48.00ms
step:1468/2160 train_time:70499ms step_avg:48.02ms
step:1469/2160 train_time:70589ms step_avg:48.05ms
step:1470/2160 train_time:70676ms step_avg:48.08ms
step:1471/2160 train_time:70764ms step_avg:48.11ms
step:1472/2160 train_time:70851ms step_avg:48.13ms
step:1473/2160 train_time:70940ms step_avg:48.16ms
step:1474/2160 train_time:71027ms step_avg:48.19ms
step:1475/2160 train_time:71116ms step_avg:48.21ms
step:1476/2160 train_time:71202ms step_avg:48.24ms
step:1477/2160 train_time:71291ms step_avg:48.27ms
step:1478/2160 train_time:71378ms step_avg:48.29ms
step:1479/2160 train_time:71467ms step_avg:48.32ms
step:1480/2160 train_time:71555ms step_avg:48.35ms
step:1481/2160 train_time:71643ms step_avg:48.37ms
step:1482/2160 train_time:71729ms step_avg:48.40ms
step:1483/2160 train_time:71818ms step_avg:48.43ms
step:1484/2160 train_time:71904ms step_avg:48.45ms
step:1485/2160 train_time:71994ms step_avg:48.48ms
step:1486/2160 train_time:72080ms step_avg:48.51ms
step:1487/2160 train_time:72169ms step_avg:48.53ms
step:1488/2160 train_time:72257ms step_avg:48.56ms
step:1489/2160 train_time:72344ms step_avg:48.59ms
step:1490/2160 train_time:72432ms step_avg:48.61ms
step:1491/2160 train_time:72520ms step_avg:48.64ms
step:1492/2160 train_time:72607ms step_avg:48.66ms
step:1493/2160 train_time:72697ms step_avg:48.69ms
step:1494/2160 train_time:72784ms step_avg:48.72ms
step:1495/2160 train_time:72873ms step_avg:48.74ms
step:1496/2160 train_time:72961ms step_avg:48.77ms
step:1497/2160 train_time:73048ms step_avg:48.80ms
step:1498/2160 train_time:73134ms step_avg:48.82ms
step:1499/2160 train_time:73223ms step_avg:48.85ms
step:1500/2160 train_time:73309ms step_avg:48.87ms
step:1500/2160 val_loss:3.4896 train_time:73399ms step_avg:48.93ms
step:1501/2160 train_time:73423ms step_avg:48.92ms
step:1502/2160 train_time:73490ms step_avg:48.93ms
step:1503/2160 train_time:73582ms step_avg:48.96ms
step:1504/2160 train_time:73670ms step_avg:48.98ms
step:1505/2160 train_time:73758ms step_avg:49.01ms
step:1506/2160 train_time:73844ms step_avg:49.03ms
step:1507/2160 train_time:73931ms step_avg:49.06ms
step:1508/2160 train_time:74016ms step_avg:49.08ms
step:1509/2160 train_time:74104ms step_avg:49.11ms
step:1510/2160 train_time:74190ms step_avg:49.13ms
step:1511/2160 train_time:74278ms step_avg:49.16ms
step:1512/2160 train_time:74367ms step_avg:49.18ms
step:1513/2160 train_time:74459ms step_avg:49.21ms
step:1514/2160 train_time:74549ms step_avg:49.24ms
step:1515/2160 train_time:74639ms step_avg:49.27ms
step:1516/2160 train_time:74726ms step_avg:49.29ms
step:1517/2160 train_time:74814ms step_avg:49.32ms
step:1518/2160 train_time:74900ms step_avg:49.34ms
step:1519/2160 train_time:74987ms step_avg:49.37ms
step:1520/2160 train_time:75073ms step_avg:49.39ms
step:1521/2160 train_time:75161ms step_avg:49.42ms
step:1522/2160 train_time:75248ms step_avg:49.44ms
step:1523/2160 train_time:75336ms step_avg:49.47ms
step:1524/2160 train_time:75424ms step_avg:49.49ms
step:1525/2160 train_time:75514ms step_avg:49.52ms
step:1526/2160 train_time:75602ms step_avg:49.54ms
step:1527/2160 train_time:75691ms step_avg:49.57ms
step:1528/2160 train_time:75778ms step_avg:49.59ms
step:1529/2160 train_time:75867ms step_avg:49.62ms
step:1530/2160 train_time:75952ms step_avg:49.64ms
step:1531/2160 train_time:76040ms step_avg:49.67ms
step:1532/2160 train_time:76127ms step_avg:49.69ms
step:1533/2160 train_time:76215ms step_avg:49.72ms
step:1534/2160 train_time:76302ms step_avg:49.74ms
step:1535/2160 train_time:76392ms step_avg:49.77ms
step:1536/2160 train_time:76480ms step_avg:49.79ms
step:1537/2160 train_time:76570ms step_avg:49.82ms
step:1538/2160 train_time:76657ms step_avg:49.84ms
step:1539/2160 train_time:76746ms step_avg:49.87ms
step:1540/2160 train_time:76832ms step_avg:49.89ms
step:1541/2160 train_time:76920ms step_avg:49.92ms
step:1542/2160 train_time:77007ms step_avg:49.94ms
step:1543/2160 train_time:77094ms step_avg:49.96ms
step:1544/2160 train_time:77181ms step_avg:49.99ms
step:1545/2160 train_time:77270ms step_avg:50.01ms
step:1546/2160 train_time:77357ms step_avg:50.04ms
step:1547/2160 train_time:77447ms step_avg:50.06ms
step:1548/2160 train_time:77533ms step_avg:50.09ms
step:1549/2160 train_time:77622ms step_avg:50.11ms
step:1550/2160 train_time:77709ms step_avg:50.13ms
step:1551/2160 train_time:77797ms step_avg:50.16ms
step:1552/2160 train_time:77884ms step_avg:50.18ms
step:1553/2160 train_time:77972ms step_avg:50.21ms
step:1554/2160 train_time:78059ms step_avg:50.23ms
step:1555/2160 train_time:78148ms step_avg:50.26ms
step:1556/2160 train_time:78235ms step_avg:50.28ms
step:1557/2160 train_time:78325ms step_avg:50.30ms
step:1558/2160 train_time:78411ms step_avg:50.33ms
step:1559/2160 train_time:78501ms step_avg:50.35ms
step:1560/2160 train_time:78588ms step_avg:50.38ms
step:1561/2160 train_time:78676ms step_avg:50.40ms
step:1562/2160 train_time:78763ms step_avg:50.42ms
step:1563/2160 train_time:78851ms step_avg:50.45ms
step:1564/2160 train_time:78938ms step_avg:50.47ms
step:1565/2160 train_time:79027ms step_avg:50.50ms
step:1566/2160 train_time:79113ms step_avg:50.52ms
step:1567/2160 train_time:79201ms step_avg:50.54ms
step:1568/2160 train_time:79289ms step_avg:50.57ms
step:1569/2160 train_time:79377ms step_avg:50.59ms
step:1570/2160 train_time:79464ms step_avg:50.61ms
step:1571/2160 train_time:79553ms step_avg:50.64ms
step:1572/2160 train_time:79639ms step_avg:50.66ms
step:1573/2160 train_time:79728ms step_avg:50.69ms
step:1574/2160 train_time:79814ms step_avg:50.71ms
step:1575/2160 train_time:79903ms step_avg:50.73ms
step:1576/2160 train_time:79990ms step_avg:50.76ms
step:1577/2160 train_time:80078ms step_avg:50.78ms
step:1578/2160 train_time:80166ms step_avg:50.80ms
step:1579/2160 train_time:80254ms step_avg:50.83ms
step:1580/2160 train_time:80342ms step_avg:50.85ms
step:1581/2160 train_time:80430ms step_avg:50.87ms
step:1582/2160 train_time:80516ms step_avg:50.90ms
step:1583/2160 train_time:80605ms step_avg:50.92ms
step:1584/2160 train_time:80691ms step_avg:50.94ms
step:1585/2160 train_time:80780ms step_avg:50.97ms
step:1586/2160 train_time:80867ms step_avg:50.99ms
step:1587/2160 train_time:80955ms step_avg:51.01ms
step:1588/2160 train_time:81042ms step_avg:51.03ms
step:1589/2160 train_time:81131ms step_avg:51.06ms
step:1590/2160 train_time:81218ms step_avg:51.08ms
step:1591/2160 train_time:81308ms step_avg:51.10ms
step:1592/2160 train_time:81394ms step_avg:51.13ms
step:1593/2160 train_time:81483ms step_avg:51.15ms
step:1594/2160 train_time:81571ms step_avg:51.17ms
step:1595/2160 train_time:81658ms step_avg:51.20ms
step:1596/2160 train_time:81745ms step_avg:51.22ms
step:1597/2160 train_time:81833ms step_avg:51.24ms
step:1598/2160 train_time:81921ms step_avg:51.26ms
step:1599/2160 train_time:82010ms step_avg:51.29ms
step:1600/2160 train_time:82097ms step_avg:51.31ms
step:1601/2160 train_time:82186ms step_avg:51.33ms
step:1602/2160 train_time:82273ms step_avg:51.36ms
step:1603/2160 train_time:82361ms step_avg:51.38ms
step:1604/2160 train_time:82449ms step_avg:51.40ms
step:1605/2160 train_time:82538ms step_avg:51.43ms
step:1606/2160 train_time:82625ms step_avg:51.45ms
step:1607/2160 train_time:82713ms step_avg:51.47ms
step:1608/2160 train_time:82799ms step_avg:51.49ms
step:1609/2160 train_time:82889ms step_avg:51.52ms
step:1610/2160 train_time:82976ms step_avg:51.54ms
step:1611/2160 train_time:83064ms step_avg:51.56ms
step:1612/2160 train_time:83151ms step_avg:51.58ms
step:1613/2160 train_time:83238ms step_avg:51.60ms
step:1614/2160 train_time:83326ms step_avg:51.63ms
step:1615/2160 train_time:83413ms step_avg:51.65ms
step:1616/2160 train_time:83500ms step_avg:51.67ms
step:1617/2160 train_time:83590ms step_avg:51.69ms
step:1618/2160 train_time:83676ms step_avg:51.72ms
step:1619/2160 train_time:83766ms step_avg:51.74ms
step:1620/2160 train_time:83852ms step_avg:51.76ms
step:1621/2160 train_time:83941ms step_avg:51.78ms
step:1622/2160 train_time:84028ms step_avg:51.81ms
step:1623/2160 train_time:84117ms step_avg:51.83ms
step:1624/2160 train_time:84203ms step_avg:51.85ms
step:1625/2160 train_time:84292ms step_avg:51.87ms
step:1626/2160 train_time:84379ms step_avg:51.89ms
step:1627/2160 train_time:84469ms step_avg:51.92ms
step:1628/2160 train_time:84555ms step_avg:51.94ms
step:1629/2160 train_time:84644ms step_avg:51.96ms
step:1630/2160 train_time:84731ms step_avg:51.98ms
step:1631/2160 train_time:84820ms step_avg:52.01ms
step:1632/2160 train_time:84908ms step_avg:52.03ms
step:1633/2160 train_time:84997ms step_avg:52.05ms
step:1634/2160 train_time:85083ms step_avg:52.07ms
step:1635/2160 train_time:85172ms step_avg:52.09ms
step:1636/2160 train_time:85260ms step_avg:52.11ms
step:1637/2160 train_time:85349ms step_avg:52.14ms
step:1638/2160 train_time:85435ms step_avg:52.16ms
step:1639/2160 train_time:85524ms step_avg:52.18ms
step:1640/2160 train_time:85611ms step_avg:52.20ms
step:1641/2160 train_time:85699ms step_avg:52.22ms
step:1642/2160 train_time:85786ms step_avg:52.24ms
step:1643/2160 train_time:85874ms step_avg:52.27ms
step:1644/2160 train_time:85961ms step_avg:52.29ms
step:1645/2160 train_time:86049ms step_avg:52.31ms
step:1646/2160 train_time:86135ms step_avg:52.33ms
step:1647/2160 train_time:86225ms step_avg:52.35ms
step:1648/2160 train_time:86312ms step_avg:52.37ms
step:1649/2160 train_time:86399ms step_avg:52.39ms
step:1650/2160 train_time:86486ms step_avg:52.42ms
step:1651/2160 train_time:86574ms step_avg:52.44ms
step:1652/2160 train_time:86661ms step_avg:52.46ms
step:1653/2160 train_time:86750ms step_avg:52.48ms
step:1654/2160 train_time:86837ms step_avg:52.50ms
step:1655/2160 train_time:86926ms step_avg:52.52ms
step:1656/2160 train_time:87013ms step_avg:52.54ms
step:1657/2160 train_time:87101ms step_avg:52.57ms
step:1658/2160 train_time:87189ms step_avg:52.59ms
step:1659/2160 train_time:87278ms step_avg:52.61ms
step:1660/2160 train_time:87365ms step_avg:52.63ms
step:1661/2160 train_time:87454ms step_avg:52.65ms
step:1662/2160 train_time:87540ms step_avg:52.67ms
step:1663/2160 train_time:87629ms step_avg:52.69ms
step:1664/2160 train_time:87717ms step_avg:52.71ms
step:1665/2160 train_time:87805ms step_avg:52.74ms
step:1666/2160 train_time:87892ms step_avg:52.76ms
step:1667/2160 train_time:87981ms step_avg:52.78ms
step:1668/2160 train_time:88068ms step_avg:52.80ms
step:1669/2160 train_time:88157ms step_avg:52.82ms
step:1670/2160 train_time:88244ms step_avg:52.84ms
step:1671/2160 train_time:88332ms step_avg:52.86ms
step:1672/2160 train_time:88418ms step_avg:52.88ms
step:1673/2160 train_time:88507ms step_avg:52.90ms
step:1674/2160 train_time:88593ms step_avg:52.92ms
step:1675/2160 train_time:88682ms step_avg:52.94ms
step:1676/2160 train_time:88769ms step_avg:52.96ms
step:1677/2160 train_time:88857ms step_avg:52.99ms
step:1678/2160 train_time:88944ms step_avg:53.01ms
step:1679/2160 train_time:89032ms step_avg:53.03ms
step:1680/2160 train_time:89120ms step_avg:53.05ms
step:1681/2160 train_time:89209ms step_avg:53.07ms
step:1682/2160 train_time:89296ms step_avg:53.09ms
step:1683/2160 train_time:89386ms step_avg:53.11ms
step:1684/2160 train_time:89471ms step_avg:53.13ms
step:1685/2160 train_time:89559ms step_avg:53.15ms
step:1686/2160 train_time:89646ms step_avg:53.17ms
step:1687/2160 train_time:89734ms step_avg:53.19ms
step:1688/2160 train_time:89822ms step_avg:53.21ms
step:1689/2160 train_time:89911ms step_avg:53.23ms
step:1690/2160 train_time:89998ms step_avg:53.25ms
step:1691/2160 train_time:90087ms step_avg:53.27ms
step:1692/2160 train_time:90174ms step_avg:53.29ms
step:1693/2160 train_time:90262ms step_avg:53.31ms
step:1694/2160 train_time:90349ms step_avg:53.33ms
step:1695/2160 train_time:90437ms step_avg:53.36ms
step:1696/2160 train_time:90524ms step_avg:53.38ms
step:1697/2160 train_time:90613ms step_avg:53.40ms
step:1698/2160 train_time:90700ms step_avg:53.42ms
step:1699/2160 train_time:90788ms step_avg:53.44ms
step:1700/2160 train_time:90875ms step_avg:53.46ms
step:1701/2160 train_time:90963ms step_avg:53.48ms
step:1702/2160 train_time:91051ms step_avg:53.50ms
step:1703/2160 train_time:91140ms step_avg:53.52ms
step:1704/2160 train_time:91227ms step_avg:53.54ms
step:1705/2160 train_time:91315ms step_avg:53.56ms
step:1706/2160 train_time:91402ms step_avg:53.58ms
step:1707/2160 train_time:91491ms step_avg:53.60ms
step:1708/2160 train_time:91579ms step_avg:53.62ms
step:1709/2160 train_time:91668ms step_avg:53.64ms
step:1710/2160 train_time:91755ms step_avg:53.66ms
step:1711/2160 train_time:91843ms step_avg:53.68ms
step:1712/2160 train_time:91930ms step_avg:53.70ms
step:1713/2160 train_time:92018ms step_avg:53.72ms
step:1714/2160 train_time:92105ms step_avg:53.74ms
step:1715/2160 train_time:92194ms step_avg:53.76ms
step:1716/2160 train_time:92281ms step_avg:53.78ms
step:1717/2160 train_time:92370ms step_avg:53.80ms
step:1718/2160 train_time:92457ms step_avg:53.82ms
step:1719/2160 train_time:92546ms step_avg:53.84ms
step:1720/2160 train_time:92632ms step_avg:53.86ms
step:1721/2160 train_time:92721ms step_avg:53.88ms
step:1722/2160 train_time:92808ms step_avg:53.90ms
step:1723/2160 train_time:92896ms step_avg:53.92ms
step:1724/2160 train_time:92984ms step_avg:53.94ms
step:1725/2160 train_time:93072ms step_avg:53.95ms
step:1726/2160 train_time:93159ms step_avg:53.97ms
step:1727/2160 train_time:93248ms step_avg:53.99ms
step:1728/2160 train_time:93334ms step_avg:54.01ms
step:1729/2160 train_time:93423ms step_avg:54.03ms
step:1730/2160 train_time:93509ms step_avg:54.05ms
step:1731/2160 train_time:93596ms step_avg:54.07ms
step:1732/2160 train_time:93682ms step_avg:54.09ms
step:1733/2160 train_time:93771ms step_avg:54.11ms
step:1734/2160 train_time:93858ms step_avg:54.13ms
step:1735/2160 train_time:93947ms step_avg:54.15ms
step:1736/2160 train_time:94034ms step_avg:54.17ms
step:1737/2160 train_time:94124ms step_avg:54.19ms
step:1738/2160 train_time:94210ms step_avg:54.21ms
step:1739/2160 train_time:94299ms step_avg:54.23ms
step:1740/2160 train_time:94386ms step_avg:54.25ms
step:1741/2160 train_time:94475ms step_avg:54.26ms
step:1742/2160 train_time:94562ms step_avg:54.28ms
step:1743/2160 train_time:94651ms step_avg:54.30ms
step:1744/2160 train_time:94736ms step_avg:54.32ms
step:1745/2160 train_time:94826ms step_avg:54.34ms
step:1746/2160 train_time:94912ms step_avg:54.36ms
step:1747/2160 train_time:95000ms step_avg:54.38ms
step:1748/2160 train_time:95088ms step_avg:54.40ms
step:1749/2160 train_time:95177ms step_avg:54.42ms
step:1750/2160 train_time:95264ms step_avg:54.44ms
step:1750/2160 val_loss:3.3914 train_time:95354ms step_avg:54.49ms
step:1751/2160 train_time:95377ms step_avg:54.47ms
step:1752/2160 train_time:95444ms step_avg:54.48ms
step:1753/2160 train_time:95535ms step_avg:54.50ms
step:1754/2160 train_time:95625ms step_avg:54.52ms
step:1755/2160 train_time:95714ms step_avg:54.54ms
step:1756/2160 train_time:95799ms step_avg:54.56ms
step:1757/2160 train_time:95886ms step_avg:54.57ms
step:1758/2160 train_time:95972ms step_avg:54.59ms
step:1759/2160 train_time:96058ms step_avg:54.61ms
step:1760/2160 train_time:96146ms step_avg:54.63ms
step:1761/2160 train_time:96232ms step_avg:54.65ms
step:1762/2160 train_time:96320ms step_avg:54.67ms
step:1763/2160 train_time:96410ms step_avg:54.69ms
step:1764/2160 train_time:96499ms step_avg:54.70ms
step:1765/2160 train_time:96591ms step_avg:54.73ms
step:1766/2160 train_time:96679ms step_avg:54.74ms
step:1767/2160 train_time:96767ms step_avg:54.76ms
step:1768/2160 train_time:96852ms step_avg:54.78ms
step:1769/2160 train_time:96939ms step_avg:54.80ms
step:1770/2160 train_time:97025ms step_avg:54.82ms
step:1771/2160 train_time:97111ms step_avg:54.83ms
step:1772/2160 train_time:97198ms step_avg:54.85ms
step:1773/2160 train_time:97287ms step_avg:54.87ms
step:1774/2160 train_time:97375ms step_avg:54.89ms
step:1775/2160 train_time:97467ms step_avg:54.91ms
step:1776/2160 train_time:97556ms step_avg:54.93ms
step:1777/2160 train_time:97647ms step_avg:54.95ms
step:1778/2160 train_time:97734ms step_avg:54.97ms
step:1779/2160 train_time:97823ms step_avg:54.99ms
step:1780/2160 train_time:97909ms step_avg:55.00ms
step:1781/2160 train_time:97996ms step_avg:55.02ms
step:1782/2160 train_time:98082ms step_avg:55.04ms
step:1783/2160 train_time:98168ms step_avg:55.06ms
step:1784/2160 train_time:98255ms step_avg:55.08ms
step:1785/2160 train_time:98344ms step_avg:55.09ms
step:1786/2160 train_time:98433ms step_avg:55.11ms
step:1787/2160 train_time:98523ms step_avg:55.13ms
step:1788/2160 train_time:98611ms step_avg:55.15ms
step:1789/2160 train_time:98700ms step_avg:55.17ms
step:1790/2160 train_time:98786ms step_avg:55.19ms
step:1791/2160 train_time:98875ms step_avg:55.21ms
step:1792/2160 train_time:98961ms step_avg:55.22ms
step:1793/2160 train_time:99048ms step_avg:55.24ms
step:1794/2160 train_time:99134ms step_avg:55.26ms
step:1795/2160 train_time:99221ms step_avg:55.28ms
step:1796/2160 train_time:99307ms step_avg:55.29ms
step:1797/2160 train_time:99397ms step_avg:55.31ms
step:1798/2160 train_time:99485ms step_avg:55.33ms
step:1799/2160 train_time:99575ms step_avg:55.35ms
step:1800/2160 train_time:99664ms step_avg:55.37ms
step:1801/2160 train_time:99753ms step_avg:55.39ms
step:1802/2160 train_time:99839ms step_avg:55.40ms
step:1803/2160 train_time:99928ms step_avg:55.42ms
step:1804/2160 train_time:100013ms step_avg:55.44ms
step:1805/2160 train_time:100101ms step_avg:55.46ms
step:1806/2160 train_time:100187ms step_avg:55.47ms
step:1807/2160 train_time:100275ms step_avg:55.49ms
step:1808/2160 train_time:100363ms step_avg:55.51ms
step:1809/2160 train_time:100452ms step_avg:55.53ms
step:1810/2160 train_time:100540ms step_avg:55.55ms
step:1811/2160 train_time:100629ms step_avg:55.57ms
step:1812/2160 train_time:100716ms step_avg:55.58ms
step:1813/2160 train_time:100805ms step_avg:55.60ms
step:1814/2160 train_time:100891ms step_avg:55.62ms
step:1815/2160 train_time:100980ms step_avg:55.64ms
step:1816/2160 train_time:101066ms step_avg:55.65ms
step:1817/2160 train_time:101153ms step_avg:55.67ms
step:1818/2160 train_time:101240ms step_avg:55.69ms
step:1819/2160 train_time:101328ms step_avg:55.71ms
step:1820/2160 train_time:101415ms step_avg:55.72ms
step:1821/2160 train_time:101505ms step_avg:55.74ms
step:1822/2160 train_time:101593ms step_avg:55.76ms
step:1823/2160 train_time:101683ms step_avg:55.78ms
step:1824/2160 train_time:101769ms step_avg:55.79ms
step:1825/2160 train_time:101858ms step_avg:55.81ms
step:1826/2160 train_time:101944ms step_avg:55.83ms
step:1827/2160 train_time:102031ms step_avg:55.85ms
step:1828/2160 train_time:102117ms step_avg:55.86ms
step:1829/2160 train_time:102206ms step_avg:55.88ms
step:1830/2160 train_time:102292ms step_avg:55.90ms
step:1831/2160 train_time:102381ms step_avg:55.92ms
step:1832/2160 train_time:102469ms step_avg:55.93ms
step:1833/2160 train_time:102557ms step_avg:55.95ms
step:1834/2160 train_time:102644ms step_avg:55.97ms
step:1835/2160 train_time:102733ms step_avg:55.99ms
step:1836/2160 train_time:102821ms step_avg:56.00ms
step:1837/2160 train_time:102909ms step_avg:56.02ms
step:1838/2160 train_time:102995ms step_avg:56.04ms
step:1839/2160 train_time:103084ms step_avg:56.05ms
step:1840/2160 train_time:103170ms step_avg:56.07ms
step:1841/2160 train_time:103259ms step_avg:56.09ms
step:1842/2160 train_time:103346ms step_avg:56.11ms
step:1843/2160 train_time:103434ms step_avg:56.12ms
step:1844/2160 train_time:103523ms step_avg:56.14ms
step:1845/2160 train_time:103612ms step_avg:56.16ms
step:1846/2160 train_time:103700ms step_avg:56.18ms
step:1847/2160 train_time:103788ms step_avg:56.19ms
step:1848/2160 train_time:103875ms step_avg:56.21ms
step:1849/2160 train_time:103963ms step_avg:56.23ms
step:1850/2160 train_time:104049ms step_avg:56.24ms
step:1851/2160 train_time:104137ms step_avg:56.26ms
step:1852/2160 train_time:104224ms step_avg:56.28ms
step:1853/2160 train_time:104313ms step_avg:56.29ms
step:1854/2160 train_time:104400ms step_avg:56.31ms
step:1855/2160 train_time:104488ms step_avg:56.33ms
step:1856/2160 train_time:104575ms step_avg:56.34ms
step:1857/2160 train_time:104664ms step_avg:56.36ms
step:1858/2160 train_time:104752ms step_avg:56.38ms
step:1859/2160 train_time:104841ms step_avg:56.40ms
step:1860/2160 train_time:104927ms step_avg:56.41ms
step:1861/2160 train_time:105015ms step_avg:56.43ms
step:1862/2160 train_time:105102ms step_avg:56.45ms
step:1863/2160 train_time:105191ms step_avg:56.46ms
step:1864/2160 train_time:105278ms step_avg:56.48ms
step:1865/2160 train_time:105367ms step_avg:56.50ms
step:1866/2160 train_time:105454ms step_avg:56.51ms
step:1867/2160 train_time:105543ms step_avg:56.53ms
step:1868/2160 train_time:105629ms step_avg:56.55ms
step:1869/2160 train_time:105719ms step_avg:56.56ms
step:1870/2160 train_time:105805ms step_avg:56.58ms
step:1871/2160 train_time:105893ms step_avg:56.60ms
step:1872/2160 train_time:105980ms step_avg:56.61ms
step:1873/2160 train_time:106068ms step_avg:56.63ms
step:1874/2160 train_time:106155ms step_avg:56.65ms
step:1875/2160 train_time:106245ms step_avg:56.66ms
step:1876/2160 train_time:106331ms step_avg:56.68ms
step:1877/2160 train_time:106421ms step_avg:56.70ms
step:1878/2160 train_time:106508ms step_avg:56.71ms
step:1879/2160 train_time:106597ms step_avg:56.73ms
step:1880/2160 train_time:106684ms step_avg:56.75ms
step:1881/2160 train_time:106773ms step_avg:56.76ms
step:1882/2160 train_time:106860ms step_avg:56.78ms
step:1883/2160 train_time:106948ms step_avg:56.80ms
step:1884/2160 train_time:107034ms step_avg:56.81ms
step:1885/2160 train_time:107123ms step_avg:56.83ms
step:1886/2160 train_time:107209ms step_avg:56.84ms
step:1887/2160 train_time:107298ms step_avg:56.86ms
step:1888/2160 train_time:107385ms step_avg:56.88ms
step:1889/2160 train_time:107473ms step_avg:56.89ms
step:1890/2160 train_time:107560ms step_avg:56.91ms
step:1891/2160 train_time:107649ms step_avg:56.93ms
step:1892/2160 train_time:107735ms step_avg:56.94ms
step:1893/2160 train_time:107824ms step_avg:56.96ms
step:1894/2160 train_time:107910ms step_avg:56.97ms
step:1895/2160 train_time:107998ms step_avg:56.99ms
step:1896/2160 train_time:108085ms step_avg:57.01ms
step:1897/2160 train_time:108172ms step_avg:57.02ms
step:1898/2160 train_time:108259ms step_avg:57.04ms
step:1899/2160 train_time:108348ms step_avg:57.06ms
step:1900/2160 train_time:108434ms step_avg:57.07ms
step:1901/2160 train_time:108524ms step_avg:57.09ms
step:1902/2160 train_time:108610ms step_avg:57.10ms
step:1903/2160 train_time:108698ms step_avg:57.12ms
step:1904/2160 train_time:108784ms step_avg:57.13ms
step:1905/2160 train_time:108873ms step_avg:57.15ms
step:1906/2160 train_time:108959ms step_avg:57.17ms
step:1907/2160 train_time:109048ms step_avg:57.18ms
step:1908/2160 train_time:109134ms step_avg:57.20ms
step:1909/2160 train_time:109224ms step_avg:57.22ms
step:1910/2160 train_time:109309ms step_avg:57.23ms
step:1911/2160 train_time:109399ms step_avg:57.25ms
step:1912/2160 train_time:109486ms step_avg:57.26ms
step:1913/2160 train_time:109575ms step_avg:57.28ms
step:1914/2160 train_time:109661ms step_avg:57.29ms
step:1915/2160 train_time:109749ms step_avg:57.31ms
step:1916/2160 train_time:109836ms step_avg:57.33ms
step:1917/2160 train_time:109925ms step_avg:57.34ms
step:1918/2160 train_time:110011ms step_avg:57.36ms
step:1919/2160 train_time:110100ms step_avg:57.37ms
step:1920/2160 train_time:110186ms step_avg:57.39ms
step:1921/2160 train_time:110274ms step_avg:57.40ms
step:1922/2160 train_time:110361ms step_avg:57.42ms
step:1923/2160 train_time:110449ms step_avg:57.44ms
step:1924/2160 train_time:110536ms step_avg:57.45ms
step:1925/2160 train_time:110625ms step_avg:57.47ms
step:1926/2160 train_time:110713ms step_avg:57.48ms
step:1927/2160 train_time:110802ms step_avg:57.50ms
step:1928/2160 train_time:110888ms step_avg:57.51ms
step:1929/2160 train_time:110976ms step_avg:57.53ms
step:1930/2160 train_time:111063ms step_avg:57.55ms
step:1931/2160 train_time:111152ms step_avg:57.56ms
step:1932/2160 train_time:111238ms step_avg:57.58ms
step:1933/2160 train_time:111327ms step_avg:57.59ms
step:1934/2160 train_time:111414ms step_avg:57.61ms
step:1935/2160 train_time:111503ms step_avg:57.62ms
step:1936/2160 train_time:111590ms step_avg:57.64ms
step:1937/2160 train_time:111680ms step_avg:57.66ms
step:1938/2160 train_time:111766ms step_avg:57.67ms
step:1939/2160 train_time:111854ms step_avg:57.69ms
step:1940/2160 train_time:111941ms step_avg:57.70ms
step:1941/2160 train_time:112029ms step_avg:57.72ms
step:1942/2160 train_time:112116ms step_avg:57.73ms
step:1943/2160 train_time:112206ms step_avg:57.75ms
step:1944/2160 train_time:112293ms step_avg:57.76ms
step:1945/2160 train_time:112382ms step_avg:57.78ms
step:1946/2160 train_time:112467ms step_avg:57.79ms
step:1947/2160 train_time:112556ms step_avg:57.81ms
step:1948/2160 train_time:112642ms step_avg:57.82ms
step:1949/2160 train_time:112729ms step_avg:57.84ms
step:1950/2160 train_time:112816ms step_avg:57.85ms
step:1951/2160 train_time:112905ms step_avg:57.87ms
step:1952/2160 train_time:112991ms step_avg:57.88ms
step:1953/2160 train_time:113080ms step_avg:57.90ms
step:1954/2160 train_time:113166ms step_avg:57.92ms
step:1955/2160 train_time:113254ms step_avg:57.93ms
step:1956/2160 train_time:113342ms step_avg:57.95ms
step:1957/2160 train_time:113429ms step_avg:57.96ms
step:1958/2160 train_time:113516ms step_avg:57.98ms
step:1959/2160 train_time:113605ms step_avg:57.99ms
step:1960/2160 train_time:113691ms step_avg:58.01ms
step:1961/2160 train_time:113779ms step_avg:58.02ms
step:1962/2160 train_time:113866ms step_avg:58.04ms
step:1963/2160 train_time:113954ms step_avg:58.05ms
step:1964/2160 train_time:114041ms step_avg:58.07ms
step:1965/2160 train_time:114129ms step_avg:58.08ms
step:1966/2160 train_time:114216ms step_avg:58.10ms
step:1967/2160 train_time:114306ms step_avg:58.11ms
step:1968/2160 train_time:114393ms step_avg:58.13ms
step:1969/2160 train_time:114481ms step_avg:58.14ms
step:1970/2160 train_time:114567ms step_avg:58.16ms
step:1971/2160 train_time:114656ms step_avg:58.17ms
step:1972/2160 train_time:114742ms step_avg:58.19ms
step:1973/2160 train_time:114830ms step_avg:58.20ms
step:1974/2160 train_time:114917ms step_avg:58.22ms
step:1975/2160 train_time:115005ms step_avg:58.23ms
step:1976/2160 train_time:115091ms step_avg:58.24ms
step:1977/2160 train_time:115180ms step_avg:58.26ms
step:1978/2160 train_time:115266ms step_avg:58.27ms
step:1979/2160 train_time:115355ms step_avg:58.29ms
step:1980/2160 train_time:115442ms step_avg:58.30ms
step:1981/2160 train_time:115530ms step_avg:58.32ms
step:1982/2160 train_time:115617ms step_avg:58.33ms
step:1983/2160 train_time:115706ms step_avg:58.35ms
step:1984/2160 train_time:115792ms step_avg:58.36ms
step:1985/2160 train_time:115881ms step_avg:58.38ms
step:1986/2160 train_time:115967ms step_avg:58.39ms
step:1987/2160 train_time:116055ms step_avg:58.41ms
step:1988/2160 train_time:116142ms step_avg:58.42ms
step:1989/2160 train_time:116231ms step_avg:58.44ms
step:1990/2160 train_time:116318ms step_avg:58.45ms
step:1991/2160 train_time:116407ms step_avg:58.47ms
step:1992/2160 train_time:116493ms step_avg:58.48ms
step:1993/2160 train_time:116582ms step_avg:58.50ms
step:1994/2160 train_time:116669ms step_avg:58.51ms
step:1995/2160 train_time:116758ms step_avg:58.53ms
step:1996/2160 train_time:116845ms step_avg:58.54ms
step:1997/2160 train_time:116932ms step_avg:58.55ms
step:1998/2160 train_time:117019ms step_avg:58.57ms
step:1999/2160 train_time:117108ms step_avg:58.58ms
step:2000/2160 train_time:117195ms step_avg:58.60ms
step:2000/2160 val_loss:3.3129 train_time:117286ms step_avg:58.64ms
step:2001/2160 train_time:117309ms step_avg:58.63ms
step:2002/2160 train_time:117374ms step_avg:58.63ms
step:2003/2160 train_time:117466ms step_avg:58.65ms
step:2004/2160 train_time:117554ms step_avg:58.66ms
step:2005/2160 train_time:117643ms step_avg:58.67ms
step:2006/2160 train_time:117729ms step_avg:58.69ms
step:2007/2160 train_time:117817ms step_avg:58.70ms
step:2008/2160 train_time:117902ms step_avg:58.72ms
step:2009/2160 train_time:117989ms step_avg:58.73ms
step:2010/2160 train_time:118075ms step_avg:58.74ms
step:2011/2160 train_time:118161ms step_avg:58.76ms
step:2012/2160 train_time:118248ms step_avg:58.77ms
step:2013/2160 train_time:118339ms step_avg:58.79ms
step:2014/2160 train_time:118428ms step_avg:58.80ms
step:2015/2160 train_time:118520ms step_avg:58.82ms
step:2016/2160 train_time:118606ms step_avg:58.83ms
step:2017/2160 train_time:118695ms step_avg:58.85ms
step:2018/2160 train_time:118781ms step_avg:58.86ms
step:2019/2160 train_time:118867ms step_avg:58.87ms
step:2020/2160 train_time:118953ms step_avg:58.89ms
step:2021/2160 train_time:119041ms step_avg:58.90ms
step:2022/2160 train_time:119126ms step_avg:58.92ms
step:2023/2160 train_time:119215ms step_avg:58.93ms
step:2024/2160 train_time:119302ms step_avg:58.94ms
step:2025/2160 train_time:119393ms step_avg:58.96ms
step:2026/2160 train_time:119481ms step_avg:58.97ms
step:2027/2160 train_time:119569ms step_avg:58.99ms
step:2028/2160 train_time:119656ms step_avg:59.00ms
step:2029/2160 train_time:119744ms step_avg:59.02ms
step:2030/2160 train_time:119831ms step_avg:59.03ms
step:2031/2160 train_time:119919ms step_avg:59.04ms
step:2032/2160 train_time:120004ms step_avg:59.06ms
step:2033/2160 train_time:120091ms step_avg:59.07ms
step:2034/2160 train_time:120178ms step_avg:59.08ms
step:2035/2160 train_time:120266ms step_avg:59.10ms
step:2036/2160 train_time:120355ms step_avg:59.11ms
step:2037/2160 train_time:120444ms step_avg:59.13ms
step:2038/2160 train_time:120531ms step_avg:59.14ms
step:2039/2160 train_time:120621ms step_avg:59.16ms
step:2040/2160 train_time:120707ms step_avg:59.17ms
step:2041/2160 train_time:120795ms step_avg:59.18ms
step:2042/2160 train_time:120880ms step_avg:59.20ms
step:2043/2160 train_time:120967ms step_avg:59.21ms
step:2044/2160 train_time:121053ms step_avg:59.22ms
step:2045/2160 train_time:121141ms step_avg:59.24ms
step:2046/2160 train_time:121228ms step_avg:59.25ms
step:2047/2160 train_time:121318ms step_avg:59.27ms
step:2048/2160 train_time:121406ms step_avg:59.28ms
step:2049/2160 train_time:121495ms step_avg:59.29ms
step:2050/2160 train_time:121582ms step_avg:59.31ms
step:2051/2160 train_time:121671ms step_avg:59.32ms
step:2052/2160 train_time:121758ms step_avg:59.34ms
step:2053/2160 train_time:121846ms step_avg:59.35ms
step:2054/2160 train_time:121933ms step_avg:59.36ms
step:2055/2160 train_time:122020ms step_avg:59.38ms
step:2056/2160 train_time:122106ms step_avg:59.39ms
step:2057/2160 train_time:122194ms step_avg:59.40ms
step:2058/2160 train_time:122281ms step_avg:59.42ms
step:2059/2160 train_time:122369ms step_avg:59.43ms
step:2060/2160 train_time:122456ms step_avg:59.44ms
step:2061/2160 train_time:122544ms step_avg:59.46ms
step:2062/2160 train_time:122631ms step_avg:59.47ms
step:2063/2160 train_time:122720ms step_avg:59.49ms
step:2064/2160 train_time:122807ms step_avg:59.50ms
step:2065/2160 train_time:122895ms step_avg:59.51ms
step:2066/2160 train_time:122981ms step_avg:59.53ms
step:2067/2160 train_time:123069ms step_avg:59.54ms
step:2068/2160 train_time:123156ms step_avg:59.55ms
step:2069/2160 train_time:123243ms step_avg:59.57ms
step:2070/2160 train_time:123329ms step_avg:59.58ms
step:2071/2160 train_time:123419ms step_avg:59.59ms
step:2072/2160 train_time:123507ms step_avg:59.61ms
step:2073/2160 train_time:123595ms step_avg:59.62ms
step:2074/2160 train_time:123681ms step_avg:59.63ms
step:2075/2160 train_time:123768ms step_avg:59.65ms
step:2076/2160 train_time:123856ms step_avg:59.66ms
step:2077/2160 train_time:123944ms step_avg:59.67ms
step:2078/2160 train_time:124030ms step_avg:59.69ms
step:2079/2160 train_time:124117ms step_avg:59.70ms
step:2080/2160 train_time:124204ms step_avg:59.71ms
step:2081/2160 train_time:124293ms step_avg:59.73ms
step:2082/2160 train_time:124380ms step_avg:59.74ms
step:2083/2160 train_time:124468ms step_avg:59.75ms
step:2084/2160 train_time:124555ms step_avg:59.77ms
step:2085/2160 train_time:124643ms step_avg:59.78ms
step:2086/2160 train_time:124729ms step_avg:59.79ms
step:2087/2160 train_time:124819ms step_avg:59.81ms
step:2088/2160 train_time:124905ms step_avg:59.82ms
step:2089/2160 train_time:124993ms step_avg:59.83ms
step:2090/2160 train_time:125078ms step_avg:59.85ms
step:2091/2160 train_time:125167ms step_avg:59.86ms
step:2092/2160 train_time:125254ms step_avg:59.87ms
step:2093/2160 train_time:125343ms step_avg:59.89ms
step:2094/2160 train_time:125430ms step_avg:59.90ms
step:2095/2160 train_time:125520ms step_avg:59.91ms
step:2096/2160 train_time:125605ms step_avg:59.93ms
step:2097/2160 train_time:125693ms step_avg:59.94ms
step:2098/2160 train_time:125780ms step_avg:59.95ms
step:2099/2160 train_time:125867ms step_avg:59.97ms
step:2100/2160 train_time:125953ms step_avg:59.98ms
step:2101/2160 train_time:126041ms step_avg:59.99ms
step:2102/2160 train_time:126127ms step_avg:60.00ms
step:2103/2160 train_time:126217ms step_avg:60.02ms
step:2104/2160 train_time:126302ms step_avg:60.03ms
step:2105/2160 train_time:126392ms step_avg:60.04ms
step:2106/2160 train_time:126479ms step_avg:60.06ms
step:2107/2160 train_time:126567ms step_avg:60.07ms
step:2108/2160 train_time:126654ms step_avg:60.08ms
step:2109/2160 train_time:126742ms step_avg:60.10ms
step:2110/2160 train_time:126829ms step_avg:60.11ms
step:2111/2160 train_time:126918ms step_avg:60.12ms
step:2112/2160 train_time:127004ms step_avg:60.13ms
step:2113/2160 train_time:127093ms step_avg:60.15ms
step:2114/2160 train_time:127181ms step_avg:60.16ms
step:2115/2160 train_time:127267ms step_avg:60.17ms
step:2116/2160 train_time:127355ms step_avg:60.19ms
step:2117/2160 train_time:127444ms step_avg:60.20ms
step:2118/2160 train_time:127530ms step_avg:60.21ms
step:2119/2160 train_time:127620ms step_avg:60.23ms
step:2120/2160 train_time:127706ms step_avg:60.24ms
step:2121/2160 train_time:127795ms step_avg:60.25ms
step:2122/2160 train_time:127881ms step_avg:60.26ms
step:2123/2160 train_time:127970ms step_avg:60.28ms
step:2124/2160 train_time:128057ms step_avg:60.29ms
step:2125/2160 train_time:128146ms step_avg:60.30ms
step:2126/2160 train_time:128232ms step_avg:60.32ms
step:2127/2160 train_time:128321ms step_avg:60.33ms
step:2128/2160 train_time:128408ms step_avg:60.34ms
step:2129/2160 train_time:128497ms step_avg:60.36ms
step:2130/2160 train_time:128584ms step_avg:60.37ms
step:2131/2160 train_time:128672ms step_avg:60.38ms
step:2132/2160 train_time:128760ms step_avg:60.39ms
step:2133/2160 train_time:128847ms step_avg:60.41ms
step:2134/2160 train_time:128934ms step_avg:60.42ms
step:2135/2160 train_time:129023ms step_avg:60.43ms
step:2136/2160 train_time:129110ms step_avg:60.44ms
step:2137/2160 train_time:129199ms step_avg:60.46ms
step:2138/2160 train_time:129286ms step_avg:60.47ms
step:2139/2160 train_time:129375ms step_avg:60.48ms
step:2140/2160 train_time:129462ms step_avg:60.50ms
step:2141/2160 train_time:129550ms step_avg:60.51ms
step:2142/2160 train_time:129637ms step_avg:60.52ms
step:2143/2160 train_time:129725ms step_avg:60.53ms
step:2144/2160 train_time:129812ms step_avg:60.55ms
step:2145/2160 train_time:129901ms step_avg:60.56ms
step:2146/2160 train_time:129987ms step_avg:60.57ms
step:2147/2160 train_time:130076ms step_avg:60.59ms
step:2148/2160 train_time:130162ms step_avg:60.60ms
step:2149/2160 train_time:130250ms step_avg:60.61ms
step:2150/2160 train_time:130337ms step_avg:60.62ms
step:2151/2160 train_time:130426ms step_avg:60.64ms
step:2152/2160 train_time:130513ms step_avg:60.65ms
step:2153/2160 train_time:130601ms step_avg:60.66ms
step:2154/2160 train_time:130688ms step_avg:60.67ms
step:2155/2160 train_time:130777ms step_avg:60.69ms
step:2156/2160 train_time:130863ms step_avg:60.70ms
step:2157/2160 train_time:130951ms step_avg:60.71ms
step:2158/2160 train_time:131038ms step_avg:60.72ms
step:2159/2160 train_time:131127ms step_avg:60.73ms
step:2160/2160 train_time:131213ms step_avg:60.75ms
step:2160/2160 val_loss:3.2764 train_time:131304ms step_avg:60.79ms
peak memory allocated: 29892 MiB reserved: 43936 MiB

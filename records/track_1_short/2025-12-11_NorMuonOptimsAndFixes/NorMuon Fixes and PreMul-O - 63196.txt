import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 12:05:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           17530      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           17531      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17532      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17533      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17534      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17535      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17536      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17537      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           17531      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           17532      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           17533      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           17534      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           17535      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           17536      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           17537      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:92ms step_avg:91.62ms
step:2/2160 train_time:117ms step_avg:58.50ms
step:3/2160 train_time:139ms step_avg:46.46ms
step:4/2160 train_time:163ms step_avg:40.70ms
step:5/2160 train_time:195ms step_avg:39.01ms
step:6/2160 train_time:351ms step_avg:58.44ms
step:7/2160 train_time:371ms step_avg:52.96ms
step:8/2160 train_time:393ms step_avg:49.10ms
step:9/2160 train_time:426ms step_avg:47.32ms
step:10/2160 train_time:458ms step_avg:45.81ms
step:11/2160 train_time:491ms step_avg:44.67ms
step:12/2160 train_time:524ms step_avg:43.64ms
step:13/2160 train_time:557ms step_avg:42.86ms
step:14/2160 train_time:589ms step_avg:42.10ms
step:15/2160 train_time:623ms step_avg:41.53ms
step:16/2160 train_time:655ms step_avg:40.94ms
step:17/2160 train_time:689ms step_avg:40.52ms
step:18/2160 train_time:721ms step_avg:40.08ms
step:19/2160 train_time:755ms step_avg:39.72ms
step:20/2160 train_time:787ms step_avg:39.34ms
step:21/2160 train_time:820ms step_avg:39.07ms
step:22/2160 train_time:853ms step_avg:38.76ms
step:23/2160 train_time:886ms step_avg:38.53ms
step:24/2160 train_time:918ms step_avg:38.27ms
step:25/2160 train_time:952ms step_avg:38.07ms
step:26/2160 train_time:984ms step_avg:37.85ms
step:27/2160 train_time:1017ms step_avg:37.68ms
step:28/2160 train_time:1050ms step_avg:37.49ms
step:29/2160 train_time:1083ms step_avg:37.36ms
step:30/2160 train_time:1116ms step_avg:37.19ms
step:31/2160 train_time:1149ms step_avg:37.06ms
step:32/2160 train_time:1181ms step_avg:36.91ms
step:33/2160 train_time:1215ms step_avg:36.82ms
step:34/2160 train_time:1248ms step_avg:36.71ms
step:35/2160 train_time:1284ms step_avg:36.68ms
step:36/2160 train_time:1316ms step_avg:36.57ms
step:37/2160 train_time:1351ms step_avg:36.51ms
step:38/2160 train_time:1384ms step_avg:36.41ms
step:39/2160 train_time:1418ms step_avg:36.36ms
step:40/2160 train_time:1450ms step_avg:36.25ms
step:41/2160 train_time:1484ms step_avg:36.20ms
step:42/2160 train_time:1516ms step_avg:36.10ms
step:43/2160 train_time:1550ms step_avg:36.05ms
step:44/2160 train_time:1582ms step_avg:35.96ms
step:45/2160 train_time:1616ms step_avg:35.90ms
step:46/2160 train_time:1648ms step_avg:35.83ms
step:47/2160 train_time:1682ms step_avg:35.78ms
step:48/2160 train_time:1714ms step_avg:35.71ms
step:49/2160 train_time:1748ms step_avg:35.67ms
step:50/2160 train_time:1780ms step_avg:35.60ms
step:51/2160 train_time:1813ms step_avg:35.55ms
step:52/2160 train_time:1845ms step_avg:35.48ms
step:53/2160 train_time:1879ms step_avg:35.45ms
step:54/2160 train_time:1911ms step_avg:35.39ms
step:55/2160 train_time:1944ms step_avg:35.35ms
step:56/2160 train_time:1977ms step_avg:35.30ms
step:57/2160 train_time:2010ms step_avg:35.26ms
step:58/2160 train_time:2042ms step_avg:35.21ms
step:59/2160 train_time:2075ms step_avg:35.18ms
step:60/2160 train_time:2108ms step_avg:35.13ms
step:61/2160 train_time:2141ms step_avg:35.10ms
step:62/2160 train_time:2174ms step_avg:35.06ms
step:63/2160 train_time:2207ms step_avg:35.04ms
step:64/2160 train_time:2240ms step_avg:35.00ms
step:65/2160 train_time:2273ms step_avg:34.97ms
step:66/2160 train_time:2305ms step_avg:34.93ms
step:67/2160 train_time:2339ms step_avg:34.92ms
step:68/2160 train_time:2372ms step_avg:34.88ms
step:69/2160 train_time:2406ms step_avg:34.87ms
step:70/2160 train_time:2438ms step_avg:34.83ms
step:71/2160 train_time:2472ms step_avg:34.81ms
step:72/2160 train_time:2504ms step_avg:34.78ms
step:73/2160 train_time:2538ms step_avg:34.77ms
step:74/2160 train_time:2571ms step_avg:34.74ms
step:75/2160 train_time:2604ms step_avg:34.73ms
step:76/2160 train_time:2637ms step_avg:34.69ms
step:77/2160 train_time:2670ms step_avg:34.68ms
step:78/2160 train_time:2703ms step_avg:34.65ms
step:79/2160 train_time:2736ms step_avg:34.63ms
step:80/2160 train_time:2768ms step_avg:34.60ms
step:81/2160 train_time:2802ms step_avg:34.59ms
step:82/2160 train_time:2834ms step_avg:34.56ms
step:83/2160 train_time:2867ms step_avg:34.55ms
step:84/2160 train_time:2900ms step_avg:34.52ms
step:85/2160 train_time:2933ms step_avg:34.50ms
step:86/2160 train_time:2965ms step_avg:34.48ms
step:87/2160 train_time:2999ms step_avg:34.47ms
step:88/2160 train_time:3031ms step_avg:34.45ms
step:89/2160 train_time:3065ms step_avg:34.43ms
step:90/2160 train_time:3097ms step_avg:34.41ms
step:91/2160 train_time:3130ms step_avg:34.40ms
step:92/2160 train_time:3163ms step_avg:34.38ms
step:93/2160 train_time:3196ms step_avg:34.37ms
step:94/2160 train_time:3229ms step_avg:34.35ms
step:95/2160 train_time:3262ms step_avg:34.34ms
step:96/2160 train_time:3295ms step_avg:34.32ms
step:97/2160 train_time:3328ms step_avg:34.31ms
step:98/2160 train_time:3361ms step_avg:34.29ms
step:99/2160 train_time:3394ms step_avg:34.29ms
step:100/2160 train_time:3427ms step_avg:34.27ms
step:101/2160 train_time:3460ms step_avg:34.26ms
step:102/2160 train_time:3493ms step_avg:34.24ms
step:103/2160 train_time:3526ms step_avg:34.23ms
step:104/2160 train_time:3558ms step_avg:34.21ms
step:105/2160 train_time:3592ms step_avg:34.21ms
step:106/2160 train_time:3624ms step_avg:34.19ms
step:107/2160 train_time:3657ms step_avg:34.18ms
step:108/2160 train_time:3690ms step_avg:34.16ms
step:109/2160 train_time:3723ms step_avg:34.16ms
step:110/2160 train_time:3755ms step_avg:34.14ms
step:111/2160 train_time:3789ms step_avg:34.14ms
step:112/2160 train_time:3821ms step_avg:34.12ms
step:113/2160 train_time:3854ms step_avg:34.11ms
step:114/2160 train_time:3887ms step_avg:34.09ms
step:115/2160 train_time:3920ms step_avg:34.09ms
step:116/2160 train_time:3952ms step_avg:34.07ms
step:117/2160 train_time:3986ms step_avg:34.06ms
step:118/2160 train_time:4018ms step_avg:34.05ms
step:119/2160 train_time:4051ms step_avg:34.04ms
step:120/2160 train_time:4083ms step_avg:34.03ms
step:121/2160 train_time:4117ms step_avg:34.02ms
step:122/2160 train_time:4149ms step_avg:34.01ms
step:123/2160 train_time:4182ms step_avg:34.00ms
step:124/2160 train_time:4215ms step_avg:33.99ms
step:125/2160 train_time:4248ms step_avg:33.98ms
step:126/2160 train_time:4280ms step_avg:33.97ms
step:127/2160 train_time:4313ms step_avg:33.96ms
step:128/2160 train_time:4346ms step_avg:33.95ms
step:129/2160 train_time:4379ms step_avg:33.95ms
step:130/2160 train_time:4412ms step_avg:33.93ms
step:131/2160 train_time:4445ms step_avg:33.93ms
step:132/2160 train_time:4477ms step_avg:33.92ms
step:133/2160 train_time:4510ms step_avg:33.91ms
step:134/2160 train_time:4543ms step_avg:33.90ms
step:135/2160 train_time:4576ms step_avg:33.90ms
step:136/2160 train_time:4608ms step_avg:33.89ms
step:137/2160 train_time:4642ms step_avg:33.88ms
step:138/2160 train_time:4674ms step_avg:33.87ms
step:139/2160 train_time:4707ms step_avg:33.87ms
step:140/2160 train_time:4740ms step_avg:33.86ms
step:141/2160 train_time:4773ms step_avg:33.85ms
step:142/2160 train_time:4805ms step_avg:33.84ms
step:143/2160 train_time:4839ms step_avg:33.84ms
step:144/2160 train_time:4872ms step_avg:33.83ms
step:145/2160 train_time:4905ms step_avg:33.83ms
step:146/2160 train_time:4937ms step_avg:33.82ms
step:147/2160 train_time:4970ms step_avg:33.81ms
step:148/2160 train_time:5003ms step_avg:33.80ms
step:149/2160 train_time:5037ms step_avg:33.80ms
step:150/2160 train_time:5069ms step_avg:33.79ms
step:151/2160 train_time:5103ms step_avg:33.79ms
step:152/2160 train_time:5135ms step_avg:33.78ms
step:153/2160 train_time:5168ms step_avg:33.78ms
step:154/2160 train_time:5201ms step_avg:33.77ms
step:155/2160 train_time:5234ms step_avg:33.77ms
step:156/2160 train_time:5266ms step_avg:33.76ms
step:157/2160 train_time:5300ms step_avg:33.76ms
step:158/2160 train_time:5332ms step_avg:33.75ms
step:159/2160 train_time:5366ms step_avg:33.75ms
step:160/2160 train_time:5398ms step_avg:33.74ms
step:161/2160 train_time:5431ms step_avg:33.73ms
step:162/2160 train_time:5463ms step_avg:33.72ms
step:163/2160 train_time:5497ms step_avg:33.73ms
step:164/2160 train_time:5530ms step_avg:33.72ms
step:165/2160 train_time:5563ms step_avg:33.71ms
step:166/2160 train_time:5595ms step_avg:33.71ms
step:167/2160 train_time:5629ms step_avg:33.70ms
step:168/2160 train_time:5661ms step_avg:33.69ms
step:169/2160 train_time:5694ms step_avg:33.70ms
step:170/2160 train_time:5727ms step_avg:33.69ms
step:171/2160 train_time:5760ms step_avg:33.69ms
step:172/2160 train_time:5792ms step_avg:33.68ms
step:173/2160 train_time:5826ms step_avg:33.68ms
step:174/2160 train_time:5858ms step_avg:33.67ms
step:175/2160 train_time:5891ms step_avg:33.66ms
step:176/2160 train_time:5923ms step_avg:33.65ms
step:177/2160 train_time:5956ms step_avg:33.65ms
step:178/2160 train_time:5988ms step_avg:33.64ms
step:179/2160 train_time:6022ms step_avg:33.64ms
step:180/2160 train_time:6054ms step_avg:33.63ms
step:181/2160 train_time:6088ms step_avg:33.63ms
step:182/2160 train_time:6120ms step_avg:33.63ms
step:183/2160 train_time:6153ms step_avg:33.62ms
step:184/2160 train_time:6186ms step_avg:33.62ms
step:185/2160 train_time:6220ms step_avg:33.62ms
step:186/2160 train_time:6252ms step_avg:33.61ms
step:187/2160 train_time:6285ms step_avg:33.61ms
step:188/2160 train_time:6317ms step_avg:33.60ms
step:189/2160 train_time:6350ms step_avg:33.60ms
step:190/2160 train_time:6383ms step_avg:33.59ms
step:191/2160 train_time:6416ms step_avg:33.59ms
step:192/2160 train_time:6448ms step_avg:33.58ms
step:193/2160 train_time:6482ms step_avg:33.58ms
step:194/2160 train_time:6514ms step_avg:33.58ms
step:195/2160 train_time:6548ms step_avg:33.58ms
step:196/2160 train_time:6580ms step_avg:33.57ms
step:197/2160 train_time:6613ms step_avg:33.57ms
step:198/2160 train_time:6645ms step_avg:33.56ms
step:199/2160 train_time:6678ms step_avg:33.56ms
step:200/2160 train_time:6711ms step_avg:33.55ms
step:201/2160 train_time:6744ms step_avg:33.55ms
step:202/2160 train_time:6776ms step_avg:33.55ms
step:203/2160 train_time:6809ms step_avg:33.54ms
step:204/2160 train_time:6842ms step_avg:33.54ms
step:205/2160 train_time:6875ms step_avg:33.54ms
step:206/2160 train_time:6907ms step_avg:33.53ms
step:207/2160 train_time:6941ms step_avg:33.53ms
step:208/2160 train_time:6973ms step_avg:33.52ms
step:209/2160 train_time:7007ms step_avg:33.52ms
step:210/2160 train_time:7039ms step_avg:33.52ms
step:211/2160 train_time:7072ms step_avg:33.52ms
step:212/2160 train_time:7104ms step_avg:33.51ms
step:213/2160 train_time:7138ms step_avg:33.51ms
step:214/2160 train_time:7170ms step_avg:33.51ms
step:215/2160 train_time:7204ms step_avg:33.51ms
step:216/2160 train_time:7236ms step_avg:33.50ms
step:217/2160 train_time:7270ms step_avg:33.50ms
step:218/2160 train_time:7302ms step_avg:33.50ms
step:219/2160 train_time:7336ms step_avg:33.50ms
step:220/2160 train_time:7368ms step_avg:33.49ms
step:221/2160 train_time:7401ms step_avg:33.49ms
step:222/2160 train_time:7433ms step_avg:33.48ms
step:223/2160 train_time:7467ms step_avg:33.48ms
step:224/2160 train_time:7498ms step_avg:33.48ms
step:225/2160 train_time:7532ms step_avg:33.48ms
step:226/2160 train_time:7564ms step_avg:33.47ms
step:227/2160 train_time:7597ms step_avg:33.47ms
step:228/2160 train_time:7630ms step_avg:33.46ms
step:229/2160 train_time:7663ms step_avg:33.46ms
step:230/2160 train_time:7695ms step_avg:33.46ms
step:231/2160 train_time:7729ms step_avg:33.46ms
step:232/2160 train_time:7761ms step_avg:33.45ms
step:233/2160 train_time:7794ms step_avg:33.45ms
step:234/2160 train_time:7826ms step_avg:33.45ms
step:235/2160 train_time:7859ms step_avg:33.44ms
step:236/2160 train_time:7892ms step_avg:33.44ms
step:237/2160 train_time:7925ms step_avg:33.44ms
step:238/2160 train_time:7957ms step_avg:33.43ms
step:239/2160 train_time:7990ms step_avg:33.43ms
step:240/2160 train_time:8023ms step_avg:33.43ms
step:241/2160 train_time:8055ms step_avg:33.43ms
step:242/2160 train_time:8088ms step_avg:33.42ms
step:243/2160 train_time:8121ms step_avg:33.42ms
step:244/2160 train_time:8153ms step_avg:33.41ms
step:245/2160 train_time:8187ms step_avg:33.41ms
step:246/2160 train_time:8219ms step_avg:33.41ms
step:247/2160 train_time:8252ms step_avg:33.41ms
step:248/2160 train_time:8284ms step_avg:33.40ms
step:249/2160 train_time:8318ms step_avg:33.41ms
step:250/2160 train_time:8350ms step_avg:33.40ms
step:250/2160 val_loss:4.3069 train_time:8386ms step_avg:33.54ms
step:251/2160 train_time:8408ms step_avg:33.50ms
step:252/2160 train_time:8430ms step_avg:33.45ms
step:253/2160 train_time:8453ms step_avg:33.41ms
step:254/2160 train_time:8486ms step_avg:33.41ms
step:255/2160 train_time:8521ms step_avg:33.41ms
step:256/2160 train_time:8554ms step_avg:33.41ms
step:257/2160 train_time:8588ms step_avg:33.42ms
step:258/2160 train_time:8621ms step_avg:33.41ms
step:259/2160 train_time:8654ms step_avg:33.41ms
step:260/2160 train_time:8686ms step_avg:33.41ms
step:261/2160 train_time:8720ms step_avg:33.41ms
step:262/2160 train_time:8752ms step_avg:33.40ms
step:263/2160 train_time:8785ms step_avg:33.40ms
step:264/2160 train_time:8817ms step_avg:33.40ms
step:265/2160 train_time:8851ms step_avg:33.40ms
step:266/2160 train_time:8883ms step_avg:33.39ms
step:267/2160 train_time:8916ms step_avg:33.39ms
step:268/2160 train_time:8948ms step_avg:33.39ms
step:269/2160 train_time:8981ms step_avg:33.39ms
step:270/2160 train_time:9013ms step_avg:33.38ms
step:271/2160 train_time:9046ms step_avg:33.38ms
step:272/2160 train_time:9078ms step_avg:33.37ms
step:273/2160 train_time:9111ms step_avg:33.37ms
step:274/2160 train_time:9143ms step_avg:33.37ms
step:275/2160 train_time:9176ms step_avg:33.37ms
step:276/2160 train_time:9208ms step_avg:33.36ms
step:277/2160 train_time:9241ms step_avg:33.36ms
step:278/2160 train_time:9273ms step_avg:33.36ms
step:279/2160 train_time:9306ms step_avg:33.36ms
step:280/2160 train_time:9338ms step_avg:33.35ms
step:281/2160 train_time:9371ms step_avg:33.35ms
step:282/2160 train_time:9404ms step_avg:33.35ms
step:283/2160 train_time:9437ms step_avg:33.35ms
step:284/2160 train_time:9469ms step_avg:33.34ms
step:285/2160 train_time:9503ms step_avg:33.34ms
step:286/2160 train_time:9535ms step_avg:33.34ms
step:287/2160 train_time:9570ms step_avg:33.34ms
step:288/2160 train_time:9602ms step_avg:33.34ms
step:289/2160 train_time:9636ms step_avg:33.34ms
step:290/2160 train_time:9668ms step_avg:33.34ms
step:291/2160 train_time:9701ms step_avg:33.34ms
step:292/2160 train_time:9733ms step_avg:33.33ms
step:293/2160 train_time:9767ms step_avg:33.33ms
step:294/2160 train_time:9799ms step_avg:33.33ms
step:295/2160 train_time:9832ms step_avg:33.33ms
step:296/2160 train_time:9864ms step_avg:33.33ms
step:297/2160 train_time:9898ms step_avg:33.33ms
step:298/2160 train_time:9930ms step_avg:33.32ms
step:299/2160 train_time:9963ms step_avg:33.32ms
step:300/2160 train_time:9995ms step_avg:33.32ms
step:301/2160 train_time:10028ms step_avg:33.32ms
step:302/2160 train_time:10060ms step_avg:33.31ms
step:303/2160 train_time:10094ms step_avg:33.31ms
step:304/2160 train_time:10126ms step_avg:33.31ms
step:305/2160 train_time:10159ms step_avg:33.31ms
step:306/2160 train_time:10191ms step_avg:33.30ms
step:307/2160 train_time:10224ms step_avg:33.30ms
step:308/2160 train_time:10256ms step_avg:33.30ms
step:309/2160 train_time:10289ms step_avg:33.30ms
step:310/2160 train_time:10321ms step_avg:33.29ms
step:311/2160 train_time:10355ms step_avg:33.29ms
step:312/2160 train_time:10387ms step_avg:33.29ms
step:313/2160 train_time:10420ms step_avg:33.29ms
step:314/2160 train_time:10452ms step_avg:33.29ms
step:315/2160 train_time:10486ms step_avg:33.29ms
step:316/2160 train_time:10518ms step_avg:33.28ms
step:317/2160 train_time:10551ms step_avg:33.28ms
step:318/2160 train_time:10584ms step_avg:33.28ms
step:319/2160 train_time:10617ms step_avg:33.28ms
step:320/2160 train_time:10649ms step_avg:33.28ms
step:321/2160 train_time:10683ms step_avg:33.28ms
step:322/2160 train_time:10715ms step_avg:33.28ms
step:323/2160 train_time:10748ms step_avg:33.28ms
step:324/2160 train_time:10781ms step_avg:33.27ms
step:325/2160 train_time:10814ms step_avg:33.27ms
step:326/2160 train_time:10847ms step_avg:33.27ms
step:327/2160 train_time:10879ms step_avg:33.27ms
step:328/2160 train_time:10911ms step_avg:33.27ms
step:329/2160 train_time:10945ms step_avg:33.27ms
step:330/2160 train_time:10977ms step_avg:33.26ms
step:331/2160 train_time:11011ms step_avg:33.26ms
step:332/2160 train_time:11043ms step_avg:33.26ms
step:333/2160 train_time:11076ms step_avg:33.26ms
step:334/2160 train_time:11108ms step_avg:33.26ms
step:335/2160 train_time:11141ms step_avg:33.26ms
step:336/2160 train_time:11173ms step_avg:33.25ms
step:337/2160 train_time:11206ms step_avg:33.25ms
step:338/2160 train_time:11238ms step_avg:33.25ms
step:339/2160 train_time:11271ms step_avg:33.25ms
step:340/2160 train_time:11303ms step_avg:33.24ms
step:341/2160 train_time:11336ms step_avg:33.24ms
step:342/2160 train_time:11368ms step_avg:33.24ms
step:343/2160 train_time:11401ms step_avg:33.24ms
step:344/2160 train_time:11434ms step_avg:33.24ms
step:345/2160 train_time:11467ms step_avg:33.24ms
step:346/2160 train_time:11499ms step_avg:33.23ms
step:347/2160 train_time:11532ms step_avg:33.23ms
step:348/2160 train_time:11564ms step_avg:33.23ms
step:349/2160 train_time:11597ms step_avg:33.23ms
step:350/2160 train_time:11629ms step_avg:33.23ms
step:351/2160 train_time:11663ms step_avg:33.23ms
step:352/2160 train_time:11695ms step_avg:33.22ms
step:353/2160 train_time:11729ms step_avg:33.23ms
step:354/2160 train_time:11761ms step_avg:33.22ms
step:355/2160 train_time:11794ms step_avg:33.22ms
step:356/2160 train_time:11826ms step_avg:33.22ms
step:357/2160 train_time:11859ms step_avg:33.22ms
step:358/2160 train_time:11891ms step_avg:33.22ms
step:359/2160 train_time:11924ms step_avg:33.22ms
step:360/2160 train_time:11957ms step_avg:33.21ms
step:361/2160 train_time:11990ms step_avg:33.21ms
step:362/2160 train_time:12022ms step_avg:33.21ms
step:363/2160 train_time:12055ms step_avg:33.21ms
step:364/2160 train_time:12087ms step_avg:33.21ms
step:365/2160 train_time:12121ms step_avg:33.21ms
step:366/2160 train_time:12153ms step_avg:33.20ms
step:367/2160 train_time:12186ms step_avg:33.20ms
step:368/2160 train_time:12218ms step_avg:33.20ms
step:369/2160 train_time:12251ms step_avg:33.20ms
step:370/2160 train_time:12283ms step_avg:33.20ms
step:371/2160 train_time:12317ms step_avg:33.20ms
step:372/2160 train_time:12349ms step_avg:33.20ms
step:373/2160 train_time:12382ms step_avg:33.20ms
step:374/2160 train_time:12415ms step_avg:33.19ms
step:375/2160 train_time:12448ms step_avg:33.19ms
step:376/2160 train_time:12480ms step_avg:33.19ms
step:377/2160 train_time:12513ms step_avg:33.19ms
step:378/2160 train_time:12545ms step_avg:33.19ms
step:379/2160 train_time:12578ms step_avg:33.19ms
step:380/2160 train_time:12610ms step_avg:33.18ms
step:381/2160 train_time:12644ms step_avg:33.19ms
step:382/2160 train_time:12676ms step_avg:33.18ms
step:383/2160 train_time:12709ms step_avg:33.18ms
step:384/2160 train_time:12741ms step_avg:33.18ms
step:385/2160 train_time:12775ms step_avg:33.18ms
step:386/2160 train_time:12807ms step_avg:33.18ms
step:387/2160 train_time:12840ms step_avg:33.18ms
step:388/2160 train_time:12872ms step_avg:33.18ms
step:389/2160 train_time:12906ms step_avg:33.18ms
step:390/2160 train_time:12938ms step_avg:33.18ms
step:391/2160 train_time:12972ms step_avg:33.18ms
step:392/2160 train_time:13004ms step_avg:33.17ms
step:393/2160 train_time:13037ms step_avg:33.17ms
step:394/2160 train_time:13070ms step_avg:33.17ms
step:395/2160 train_time:13103ms step_avg:33.17ms
step:396/2160 train_time:13135ms step_avg:33.17ms
step:397/2160 train_time:13168ms step_avg:33.17ms
step:398/2160 train_time:13200ms step_avg:33.17ms
step:399/2160 train_time:13233ms step_avg:33.17ms
step:400/2160 train_time:13265ms step_avg:33.16ms
step:401/2160 train_time:13298ms step_avg:33.16ms
step:402/2160 train_time:13330ms step_avg:33.16ms
step:403/2160 train_time:13364ms step_avg:33.16ms
step:404/2160 train_time:13396ms step_avg:33.16ms
step:405/2160 train_time:13429ms step_avg:33.16ms
step:406/2160 train_time:13461ms step_avg:33.16ms
step:407/2160 train_time:13495ms step_avg:33.16ms
step:408/2160 train_time:13527ms step_avg:33.15ms
step:409/2160 train_time:13560ms step_avg:33.15ms
step:410/2160 train_time:13592ms step_avg:33.15ms
step:411/2160 train_time:13626ms step_avg:33.15ms
step:412/2160 train_time:13658ms step_avg:33.15ms
step:413/2160 train_time:13692ms step_avg:33.15ms
step:414/2160 train_time:13724ms step_avg:33.15ms
step:415/2160 train_time:13757ms step_avg:33.15ms
step:416/2160 train_time:13789ms step_avg:33.15ms
step:417/2160 train_time:13822ms step_avg:33.15ms
step:418/2160 train_time:13855ms step_avg:33.14ms
step:419/2160 train_time:13888ms step_avg:33.15ms
step:420/2160 train_time:13920ms step_avg:33.14ms
step:421/2160 train_time:13953ms step_avg:33.14ms
step:422/2160 train_time:13986ms step_avg:33.14ms
step:423/2160 train_time:14019ms step_avg:33.14ms
step:424/2160 train_time:14051ms step_avg:33.14ms
step:425/2160 train_time:14084ms step_avg:33.14ms
step:426/2160 train_time:14116ms step_avg:33.14ms
step:427/2160 train_time:14149ms step_avg:33.14ms
step:428/2160 train_time:14181ms step_avg:33.13ms
step:429/2160 train_time:14215ms step_avg:33.13ms
step:430/2160 train_time:14247ms step_avg:33.13ms
step:431/2160 train_time:14280ms step_avg:33.13ms
step:432/2160 train_time:14312ms step_avg:33.13ms
step:433/2160 train_time:14345ms step_avg:33.13ms
step:434/2160 train_time:14378ms step_avg:33.13ms
step:435/2160 train_time:14411ms step_avg:33.13ms
step:436/2160 train_time:14443ms step_avg:33.13ms
step:437/2160 train_time:14476ms step_avg:33.13ms
step:438/2160 train_time:14509ms step_avg:33.13ms
step:439/2160 train_time:14542ms step_avg:33.13ms
step:440/2160 train_time:14574ms step_avg:33.12ms
step:441/2160 train_time:14608ms step_avg:33.12ms
step:442/2160 train_time:14640ms step_avg:33.12ms
step:443/2160 train_time:14673ms step_avg:33.12ms
step:444/2160 train_time:14705ms step_avg:33.12ms
step:445/2160 train_time:14738ms step_avg:33.12ms
step:446/2160 train_time:14770ms step_avg:33.12ms
step:447/2160 train_time:14804ms step_avg:33.12ms
step:448/2160 train_time:14836ms step_avg:33.12ms
step:449/2160 train_time:14869ms step_avg:33.12ms
step:450/2160 train_time:14901ms step_avg:33.11ms
step:451/2160 train_time:14935ms step_avg:33.11ms
step:452/2160 train_time:14967ms step_avg:33.11ms
step:453/2160 train_time:15000ms step_avg:33.11ms
step:454/2160 train_time:15032ms step_avg:33.11ms
step:455/2160 train_time:15065ms step_avg:33.11ms
step:456/2160 train_time:15097ms step_avg:33.11ms
step:457/2160 train_time:15130ms step_avg:33.11ms
step:458/2160 train_time:15162ms step_avg:33.11ms
step:459/2160 train_time:15196ms step_avg:33.11ms
step:460/2160 train_time:15228ms step_avg:33.10ms
step:461/2160 train_time:15262ms step_avg:33.11ms
step:462/2160 train_time:15294ms step_avg:33.10ms
step:463/2160 train_time:15327ms step_avg:33.10ms
step:464/2160 train_time:15359ms step_avg:33.10ms
step:465/2160 train_time:15393ms step_avg:33.10ms
step:466/2160 train_time:15425ms step_avg:33.10ms
step:467/2160 train_time:15458ms step_avg:33.10ms
step:468/2160 train_time:15490ms step_avg:33.10ms
step:469/2160 train_time:15523ms step_avg:33.10ms
step:470/2160 train_time:15555ms step_avg:33.10ms
step:471/2160 train_time:15589ms step_avg:33.10ms
step:472/2160 train_time:15621ms step_avg:33.09ms
step:473/2160 train_time:15654ms step_avg:33.09ms
step:474/2160 train_time:15686ms step_avg:33.09ms
step:475/2160 train_time:15720ms step_avg:33.09ms
step:476/2160 train_time:15752ms step_avg:33.09ms
step:477/2160 train_time:15785ms step_avg:33.09ms
step:478/2160 train_time:15817ms step_avg:33.09ms
step:479/2160 train_time:15851ms step_avg:33.09ms
step:480/2160 train_time:15883ms step_avg:33.09ms
step:481/2160 train_time:15917ms step_avg:33.09ms
step:482/2160 train_time:15949ms step_avg:33.09ms
step:483/2160 train_time:15982ms step_avg:33.09ms
step:484/2160 train_time:16014ms step_avg:33.09ms
step:485/2160 train_time:16047ms step_avg:33.09ms
step:486/2160 train_time:16080ms step_avg:33.09ms
step:487/2160 train_time:16113ms step_avg:33.09ms
step:488/2160 train_time:16145ms step_avg:33.08ms
step:489/2160 train_time:16178ms step_avg:33.08ms
step:490/2160 train_time:16210ms step_avg:33.08ms
step:491/2160 train_time:16244ms step_avg:33.08ms
step:492/2160 train_time:16276ms step_avg:33.08ms
step:493/2160 train_time:16310ms step_avg:33.08ms
step:494/2160 train_time:16342ms step_avg:33.08ms
step:495/2160 train_time:16375ms step_avg:33.08ms
step:496/2160 train_time:16407ms step_avg:33.08ms
step:497/2160 train_time:16440ms step_avg:33.08ms
step:498/2160 train_time:16473ms step_avg:33.08ms
step:499/2160 train_time:16506ms step_avg:33.08ms
step:500/2160 train_time:16538ms step_avg:33.08ms
step:500/2160 val_loss:4.0226 train_time:16573ms step_avg:33.15ms
step:501/2160 train_time:16595ms step_avg:33.12ms
step:502/2160 train_time:16616ms step_avg:33.10ms
step:503/2160 train_time:16642ms step_avg:33.09ms
step:504/2160 train_time:16674ms step_avg:33.08ms
step:505/2160 train_time:16710ms step_avg:33.09ms
step:506/2160 train_time:16743ms step_avg:33.09ms
step:507/2160 train_time:16777ms step_avg:33.09ms
step:508/2160 train_time:16809ms step_avg:33.09ms
step:509/2160 train_time:16843ms step_avg:33.09ms
step:510/2160 train_time:16875ms step_avg:33.09ms
step:511/2160 train_time:16908ms step_avg:33.09ms
step:512/2160 train_time:16941ms step_avg:33.09ms
step:513/2160 train_time:16974ms step_avg:33.09ms
step:514/2160 train_time:17006ms step_avg:33.09ms
step:515/2160 train_time:17039ms step_avg:33.09ms
step:516/2160 train_time:17071ms step_avg:33.08ms
step:517/2160 train_time:17104ms step_avg:33.08ms
step:518/2160 train_time:17137ms step_avg:33.08ms
step:519/2160 train_time:17170ms step_avg:33.08ms
step:520/2160 train_time:17202ms step_avg:33.08ms
step:521/2160 train_time:17235ms step_avg:33.08ms
step:522/2160 train_time:17266ms step_avg:33.08ms
step:523/2160 train_time:17299ms step_avg:33.08ms
step:524/2160 train_time:17331ms step_avg:33.08ms
step:525/2160 train_time:17365ms step_avg:33.08ms
step:526/2160 train_time:17397ms step_avg:33.07ms
step:527/2160 train_time:17430ms step_avg:33.07ms
step:528/2160 train_time:17462ms step_avg:33.07ms
step:529/2160 train_time:17495ms step_avg:33.07ms
step:530/2160 train_time:17527ms step_avg:33.07ms
step:531/2160 train_time:17560ms step_avg:33.07ms
step:532/2160 train_time:17593ms step_avg:33.07ms
step:533/2160 train_time:17627ms step_avg:33.07ms
step:534/2160 train_time:17659ms step_avg:33.07ms
step:535/2160 train_time:17694ms step_avg:33.07ms
step:536/2160 train_time:17726ms step_avg:33.07ms
step:537/2160 train_time:17760ms step_avg:33.07ms
step:538/2160 train_time:17792ms step_avg:33.07ms
step:539/2160 train_time:17826ms step_avg:33.07ms
step:540/2160 train_time:17858ms step_avg:33.07ms
step:541/2160 train_time:17892ms step_avg:33.07ms
step:542/2160 train_time:17924ms step_avg:33.07ms
step:543/2160 train_time:17957ms step_avg:33.07ms
step:544/2160 train_time:17989ms step_avg:33.07ms
step:545/2160 train_time:18022ms step_avg:33.07ms
step:546/2160 train_time:18054ms step_avg:33.07ms
step:547/2160 train_time:18087ms step_avg:33.07ms
step:548/2160 train_time:18120ms step_avg:33.07ms
step:549/2160 train_time:18153ms step_avg:33.07ms
step:550/2160 train_time:18185ms step_avg:33.06ms
step:551/2160 train_time:18218ms step_avg:33.06ms
step:552/2160 train_time:18250ms step_avg:33.06ms
step:553/2160 train_time:18283ms step_avg:33.06ms
step:554/2160 train_time:18315ms step_avg:33.06ms
step:555/2160 train_time:18348ms step_avg:33.06ms
step:556/2160 train_time:18380ms step_avg:33.06ms
step:557/2160 train_time:18414ms step_avg:33.06ms
step:558/2160 train_time:18446ms step_avg:33.06ms
step:559/2160 train_time:18479ms step_avg:33.06ms
step:560/2160 train_time:18511ms step_avg:33.06ms
step:561/2160 train_time:18544ms step_avg:33.06ms
step:562/2160 train_time:18577ms step_avg:33.05ms
step:563/2160 train_time:18611ms step_avg:33.06ms
step:564/2160 train_time:18643ms step_avg:33.05ms
step:565/2160 train_time:18677ms step_avg:33.06ms
step:566/2160 train_time:18709ms step_avg:33.06ms
step:567/2160 train_time:18742ms step_avg:33.06ms
step:568/2160 train_time:18775ms step_avg:33.05ms
step:569/2160 train_time:18808ms step_avg:33.05ms
step:570/2160 train_time:18840ms step_avg:33.05ms
step:571/2160 train_time:18874ms step_avg:33.05ms
step:572/2160 train_time:18906ms step_avg:33.05ms
step:573/2160 train_time:18939ms step_avg:33.05ms
step:574/2160 train_time:18971ms step_avg:33.05ms
step:575/2160 train_time:19005ms step_avg:33.05ms
step:576/2160 train_time:19037ms step_avg:33.05ms
step:577/2160 train_time:19071ms step_avg:33.05ms
step:578/2160 train_time:19103ms step_avg:33.05ms
step:579/2160 train_time:19136ms step_avg:33.05ms
step:580/2160 train_time:19168ms step_avg:33.05ms
step:581/2160 train_time:19202ms step_avg:33.05ms
step:582/2160 train_time:19234ms step_avg:33.05ms
step:583/2160 train_time:19267ms step_avg:33.05ms
step:584/2160 train_time:19299ms step_avg:33.05ms
step:585/2160 train_time:19332ms step_avg:33.05ms
step:586/2160 train_time:19364ms step_avg:33.05ms
step:587/2160 train_time:19398ms step_avg:33.05ms
step:588/2160 train_time:19430ms step_avg:33.04ms
step:589/2160 train_time:19463ms step_avg:33.04ms
step:590/2160 train_time:19495ms step_avg:33.04ms
step:591/2160 train_time:19529ms step_avg:33.04ms
step:592/2160 train_time:19561ms step_avg:33.04ms
step:593/2160 train_time:19594ms step_avg:33.04ms
step:594/2160 train_time:19626ms step_avg:33.04ms
step:595/2160 train_time:19660ms step_avg:33.04ms
step:596/2160 train_time:19692ms step_avg:33.04ms
step:597/2160 train_time:19726ms step_avg:33.04ms
step:598/2160 train_time:19758ms step_avg:33.04ms
step:599/2160 train_time:19792ms step_avg:33.04ms
step:600/2160 train_time:19824ms step_avg:33.04ms
step:601/2160 train_time:19857ms step_avg:33.04ms
step:602/2160 train_time:19889ms step_avg:33.04ms
step:603/2160 train_time:19923ms step_avg:33.04ms
step:604/2160 train_time:19955ms step_avg:33.04ms
step:605/2160 train_time:19989ms step_avg:33.04ms
step:606/2160 train_time:20021ms step_avg:33.04ms
step:607/2160 train_time:20054ms step_avg:33.04ms
step:608/2160 train_time:20087ms step_avg:33.04ms
step:609/2160 train_time:20120ms step_avg:33.04ms
step:610/2160 train_time:20152ms step_avg:33.04ms
step:611/2160 train_time:20186ms step_avg:33.04ms
step:612/2160 train_time:20218ms step_avg:33.04ms
step:613/2160 train_time:20252ms step_avg:33.04ms
step:614/2160 train_time:20284ms step_avg:33.04ms
step:615/2160 train_time:20317ms step_avg:33.04ms
step:616/2160 train_time:20349ms step_avg:33.03ms
step:617/2160 train_time:20382ms step_avg:33.03ms
step:618/2160 train_time:20414ms step_avg:33.03ms
step:619/2160 train_time:20448ms step_avg:33.03ms
step:620/2160 train_time:20480ms step_avg:33.03ms
step:621/2160 train_time:20514ms step_avg:33.03ms
step:622/2160 train_time:20546ms step_avg:33.03ms
step:623/2160 train_time:20579ms step_avg:33.03ms
step:624/2160 train_time:20612ms step_avg:33.03ms
step:625/2160 train_time:20645ms step_avg:33.03ms
step:626/2160 train_time:20677ms step_avg:33.03ms
step:627/2160 train_time:20711ms step_avg:33.03ms
step:628/2160 train_time:20744ms step_avg:33.03ms
step:629/2160 train_time:20777ms step_avg:33.03ms
step:630/2160 train_time:20810ms step_avg:33.03ms
step:631/2160 train_time:20843ms step_avg:33.03ms
step:632/2160 train_time:20875ms step_avg:33.03ms
step:633/2160 train_time:20909ms step_avg:33.03ms
step:634/2160 train_time:20941ms step_avg:33.03ms
step:635/2160 train_time:20975ms step_avg:33.03ms
step:636/2160 train_time:21007ms step_avg:33.03ms
step:637/2160 train_time:21040ms step_avg:33.03ms
step:638/2160 train_time:21072ms step_avg:33.03ms
step:639/2160 train_time:21106ms step_avg:33.03ms
step:640/2160 train_time:21138ms step_avg:33.03ms
step:641/2160 train_time:21172ms step_avg:33.03ms
step:642/2160 train_time:21204ms step_avg:33.03ms
step:643/2160 train_time:21237ms step_avg:33.03ms
step:644/2160 train_time:21269ms step_avg:33.03ms
step:645/2160 train_time:21303ms step_avg:33.03ms
step:646/2160 train_time:21335ms step_avg:33.03ms
step:647/2160 train_time:21369ms step_avg:33.03ms
step:648/2160 train_time:21401ms step_avg:33.03ms
step:649/2160 train_time:21434ms step_avg:33.03ms
step:650/2160 train_time:21466ms step_avg:33.03ms
step:651/2160 train_time:21500ms step_avg:33.03ms
step:652/2160 train_time:21532ms step_avg:33.02ms
step:653/2160 train_time:21565ms step_avg:33.02ms
step:654/2160 train_time:21597ms step_avg:33.02ms
step:655/2160 train_time:21631ms step_avg:33.02ms
step:656/2160 train_time:21663ms step_avg:33.02ms
step:657/2160 train_time:21697ms step_avg:33.02ms
step:658/2160 train_time:21729ms step_avg:33.02ms
step:659/2160 train_time:21763ms step_avg:33.02ms
step:660/2160 train_time:21795ms step_avg:33.02ms
step:661/2160 train_time:21828ms step_avg:33.02ms
step:662/2160 train_time:21860ms step_avg:33.02ms
step:663/2160 train_time:21894ms step_avg:33.02ms
step:664/2160 train_time:21926ms step_avg:33.02ms
step:665/2160 train_time:21959ms step_avg:33.02ms
step:666/2160 train_time:21991ms step_avg:33.02ms
step:667/2160 train_time:22025ms step_avg:33.02ms
step:668/2160 train_time:22057ms step_avg:33.02ms
step:669/2160 train_time:22090ms step_avg:33.02ms
step:670/2160 train_time:22122ms step_avg:33.02ms
step:671/2160 train_time:22156ms step_avg:33.02ms
step:672/2160 train_time:22188ms step_avg:33.02ms
step:673/2160 train_time:22222ms step_avg:33.02ms
step:674/2160 train_time:22254ms step_avg:33.02ms
step:675/2160 train_time:22287ms step_avg:33.02ms
step:676/2160 train_time:22320ms step_avg:33.02ms
step:677/2160 train_time:22353ms step_avg:33.02ms
step:678/2160 train_time:22385ms step_avg:33.02ms
step:679/2160 train_time:22418ms step_avg:33.02ms
step:680/2160 train_time:22450ms step_avg:33.02ms
step:681/2160 train_time:22484ms step_avg:33.02ms
step:682/2160 train_time:22516ms step_avg:33.01ms
step:683/2160 train_time:22550ms step_avg:33.02ms
step:684/2160 train_time:22582ms step_avg:33.01ms
step:685/2160 train_time:22615ms step_avg:33.02ms
step:686/2160 train_time:22648ms step_avg:33.01ms
step:687/2160 train_time:22681ms step_avg:33.01ms
step:688/2160 train_time:22713ms step_avg:33.01ms
step:689/2160 train_time:22746ms step_avg:33.01ms
step:690/2160 train_time:22778ms step_avg:33.01ms
step:691/2160 train_time:22812ms step_avg:33.01ms
step:692/2160 train_time:22844ms step_avg:33.01ms
step:693/2160 train_time:22878ms step_avg:33.01ms
step:694/2160 train_time:22910ms step_avg:33.01ms
step:695/2160 train_time:22944ms step_avg:33.01ms
step:696/2160 train_time:22976ms step_avg:33.01ms
step:697/2160 train_time:23009ms step_avg:33.01ms
step:698/2160 train_time:23042ms step_avg:33.01ms
step:699/2160 train_time:23075ms step_avg:33.01ms
step:700/2160 train_time:23107ms step_avg:33.01ms
step:701/2160 train_time:23141ms step_avg:33.01ms
step:702/2160 train_time:23173ms step_avg:33.01ms
step:703/2160 train_time:23207ms step_avg:33.01ms
step:704/2160 train_time:23239ms step_avg:33.01ms
step:705/2160 train_time:23272ms step_avg:33.01ms
step:706/2160 train_time:23304ms step_avg:33.01ms
step:707/2160 train_time:23338ms step_avg:33.01ms
step:708/2160 train_time:23371ms step_avg:33.01ms
step:709/2160 train_time:23430ms step_avg:33.05ms
step:710/2160 train_time:23488ms step_avg:33.08ms
step:711/2160 train_time:23548ms step_avg:33.12ms
step:712/2160 train_time:23606ms step_avg:33.15ms
step:713/2160 train_time:23667ms step_avg:33.19ms
step:714/2160 train_time:23725ms step_avg:33.23ms
step:715/2160 train_time:23786ms step_avg:33.27ms
step:716/2160 train_time:23844ms step_avg:33.30ms
step:717/2160 train_time:23904ms step_avg:33.34ms
step:718/2160 train_time:23964ms step_avg:33.38ms
step:719/2160 train_time:24024ms step_avg:33.41ms
step:720/2160 train_time:24083ms step_avg:33.45ms
step:721/2160 train_time:24144ms step_avg:33.49ms
step:722/2160 train_time:24203ms step_avg:33.52ms
step:723/2160 train_time:24263ms step_avg:33.56ms
step:724/2160 train_time:24322ms step_avg:33.59ms
step:725/2160 train_time:24382ms step_avg:33.63ms
step:726/2160 train_time:24441ms step_avg:33.67ms
step:727/2160 train_time:24502ms step_avg:33.70ms
step:728/2160 train_time:24561ms step_avg:33.74ms
step:729/2160 train_time:24622ms step_avg:33.77ms
step:730/2160 train_time:24681ms step_avg:33.81ms
step:731/2160 train_time:24741ms step_avg:33.85ms
step:732/2160 train_time:24800ms step_avg:33.88ms
step:733/2160 train_time:24860ms step_avg:33.92ms
step:734/2160 train_time:24920ms step_avg:33.95ms
step:735/2160 train_time:24981ms step_avg:33.99ms
step:736/2160 train_time:25040ms step_avg:34.02ms
step:737/2160 train_time:25101ms step_avg:34.06ms
step:738/2160 train_time:25160ms step_avg:34.09ms
step:739/2160 train_time:25221ms step_avg:34.13ms
step:740/2160 train_time:25280ms step_avg:34.16ms
step:741/2160 train_time:25340ms step_avg:34.20ms
step:742/2160 train_time:25399ms step_avg:34.23ms
step:743/2160 train_time:25460ms step_avg:34.27ms
step:744/2160 train_time:25520ms step_avg:34.30ms
step:745/2160 train_time:25581ms step_avg:34.34ms
step:746/2160 train_time:25640ms step_avg:34.37ms
step:747/2160 train_time:25701ms step_avg:34.41ms
step:748/2160 train_time:25760ms step_avg:34.44ms
step:749/2160 train_time:25821ms step_avg:34.47ms
step:750/2160 train_time:25880ms step_avg:34.51ms
step:750/2160 val_loss:3.8726 train_time:25943ms step_avg:34.59ms
step:751/2160 train_time:25965ms step_avg:34.57ms
step:752/2160 train_time:26001ms step_avg:34.58ms
step:753/2160 train_time:26067ms step_avg:34.62ms
step:754/2160 train_time:26129ms step_avg:34.65ms
step:755/2160 train_time:26190ms step_avg:34.69ms
step:756/2160 train_time:26248ms step_avg:34.72ms
step:757/2160 train_time:26308ms step_avg:34.75ms
step:758/2160 train_time:26365ms step_avg:34.78ms
step:759/2160 train_time:26424ms step_avg:34.81ms
step:760/2160 train_time:26482ms step_avg:34.85ms
step:761/2160 train_time:26542ms step_avg:34.88ms
step:762/2160 train_time:26599ms step_avg:34.91ms
step:763/2160 train_time:26659ms step_avg:34.94ms
step:764/2160 train_time:26717ms step_avg:34.97ms
step:765/2160 train_time:26777ms step_avg:35.00ms
step:766/2160 train_time:26835ms step_avg:35.03ms
step:767/2160 train_time:26897ms step_avg:35.07ms
step:768/2160 train_time:26957ms step_avg:35.10ms
step:769/2160 train_time:27020ms step_avg:35.14ms
step:770/2160 train_time:27082ms step_avg:35.17ms
step:771/2160 train_time:27144ms step_avg:35.21ms
step:772/2160 train_time:27203ms step_avg:35.24ms
step:773/2160 train_time:27263ms step_avg:35.27ms
step:774/2160 train_time:27322ms step_avg:35.30ms
step:775/2160 train_time:27382ms step_avg:35.33ms
step:776/2160 train_time:27440ms step_avg:35.36ms
step:777/2160 train_time:27500ms step_avg:35.39ms
step:778/2160 train_time:27558ms step_avg:35.42ms
step:779/2160 train_time:27618ms step_avg:35.45ms
step:780/2160 train_time:27676ms step_avg:35.48ms
step:781/2160 train_time:27735ms step_avg:35.51ms
step:782/2160 train_time:27794ms step_avg:35.54ms
step:783/2160 train_time:27855ms step_avg:35.57ms
step:784/2160 train_time:27915ms step_avg:35.61ms
step:785/2160 train_time:27976ms step_avg:35.64ms
step:786/2160 train_time:28037ms step_avg:35.67ms
step:787/2160 train_time:28100ms step_avg:35.71ms
step:788/2160 train_time:28161ms step_avg:35.74ms
step:789/2160 train_time:28221ms step_avg:35.77ms
step:790/2160 train_time:28280ms step_avg:35.80ms
step:791/2160 train_time:28340ms step_avg:35.83ms
step:792/2160 train_time:28399ms step_avg:35.86ms
step:793/2160 train_time:28459ms step_avg:35.89ms
step:794/2160 train_time:28518ms step_avg:35.92ms
step:795/2160 train_time:28577ms step_avg:35.95ms
step:796/2160 train_time:28636ms step_avg:35.97ms
step:797/2160 train_time:28695ms step_avg:36.00ms
step:798/2160 train_time:28753ms step_avg:36.03ms
step:799/2160 train_time:28814ms step_avg:36.06ms
step:800/2160 train_time:28873ms step_avg:36.09ms
step:801/2160 train_time:28934ms step_avg:36.12ms
step:802/2160 train_time:28994ms step_avg:36.15ms
step:803/2160 train_time:29057ms step_avg:36.19ms
step:804/2160 train_time:29117ms step_avg:36.22ms
step:805/2160 train_time:29179ms step_avg:36.25ms
step:806/2160 train_time:29239ms step_avg:36.28ms
step:807/2160 train_time:29299ms step_avg:36.31ms
step:808/2160 train_time:29359ms step_avg:36.33ms
step:809/2160 train_time:29419ms step_avg:36.37ms
step:810/2160 train_time:29478ms step_avg:36.39ms
step:811/2160 train_time:29538ms step_avg:36.42ms
step:812/2160 train_time:29597ms step_avg:36.45ms
step:813/2160 train_time:29657ms step_avg:36.48ms
step:814/2160 train_time:29716ms step_avg:36.51ms
step:815/2160 train_time:29775ms step_avg:36.53ms
step:816/2160 train_time:29834ms step_avg:36.56ms
step:817/2160 train_time:29895ms step_avg:36.59ms
step:818/2160 train_time:29954ms step_avg:36.62ms
step:819/2160 train_time:30016ms step_avg:36.65ms
step:820/2160 train_time:30075ms step_avg:36.68ms
step:821/2160 train_time:30136ms step_avg:36.71ms
step:822/2160 train_time:30196ms step_avg:36.73ms
step:823/2160 train_time:30257ms step_avg:36.76ms
step:824/2160 train_time:30317ms step_avg:36.79ms
step:825/2160 train_time:30378ms step_avg:36.82ms
step:826/2160 train_time:30437ms step_avg:36.85ms
step:827/2160 train_time:30498ms step_avg:36.88ms
step:828/2160 train_time:30556ms step_avg:36.90ms
step:829/2160 train_time:30617ms step_avg:36.93ms
step:830/2160 train_time:30675ms step_avg:36.96ms
step:831/2160 train_time:30735ms step_avg:36.99ms
step:832/2160 train_time:30794ms step_avg:37.01ms
step:833/2160 train_time:30855ms step_avg:37.04ms
step:834/2160 train_time:30915ms step_avg:37.07ms
step:835/2160 train_time:30976ms step_avg:37.10ms
step:836/2160 train_time:31035ms step_avg:37.12ms
step:837/2160 train_time:31096ms step_avg:37.15ms
step:838/2160 train_time:31156ms step_avg:37.18ms
step:839/2160 train_time:31216ms step_avg:37.21ms
step:840/2160 train_time:31276ms step_avg:37.23ms
step:841/2160 train_time:31337ms step_avg:37.26ms
step:842/2160 train_time:31396ms step_avg:37.29ms
step:843/2160 train_time:31457ms step_avg:37.32ms
step:844/2160 train_time:31516ms step_avg:37.34ms
step:845/2160 train_time:31576ms step_avg:37.37ms
step:846/2160 train_time:31635ms step_avg:37.39ms
step:847/2160 train_time:31695ms step_avg:37.42ms
step:848/2160 train_time:31754ms step_avg:37.45ms
step:849/2160 train_time:31815ms step_avg:37.47ms
step:850/2160 train_time:31874ms step_avg:37.50ms
step:851/2160 train_time:31935ms step_avg:37.53ms
step:852/2160 train_time:31995ms step_avg:37.55ms
step:853/2160 train_time:32056ms step_avg:37.58ms
step:854/2160 train_time:32115ms step_avg:37.61ms
step:855/2160 train_time:32177ms step_avg:37.63ms
step:856/2160 train_time:32236ms step_avg:37.66ms
step:857/2160 train_time:32296ms step_avg:37.69ms
step:858/2160 train_time:32356ms step_avg:37.71ms
step:859/2160 train_time:32417ms step_avg:37.74ms
step:860/2160 train_time:32476ms step_avg:37.76ms
step:861/2160 train_time:32536ms step_avg:37.79ms
step:862/2160 train_time:32595ms step_avg:37.81ms
step:863/2160 train_time:32655ms step_avg:37.84ms
step:864/2160 train_time:32714ms step_avg:37.86ms
step:865/2160 train_time:32775ms step_avg:37.89ms
step:866/2160 train_time:32834ms step_avg:37.91ms
step:867/2160 train_time:32895ms step_avg:37.94ms
step:868/2160 train_time:32954ms step_avg:37.97ms
step:869/2160 train_time:33015ms step_avg:37.99ms
step:870/2160 train_time:33075ms step_avg:38.02ms
step:871/2160 train_time:33136ms step_avg:38.04ms
step:872/2160 train_time:33196ms step_avg:38.07ms
step:873/2160 train_time:33256ms step_avg:38.09ms
step:874/2160 train_time:33316ms step_avg:38.12ms
step:875/2160 train_time:33376ms step_avg:38.14ms
step:876/2160 train_time:33436ms step_avg:38.17ms
step:877/2160 train_time:33497ms step_avg:38.19ms
step:878/2160 train_time:33555ms step_avg:38.22ms
step:879/2160 train_time:33616ms step_avg:38.24ms
step:880/2160 train_time:33675ms step_avg:38.27ms
step:881/2160 train_time:33735ms step_avg:38.29ms
step:882/2160 train_time:33795ms step_avg:38.32ms
step:883/2160 train_time:33856ms step_avg:38.34ms
step:884/2160 train_time:33916ms step_avg:38.37ms
step:885/2160 train_time:33977ms step_avg:38.39ms
step:886/2160 train_time:34036ms step_avg:38.42ms
step:887/2160 train_time:34098ms step_avg:38.44ms
step:888/2160 train_time:34157ms step_avg:38.46ms
step:889/2160 train_time:34217ms step_avg:38.49ms
step:890/2160 train_time:34277ms step_avg:38.51ms
step:891/2160 train_time:34338ms step_avg:38.54ms
step:892/2160 train_time:34399ms step_avg:38.56ms
step:893/2160 train_time:34460ms step_avg:38.59ms
step:894/2160 train_time:34519ms step_avg:38.61ms
step:895/2160 train_time:34579ms step_avg:38.64ms
step:896/2160 train_time:34638ms step_avg:38.66ms
step:897/2160 train_time:34698ms step_avg:38.68ms
step:898/2160 train_time:34758ms step_avg:38.71ms
step:899/2160 train_time:34819ms step_avg:38.73ms
step:900/2160 train_time:34878ms step_avg:38.75ms
step:901/2160 train_time:34938ms step_avg:38.78ms
step:902/2160 train_time:34999ms step_avg:38.80ms
step:903/2160 train_time:35060ms step_avg:38.83ms
step:904/2160 train_time:35119ms step_avg:38.85ms
step:905/2160 train_time:35179ms step_avg:38.87ms
step:906/2160 train_time:35239ms step_avg:38.89ms
step:907/2160 train_time:35300ms step_avg:38.92ms
step:908/2160 train_time:35358ms step_avg:38.94ms
step:909/2160 train_time:35419ms step_avg:38.97ms
step:910/2160 train_time:35479ms step_avg:38.99ms
step:911/2160 train_time:35539ms step_avg:39.01ms
step:912/2160 train_time:35599ms step_avg:39.03ms
step:913/2160 train_time:35660ms step_avg:39.06ms
step:914/2160 train_time:35719ms step_avg:39.08ms
step:915/2160 train_time:35779ms step_avg:39.10ms
step:916/2160 train_time:35838ms step_avg:39.12ms
step:917/2160 train_time:35899ms step_avg:39.15ms
step:918/2160 train_time:35958ms step_avg:39.17ms
step:919/2160 train_time:36019ms step_avg:39.19ms
step:920/2160 train_time:36078ms step_avg:39.22ms
step:921/2160 train_time:36139ms step_avg:39.24ms
step:922/2160 train_time:36200ms step_avg:39.26ms
step:923/2160 train_time:36261ms step_avg:39.29ms
step:924/2160 train_time:36320ms step_avg:39.31ms
step:925/2160 train_time:36381ms step_avg:39.33ms
step:926/2160 train_time:36440ms step_avg:39.35ms
step:927/2160 train_time:36501ms step_avg:39.38ms
step:928/2160 train_time:36560ms step_avg:39.40ms
step:929/2160 train_time:36620ms step_avg:39.42ms
step:930/2160 train_time:36679ms step_avg:39.44ms
step:931/2160 train_time:36740ms step_avg:39.46ms
step:932/2160 train_time:36799ms step_avg:39.48ms
step:933/2160 train_time:36859ms step_avg:39.51ms
step:934/2160 train_time:36919ms step_avg:39.53ms
step:935/2160 train_time:36979ms step_avg:39.55ms
step:936/2160 train_time:37038ms step_avg:39.57ms
step:937/2160 train_time:37099ms step_avg:39.59ms
step:938/2160 train_time:37159ms step_avg:39.62ms
step:939/2160 train_time:37220ms step_avg:39.64ms
step:940/2160 train_time:37279ms step_avg:39.66ms
step:941/2160 train_time:37339ms step_avg:39.68ms
step:942/2160 train_time:37400ms step_avg:39.70ms
step:943/2160 train_time:37461ms step_avg:39.73ms
step:944/2160 train_time:37520ms step_avg:39.75ms
step:945/2160 train_time:37581ms step_avg:39.77ms
step:946/2160 train_time:37639ms step_avg:39.79ms
step:947/2160 train_time:37700ms step_avg:39.81ms
step:948/2160 train_time:37759ms step_avg:39.83ms
step:949/2160 train_time:37819ms step_avg:39.85ms
step:950/2160 train_time:37878ms step_avg:39.87ms
step:951/2160 train_time:37939ms step_avg:39.89ms
step:952/2160 train_time:37998ms step_avg:39.91ms
step:953/2160 train_time:38060ms step_avg:39.94ms
step:954/2160 train_time:38120ms step_avg:39.96ms
step:955/2160 train_time:38181ms step_avg:39.98ms
step:956/2160 train_time:38240ms step_avg:40.00ms
step:957/2160 train_time:38301ms step_avg:40.02ms
step:958/2160 train_time:38360ms step_avg:40.04ms
step:959/2160 train_time:38421ms step_avg:40.06ms
step:960/2160 train_time:38480ms step_avg:40.08ms
step:961/2160 train_time:38541ms step_avg:40.10ms
step:962/2160 train_time:38600ms step_avg:40.12ms
step:963/2160 train_time:38661ms step_avg:40.15ms
step:964/2160 train_time:38720ms step_avg:40.17ms
step:965/2160 train_time:38780ms step_avg:40.19ms
step:966/2160 train_time:38839ms step_avg:40.21ms
step:967/2160 train_time:38900ms step_avg:40.23ms
step:968/2160 train_time:38959ms step_avg:40.25ms
step:969/2160 train_time:39020ms step_avg:40.27ms
step:970/2160 train_time:39079ms step_avg:40.29ms
step:971/2160 train_time:39140ms step_avg:40.31ms
step:972/2160 train_time:39200ms step_avg:40.33ms
step:973/2160 train_time:39260ms step_avg:40.35ms
step:974/2160 train_time:39320ms step_avg:40.37ms
step:975/2160 train_time:39380ms step_avg:40.39ms
step:976/2160 train_time:39439ms step_avg:40.41ms
step:977/2160 train_time:39501ms step_avg:40.43ms
step:978/2160 train_time:39560ms step_avg:40.45ms
step:979/2160 train_time:39621ms step_avg:40.47ms
step:980/2160 train_time:39680ms step_avg:40.49ms
step:981/2160 train_time:39741ms step_avg:40.51ms
step:982/2160 train_time:39800ms step_avg:40.53ms
step:983/2160 train_time:39860ms step_avg:40.55ms
step:984/2160 train_time:39919ms step_avg:40.57ms
step:985/2160 train_time:39980ms step_avg:40.59ms
step:986/2160 train_time:40039ms step_avg:40.61ms
step:987/2160 train_time:40100ms step_avg:40.63ms
step:988/2160 train_time:40160ms step_avg:40.65ms
step:989/2160 train_time:40221ms step_avg:40.67ms
step:990/2160 train_time:40280ms step_avg:40.69ms
step:991/2160 train_time:40341ms step_avg:40.71ms
step:992/2160 train_time:40400ms step_avg:40.73ms
step:993/2160 train_time:40461ms step_avg:40.75ms
step:994/2160 train_time:40520ms step_avg:40.76ms
step:995/2160 train_time:40580ms step_avg:40.78ms
step:996/2160 train_time:40639ms step_avg:40.80ms
step:997/2160 train_time:40700ms step_avg:40.82ms
step:998/2160 train_time:40759ms step_avg:40.84ms
step:999/2160 train_time:40821ms step_avg:40.86ms
step:1000/2160 train_time:40880ms step_avg:40.88ms
step:1000/2160 val_loss:3.7129 train_time:40942ms step_avg:40.94ms
step:1001/2160 train_time:40964ms step_avg:40.92ms
step:1002/2160 train_time:41001ms step_avg:40.92ms
step:1003/2160 train_time:41064ms step_avg:40.94ms
step:1004/2160 train_time:41126ms step_avg:40.96ms
step:1005/2160 train_time:41188ms step_avg:40.98ms
step:1006/2160 train_time:41248ms step_avg:41.00ms
step:1007/2160 train_time:41309ms step_avg:41.02ms
step:1008/2160 train_time:41367ms step_avg:41.04ms
step:1009/2160 train_time:41427ms step_avg:41.06ms
step:1010/2160 train_time:41485ms step_avg:41.07ms
step:1011/2160 train_time:41545ms step_avg:41.09ms
step:1012/2160 train_time:41603ms step_avg:41.11ms
step:1013/2160 train_time:41662ms step_avg:41.13ms
step:1014/2160 train_time:41720ms step_avg:41.14ms
step:1015/2160 train_time:41779ms step_avg:41.16ms
step:1016/2160 train_time:41838ms step_avg:41.18ms
step:1017/2160 train_time:41899ms step_avg:41.20ms
step:1018/2160 train_time:41959ms step_avg:41.22ms
step:1019/2160 train_time:42021ms step_avg:41.24ms
step:1020/2160 train_time:42081ms step_avg:41.26ms
step:1021/2160 train_time:42143ms step_avg:41.28ms
step:1022/2160 train_time:42203ms step_avg:41.29ms
step:1023/2160 train_time:42264ms step_avg:41.31ms
step:1024/2160 train_time:42322ms step_avg:41.33ms
step:1025/2160 train_time:42382ms step_avg:41.35ms
step:1026/2160 train_time:42441ms step_avg:41.37ms
step:1027/2160 train_time:42501ms step_avg:41.38ms
step:1028/2160 train_time:42559ms step_avg:41.40ms
step:1029/2160 train_time:42619ms step_avg:41.42ms
step:1030/2160 train_time:42678ms step_avg:41.43ms
step:1031/2160 train_time:42738ms step_avg:41.45ms
step:1032/2160 train_time:42797ms step_avg:41.47ms
step:1033/2160 train_time:42857ms step_avg:41.49ms
step:1034/2160 train_time:42916ms step_avg:41.50ms
step:1035/2160 train_time:42977ms step_avg:41.52ms
step:1036/2160 train_time:43037ms step_avg:41.54ms
step:1037/2160 train_time:43100ms step_avg:41.56ms
step:1038/2160 train_time:43160ms step_avg:41.58ms
step:1039/2160 train_time:43220ms step_avg:41.60ms
step:1040/2160 train_time:43279ms step_avg:41.61ms
step:1041/2160 train_time:43341ms step_avg:41.63ms
step:1042/2160 train_time:43399ms step_avg:41.65ms
step:1043/2160 train_time:43460ms step_avg:41.67ms
step:1044/2160 train_time:43518ms step_avg:41.68ms
step:1045/2160 train_time:43578ms step_avg:41.70ms
step:1046/2160 train_time:43637ms step_avg:41.72ms
step:1047/2160 train_time:43697ms step_avg:41.74ms
step:1048/2160 train_time:43756ms step_avg:41.75ms
step:1049/2160 train_time:43817ms step_avg:41.77ms
step:1050/2160 train_time:43876ms step_avg:41.79ms
step:1051/2160 train_time:43938ms step_avg:41.81ms
step:1052/2160 train_time:43998ms step_avg:41.82ms
step:1053/2160 train_time:44059ms step_avg:41.84ms
step:1054/2160 train_time:44118ms step_avg:41.86ms
step:1055/2160 train_time:44180ms step_avg:41.88ms
step:1056/2160 train_time:44240ms step_avg:41.89ms
step:1057/2160 train_time:44301ms step_avg:41.91ms
step:1058/2160 train_time:44360ms step_avg:41.93ms
step:1059/2160 train_time:44420ms step_avg:41.95ms
step:1060/2160 train_time:44479ms step_avg:41.96ms
step:1061/2160 train_time:44540ms step_avg:41.98ms
step:1062/2160 train_time:44599ms step_avg:42.00ms
step:1063/2160 train_time:44658ms step_avg:42.01ms
step:1064/2160 train_time:44717ms step_avg:42.03ms
step:1065/2160 train_time:44778ms step_avg:42.04ms
step:1066/2160 train_time:44837ms step_avg:42.06ms
step:1067/2160 train_time:44899ms step_avg:42.08ms
step:1068/2160 train_time:44958ms step_avg:42.10ms
step:1069/2160 train_time:45019ms step_avg:42.11ms
step:1070/2160 train_time:45079ms step_avg:42.13ms
step:1071/2160 train_time:45140ms step_avg:42.15ms
step:1072/2160 train_time:45199ms step_avg:42.16ms
step:1073/2160 train_time:45261ms step_avg:42.18ms
step:1074/2160 train_time:45320ms step_avg:42.20ms
step:1075/2160 train_time:45380ms step_avg:42.21ms
step:1076/2160 train_time:45439ms step_avg:42.23ms
step:1077/2160 train_time:45500ms step_avg:42.25ms
step:1078/2160 train_time:45559ms step_avg:42.26ms
step:1079/2160 train_time:45619ms step_avg:42.28ms
step:1080/2160 train_time:45678ms step_avg:42.29ms
step:1081/2160 train_time:45738ms step_avg:42.31ms
step:1082/2160 train_time:45798ms step_avg:42.33ms
step:1083/2160 train_time:45859ms step_avg:42.34ms
step:1084/2160 train_time:45918ms step_avg:42.36ms
step:1085/2160 train_time:45979ms step_avg:42.38ms
step:1086/2160 train_time:46038ms step_avg:42.39ms
step:1087/2160 train_time:46099ms step_avg:42.41ms
step:1088/2160 train_time:46159ms step_avg:42.43ms
step:1089/2160 train_time:46220ms step_avg:42.44ms
step:1090/2160 train_time:46279ms step_avg:42.46ms
step:1091/2160 train_time:46341ms step_avg:42.48ms
step:1092/2160 train_time:46400ms step_avg:42.49ms
step:1093/2160 train_time:46460ms step_avg:42.51ms
step:1094/2160 train_time:46519ms step_avg:42.52ms
step:1095/2160 train_time:46579ms step_avg:42.54ms
step:1096/2160 train_time:46638ms step_avg:42.55ms
step:1097/2160 train_time:46698ms step_avg:42.57ms
step:1098/2160 train_time:46757ms step_avg:42.58ms
step:1099/2160 train_time:46818ms step_avg:42.60ms
step:1100/2160 train_time:46877ms step_avg:42.62ms
step:1101/2160 train_time:46938ms step_avg:42.63ms
step:1102/2160 train_time:46998ms step_avg:42.65ms
step:1103/2160 train_time:47059ms step_avg:42.66ms
step:1104/2160 train_time:47118ms step_avg:42.68ms
step:1105/2160 train_time:47180ms step_avg:42.70ms
step:1106/2160 train_time:47240ms step_avg:42.71ms
step:1107/2160 train_time:47300ms step_avg:42.73ms
step:1108/2160 train_time:47359ms step_avg:42.74ms
step:1109/2160 train_time:47419ms step_avg:42.76ms
step:1110/2160 train_time:47478ms step_avg:42.77ms
step:1111/2160 train_time:47540ms step_avg:42.79ms
step:1112/2160 train_time:47599ms step_avg:42.80ms
step:1113/2160 train_time:47659ms step_avg:42.82ms
step:1114/2160 train_time:47718ms step_avg:42.84ms
step:1115/2160 train_time:47779ms step_avg:42.85ms
step:1116/2160 train_time:47839ms step_avg:42.87ms
step:1117/2160 train_time:47900ms step_avg:42.88ms
step:1118/2160 train_time:47959ms step_avg:42.90ms
step:1119/2160 train_time:48019ms step_avg:42.91ms
step:1120/2160 train_time:48079ms step_avg:42.93ms
step:1121/2160 train_time:48140ms step_avg:42.94ms
step:1122/2160 train_time:48199ms step_avg:42.96ms
step:1123/2160 train_time:48259ms step_avg:42.97ms
step:1124/2160 train_time:48319ms step_avg:42.99ms
step:1125/2160 train_time:48379ms step_avg:43.00ms
step:1126/2160 train_time:48439ms step_avg:43.02ms
step:1127/2160 train_time:48500ms step_avg:43.03ms
step:1128/2160 train_time:48558ms step_avg:43.05ms
step:1129/2160 train_time:48618ms step_avg:43.06ms
step:1130/2160 train_time:48677ms step_avg:43.08ms
step:1131/2160 train_time:48738ms step_avg:43.09ms
step:1132/2160 train_time:48797ms step_avg:43.11ms
step:1133/2160 train_time:48857ms step_avg:43.12ms
step:1134/2160 train_time:48917ms step_avg:43.14ms
step:1135/2160 train_time:48978ms step_avg:43.15ms
step:1136/2160 train_time:49038ms step_avg:43.17ms
step:1137/2160 train_time:49099ms step_avg:43.18ms
step:1138/2160 train_time:49159ms step_avg:43.20ms
step:1139/2160 train_time:49220ms step_avg:43.21ms
step:1140/2160 train_time:49279ms step_avg:43.23ms
step:1141/2160 train_time:49340ms step_avg:43.24ms
step:1142/2160 train_time:49399ms step_avg:43.26ms
step:1143/2160 train_time:49460ms step_avg:43.27ms
step:1144/2160 train_time:49519ms step_avg:43.29ms
step:1145/2160 train_time:49580ms step_avg:43.30ms
step:1146/2160 train_time:49640ms step_avg:43.32ms
step:1147/2160 train_time:49700ms step_avg:43.33ms
step:1148/2160 train_time:49759ms step_avg:43.34ms
step:1149/2160 train_time:49819ms step_avg:43.36ms
step:1150/2160 train_time:49878ms step_avg:43.37ms
step:1151/2160 train_time:49940ms step_avg:43.39ms
step:1152/2160 train_time:49999ms step_avg:43.40ms
step:1153/2160 train_time:50060ms step_avg:43.42ms
step:1154/2160 train_time:50119ms step_avg:43.43ms
step:1155/2160 train_time:50180ms step_avg:43.45ms
step:1156/2160 train_time:50239ms step_avg:43.46ms
step:1157/2160 train_time:50300ms step_avg:43.47ms
step:1158/2160 train_time:50359ms step_avg:43.49ms
step:1159/2160 train_time:50420ms step_avg:43.50ms
step:1160/2160 train_time:50479ms step_avg:43.52ms
step:1161/2160 train_time:50539ms step_avg:43.53ms
step:1162/2160 train_time:50599ms step_avg:43.54ms
step:1163/2160 train_time:50659ms step_avg:43.56ms
step:1164/2160 train_time:50718ms step_avg:43.57ms
step:1165/2160 train_time:50779ms step_avg:43.59ms
step:1166/2160 train_time:50838ms step_avg:43.60ms
step:1167/2160 train_time:50899ms step_avg:43.62ms
step:1168/2160 train_time:50958ms step_avg:43.63ms
step:1169/2160 train_time:51018ms step_avg:43.64ms
step:1170/2160 train_time:51078ms step_avg:43.66ms
step:1171/2160 train_time:51140ms step_avg:43.67ms
step:1172/2160 train_time:51199ms step_avg:43.69ms
step:1173/2160 train_time:51260ms step_avg:43.70ms
step:1174/2160 train_time:51320ms step_avg:43.71ms
step:1175/2160 train_time:51380ms step_avg:43.73ms
step:1176/2160 train_time:51439ms step_avg:43.74ms
step:1177/2160 train_time:51500ms step_avg:43.76ms
step:1178/2160 train_time:51559ms step_avg:43.77ms
step:1179/2160 train_time:51619ms step_avg:43.78ms
step:1180/2160 train_time:51678ms step_avg:43.80ms
step:1181/2160 train_time:51739ms step_avg:43.81ms
step:1182/2160 train_time:51798ms step_avg:43.82ms
step:1183/2160 train_time:51859ms step_avg:43.84ms
step:1184/2160 train_time:51917ms step_avg:43.85ms
step:1185/2160 train_time:51979ms step_avg:43.86ms
step:1186/2160 train_time:52039ms step_avg:43.88ms
step:1187/2160 train_time:52099ms step_avg:43.89ms
step:1188/2160 train_time:52159ms step_avg:43.90ms
step:1189/2160 train_time:52219ms step_avg:43.92ms
step:1190/2160 train_time:52278ms step_avg:43.93ms
step:1191/2160 train_time:52339ms step_avg:43.95ms
step:1192/2160 train_time:52398ms step_avg:43.96ms
step:1193/2160 train_time:52458ms step_avg:43.97ms
step:1194/2160 train_time:52518ms step_avg:43.98ms
step:1195/2160 train_time:52578ms step_avg:44.00ms
step:1196/2160 train_time:52638ms step_avg:44.01ms
step:1197/2160 train_time:52699ms step_avg:44.03ms
step:1198/2160 train_time:52759ms step_avg:44.04ms
step:1199/2160 train_time:52819ms step_avg:44.05ms
step:1200/2160 train_time:52878ms step_avg:44.06ms
step:1201/2160 train_time:52939ms step_avg:44.08ms
step:1202/2160 train_time:52998ms step_avg:44.09ms
step:1203/2160 train_time:53060ms step_avg:44.11ms
step:1204/2160 train_time:53119ms step_avg:44.12ms
step:1205/2160 train_time:53179ms step_avg:44.13ms
step:1206/2160 train_time:53238ms step_avg:44.14ms
step:1207/2160 train_time:53300ms step_avg:44.16ms
step:1208/2160 train_time:53360ms step_avg:44.17ms
step:1209/2160 train_time:53420ms step_avg:44.19ms
step:1210/2160 train_time:53478ms step_avg:44.20ms
step:1211/2160 train_time:53539ms step_avg:44.21ms
step:1212/2160 train_time:53598ms step_avg:44.22ms
step:1213/2160 train_time:53659ms step_avg:44.24ms
step:1214/2160 train_time:53717ms step_avg:44.25ms
step:1215/2160 train_time:53779ms step_avg:44.26ms
step:1216/2160 train_time:53838ms step_avg:44.27ms
step:1217/2160 train_time:53899ms step_avg:44.29ms
step:1218/2160 train_time:53959ms step_avg:44.30ms
step:1219/2160 train_time:54020ms step_avg:44.31ms
step:1220/2160 train_time:54079ms step_avg:44.33ms
step:1221/2160 train_time:54140ms step_avg:44.34ms
step:1222/2160 train_time:54199ms step_avg:44.35ms
step:1223/2160 train_time:54260ms step_avg:44.37ms
step:1224/2160 train_time:54319ms step_avg:44.38ms
step:1225/2160 train_time:54380ms step_avg:44.39ms
step:1226/2160 train_time:54440ms step_avg:44.40ms
step:1227/2160 train_time:54500ms step_avg:44.42ms
step:1228/2160 train_time:54560ms step_avg:44.43ms
step:1229/2160 train_time:54620ms step_avg:44.44ms
step:1230/2160 train_time:54679ms step_avg:44.45ms
step:1231/2160 train_time:54740ms step_avg:44.47ms
step:1232/2160 train_time:54799ms step_avg:44.48ms
step:1233/2160 train_time:54860ms step_avg:44.49ms
step:1234/2160 train_time:54918ms step_avg:44.50ms
step:1235/2160 train_time:54979ms step_avg:44.52ms
step:1236/2160 train_time:55038ms step_avg:44.53ms
step:1237/2160 train_time:55099ms step_avg:44.54ms
step:1238/2160 train_time:55158ms step_avg:44.55ms
step:1239/2160 train_time:55219ms step_avg:44.57ms
step:1240/2160 train_time:55278ms step_avg:44.58ms
step:1241/2160 train_time:55339ms step_avg:44.59ms
step:1242/2160 train_time:55398ms step_avg:44.60ms
step:1243/2160 train_time:55459ms step_avg:44.62ms
step:1244/2160 train_time:55518ms step_avg:44.63ms
step:1245/2160 train_time:55579ms step_avg:44.64ms
step:1246/2160 train_time:55638ms step_avg:44.65ms
step:1247/2160 train_time:55699ms step_avg:44.67ms
step:1248/2160 train_time:55758ms step_avg:44.68ms
step:1249/2160 train_time:55818ms step_avg:44.69ms
step:1250/2160 train_time:55878ms step_avg:44.70ms
step:1250/2160 val_loss:3.5938 train_time:55942ms step_avg:44.75ms
step:1251/2160 train_time:55964ms step_avg:44.74ms
step:1252/2160 train_time:56001ms step_avg:44.73ms
step:1253/2160 train_time:56066ms step_avg:44.75ms
step:1254/2160 train_time:56128ms step_avg:44.76ms
step:1255/2160 train_time:56189ms step_avg:44.77ms
step:1256/2160 train_time:56247ms step_avg:44.78ms
step:1257/2160 train_time:56307ms step_avg:44.79ms
step:1258/2160 train_time:56365ms step_avg:44.80ms
step:1259/2160 train_time:56425ms step_avg:44.82ms
step:1260/2160 train_time:56483ms step_avg:44.83ms
step:1261/2160 train_time:56542ms step_avg:44.84ms
step:1262/2160 train_time:56600ms step_avg:44.85ms
step:1263/2160 train_time:56659ms step_avg:44.86ms
step:1264/2160 train_time:56717ms step_avg:44.87ms
step:1265/2160 train_time:56778ms step_avg:44.88ms
step:1266/2160 train_time:56837ms step_avg:44.89ms
step:1267/2160 train_time:56898ms step_avg:44.91ms
step:1268/2160 train_time:56958ms step_avg:44.92ms
step:1269/2160 train_time:57021ms step_avg:44.93ms
step:1270/2160 train_time:57082ms step_avg:44.95ms
step:1271/2160 train_time:57143ms step_avg:44.96ms
step:1272/2160 train_time:57203ms step_avg:44.97ms
step:1273/2160 train_time:57263ms step_avg:44.98ms
step:1274/2160 train_time:57322ms step_avg:44.99ms
step:1275/2160 train_time:57382ms step_avg:45.01ms
step:1276/2160 train_time:57440ms step_avg:45.02ms
step:1277/2160 train_time:57500ms step_avg:45.03ms
step:1278/2160 train_time:57558ms step_avg:45.04ms
step:1279/2160 train_time:57618ms step_avg:45.05ms
step:1280/2160 train_time:57676ms step_avg:45.06ms
step:1281/2160 train_time:57737ms step_avg:45.07ms
step:1282/2160 train_time:57795ms step_avg:45.08ms
step:1283/2160 train_time:57856ms step_avg:45.09ms
step:1284/2160 train_time:57916ms step_avg:45.11ms
step:1285/2160 train_time:57978ms step_avg:45.12ms
step:1286/2160 train_time:58038ms step_avg:45.13ms
step:1287/2160 train_time:58099ms step_avg:45.14ms
step:1288/2160 train_time:58160ms step_avg:45.16ms
step:1289/2160 train_time:58221ms step_avg:45.17ms
step:1290/2160 train_time:58280ms step_avg:45.18ms
step:1291/2160 train_time:58340ms step_avg:45.19ms
step:1292/2160 train_time:58399ms step_avg:45.20ms
step:1293/2160 train_time:58459ms step_avg:45.21ms
step:1294/2160 train_time:58517ms step_avg:45.22ms
step:1295/2160 train_time:58577ms step_avg:45.23ms
step:1296/2160 train_time:58636ms step_avg:45.24ms
step:1297/2160 train_time:58696ms step_avg:45.26ms
step:1298/2160 train_time:58755ms step_avg:45.27ms
step:1299/2160 train_time:58816ms step_avg:45.28ms
step:1300/2160 train_time:58875ms step_avg:45.29ms
step:1301/2160 train_time:58936ms step_avg:45.30ms
step:1302/2160 train_time:58996ms step_avg:45.31ms
step:1303/2160 train_time:59058ms step_avg:45.32ms
step:1304/2160 train_time:59118ms step_avg:45.34ms
step:1305/2160 train_time:59180ms step_avg:45.35ms
step:1306/2160 train_time:59240ms step_avg:45.36ms
step:1307/2160 train_time:59300ms step_avg:45.37ms
step:1308/2160 train_time:59358ms step_avg:45.38ms
step:1309/2160 train_time:59419ms step_avg:45.39ms
step:1310/2160 train_time:59477ms step_avg:45.40ms
step:1311/2160 train_time:59537ms step_avg:45.41ms
step:1312/2160 train_time:59596ms step_avg:45.42ms
step:1313/2160 train_time:59655ms step_avg:45.43ms
step:1314/2160 train_time:59714ms step_avg:45.44ms
step:1315/2160 train_time:59775ms step_avg:45.46ms
step:1316/2160 train_time:59834ms step_avg:45.47ms
step:1317/2160 train_time:59895ms step_avg:45.48ms
step:1318/2160 train_time:59955ms step_avg:45.49ms
step:1319/2160 train_time:60017ms step_avg:45.50ms
step:1320/2160 train_time:60077ms step_avg:45.51ms
step:1321/2160 train_time:60138ms step_avg:45.52ms
step:1322/2160 train_time:60198ms step_avg:45.54ms
step:1323/2160 train_time:60259ms step_avg:45.55ms
step:1324/2160 train_time:60318ms step_avg:45.56ms
step:1325/2160 train_time:60379ms step_avg:45.57ms
step:1326/2160 train_time:60437ms step_avg:45.58ms
step:1327/2160 train_time:60497ms step_avg:45.59ms
step:1328/2160 train_time:60556ms step_avg:45.60ms
step:1329/2160 train_time:60616ms step_avg:45.61ms
step:1330/2160 train_time:60675ms step_avg:45.62ms
step:1331/2160 train_time:60736ms step_avg:45.63ms
step:1332/2160 train_time:60795ms step_avg:45.64ms
step:1333/2160 train_time:60856ms step_avg:45.65ms
step:1334/2160 train_time:60915ms step_avg:45.66ms
step:1335/2160 train_time:60977ms step_avg:45.68ms
step:1336/2160 train_time:61037ms step_avg:45.69ms
step:1337/2160 train_time:61100ms step_avg:45.70ms
step:1338/2160 train_time:61159ms step_avg:45.71ms
step:1339/2160 train_time:61220ms step_avg:45.72ms
step:1340/2160 train_time:61280ms step_avg:45.73ms
step:1341/2160 train_time:61340ms step_avg:45.74ms
step:1342/2160 train_time:61399ms step_avg:45.75ms
step:1343/2160 train_time:61458ms step_avg:45.76ms
step:1344/2160 train_time:61517ms step_avg:45.77ms
step:1345/2160 train_time:61577ms step_avg:45.78ms
step:1346/2160 train_time:61636ms step_avg:45.79ms
step:1347/2160 train_time:61696ms step_avg:45.80ms
step:1348/2160 train_time:61756ms step_avg:45.81ms
step:1349/2160 train_time:61817ms step_avg:45.82ms
step:1350/2160 train_time:61877ms step_avg:45.83ms
step:1351/2160 train_time:61939ms step_avg:45.85ms
step:1352/2160 train_time:61999ms step_avg:45.86ms
step:1353/2160 train_time:62058ms step_avg:45.87ms
step:1354/2160 train_time:62118ms step_avg:45.88ms
step:1355/2160 train_time:62179ms step_avg:45.89ms
step:1356/2160 train_time:62238ms step_avg:45.90ms
step:1357/2160 train_time:62299ms step_avg:45.91ms
step:1358/2160 train_time:62358ms step_avg:45.92ms
step:1359/2160 train_time:62418ms step_avg:45.93ms
step:1360/2160 train_time:62477ms step_avg:45.94ms
step:1361/2160 train_time:62538ms step_avg:45.95ms
step:1362/2160 train_time:62597ms step_avg:45.96ms
step:1363/2160 train_time:62657ms step_avg:45.97ms
step:1364/2160 train_time:62716ms step_avg:45.98ms
step:1365/2160 train_time:62777ms step_avg:45.99ms
step:1366/2160 train_time:62837ms step_avg:46.00ms
step:1367/2160 train_time:62898ms step_avg:46.01ms
step:1368/2160 train_time:62957ms step_avg:46.02ms
step:1369/2160 train_time:63018ms step_avg:46.03ms
step:1370/2160 train_time:63079ms step_avg:46.04ms
step:1371/2160 train_time:63139ms step_avg:46.05ms
step:1372/2160 train_time:63198ms step_avg:46.06ms
step:1373/2160 train_time:63259ms step_avg:46.07ms
step:1374/2160 train_time:63318ms step_avg:46.08ms
step:1375/2160 train_time:63379ms step_avg:46.09ms
step:1376/2160 train_time:63438ms step_avg:46.10ms
step:1377/2160 train_time:63498ms step_avg:46.11ms
step:1378/2160 train_time:63557ms step_avg:46.12ms
step:1379/2160 train_time:63617ms step_avg:46.13ms
step:1380/2160 train_time:63676ms step_avg:46.14ms
step:1381/2160 train_time:63737ms step_avg:46.15ms
step:1382/2160 train_time:63797ms step_avg:46.16ms
step:1383/2160 train_time:63858ms step_avg:46.17ms
step:1384/2160 train_time:63917ms step_avg:46.18ms
step:1385/2160 train_time:63978ms step_avg:46.19ms
step:1386/2160 train_time:64038ms step_avg:46.20ms
step:1387/2160 train_time:64099ms step_avg:46.21ms
step:1388/2160 train_time:64158ms step_avg:46.22ms
step:1389/2160 train_time:64219ms step_avg:46.23ms
step:1390/2160 train_time:64278ms step_avg:46.24ms
step:1391/2160 train_time:64339ms step_avg:46.25ms
step:1392/2160 train_time:64398ms step_avg:46.26ms
step:1393/2160 train_time:64459ms step_avg:46.27ms
step:1394/2160 train_time:64518ms step_avg:46.28ms
step:1395/2160 train_time:64579ms step_avg:46.29ms
step:1396/2160 train_time:64637ms step_avg:46.30ms
step:1397/2160 train_time:64697ms step_avg:46.31ms
step:1398/2160 train_time:64757ms step_avg:46.32ms
step:1399/2160 train_time:64817ms step_avg:46.33ms
step:1400/2160 train_time:64878ms step_avg:46.34ms
step:1401/2160 train_time:64939ms step_avg:46.35ms
step:1402/2160 train_time:64998ms step_avg:46.36ms
step:1403/2160 train_time:65059ms step_avg:46.37ms
step:1404/2160 train_time:65118ms step_avg:46.38ms
step:1405/2160 train_time:65180ms step_avg:46.39ms
step:1406/2160 train_time:65238ms step_avg:46.40ms
step:1407/2160 train_time:65299ms step_avg:46.41ms
step:1408/2160 train_time:65358ms step_avg:46.42ms
step:1409/2160 train_time:65419ms step_avg:46.43ms
step:1410/2160 train_time:65479ms step_avg:46.44ms
step:1411/2160 train_time:65539ms step_avg:46.45ms
step:1412/2160 train_time:65598ms step_avg:46.46ms
step:1413/2160 train_time:65658ms step_avg:46.47ms
step:1414/2160 train_time:65717ms step_avg:46.48ms
step:1415/2160 train_time:65779ms step_avg:46.49ms
step:1416/2160 train_time:65867ms step_avg:46.52ms
step:1417/2160 train_time:65955ms step_avg:46.55ms
step:1418/2160 train_time:66042ms step_avg:46.57ms
step:1419/2160 train_time:66131ms step_avg:46.60ms
step:1420/2160 train_time:66217ms step_avg:46.63ms
step:1421/2160 train_time:66306ms step_avg:46.66ms
step:1422/2160 train_time:66392ms step_avg:46.69ms
step:1423/2160 train_time:66481ms step_avg:46.72ms
step:1424/2160 train_time:66568ms step_avg:46.75ms
step:1425/2160 train_time:66655ms step_avg:46.78ms
step:1426/2160 train_time:66743ms step_avg:46.80ms
step:1427/2160 train_time:66831ms step_avg:46.83ms
step:1428/2160 train_time:66917ms step_avg:46.86ms
step:1429/2160 train_time:67007ms step_avg:46.89ms
step:1430/2160 train_time:67094ms step_avg:46.92ms
step:1431/2160 train_time:67184ms step_avg:46.95ms
step:1432/2160 train_time:67271ms step_avg:46.98ms
step:1433/2160 train_time:67359ms step_avg:47.01ms
step:1434/2160 train_time:67446ms step_avg:47.03ms
step:1435/2160 train_time:67535ms step_avg:47.06ms
step:1436/2160 train_time:67622ms step_avg:47.09ms
step:1437/2160 train_time:67711ms step_avg:47.12ms
step:1438/2160 train_time:67797ms step_avg:47.15ms
step:1439/2160 train_time:67886ms step_avg:47.18ms
step:1440/2160 train_time:67973ms step_avg:47.20ms
step:1441/2160 train_time:68060ms step_avg:47.23ms
step:1442/2160 train_time:68148ms step_avg:47.26ms
step:1443/2160 train_time:68236ms step_avg:47.29ms
step:1444/2160 train_time:68324ms step_avg:47.32ms
step:1445/2160 train_time:68413ms step_avg:47.34ms
step:1446/2160 train_time:68500ms step_avg:47.37ms
step:1447/2160 train_time:68589ms step_avg:47.40ms
step:1448/2160 train_time:68675ms step_avg:47.43ms
step:1449/2160 train_time:68763ms step_avg:47.46ms
step:1450/2160 train_time:68851ms step_avg:47.48ms
step:1451/2160 train_time:68939ms step_avg:47.51ms
step:1452/2160 train_time:69026ms step_avg:47.54ms
step:1453/2160 train_time:69115ms step_avg:47.57ms
step:1454/2160 train_time:69202ms step_avg:47.59ms
step:1455/2160 train_time:69292ms step_avg:47.62ms
step:1456/2160 train_time:69379ms step_avg:47.65ms
step:1457/2160 train_time:69469ms step_avg:47.68ms
step:1458/2160 train_time:69555ms step_avg:47.71ms
step:1459/2160 train_time:69644ms step_avg:47.73ms
step:1460/2160 train_time:69731ms step_avg:47.76ms
step:1461/2160 train_time:69820ms step_avg:47.79ms
step:1462/2160 train_time:69906ms step_avg:47.82ms
step:1463/2160 train_time:69995ms step_avg:47.84ms
step:1464/2160 train_time:70081ms step_avg:47.87ms
step:1465/2160 train_time:70171ms step_avg:47.90ms
step:1466/2160 train_time:70257ms step_avg:47.92ms
step:1467/2160 train_time:70347ms step_avg:47.95ms
step:1468/2160 train_time:70433ms step_avg:47.98ms
step:1469/2160 train_time:70523ms step_avg:48.01ms
step:1470/2160 train_time:70610ms step_avg:48.03ms
step:1471/2160 train_time:70698ms step_avg:48.06ms
step:1472/2160 train_time:70785ms step_avg:48.09ms
step:1473/2160 train_time:70874ms step_avg:48.12ms
step:1474/2160 train_time:70961ms step_avg:48.14ms
step:1475/2160 train_time:71052ms step_avg:48.17ms
step:1476/2160 train_time:71138ms step_avg:48.20ms
step:1477/2160 train_time:71227ms step_avg:48.22ms
step:1478/2160 train_time:71314ms step_avg:48.25ms
step:1479/2160 train_time:71403ms step_avg:48.28ms
step:1480/2160 train_time:71491ms step_avg:48.30ms
step:1481/2160 train_time:71580ms step_avg:48.33ms
step:1482/2160 train_time:71667ms step_avg:48.36ms
step:1483/2160 train_time:71756ms step_avg:48.39ms
step:1484/2160 train_time:71842ms step_avg:48.41ms
step:1485/2160 train_time:71931ms step_avg:48.44ms
step:1486/2160 train_time:72018ms step_avg:48.46ms
step:1487/2160 train_time:72107ms step_avg:48.49ms
step:1488/2160 train_time:72193ms step_avg:48.52ms
step:1489/2160 train_time:72282ms step_avg:48.54ms
step:1490/2160 train_time:72369ms step_avg:48.57ms
step:1491/2160 train_time:72456ms step_avg:48.60ms
step:1492/2160 train_time:72543ms step_avg:48.62ms
step:1493/2160 train_time:72632ms step_avg:48.65ms
step:1494/2160 train_time:72718ms step_avg:48.67ms
step:1495/2160 train_time:72807ms step_avg:48.70ms
step:1496/2160 train_time:72893ms step_avg:48.73ms
step:1497/2160 train_time:72981ms step_avg:48.75ms
step:1498/2160 train_time:73068ms step_avg:48.78ms
step:1499/2160 train_time:73156ms step_avg:48.80ms
step:1500/2160 train_time:73243ms step_avg:48.83ms
step:1500/2160 val_loss:3.4909 train_time:73334ms step_avg:48.89ms
step:1501/2160 train_time:73356ms step_avg:48.87ms
step:1502/2160 train_time:73424ms step_avg:48.88ms
step:1503/2160 train_time:73520ms step_avg:48.92ms
step:1504/2160 train_time:73606ms step_avg:48.94ms
step:1505/2160 train_time:73696ms step_avg:48.97ms
step:1506/2160 train_time:73782ms step_avg:48.99ms
step:1507/2160 train_time:73869ms step_avg:49.02ms
step:1508/2160 train_time:73954ms step_avg:49.04ms
step:1509/2160 train_time:74042ms step_avg:49.07ms
step:1510/2160 train_time:74127ms step_avg:49.09ms
step:1511/2160 train_time:74215ms step_avg:49.12ms
step:1512/2160 train_time:74302ms step_avg:49.14ms
step:1513/2160 train_time:74394ms step_avg:49.17ms
step:1514/2160 train_time:74485ms step_avg:49.20ms
step:1515/2160 train_time:74577ms step_avg:49.23ms
step:1516/2160 train_time:74664ms step_avg:49.25ms
step:1517/2160 train_time:74752ms step_avg:49.28ms
step:1518/2160 train_time:74838ms step_avg:49.30ms
step:1519/2160 train_time:74924ms step_avg:49.32ms
step:1520/2160 train_time:75009ms step_avg:49.35ms
step:1521/2160 train_time:75097ms step_avg:49.37ms
step:1522/2160 train_time:75183ms step_avg:49.40ms
step:1523/2160 train_time:75271ms step_avg:49.42ms
step:1524/2160 train_time:75359ms step_avg:49.45ms
step:1525/2160 train_time:75449ms step_avg:49.47ms
step:1526/2160 train_time:75539ms step_avg:49.50ms
step:1527/2160 train_time:75628ms step_avg:49.53ms
step:1528/2160 train_time:75714ms step_avg:49.55ms
step:1529/2160 train_time:75803ms step_avg:49.58ms
step:1530/2160 train_time:75889ms step_avg:49.60ms
step:1531/2160 train_time:75978ms step_avg:49.63ms
step:1532/2160 train_time:76063ms step_avg:49.65ms
step:1533/2160 train_time:76150ms step_avg:49.67ms
step:1534/2160 train_time:76237ms step_avg:49.70ms
step:1535/2160 train_time:76326ms step_avg:49.72ms
step:1536/2160 train_time:76413ms step_avg:49.75ms
step:1537/2160 train_time:76504ms step_avg:49.78ms
step:1538/2160 train_time:76593ms step_avg:49.80ms
step:1539/2160 train_time:76684ms step_avg:49.83ms
step:1540/2160 train_time:76771ms step_avg:49.85ms
step:1541/2160 train_time:76859ms step_avg:49.88ms
step:1542/2160 train_time:76945ms step_avg:49.90ms
step:1543/2160 train_time:77033ms step_avg:49.92ms
step:1544/2160 train_time:77119ms step_avg:49.95ms
step:1545/2160 train_time:77207ms step_avg:49.97ms
step:1546/2160 train_time:77294ms step_avg:50.00ms
step:1547/2160 train_time:77384ms step_avg:50.02ms
step:1548/2160 train_time:77472ms step_avg:50.05ms
step:1549/2160 train_time:77563ms step_avg:50.07ms
step:1550/2160 train_time:77651ms step_avg:50.10ms
step:1551/2160 train_time:77740ms step_avg:50.12ms
step:1552/2160 train_time:77826ms step_avg:50.15ms
step:1553/2160 train_time:77914ms step_avg:50.17ms
step:1554/2160 train_time:78001ms step_avg:50.19ms
step:1555/2160 train_time:78086ms step_avg:50.22ms
step:1556/2160 train_time:78173ms step_avg:50.24ms
step:1557/2160 train_time:78262ms step_avg:50.26ms
step:1558/2160 train_time:78350ms step_avg:50.29ms
step:1559/2160 train_time:78440ms step_avg:50.31ms
step:1560/2160 train_time:78527ms step_avg:50.34ms
step:1561/2160 train_time:78617ms step_avg:50.36ms
step:1562/2160 train_time:78704ms step_avg:50.39ms
step:1563/2160 train_time:78792ms step_avg:50.41ms
step:1564/2160 train_time:78879ms step_avg:50.43ms
step:1565/2160 train_time:78966ms step_avg:50.46ms
step:1566/2160 train_time:79052ms step_avg:50.48ms
step:1567/2160 train_time:79140ms step_avg:50.50ms
step:1568/2160 train_time:79226ms step_avg:50.53ms
step:1569/2160 train_time:79315ms step_avg:50.55ms
step:1570/2160 train_time:79402ms step_avg:50.57ms
step:1571/2160 train_time:79492ms step_avg:50.60ms
step:1572/2160 train_time:79580ms step_avg:50.62ms
step:1573/2160 train_time:79669ms step_avg:50.65ms
step:1574/2160 train_time:79756ms step_avg:50.67ms
step:1575/2160 train_time:79844ms step_avg:50.69ms
step:1576/2160 train_time:79930ms step_avg:50.72ms
step:1577/2160 train_time:80019ms step_avg:50.74ms
step:1578/2160 train_time:80105ms step_avg:50.76ms
step:1579/2160 train_time:80193ms step_avg:50.79ms
step:1580/2160 train_time:80280ms step_avg:50.81ms
step:1581/2160 train_time:80369ms step_avg:50.83ms
step:1582/2160 train_time:80456ms step_avg:50.86ms
step:1583/2160 train_time:80545ms step_avg:50.88ms
step:1584/2160 train_time:80633ms step_avg:50.90ms
step:1585/2160 train_time:80723ms step_avg:50.93ms
step:1586/2160 train_time:80810ms step_avg:50.95ms
step:1587/2160 train_time:80899ms step_avg:50.98ms
step:1588/2160 train_time:80985ms step_avg:51.00ms
step:1589/2160 train_time:81074ms step_avg:51.02ms
step:1590/2160 train_time:81161ms step_avg:51.04ms
step:1591/2160 train_time:81250ms step_avg:51.07ms
step:1592/2160 train_time:81338ms step_avg:51.09ms
step:1593/2160 train_time:81427ms step_avg:51.12ms
step:1594/2160 train_time:81514ms step_avg:51.14ms
step:1595/2160 train_time:81604ms step_avg:51.16ms
step:1596/2160 train_time:81691ms step_avg:51.18ms
step:1597/2160 train_time:81780ms step_avg:51.21ms
step:1598/2160 train_time:81867ms step_avg:51.23ms
step:1599/2160 train_time:81955ms step_avg:51.25ms
step:1600/2160 train_time:82042ms step_avg:51.28ms
step:1601/2160 train_time:82130ms step_avg:51.30ms
step:1602/2160 train_time:82216ms step_avg:51.32ms
step:1603/2160 train_time:82305ms step_avg:51.34ms
step:1604/2160 train_time:82391ms step_avg:51.37ms
step:1605/2160 train_time:82482ms step_avg:51.39ms
step:1606/2160 train_time:82570ms step_avg:51.41ms
step:1607/2160 train_time:82658ms step_avg:51.44ms
step:1608/2160 train_time:82745ms step_avg:51.46ms
step:1609/2160 train_time:82834ms step_avg:51.48ms
step:1610/2160 train_time:82921ms step_avg:51.50ms
step:1611/2160 train_time:83009ms step_avg:51.53ms
step:1612/2160 train_time:83096ms step_avg:51.55ms
step:1613/2160 train_time:83184ms step_avg:51.57ms
step:1614/2160 train_time:83270ms step_avg:51.59ms
step:1615/2160 train_time:83359ms step_avg:51.62ms
step:1616/2160 train_time:83446ms step_avg:51.64ms
step:1617/2160 train_time:83536ms step_avg:51.66ms
step:1618/2160 train_time:83623ms step_avg:51.68ms
step:1619/2160 train_time:83711ms step_avg:51.71ms
step:1620/2160 train_time:83798ms step_avg:51.73ms
step:1621/2160 train_time:83887ms step_avg:51.75ms
step:1622/2160 train_time:83974ms step_avg:51.77ms
step:1623/2160 train_time:84064ms step_avg:51.80ms
step:1624/2160 train_time:84151ms step_avg:51.82ms
step:1625/2160 train_time:84240ms step_avg:51.84ms
step:1626/2160 train_time:84326ms step_avg:51.86ms
step:1627/2160 train_time:84416ms step_avg:51.88ms
step:1628/2160 train_time:84503ms step_avg:51.91ms
step:1629/2160 train_time:84591ms step_avg:51.93ms
step:1630/2160 train_time:84679ms step_avg:51.95ms
step:1631/2160 train_time:84768ms step_avg:51.97ms
step:1632/2160 train_time:84854ms step_avg:51.99ms
step:1633/2160 train_time:84944ms step_avg:52.02ms
step:1634/2160 train_time:85030ms step_avg:52.04ms
step:1635/2160 train_time:85119ms step_avg:52.06ms
step:1636/2160 train_time:85205ms step_avg:52.08ms
step:1637/2160 train_time:85294ms step_avg:52.10ms
step:1638/2160 train_time:85382ms step_avg:52.13ms
step:1639/2160 train_time:85470ms step_avg:52.15ms
step:1640/2160 train_time:85557ms step_avg:52.17ms
step:1641/2160 train_time:85646ms step_avg:52.19ms
step:1642/2160 train_time:85733ms step_avg:52.21ms
step:1643/2160 train_time:85821ms step_avg:52.23ms
step:1644/2160 train_time:85907ms step_avg:52.25ms
step:1645/2160 train_time:85995ms step_avg:52.28ms
step:1646/2160 train_time:86082ms step_avg:52.30ms
step:1647/2160 train_time:86170ms step_avg:52.32ms
step:1648/2160 train_time:86257ms step_avg:52.34ms
step:1649/2160 train_time:86345ms step_avg:52.36ms
step:1650/2160 train_time:86432ms step_avg:52.38ms
step:1651/2160 train_time:86522ms step_avg:52.41ms
step:1652/2160 train_time:86610ms step_avg:52.43ms
step:1653/2160 train_time:86699ms step_avg:52.45ms
step:1654/2160 train_time:86785ms step_avg:52.47ms
step:1655/2160 train_time:86873ms step_avg:52.49ms
step:1656/2160 train_time:86961ms step_avg:52.51ms
step:1657/2160 train_time:87049ms step_avg:52.53ms
step:1658/2160 train_time:87137ms step_avg:52.56ms
step:1659/2160 train_time:87226ms step_avg:52.58ms
step:1660/2160 train_time:87313ms step_avg:52.60ms
step:1661/2160 train_time:87402ms step_avg:52.62ms
step:1662/2160 train_time:87488ms step_avg:52.64ms
step:1663/2160 train_time:87578ms step_avg:52.66ms
step:1664/2160 train_time:87664ms step_avg:52.68ms
step:1665/2160 train_time:87752ms step_avg:52.70ms
step:1666/2160 train_time:87840ms step_avg:52.73ms
step:1667/2160 train_time:87928ms step_avg:52.75ms
step:1668/2160 train_time:88015ms step_avg:52.77ms
step:1669/2160 train_time:88104ms step_avg:52.79ms
step:1670/2160 train_time:88191ms step_avg:52.81ms
step:1671/2160 train_time:88280ms step_avg:52.83ms
step:1672/2160 train_time:88365ms step_avg:52.85ms
step:1673/2160 train_time:88454ms step_avg:52.87ms
step:1674/2160 train_time:88542ms step_avg:52.89ms
step:1675/2160 train_time:88631ms step_avg:52.91ms
step:1676/2160 train_time:88717ms step_avg:52.93ms
step:1677/2160 train_time:88805ms step_avg:52.95ms
step:1678/2160 train_time:88891ms step_avg:52.97ms
step:1679/2160 train_time:88980ms step_avg:53.00ms
step:1680/2160 train_time:89067ms step_avg:53.02ms
step:1681/2160 train_time:89156ms step_avg:53.04ms
step:1682/2160 train_time:89242ms step_avg:53.06ms
step:1683/2160 train_time:89331ms step_avg:53.08ms
step:1684/2160 train_time:89419ms step_avg:53.10ms
step:1685/2160 train_time:89507ms step_avg:53.12ms
step:1686/2160 train_time:89594ms step_avg:53.14ms
step:1687/2160 train_time:89684ms step_avg:53.16ms
step:1688/2160 train_time:89771ms step_avg:53.18ms
step:1689/2160 train_time:89859ms step_avg:53.20ms
step:1690/2160 train_time:89946ms step_avg:53.22ms
step:1691/2160 train_time:90034ms step_avg:53.24ms
step:1692/2160 train_time:90121ms step_avg:53.26ms
step:1693/2160 train_time:90210ms step_avg:53.28ms
step:1694/2160 train_time:90297ms step_avg:53.30ms
step:1695/2160 train_time:90385ms step_avg:53.32ms
step:1696/2160 train_time:90472ms step_avg:53.34ms
step:1697/2160 train_time:90561ms step_avg:53.37ms
step:1698/2160 train_time:90649ms step_avg:53.39ms
step:1699/2160 train_time:90737ms step_avg:53.41ms
step:1700/2160 train_time:90823ms step_avg:53.43ms
step:1701/2160 train_time:90912ms step_avg:53.45ms
step:1702/2160 train_time:90999ms step_avg:53.47ms
step:1703/2160 train_time:91087ms step_avg:53.49ms
step:1704/2160 train_time:91174ms step_avg:53.51ms
step:1705/2160 train_time:91263ms step_avg:53.53ms
step:1706/2160 train_time:91350ms step_avg:53.55ms
step:1707/2160 train_time:91439ms step_avg:53.57ms
step:1708/2160 train_time:91524ms step_avg:53.59ms
step:1709/2160 train_time:91614ms step_avg:53.61ms
step:1710/2160 train_time:91701ms step_avg:53.63ms
step:1711/2160 train_time:91789ms step_avg:53.65ms
step:1712/2160 train_time:91876ms step_avg:53.67ms
step:1713/2160 train_time:91965ms step_avg:53.69ms
step:1714/2160 train_time:92051ms step_avg:53.71ms
step:1715/2160 train_time:92140ms step_avg:53.73ms
step:1716/2160 train_time:92226ms step_avg:53.74ms
step:1717/2160 train_time:92314ms step_avg:53.76ms
step:1718/2160 train_time:92402ms step_avg:53.78ms
step:1719/2160 train_time:92489ms step_avg:53.80ms
step:1720/2160 train_time:92576ms step_avg:53.82ms
step:1721/2160 train_time:92665ms step_avg:53.84ms
step:1722/2160 train_time:92752ms step_avg:53.86ms
step:1723/2160 train_time:92841ms step_avg:53.88ms
step:1724/2160 train_time:92928ms step_avg:53.90ms
step:1725/2160 train_time:93016ms step_avg:53.92ms
step:1726/2160 train_time:93103ms step_avg:53.94ms
step:1727/2160 train_time:93192ms step_avg:53.96ms
step:1728/2160 train_time:93280ms step_avg:53.98ms
step:1729/2160 train_time:93368ms step_avg:54.00ms
step:1730/2160 train_time:93455ms step_avg:54.02ms
step:1731/2160 train_time:93545ms step_avg:54.04ms
step:1732/2160 train_time:93632ms step_avg:54.06ms
step:1733/2160 train_time:93722ms step_avg:54.08ms
step:1734/2160 train_time:93809ms step_avg:54.10ms
step:1735/2160 train_time:93898ms step_avg:54.12ms
step:1736/2160 train_time:93984ms step_avg:54.14ms
step:1737/2160 train_time:94072ms step_avg:54.16ms
step:1738/2160 train_time:94159ms step_avg:54.18ms
step:1739/2160 train_time:94247ms step_avg:54.20ms
step:1740/2160 train_time:94334ms step_avg:54.21ms
step:1741/2160 train_time:94423ms step_avg:54.24ms
step:1742/2160 train_time:94510ms step_avg:54.25ms
step:1743/2160 train_time:94599ms step_avg:54.27ms
step:1744/2160 train_time:94685ms step_avg:54.29ms
step:1745/2160 train_time:94774ms step_avg:54.31ms
step:1746/2160 train_time:94861ms step_avg:54.33ms
step:1747/2160 train_time:94950ms step_avg:54.35ms
step:1748/2160 train_time:95037ms step_avg:54.37ms
step:1749/2160 train_time:95125ms step_avg:54.39ms
step:1750/2160 train_time:95211ms step_avg:54.41ms
step:1750/2160 val_loss:3.3927 train_time:95302ms step_avg:54.46ms
step:1751/2160 train_time:95324ms step_avg:54.44ms
step:1752/2160 train_time:95392ms step_avg:54.45ms
step:1753/2160 train_time:95485ms step_avg:54.47ms
step:1754/2160 train_time:95572ms step_avg:54.49ms
step:1755/2160 train_time:95661ms step_avg:54.51ms
step:1756/2160 train_time:95747ms step_avg:54.53ms
step:1757/2160 train_time:95834ms step_avg:54.54ms
step:1758/2160 train_time:95920ms step_avg:54.56ms
step:1759/2160 train_time:96007ms step_avg:54.58ms
step:1760/2160 train_time:96093ms step_avg:54.60ms
step:1761/2160 train_time:96180ms step_avg:54.62ms
step:1762/2160 train_time:96270ms step_avg:54.64ms
step:1763/2160 train_time:96362ms step_avg:54.66ms
step:1764/2160 train_time:96452ms step_avg:54.68ms
step:1765/2160 train_time:96542ms step_avg:54.70ms
step:1766/2160 train_time:96629ms step_avg:54.72ms
step:1767/2160 train_time:96717ms step_avg:54.74ms
step:1768/2160 train_time:96803ms step_avg:54.75ms
step:1769/2160 train_time:96891ms step_avg:54.77ms
step:1770/2160 train_time:96976ms step_avg:54.79ms
step:1771/2160 train_time:97065ms step_avg:54.81ms
step:1772/2160 train_time:97151ms step_avg:54.83ms
step:1773/2160 train_time:97239ms step_avg:54.84ms
step:1774/2160 train_time:97328ms step_avg:54.86ms
step:1775/2160 train_time:97416ms step_avg:54.88ms
step:1776/2160 train_time:97504ms step_avg:54.90ms
step:1777/2160 train_time:97593ms step_avg:54.92ms
step:1778/2160 train_time:97680ms step_avg:54.94ms
step:1779/2160 train_time:97768ms step_avg:54.96ms
step:1780/2160 train_time:97854ms step_avg:54.97ms
step:1781/2160 train_time:97942ms step_avg:54.99ms
step:1782/2160 train_time:98028ms step_avg:55.01ms
step:1783/2160 train_time:98115ms step_avg:55.03ms
step:1784/2160 train_time:98202ms step_avg:55.05ms
step:1785/2160 train_time:98293ms step_avg:55.07ms
step:1786/2160 train_time:98381ms step_avg:55.08ms
step:1787/2160 train_time:98473ms step_avg:55.11ms
step:1788/2160 train_time:98560ms step_avg:55.12ms
step:1789/2160 train_time:98649ms step_avg:55.14ms
step:1790/2160 train_time:98735ms step_avg:55.16ms
step:1791/2160 train_time:98823ms step_avg:55.18ms
step:1792/2160 train_time:98909ms step_avg:55.19ms
step:1793/2160 train_time:98997ms step_avg:55.21ms
step:1794/2160 train_time:99083ms step_avg:55.23ms
step:1795/2160 train_time:99172ms step_avg:55.25ms
step:1796/2160 train_time:99259ms step_avg:55.27ms
step:1797/2160 train_time:99349ms step_avg:55.29ms
step:1798/2160 train_time:99436ms step_avg:55.30ms
step:1799/2160 train_time:99525ms step_avg:55.32ms
step:1800/2160 train_time:99612ms step_avg:55.34ms
step:1801/2160 train_time:99700ms step_avg:55.36ms
step:1802/2160 train_time:99787ms step_avg:55.38ms
step:1803/2160 train_time:99874ms step_avg:55.39ms
step:1804/2160 train_time:99960ms step_avg:55.41ms
step:1805/2160 train_time:100048ms step_avg:55.43ms
step:1806/2160 train_time:100134ms step_avg:55.45ms
step:1807/2160 train_time:100223ms step_avg:55.46ms
step:1808/2160 train_time:100311ms step_avg:55.48ms
step:1809/2160 train_time:100400ms step_avg:55.50ms
step:1810/2160 train_time:100488ms step_avg:55.52ms
step:1811/2160 train_time:100576ms step_avg:55.54ms
step:1812/2160 train_time:100664ms step_avg:55.55ms
step:1813/2160 train_time:100752ms step_avg:55.57ms
step:1814/2160 train_time:100839ms step_avg:55.59ms
step:1815/2160 train_time:100927ms step_avg:55.61ms
step:1816/2160 train_time:101013ms step_avg:55.62ms
step:1817/2160 train_time:101100ms step_avg:55.64ms
step:1818/2160 train_time:101188ms step_avg:55.66ms
step:1819/2160 train_time:101275ms step_avg:55.68ms
step:1820/2160 train_time:101363ms step_avg:55.69ms
step:1821/2160 train_time:101452ms step_avg:55.71ms
step:1822/2160 train_time:101539ms step_avg:55.73ms
step:1823/2160 train_time:101628ms step_avg:55.75ms
step:1824/2160 train_time:101714ms step_avg:55.76ms
step:1825/2160 train_time:101801ms step_avg:55.78ms
step:1826/2160 train_time:101888ms step_avg:55.80ms
step:1827/2160 train_time:101976ms step_avg:55.82ms
step:1828/2160 train_time:102063ms step_avg:55.83ms
step:1829/2160 train_time:102152ms step_avg:55.85ms
step:1830/2160 train_time:102240ms step_avg:55.87ms
step:1831/2160 train_time:102329ms step_avg:55.89ms
step:1832/2160 train_time:102414ms step_avg:55.90ms
step:1833/2160 train_time:102504ms step_avg:55.92ms
step:1834/2160 train_time:102591ms step_avg:55.94ms
step:1835/2160 train_time:102680ms step_avg:55.96ms
step:1836/2160 train_time:102767ms step_avg:55.97ms
step:1837/2160 train_time:102854ms step_avg:55.99ms
step:1838/2160 train_time:102941ms step_avg:56.01ms
step:1839/2160 train_time:103030ms step_avg:56.02ms
step:1840/2160 train_time:103115ms step_avg:56.04ms
step:1841/2160 train_time:103204ms step_avg:56.06ms
step:1842/2160 train_time:103292ms step_avg:56.08ms
step:1843/2160 train_time:103381ms step_avg:56.09ms
step:1844/2160 train_time:103468ms step_avg:56.11ms
step:1845/2160 train_time:103557ms step_avg:56.13ms
step:1846/2160 train_time:103644ms step_avg:56.14ms
step:1847/2160 train_time:103732ms step_avg:56.16ms
step:1848/2160 train_time:103819ms step_avg:56.18ms
step:1849/2160 train_time:103907ms step_avg:56.20ms
step:1850/2160 train_time:103993ms step_avg:56.21ms
step:1851/2160 train_time:104082ms step_avg:56.23ms
step:1852/2160 train_time:104168ms step_avg:56.25ms
step:1853/2160 train_time:104256ms step_avg:56.26ms
step:1854/2160 train_time:104344ms step_avg:56.28ms
step:1855/2160 train_time:104433ms step_avg:56.30ms
step:1856/2160 train_time:104520ms step_avg:56.31ms
step:1857/2160 train_time:104610ms step_avg:56.33ms
step:1858/2160 train_time:104696ms step_avg:56.35ms
step:1859/2160 train_time:104783ms step_avg:56.37ms
step:1860/2160 train_time:104871ms step_avg:56.38ms
step:1861/2160 train_time:104959ms step_avg:56.40ms
step:1862/2160 train_time:105046ms step_avg:56.42ms
step:1863/2160 train_time:105134ms step_avg:56.43ms
step:1864/2160 train_time:105221ms step_avg:56.45ms
step:1865/2160 train_time:105310ms step_avg:56.47ms
step:1866/2160 train_time:105395ms step_avg:56.48ms
step:1867/2160 train_time:105484ms step_avg:56.50ms
step:1868/2160 train_time:105572ms step_avg:56.52ms
step:1869/2160 train_time:105661ms step_avg:56.53ms
step:1870/2160 train_time:105747ms step_avg:56.55ms
step:1871/2160 train_time:105835ms step_avg:56.57ms
step:1872/2160 train_time:105922ms step_avg:56.58ms
step:1873/2160 train_time:106012ms step_avg:56.60ms
step:1874/2160 train_time:106099ms step_avg:56.62ms
step:1875/2160 train_time:106187ms step_avg:56.63ms
step:1876/2160 train_time:106273ms step_avg:56.65ms
step:1877/2160 train_time:106362ms step_avg:56.67ms
step:1878/2160 train_time:106448ms step_avg:56.68ms
step:1879/2160 train_time:106537ms step_avg:56.70ms
step:1880/2160 train_time:106624ms step_avg:56.72ms
step:1881/2160 train_time:106714ms step_avg:56.73ms
step:1882/2160 train_time:106800ms step_avg:56.75ms
step:1883/2160 train_time:106889ms step_avg:56.77ms
step:1884/2160 train_time:106975ms step_avg:56.78ms
step:1885/2160 train_time:107064ms step_avg:56.80ms
step:1886/2160 train_time:107151ms step_avg:56.81ms
step:1887/2160 train_time:107238ms step_avg:56.83ms
step:1888/2160 train_time:107325ms step_avg:56.85ms
step:1889/2160 train_time:107414ms step_avg:56.86ms
step:1890/2160 train_time:107502ms step_avg:56.88ms
step:1891/2160 train_time:107591ms step_avg:56.90ms
step:1892/2160 train_time:107677ms step_avg:56.91ms
step:1893/2160 train_time:107765ms step_avg:56.93ms
step:1894/2160 train_time:107852ms step_avg:56.94ms
step:1895/2160 train_time:107940ms step_avg:56.96ms
step:1896/2160 train_time:108027ms step_avg:56.98ms
step:1897/2160 train_time:108115ms step_avg:56.99ms
step:1898/2160 train_time:108202ms step_avg:57.01ms
step:1899/2160 train_time:108291ms step_avg:57.03ms
step:1900/2160 train_time:108377ms step_avg:57.04ms
step:1901/2160 train_time:108467ms step_avg:57.06ms
step:1902/2160 train_time:108553ms step_avg:57.07ms
step:1903/2160 train_time:108641ms step_avg:57.09ms
step:1904/2160 train_time:108728ms step_avg:57.11ms
step:1905/2160 train_time:108815ms step_avg:57.12ms
step:1906/2160 train_time:108902ms step_avg:57.14ms
step:1907/2160 train_time:108992ms step_avg:57.15ms
step:1908/2160 train_time:109079ms step_avg:57.17ms
step:1909/2160 train_time:109168ms step_avg:57.19ms
step:1910/2160 train_time:109254ms step_avg:57.20ms
step:1911/2160 train_time:109342ms step_avg:57.22ms
step:1912/2160 train_time:109430ms step_avg:57.23ms
step:1913/2160 train_time:109518ms step_avg:57.25ms
step:1914/2160 train_time:109605ms step_avg:57.26ms
step:1915/2160 train_time:109694ms step_avg:57.28ms
step:1916/2160 train_time:109781ms step_avg:57.30ms
step:1917/2160 train_time:109870ms step_avg:57.31ms
step:1918/2160 train_time:109958ms step_avg:57.33ms
step:1919/2160 train_time:110046ms step_avg:57.35ms
step:1920/2160 train_time:110132ms step_avg:57.36ms
step:1921/2160 train_time:110220ms step_avg:57.38ms
step:1922/2160 train_time:110306ms step_avg:57.39ms
step:1923/2160 train_time:110396ms step_avg:57.41ms
step:1924/2160 train_time:110482ms step_avg:57.42ms
step:1925/2160 train_time:110571ms step_avg:57.44ms
step:1926/2160 train_time:110657ms step_avg:57.45ms
step:1927/2160 train_time:110746ms step_avg:57.47ms
step:1928/2160 train_time:110832ms step_avg:57.49ms
step:1929/2160 train_time:110920ms step_avg:57.50ms
step:1930/2160 train_time:111008ms step_avg:57.52ms
step:1931/2160 train_time:111096ms step_avg:57.53ms
step:1932/2160 train_time:111183ms step_avg:57.55ms
step:1933/2160 train_time:111272ms step_avg:57.56ms
step:1934/2160 train_time:111359ms step_avg:57.58ms
step:1935/2160 train_time:111447ms step_avg:57.60ms
step:1936/2160 train_time:111534ms step_avg:57.61ms
step:1937/2160 train_time:111622ms step_avg:57.63ms
step:1938/2160 train_time:111709ms step_avg:57.64ms
step:1939/2160 train_time:111797ms step_avg:57.66ms
step:1940/2160 train_time:111884ms step_avg:57.67ms
step:1941/2160 train_time:111973ms step_avg:57.69ms
step:1942/2160 train_time:112060ms step_avg:57.70ms
step:1943/2160 train_time:112149ms step_avg:57.72ms
step:1944/2160 train_time:112235ms step_avg:57.73ms
step:1945/2160 train_time:112324ms step_avg:57.75ms
step:1946/2160 train_time:112411ms step_avg:57.77ms
step:1947/2160 train_time:112499ms step_avg:57.78ms
step:1948/2160 train_time:112586ms step_avg:57.80ms
step:1949/2160 train_time:112674ms step_avg:57.81ms
step:1950/2160 train_time:112761ms step_avg:57.83ms
step:1951/2160 train_time:112850ms step_avg:57.84ms
step:1952/2160 train_time:112936ms step_avg:57.86ms
step:1953/2160 train_time:113024ms step_avg:57.87ms
step:1954/2160 train_time:113111ms step_avg:57.89ms
step:1955/2160 train_time:113198ms step_avg:57.90ms
step:1956/2160 train_time:113285ms step_avg:57.92ms
step:1957/2160 train_time:113373ms step_avg:57.93ms
step:1958/2160 train_time:113462ms step_avg:57.95ms
step:1959/2160 train_time:113551ms step_avg:57.96ms
step:1960/2160 train_time:113636ms step_avg:57.98ms
step:1961/2160 train_time:113724ms step_avg:57.99ms
step:1962/2160 train_time:113811ms step_avg:58.01ms
step:1963/2160 train_time:113900ms step_avg:58.02ms
step:1964/2160 train_time:113987ms step_avg:58.04ms
step:1965/2160 train_time:114075ms step_avg:58.05ms
step:1966/2160 train_time:114161ms step_avg:58.07ms
step:1967/2160 train_time:114249ms step_avg:58.08ms
step:1968/2160 train_time:114335ms step_avg:58.10ms
step:1969/2160 train_time:114424ms step_avg:58.11ms
step:1970/2160 train_time:114511ms step_avg:58.13ms
step:1971/2160 train_time:114600ms step_avg:58.14ms
step:1972/2160 train_time:114687ms step_avg:58.16ms
step:1973/2160 train_time:114775ms step_avg:58.17ms
step:1974/2160 train_time:114861ms step_avg:58.19ms
step:1975/2160 train_time:114951ms step_avg:58.20ms
step:1976/2160 train_time:115036ms step_avg:58.22ms
step:1977/2160 train_time:115124ms step_avg:58.23ms
step:1978/2160 train_time:115211ms step_avg:58.25ms
step:1979/2160 train_time:115299ms step_avg:58.26ms
step:1980/2160 train_time:115386ms step_avg:58.28ms
step:1981/2160 train_time:115476ms step_avg:58.29ms
step:1982/2160 train_time:115563ms step_avg:58.31ms
step:1983/2160 train_time:115651ms step_avg:58.32ms
step:1984/2160 train_time:115738ms step_avg:58.34ms
step:1985/2160 train_time:115827ms step_avg:58.35ms
step:1986/2160 train_time:115913ms step_avg:58.37ms
step:1987/2160 train_time:116001ms step_avg:58.38ms
step:1988/2160 train_time:116089ms step_avg:58.39ms
step:1989/2160 train_time:116177ms step_avg:58.41ms
step:1990/2160 train_time:116264ms step_avg:58.42ms
step:1991/2160 train_time:116352ms step_avg:58.44ms
step:1992/2160 train_time:116438ms step_avg:58.45ms
step:1993/2160 train_time:116527ms step_avg:58.47ms
step:1994/2160 train_time:116613ms step_avg:58.48ms
step:1995/2160 train_time:116700ms step_avg:58.50ms
step:1996/2160 train_time:116787ms step_avg:58.51ms
step:1997/2160 train_time:116875ms step_avg:58.53ms
step:1998/2160 train_time:116962ms step_avg:58.54ms
step:1999/2160 train_time:117052ms step_avg:58.56ms
step:2000/2160 train_time:117138ms step_avg:58.57ms
step:2000/2160 val_loss:3.3156 train_time:117228ms step_avg:58.61ms
step:2001/2160 train_time:117251ms step_avg:58.60ms
step:2002/2160 train_time:117318ms step_avg:58.60ms
step:2003/2160 train_time:117411ms step_avg:58.62ms
step:2004/2160 train_time:117499ms step_avg:58.63ms
step:2005/2160 train_time:117586ms step_avg:58.65ms
step:2006/2160 train_time:117671ms step_avg:58.66ms
step:2007/2160 train_time:117759ms step_avg:58.67ms
step:2008/2160 train_time:117845ms step_avg:58.69ms
step:2009/2160 train_time:117932ms step_avg:58.70ms
step:2010/2160 train_time:118017ms step_avg:58.72ms
step:2011/2160 train_time:118105ms step_avg:58.73ms
step:2012/2160 train_time:118192ms step_avg:58.74ms
step:2013/2160 train_time:118284ms step_avg:58.76ms
step:2014/2160 train_time:118372ms step_avg:58.77ms
step:2015/2160 train_time:118462ms step_avg:58.79ms
step:2016/2160 train_time:118549ms step_avg:58.80ms
step:2017/2160 train_time:118636ms step_avg:58.82ms
step:2018/2160 train_time:118723ms step_avg:58.83ms
step:2019/2160 train_time:118809ms step_avg:58.85ms
step:2020/2160 train_time:118895ms step_avg:58.86ms
step:2021/2160 train_time:118984ms step_avg:58.87ms
step:2022/2160 train_time:119069ms step_avg:58.89ms
step:2023/2160 train_time:119158ms step_avg:58.90ms
step:2024/2160 train_time:119246ms step_avg:58.92ms
step:2025/2160 train_time:119338ms step_avg:58.93ms
step:2026/2160 train_time:119425ms step_avg:58.95ms
step:2027/2160 train_time:119514ms step_avg:58.96ms
step:2028/2160 train_time:119601ms step_avg:58.97ms
step:2029/2160 train_time:119689ms step_avg:58.99ms
step:2030/2160 train_time:119775ms step_avg:59.00ms
step:2031/2160 train_time:119864ms step_avg:59.02ms
step:2032/2160 train_time:119950ms step_avg:59.03ms
step:2033/2160 train_time:120037ms step_avg:59.04ms
step:2034/2160 train_time:120124ms step_avg:59.06ms
step:2035/2160 train_time:120213ms step_avg:59.07ms
step:2036/2160 train_time:120302ms step_avg:59.09ms
step:2037/2160 train_time:120390ms step_avg:59.10ms
step:2038/2160 train_time:120477ms step_avg:59.12ms
step:2039/2160 train_time:120567ms step_avg:59.13ms
step:2040/2160 train_time:120653ms step_avg:59.14ms
step:2041/2160 train_time:120740ms step_avg:59.16ms
step:2042/2160 train_time:120826ms step_avg:59.17ms
step:2043/2160 train_time:120914ms step_avg:59.18ms
step:2044/2160 train_time:121000ms step_avg:59.20ms
step:2045/2160 train_time:121088ms step_avg:59.21ms
step:2046/2160 train_time:121175ms step_avg:59.23ms
step:2047/2160 train_time:121265ms step_avg:59.24ms
step:2048/2160 train_time:121353ms step_avg:59.25ms
step:2049/2160 train_time:121443ms step_avg:59.27ms
step:2050/2160 train_time:121530ms step_avg:59.28ms
step:2051/2160 train_time:121619ms step_avg:59.30ms
step:2052/2160 train_time:121705ms step_avg:59.31ms
step:2053/2160 train_time:121792ms step_avg:59.32ms
step:2054/2160 train_time:121878ms step_avg:59.34ms
step:2055/2160 train_time:121966ms step_avg:59.35ms
step:2056/2160 train_time:122052ms step_avg:59.36ms
step:2057/2160 train_time:122141ms step_avg:59.38ms
step:2058/2160 train_time:122227ms step_avg:59.39ms
step:2059/2160 train_time:122316ms step_avg:59.41ms
step:2060/2160 train_time:122403ms step_avg:59.42ms
step:2061/2160 train_time:122491ms step_avg:59.43ms
step:2062/2160 train_time:122578ms step_avg:59.45ms
step:2063/2160 train_time:122667ms step_avg:59.46ms
step:2064/2160 train_time:122753ms step_avg:59.47ms
step:2065/2160 train_time:122841ms step_avg:59.49ms
step:2066/2160 train_time:122927ms step_avg:59.50ms
step:2067/2160 train_time:123015ms step_avg:59.51ms
step:2068/2160 train_time:123101ms step_avg:59.53ms
step:2069/2160 train_time:123189ms step_avg:59.54ms
step:2070/2160 train_time:123276ms step_avg:59.55ms
step:2071/2160 train_time:123366ms step_avg:59.57ms
step:2072/2160 train_time:123453ms step_avg:59.58ms
step:2073/2160 train_time:123542ms step_avg:59.60ms
step:2074/2160 train_time:123628ms step_avg:59.61ms
step:2075/2160 train_time:123718ms step_avg:59.62ms
step:2076/2160 train_time:123805ms step_avg:59.64ms
step:2077/2160 train_time:123893ms step_avg:59.65ms
step:2078/2160 train_time:123980ms step_avg:59.66ms
step:2079/2160 train_time:124067ms step_avg:59.68ms
step:2080/2160 train_time:124154ms step_avg:59.69ms
step:2081/2160 train_time:124243ms step_avg:59.70ms
step:2082/2160 train_time:124329ms step_avg:59.72ms
step:2083/2160 train_time:124419ms step_avg:59.73ms
step:2084/2160 train_time:124505ms step_avg:59.74ms
step:2085/2160 train_time:124593ms step_avg:59.76ms
step:2086/2160 train_time:124680ms step_avg:59.77ms
step:2087/2160 train_time:124768ms step_avg:59.78ms
step:2088/2160 train_time:124854ms step_avg:59.80ms
step:2089/2160 train_time:124944ms step_avg:59.81ms
step:2090/2160 train_time:125029ms step_avg:59.82ms
step:2091/2160 train_time:125117ms step_avg:59.84ms
step:2092/2160 train_time:125205ms step_avg:59.85ms
step:2093/2160 train_time:125293ms step_avg:59.86ms
step:2094/2160 train_time:125381ms step_avg:59.88ms
step:2095/2160 train_time:125469ms step_avg:59.89ms
step:2096/2160 train_time:125555ms step_avg:59.90ms
step:2097/2160 train_time:125644ms step_avg:59.92ms
step:2098/2160 train_time:125729ms step_avg:59.93ms
step:2099/2160 train_time:125817ms step_avg:59.94ms
step:2100/2160 train_time:125904ms step_avg:59.95ms
step:2101/2160 train_time:125992ms step_avg:59.97ms
step:2102/2160 train_time:126079ms step_avg:59.98ms
step:2103/2160 train_time:126168ms step_avg:59.99ms
step:2104/2160 train_time:126254ms step_avg:60.01ms
step:2105/2160 train_time:126345ms step_avg:60.02ms
step:2106/2160 train_time:126430ms step_avg:60.03ms
step:2107/2160 train_time:126518ms step_avg:60.05ms
step:2108/2160 train_time:126606ms step_avg:60.06ms
step:2109/2160 train_time:126694ms step_avg:60.07ms
step:2110/2160 train_time:126780ms step_avg:60.09ms
step:2111/2160 train_time:126868ms step_avg:60.10ms
step:2112/2160 train_time:126954ms step_avg:60.11ms
step:2113/2160 train_time:127043ms step_avg:60.12ms
step:2114/2160 train_time:127129ms step_avg:60.14ms
step:2115/2160 train_time:127217ms step_avg:60.15ms
step:2116/2160 train_time:127304ms step_avg:60.16ms
step:2117/2160 train_time:127392ms step_avg:60.18ms
step:2118/2160 train_time:127479ms step_avg:60.19ms
step:2119/2160 train_time:127567ms step_avg:60.20ms
step:2120/2160 train_time:127654ms step_avg:60.21ms
step:2121/2160 train_time:127742ms step_avg:60.23ms
step:2122/2160 train_time:127828ms step_avg:60.24ms
step:2123/2160 train_time:127917ms step_avg:60.25ms
step:2124/2160 train_time:128004ms step_avg:60.27ms
step:2125/2160 train_time:128092ms step_avg:60.28ms
step:2126/2160 train_time:128179ms step_avg:60.29ms
step:2127/2160 train_time:128268ms step_avg:60.30ms
step:2128/2160 train_time:128355ms step_avg:60.32ms
step:2129/2160 train_time:128445ms step_avg:60.33ms
step:2130/2160 train_time:128531ms step_avg:60.34ms
step:2131/2160 train_time:128621ms step_avg:60.36ms
step:2132/2160 train_time:128707ms step_avg:60.37ms
step:2133/2160 train_time:128795ms step_avg:60.38ms
step:2134/2160 train_time:128883ms step_avg:60.39ms
step:2135/2160 train_time:128971ms step_avg:60.41ms
step:2136/2160 train_time:129059ms step_avg:60.42ms
step:2137/2160 train_time:129147ms step_avg:60.43ms
step:2138/2160 train_time:129234ms step_avg:60.45ms
step:2139/2160 train_time:129322ms step_avg:60.46ms
step:2140/2160 train_time:129409ms step_avg:60.47ms
step:2141/2160 train_time:129498ms step_avg:60.48ms
step:2142/2160 train_time:129585ms step_avg:60.50ms
step:2143/2160 train_time:129673ms step_avg:60.51ms
step:2144/2160 train_time:129760ms step_avg:60.52ms
step:2145/2160 train_time:129849ms step_avg:60.54ms
step:2146/2160 train_time:129936ms step_avg:60.55ms
step:2147/2160 train_time:130025ms step_avg:60.56ms
step:2148/2160 train_time:130111ms step_avg:60.57ms
step:2149/2160 train_time:130200ms step_avg:60.59ms
step:2150/2160 train_time:130286ms step_avg:60.60ms
step:2151/2160 train_time:130374ms step_avg:60.61ms
step:2152/2160 train_time:130461ms step_avg:60.62ms
step:2153/2160 train_time:130550ms step_avg:60.64ms
step:2154/2160 train_time:130636ms step_avg:60.65ms
step:2155/2160 train_time:130726ms step_avg:60.66ms
step:2156/2160 train_time:130812ms step_avg:60.67ms
step:2157/2160 train_time:130901ms step_avg:60.69ms
step:2158/2160 train_time:130987ms step_avg:60.70ms
step:2159/2160 train_time:131076ms step_avg:60.71ms
step:2160/2160 train_time:131164ms step_avg:60.72ms
step:2160/2160 val_loss:3.2793 train_time:131254ms step_avg:60.77ms
peak memory allocated: 29862 MiB reserved: 44796 MiB

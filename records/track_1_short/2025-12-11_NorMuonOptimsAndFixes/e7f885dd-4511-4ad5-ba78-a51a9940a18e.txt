import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 08:21:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   35C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              75      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A              76      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              77      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              78      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              79      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              80      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A              76      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A              77      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A              78      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A              79      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A              80      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A              81      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A              82      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:105ms step_avg:105.46ms
step:2/2160 train_time:132ms step_avg:65.99ms
step:3/2160 train_time:155ms step_avg:51.76ms
step:4/2160 train_time:180ms step_avg:44.98ms
step:5/2160 train_time:206ms step_avg:41.28ms
step:6/2160 train_time:326ms step_avg:54.36ms
step:7/2160 train_time:365ms step_avg:52.09ms
step:8/2160 train_time:398ms step_avg:49.71ms
step:9/2160 train_time:431ms step_avg:47.88ms
step:10/2160 train_time:464ms step_avg:46.40ms
step:11/2160 train_time:498ms step_avg:45.25ms
step:12/2160 train_time:531ms step_avg:44.25ms
step:13/2160 train_time:565ms step_avg:43.45ms
step:14/2160 train_time:598ms step_avg:42.73ms
step:15/2160 train_time:632ms step_avg:42.12ms
step:16/2160 train_time:665ms step_avg:41.57ms
step:17/2160 train_time:699ms step_avg:41.11ms
step:18/2160 train_time:732ms step_avg:40.67ms
step:19/2160 train_time:766ms step_avg:40.30ms
step:20/2160 train_time:799ms step_avg:39.95ms
step:21/2160 train_time:833ms step_avg:39.65ms
step:22/2160 train_time:866ms step_avg:39.37ms
step:23/2160 train_time:900ms step_avg:39.13ms
step:24/2160 train_time:934ms step_avg:38.90ms
step:25/2160 train_time:967ms step_avg:38.68ms
step:26/2160 train_time:1001ms step_avg:38.48ms
step:27/2160 train_time:1034ms step_avg:38.30ms
step:28/2160 train_time:1067ms step_avg:38.12ms
step:29/2160 train_time:1101ms step_avg:37.98ms
step:30/2160 train_time:1135ms step_avg:37.83ms
step:31/2160 train_time:1169ms step_avg:37.70ms
step:32/2160 train_time:1202ms step_avg:37.56ms
step:33/2160 train_time:1236ms step_avg:37.45ms
step:34/2160 train_time:1269ms step_avg:37.34ms
step:35/2160 train_time:1305ms step_avg:37.29ms
step:36/2160 train_time:1339ms step_avg:37.19ms
step:37/2160 train_time:1373ms step_avg:37.10ms
step:38/2160 train_time:1406ms step_avg:37.01ms
step:39/2160 train_time:1441ms step_avg:36.94ms
step:40/2160 train_time:1474ms step_avg:36.85ms
step:41/2160 train_time:1508ms step_avg:36.79ms
step:42/2160 train_time:1542ms step_avg:36.71ms
step:43/2160 train_time:1576ms step_avg:36.65ms
step:44/2160 train_time:1609ms step_avg:36.57ms
step:45/2160 train_time:1643ms step_avg:36.51ms
step:46/2160 train_time:1677ms step_avg:36.45ms
step:47/2160 train_time:1710ms step_avg:36.39ms
step:48/2160 train_time:1744ms step_avg:36.33ms
step:49/2160 train_time:1777ms step_avg:36.27ms
step:50/2160 train_time:1811ms step_avg:36.21ms
step:51/2160 train_time:1844ms step_avg:36.16ms
step:52/2160 train_time:1877ms step_avg:36.11ms
step:53/2160 train_time:1911ms step_avg:36.06ms
step:54/2160 train_time:1945ms step_avg:36.02ms
step:55/2160 train_time:1979ms step_avg:35.98ms
step:56/2160 train_time:2012ms step_avg:35.93ms
step:57/2160 train_time:2046ms step_avg:35.89ms
step:58/2160 train_time:2079ms step_avg:35.85ms
step:59/2160 train_time:2113ms step_avg:35.82ms
step:60/2160 train_time:2147ms step_avg:35.78ms
step:61/2160 train_time:2180ms step_avg:35.74ms
step:62/2160 train_time:2214ms step_avg:35.70ms
step:63/2160 train_time:2247ms step_avg:35.67ms
step:64/2160 train_time:2281ms step_avg:35.64ms
step:65/2160 train_time:2315ms step_avg:35.62ms
step:66/2160 train_time:2349ms step_avg:35.59ms
step:67/2160 train_time:2383ms step_avg:35.57ms
step:68/2160 train_time:2416ms step_avg:35.53ms
step:69/2160 train_time:2451ms step_avg:35.52ms
step:70/2160 train_time:2484ms step_avg:35.49ms
step:71/2160 train_time:2519ms step_avg:35.48ms
step:72/2160 train_time:2552ms step_avg:35.45ms
step:73/2160 train_time:2587ms step_avg:35.43ms
step:74/2160 train_time:2620ms step_avg:35.40ms
step:75/2160 train_time:2654ms step_avg:35.38ms
step:76/2160 train_time:2687ms step_avg:35.35ms
step:77/2160 train_time:2721ms step_avg:35.34ms
step:78/2160 train_time:2754ms step_avg:35.31ms
step:79/2160 train_time:2788ms step_avg:35.29ms
step:80/2160 train_time:2821ms step_avg:35.27ms
step:81/2160 train_time:2855ms step_avg:35.25ms
step:82/2160 train_time:2888ms step_avg:35.22ms
step:83/2160 train_time:2922ms step_avg:35.21ms
step:84/2160 train_time:2956ms step_avg:35.19ms
step:85/2160 train_time:2989ms step_avg:35.17ms
step:86/2160 train_time:3022ms step_avg:35.14ms
step:87/2160 train_time:3056ms step_avg:35.13ms
step:88/2160 train_time:3089ms step_avg:35.11ms
step:89/2160 train_time:3123ms step_avg:35.09ms
step:90/2160 train_time:3157ms step_avg:35.08ms
step:91/2160 train_time:3190ms step_avg:35.06ms
step:92/2160 train_time:3224ms step_avg:35.04ms
step:93/2160 train_time:3257ms step_avg:35.03ms
step:94/2160 train_time:3290ms step_avg:35.01ms
step:95/2160 train_time:3325ms step_avg:35.00ms
step:96/2160 train_time:3358ms step_avg:34.98ms
step:97/2160 train_time:3392ms step_avg:34.97ms
step:98/2160 train_time:3426ms step_avg:34.96ms
step:99/2160 train_time:3459ms step_avg:34.94ms
step:100/2160 train_time:3493ms step_avg:34.93ms
step:101/2160 train_time:3527ms step_avg:34.92ms
step:102/2160 train_time:3560ms step_avg:34.90ms
step:103/2160 train_time:3594ms step_avg:34.89ms
step:104/2160 train_time:3628ms step_avg:34.88ms
step:105/2160 train_time:3662ms step_avg:34.88ms
step:106/2160 train_time:3695ms step_avg:34.86ms
step:107/2160 train_time:3729ms step_avg:34.85ms
step:108/2160 train_time:3762ms step_avg:34.83ms
step:109/2160 train_time:3796ms step_avg:34.83ms
step:110/2160 train_time:3830ms step_avg:34.82ms
step:111/2160 train_time:3864ms step_avg:34.81ms
step:112/2160 train_time:3897ms step_avg:34.79ms
step:113/2160 train_time:3931ms step_avg:34.79ms
step:114/2160 train_time:3964ms step_avg:34.77ms
step:115/2160 train_time:3998ms step_avg:34.77ms
step:116/2160 train_time:4031ms step_avg:34.75ms
step:117/2160 train_time:4065ms step_avg:34.74ms
step:118/2160 train_time:4098ms step_avg:34.73ms
step:119/2160 train_time:4132ms step_avg:34.72ms
step:120/2160 train_time:4165ms step_avg:34.71ms
step:121/2160 train_time:4199ms step_avg:34.70ms
step:122/2160 train_time:4233ms step_avg:34.69ms
step:123/2160 train_time:4267ms step_avg:34.69ms
step:124/2160 train_time:4300ms step_avg:34.68ms
step:125/2160 train_time:4334ms step_avg:34.67ms
step:126/2160 train_time:4367ms step_avg:34.66ms
step:127/2160 train_time:4402ms step_avg:34.66ms
step:128/2160 train_time:4435ms step_avg:34.65ms
step:129/2160 train_time:4469ms step_avg:34.64ms
step:130/2160 train_time:4502ms step_avg:34.63ms
step:131/2160 train_time:4536ms step_avg:34.62ms
step:132/2160 train_time:4569ms step_avg:34.61ms
step:133/2160 train_time:4603ms step_avg:34.61ms
step:134/2160 train_time:4637ms step_avg:34.60ms
step:135/2160 train_time:4670ms step_avg:34.59ms
step:136/2160 train_time:4703ms step_avg:34.58ms
step:137/2160 train_time:4738ms step_avg:34.58ms
step:138/2160 train_time:4771ms step_avg:34.57ms
step:139/2160 train_time:4805ms step_avg:34.56ms
step:140/2160 train_time:4839ms step_avg:34.56ms
step:141/2160 train_time:4872ms step_avg:34.56ms
step:142/2160 train_time:4906ms step_avg:34.55ms
step:143/2160 train_time:4940ms step_avg:34.54ms
step:144/2160 train_time:4973ms step_avg:34.53ms
step:145/2160 train_time:5007ms step_avg:34.53ms
step:146/2160 train_time:5040ms step_avg:34.52ms
step:147/2160 train_time:5074ms step_avg:34.52ms
step:148/2160 train_time:5107ms step_avg:34.51ms
step:149/2160 train_time:5142ms step_avg:34.51ms
step:150/2160 train_time:5174ms step_avg:34.50ms
step:151/2160 train_time:5208ms step_avg:34.49ms
step:152/2160 train_time:5242ms step_avg:34.48ms
step:153/2160 train_time:5275ms step_avg:34.48ms
step:154/2160 train_time:5308ms step_avg:34.47ms
step:155/2160 train_time:5342ms step_avg:34.47ms
step:156/2160 train_time:5375ms step_avg:34.46ms
step:157/2160 train_time:5409ms step_avg:34.45ms
step:158/2160 train_time:5442ms step_avg:34.44ms
step:159/2160 train_time:5476ms step_avg:34.44ms
step:160/2160 train_time:5509ms step_avg:34.43ms
step:161/2160 train_time:5543ms step_avg:34.43ms
step:162/2160 train_time:5576ms step_avg:34.42ms
step:163/2160 train_time:5610ms step_avg:34.42ms
step:164/2160 train_time:5643ms step_avg:34.41ms
step:165/2160 train_time:5677ms step_avg:34.41ms
step:166/2160 train_time:5710ms step_avg:34.40ms
step:167/2160 train_time:5744ms step_avg:34.40ms
step:168/2160 train_time:5778ms step_avg:34.39ms
step:169/2160 train_time:5811ms step_avg:34.39ms
step:170/2160 train_time:5845ms step_avg:34.38ms
step:171/2160 train_time:5878ms step_avg:34.38ms
step:172/2160 train_time:5912ms step_avg:34.37ms
step:173/2160 train_time:5946ms step_avg:34.37ms
step:174/2160 train_time:5979ms step_avg:34.36ms
step:175/2160 train_time:6012ms step_avg:34.35ms
step:176/2160 train_time:6045ms step_avg:34.35ms
step:177/2160 train_time:6079ms step_avg:34.34ms
step:178/2160 train_time:6112ms step_avg:34.34ms
step:179/2160 train_time:6146ms step_avg:34.33ms
step:180/2160 train_time:6179ms step_avg:34.33ms
step:181/2160 train_time:6212ms step_avg:34.32ms
step:182/2160 train_time:6245ms step_avg:34.32ms
step:183/2160 train_time:6279ms step_avg:34.31ms
step:184/2160 train_time:6313ms step_avg:34.31ms
step:185/2160 train_time:6346ms step_avg:34.30ms
step:186/2160 train_time:6379ms step_avg:34.30ms
step:187/2160 train_time:6413ms step_avg:34.29ms
step:188/2160 train_time:6446ms step_avg:34.29ms
step:189/2160 train_time:6479ms step_avg:34.28ms
step:190/2160 train_time:6512ms step_avg:34.28ms
step:191/2160 train_time:6546ms step_avg:34.27ms
step:192/2160 train_time:6580ms step_avg:34.27ms
step:193/2160 train_time:6613ms step_avg:34.26ms
step:194/2160 train_time:6646ms step_avg:34.26ms
step:195/2160 train_time:6680ms step_avg:34.26ms
step:196/2160 train_time:6713ms step_avg:34.25ms
step:197/2160 train_time:6747ms step_avg:34.25ms
step:198/2160 train_time:6780ms step_avg:34.24ms
step:199/2160 train_time:6814ms step_avg:34.24ms
step:200/2160 train_time:6847ms step_avg:34.24ms
step:201/2160 train_time:6881ms step_avg:34.24ms
step:202/2160 train_time:6915ms step_avg:34.23ms
step:203/2160 train_time:6948ms step_avg:34.23ms
step:204/2160 train_time:6982ms step_avg:34.22ms
step:205/2160 train_time:7016ms step_avg:34.22ms
step:206/2160 train_time:7049ms step_avg:34.22ms
step:207/2160 train_time:7083ms step_avg:34.22ms
step:208/2160 train_time:7116ms step_avg:34.21ms
step:209/2160 train_time:7150ms step_avg:34.21ms
step:210/2160 train_time:7183ms step_avg:34.21ms
step:211/2160 train_time:7217ms step_avg:34.21ms
step:212/2160 train_time:7251ms step_avg:34.20ms
step:213/2160 train_time:7285ms step_avg:34.20ms
step:214/2160 train_time:7318ms step_avg:34.20ms
step:215/2160 train_time:7352ms step_avg:34.19ms
step:216/2160 train_time:7385ms step_avg:34.19ms
step:217/2160 train_time:7418ms step_avg:34.19ms
step:218/2160 train_time:7451ms step_avg:34.18ms
step:219/2160 train_time:7485ms step_avg:34.18ms
step:220/2160 train_time:7518ms step_avg:34.17ms
step:221/2160 train_time:7552ms step_avg:34.17ms
step:222/2160 train_time:7585ms step_avg:34.17ms
step:223/2160 train_time:7619ms step_avg:34.17ms
step:224/2160 train_time:7652ms step_avg:34.16ms
step:225/2160 train_time:7686ms step_avg:34.16ms
step:226/2160 train_time:7719ms step_avg:34.15ms
step:227/2160 train_time:7752ms step_avg:34.15ms
step:228/2160 train_time:7785ms step_avg:34.15ms
step:229/2160 train_time:7819ms step_avg:34.15ms
step:230/2160 train_time:7852ms step_avg:34.14ms
step:231/2160 train_time:7886ms step_avg:34.14ms
step:232/2160 train_time:7919ms step_avg:34.14ms
step:233/2160 train_time:7953ms step_avg:34.13ms
step:234/2160 train_time:7986ms step_avg:34.13ms
step:235/2160 train_time:8020ms step_avg:34.13ms
step:236/2160 train_time:8053ms step_avg:34.12ms
step:237/2160 train_time:8087ms step_avg:34.12ms
step:238/2160 train_time:8120ms step_avg:34.12ms
step:239/2160 train_time:8154ms step_avg:34.12ms
step:240/2160 train_time:8187ms step_avg:34.11ms
step:241/2160 train_time:8221ms step_avg:34.11ms
step:242/2160 train_time:8254ms step_avg:34.11ms
step:243/2160 train_time:8288ms step_avg:34.11ms
step:244/2160 train_time:8321ms step_avg:34.10ms
step:245/2160 train_time:8355ms step_avg:34.10ms
step:246/2160 train_time:8388ms step_avg:34.10ms
step:247/2160 train_time:8422ms step_avg:34.10ms
step:248/2160 train_time:8455ms step_avg:34.09ms
step:249/2160 train_time:8489ms step_avg:34.09ms
step:250/2160 train_time:8522ms step_avg:34.09ms
step:250/2160 val_loss:4.3008 train_time:8556ms step_avg:34.23ms
step:251/2160 train_time:8579ms step_avg:34.18ms
step:252/2160 train_time:8601ms step_avg:34.13ms
step:253/2160 train_time:8626ms step_avg:34.09ms
step:254/2160 train_time:8659ms step_avg:34.09ms
step:255/2160 train_time:8695ms step_avg:34.10ms
step:256/2160 train_time:8729ms step_avg:34.10ms
step:257/2160 train_time:8764ms step_avg:34.10ms
step:258/2160 train_time:8798ms step_avg:34.10ms
step:259/2160 train_time:8832ms step_avg:34.10ms
step:260/2160 train_time:8865ms step_avg:34.10ms
step:261/2160 train_time:8899ms step_avg:34.09ms
step:262/2160 train_time:8932ms step_avg:34.09ms
step:263/2160 train_time:8966ms step_avg:34.09ms
step:264/2160 train_time:8999ms step_avg:34.09ms
step:265/2160 train_time:9033ms step_avg:34.09ms
step:266/2160 train_time:9066ms step_avg:34.08ms
step:267/2160 train_time:9099ms step_avg:34.08ms
step:268/2160 train_time:9132ms step_avg:34.08ms
step:269/2160 train_time:9166ms step_avg:34.07ms
step:270/2160 train_time:9199ms step_avg:34.07ms
step:271/2160 train_time:9232ms step_avg:34.07ms
step:272/2160 train_time:9265ms step_avg:34.06ms
step:273/2160 train_time:9299ms step_avg:34.06ms
step:274/2160 train_time:9332ms step_avg:34.06ms
step:275/2160 train_time:9366ms step_avg:34.06ms
step:276/2160 train_time:9399ms step_avg:34.05ms
step:277/2160 train_time:9432ms step_avg:34.05ms
step:278/2160 train_time:9465ms step_avg:34.05ms
step:279/2160 train_time:9498ms step_avg:34.04ms
step:280/2160 train_time:9532ms step_avg:34.04ms
step:281/2160 train_time:9565ms step_avg:34.04ms
step:282/2160 train_time:9599ms step_avg:34.04ms
step:283/2160 train_time:9633ms step_avg:34.04ms
step:284/2160 train_time:9666ms step_avg:34.04ms
step:285/2160 train_time:9701ms step_avg:34.04ms
step:286/2160 train_time:9734ms step_avg:34.04ms
step:287/2160 train_time:9768ms step_avg:34.04ms
step:288/2160 train_time:9802ms step_avg:34.03ms
step:289/2160 train_time:9836ms step_avg:34.03ms
step:290/2160 train_time:9869ms step_avg:34.03ms
step:291/2160 train_time:9903ms step_avg:34.03ms
step:292/2160 train_time:9936ms step_avg:34.03ms
step:293/2160 train_time:9970ms step_avg:34.03ms
step:294/2160 train_time:10004ms step_avg:34.03ms
step:295/2160 train_time:10037ms step_avg:34.02ms
step:296/2160 train_time:10070ms step_avg:34.02ms
step:297/2160 train_time:10104ms step_avg:34.02ms
step:298/2160 train_time:10138ms step_avg:34.02ms
step:299/2160 train_time:10171ms step_avg:34.02ms
step:300/2160 train_time:10204ms step_avg:34.01ms
step:301/2160 train_time:10238ms step_avg:34.01ms
step:302/2160 train_time:10271ms step_avg:34.01ms
step:303/2160 train_time:10304ms step_avg:34.01ms
step:304/2160 train_time:10338ms step_avg:34.01ms
step:305/2160 train_time:10371ms step_avg:34.00ms
step:306/2160 train_time:10404ms step_avg:34.00ms
step:307/2160 train_time:10438ms step_avg:34.00ms
step:308/2160 train_time:10471ms step_avg:34.00ms
step:309/2160 train_time:10504ms step_avg:33.99ms
step:310/2160 train_time:10538ms step_avg:33.99ms
step:311/2160 train_time:10572ms step_avg:33.99ms
step:312/2160 train_time:10605ms step_avg:33.99ms
step:313/2160 train_time:10639ms step_avg:33.99ms
step:314/2160 train_time:10672ms step_avg:33.99ms
step:315/2160 train_time:10706ms step_avg:33.99ms
step:316/2160 train_time:10739ms step_avg:33.99ms
step:317/2160 train_time:10773ms step_avg:33.98ms
step:318/2160 train_time:10806ms step_avg:33.98ms
step:319/2160 train_time:10840ms step_avg:33.98ms
step:320/2160 train_time:10873ms step_avg:33.98ms
step:321/2160 train_time:10907ms step_avg:33.98ms
step:322/2160 train_time:10940ms step_avg:33.98ms
step:323/2160 train_time:10974ms step_avg:33.97ms
step:324/2160 train_time:11007ms step_avg:33.97ms
step:325/2160 train_time:11041ms step_avg:33.97ms
step:326/2160 train_time:11074ms step_avg:33.97ms
step:327/2160 train_time:11108ms step_avg:33.97ms
step:328/2160 train_time:11141ms step_avg:33.97ms
step:329/2160 train_time:11175ms step_avg:33.97ms
step:330/2160 train_time:11208ms step_avg:33.96ms
step:331/2160 train_time:11242ms step_avg:33.96ms
step:332/2160 train_time:11275ms step_avg:33.96ms
step:333/2160 train_time:11309ms step_avg:33.96ms
step:334/2160 train_time:11342ms step_avg:33.96ms
step:335/2160 train_time:11376ms step_avg:33.96ms
step:336/2160 train_time:11409ms step_avg:33.95ms
step:337/2160 train_time:11442ms step_avg:33.95ms
step:338/2160 train_time:11476ms step_avg:33.95ms
step:339/2160 train_time:11509ms step_avg:33.95ms
step:340/2160 train_time:11542ms step_avg:33.95ms
step:341/2160 train_time:11576ms step_avg:33.95ms
step:342/2160 train_time:11609ms step_avg:33.94ms
step:343/2160 train_time:11643ms step_avg:33.94ms
step:344/2160 train_time:11676ms step_avg:33.94ms
step:345/2160 train_time:11710ms step_avg:33.94ms
step:346/2160 train_time:11743ms step_avg:33.94ms
step:347/2160 train_time:11777ms step_avg:33.94ms
step:348/2160 train_time:11810ms step_avg:33.94ms
step:349/2160 train_time:11844ms step_avg:33.94ms
step:350/2160 train_time:11877ms step_avg:33.93ms
step:351/2160 train_time:11911ms step_avg:33.94ms
step:352/2160 train_time:11944ms step_avg:33.93ms
step:353/2160 train_time:11978ms step_avg:33.93ms
step:354/2160 train_time:12012ms step_avg:33.93ms
step:355/2160 train_time:12045ms step_avg:33.93ms
step:356/2160 train_time:12078ms step_avg:33.93ms
step:357/2160 train_time:12112ms step_avg:33.93ms
step:358/2160 train_time:12145ms step_avg:33.92ms
step:359/2160 train_time:12179ms step_avg:33.92ms
step:360/2160 train_time:12212ms step_avg:33.92ms
step:361/2160 train_time:12246ms step_avg:33.92ms
step:362/2160 train_time:12279ms step_avg:33.92ms
step:363/2160 train_time:12313ms step_avg:33.92ms
step:364/2160 train_time:12346ms step_avg:33.92ms
step:365/2160 train_time:12380ms step_avg:33.92ms
step:366/2160 train_time:12413ms step_avg:33.92ms
step:367/2160 train_time:12446ms step_avg:33.91ms
step:368/2160 train_time:12480ms step_avg:33.91ms
step:369/2160 train_time:12514ms step_avg:33.91ms
step:370/2160 train_time:12547ms step_avg:33.91ms
step:371/2160 train_time:12581ms step_avg:33.91ms
step:372/2160 train_time:12614ms step_avg:33.91ms
step:373/2160 train_time:12648ms step_avg:33.91ms
step:374/2160 train_time:12681ms step_avg:33.91ms
step:375/2160 train_time:12715ms step_avg:33.91ms
step:376/2160 train_time:12748ms step_avg:33.90ms
step:377/2160 train_time:12782ms step_avg:33.91ms
step:378/2160 train_time:12816ms step_avg:33.90ms
step:379/2160 train_time:12849ms step_avg:33.90ms
step:380/2160 train_time:12882ms step_avg:33.90ms
step:381/2160 train_time:12916ms step_avg:33.90ms
step:382/2160 train_time:12949ms step_avg:33.90ms
step:383/2160 train_time:12983ms step_avg:33.90ms
step:384/2160 train_time:13016ms step_avg:33.90ms
step:385/2160 train_time:13050ms step_avg:33.90ms
step:386/2160 train_time:13084ms step_avg:33.90ms
step:387/2160 train_time:13117ms step_avg:33.89ms
step:388/2160 train_time:13150ms step_avg:33.89ms
step:389/2160 train_time:13184ms step_avg:33.89ms
step:390/2160 train_time:13218ms step_avg:33.89ms
step:391/2160 train_time:13251ms step_avg:33.89ms
step:392/2160 train_time:13285ms step_avg:33.89ms
step:393/2160 train_time:13318ms step_avg:33.89ms
step:394/2160 train_time:13351ms step_avg:33.89ms
step:395/2160 train_time:13385ms step_avg:33.89ms
step:396/2160 train_time:13418ms step_avg:33.88ms
step:397/2160 train_time:13452ms step_avg:33.88ms
step:398/2160 train_time:13485ms step_avg:33.88ms
step:399/2160 train_time:13518ms step_avg:33.88ms
step:400/2160 train_time:13551ms step_avg:33.88ms
step:401/2160 train_time:13585ms step_avg:33.88ms
step:402/2160 train_time:13619ms step_avg:33.88ms
step:403/2160 train_time:13652ms step_avg:33.88ms
step:404/2160 train_time:13685ms step_avg:33.87ms
step:405/2160 train_time:13719ms step_avg:33.87ms
step:406/2160 train_time:13752ms step_avg:33.87ms
step:407/2160 train_time:13786ms step_avg:33.87ms
step:408/2160 train_time:13819ms step_avg:33.87ms
step:409/2160 train_time:13853ms step_avg:33.87ms
step:410/2160 train_time:13886ms step_avg:33.87ms
step:411/2160 train_time:13920ms step_avg:33.87ms
step:412/2160 train_time:13954ms step_avg:33.87ms
step:413/2160 train_time:13987ms step_avg:33.87ms
step:414/2160 train_time:14021ms step_avg:33.87ms
step:415/2160 train_time:14055ms step_avg:33.87ms
step:416/2160 train_time:14088ms step_avg:33.87ms
step:417/2160 train_time:14122ms step_avg:33.87ms
step:418/2160 train_time:14155ms step_avg:33.86ms
step:419/2160 train_time:14189ms step_avg:33.86ms
step:420/2160 train_time:14222ms step_avg:33.86ms
step:421/2160 train_time:14256ms step_avg:33.86ms
step:422/2160 train_time:14289ms step_avg:33.86ms
step:423/2160 train_time:14323ms step_avg:33.86ms
step:424/2160 train_time:14356ms step_avg:33.86ms
step:425/2160 train_time:14390ms step_avg:33.86ms
step:426/2160 train_time:14423ms step_avg:33.86ms
step:427/2160 train_time:14457ms step_avg:33.86ms
step:428/2160 train_time:14490ms step_avg:33.86ms
step:429/2160 train_time:14524ms step_avg:33.85ms
step:430/2160 train_time:14557ms step_avg:33.85ms
step:431/2160 train_time:14591ms step_avg:33.85ms
step:432/2160 train_time:14624ms step_avg:33.85ms
step:433/2160 train_time:14657ms step_avg:33.85ms
step:434/2160 train_time:14690ms step_avg:33.85ms
step:435/2160 train_time:14724ms step_avg:33.85ms
step:436/2160 train_time:14757ms step_avg:33.85ms
step:437/2160 train_time:14791ms step_avg:33.85ms
step:438/2160 train_time:14825ms step_avg:33.85ms
step:439/2160 train_time:14859ms step_avg:33.85ms
step:440/2160 train_time:14892ms step_avg:33.84ms
step:441/2160 train_time:14926ms step_avg:33.85ms
step:442/2160 train_time:14960ms step_avg:33.85ms
step:443/2160 train_time:14993ms step_avg:33.84ms
step:444/2160 train_time:15026ms step_avg:33.84ms
step:445/2160 train_time:15060ms step_avg:33.84ms
step:446/2160 train_time:15093ms step_avg:33.84ms
step:447/2160 train_time:15127ms step_avg:33.84ms
step:448/2160 train_time:15160ms step_avg:33.84ms
step:449/2160 train_time:15194ms step_avg:33.84ms
step:450/2160 train_time:15227ms step_avg:33.84ms
step:451/2160 train_time:15262ms step_avg:33.84ms
step:452/2160 train_time:15295ms step_avg:33.84ms
step:453/2160 train_time:15329ms step_avg:33.84ms
step:454/2160 train_time:15362ms step_avg:33.84ms
step:455/2160 train_time:15396ms step_avg:33.84ms
step:456/2160 train_time:15429ms step_avg:33.84ms
step:457/2160 train_time:15463ms step_avg:33.84ms
step:458/2160 train_time:15496ms step_avg:33.84ms
step:459/2160 train_time:15530ms step_avg:33.83ms
step:460/2160 train_time:15564ms step_avg:33.83ms
step:461/2160 train_time:15597ms step_avg:33.83ms
step:462/2160 train_time:15630ms step_avg:33.83ms
step:463/2160 train_time:15664ms step_avg:33.83ms
step:464/2160 train_time:15698ms step_avg:33.83ms
step:465/2160 train_time:15731ms step_avg:33.83ms
step:466/2160 train_time:15765ms step_avg:33.83ms
step:467/2160 train_time:15798ms step_avg:33.83ms
step:468/2160 train_time:15832ms step_avg:33.83ms
step:469/2160 train_time:15866ms step_avg:33.83ms
step:470/2160 train_time:15900ms step_avg:33.83ms
step:471/2160 train_time:15934ms step_avg:33.83ms
step:472/2160 train_time:15967ms step_avg:33.83ms
step:473/2160 train_time:16001ms step_avg:33.83ms
step:474/2160 train_time:16035ms step_avg:33.83ms
step:475/2160 train_time:16069ms step_avg:33.83ms
step:476/2160 train_time:16102ms step_avg:33.83ms
step:477/2160 train_time:16136ms step_avg:33.83ms
step:478/2160 train_time:16169ms step_avg:33.83ms
step:479/2160 train_time:16204ms step_avg:33.83ms
step:480/2160 train_time:16237ms step_avg:33.83ms
step:481/2160 train_time:16271ms step_avg:33.83ms
step:482/2160 train_time:16304ms step_avg:33.83ms
step:483/2160 train_time:16338ms step_avg:33.83ms
step:484/2160 train_time:16371ms step_avg:33.82ms
step:485/2160 train_time:16404ms step_avg:33.82ms
step:486/2160 train_time:16438ms step_avg:33.82ms
step:487/2160 train_time:16472ms step_avg:33.82ms
step:488/2160 train_time:16505ms step_avg:33.82ms
step:489/2160 train_time:16539ms step_avg:33.82ms
step:490/2160 train_time:16572ms step_avg:33.82ms
step:491/2160 train_time:16605ms step_avg:33.82ms
step:492/2160 train_time:16638ms step_avg:33.82ms
step:493/2160 train_time:16672ms step_avg:33.82ms
step:494/2160 train_time:16705ms step_avg:33.82ms
step:495/2160 train_time:16739ms step_avg:33.82ms
step:496/2160 train_time:16772ms step_avg:33.81ms
step:497/2160 train_time:16806ms step_avg:33.81ms
step:498/2160 train_time:16839ms step_avg:33.81ms
step:499/2160 train_time:16873ms step_avg:33.81ms
step:500/2160 train_time:16906ms step_avg:33.81ms
step:500/2160 val_loss:4.0125 train_time:16941ms step_avg:33.88ms
step:501/2160 train_time:16964ms step_avg:33.86ms
step:502/2160 train_time:16987ms step_avg:33.84ms
step:503/2160 train_time:17013ms step_avg:33.82ms
step:504/2160 train_time:17046ms step_avg:33.82ms
step:505/2160 train_time:17083ms step_avg:33.83ms
step:506/2160 train_time:17117ms step_avg:33.83ms
step:507/2160 train_time:17151ms step_avg:33.83ms
step:508/2160 train_time:17185ms step_avg:33.83ms
step:509/2160 train_time:17219ms step_avg:33.83ms
step:510/2160 train_time:17253ms step_avg:33.83ms
step:511/2160 train_time:17286ms step_avg:33.83ms
step:512/2160 train_time:17319ms step_avg:33.83ms
step:513/2160 train_time:17353ms step_avg:33.83ms
step:514/2160 train_time:17386ms step_avg:33.82ms
step:515/2160 train_time:17419ms step_avg:33.82ms
step:516/2160 train_time:17453ms step_avg:33.82ms
step:517/2160 train_time:17486ms step_avg:33.82ms
step:518/2160 train_time:17520ms step_avg:33.82ms
step:519/2160 train_time:17553ms step_avg:33.82ms
step:520/2160 train_time:17586ms step_avg:33.82ms
step:521/2160 train_time:17620ms step_avg:33.82ms
step:522/2160 train_time:17653ms step_avg:33.82ms
step:523/2160 train_time:17686ms step_avg:33.82ms
step:524/2160 train_time:17720ms step_avg:33.82ms
step:525/2160 train_time:17753ms step_avg:33.82ms
step:526/2160 train_time:17786ms step_avg:33.81ms
step:527/2160 train_time:17820ms step_avg:33.81ms
step:528/2160 train_time:17853ms step_avg:33.81ms
step:529/2160 train_time:17887ms step_avg:33.81ms
step:530/2160 train_time:17920ms step_avg:33.81ms
step:531/2160 train_time:17954ms step_avg:33.81ms
step:532/2160 train_time:17987ms step_avg:33.81ms
step:533/2160 train_time:18021ms step_avg:33.81ms
step:534/2160 train_time:18055ms step_avg:33.81ms
step:535/2160 train_time:18089ms step_avg:33.81ms
step:536/2160 train_time:18122ms step_avg:33.81ms
step:537/2160 train_time:18157ms step_avg:33.81ms
step:538/2160 train_time:18190ms step_avg:33.81ms
step:539/2160 train_time:18224ms step_avg:33.81ms
step:540/2160 train_time:18258ms step_avg:33.81ms
step:541/2160 train_time:18291ms step_avg:33.81ms
step:542/2160 train_time:18325ms step_avg:33.81ms
step:543/2160 train_time:18359ms step_avg:33.81ms
step:544/2160 train_time:18392ms step_avg:33.81ms
step:545/2160 train_time:18426ms step_avg:33.81ms
step:546/2160 train_time:18459ms step_avg:33.81ms
step:547/2160 train_time:18493ms step_avg:33.81ms
step:548/2160 train_time:18526ms step_avg:33.81ms
step:549/2160 train_time:18560ms step_avg:33.81ms
step:550/2160 train_time:18593ms step_avg:33.81ms
step:551/2160 train_time:18626ms step_avg:33.80ms
step:552/2160 train_time:18659ms step_avg:33.80ms
step:553/2160 train_time:18693ms step_avg:33.80ms
step:554/2160 train_time:18726ms step_avg:33.80ms
step:555/2160 train_time:18760ms step_avg:33.80ms
step:556/2160 train_time:18793ms step_avg:33.80ms
step:557/2160 train_time:18826ms step_avg:33.80ms
step:558/2160 train_time:18860ms step_avg:33.80ms
step:559/2160 train_time:18893ms step_avg:33.80ms
step:560/2160 train_time:18926ms step_avg:33.80ms
step:561/2160 train_time:18961ms step_avg:33.80ms
step:562/2160 train_time:18994ms step_avg:33.80ms
step:563/2160 train_time:19028ms step_avg:33.80ms
step:564/2160 train_time:19061ms step_avg:33.80ms
step:565/2160 train_time:19096ms step_avg:33.80ms
step:566/2160 train_time:19129ms step_avg:33.80ms
step:567/2160 train_time:19163ms step_avg:33.80ms
step:568/2160 train_time:19197ms step_avg:33.80ms
step:569/2160 train_time:19230ms step_avg:33.80ms
step:570/2160 train_time:19264ms step_avg:33.80ms
step:571/2160 train_time:19297ms step_avg:33.80ms
step:572/2160 train_time:19331ms step_avg:33.80ms
step:573/2160 train_time:19365ms step_avg:33.80ms
step:574/2160 train_time:19398ms step_avg:33.79ms
step:575/2160 train_time:19432ms step_avg:33.79ms
step:576/2160 train_time:19465ms step_avg:33.79ms
step:577/2160 train_time:19499ms step_avg:33.79ms
step:578/2160 train_time:19532ms step_avg:33.79ms
step:579/2160 train_time:19566ms step_avg:33.79ms
step:580/2160 train_time:19599ms step_avg:33.79ms
step:581/2160 train_time:19633ms step_avg:33.79ms
step:582/2160 train_time:19666ms step_avg:33.79ms
step:583/2160 train_time:19700ms step_avg:33.79ms
step:584/2160 train_time:19733ms step_avg:33.79ms
step:585/2160 train_time:19766ms step_avg:33.79ms
step:586/2160 train_time:19800ms step_avg:33.79ms
step:587/2160 train_time:19833ms step_avg:33.79ms
step:588/2160 train_time:19866ms step_avg:33.79ms
step:589/2160 train_time:19900ms step_avg:33.79ms
step:590/2160 train_time:19933ms step_avg:33.79ms
step:591/2160 train_time:19968ms step_avg:33.79ms
step:592/2160 train_time:20000ms step_avg:33.78ms
step:593/2160 train_time:20034ms step_avg:33.78ms
step:594/2160 train_time:20067ms step_avg:33.78ms
step:595/2160 train_time:20102ms step_avg:33.78ms
step:596/2160 train_time:20135ms step_avg:33.78ms
step:597/2160 train_time:20169ms step_avg:33.78ms
step:598/2160 train_time:20202ms step_avg:33.78ms
step:599/2160 train_time:20236ms step_avg:33.78ms
step:600/2160 train_time:20270ms step_avg:33.78ms
step:601/2160 train_time:20303ms step_avg:33.78ms
step:602/2160 train_time:20336ms step_avg:33.78ms
step:603/2160 train_time:20370ms step_avg:33.78ms
step:604/2160 train_time:20403ms step_avg:33.78ms
step:605/2160 train_time:20437ms step_avg:33.78ms
step:606/2160 train_time:20471ms step_avg:33.78ms
step:607/2160 train_time:20504ms step_avg:33.78ms
step:608/2160 train_time:20538ms step_avg:33.78ms
step:609/2160 train_time:20571ms step_avg:33.78ms
step:610/2160 train_time:20605ms step_avg:33.78ms
step:611/2160 train_time:20638ms step_avg:33.78ms
step:612/2160 train_time:20672ms step_avg:33.78ms
step:613/2160 train_time:20705ms step_avg:33.78ms
step:614/2160 train_time:20738ms step_avg:33.78ms
step:615/2160 train_time:20772ms step_avg:33.78ms
step:616/2160 train_time:20806ms step_avg:33.78ms
step:617/2160 train_time:20839ms step_avg:33.78ms
step:618/2160 train_time:20873ms step_avg:33.77ms
step:619/2160 train_time:20906ms step_avg:33.77ms
step:620/2160 train_time:20940ms step_avg:33.77ms
step:621/2160 train_time:20973ms step_avg:33.77ms
step:622/2160 train_time:21006ms step_avg:33.77ms
step:623/2160 train_time:21041ms step_avg:33.77ms
step:624/2160 train_time:21074ms step_avg:33.77ms
step:625/2160 train_time:21108ms step_avg:33.77ms
step:626/2160 train_time:21141ms step_avg:33.77ms
step:627/2160 train_time:21175ms step_avg:33.77ms
step:628/2160 train_time:21209ms step_avg:33.77ms
step:629/2160 train_time:21242ms step_avg:33.77ms
step:630/2160 train_time:21276ms step_avg:33.77ms
step:631/2160 train_time:21309ms step_avg:33.77ms
step:632/2160 train_time:21343ms step_avg:33.77ms
step:633/2160 train_time:21376ms step_avg:33.77ms
step:634/2160 train_time:21410ms step_avg:33.77ms
step:635/2160 train_time:21443ms step_avg:33.77ms
step:636/2160 train_time:21477ms step_avg:33.77ms
step:637/2160 train_time:21511ms step_avg:33.77ms
step:638/2160 train_time:21544ms step_avg:33.77ms
step:639/2160 train_time:21578ms step_avg:33.77ms
step:640/2160 train_time:21611ms step_avg:33.77ms
step:641/2160 train_time:21645ms step_avg:33.77ms
step:642/2160 train_time:21678ms step_avg:33.77ms
step:643/2160 train_time:21712ms step_avg:33.77ms
step:644/2160 train_time:21745ms step_avg:33.77ms
step:645/2160 train_time:21779ms step_avg:33.77ms
step:646/2160 train_time:21812ms step_avg:33.77ms
step:647/2160 train_time:21846ms step_avg:33.77ms
step:648/2160 train_time:21880ms step_avg:33.76ms
step:649/2160 train_time:21913ms step_avg:33.76ms
step:650/2160 train_time:21947ms step_avg:33.76ms
step:651/2160 train_time:21981ms step_avg:33.76ms
step:652/2160 train_time:22014ms step_avg:33.76ms
step:653/2160 train_time:22047ms step_avg:33.76ms
step:654/2160 train_time:22081ms step_avg:33.76ms
step:655/2160 train_time:22115ms step_avg:33.76ms
step:656/2160 train_time:22148ms step_avg:33.76ms
step:657/2160 train_time:22183ms step_avg:33.76ms
step:658/2160 train_time:22216ms step_avg:33.76ms
step:659/2160 train_time:22250ms step_avg:33.76ms
step:660/2160 train_time:22283ms step_avg:33.76ms
step:661/2160 train_time:22317ms step_avg:33.76ms
step:662/2160 train_time:22351ms step_avg:33.76ms
step:663/2160 train_time:22384ms step_avg:33.76ms
step:664/2160 train_time:22418ms step_avg:33.76ms
step:665/2160 train_time:22451ms step_avg:33.76ms
step:666/2160 train_time:22485ms step_avg:33.76ms
step:667/2160 train_time:22518ms step_avg:33.76ms
step:668/2160 train_time:22552ms step_avg:33.76ms
step:669/2160 train_time:22585ms step_avg:33.76ms
step:670/2160 train_time:22619ms step_avg:33.76ms
step:671/2160 train_time:22652ms step_avg:33.76ms
step:672/2160 train_time:22686ms step_avg:33.76ms
step:673/2160 train_time:22720ms step_avg:33.76ms
step:674/2160 train_time:22753ms step_avg:33.76ms
step:675/2160 train_time:22787ms step_avg:33.76ms
step:676/2160 train_time:22820ms step_avg:33.76ms
step:677/2160 train_time:22854ms step_avg:33.76ms
step:678/2160 train_time:22887ms step_avg:33.76ms
step:679/2160 train_time:22921ms step_avg:33.76ms
step:680/2160 train_time:22955ms step_avg:33.76ms
step:681/2160 train_time:22988ms step_avg:33.76ms
step:682/2160 train_time:23022ms step_avg:33.76ms
step:683/2160 train_time:23056ms step_avg:33.76ms
step:684/2160 train_time:23089ms step_avg:33.76ms
step:685/2160 train_time:23123ms step_avg:33.76ms
step:686/2160 train_time:23156ms step_avg:33.76ms
step:687/2160 train_time:23190ms step_avg:33.75ms
step:688/2160 train_time:23223ms step_avg:33.75ms
step:689/2160 train_time:23257ms step_avg:33.75ms
step:690/2160 train_time:23291ms step_avg:33.75ms
step:691/2160 train_time:23324ms step_avg:33.75ms
step:692/2160 train_time:23358ms step_avg:33.75ms
step:693/2160 train_time:23391ms step_avg:33.75ms
step:694/2160 train_time:23424ms step_avg:33.75ms
step:695/2160 train_time:23458ms step_avg:33.75ms
step:696/2160 train_time:23492ms step_avg:33.75ms
step:697/2160 train_time:23526ms step_avg:33.75ms
step:698/2160 train_time:23559ms step_avg:33.75ms
step:699/2160 train_time:23593ms step_avg:33.75ms
step:700/2160 train_time:23627ms step_avg:33.75ms
step:701/2160 train_time:23661ms step_avg:33.75ms
step:702/2160 train_time:23694ms step_avg:33.75ms
step:703/2160 train_time:23728ms step_avg:33.75ms
step:704/2160 train_time:23761ms step_avg:33.75ms
step:705/2160 train_time:23795ms step_avg:33.75ms
step:706/2160 train_time:23828ms step_avg:33.75ms
step:707/2160 train_time:23862ms step_avg:33.75ms
step:708/2160 train_time:23896ms step_avg:33.75ms
step:709/2160 train_time:23955ms step_avg:33.79ms
step:710/2160 train_time:24014ms step_avg:33.82ms
step:711/2160 train_time:24075ms step_avg:33.86ms
step:712/2160 train_time:24134ms step_avg:33.90ms
step:713/2160 train_time:24196ms step_avg:33.94ms
step:714/2160 train_time:24254ms step_avg:33.97ms
step:715/2160 train_time:24316ms step_avg:34.01ms
step:716/2160 train_time:24375ms step_avg:34.04ms
step:717/2160 train_time:24436ms step_avg:34.08ms
step:718/2160 train_time:24496ms step_avg:34.12ms
step:719/2160 train_time:24558ms step_avg:34.16ms
step:720/2160 train_time:24618ms step_avg:34.19ms
step:721/2160 train_time:24679ms step_avg:34.23ms
step:722/2160 train_time:24739ms step_avg:34.26ms
step:723/2160 train_time:24800ms step_avg:34.30ms
step:724/2160 train_time:24860ms step_avg:34.34ms
step:725/2160 train_time:24920ms step_avg:34.37ms
step:726/2160 train_time:24979ms step_avg:34.41ms
step:727/2160 train_time:25039ms step_avg:34.44ms
step:728/2160 train_time:25099ms step_avg:34.48ms
step:729/2160 train_time:25159ms step_avg:34.51ms
step:730/2160 train_time:25219ms step_avg:34.55ms
step:731/2160 train_time:25280ms step_avg:34.58ms
step:732/2160 train_time:25340ms step_avg:34.62ms
step:733/2160 train_time:25402ms step_avg:34.66ms
step:734/2160 train_time:25462ms step_avg:34.69ms
step:735/2160 train_time:25523ms step_avg:34.73ms
step:736/2160 train_time:25584ms step_avg:34.76ms
step:737/2160 train_time:25645ms step_avg:34.80ms
step:738/2160 train_time:25704ms step_avg:34.83ms
step:739/2160 train_time:25766ms step_avg:34.87ms
step:740/2160 train_time:25825ms step_avg:34.90ms
step:741/2160 train_time:25886ms step_avg:34.93ms
step:742/2160 train_time:25946ms step_avg:34.97ms
step:743/2160 train_time:26007ms step_avg:35.00ms
step:744/2160 train_time:26067ms step_avg:35.04ms
step:745/2160 train_time:26129ms step_avg:35.07ms
step:746/2160 train_time:26189ms step_avg:35.11ms
step:747/2160 train_time:26252ms step_avg:35.14ms
step:748/2160 train_time:26311ms step_avg:35.18ms
step:749/2160 train_time:26373ms step_avg:35.21ms
step:750/2160 train_time:26432ms step_avg:35.24ms
step:750/2160 val_loss:3.8530 train_time:26495ms step_avg:35.33ms
step:751/2160 train_time:26518ms step_avg:35.31ms
step:752/2160 train_time:26560ms step_avg:35.32ms
step:753/2160 train_time:26624ms step_avg:35.36ms
step:754/2160 train_time:26688ms step_avg:35.40ms
step:755/2160 train_time:26750ms step_avg:35.43ms
step:756/2160 train_time:26809ms step_avg:35.46ms
step:757/2160 train_time:26870ms step_avg:35.50ms
step:758/2160 train_time:26928ms step_avg:35.53ms
step:759/2160 train_time:26989ms step_avg:35.56ms
step:760/2160 train_time:27048ms step_avg:35.59ms
step:761/2160 train_time:27109ms step_avg:35.62ms
step:762/2160 train_time:27167ms step_avg:35.65ms
step:763/2160 train_time:27228ms step_avg:35.69ms
step:764/2160 train_time:27286ms step_avg:35.72ms
step:765/2160 train_time:27347ms step_avg:35.75ms
step:766/2160 train_time:27406ms step_avg:35.78ms
step:767/2160 train_time:27467ms step_avg:35.81ms
step:768/2160 train_time:27528ms step_avg:35.84ms
step:769/2160 train_time:27591ms step_avg:35.88ms
step:770/2160 train_time:27654ms step_avg:35.91ms
step:771/2160 train_time:27716ms step_avg:35.95ms
step:772/2160 train_time:27776ms step_avg:35.98ms
step:773/2160 train_time:27838ms step_avg:36.01ms
step:774/2160 train_time:27898ms step_avg:36.04ms
step:775/2160 train_time:27959ms step_avg:36.08ms
step:776/2160 train_time:28018ms step_avg:36.11ms
step:777/2160 train_time:28078ms step_avg:36.14ms
step:778/2160 train_time:28137ms step_avg:36.17ms
step:779/2160 train_time:28199ms step_avg:36.20ms
step:780/2160 train_time:28258ms step_avg:36.23ms
step:781/2160 train_time:28320ms step_avg:36.26ms
step:782/2160 train_time:28379ms step_avg:36.29ms
step:783/2160 train_time:28440ms step_avg:36.32ms
step:784/2160 train_time:28499ms step_avg:36.35ms
step:785/2160 train_time:28561ms step_avg:36.38ms
step:786/2160 train_time:28622ms step_avg:36.41ms
step:787/2160 train_time:28683ms step_avg:36.45ms
step:788/2160 train_time:28745ms step_avg:36.48ms
step:789/2160 train_time:28806ms step_avg:36.51ms
step:790/2160 train_time:28867ms step_avg:36.54ms
step:791/2160 train_time:28928ms step_avg:36.57ms
step:792/2160 train_time:28987ms step_avg:36.60ms
step:793/2160 train_time:29048ms step_avg:36.63ms
step:794/2160 train_time:29108ms step_avg:36.66ms
step:795/2160 train_time:29169ms step_avg:36.69ms
step:796/2160 train_time:29228ms step_avg:36.72ms
step:797/2160 train_time:29289ms step_avg:36.75ms
step:798/2160 train_time:29348ms step_avg:36.78ms
step:799/2160 train_time:29409ms step_avg:36.81ms
step:800/2160 train_time:29469ms step_avg:36.84ms
step:801/2160 train_time:29531ms step_avg:36.87ms
step:802/2160 train_time:29591ms step_avg:36.90ms
step:803/2160 train_time:29654ms step_avg:36.93ms
step:804/2160 train_time:29715ms step_avg:36.96ms
step:805/2160 train_time:29779ms step_avg:36.99ms
step:806/2160 train_time:29839ms step_avg:37.02ms
step:807/2160 train_time:29901ms step_avg:37.05ms
step:808/2160 train_time:29960ms step_avg:37.08ms
step:809/2160 train_time:30020ms step_avg:37.11ms
step:810/2160 train_time:30079ms step_avg:37.14ms
step:811/2160 train_time:30140ms step_avg:37.16ms
step:812/2160 train_time:30199ms step_avg:37.19ms
step:813/2160 train_time:30260ms step_avg:37.22ms
step:814/2160 train_time:30319ms step_avg:37.25ms
step:815/2160 train_time:30380ms step_avg:37.28ms
step:816/2160 train_time:30439ms step_avg:37.30ms
step:817/2160 train_time:30501ms step_avg:37.33ms
step:818/2160 train_time:30561ms step_avg:37.36ms
step:819/2160 train_time:30622ms step_avg:37.39ms
step:820/2160 train_time:30682ms step_avg:37.42ms
step:821/2160 train_time:30744ms step_avg:37.45ms
step:822/2160 train_time:30804ms step_avg:37.47ms
step:823/2160 train_time:30865ms step_avg:37.50ms
step:824/2160 train_time:30926ms step_avg:37.53ms
step:825/2160 train_time:30987ms step_avg:37.56ms
step:826/2160 train_time:31046ms step_avg:37.59ms
step:827/2160 train_time:31106ms step_avg:37.61ms
step:828/2160 train_time:31166ms step_avg:37.64ms
step:829/2160 train_time:31228ms step_avg:37.67ms
step:830/2160 train_time:31287ms step_avg:37.70ms
step:831/2160 train_time:31348ms step_avg:37.72ms
step:832/2160 train_time:31408ms step_avg:37.75ms
step:833/2160 train_time:31470ms step_avg:37.78ms
step:834/2160 train_time:31529ms step_avg:37.80ms
step:835/2160 train_time:31590ms step_avg:37.83ms
step:836/2160 train_time:31651ms step_avg:37.86ms
step:837/2160 train_time:31713ms step_avg:37.89ms
step:838/2160 train_time:31772ms step_avg:37.91ms
step:839/2160 train_time:31835ms step_avg:37.94ms
step:840/2160 train_time:31896ms step_avg:37.97ms
step:841/2160 train_time:31959ms step_avg:38.00ms
step:842/2160 train_time:32018ms step_avg:38.03ms
step:843/2160 train_time:32080ms step_avg:38.05ms
step:844/2160 train_time:32139ms step_avg:38.08ms
step:845/2160 train_time:32201ms step_avg:38.11ms
step:846/2160 train_time:32260ms step_avg:38.13ms
step:847/2160 train_time:32321ms step_avg:38.16ms
step:848/2160 train_time:32380ms step_avg:38.18ms
step:849/2160 train_time:32441ms step_avg:38.21ms
step:850/2160 train_time:32500ms step_avg:38.24ms
step:851/2160 train_time:32562ms step_avg:38.26ms
step:852/2160 train_time:32622ms step_avg:38.29ms
step:853/2160 train_time:32684ms step_avg:38.32ms
step:854/2160 train_time:32745ms step_avg:38.34ms
step:855/2160 train_time:32807ms step_avg:38.37ms
step:856/2160 train_time:32867ms step_avg:38.40ms
step:857/2160 train_time:32929ms step_avg:38.42ms
step:858/2160 train_time:32988ms step_avg:38.45ms
step:859/2160 train_time:33049ms step_avg:38.47ms
step:860/2160 train_time:33108ms step_avg:38.50ms
step:861/2160 train_time:33170ms step_avg:38.52ms
step:862/2160 train_time:33229ms step_avg:38.55ms
step:863/2160 train_time:33290ms step_avg:38.57ms
step:864/2160 train_time:33350ms step_avg:38.60ms
step:865/2160 train_time:33412ms step_avg:38.63ms
step:866/2160 train_time:33472ms step_avg:38.65ms
step:867/2160 train_time:33534ms step_avg:38.68ms
step:868/2160 train_time:33594ms step_avg:38.70ms
step:869/2160 train_time:33656ms step_avg:38.73ms
step:870/2160 train_time:33716ms step_avg:38.75ms
step:871/2160 train_time:33778ms step_avg:38.78ms
step:872/2160 train_time:33838ms step_avg:38.81ms
step:873/2160 train_time:33900ms step_avg:38.83ms
step:874/2160 train_time:33960ms step_avg:38.86ms
step:875/2160 train_time:34021ms step_avg:38.88ms
step:876/2160 train_time:34080ms step_avg:38.90ms
step:877/2160 train_time:34142ms step_avg:38.93ms
step:878/2160 train_time:34201ms step_avg:38.95ms
step:879/2160 train_time:34262ms step_avg:38.98ms
step:880/2160 train_time:34322ms step_avg:39.00ms
step:881/2160 train_time:34383ms step_avg:39.03ms
step:882/2160 train_time:34443ms step_avg:39.05ms
step:883/2160 train_time:34504ms step_avg:39.08ms
step:884/2160 train_time:34563ms step_avg:39.10ms
step:885/2160 train_time:34625ms step_avg:39.12ms
step:886/2160 train_time:34685ms step_avg:39.15ms
step:887/2160 train_time:34746ms step_avg:39.17ms
step:888/2160 train_time:34806ms step_avg:39.20ms
step:889/2160 train_time:34867ms step_avg:39.22ms
step:890/2160 train_time:34927ms step_avg:39.24ms
step:891/2160 train_time:34987ms step_avg:39.27ms
step:892/2160 train_time:35047ms step_avg:39.29ms
step:893/2160 train_time:35109ms step_avg:39.32ms
step:894/2160 train_time:35168ms step_avg:39.34ms
step:895/2160 train_time:35230ms step_avg:39.36ms
step:896/2160 train_time:35290ms step_avg:39.39ms
step:897/2160 train_time:35352ms step_avg:39.41ms
step:898/2160 train_time:35411ms step_avg:39.43ms
step:899/2160 train_time:35473ms step_avg:39.46ms
step:900/2160 train_time:35534ms step_avg:39.48ms
step:901/2160 train_time:35596ms step_avg:39.51ms
step:902/2160 train_time:35656ms step_avg:39.53ms
step:903/2160 train_time:35719ms step_avg:39.56ms
step:904/2160 train_time:35778ms step_avg:39.58ms
step:905/2160 train_time:35840ms step_avg:39.60ms
step:906/2160 train_time:35899ms step_avg:39.62ms
step:907/2160 train_time:35960ms step_avg:39.65ms
step:908/2160 train_time:36020ms step_avg:39.67ms
step:909/2160 train_time:36081ms step_avg:39.69ms
step:910/2160 train_time:36140ms step_avg:39.71ms
step:911/2160 train_time:36202ms step_avg:39.74ms
step:912/2160 train_time:36262ms step_avg:39.76ms
step:913/2160 train_time:36323ms step_avg:39.78ms
step:914/2160 train_time:36384ms step_avg:39.81ms
step:915/2160 train_time:36445ms step_avg:39.83ms
step:916/2160 train_time:36505ms step_avg:39.85ms
step:917/2160 train_time:36567ms step_avg:39.88ms
step:918/2160 train_time:36627ms step_avg:39.90ms
step:919/2160 train_time:36688ms step_avg:39.92ms
step:920/2160 train_time:36748ms step_avg:39.94ms
step:921/2160 train_time:36809ms step_avg:39.97ms
step:922/2160 train_time:36869ms step_avg:39.99ms
step:923/2160 train_time:36931ms step_avg:40.01ms
step:924/2160 train_time:36991ms step_avg:40.03ms
step:925/2160 train_time:37053ms step_avg:40.06ms
step:926/2160 train_time:37113ms step_avg:40.08ms
step:927/2160 train_time:37175ms step_avg:40.10ms
step:928/2160 train_time:37235ms step_avg:40.12ms
step:929/2160 train_time:37296ms step_avg:40.15ms
step:930/2160 train_time:37355ms step_avg:40.17ms
step:931/2160 train_time:37418ms step_avg:40.19ms
step:932/2160 train_time:37477ms step_avg:40.21ms
step:933/2160 train_time:37539ms step_avg:40.24ms
step:934/2160 train_time:37599ms step_avg:40.26ms
step:935/2160 train_time:37661ms step_avg:40.28ms
step:936/2160 train_time:37719ms step_avg:40.30ms
step:937/2160 train_time:37780ms step_avg:40.32ms
step:938/2160 train_time:37840ms step_avg:40.34ms
step:939/2160 train_time:37901ms step_avg:40.36ms
step:940/2160 train_time:37960ms step_avg:40.38ms
step:941/2160 train_time:38022ms step_avg:40.41ms
step:942/2160 train_time:38082ms step_avg:40.43ms
step:943/2160 train_time:38144ms step_avg:40.45ms
step:944/2160 train_time:38204ms step_avg:40.47ms
step:945/2160 train_time:38265ms step_avg:40.49ms
step:946/2160 train_time:38325ms step_avg:40.51ms
step:947/2160 train_time:38386ms step_avg:40.53ms
step:948/2160 train_time:38446ms step_avg:40.56ms
step:949/2160 train_time:38508ms step_avg:40.58ms
step:950/2160 train_time:38568ms step_avg:40.60ms
step:951/2160 train_time:38629ms step_avg:40.62ms
step:952/2160 train_time:38688ms step_avg:40.64ms
step:953/2160 train_time:38749ms step_avg:40.66ms
step:954/2160 train_time:38808ms step_avg:40.68ms
step:955/2160 train_time:38871ms step_avg:40.70ms
step:956/2160 train_time:38930ms step_avg:40.72ms
step:957/2160 train_time:38992ms step_avg:40.74ms
step:958/2160 train_time:39052ms step_avg:40.76ms
step:959/2160 train_time:39114ms step_avg:40.79ms
step:960/2160 train_time:39173ms step_avg:40.81ms
step:961/2160 train_time:39236ms step_avg:40.83ms
step:962/2160 train_time:39297ms step_avg:40.85ms
step:963/2160 train_time:39359ms step_avg:40.87ms
step:964/2160 train_time:39419ms step_avg:40.89ms
step:965/2160 train_time:39480ms step_avg:40.91ms
step:966/2160 train_time:39540ms step_avg:40.93ms
step:967/2160 train_time:39602ms step_avg:40.95ms
step:968/2160 train_time:39660ms step_avg:40.97ms
step:969/2160 train_time:39721ms step_avg:40.99ms
step:970/2160 train_time:39781ms step_avg:41.01ms
step:971/2160 train_time:39842ms step_avg:41.03ms
step:972/2160 train_time:39901ms step_avg:41.05ms
step:973/2160 train_time:39963ms step_avg:41.07ms
step:974/2160 train_time:40023ms step_avg:41.09ms
step:975/2160 train_time:40084ms step_avg:41.11ms
step:976/2160 train_time:40145ms step_avg:41.13ms
step:977/2160 train_time:40206ms step_avg:41.15ms
step:978/2160 train_time:40267ms step_avg:41.17ms
step:979/2160 train_time:40329ms step_avg:41.19ms
step:980/2160 train_time:40389ms step_avg:41.21ms
step:981/2160 train_time:40450ms step_avg:41.23ms
step:982/2160 train_time:40510ms step_avg:41.25ms
step:983/2160 train_time:40572ms step_avg:41.27ms
step:984/2160 train_time:40632ms step_avg:41.29ms
step:985/2160 train_time:40694ms step_avg:41.31ms
step:986/2160 train_time:40753ms step_avg:41.33ms
step:987/2160 train_time:40815ms step_avg:41.35ms
step:988/2160 train_time:40876ms step_avg:41.37ms
step:989/2160 train_time:40938ms step_avg:41.39ms
step:990/2160 train_time:40998ms step_avg:41.41ms
step:991/2160 train_time:41060ms step_avg:41.43ms
step:992/2160 train_time:41119ms step_avg:41.45ms
step:993/2160 train_time:41181ms step_avg:41.47ms
step:994/2160 train_time:41240ms step_avg:41.49ms
step:995/2160 train_time:41302ms step_avg:41.51ms
step:996/2160 train_time:41361ms step_avg:41.53ms
step:997/2160 train_time:41422ms step_avg:41.55ms
step:998/2160 train_time:41482ms step_avg:41.56ms
step:999/2160 train_time:41542ms step_avg:41.58ms
step:1000/2160 train_time:41602ms step_avg:41.60ms
step:1000/2160 val_loss:3.6871 train_time:41663ms step_avg:41.66ms
step:1001/2160 train_time:41686ms step_avg:41.64ms
step:1002/2160 train_time:41725ms step_avg:41.64ms
step:1003/2160 train_time:41789ms step_avg:41.66ms
step:1004/2160 train_time:41852ms step_avg:41.69ms
step:1005/2160 train_time:41914ms step_avg:41.71ms
step:1006/2160 train_time:41975ms step_avg:41.72ms
step:1007/2160 train_time:42036ms step_avg:41.74ms
step:1008/2160 train_time:42094ms step_avg:41.76ms
step:1009/2160 train_time:42156ms step_avg:41.78ms
step:1010/2160 train_time:42214ms step_avg:41.80ms
step:1011/2160 train_time:42276ms step_avg:41.82ms
step:1012/2160 train_time:42335ms step_avg:41.83ms
step:1013/2160 train_time:42396ms step_avg:41.85ms
step:1014/2160 train_time:42456ms step_avg:41.87ms
step:1015/2160 train_time:42516ms step_avg:41.89ms
step:1016/2160 train_time:42576ms step_avg:41.91ms
step:1017/2160 train_time:42639ms step_avg:41.93ms
step:1018/2160 train_time:42700ms step_avg:41.95ms
step:1019/2160 train_time:42764ms step_avg:41.97ms
step:1020/2160 train_time:42826ms step_avg:41.99ms
step:1021/2160 train_time:42888ms step_avg:42.01ms
step:1022/2160 train_time:42948ms step_avg:42.02ms
step:1023/2160 train_time:43009ms step_avg:42.04ms
step:1024/2160 train_time:43069ms step_avg:42.06ms
step:1025/2160 train_time:43130ms step_avg:42.08ms
step:1026/2160 train_time:43190ms step_avg:42.10ms
step:1027/2160 train_time:43251ms step_avg:42.11ms
step:1028/2160 train_time:43310ms step_avg:42.13ms
step:1029/2160 train_time:43371ms step_avg:42.15ms
step:1030/2160 train_time:43431ms step_avg:42.17ms
step:1031/2160 train_time:43493ms step_avg:42.18ms
step:1032/2160 train_time:43553ms step_avg:42.20ms
step:1033/2160 train_time:43616ms step_avg:42.22ms
step:1034/2160 train_time:43677ms step_avg:42.24ms
step:1035/2160 train_time:43740ms step_avg:42.26ms
step:1036/2160 train_time:43799ms step_avg:42.28ms
step:1037/2160 train_time:43861ms step_avg:42.30ms
step:1038/2160 train_time:43921ms step_avg:42.31ms
step:1039/2160 train_time:43982ms step_avg:42.33ms
step:1040/2160 train_time:44042ms step_avg:42.35ms
step:1041/2160 train_time:44103ms step_avg:42.37ms
step:1042/2160 train_time:44163ms step_avg:42.38ms
step:1043/2160 train_time:44225ms step_avg:42.40ms
step:1044/2160 train_time:44285ms step_avg:42.42ms
step:1045/2160 train_time:44345ms step_avg:42.44ms
step:1046/2160 train_time:44405ms step_avg:42.45ms
step:1047/2160 train_time:44466ms step_avg:42.47ms
step:1048/2160 train_time:44525ms step_avg:42.49ms
step:1049/2160 train_time:44589ms step_avg:42.51ms
step:1050/2160 train_time:44650ms step_avg:42.52ms
step:1051/2160 train_time:44713ms step_avg:42.54ms
step:1052/2160 train_time:44774ms step_avg:42.56ms
step:1053/2160 train_time:44837ms step_avg:42.58ms
step:1054/2160 train_time:44897ms step_avg:42.60ms
step:1055/2160 train_time:44958ms step_avg:42.61ms
step:1056/2160 train_time:45018ms step_avg:42.63ms
step:1057/2160 train_time:45079ms step_avg:42.65ms
step:1058/2160 train_time:45139ms step_avg:42.66ms
step:1059/2160 train_time:45199ms step_avg:42.68ms
step:1060/2160 train_time:45259ms step_avg:42.70ms
step:1061/2160 train_time:45322ms step_avg:42.72ms
step:1062/2160 train_time:45382ms step_avg:42.73ms
step:1063/2160 train_time:45443ms step_avg:42.75ms
step:1064/2160 train_time:45503ms step_avg:42.77ms
step:1065/2160 train_time:45564ms step_avg:42.78ms
step:1066/2160 train_time:45625ms step_avg:42.80ms
step:1067/2160 train_time:45686ms step_avg:42.82ms
step:1068/2160 train_time:45747ms step_avg:42.83ms
step:1069/2160 train_time:45810ms step_avg:42.85ms
step:1070/2160 train_time:45869ms step_avg:42.87ms
step:1071/2160 train_time:45931ms step_avg:42.89ms
step:1072/2160 train_time:45990ms step_avg:42.90ms
step:1073/2160 train_time:46052ms step_avg:42.92ms
step:1074/2160 train_time:46111ms step_avg:42.93ms
step:1075/2160 train_time:46174ms step_avg:42.95ms
step:1076/2160 train_time:46234ms step_avg:42.97ms
step:1077/2160 train_time:46296ms step_avg:42.99ms
step:1078/2160 train_time:46355ms step_avg:43.00ms
step:1079/2160 train_time:46416ms step_avg:43.02ms
step:1080/2160 train_time:46476ms step_avg:43.03ms
step:1081/2160 train_time:46538ms step_avg:43.05ms
step:1082/2160 train_time:46597ms step_avg:43.07ms
step:1083/2160 train_time:46659ms step_avg:43.08ms
step:1084/2160 train_time:46720ms step_avg:43.10ms
step:1085/2160 train_time:46781ms step_avg:43.12ms
step:1086/2160 train_time:46842ms step_avg:43.13ms
step:1087/2160 train_time:46903ms step_avg:43.15ms
step:1088/2160 train_time:46963ms step_avg:43.16ms
step:1089/2160 train_time:47024ms step_avg:43.18ms
step:1090/2160 train_time:47085ms step_avg:43.20ms
step:1091/2160 train_time:47147ms step_avg:43.21ms
step:1092/2160 train_time:47206ms step_avg:43.23ms
step:1093/2160 train_time:47268ms step_avg:43.25ms
step:1094/2160 train_time:47327ms step_avg:43.26ms
step:1095/2160 train_time:47389ms step_avg:43.28ms
step:1096/2160 train_time:47449ms step_avg:43.29ms
step:1097/2160 train_time:47511ms step_avg:43.31ms
step:1098/2160 train_time:47571ms step_avg:43.33ms
step:1099/2160 train_time:47635ms step_avg:43.34ms
step:1100/2160 train_time:47695ms step_avg:43.36ms
step:1101/2160 train_time:47757ms step_avg:43.38ms
step:1102/2160 train_time:47816ms step_avg:43.39ms
step:1103/2160 train_time:47878ms step_avg:43.41ms
step:1104/2160 train_time:47938ms step_avg:43.42ms
step:1105/2160 train_time:47999ms step_avg:43.44ms
step:1106/2160 train_time:48059ms step_avg:43.45ms
step:1107/2160 train_time:48121ms step_avg:43.47ms
step:1108/2160 train_time:48180ms step_avg:43.48ms
step:1109/2160 train_time:48241ms step_avg:43.50ms
step:1110/2160 train_time:48301ms step_avg:43.51ms
step:1111/2160 train_time:48363ms step_avg:43.53ms
step:1112/2160 train_time:48423ms step_avg:43.55ms
step:1113/2160 train_time:48484ms step_avg:43.56ms
step:1114/2160 train_time:48544ms step_avg:43.58ms
step:1115/2160 train_time:48606ms step_avg:43.59ms
step:1116/2160 train_time:48667ms step_avg:43.61ms
step:1117/2160 train_time:48728ms step_avg:43.62ms
step:1118/2160 train_time:48788ms step_avg:43.64ms
step:1119/2160 train_time:48851ms step_avg:43.66ms
step:1120/2160 train_time:48910ms step_avg:43.67ms
step:1121/2160 train_time:48973ms step_avg:43.69ms
step:1122/2160 train_time:49034ms step_avg:43.70ms
step:1123/2160 train_time:49096ms step_avg:43.72ms
step:1124/2160 train_time:49156ms step_avg:43.73ms
step:1125/2160 train_time:49217ms step_avg:43.75ms
step:1126/2160 train_time:49276ms step_avg:43.76ms
step:1127/2160 train_time:49337ms step_avg:43.78ms
step:1128/2160 train_time:49397ms step_avg:43.79ms
step:1129/2160 train_time:49458ms step_avg:43.81ms
step:1130/2160 train_time:49518ms step_avg:43.82ms
step:1131/2160 train_time:49580ms step_avg:43.84ms
step:1132/2160 train_time:49640ms step_avg:43.85ms
step:1133/2160 train_time:49702ms step_avg:43.87ms
step:1134/2160 train_time:49762ms step_avg:43.88ms
step:1135/2160 train_time:49824ms step_avg:43.90ms
step:1136/2160 train_time:49884ms step_avg:43.91ms
step:1137/2160 train_time:49945ms step_avg:43.93ms
step:1138/2160 train_time:50006ms step_avg:43.94ms
step:1139/2160 train_time:50067ms step_avg:43.96ms
step:1140/2160 train_time:50127ms step_avg:43.97ms
step:1141/2160 train_time:50189ms step_avg:43.99ms
step:1142/2160 train_time:50249ms step_avg:44.00ms
step:1143/2160 train_time:50310ms step_avg:44.02ms
step:1144/2160 train_time:50370ms step_avg:44.03ms
step:1145/2160 train_time:50432ms step_avg:44.05ms
step:1146/2160 train_time:50492ms step_avg:44.06ms
step:1147/2160 train_time:50555ms step_avg:44.08ms
step:1148/2160 train_time:50615ms step_avg:44.09ms
step:1149/2160 train_time:50677ms step_avg:44.10ms
step:1150/2160 train_time:50737ms step_avg:44.12ms
step:1151/2160 train_time:50798ms step_avg:44.13ms
step:1152/2160 train_time:50857ms step_avg:44.15ms
step:1153/2160 train_time:50919ms step_avg:44.16ms
step:1154/2160 train_time:50978ms step_avg:44.18ms
step:1155/2160 train_time:51039ms step_avg:44.19ms
step:1156/2160 train_time:51099ms step_avg:44.20ms
step:1157/2160 train_time:51161ms step_avg:44.22ms
step:1158/2160 train_time:51221ms step_avg:44.23ms
step:1159/2160 train_time:51282ms step_avg:44.25ms
step:1160/2160 train_time:51343ms step_avg:44.26ms
step:1161/2160 train_time:51404ms step_avg:44.28ms
step:1162/2160 train_time:51464ms step_avg:44.29ms
step:1163/2160 train_time:51526ms step_avg:44.30ms
step:1164/2160 train_time:51585ms step_avg:44.32ms
step:1165/2160 train_time:51648ms step_avg:44.33ms
step:1166/2160 train_time:51708ms step_avg:44.35ms
step:1167/2160 train_time:51770ms step_avg:44.36ms
step:1168/2160 train_time:51830ms step_avg:44.37ms
step:1169/2160 train_time:51892ms step_avg:44.39ms
step:1170/2160 train_time:51952ms step_avg:44.40ms
step:1171/2160 train_time:52014ms step_avg:44.42ms
step:1172/2160 train_time:52074ms step_avg:44.43ms
step:1173/2160 train_time:52136ms step_avg:44.45ms
step:1174/2160 train_time:52195ms step_avg:44.46ms
step:1175/2160 train_time:52257ms step_avg:44.47ms
step:1176/2160 train_time:52316ms step_avg:44.49ms
step:1177/2160 train_time:52378ms step_avg:44.50ms
step:1178/2160 train_time:52437ms step_avg:44.51ms
step:1179/2160 train_time:52499ms step_avg:44.53ms
step:1180/2160 train_time:52558ms step_avg:44.54ms
step:1181/2160 train_time:52619ms step_avg:44.55ms
step:1182/2160 train_time:52679ms step_avg:44.57ms
step:1183/2160 train_time:52741ms step_avg:44.58ms
step:1184/2160 train_time:52800ms step_avg:44.59ms
step:1185/2160 train_time:52862ms step_avg:44.61ms
step:1186/2160 train_time:52922ms step_avg:44.62ms
step:1187/2160 train_time:52983ms step_avg:44.64ms
step:1188/2160 train_time:53044ms step_avg:44.65ms
step:1189/2160 train_time:53105ms step_avg:44.66ms
step:1190/2160 train_time:53165ms step_avg:44.68ms
step:1191/2160 train_time:53226ms step_avg:44.69ms
step:1192/2160 train_time:53286ms step_avg:44.70ms
step:1193/2160 train_time:53347ms step_avg:44.72ms
step:1194/2160 train_time:53407ms step_avg:44.73ms
step:1195/2160 train_time:53469ms step_avg:44.74ms
step:1196/2160 train_time:53529ms step_avg:44.76ms
step:1197/2160 train_time:53591ms step_avg:44.77ms
step:1198/2160 train_time:53651ms step_avg:44.78ms
step:1199/2160 train_time:53713ms step_avg:44.80ms
step:1200/2160 train_time:53773ms step_avg:44.81ms
step:1201/2160 train_time:53836ms step_avg:44.83ms
step:1202/2160 train_time:53896ms step_avg:44.84ms
step:1203/2160 train_time:53958ms step_avg:44.85ms
step:1204/2160 train_time:54017ms step_avg:44.86ms
step:1205/2160 train_time:54078ms step_avg:44.88ms
step:1206/2160 train_time:54137ms step_avg:44.89ms
step:1207/2160 train_time:54199ms step_avg:44.90ms
step:1208/2160 train_time:54258ms step_avg:44.92ms
step:1209/2160 train_time:54319ms step_avg:44.93ms
step:1210/2160 train_time:54379ms step_avg:44.94ms
step:1211/2160 train_time:54440ms step_avg:44.95ms
step:1212/2160 train_time:54500ms step_avg:44.97ms
step:1213/2160 train_time:54561ms step_avg:44.98ms
step:1214/2160 train_time:54622ms step_avg:44.99ms
step:1215/2160 train_time:54684ms step_avg:45.01ms
step:1216/2160 train_time:54744ms step_avg:45.02ms
step:1217/2160 train_time:54805ms step_avg:45.03ms
step:1218/2160 train_time:54865ms step_avg:45.05ms
step:1219/2160 train_time:54927ms step_avg:45.06ms
step:1220/2160 train_time:54986ms step_avg:45.07ms
step:1221/2160 train_time:55049ms step_avg:45.09ms
step:1222/2160 train_time:55109ms step_avg:45.10ms
step:1223/2160 train_time:55171ms step_avg:45.11ms
step:1224/2160 train_time:55231ms step_avg:45.12ms
step:1225/2160 train_time:55293ms step_avg:45.14ms
step:1226/2160 train_time:55353ms step_avg:45.15ms
step:1227/2160 train_time:55415ms step_avg:45.16ms
step:1228/2160 train_time:55474ms step_avg:45.17ms
step:1229/2160 train_time:55536ms step_avg:45.19ms
step:1230/2160 train_time:55595ms step_avg:45.20ms
step:1231/2160 train_time:55657ms step_avg:45.21ms
step:1232/2160 train_time:55717ms step_avg:45.22ms
step:1233/2160 train_time:55778ms step_avg:45.24ms
step:1234/2160 train_time:55837ms step_avg:45.25ms
step:1235/2160 train_time:55899ms step_avg:45.26ms
step:1236/2160 train_time:55959ms step_avg:45.27ms
step:1237/2160 train_time:56020ms step_avg:45.29ms
step:1238/2160 train_time:56080ms step_avg:45.30ms
step:1239/2160 train_time:56141ms step_avg:45.31ms
step:1240/2160 train_time:56201ms step_avg:45.32ms
step:1241/2160 train_time:56262ms step_avg:45.34ms
step:1242/2160 train_time:56322ms step_avg:45.35ms
step:1243/2160 train_time:56384ms step_avg:45.36ms
step:1244/2160 train_time:56443ms step_avg:45.37ms
step:1245/2160 train_time:56505ms step_avg:45.39ms
step:1246/2160 train_time:56565ms step_avg:45.40ms
step:1247/2160 train_time:56627ms step_avg:45.41ms
step:1248/2160 train_time:56687ms step_avg:45.42ms
step:1249/2160 train_time:56749ms step_avg:45.44ms
step:1250/2160 train_time:56808ms step_avg:45.45ms
step:1250/2160 val_loss:3.5708 train_time:56870ms step_avg:45.50ms
step:1251/2160 train_time:56893ms step_avg:45.48ms
step:1252/2160 train_time:56932ms step_avg:45.47ms
step:1253/2160 train_time:56998ms step_avg:45.49ms
step:1254/2160 train_time:57062ms step_avg:45.50ms
step:1255/2160 train_time:57123ms step_avg:45.52ms
step:1256/2160 train_time:57183ms step_avg:45.53ms
step:1257/2160 train_time:57243ms step_avg:45.54ms
step:1258/2160 train_time:57302ms step_avg:45.55ms
step:1259/2160 train_time:57363ms step_avg:45.56ms
step:1260/2160 train_time:57421ms step_avg:45.57ms
step:1261/2160 train_time:57482ms step_avg:45.58ms
step:1262/2160 train_time:57541ms step_avg:45.60ms
step:1263/2160 train_time:57602ms step_avg:45.61ms
step:1264/2160 train_time:57662ms step_avg:45.62ms
step:1265/2160 train_time:57722ms step_avg:45.63ms
step:1266/2160 train_time:57783ms step_avg:45.64ms
step:1267/2160 train_time:57846ms step_avg:45.66ms
step:1268/2160 train_time:57906ms step_avg:45.67ms
step:1269/2160 train_time:57972ms step_avg:45.68ms
step:1270/2160 train_time:58034ms step_avg:45.70ms
step:1271/2160 train_time:58096ms step_avg:45.71ms
step:1272/2160 train_time:58155ms step_avg:45.72ms
step:1273/2160 train_time:58217ms step_avg:45.73ms
step:1274/2160 train_time:58276ms step_avg:45.74ms
step:1275/2160 train_time:58336ms step_avg:45.75ms
step:1276/2160 train_time:58395ms step_avg:45.76ms
step:1277/2160 train_time:58456ms step_avg:45.78ms
step:1278/2160 train_time:58515ms step_avg:45.79ms
step:1279/2160 train_time:58576ms step_avg:45.80ms
step:1280/2160 train_time:58634ms step_avg:45.81ms
step:1281/2160 train_time:58695ms step_avg:45.82ms
step:1282/2160 train_time:58755ms step_avg:45.83ms
step:1283/2160 train_time:58818ms step_avg:45.84ms
step:1284/2160 train_time:58879ms step_avg:45.86ms
step:1285/2160 train_time:58942ms step_avg:45.87ms
step:1286/2160 train_time:59002ms step_avg:45.88ms
step:1287/2160 train_time:59064ms step_avg:45.89ms
step:1288/2160 train_time:59124ms step_avg:45.90ms
step:1289/2160 train_time:59187ms step_avg:45.92ms
step:1290/2160 train_time:59247ms step_avg:45.93ms
step:1291/2160 train_time:59309ms step_avg:45.94ms
step:1292/2160 train_time:59368ms step_avg:45.95ms
step:1293/2160 train_time:59430ms step_avg:45.96ms
step:1294/2160 train_time:59489ms step_avg:45.97ms
step:1295/2160 train_time:59549ms step_avg:45.98ms
step:1296/2160 train_time:59608ms step_avg:45.99ms
step:1297/2160 train_time:59670ms step_avg:46.01ms
step:1298/2160 train_time:59730ms step_avg:46.02ms
step:1299/2160 train_time:59792ms step_avg:46.03ms
step:1300/2160 train_time:59853ms step_avg:46.04ms
step:1301/2160 train_time:59915ms step_avg:46.05ms
step:1302/2160 train_time:59975ms step_avg:46.06ms
step:1303/2160 train_time:60037ms step_avg:46.08ms
step:1304/2160 train_time:60096ms step_avg:46.09ms
step:1305/2160 train_time:60158ms step_avg:46.10ms
step:1306/2160 train_time:60218ms step_avg:46.11ms
step:1307/2160 train_time:60279ms step_avg:46.12ms
step:1308/2160 train_time:60339ms step_avg:46.13ms
step:1309/2160 train_time:60400ms step_avg:46.14ms
step:1310/2160 train_time:60460ms step_avg:46.15ms
step:1311/2160 train_time:60521ms step_avg:46.16ms
step:1312/2160 train_time:60581ms step_avg:46.17ms
step:1313/2160 train_time:60642ms step_avg:46.19ms
step:1314/2160 train_time:60702ms step_avg:46.20ms
step:1315/2160 train_time:60763ms step_avg:46.21ms
step:1316/2160 train_time:60823ms step_avg:46.22ms
step:1317/2160 train_time:60886ms step_avg:46.23ms
step:1318/2160 train_time:60946ms step_avg:46.24ms
step:1319/2160 train_time:61009ms step_avg:46.25ms
step:1320/2160 train_time:61069ms step_avg:46.26ms
step:1321/2160 train_time:61132ms step_avg:46.28ms
step:1322/2160 train_time:61192ms step_avg:46.29ms
step:1323/2160 train_time:61254ms step_avg:46.30ms
step:1324/2160 train_time:61314ms step_avg:46.31ms
step:1325/2160 train_time:61375ms step_avg:46.32ms
step:1326/2160 train_time:61434ms step_avg:46.33ms
step:1327/2160 train_time:61495ms step_avg:46.34ms
step:1328/2160 train_time:61555ms step_avg:46.35ms
step:1329/2160 train_time:61616ms step_avg:46.36ms
step:1330/2160 train_time:61676ms step_avg:46.37ms
step:1331/2160 train_time:61737ms step_avg:46.38ms
step:1332/2160 train_time:61796ms step_avg:46.39ms
step:1333/2160 train_time:61857ms step_avg:46.40ms
step:1334/2160 train_time:61917ms step_avg:46.41ms
step:1335/2160 train_time:61979ms step_avg:46.43ms
step:1336/2160 train_time:62040ms step_avg:46.44ms
step:1337/2160 train_time:62102ms step_avg:46.45ms
step:1338/2160 train_time:62162ms step_avg:46.46ms
step:1339/2160 train_time:62223ms step_avg:46.47ms
step:1340/2160 train_time:62283ms step_avg:46.48ms
step:1341/2160 train_time:62345ms step_avg:46.49ms
step:1342/2160 train_time:62405ms step_avg:46.50ms
step:1343/2160 train_time:62467ms step_avg:46.51ms
step:1344/2160 train_time:62526ms step_avg:46.52ms
step:1345/2160 train_time:62588ms step_avg:46.53ms
step:1346/2160 train_time:62647ms step_avg:46.54ms
step:1347/2160 train_time:62709ms step_avg:46.55ms
step:1348/2160 train_time:62769ms step_avg:46.56ms
step:1349/2160 train_time:62831ms step_avg:46.58ms
step:1350/2160 train_time:62891ms step_avg:46.59ms
step:1351/2160 train_time:62953ms step_avg:46.60ms
step:1352/2160 train_time:63013ms step_avg:46.61ms
step:1353/2160 train_time:63075ms step_avg:46.62ms
step:1354/2160 train_time:63135ms step_avg:46.63ms
step:1355/2160 train_time:63196ms step_avg:46.64ms
step:1356/2160 train_time:63255ms step_avg:46.65ms
step:1357/2160 train_time:63317ms step_avg:46.66ms
step:1358/2160 train_time:63377ms step_avg:46.67ms
step:1359/2160 train_time:63438ms step_avg:46.68ms
step:1360/2160 train_time:63498ms step_avg:46.69ms
step:1361/2160 train_time:63559ms step_avg:46.70ms
step:1362/2160 train_time:63619ms step_avg:46.71ms
step:1363/2160 train_time:63680ms step_avg:46.72ms
step:1364/2160 train_time:63739ms step_avg:46.73ms
step:1365/2160 train_time:63801ms step_avg:46.74ms
step:1366/2160 train_time:63861ms step_avg:46.75ms
step:1367/2160 train_time:63922ms step_avg:46.76ms
step:1368/2160 train_time:63982ms step_avg:46.77ms
step:1369/2160 train_time:64044ms step_avg:46.78ms
step:1370/2160 train_time:64104ms step_avg:46.79ms
step:1371/2160 train_time:64165ms step_avg:46.80ms
step:1372/2160 train_time:64225ms step_avg:46.81ms
step:1373/2160 train_time:64287ms step_avg:46.82ms
step:1374/2160 train_time:64347ms step_avg:46.83ms
step:1375/2160 train_time:64410ms step_avg:46.84ms
step:1376/2160 train_time:64470ms step_avg:46.85ms
step:1377/2160 train_time:64533ms step_avg:46.86ms
step:1378/2160 train_time:64592ms step_avg:46.87ms
step:1379/2160 train_time:64654ms step_avg:46.88ms
step:1380/2160 train_time:64713ms step_avg:46.89ms
step:1381/2160 train_time:64774ms step_avg:46.90ms
step:1382/2160 train_time:64834ms step_avg:46.91ms
step:1383/2160 train_time:64895ms step_avg:46.92ms
step:1384/2160 train_time:64954ms step_avg:46.93ms
step:1385/2160 train_time:65016ms step_avg:46.94ms
step:1386/2160 train_time:65075ms step_avg:46.95ms
step:1387/2160 train_time:65137ms step_avg:46.96ms
step:1388/2160 train_time:65197ms step_avg:46.97ms
step:1389/2160 train_time:65258ms step_avg:46.98ms
step:1390/2160 train_time:65319ms step_avg:46.99ms
step:1391/2160 train_time:65380ms step_avg:47.00ms
step:1392/2160 train_time:65440ms step_avg:47.01ms
step:1393/2160 train_time:65501ms step_avg:47.02ms
step:1394/2160 train_time:65561ms step_avg:47.03ms
step:1395/2160 train_time:65623ms step_avg:47.04ms
step:1396/2160 train_time:65683ms step_avg:47.05ms
step:1397/2160 train_time:65744ms step_avg:47.06ms
step:1398/2160 train_time:65804ms step_avg:47.07ms
step:1399/2160 train_time:65865ms step_avg:47.08ms
step:1400/2160 train_time:65926ms step_avg:47.09ms
step:1401/2160 train_time:65988ms step_avg:47.10ms
step:1402/2160 train_time:66048ms step_avg:47.11ms
step:1403/2160 train_time:66109ms step_avg:47.12ms
step:1404/2160 train_time:66170ms step_avg:47.13ms
step:1405/2160 train_time:66231ms step_avg:47.14ms
step:1406/2160 train_time:66291ms step_avg:47.15ms
step:1407/2160 train_time:66353ms step_avg:47.16ms
step:1408/2160 train_time:66413ms step_avg:47.17ms
step:1409/2160 train_time:66474ms step_avg:47.18ms
step:1410/2160 train_time:66534ms step_avg:47.19ms
step:1411/2160 train_time:66595ms step_avg:47.20ms
step:1412/2160 train_time:66655ms step_avg:47.21ms
step:1413/2160 train_time:66716ms step_avg:47.22ms
step:1414/2160 train_time:66776ms step_avg:47.22ms
step:1415/2160 train_time:66838ms step_avg:47.24ms
step:1416/2160 train_time:66926ms step_avg:47.26ms
step:1417/2160 train_time:67016ms step_avg:47.29ms
step:1418/2160 train_time:67104ms step_avg:47.32ms
step:1419/2160 train_time:67194ms step_avg:47.35ms
step:1420/2160 train_time:67282ms step_avg:47.38ms
step:1421/2160 train_time:67371ms step_avg:47.41ms
step:1422/2160 train_time:67458ms step_avg:47.44ms
step:1423/2160 train_time:67548ms step_avg:47.47ms
step:1424/2160 train_time:67635ms step_avg:47.50ms
step:1425/2160 train_time:67725ms step_avg:47.53ms
step:1426/2160 train_time:67812ms step_avg:47.55ms
step:1427/2160 train_time:67902ms step_avg:47.58ms
step:1428/2160 train_time:67989ms step_avg:47.61ms
step:1429/2160 train_time:68080ms step_avg:47.64ms
step:1430/2160 train_time:68168ms step_avg:47.67ms
step:1431/2160 train_time:68258ms step_avg:47.70ms
step:1432/2160 train_time:68346ms step_avg:47.73ms
step:1433/2160 train_time:68435ms step_avg:47.76ms
step:1434/2160 train_time:68524ms step_avg:47.79ms
step:1435/2160 train_time:68613ms step_avg:47.81ms
step:1436/2160 train_time:68700ms step_avg:47.84ms
step:1437/2160 train_time:68789ms step_avg:47.87ms
step:1438/2160 train_time:68877ms step_avg:47.90ms
step:1439/2160 train_time:68967ms step_avg:47.93ms
step:1440/2160 train_time:69055ms step_avg:47.95ms
step:1441/2160 train_time:69144ms step_avg:47.98ms
step:1442/2160 train_time:69232ms step_avg:48.01ms
step:1443/2160 train_time:69322ms step_avg:48.04ms
step:1444/2160 train_time:69409ms step_avg:48.07ms
step:1445/2160 train_time:69499ms step_avg:48.10ms
step:1446/2160 train_time:69586ms step_avg:48.12ms
step:1447/2160 train_time:69676ms step_avg:48.15ms
step:1448/2160 train_time:69764ms step_avg:48.18ms
step:1449/2160 train_time:69853ms step_avg:48.21ms
step:1450/2160 train_time:69941ms step_avg:48.24ms
step:1451/2160 train_time:70030ms step_avg:48.26ms
step:1452/2160 train_time:70118ms step_avg:48.29ms
step:1453/2160 train_time:70208ms step_avg:48.32ms
step:1454/2160 train_time:70296ms step_avg:48.35ms
step:1455/2160 train_time:70387ms step_avg:48.38ms
step:1456/2160 train_time:70475ms step_avg:48.40ms
step:1457/2160 train_time:70565ms step_avg:48.43ms
step:1458/2160 train_time:70652ms step_avg:48.46ms
step:1459/2160 train_time:70741ms step_avg:48.49ms
step:1460/2160 train_time:70829ms step_avg:48.51ms
step:1461/2160 train_time:70918ms step_avg:48.54ms
step:1462/2160 train_time:71005ms step_avg:48.57ms
step:1463/2160 train_time:71094ms step_avg:48.59ms
step:1464/2160 train_time:71181ms step_avg:48.62ms
step:1465/2160 train_time:71271ms step_avg:48.65ms
step:1466/2160 train_time:71358ms step_avg:48.68ms
step:1467/2160 train_time:71447ms step_avg:48.70ms
step:1468/2160 train_time:71535ms step_avg:48.73ms
step:1469/2160 train_time:71624ms step_avg:48.76ms
step:1470/2160 train_time:71711ms step_avg:48.78ms
step:1471/2160 train_time:71801ms step_avg:48.81ms
step:1472/2160 train_time:71888ms step_avg:48.84ms
step:1473/2160 train_time:71978ms step_avg:48.86ms
step:1474/2160 train_time:72066ms step_avg:48.89ms
step:1475/2160 train_time:72155ms step_avg:48.92ms
step:1476/2160 train_time:72244ms step_avg:48.95ms
step:1477/2160 train_time:72333ms step_avg:48.97ms
step:1478/2160 train_time:72421ms step_avg:49.00ms
step:1479/2160 train_time:72510ms step_avg:49.03ms
step:1480/2160 train_time:72596ms step_avg:49.05ms
step:1481/2160 train_time:72687ms step_avg:49.08ms
step:1482/2160 train_time:72775ms step_avg:49.11ms
step:1483/2160 train_time:72865ms step_avg:49.13ms
step:1484/2160 train_time:72951ms step_avg:49.16ms
step:1485/2160 train_time:73042ms step_avg:49.19ms
step:1486/2160 train_time:73128ms step_avg:49.21ms
step:1487/2160 train_time:73218ms step_avg:49.24ms
step:1488/2160 train_time:73308ms step_avg:49.27ms
step:1489/2160 train_time:73397ms step_avg:49.29ms
step:1490/2160 train_time:73484ms step_avg:49.32ms
step:1491/2160 train_time:73573ms step_avg:49.34ms
step:1492/2160 train_time:73661ms step_avg:49.37ms
step:1493/2160 train_time:73749ms step_avg:49.40ms
step:1494/2160 train_time:73837ms step_avg:49.42ms
step:1495/2160 train_time:73926ms step_avg:49.45ms
step:1496/2160 train_time:74013ms step_avg:49.47ms
step:1497/2160 train_time:74103ms step_avg:49.50ms
step:1498/2160 train_time:74189ms step_avg:49.53ms
step:1499/2160 train_time:74280ms step_avg:49.55ms
step:1500/2160 train_time:74367ms step_avg:49.58ms
step:1500/2160 val_loss:3.4705 train_time:74456ms step_avg:49.64ms
step:1501/2160 train_time:74479ms step_avg:49.62ms
step:1502/2160 train_time:74548ms step_avg:49.63ms
step:1503/2160 train_time:74644ms step_avg:49.66ms
step:1504/2160 train_time:74733ms step_avg:49.69ms
step:1505/2160 train_time:74822ms step_avg:49.72ms
step:1506/2160 train_time:74908ms step_avg:49.74ms
step:1507/2160 train_time:74996ms step_avg:49.76ms
step:1508/2160 train_time:75082ms step_avg:49.79ms
step:1509/2160 train_time:75170ms step_avg:49.81ms
step:1510/2160 train_time:75256ms step_avg:49.84ms
step:1511/2160 train_time:75344ms step_avg:49.86ms
step:1512/2160 train_time:75433ms step_avg:49.89ms
step:1513/2160 train_time:75524ms step_avg:49.92ms
step:1514/2160 train_time:75616ms step_avg:49.94ms
step:1515/2160 train_time:75706ms step_avg:49.97ms
step:1516/2160 train_time:75794ms step_avg:50.00ms
step:1517/2160 train_time:75882ms step_avg:50.02ms
step:1518/2160 train_time:75969ms step_avg:50.05ms
step:1519/2160 train_time:76058ms step_avg:50.07ms
step:1520/2160 train_time:76144ms step_avg:50.09ms
step:1521/2160 train_time:76233ms step_avg:50.12ms
step:1522/2160 train_time:76320ms step_avg:50.14ms
step:1523/2160 train_time:76410ms step_avg:50.17ms
step:1524/2160 train_time:76499ms step_avg:50.20ms
step:1525/2160 train_time:76591ms step_avg:50.22ms
step:1526/2160 train_time:76681ms step_avg:50.25ms
step:1527/2160 train_time:76773ms step_avg:50.28ms
step:1528/2160 train_time:76859ms step_avg:50.30ms
step:1529/2160 train_time:76948ms step_avg:50.33ms
step:1530/2160 train_time:77034ms step_avg:50.35ms
step:1531/2160 train_time:77122ms step_avg:50.37ms
step:1532/2160 train_time:77209ms step_avg:50.40ms
step:1533/2160 train_time:77299ms step_avg:50.42ms
step:1534/2160 train_time:77386ms step_avg:50.45ms
step:1535/2160 train_time:77477ms step_avg:50.47ms
step:1536/2160 train_time:77567ms step_avg:50.50ms
step:1537/2160 train_time:77658ms step_avg:50.53ms
step:1538/2160 train_time:77747ms step_avg:50.55ms
step:1539/2160 train_time:77838ms step_avg:50.58ms
step:1540/2160 train_time:77925ms step_avg:50.60ms
step:1541/2160 train_time:78013ms step_avg:50.63ms
step:1542/2160 train_time:78099ms step_avg:50.65ms
step:1543/2160 train_time:78187ms step_avg:50.67ms
step:1544/2160 train_time:78274ms step_avg:50.70ms
step:1545/2160 train_time:78363ms step_avg:50.72ms
step:1546/2160 train_time:78450ms step_avg:50.74ms
step:1547/2160 train_time:78542ms step_avg:50.77ms
step:1548/2160 train_time:78632ms step_avg:50.80ms
step:1549/2160 train_time:78723ms step_avg:50.82ms
step:1550/2160 train_time:78813ms step_avg:50.85ms
step:1551/2160 train_time:78901ms step_avg:50.87ms
step:1552/2160 train_time:78989ms step_avg:50.89ms
step:1553/2160 train_time:79077ms step_avg:50.92ms
step:1554/2160 train_time:79164ms step_avg:50.94ms
step:1555/2160 train_time:79252ms step_avg:50.97ms
step:1556/2160 train_time:79340ms step_avg:50.99ms
step:1557/2160 train_time:79430ms step_avg:51.01ms
step:1558/2160 train_time:79518ms step_avg:51.04ms
step:1559/2160 train_time:79608ms step_avg:51.06ms
step:1560/2160 train_time:79696ms step_avg:51.09ms
step:1561/2160 train_time:79786ms step_avg:51.11ms
step:1562/2160 train_time:79873ms step_avg:51.14ms
step:1563/2160 train_time:79963ms step_avg:51.16ms
step:1564/2160 train_time:80050ms step_avg:51.18ms
step:1565/2160 train_time:80139ms step_avg:51.21ms
step:1566/2160 train_time:80226ms step_avg:51.23ms
step:1567/2160 train_time:80314ms step_avg:51.25ms
step:1568/2160 train_time:80400ms step_avg:51.28ms
step:1569/2160 train_time:80489ms step_avg:51.30ms
step:1570/2160 train_time:80577ms step_avg:51.32ms
step:1571/2160 train_time:80668ms step_avg:51.35ms
step:1572/2160 train_time:80756ms step_avg:51.37ms
step:1573/2160 train_time:80845ms step_avg:51.40ms
step:1574/2160 train_time:80932ms step_avg:51.42ms
step:1575/2160 train_time:81021ms step_avg:51.44ms
step:1576/2160 train_time:81107ms step_avg:51.46ms
step:1577/2160 train_time:81196ms step_avg:51.49ms
step:1578/2160 train_time:81283ms step_avg:51.51ms
step:1579/2160 train_time:81372ms step_avg:51.53ms
step:1580/2160 train_time:81458ms step_avg:51.56ms
step:1581/2160 train_time:81548ms step_avg:51.58ms
step:1582/2160 train_time:81636ms step_avg:51.60ms
step:1583/2160 train_time:81725ms step_avg:51.63ms
step:1584/2160 train_time:81814ms step_avg:51.65ms
step:1585/2160 train_time:81903ms step_avg:51.67ms
step:1586/2160 train_time:81990ms step_avg:51.70ms
step:1587/2160 train_time:82081ms step_avg:51.72ms
step:1588/2160 train_time:82168ms step_avg:51.74ms
step:1589/2160 train_time:82257ms step_avg:51.77ms
step:1590/2160 train_time:82344ms step_avg:51.79ms
step:1591/2160 train_time:82433ms step_avg:51.81ms
step:1592/2160 train_time:82520ms step_avg:51.83ms
step:1593/2160 train_time:82610ms step_avg:51.86ms
step:1594/2160 train_time:82697ms step_avg:51.88ms
step:1595/2160 train_time:82787ms step_avg:51.90ms
step:1596/2160 train_time:82875ms step_avg:51.93ms
step:1597/2160 train_time:82964ms step_avg:51.95ms
step:1598/2160 train_time:83052ms step_avg:51.97ms
step:1599/2160 train_time:83142ms step_avg:52.00ms
step:1600/2160 train_time:83229ms step_avg:52.02ms
step:1601/2160 train_time:83319ms step_avg:52.04ms
step:1602/2160 train_time:83406ms step_avg:52.06ms
step:1603/2160 train_time:83495ms step_avg:52.09ms
step:1604/2160 train_time:83582ms step_avg:52.11ms
step:1605/2160 train_time:83673ms step_avg:52.13ms
step:1606/2160 train_time:83761ms step_avg:52.16ms
step:1607/2160 train_time:83851ms step_avg:52.18ms
step:1608/2160 train_time:83938ms step_avg:52.20ms
step:1609/2160 train_time:84027ms step_avg:52.22ms
step:1610/2160 train_time:84115ms step_avg:52.25ms
step:1611/2160 train_time:84204ms step_avg:52.27ms
step:1612/2160 train_time:84291ms step_avg:52.29ms
step:1613/2160 train_time:84381ms step_avg:52.31ms
step:1614/2160 train_time:84469ms step_avg:52.34ms
step:1615/2160 train_time:84558ms step_avg:52.36ms
step:1616/2160 train_time:84646ms step_avg:52.38ms
step:1617/2160 train_time:84736ms step_avg:52.40ms
step:1618/2160 train_time:84823ms step_avg:52.42ms
step:1619/2160 train_time:84912ms step_avg:52.45ms
step:1620/2160 train_time:84999ms step_avg:52.47ms
step:1621/2160 train_time:85088ms step_avg:52.49ms
step:1622/2160 train_time:85175ms step_avg:52.51ms
step:1623/2160 train_time:85264ms step_avg:52.53ms
step:1624/2160 train_time:85351ms step_avg:52.56ms
step:1625/2160 train_time:85441ms step_avg:52.58ms
step:1626/2160 train_time:85530ms step_avg:52.60ms
step:1627/2160 train_time:85620ms step_avg:52.62ms
step:1628/2160 train_time:85707ms step_avg:52.65ms
step:1629/2160 train_time:85797ms step_avg:52.67ms
step:1630/2160 train_time:85885ms step_avg:52.69ms
step:1631/2160 train_time:85975ms step_avg:52.71ms
step:1632/2160 train_time:86061ms step_avg:52.73ms
step:1633/2160 train_time:86151ms step_avg:52.76ms
step:1634/2160 train_time:86239ms step_avg:52.78ms
step:1635/2160 train_time:86329ms step_avg:52.80ms
step:1636/2160 train_time:86415ms step_avg:52.82ms
step:1637/2160 train_time:86504ms step_avg:52.84ms
step:1638/2160 train_time:86592ms step_avg:52.86ms
step:1639/2160 train_time:86682ms step_avg:52.89ms
step:1640/2160 train_time:86769ms step_avg:52.91ms
step:1641/2160 train_time:86858ms step_avg:52.93ms
step:1642/2160 train_time:86945ms step_avg:52.95ms
step:1643/2160 train_time:87034ms step_avg:52.97ms
step:1644/2160 train_time:87121ms step_avg:52.99ms
step:1645/2160 train_time:87210ms step_avg:53.02ms
step:1646/2160 train_time:87298ms step_avg:53.04ms
step:1647/2160 train_time:87387ms step_avg:53.06ms
step:1648/2160 train_time:87474ms step_avg:53.08ms
step:1649/2160 train_time:87564ms step_avg:53.10ms
step:1650/2160 train_time:87651ms step_avg:53.12ms
step:1651/2160 train_time:87742ms step_avg:53.14ms
step:1652/2160 train_time:87829ms step_avg:53.17ms
step:1653/2160 train_time:87920ms step_avg:53.19ms
step:1654/2160 train_time:88007ms step_avg:53.21ms
step:1655/2160 train_time:88096ms step_avg:53.23ms
step:1656/2160 train_time:88184ms step_avg:53.25ms
step:1657/2160 train_time:88273ms step_avg:53.27ms
step:1658/2160 train_time:88360ms step_avg:53.29ms
step:1659/2160 train_time:88450ms step_avg:53.32ms
step:1660/2160 train_time:88538ms step_avg:53.34ms
step:1661/2160 train_time:88627ms step_avg:53.36ms
step:1662/2160 train_time:88714ms step_avg:53.38ms
step:1663/2160 train_time:88803ms step_avg:53.40ms
step:1664/2160 train_time:88890ms step_avg:53.42ms
step:1665/2160 train_time:88980ms step_avg:53.44ms
step:1666/2160 train_time:89067ms step_avg:53.46ms
step:1667/2160 train_time:89157ms step_avg:53.48ms
step:1668/2160 train_time:89243ms step_avg:53.50ms
step:1669/2160 train_time:89332ms step_avg:53.52ms
step:1670/2160 train_time:89421ms step_avg:53.55ms
step:1671/2160 train_time:89511ms step_avg:53.57ms
step:1672/2160 train_time:89598ms step_avg:53.59ms
step:1673/2160 train_time:89687ms step_avg:53.61ms
step:1674/2160 train_time:89774ms step_avg:53.63ms
step:1675/2160 train_time:89863ms step_avg:53.65ms
step:1676/2160 train_time:89951ms step_avg:53.67ms
step:1677/2160 train_time:90041ms step_avg:53.69ms
step:1678/2160 train_time:90128ms step_avg:53.71ms
step:1679/2160 train_time:90218ms step_avg:53.73ms
step:1680/2160 train_time:90305ms step_avg:53.75ms
step:1681/2160 train_time:90395ms step_avg:53.77ms
step:1682/2160 train_time:90483ms step_avg:53.79ms
step:1683/2160 train_time:90573ms step_avg:53.82ms
step:1684/2160 train_time:90660ms step_avg:53.84ms
step:1685/2160 train_time:90749ms step_avg:53.86ms
step:1686/2160 train_time:90837ms step_avg:53.88ms
step:1687/2160 train_time:90925ms step_avg:53.90ms
step:1688/2160 train_time:91013ms step_avg:53.92ms
step:1689/2160 train_time:91102ms step_avg:53.94ms
step:1690/2160 train_time:91190ms step_avg:53.96ms
step:1691/2160 train_time:91280ms step_avg:53.98ms
step:1692/2160 train_time:91367ms step_avg:54.00ms
step:1693/2160 train_time:91457ms step_avg:54.02ms
step:1694/2160 train_time:91544ms step_avg:54.04ms
step:1695/2160 train_time:91634ms step_avg:54.06ms
step:1696/2160 train_time:91721ms step_avg:54.08ms
step:1697/2160 train_time:91810ms step_avg:54.10ms
step:1698/2160 train_time:91897ms step_avg:54.12ms
step:1699/2160 train_time:91986ms step_avg:54.14ms
step:1700/2160 train_time:92073ms step_avg:54.16ms
step:1701/2160 train_time:92162ms step_avg:54.18ms
step:1702/2160 train_time:92249ms step_avg:54.20ms
step:1703/2160 train_time:92339ms step_avg:54.22ms
step:1704/2160 train_time:92426ms step_avg:54.24ms
step:1705/2160 train_time:92516ms step_avg:54.26ms
step:1706/2160 train_time:92603ms step_avg:54.28ms
step:1707/2160 train_time:92693ms step_avg:54.30ms
step:1708/2160 train_time:92780ms step_avg:54.32ms
step:1709/2160 train_time:92870ms step_avg:54.34ms
step:1710/2160 train_time:92957ms step_avg:54.36ms
step:1711/2160 train_time:93046ms step_avg:54.38ms
step:1712/2160 train_time:93134ms step_avg:54.40ms
step:1713/2160 train_time:93223ms step_avg:54.42ms
step:1714/2160 train_time:93310ms step_avg:54.44ms
step:1715/2160 train_time:93400ms step_avg:54.46ms
step:1716/2160 train_time:93487ms step_avg:54.48ms
step:1717/2160 train_time:93576ms step_avg:54.50ms
step:1718/2160 train_time:93663ms step_avg:54.52ms
step:1719/2160 train_time:93753ms step_avg:54.54ms
step:1720/2160 train_time:93840ms step_avg:54.56ms
step:1721/2160 train_time:93930ms step_avg:54.58ms
step:1722/2160 train_time:94017ms step_avg:54.60ms
step:1723/2160 train_time:94107ms step_avg:54.62ms
step:1724/2160 train_time:94194ms step_avg:54.64ms
step:1725/2160 train_time:94283ms step_avg:54.66ms
step:1726/2160 train_time:94370ms step_avg:54.68ms
step:1727/2160 train_time:94460ms step_avg:54.70ms
step:1728/2160 train_time:94547ms step_avg:54.71ms
step:1729/2160 train_time:94637ms step_avg:54.74ms
step:1730/2160 train_time:94724ms step_avg:54.75ms
step:1731/2160 train_time:94813ms step_avg:54.77ms
step:1732/2160 train_time:94899ms step_avg:54.79ms
step:1733/2160 train_time:94989ms step_avg:54.81ms
step:1734/2160 train_time:95077ms step_avg:54.83ms
step:1735/2160 train_time:95165ms step_avg:54.85ms
step:1736/2160 train_time:95252ms step_avg:54.87ms
step:1737/2160 train_time:95341ms step_avg:54.89ms
step:1738/2160 train_time:95429ms step_avg:54.91ms
step:1739/2160 train_time:95519ms step_avg:54.93ms
step:1740/2160 train_time:95606ms step_avg:54.95ms
step:1741/2160 train_time:95697ms step_avg:54.97ms
step:1742/2160 train_time:95783ms step_avg:54.98ms
step:1743/2160 train_time:95872ms step_avg:55.00ms
step:1744/2160 train_time:95960ms step_avg:55.02ms
step:1745/2160 train_time:96049ms step_avg:55.04ms
step:1746/2160 train_time:96136ms step_avg:55.06ms
step:1747/2160 train_time:96225ms step_avg:55.08ms
step:1748/2160 train_time:96312ms step_avg:55.10ms
step:1749/2160 train_time:96401ms step_avg:55.12ms
step:1750/2160 train_time:96488ms step_avg:55.14ms
step:1750/2160 val_loss:3.3798 train_time:96578ms step_avg:55.19ms
step:1751/2160 train_time:96601ms step_avg:55.17ms
step:1752/2160 train_time:96669ms step_avg:55.18ms
step:1753/2160 train_time:96763ms step_avg:55.20ms
step:1754/2160 train_time:96851ms step_avg:55.22ms
step:1755/2160 train_time:96940ms step_avg:55.24ms
step:1756/2160 train_time:97026ms step_avg:55.25ms
step:1757/2160 train_time:97115ms step_avg:55.27ms
step:1758/2160 train_time:97201ms step_avg:55.29ms
step:1759/2160 train_time:97288ms step_avg:55.31ms
step:1760/2160 train_time:97376ms step_avg:55.33ms
step:1761/2160 train_time:97465ms step_avg:55.35ms
step:1762/2160 train_time:97553ms step_avg:55.37ms
step:1763/2160 train_time:97645ms step_avg:55.39ms
step:1764/2160 train_time:97734ms step_avg:55.40ms
step:1765/2160 train_time:97825ms step_avg:55.42ms
step:1766/2160 train_time:97913ms step_avg:55.44ms
step:1767/2160 train_time:98002ms step_avg:55.46ms
step:1768/2160 train_time:98089ms step_avg:55.48ms
step:1769/2160 train_time:98177ms step_avg:55.50ms
step:1770/2160 train_time:98264ms step_avg:55.52ms
step:1771/2160 train_time:98353ms step_avg:55.54ms
step:1772/2160 train_time:98440ms step_avg:55.55ms
step:1773/2160 train_time:98530ms step_avg:55.57ms
step:1774/2160 train_time:98618ms step_avg:55.59ms
step:1775/2160 train_time:98708ms step_avg:55.61ms
step:1776/2160 train_time:98796ms step_avg:55.63ms
step:1777/2160 train_time:98885ms step_avg:55.65ms
step:1778/2160 train_time:98972ms step_avg:55.66ms
step:1779/2160 train_time:99060ms step_avg:55.68ms
step:1780/2160 train_time:99149ms step_avg:55.70ms
step:1781/2160 train_time:99235ms step_avg:55.72ms
step:1782/2160 train_time:99322ms step_avg:55.74ms
step:1783/2160 train_time:99412ms step_avg:55.76ms
step:1784/2160 train_time:99499ms step_avg:55.77ms
step:1785/2160 train_time:99588ms step_avg:55.79ms
step:1786/2160 train_time:99676ms step_avg:55.81ms
step:1787/2160 train_time:99767ms step_avg:55.83ms
step:1788/2160 train_time:99854ms step_avg:55.85ms
step:1789/2160 train_time:99943ms step_avg:55.87ms
step:1790/2160 train_time:100031ms step_avg:55.88ms
step:1791/2160 train_time:100119ms step_avg:55.90ms
step:1792/2160 train_time:100205ms step_avg:55.92ms
step:1793/2160 train_time:100295ms step_avg:55.94ms
step:1794/2160 train_time:100382ms step_avg:55.95ms
step:1795/2160 train_time:100472ms step_avg:55.97ms
step:1796/2160 train_time:100558ms step_avg:55.99ms
step:1797/2160 train_time:100649ms step_avg:56.01ms
step:1798/2160 train_time:100736ms step_avg:56.03ms
step:1799/2160 train_time:100825ms step_avg:56.05ms
step:1800/2160 train_time:100912ms step_avg:56.06ms
step:1801/2160 train_time:101001ms step_avg:56.08ms
step:1802/2160 train_time:101088ms step_avg:56.10ms
step:1803/2160 train_time:101176ms step_avg:56.12ms
step:1804/2160 train_time:101263ms step_avg:56.13ms
step:1805/2160 train_time:101353ms step_avg:56.15ms
step:1806/2160 train_time:101439ms step_avg:56.17ms
step:1807/2160 train_time:101529ms step_avg:56.19ms
step:1808/2160 train_time:101616ms step_avg:56.20ms
step:1809/2160 train_time:101705ms step_avg:56.22ms
step:1810/2160 train_time:101792ms step_avg:56.24ms
step:1811/2160 train_time:101882ms step_avg:56.26ms
step:1812/2160 train_time:101969ms step_avg:56.27ms
step:1813/2160 train_time:102057ms step_avg:56.29ms
step:1814/2160 train_time:102144ms step_avg:56.31ms
step:1815/2160 train_time:102234ms step_avg:56.33ms
step:1816/2160 train_time:102321ms step_avg:56.34ms
step:1817/2160 train_time:102409ms step_avg:56.36ms
step:1818/2160 train_time:102496ms step_avg:56.38ms
step:1819/2160 train_time:102585ms step_avg:56.40ms
step:1820/2160 train_time:102672ms step_avg:56.41ms
step:1821/2160 train_time:102761ms step_avg:56.43ms
step:1822/2160 train_time:102848ms step_avg:56.45ms
step:1823/2160 train_time:102938ms step_avg:56.47ms
step:1824/2160 train_time:103025ms step_avg:56.48ms
step:1825/2160 train_time:103115ms step_avg:56.50ms
step:1826/2160 train_time:103203ms step_avg:56.52ms
step:1827/2160 train_time:103292ms step_avg:56.54ms
step:1828/2160 train_time:103379ms step_avg:56.55ms
step:1829/2160 train_time:103468ms step_avg:56.57ms
step:1830/2160 train_time:103554ms step_avg:56.59ms
step:1831/2160 train_time:103644ms step_avg:56.60ms
step:1832/2160 train_time:103731ms step_avg:56.62ms
step:1833/2160 train_time:103820ms step_avg:56.64ms
step:1834/2160 train_time:103907ms step_avg:56.66ms
step:1835/2160 train_time:103996ms step_avg:56.67ms
step:1836/2160 train_time:104083ms step_avg:56.69ms
step:1837/2160 train_time:104174ms step_avg:56.71ms
step:1838/2160 train_time:104261ms step_avg:56.73ms
step:1839/2160 train_time:104350ms step_avg:56.74ms
step:1840/2160 train_time:104436ms step_avg:56.76ms
step:1841/2160 train_time:104525ms step_avg:56.78ms
step:1842/2160 train_time:104612ms step_avg:56.79ms
step:1843/2160 train_time:104702ms step_avg:56.81ms
step:1844/2160 train_time:104789ms step_avg:56.83ms
step:1845/2160 train_time:104878ms step_avg:56.84ms
step:1846/2160 train_time:104965ms step_avg:56.86ms
step:1847/2160 train_time:105055ms step_avg:56.88ms
step:1848/2160 train_time:105143ms step_avg:56.90ms
step:1849/2160 train_time:105232ms step_avg:56.91ms
step:1850/2160 train_time:105319ms step_avg:56.93ms
step:1851/2160 train_time:105408ms step_avg:56.95ms
step:1852/2160 train_time:105495ms step_avg:56.96ms
step:1853/2160 train_time:105584ms step_avg:56.98ms
step:1854/2160 train_time:105672ms step_avg:57.00ms
step:1855/2160 train_time:105760ms step_avg:57.01ms
step:1856/2160 train_time:105847ms step_avg:57.03ms
step:1857/2160 train_time:105936ms step_avg:57.05ms
step:1858/2160 train_time:106023ms step_avg:57.06ms
step:1859/2160 train_time:106113ms step_avg:57.08ms
step:1860/2160 train_time:106200ms step_avg:57.10ms
step:1861/2160 train_time:106289ms step_avg:57.11ms
step:1862/2160 train_time:106376ms step_avg:57.13ms
step:1863/2160 train_time:106466ms step_avg:57.15ms
step:1864/2160 train_time:106554ms step_avg:57.16ms
step:1865/2160 train_time:106644ms step_avg:57.18ms
step:1866/2160 train_time:106731ms step_avg:57.20ms
step:1867/2160 train_time:106819ms step_avg:57.21ms
step:1868/2160 train_time:106907ms step_avg:57.23ms
step:1869/2160 train_time:106996ms step_avg:57.25ms
step:1870/2160 train_time:107083ms step_avg:57.26ms
step:1871/2160 train_time:107172ms step_avg:57.28ms
step:1872/2160 train_time:107259ms step_avg:57.30ms
step:1873/2160 train_time:107348ms step_avg:57.31ms
step:1874/2160 train_time:107435ms step_avg:57.33ms
step:1875/2160 train_time:107524ms step_avg:57.35ms
step:1876/2160 train_time:107612ms step_avg:57.36ms
step:1877/2160 train_time:107703ms step_avg:57.38ms
step:1878/2160 train_time:107790ms step_avg:57.40ms
step:1879/2160 train_time:107878ms step_avg:57.41ms
step:1880/2160 train_time:107965ms step_avg:57.43ms
step:1881/2160 train_time:108054ms step_avg:57.45ms
step:1882/2160 train_time:108142ms step_avg:57.46ms
step:1883/2160 train_time:108232ms step_avg:57.48ms
step:1884/2160 train_time:108318ms step_avg:57.49ms
step:1885/2160 train_time:108408ms step_avg:57.51ms
step:1886/2160 train_time:108495ms step_avg:57.53ms
step:1887/2160 train_time:108585ms step_avg:57.54ms
step:1888/2160 train_time:108672ms step_avg:57.56ms
step:1889/2160 train_time:108761ms step_avg:57.58ms
step:1890/2160 train_time:108849ms step_avg:57.59ms
step:1891/2160 train_time:108938ms step_avg:57.61ms
step:1892/2160 train_time:109024ms step_avg:57.62ms
step:1893/2160 train_time:109114ms step_avg:57.64ms
step:1894/2160 train_time:109202ms step_avg:57.66ms
step:1895/2160 train_time:109290ms step_avg:57.67ms
step:1896/2160 train_time:109377ms step_avg:57.69ms
step:1897/2160 train_time:109467ms step_avg:57.71ms
step:1898/2160 train_time:109554ms step_avg:57.72ms
step:1899/2160 train_time:109643ms step_avg:57.74ms
step:1900/2160 train_time:109730ms step_avg:57.75ms
step:1901/2160 train_time:109819ms step_avg:57.77ms
step:1902/2160 train_time:109906ms step_avg:57.78ms
step:1903/2160 train_time:109996ms step_avg:57.80ms
step:1904/2160 train_time:110084ms step_avg:57.82ms
step:1905/2160 train_time:110173ms step_avg:57.83ms
step:1906/2160 train_time:110260ms step_avg:57.85ms
step:1907/2160 train_time:110349ms step_avg:57.87ms
step:1908/2160 train_time:110436ms step_avg:57.88ms
step:1909/2160 train_time:110524ms step_avg:57.90ms
step:1910/2160 train_time:110612ms step_avg:57.91ms
step:1911/2160 train_time:110701ms step_avg:57.93ms
step:1912/2160 train_time:110789ms step_avg:57.94ms
step:1913/2160 train_time:110877ms step_avg:57.96ms
step:1914/2160 train_time:110964ms step_avg:57.98ms
step:1915/2160 train_time:111055ms step_avg:57.99ms
step:1916/2160 train_time:111143ms step_avg:58.01ms
step:1917/2160 train_time:111233ms step_avg:58.02ms
step:1918/2160 train_time:111320ms step_avg:58.04ms
step:1919/2160 train_time:111410ms step_avg:58.06ms
step:1920/2160 train_time:111497ms step_avg:58.07ms
step:1921/2160 train_time:111586ms step_avg:58.09ms
step:1922/2160 train_time:111674ms step_avg:58.10ms
step:1923/2160 train_time:111763ms step_avg:58.12ms
step:1924/2160 train_time:111850ms step_avg:58.13ms
step:1925/2160 train_time:111939ms step_avg:58.15ms
step:1926/2160 train_time:112026ms step_avg:58.17ms
step:1927/2160 train_time:112115ms step_avg:58.18ms
step:1928/2160 train_time:112203ms step_avg:58.20ms
step:1929/2160 train_time:112292ms step_avg:58.21ms
step:1930/2160 train_time:112379ms step_avg:58.23ms
step:1931/2160 train_time:112469ms step_avg:58.24ms
step:1932/2160 train_time:112555ms step_avg:58.26ms
step:1933/2160 train_time:112644ms step_avg:58.27ms
step:1934/2160 train_time:112730ms step_avg:58.29ms
step:1935/2160 train_time:112819ms step_avg:58.30ms
step:1936/2160 train_time:112907ms step_avg:58.32ms
step:1937/2160 train_time:112997ms step_avg:58.34ms
step:1938/2160 train_time:113085ms step_avg:58.35ms
step:1939/2160 train_time:113175ms step_avg:58.37ms
step:1940/2160 train_time:113262ms step_avg:58.38ms
step:1941/2160 train_time:113351ms step_avg:58.40ms
step:1942/2160 train_time:113437ms step_avg:58.41ms
step:1943/2160 train_time:113527ms step_avg:58.43ms
step:1944/2160 train_time:113614ms step_avg:58.44ms
step:1945/2160 train_time:113703ms step_avg:58.46ms
step:1946/2160 train_time:113790ms step_avg:58.47ms
step:1947/2160 train_time:113879ms step_avg:58.49ms
step:1948/2160 train_time:113967ms step_avg:58.50ms
step:1949/2160 train_time:114055ms step_avg:58.52ms
step:1950/2160 train_time:114144ms step_avg:58.54ms
step:1951/2160 train_time:114233ms step_avg:58.55ms
step:1952/2160 train_time:114320ms step_avg:58.57ms
step:1953/2160 train_time:114409ms step_avg:58.58ms
step:1954/2160 train_time:114495ms step_avg:58.60ms
step:1955/2160 train_time:114585ms step_avg:58.61ms
step:1956/2160 train_time:114673ms step_avg:58.63ms
step:1957/2160 train_time:114763ms step_avg:58.64ms
step:1958/2160 train_time:114849ms step_avg:58.66ms
step:1959/2160 train_time:114938ms step_avg:58.67ms
step:1960/2160 train_time:115024ms step_avg:58.69ms
step:1961/2160 train_time:115115ms step_avg:58.70ms
step:1962/2160 train_time:115203ms step_avg:58.72ms
step:1963/2160 train_time:115293ms step_avg:58.73ms
step:1964/2160 train_time:115379ms step_avg:58.75ms
step:1965/2160 train_time:115469ms step_avg:58.76ms
step:1966/2160 train_time:115556ms step_avg:58.78ms
step:1967/2160 train_time:115646ms step_avg:58.79ms
step:1968/2160 train_time:115734ms step_avg:58.81ms
step:1969/2160 train_time:115823ms step_avg:58.82ms
step:1970/2160 train_time:115911ms step_avg:58.84ms
step:1971/2160 train_time:116000ms step_avg:58.85ms
step:1972/2160 train_time:116087ms step_avg:58.87ms
step:1973/2160 train_time:116177ms step_avg:58.88ms
step:1974/2160 train_time:116264ms step_avg:58.90ms
step:1975/2160 train_time:116354ms step_avg:58.91ms
step:1976/2160 train_time:116441ms step_avg:58.93ms
step:1977/2160 train_time:116529ms step_avg:58.94ms
step:1978/2160 train_time:116616ms step_avg:58.96ms
step:1979/2160 train_time:116704ms step_avg:58.97ms
step:1980/2160 train_time:116791ms step_avg:58.99ms
step:1981/2160 train_time:116881ms step_avg:59.00ms
step:1982/2160 train_time:116968ms step_avg:59.02ms
step:1983/2160 train_time:117057ms step_avg:59.03ms
step:1984/2160 train_time:117144ms step_avg:59.04ms
step:1985/2160 train_time:117233ms step_avg:59.06ms
step:1986/2160 train_time:117320ms step_avg:59.07ms
step:1987/2160 train_time:117409ms step_avg:59.09ms
step:1988/2160 train_time:117495ms step_avg:59.10ms
step:1989/2160 train_time:117584ms step_avg:59.12ms
step:1990/2160 train_time:117672ms step_avg:59.13ms
step:1991/2160 train_time:117761ms step_avg:59.15ms
step:1992/2160 train_time:117848ms step_avg:59.16ms
step:1993/2160 train_time:117938ms step_avg:59.18ms
step:1994/2160 train_time:118025ms step_avg:59.19ms
step:1995/2160 train_time:118115ms step_avg:59.21ms
step:1996/2160 train_time:118202ms step_avg:59.22ms
step:1997/2160 train_time:118291ms step_avg:59.23ms
step:1998/2160 train_time:118378ms step_avg:59.25ms
step:1999/2160 train_time:118467ms step_avg:59.26ms
step:2000/2160 train_time:118554ms step_avg:59.28ms
step:2000/2160 val_loss:3.3113 train_time:118643ms step_avg:59.32ms
step:2001/2160 train_time:118666ms step_avg:59.30ms
step:2002/2160 train_time:118735ms step_avg:59.31ms
step:2003/2160 train_time:118832ms step_avg:59.33ms
step:2004/2160 train_time:118920ms step_avg:59.34ms
step:2005/2160 train_time:119010ms step_avg:59.36ms
step:2006/2160 train_time:119096ms step_avg:59.37ms
step:2007/2160 train_time:119184ms step_avg:59.38ms
step:2008/2160 train_time:119269ms step_avg:59.40ms
step:2009/2160 train_time:119358ms step_avg:59.41ms
step:2010/2160 train_time:119444ms step_avg:59.42ms
step:2011/2160 train_time:119532ms step_avg:59.44ms
step:2012/2160 train_time:119620ms step_avg:59.45ms
step:2013/2160 train_time:119713ms step_avg:59.47ms
step:2014/2160 train_time:119803ms step_avg:59.48ms
step:2015/2160 train_time:119893ms step_avg:59.50ms
step:2016/2160 train_time:119981ms step_avg:59.51ms
step:2017/2160 train_time:120069ms step_avg:59.53ms
step:2018/2160 train_time:120155ms step_avg:59.54ms
step:2019/2160 train_time:120243ms step_avg:59.56ms
step:2020/2160 train_time:120329ms step_avg:59.57ms
step:2021/2160 train_time:120418ms step_avg:59.58ms
step:2022/2160 train_time:120505ms step_avg:59.60ms
step:2023/2160 train_time:120594ms step_avg:59.61ms
step:2024/2160 train_time:120683ms step_avg:59.63ms
step:2025/2160 train_time:120774ms step_avg:59.64ms
step:2026/2160 train_time:120861ms step_avg:59.66ms
step:2027/2160 train_time:120951ms step_avg:59.67ms
step:2028/2160 train_time:121038ms step_avg:59.68ms
step:2029/2160 train_time:121126ms step_avg:59.70ms
step:2030/2160 train_time:121213ms step_avg:59.71ms
step:2031/2160 train_time:121301ms step_avg:59.72ms
step:2032/2160 train_time:121387ms step_avg:59.74ms
step:2033/2160 train_time:121476ms step_avg:59.75ms
step:2034/2160 train_time:121563ms step_avg:59.77ms
step:2035/2160 train_time:121653ms step_avg:59.78ms
step:2036/2160 train_time:121742ms step_avg:59.79ms
step:2037/2160 train_time:121834ms step_avg:59.81ms
step:2038/2160 train_time:121922ms step_avg:59.82ms
step:2039/2160 train_time:122011ms step_avg:59.84ms
step:2040/2160 train_time:122097ms step_avg:59.85ms
step:2041/2160 train_time:122186ms step_avg:59.87ms
step:2042/2160 train_time:122272ms step_avg:59.88ms
step:2043/2160 train_time:122361ms step_avg:59.89ms
step:2044/2160 train_time:122447ms step_avg:59.91ms
step:2045/2160 train_time:122536ms step_avg:59.92ms
step:2046/2160 train_time:122624ms step_avg:59.93ms
step:2047/2160 train_time:122713ms step_avg:59.95ms
step:2048/2160 train_time:122801ms step_avg:59.96ms
step:2049/2160 train_time:122891ms step_avg:59.98ms
step:2050/2160 train_time:122980ms step_avg:59.99ms
step:2051/2160 train_time:123069ms step_avg:60.00ms
step:2052/2160 train_time:123156ms step_avg:60.02ms
step:2053/2160 train_time:123245ms step_avg:60.03ms
step:2054/2160 train_time:123332ms step_avg:60.04ms
step:2055/2160 train_time:123420ms step_avg:60.06ms
step:2056/2160 train_time:123509ms step_avg:60.07ms
step:2057/2160 train_time:123597ms step_avg:60.09ms
step:2058/2160 train_time:123685ms step_avg:60.10ms
step:2059/2160 train_time:123774ms step_avg:60.11ms
step:2060/2160 train_time:123860ms step_avg:60.13ms
step:2061/2160 train_time:123951ms step_avg:60.14ms
step:2062/2160 train_time:124038ms step_avg:60.15ms
step:2063/2160 train_time:124128ms step_avg:60.17ms
step:2064/2160 train_time:124214ms step_avg:60.18ms
step:2065/2160 train_time:124303ms step_avg:60.19ms
step:2066/2160 train_time:124388ms step_avg:60.21ms
step:2067/2160 train_time:124478ms step_avg:60.22ms
step:2068/2160 train_time:124565ms step_avg:60.23ms
step:2069/2160 train_time:124654ms step_avg:60.25ms
step:2070/2160 train_time:124742ms step_avg:60.26ms
step:2071/2160 train_time:124832ms step_avg:60.28ms
step:2072/2160 train_time:124919ms step_avg:60.29ms
step:2073/2160 train_time:125009ms step_avg:60.30ms
step:2074/2160 train_time:125096ms step_avg:60.32ms
step:2075/2160 train_time:125186ms step_avg:60.33ms
step:2076/2160 train_time:125272ms step_avg:60.34ms
step:2077/2160 train_time:125361ms step_avg:60.36ms
step:2078/2160 train_time:125447ms step_avg:60.37ms
step:2079/2160 train_time:125536ms step_avg:60.38ms
step:2080/2160 train_time:125623ms step_avg:60.40ms
step:2081/2160 train_time:125712ms step_avg:60.41ms
step:2082/2160 train_time:125800ms step_avg:60.42ms
step:2083/2160 train_time:125890ms step_avg:60.44ms
step:2084/2160 train_time:125977ms step_avg:60.45ms
step:2085/2160 train_time:126066ms step_avg:60.46ms
step:2086/2160 train_time:126152ms step_avg:60.48ms
step:2087/2160 train_time:126241ms step_avg:60.49ms
step:2088/2160 train_time:126328ms step_avg:60.50ms
step:2089/2160 train_time:126418ms step_avg:60.52ms
step:2090/2160 train_time:126506ms step_avg:60.53ms
step:2091/2160 train_time:126594ms step_avg:60.54ms
step:2092/2160 train_time:126681ms step_avg:60.56ms
step:2093/2160 train_time:126771ms step_avg:60.57ms
step:2094/2160 train_time:126858ms step_avg:60.58ms
step:2095/2160 train_time:126949ms step_avg:60.60ms
step:2096/2160 train_time:127036ms step_avg:60.61ms
step:2097/2160 train_time:127126ms step_avg:60.62ms
step:2098/2160 train_time:127212ms step_avg:60.64ms
step:2099/2160 train_time:127302ms step_avg:60.65ms
step:2100/2160 train_time:127389ms step_avg:60.66ms
step:2101/2160 train_time:127478ms step_avg:60.67ms
step:2102/2160 train_time:127566ms step_avg:60.69ms
step:2103/2160 train_time:127655ms step_avg:60.70ms
step:2104/2160 train_time:127742ms step_avg:60.71ms
step:2105/2160 train_time:127832ms step_avg:60.73ms
step:2106/2160 train_time:127919ms step_avg:60.74ms
step:2107/2160 train_time:128009ms step_avg:60.75ms
step:2108/2160 train_time:128096ms step_avg:60.77ms
step:2109/2160 train_time:128184ms step_avg:60.78ms
step:2110/2160 train_time:128271ms step_avg:60.79ms
step:2111/2160 train_time:128360ms step_avg:60.81ms
step:2112/2160 train_time:128447ms step_avg:60.82ms
step:2113/2160 train_time:128535ms step_avg:60.83ms
step:2114/2160 train_time:128622ms step_avg:60.84ms
step:2115/2160 train_time:128712ms step_avg:60.86ms
step:2116/2160 train_time:128799ms step_avg:60.87ms
step:2117/2160 train_time:128889ms step_avg:60.88ms
step:2118/2160 train_time:128976ms step_avg:60.90ms
step:2119/2160 train_time:129065ms step_avg:60.91ms
step:2120/2160 train_time:129152ms step_avg:60.92ms
step:2121/2160 train_time:129242ms step_avg:60.93ms
step:2122/2160 train_time:129329ms step_avg:60.95ms
step:2123/2160 train_time:129418ms step_avg:60.96ms
step:2124/2160 train_time:129506ms step_avg:60.97ms
step:2125/2160 train_time:129595ms step_avg:60.99ms
step:2126/2160 train_time:129684ms step_avg:61.00ms
step:2127/2160 train_time:129773ms step_avg:61.01ms
step:2128/2160 train_time:129861ms step_avg:61.02ms
step:2129/2160 train_time:129950ms step_avg:61.04ms
step:2130/2160 train_time:130037ms step_avg:61.05ms
step:2131/2160 train_time:130127ms step_avg:61.06ms
step:2132/2160 train_time:130213ms step_avg:61.08ms
step:2133/2160 train_time:130304ms step_avg:61.09ms
step:2134/2160 train_time:130391ms step_avg:61.10ms
step:2135/2160 train_time:130480ms step_avg:61.11ms
step:2136/2160 train_time:130567ms step_avg:61.13ms
step:2137/2160 train_time:130656ms step_avg:61.14ms
step:2138/2160 train_time:130743ms step_avg:61.15ms
step:2139/2160 train_time:130833ms step_avg:61.17ms
step:2140/2160 train_time:130920ms step_avg:61.18ms
step:2141/2160 train_time:131010ms step_avg:61.19ms
step:2142/2160 train_time:131098ms step_avg:61.20ms
step:2143/2160 train_time:131187ms step_avg:61.22ms
step:2144/2160 train_time:131274ms step_avg:61.23ms
step:2145/2160 train_time:131363ms step_avg:61.24ms
step:2146/2160 train_time:131450ms step_avg:61.25ms
step:2147/2160 train_time:131540ms step_avg:61.27ms
step:2148/2160 train_time:131628ms step_avg:61.28ms
step:2149/2160 train_time:131717ms step_avg:61.29ms
step:2150/2160 train_time:131805ms step_avg:61.30ms
step:2151/2160 train_time:131894ms step_avg:61.32ms
step:2152/2160 train_time:131983ms step_avg:61.33ms
step:2153/2160 train_time:132072ms step_avg:61.34ms
step:2154/2160 train_time:132159ms step_avg:61.36ms
step:2155/2160 train_time:132249ms step_avg:61.37ms
step:2156/2160 train_time:132336ms step_avg:61.38ms
step:2157/2160 train_time:132424ms step_avg:61.39ms
step:2158/2160 train_time:132512ms step_avg:61.40ms
step:2159/2160 train_time:132601ms step_avg:61.42ms
step:2160/2160 train_time:132689ms step_avg:61.43ms
step:2160/2160 val_loss:3.2793 train_time:132779ms step_avg:61.47ms
peak memory allocated: 30078 MiB reserved: 44396 MiB

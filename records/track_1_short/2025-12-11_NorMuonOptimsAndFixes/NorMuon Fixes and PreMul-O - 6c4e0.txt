import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:39:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   43C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           53873      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           53874      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           53875      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           53876      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           53877      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           53878      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           53879      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           53880      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           53874      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           53875      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           53876      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           53877      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           53878      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           53879      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           53880      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2160 train_time:90ms step_avg:89.53ms
step:2/2160 train_time:116ms step_avg:57.85ms
step:3/2160 train_time:139ms step_avg:46.31ms
step:4/2160 train_time:171ms step_avg:42.64ms
step:5/2160 train_time:203ms step_avg:40.68ms
step:6/2160 train_time:336ms step_avg:55.92ms
step:7/2160 train_time:372ms step_avg:53.09ms
step:8/2160 train_time:404ms step_avg:50.50ms
step:9/2160 train_time:437ms step_avg:48.54ms
step:10/2160 train_time:469ms step_avg:46.91ms
step:11/2160 train_time:502ms step_avg:45.67ms
step:12/2160 train_time:535ms step_avg:44.55ms
step:13/2160 train_time:568ms step_avg:43.70ms
step:14/2160 train_time:601ms step_avg:42.90ms
step:15/2160 train_time:634ms step_avg:42.24ms
step:16/2160 train_time:666ms step_avg:41.63ms
step:17/2160 train_time:699ms step_avg:41.13ms
step:18/2160 train_time:731ms step_avg:40.63ms
step:19/2160 train_time:765ms step_avg:40.26ms
step:20/2160 train_time:797ms step_avg:39.87ms
step:21/2160 train_time:831ms step_avg:39.55ms
step:22/2160 train_time:863ms step_avg:39.23ms
step:23/2160 train_time:896ms step_avg:38.96ms
step:24/2160 train_time:929ms step_avg:38.70ms
step:25/2160 train_time:962ms step_avg:38.47ms
step:26/2160 train_time:994ms step_avg:38.24ms
step:27/2160 train_time:1027ms step_avg:38.05ms
step:28/2160 train_time:1060ms step_avg:37.85ms
step:29/2160 train_time:1093ms step_avg:37.68ms
step:30/2160 train_time:1125ms step_avg:37.50ms
step:31/2160 train_time:1158ms step_avg:37.37ms
step:32/2160 train_time:1191ms step_avg:37.21ms
step:33/2160 train_time:1224ms step_avg:37.10ms
step:34/2160 train_time:1256ms step_avg:36.95ms
step:35/2160 train_time:1290ms step_avg:36.87ms
step:36/2160 train_time:1323ms step_avg:36.76ms
step:37/2160 train_time:1358ms step_avg:36.72ms
step:38/2160 train_time:1391ms step_avg:36.62ms
step:39/2160 train_time:1426ms step_avg:36.57ms
step:40/2160 train_time:1459ms step_avg:36.47ms
step:41/2160 train_time:1493ms step_avg:36.41ms
step:42/2160 train_time:1525ms step_avg:36.31ms
step:43/2160 train_time:1559ms step_avg:36.26ms
step:44/2160 train_time:1592ms step_avg:36.17ms
step:45/2160 train_time:1625ms step_avg:36.11ms
step:46/2160 train_time:1657ms step_avg:36.03ms
step:47/2160 train_time:1691ms step_avg:35.97ms
step:48/2160 train_time:1723ms step_avg:35.90ms
step:49/2160 train_time:1756ms step_avg:35.85ms
step:50/2160 train_time:1789ms step_avg:35.77ms
step:51/2160 train_time:1822ms step_avg:35.73ms
step:52/2160 train_time:1854ms step_avg:35.66ms
step:53/2160 train_time:1888ms step_avg:35.62ms
step:54/2160 train_time:1920ms step_avg:35.56ms
step:55/2160 train_time:1953ms step_avg:35.51ms
step:56/2160 train_time:1986ms step_avg:35.46ms
step:57/2160 train_time:2019ms step_avg:35.42ms
step:58/2160 train_time:2052ms step_avg:35.37ms
step:59/2160 train_time:2085ms step_avg:35.34ms
step:60/2160 train_time:2117ms step_avg:35.29ms
step:61/2160 train_time:2150ms step_avg:35.25ms
step:62/2160 train_time:2183ms step_avg:35.20ms
step:63/2160 train_time:2216ms step_avg:35.17ms
step:64/2160 train_time:2249ms step_avg:35.13ms
step:65/2160 train_time:2282ms step_avg:35.11ms
step:66/2160 train_time:2314ms step_avg:35.07ms
step:67/2160 train_time:2348ms step_avg:35.05ms
step:68/2160 train_time:2381ms step_avg:35.01ms
step:69/2160 train_time:2415ms step_avg:35.00ms
step:70/2160 train_time:2447ms step_avg:34.96ms
step:71/2160 train_time:2481ms step_avg:34.94ms
step:72/2160 train_time:2513ms step_avg:34.91ms
step:73/2160 train_time:2547ms step_avg:34.89ms
step:74/2160 train_time:2579ms step_avg:34.86ms
step:75/2160 train_time:2613ms step_avg:34.84ms
step:76/2160 train_time:2646ms step_avg:34.81ms
step:77/2160 train_time:2679ms step_avg:34.79ms
step:78/2160 train_time:2711ms step_avg:34.76ms
step:79/2160 train_time:2745ms step_avg:34.75ms
step:80/2160 train_time:2777ms step_avg:34.72ms
step:81/2160 train_time:2811ms step_avg:34.70ms
step:82/2160 train_time:2843ms step_avg:34.67ms
step:83/2160 train_time:2876ms step_avg:34.65ms
step:84/2160 train_time:2908ms step_avg:34.62ms
step:85/2160 train_time:2941ms step_avg:34.61ms
step:86/2160 train_time:2974ms step_avg:34.58ms
step:87/2160 train_time:3007ms step_avg:34.56ms
step:88/2160 train_time:3039ms step_avg:34.54ms
step:89/2160 train_time:3072ms step_avg:34.52ms
step:90/2160 train_time:3105ms step_avg:34.50ms
step:91/2160 train_time:3138ms step_avg:34.48ms
step:92/2160 train_time:3170ms step_avg:34.46ms
step:93/2160 train_time:3203ms step_avg:34.45ms
step:94/2160 train_time:3236ms step_avg:34.42ms
step:95/2160 train_time:3269ms step_avg:34.41ms
step:96/2160 train_time:3302ms step_avg:34.39ms
step:97/2160 train_time:3335ms step_avg:34.38ms
step:98/2160 train_time:3367ms step_avg:34.36ms
step:99/2160 train_time:3401ms step_avg:34.35ms
step:100/2160 train_time:3433ms step_avg:34.33ms
step:101/2160 train_time:3467ms step_avg:34.33ms
step:102/2160 train_time:3500ms step_avg:34.31ms
step:103/2160 train_time:3533ms step_avg:34.30ms
step:104/2160 train_time:3566ms step_avg:34.28ms
step:105/2160 train_time:3599ms step_avg:34.28ms
step:106/2160 train_time:3631ms step_avg:34.26ms
step:107/2160 train_time:3665ms step_avg:34.25ms
step:108/2160 train_time:3698ms step_avg:34.24ms
step:109/2160 train_time:3731ms step_avg:34.23ms
step:110/2160 train_time:3763ms step_avg:34.21ms
step:111/2160 train_time:3797ms step_avg:34.20ms
step:112/2160 train_time:3829ms step_avg:34.19ms
step:113/2160 train_time:3863ms step_avg:34.18ms
step:114/2160 train_time:3895ms step_avg:34.16ms
step:115/2160 train_time:3928ms step_avg:34.16ms
step:116/2160 train_time:3960ms step_avg:34.14ms
step:117/2160 train_time:3993ms step_avg:34.13ms
step:118/2160 train_time:4026ms step_avg:34.12ms
step:119/2160 train_time:4059ms step_avg:34.11ms
step:120/2160 train_time:4092ms step_avg:34.10ms
step:121/2160 train_time:4125ms step_avg:34.09ms
step:122/2160 train_time:4157ms step_avg:34.08ms
step:123/2160 train_time:4190ms step_avg:34.07ms
step:124/2160 train_time:4222ms step_avg:34.05ms
step:125/2160 train_time:4256ms step_avg:34.05ms
step:126/2160 train_time:4288ms step_avg:34.03ms
step:127/2160 train_time:4321ms step_avg:34.03ms
step:128/2160 train_time:4354ms step_avg:34.01ms
step:129/2160 train_time:4387ms step_avg:34.01ms
step:130/2160 train_time:4419ms step_avg:33.99ms
step:131/2160 train_time:4452ms step_avg:33.99ms
step:132/2160 train_time:4485ms step_avg:33.97ms
step:133/2160 train_time:4518ms step_avg:33.97ms
step:134/2160 train_time:4550ms step_avg:33.96ms
step:135/2160 train_time:4584ms step_avg:33.96ms
step:136/2160 train_time:4616ms step_avg:33.94ms
step:137/2160 train_time:4650ms step_avg:33.94ms
step:138/2160 train_time:4682ms step_avg:33.93ms
step:139/2160 train_time:4715ms step_avg:33.92ms
step:140/2160 train_time:4748ms step_avg:33.92ms
step:141/2160 train_time:4781ms step_avg:33.91ms
step:142/2160 train_time:4814ms step_avg:33.90ms
step:143/2160 train_time:4847ms step_avg:33.89ms
step:144/2160 train_time:4879ms step_avg:33.88ms
step:145/2160 train_time:4912ms step_avg:33.88ms
step:146/2160 train_time:4945ms step_avg:33.87ms
step:147/2160 train_time:4978ms step_avg:33.86ms
step:148/2160 train_time:5010ms step_avg:33.85ms
step:149/2160 train_time:5044ms step_avg:33.85ms
step:150/2160 train_time:5076ms step_avg:33.84ms
step:151/2160 train_time:5109ms step_avg:33.84ms
step:152/2160 train_time:5141ms step_avg:33.83ms
step:153/2160 train_time:5175ms step_avg:33.82ms
step:154/2160 train_time:5207ms step_avg:33.81ms
step:155/2160 train_time:5240ms step_avg:33.81ms
step:156/2160 train_time:5273ms step_avg:33.80ms
step:157/2160 train_time:5306ms step_avg:33.80ms
step:158/2160 train_time:5339ms step_avg:33.79ms
step:159/2160 train_time:5372ms step_avg:33.79ms
step:160/2160 train_time:5404ms step_avg:33.78ms
step:161/2160 train_time:5438ms step_avg:33.78ms
step:162/2160 train_time:5470ms step_avg:33.77ms
step:163/2160 train_time:5504ms step_avg:33.77ms
step:164/2160 train_time:5536ms step_avg:33.76ms
step:165/2160 train_time:5570ms step_avg:33.75ms
step:166/2160 train_time:5602ms step_avg:33.75ms
step:167/2160 train_time:5635ms step_avg:33.74ms
step:168/2160 train_time:5667ms step_avg:33.73ms
step:169/2160 train_time:5701ms step_avg:33.73ms
step:170/2160 train_time:5733ms step_avg:33.72ms
step:171/2160 train_time:5767ms step_avg:33.72ms
step:172/2160 train_time:5799ms step_avg:33.72ms
step:173/2160 train_time:5833ms step_avg:33.72ms
step:174/2160 train_time:5865ms step_avg:33.71ms
step:175/2160 train_time:5898ms step_avg:33.70ms
step:176/2160 train_time:5930ms step_avg:33.69ms
step:177/2160 train_time:5964ms step_avg:33.69ms
step:178/2160 train_time:5996ms step_avg:33.69ms
step:179/2160 train_time:6030ms step_avg:33.68ms
step:180/2160 train_time:6062ms step_avg:33.68ms
step:181/2160 train_time:6095ms step_avg:33.67ms
step:182/2160 train_time:6127ms step_avg:33.67ms
step:183/2160 train_time:6161ms step_avg:33.67ms
step:184/2160 train_time:6193ms step_avg:33.66ms
step:185/2160 train_time:6226ms step_avg:33.66ms
step:186/2160 train_time:6258ms step_avg:33.65ms
step:187/2160 train_time:6292ms step_avg:33.64ms
step:188/2160 train_time:6324ms step_avg:33.64ms
step:189/2160 train_time:6357ms step_avg:33.63ms
step:190/2160 train_time:6389ms step_avg:33.63ms
step:191/2160 train_time:6422ms step_avg:33.62ms
step:192/2160 train_time:6454ms step_avg:33.62ms
step:193/2160 train_time:6488ms step_avg:33.61ms
step:194/2160 train_time:6520ms step_avg:33.61ms
step:195/2160 train_time:6553ms step_avg:33.61ms
step:196/2160 train_time:6586ms step_avg:33.60ms
step:197/2160 train_time:6619ms step_avg:33.60ms
step:198/2160 train_time:6651ms step_avg:33.59ms
step:199/2160 train_time:6684ms step_avg:33.59ms
step:200/2160 train_time:6717ms step_avg:33.58ms
step:201/2160 train_time:6750ms step_avg:33.58ms
step:202/2160 train_time:6782ms step_avg:33.58ms
step:203/2160 train_time:6815ms step_avg:33.57ms
step:204/2160 train_time:6848ms step_avg:33.57ms
step:205/2160 train_time:6881ms step_avg:33.57ms
step:206/2160 train_time:6913ms step_avg:33.56ms
step:207/2160 train_time:6947ms step_avg:33.56ms
step:208/2160 train_time:6979ms step_avg:33.55ms
step:209/2160 train_time:7012ms step_avg:33.55ms
step:210/2160 train_time:7044ms step_avg:33.54ms
step:211/2160 train_time:7077ms step_avg:33.54ms
step:212/2160 train_time:7110ms step_avg:33.54ms
step:213/2160 train_time:7143ms step_avg:33.53ms
step:214/2160 train_time:7176ms step_avg:33.53ms
step:215/2160 train_time:7209ms step_avg:33.53ms
step:216/2160 train_time:7241ms step_avg:33.52ms
step:217/2160 train_time:7274ms step_avg:33.52ms
step:218/2160 train_time:7306ms step_avg:33.51ms
step:219/2160 train_time:7340ms step_avg:33.51ms
step:220/2160 train_time:7372ms step_avg:33.51ms
step:221/2160 train_time:7405ms step_avg:33.51ms
step:222/2160 train_time:7438ms step_avg:33.50ms
step:223/2160 train_time:7471ms step_avg:33.50ms
step:224/2160 train_time:7503ms step_avg:33.50ms
step:225/2160 train_time:7536ms step_avg:33.49ms
step:226/2160 train_time:7569ms step_avg:33.49ms
step:227/2160 train_time:7602ms step_avg:33.49ms
step:228/2160 train_time:7634ms step_avg:33.48ms
step:229/2160 train_time:7667ms step_avg:33.48ms
step:230/2160 train_time:7700ms step_avg:33.48ms
step:231/2160 train_time:7733ms step_avg:33.48ms
step:232/2160 train_time:7765ms step_avg:33.47ms
step:233/2160 train_time:7799ms step_avg:33.47ms
step:234/2160 train_time:7831ms step_avg:33.47ms
step:235/2160 train_time:7864ms step_avg:33.46ms
step:236/2160 train_time:7897ms step_avg:33.46ms
step:237/2160 train_time:7930ms step_avg:33.46ms
step:238/2160 train_time:7962ms step_avg:33.45ms
step:239/2160 train_time:7995ms step_avg:33.45ms
step:240/2160 train_time:8027ms step_avg:33.45ms
step:241/2160 train_time:8060ms step_avg:33.44ms
step:242/2160 train_time:8093ms step_avg:33.44ms
step:243/2160 train_time:8126ms step_avg:33.44ms
step:244/2160 train_time:8158ms step_avg:33.43ms
step:245/2160 train_time:8191ms step_avg:33.43ms
step:246/2160 train_time:8224ms step_avg:33.43ms
step:247/2160 train_time:8257ms step_avg:33.43ms
step:248/2160 train_time:8289ms step_avg:33.42ms
step:249/2160 train_time:8322ms step_avg:33.42ms
step:250/2160 train_time:8354ms step_avg:33.42ms
step:250/2160 val_loss:4.2980 train_time:8390ms step_avg:33.56ms
step:251/2160 train_time:8411ms step_avg:33.51ms
step:252/2160 train_time:8433ms step_avg:33.46ms
step:253/2160 train_time:8457ms step_avg:33.43ms
step:254/2160 train_time:8489ms step_avg:33.42ms
step:255/2160 train_time:8525ms step_avg:33.43ms
step:256/2160 train_time:8558ms step_avg:33.43ms
step:257/2160 train_time:8592ms step_avg:33.43ms
step:258/2160 train_time:8625ms step_avg:33.43ms
step:259/2160 train_time:8658ms step_avg:33.43ms
step:260/2160 train_time:8690ms step_avg:33.42ms
step:261/2160 train_time:8723ms step_avg:33.42ms
step:262/2160 train_time:8756ms step_avg:33.42ms
step:263/2160 train_time:8788ms step_avg:33.42ms
step:264/2160 train_time:8821ms step_avg:33.41ms
step:265/2160 train_time:8853ms step_avg:33.41ms
step:266/2160 train_time:8885ms step_avg:33.40ms
step:267/2160 train_time:8918ms step_avg:33.40ms
step:268/2160 train_time:8951ms step_avg:33.40ms
step:269/2160 train_time:8984ms step_avg:33.40ms
step:270/2160 train_time:9016ms step_avg:33.39ms
step:271/2160 train_time:9049ms step_avg:33.39ms
step:272/2160 train_time:9081ms step_avg:33.39ms
step:273/2160 train_time:9113ms step_avg:33.38ms
step:274/2160 train_time:9146ms step_avg:33.38ms
step:275/2160 train_time:9179ms step_avg:33.38ms
step:276/2160 train_time:9211ms step_avg:33.37ms
step:277/2160 train_time:9244ms step_avg:33.37ms
step:278/2160 train_time:9276ms step_avg:33.37ms
step:279/2160 train_time:9309ms step_avg:33.37ms
step:280/2160 train_time:9341ms step_avg:33.36ms
step:281/2160 train_time:9375ms step_avg:33.36ms
step:282/2160 train_time:9407ms step_avg:33.36ms
step:283/2160 train_time:9440ms step_avg:33.36ms
step:284/2160 train_time:9472ms step_avg:33.35ms
step:285/2160 train_time:9506ms step_avg:33.35ms
step:286/2160 train_time:9538ms step_avg:33.35ms
step:287/2160 train_time:9572ms step_avg:33.35ms
step:288/2160 train_time:9605ms step_avg:33.35ms
step:289/2160 train_time:9638ms step_avg:33.35ms
step:290/2160 train_time:9670ms step_avg:33.35ms
step:291/2160 train_time:9703ms step_avg:33.35ms
step:292/2160 train_time:9736ms step_avg:33.34ms
step:293/2160 train_time:9769ms step_avg:33.34ms
step:294/2160 train_time:9802ms step_avg:33.34ms
step:295/2160 train_time:9835ms step_avg:33.34ms
step:296/2160 train_time:9867ms step_avg:33.33ms
step:297/2160 train_time:9900ms step_avg:33.33ms
step:298/2160 train_time:9932ms step_avg:33.33ms
step:299/2160 train_time:9966ms step_avg:33.33ms
step:300/2160 train_time:9998ms step_avg:33.33ms
step:301/2160 train_time:10031ms step_avg:33.33ms
step:302/2160 train_time:10063ms step_avg:33.32ms
step:303/2160 train_time:10096ms step_avg:33.32ms
step:304/2160 train_time:10128ms step_avg:33.32ms
step:305/2160 train_time:10161ms step_avg:33.32ms
step:306/2160 train_time:10193ms step_avg:33.31ms
step:307/2160 train_time:10227ms step_avg:33.31ms
step:308/2160 train_time:10259ms step_avg:33.31ms
step:309/2160 train_time:10292ms step_avg:33.31ms
step:310/2160 train_time:10324ms step_avg:33.30ms
step:311/2160 train_time:10357ms step_avg:33.30ms
step:312/2160 train_time:10389ms step_avg:33.30ms
step:313/2160 train_time:10423ms step_avg:33.30ms
step:314/2160 train_time:10455ms step_avg:33.30ms
step:315/2160 train_time:10489ms step_avg:33.30ms
step:316/2160 train_time:10521ms step_avg:33.29ms
step:317/2160 train_time:10555ms step_avg:33.30ms
step:318/2160 train_time:10587ms step_avg:33.29ms
step:319/2160 train_time:10620ms step_avg:33.29ms
step:320/2160 train_time:10653ms step_avg:33.29ms
step:321/2160 train_time:10686ms step_avg:33.29ms
step:322/2160 train_time:10718ms step_avg:33.29ms
step:323/2160 train_time:10752ms step_avg:33.29ms
step:324/2160 train_time:10784ms step_avg:33.28ms
step:325/2160 train_time:10817ms step_avg:33.28ms
step:326/2160 train_time:10849ms step_avg:33.28ms
step:327/2160 train_time:10882ms step_avg:33.28ms
step:328/2160 train_time:10915ms step_avg:33.28ms
step:329/2160 train_time:10948ms step_avg:33.28ms
step:330/2160 train_time:10980ms step_avg:33.27ms
step:331/2160 train_time:11013ms step_avg:33.27ms
step:332/2160 train_time:11045ms step_avg:33.27ms
step:333/2160 train_time:11078ms step_avg:33.27ms
step:334/2160 train_time:11111ms step_avg:33.27ms
step:335/2160 train_time:11143ms step_avg:33.26ms
step:336/2160 train_time:11176ms step_avg:33.26ms
step:337/2160 train_time:11209ms step_avg:33.26ms
step:338/2160 train_time:11241ms step_avg:33.26ms
step:339/2160 train_time:11274ms step_avg:33.26ms
step:340/2160 train_time:11306ms step_avg:33.25ms
step:341/2160 train_time:11339ms step_avg:33.25ms
step:342/2160 train_time:11372ms step_avg:33.25ms
step:343/2160 train_time:11405ms step_avg:33.25ms
step:344/2160 train_time:11437ms step_avg:33.25ms
step:345/2160 train_time:11470ms step_avg:33.25ms
step:346/2160 train_time:11503ms step_avg:33.24ms
step:347/2160 train_time:11536ms step_avg:33.24ms
step:348/2160 train_time:11568ms step_avg:33.24ms
step:349/2160 train_time:11601ms step_avg:33.24ms
step:350/2160 train_time:11634ms step_avg:33.24ms
step:351/2160 train_time:11667ms step_avg:33.24ms
step:352/2160 train_time:11699ms step_avg:33.24ms
step:353/2160 train_time:11732ms step_avg:33.23ms
step:354/2160 train_time:11764ms step_avg:33.23ms
step:355/2160 train_time:11797ms step_avg:33.23ms
step:356/2160 train_time:11830ms step_avg:33.23ms
step:357/2160 train_time:11863ms step_avg:33.23ms
step:358/2160 train_time:11896ms step_avg:33.23ms
step:359/2160 train_time:11929ms step_avg:33.23ms
step:360/2160 train_time:11961ms step_avg:33.23ms
step:361/2160 train_time:11994ms step_avg:33.22ms
step:362/2160 train_time:12026ms step_avg:33.22ms
step:363/2160 train_time:12059ms step_avg:33.22ms
step:364/2160 train_time:12092ms step_avg:33.22ms
step:365/2160 train_time:12125ms step_avg:33.22ms
step:366/2160 train_time:12158ms step_avg:33.22ms
step:367/2160 train_time:12191ms step_avg:33.22ms
step:368/2160 train_time:12223ms step_avg:33.22ms
step:369/2160 train_time:12256ms step_avg:33.21ms
step:370/2160 train_time:12288ms step_avg:33.21ms
step:371/2160 train_time:12321ms step_avg:33.21ms
step:372/2160 train_time:12353ms step_avg:33.21ms
step:373/2160 train_time:12387ms step_avg:33.21ms
step:374/2160 train_time:12419ms step_avg:33.21ms
step:375/2160 train_time:12452ms step_avg:33.21ms
step:376/2160 train_time:12484ms step_avg:33.20ms
step:377/2160 train_time:12517ms step_avg:33.20ms
step:378/2160 train_time:12549ms step_avg:33.20ms
step:379/2160 train_time:12583ms step_avg:33.20ms
step:380/2160 train_time:12615ms step_avg:33.20ms
step:381/2160 train_time:12648ms step_avg:33.20ms
step:382/2160 train_time:12681ms step_avg:33.20ms
step:383/2160 train_time:12714ms step_avg:33.19ms
step:384/2160 train_time:12746ms step_avg:33.19ms
step:385/2160 train_time:12779ms step_avg:33.19ms
step:386/2160 train_time:12811ms step_avg:33.19ms
step:387/2160 train_time:12845ms step_avg:33.19ms
step:388/2160 train_time:12877ms step_avg:33.19ms
step:389/2160 train_time:12911ms step_avg:33.19ms
step:390/2160 train_time:12943ms step_avg:33.19ms
step:391/2160 train_time:12976ms step_avg:33.19ms
step:392/2160 train_time:13008ms step_avg:33.18ms
step:393/2160 train_time:13041ms step_avg:33.18ms
step:394/2160 train_time:13074ms step_avg:33.18ms
step:395/2160 train_time:13107ms step_avg:33.18ms
step:396/2160 train_time:13139ms step_avg:33.18ms
step:397/2160 train_time:13173ms step_avg:33.18ms
step:398/2160 train_time:13205ms step_avg:33.18ms
step:399/2160 train_time:13238ms step_avg:33.18ms
step:400/2160 train_time:13270ms step_avg:33.17ms
step:401/2160 train_time:13303ms step_avg:33.18ms
step:402/2160 train_time:13336ms step_avg:33.17ms
step:403/2160 train_time:13369ms step_avg:33.17ms
step:404/2160 train_time:13401ms step_avg:33.17ms
step:405/2160 train_time:13434ms step_avg:33.17ms
step:406/2160 train_time:13467ms step_avg:33.17ms
step:407/2160 train_time:13500ms step_avg:33.17ms
step:408/2160 train_time:13532ms step_avg:33.17ms
step:409/2160 train_time:13566ms step_avg:33.17ms
step:410/2160 train_time:13598ms step_avg:33.17ms
step:411/2160 train_time:13631ms step_avg:33.17ms
step:412/2160 train_time:13663ms step_avg:33.16ms
step:413/2160 train_time:13696ms step_avg:33.16ms
step:414/2160 train_time:13728ms step_avg:33.16ms
step:415/2160 train_time:13762ms step_avg:33.16ms
step:416/2160 train_time:13794ms step_avg:33.16ms
step:417/2160 train_time:13827ms step_avg:33.16ms
step:418/2160 train_time:13860ms step_avg:33.16ms
step:419/2160 train_time:13893ms step_avg:33.16ms
step:420/2160 train_time:13925ms step_avg:33.15ms
step:421/2160 train_time:13958ms step_avg:33.15ms
step:422/2160 train_time:13990ms step_avg:33.15ms
step:423/2160 train_time:14024ms step_avg:33.15ms
step:424/2160 train_time:14056ms step_avg:33.15ms
step:425/2160 train_time:14089ms step_avg:33.15ms
step:426/2160 train_time:14121ms step_avg:33.15ms
step:427/2160 train_time:14154ms step_avg:33.15ms
step:428/2160 train_time:14186ms step_avg:33.15ms
step:429/2160 train_time:14220ms step_avg:33.15ms
step:430/2160 train_time:14252ms step_avg:33.15ms
step:431/2160 train_time:14286ms step_avg:33.15ms
step:432/2160 train_time:14318ms step_avg:33.14ms
step:433/2160 train_time:14351ms step_avg:33.14ms
step:434/2160 train_time:14383ms step_avg:33.14ms
step:435/2160 train_time:14416ms step_avg:33.14ms
step:436/2160 train_time:14448ms step_avg:33.14ms
step:437/2160 train_time:14482ms step_avg:33.14ms
step:438/2160 train_time:14514ms step_avg:33.14ms
step:439/2160 train_time:14548ms step_avg:33.14ms
step:440/2160 train_time:14580ms step_avg:33.14ms
step:441/2160 train_time:14613ms step_avg:33.14ms
step:442/2160 train_time:14645ms step_avg:33.13ms
step:443/2160 train_time:14678ms step_avg:33.13ms
step:444/2160 train_time:14711ms step_avg:33.13ms
step:445/2160 train_time:14744ms step_avg:33.13ms
step:446/2160 train_time:14776ms step_avg:33.13ms
step:447/2160 train_time:14810ms step_avg:33.13ms
step:448/2160 train_time:14842ms step_avg:33.13ms
step:449/2160 train_time:14876ms step_avg:33.13ms
step:450/2160 train_time:14908ms step_avg:33.13ms
step:451/2160 train_time:14942ms step_avg:33.13ms
step:452/2160 train_time:14974ms step_avg:33.13ms
step:453/2160 train_time:15007ms step_avg:33.13ms
step:454/2160 train_time:15040ms step_avg:33.13ms
step:455/2160 train_time:15072ms step_avg:33.13ms
step:456/2160 train_time:15105ms step_avg:33.12ms
step:457/2160 train_time:15138ms step_avg:33.12ms
step:458/2160 train_time:15170ms step_avg:33.12ms
step:459/2160 train_time:15204ms step_avg:33.12ms
step:460/2160 train_time:15236ms step_avg:33.12ms
step:461/2160 train_time:15269ms step_avg:33.12ms
step:462/2160 train_time:15301ms step_avg:33.12ms
step:463/2160 train_time:15334ms step_avg:33.12ms
step:464/2160 train_time:15366ms step_avg:33.12ms
step:465/2160 train_time:15400ms step_avg:33.12ms
step:466/2160 train_time:15432ms step_avg:33.12ms
step:467/2160 train_time:15465ms step_avg:33.12ms
step:468/2160 train_time:15497ms step_avg:33.11ms
step:469/2160 train_time:15530ms step_avg:33.11ms
step:470/2160 train_time:15563ms step_avg:33.11ms
step:471/2160 train_time:15596ms step_avg:33.11ms
step:472/2160 train_time:15628ms step_avg:33.11ms
step:473/2160 train_time:15661ms step_avg:33.11ms
step:474/2160 train_time:15694ms step_avg:33.11ms
step:475/2160 train_time:15727ms step_avg:33.11ms
step:476/2160 train_time:15759ms step_avg:33.11ms
step:477/2160 train_time:15793ms step_avg:33.11ms
step:478/2160 train_time:15825ms step_avg:33.11ms
step:479/2160 train_time:15858ms step_avg:33.11ms
step:480/2160 train_time:15890ms step_avg:33.10ms
step:481/2160 train_time:15923ms step_avg:33.10ms
step:482/2160 train_time:15955ms step_avg:33.10ms
step:483/2160 train_time:15989ms step_avg:33.10ms
step:484/2160 train_time:16021ms step_avg:33.10ms
step:485/2160 train_time:16055ms step_avg:33.10ms
step:486/2160 train_time:16087ms step_avg:33.10ms
step:487/2160 train_time:16120ms step_avg:33.10ms
step:488/2160 train_time:16152ms step_avg:33.10ms
step:489/2160 train_time:16186ms step_avg:33.10ms
step:490/2160 train_time:16218ms step_avg:33.10ms
step:491/2160 train_time:16251ms step_avg:33.10ms
step:492/2160 train_time:16284ms step_avg:33.10ms
step:493/2160 train_time:16317ms step_avg:33.10ms
step:494/2160 train_time:16349ms step_avg:33.09ms
step:495/2160 train_time:16382ms step_avg:33.09ms
step:496/2160 train_time:16414ms step_avg:33.09ms
step:497/2160 train_time:16447ms step_avg:33.09ms
step:498/2160 train_time:16480ms step_avg:33.09ms
step:499/2160 train_time:16513ms step_avg:33.09ms
step:500/2160 train_time:16545ms step_avg:33.09ms
step:500/2160 val_loss:4.0160 train_time:16581ms step_avg:33.16ms
step:501/2160 train_time:16603ms step_avg:33.14ms
step:502/2160 train_time:16625ms step_avg:33.12ms
step:503/2160 train_time:16648ms step_avg:33.10ms
step:504/2160 train_time:16681ms step_avg:33.10ms
step:505/2160 train_time:16715ms step_avg:33.10ms
step:506/2160 train_time:16749ms step_avg:33.10ms
step:507/2160 train_time:16784ms step_avg:33.10ms
step:508/2160 train_time:16816ms step_avg:33.10ms
step:509/2160 train_time:16850ms step_avg:33.10ms
step:510/2160 train_time:16882ms step_avg:33.10ms
step:511/2160 train_time:16916ms step_avg:33.10ms
step:512/2160 train_time:16948ms step_avg:33.10ms
step:513/2160 train_time:16981ms step_avg:33.10ms
step:514/2160 train_time:17013ms step_avg:33.10ms
step:515/2160 train_time:17046ms step_avg:33.10ms
step:516/2160 train_time:17078ms step_avg:33.10ms
step:517/2160 train_time:17111ms step_avg:33.10ms
step:518/2160 train_time:17143ms step_avg:33.09ms
step:519/2160 train_time:17176ms step_avg:33.09ms
step:520/2160 train_time:17208ms step_avg:33.09ms
step:521/2160 train_time:17241ms step_avg:33.09ms
step:522/2160 train_time:17273ms step_avg:33.09ms
step:523/2160 train_time:17306ms step_avg:33.09ms
step:524/2160 train_time:17338ms step_avg:33.09ms
step:525/2160 train_time:17371ms step_avg:33.09ms
step:526/2160 train_time:17403ms step_avg:33.09ms
step:527/2160 train_time:17436ms step_avg:33.09ms
step:528/2160 train_time:17468ms step_avg:33.08ms
step:529/2160 train_time:17501ms step_avg:33.08ms
step:530/2160 train_time:17533ms step_avg:33.08ms
step:531/2160 train_time:17567ms step_avg:33.08ms
step:532/2160 train_time:17599ms step_avg:33.08ms
step:533/2160 train_time:17633ms step_avg:33.08ms
step:534/2160 train_time:17665ms step_avg:33.08ms
step:535/2160 train_time:17700ms step_avg:33.08ms
step:536/2160 train_time:17732ms step_avg:33.08ms
step:537/2160 train_time:17767ms step_avg:33.09ms
step:538/2160 train_time:17799ms step_avg:33.08ms
step:539/2160 train_time:17833ms step_avg:33.09ms
step:540/2160 train_time:17866ms step_avg:33.08ms
step:541/2160 train_time:17899ms step_avg:33.08ms
step:542/2160 train_time:17931ms step_avg:33.08ms
step:543/2160 train_time:17964ms step_avg:33.08ms
step:544/2160 train_time:17997ms step_avg:33.08ms
step:545/2160 train_time:18030ms step_avg:33.08ms
step:546/2160 train_time:18062ms step_avg:33.08ms
step:547/2160 train_time:18095ms step_avg:33.08ms
step:548/2160 train_time:18127ms step_avg:33.08ms
step:549/2160 train_time:18160ms step_avg:33.08ms
step:550/2160 train_time:18192ms step_avg:33.08ms
step:551/2160 train_time:18226ms step_avg:33.08ms
step:552/2160 train_time:18258ms step_avg:33.08ms
step:553/2160 train_time:18291ms step_avg:33.08ms
step:554/2160 train_time:18323ms step_avg:33.07ms
step:555/2160 train_time:18356ms step_avg:33.07ms
step:556/2160 train_time:18388ms step_avg:33.07ms
step:557/2160 train_time:18421ms step_avg:33.07ms
step:558/2160 train_time:18453ms step_avg:33.07ms
step:559/2160 train_time:18486ms step_avg:33.07ms
step:560/2160 train_time:18518ms step_avg:33.07ms
step:561/2160 train_time:18552ms step_avg:33.07ms
step:562/2160 train_time:18584ms step_avg:33.07ms
step:563/2160 train_time:18617ms step_avg:33.07ms
step:564/2160 train_time:18650ms step_avg:33.07ms
step:565/2160 train_time:18684ms step_avg:33.07ms
step:566/2160 train_time:18716ms step_avg:33.07ms
step:567/2160 train_time:18750ms step_avg:33.07ms
step:568/2160 train_time:18782ms step_avg:33.07ms
step:569/2160 train_time:18816ms step_avg:33.07ms
step:570/2160 train_time:18849ms step_avg:33.07ms
step:571/2160 train_time:18883ms step_avg:33.07ms
step:572/2160 train_time:18915ms step_avg:33.07ms
step:573/2160 train_time:18949ms step_avg:33.07ms
step:574/2160 train_time:18981ms step_avg:33.07ms
step:575/2160 train_time:19014ms step_avg:33.07ms
step:576/2160 train_time:19047ms step_avg:33.07ms
step:577/2160 train_time:19080ms step_avg:33.07ms
step:578/2160 train_time:19112ms step_avg:33.07ms
step:579/2160 train_time:19145ms step_avg:33.07ms
step:580/2160 train_time:19177ms step_avg:33.06ms
step:581/2160 train_time:19210ms step_avg:33.06ms
step:582/2160 train_time:19243ms step_avg:33.06ms
step:583/2160 train_time:19276ms step_avg:33.06ms
step:584/2160 train_time:19308ms step_avg:33.06ms
step:585/2160 train_time:19341ms step_avg:33.06ms
step:586/2160 train_time:19373ms step_avg:33.06ms
step:587/2160 train_time:19407ms step_avg:33.06ms
step:588/2160 train_time:19439ms step_avg:33.06ms
step:589/2160 train_time:19472ms step_avg:33.06ms
step:590/2160 train_time:19504ms step_avg:33.06ms
step:591/2160 train_time:19537ms step_avg:33.06ms
step:592/2160 train_time:19569ms step_avg:33.06ms
step:593/2160 train_time:19603ms step_avg:33.06ms
step:594/2160 train_time:19635ms step_avg:33.06ms
step:595/2160 train_time:19669ms step_avg:33.06ms
step:596/2160 train_time:19701ms step_avg:33.05ms
step:597/2160 train_time:19734ms step_avg:33.06ms
step:598/2160 train_time:19767ms step_avg:33.05ms
step:599/2160 train_time:19801ms step_avg:33.06ms
step:600/2160 train_time:19833ms step_avg:33.05ms
step:601/2160 train_time:19866ms step_avg:33.06ms
step:602/2160 train_time:19898ms step_avg:33.05ms
step:603/2160 train_time:19932ms step_avg:33.05ms
step:604/2160 train_time:19964ms step_avg:33.05ms
step:605/2160 train_time:19997ms step_avg:33.05ms
step:606/2160 train_time:20030ms step_avg:33.05ms
step:607/2160 train_time:20063ms step_avg:33.05ms
step:608/2160 train_time:20096ms step_avg:33.05ms
step:609/2160 train_time:20129ms step_avg:33.05ms
step:610/2160 train_time:20161ms step_avg:33.05ms
step:611/2160 train_time:20195ms step_avg:33.05ms
step:612/2160 train_time:20227ms step_avg:33.05ms
step:613/2160 train_time:20260ms step_avg:33.05ms
step:614/2160 train_time:20293ms step_avg:33.05ms
step:615/2160 train_time:20326ms step_avg:33.05ms
step:616/2160 train_time:20358ms step_avg:33.05ms
step:617/2160 train_time:20391ms step_avg:33.05ms
step:618/2160 train_time:20424ms step_avg:33.05ms
step:619/2160 train_time:20457ms step_avg:33.05ms
step:620/2160 train_time:20489ms step_avg:33.05ms
step:621/2160 train_time:20523ms step_avg:33.05ms
step:622/2160 train_time:20555ms step_avg:33.05ms
step:623/2160 train_time:20589ms step_avg:33.05ms
step:624/2160 train_time:20621ms step_avg:33.05ms
step:625/2160 train_time:20654ms step_avg:33.05ms
step:626/2160 train_time:20687ms step_avg:33.05ms
step:627/2160 train_time:20720ms step_avg:33.05ms
step:628/2160 train_time:20752ms step_avg:33.05ms
step:629/2160 train_time:20786ms step_avg:33.05ms
step:630/2160 train_time:20818ms step_avg:33.04ms
step:631/2160 train_time:20851ms step_avg:33.05ms
step:632/2160 train_time:20884ms step_avg:33.04ms
step:633/2160 train_time:20917ms step_avg:33.04ms
step:634/2160 train_time:20950ms step_avg:33.04ms
step:635/2160 train_time:20983ms step_avg:33.04ms
step:636/2160 train_time:21016ms step_avg:33.04ms
step:637/2160 train_time:21049ms step_avg:33.04ms
step:638/2160 train_time:21081ms step_avg:33.04ms
step:639/2160 train_time:21114ms step_avg:33.04ms
step:640/2160 train_time:21146ms step_avg:33.04ms
step:641/2160 train_time:21179ms step_avg:33.04ms
step:642/2160 train_time:21212ms step_avg:33.04ms
step:643/2160 train_time:21245ms step_avg:33.04ms
step:644/2160 train_time:21278ms step_avg:33.04ms
step:645/2160 train_time:21311ms step_avg:33.04ms
step:646/2160 train_time:21343ms step_avg:33.04ms
step:647/2160 train_time:21376ms step_avg:33.04ms
step:648/2160 train_time:21409ms step_avg:33.04ms
step:649/2160 train_time:21442ms step_avg:33.04ms
step:650/2160 train_time:21474ms step_avg:33.04ms
step:651/2160 train_time:21508ms step_avg:33.04ms
step:652/2160 train_time:21540ms step_avg:33.04ms
step:653/2160 train_time:21573ms step_avg:33.04ms
step:654/2160 train_time:21606ms step_avg:33.04ms
step:655/2160 train_time:21639ms step_avg:33.04ms
step:656/2160 train_time:21671ms step_avg:33.03ms
step:657/2160 train_time:21705ms step_avg:33.04ms
step:658/2160 train_time:21737ms step_avg:33.03ms
step:659/2160 train_time:21770ms step_avg:33.03ms
step:660/2160 train_time:21802ms step_avg:33.03ms
step:661/2160 train_time:21836ms step_avg:33.03ms
step:662/2160 train_time:21868ms step_avg:33.03ms
step:663/2160 train_time:21901ms step_avg:33.03ms
step:664/2160 train_time:21934ms step_avg:33.03ms
step:665/2160 train_time:21968ms step_avg:33.03ms
step:666/2160 train_time:22000ms step_avg:33.03ms
step:667/2160 train_time:22034ms step_avg:33.03ms
step:668/2160 train_time:22066ms step_avg:33.03ms
step:669/2160 train_time:22099ms step_avg:33.03ms
step:670/2160 train_time:22131ms step_avg:33.03ms
step:671/2160 train_time:22165ms step_avg:33.03ms
step:672/2160 train_time:22197ms step_avg:33.03ms
step:673/2160 train_time:22231ms step_avg:33.03ms
step:674/2160 train_time:22263ms step_avg:33.03ms
step:675/2160 train_time:22296ms step_avg:33.03ms
step:676/2160 train_time:22329ms step_avg:33.03ms
step:677/2160 train_time:22362ms step_avg:33.03ms
step:678/2160 train_time:22394ms step_avg:33.03ms
step:679/2160 train_time:22428ms step_avg:33.03ms
step:680/2160 train_time:22460ms step_avg:33.03ms
step:681/2160 train_time:22493ms step_avg:33.03ms
step:682/2160 train_time:22525ms step_avg:33.03ms
step:683/2160 train_time:22558ms step_avg:33.03ms
step:684/2160 train_time:22591ms step_avg:33.03ms
step:685/2160 train_time:22625ms step_avg:33.03ms
step:686/2160 train_time:22657ms step_avg:33.03ms
step:687/2160 train_time:22690ms step_avg:33.03ms
step:688/2160 train_time:22722ms step_avg:33.03ms
step:689/2160 train_time:22756ms step_avg:33.03ms
step:690/2160 train_time:22788ms step_avg:33.03ms
step:691/2160 train_time:22823ms step_avg:33.03ms
step:692/2160 train_time:22855ms step_avg:33.03ms
step:693/2160 train_time:22888ms step_avg:33.03ms
step:694/2160 train_time:22920ms step_avg:33.03ms
step:695/2160 train_time:22954ms step_avg:33.03ms
step:696/2160 train_time:22986ms step_avg:33.03ms
step:697/2160 train_time:23020ms step_avg:33.03ms
step:698/2160 train_time:23052ms step_avg:33.03ms
step:699/2160 train_time:23085ms step_avg:33.03ms
step:700/2160 train_time:23118ms step_avg:33.03ms
step:701/2160 train_time:23151ms step_avg:33.03ms
step:702/2160 train_time:23183ms step_avg:33.02ms
step:703/2160 train_time:23216ms step_avg:33.02ms
step:704/2160 train_time:23249ms step_avg:33.02ms
step:705/2160 train_time:23282ms step_avg:33.02ms
step:706/2160 train_time:23314ms step_avg:33.02ms
step:707/2160 train_time:23348ms step_avg:33.02ms
step:708/2160 train_time:23381ms step_avg:33.02ms
step:709/2160 train_time:23439ms step_avg:33.06ms
step:710/2160 train_time:23498ms step_avg:33.10ms
step:711/2160 train_time:23558ms step_avg:33.13ms
step:712/2160 train_time:23617ms step_avg:33.17ms
step:713/2160 train_time:23677ms step_avg:33.21ms
step:714/2160 train_time:23736ms step_avg:33.24ms
step:715/2160 train_time:23797ms step_avg:33.28ms
step:716/2160 train_time:23856ms step_avg:33.32ms
step:717/2160 train_time:23917ms step_avg:33.36ms
step:718/2160 train_time:23976ms step_avg:33.39ms
step:719/2160 train_time:24037ms step_avg:33.43ms
step:720/2160 train_time:24096ms step_avg:33.47ms
step:721/2160 train_time:24156ms step_avg:33.50ms
step:722/2160 train_time:24216ms step_avg:33.54ms
step:723/2160 train_time:24277ms step_avg:33.58ms
step:724/2160 train_time:24335ms step_avg:33.61ms
step:725/2160 train_time:24395ms step_avg:33.65ms
step:726/2160 train_time:24454ms step_avg:33.68ms
step:727/2160 train_time:24514ms step_avg:33.72ms
step:728/2160 train_time:24573ms step_avg:33.75ms
step:729/2160 train_time:24634ms step_avg:33.79ms
step:730/2160 train_time:24693ms step_avg:33.83ms
step:731/2160 train_time:24754ms step_avg:33.86ms
step:732/2160 train_time:24814ms step_avg:33.90ms
step:733/2160 train_time:24874ms step_avg:33.93ms
step:734/2160 train_time:24934ms step_avg:33.97ms
step:735/2160 train_time:24995ms step_avg:34.01ms
step:736/2160 train_time:25054ms step_avg:34.04ms
step:737/2160 train_time:25115ms step_avg:34.08ms
step:738/2160 train_time:25175ms step_avg:34.11ms
step:739/2160 train_time:25236ms step_avg:34.15ms
step:740/2160 train_time:25294ms step_avg:34.18ms
step:741/2160 train_time:25355ms step_avg:34.22ms
step:742/2160 train_time:25414ms step_avg:34.25ms
step:743/2160 train_time:25475ms step_avg:34.29ms
step:744/2160 train_time:25534ms step_avg:34.32ms
step:745/2160 train_time:25595ms step_avg:34.36ms
step:746/2160 train_time:25654ms step_avg:34.39ms
step:747/2160 train_time:25715ms step_avg:34.42ms
step:748/2160 train_time:25774ms step_avg:34.46ms
step:749/2160 train_time:25835ms step_avg:34.49ms
step:750/2160 train_time:25893ms step_avg:34.52ms
step:750/2160 val_loss:3.8652 train_time:25956ms step_avg:34.61ms
step:751/2160 train_time:25978ms step_avg:34.59ms
step:752/2160 train_time:26017ms step_avg:34.60ms
step:753/2160 train_time:26080ms step_avg:34.64ms
step:754/2160 train_time:26142ms step_avg:34.67ms
step:755/2160 train_time:26202ms step_avg:34.70ms
step:756/2160 train_time:26261ms step_avg:34.74ms
step:757/2160 train_time:26321ms step_avg:34.77ms
step:758/2160 train_time:26379ms step_avg:34.80ms
step:759/2160 train_time:26438ms step_avg:34.83ms
step:760/2160 train_time:26496ms step_avg:34.86ms
step:761/2160 train_time:26556ms step_avg:34.90ms
step:762/2160 train_time:26614ms step_avg:34.93ms
step:763/2160 train_time:26673ms step_avg:34.96ms
step:764/2160 train_time:26731ms step_avg:34.99ms
step:765/2160 train_time:26792ms step_avg:35.02ms
step:766/2160 train_time:26850ms step_avg:35.05ms
step:767/2160 train_time:26911ms step_avg:35.09ms
step:768/2160 train_time:26972ms step_avg:35.12ms
step:769/2160 train_time:27034ms step_avg:35.15ms
step:770/2160 train_time:27095ms step_avg:35.19ms
step:771/2160 train_time:27158ms step_avg:35.22ms
step:772/2160 train_time:27218ms step_avg:35.26ms
step:773/2160 train_time:27279ms step_avg:35.29ms
step:774/2160 train_time:27338ms step_avg:35.32ms
step:775/2160 train_time:27398ms step_avg:35.35ms
step:776/2160 train_time:27457ms step_avg:35.38ms
step:777/2160 train_time:27516ms step_avg:35.41ms
step:778/2160 train_time:27575ms step_avg:35.44ms
step:779/2160 train_time:27634ms step_avg:35.47ms
step:780/2160 train_time:27692ms step_avg:35.50ms
step:781/2160 train_time:27753ms step_avg:35.53ms
step:782/2160 train_time:27811ms step_avg:35.56ms
step:783/2160 train_time:27872ms step_avg:35.60ms
step:784/2160 train_time:27932ms step_avg:35.63ms
step:785/2160 train_time:27993ms step_avg:35.66ms
step:786/2160 train_time:28053ms step_avg:35.69ms
step:787/2160 train_time:28117ms step_avg:35.73ms
step:788/2160 train_time:28178ms step_avg:35.76ms
step:789/2160 train_time:28238ms step_avg:35.79ms
step:790/2160 train_time:28297ms step_avg:35.82ms
step:791/2160 train_time:28358ms step_avg:35.85ms
step:792/2160 train_time:28417ms step_avg:35.88ms
step:793/2160 train_time:28477ms step_avg:35.91ms
step:794/2160 train_time:28535ms step_avg:35.94ms
step:795/2160 train_time:28595ms step_avg:35.97ms
step:796/2160 train_time:28653ms step_avg:36.00ms
step:797/2160 train_time:28713ms step_avg:36.03ms
step:798/2160 train_time:28771ms step_avg:36.05ms
step:799/2160 train_time:28831ms step_avg:36.08ms
step:800/2160 train_time:28890ms step_avg:36.11ms
step:801/2160 train_time:28952ms step_avg:36.15ms
step:802/2160 train_time:29013ms step_avg:36.18ms
step:803/2160 train_time:29076ms step_avg:36.21ms
step:804/2160 train_time:29136ms step_avg:36.24ms
step:805/2160 train_time:29197ms step_avg:36.27ms
step:806/2160 train_time:29257ms step_avg:36.30ms
step:807/2160 train_time:29318ms step_avg:36.33ms
step:808/2160 train_time:29377ms step_avg:36.36ms
step:809/2160 train_time:29437ms step_avg:36.39ms
step:810/2160 train_time:29496ms step_avg:36.42ms
step:811/2160 train_time:29557ms step_avg:36.45ms
step:812/2160 train_time:29616ms step_avg:36.47ms
step:813/2160 train_time:29676ms step_avg:36.50ms
step:814/2160 train_time:29734ms step_avg:36.53ms
step:815/2160 train_time:29794ms step_avg:36.56ms
step:816/2160 train_time:29854ms step_avg:36.59ms
step:817/2160 train_time:29915ms step_avg:36.62ms
step:818/2160 train_time:29975ms step_avg:36.64ms
step:819/2160 train_time:30037ms step_avg:36.67ms
step:820/2160 train_time:30097ms step_avg:36.70ms
step:821/2160 train_time:30158ms step_avg:36.73ms
step:822/2160 train_time:30218ms step_avg:36.76ms
step:823/2160 train_time:30279ms step_avg:36.79ms
step:824/2160 train_time:30338ms step_avg:36.82ms
step:825/2160 train_time:30398ms step_avg:36.85ms
step:826/2160 train_time:30457ms step_avg:36.87ms
step:827/2160 train_time:30517ms step_avg:36.90ms
step:828/2160 train_time:30577ms step_avg:36.93ms
step:829/2160 train_time:30636ms step_avg:36.96ms
step:830/2160 train_time:30695ms step_avg:36.98ms
step:831/2160 train_time:30754ms step_avg:37.01ms
step:832/2160 train_time:30813ms step_avg:37.04ms
step:833/2160 train_time:30874ms step_avg:37.06ms
step:834/2160 train_time:30934ms step_avg:37.09ms
step:835/2160 train_time:30995ms step_avg:37.12ms
step:836/2160 train_time:31056ms step_avg:37.15ms
step:837/2160 train_time:31117ms step_avg:37.18ms
step:838/2160 train_time:31177ms step_avg:37.20ms
step:839/2160 train_time:31238ms step_avg:37.23ms
step:840/2160 train_time:31297ms step_avg:37.26ms
step:841/2160 train_time:31357ms step_avg:37.29ms
step:842/2160 train_time:31417ms step_avg:37.31ms
step:843/2160 train_time:31478ms step_avg:37.34ms
step:844/2160 train_time:31536ms step_avg:37.37ms
step:845/2160 train_time:31597ms step_avg:37.39ms
step:846/2160 train_time:31656ms step_avg:37.42ms
step:847/2160 train_time:31716ms step_avg:37.44ms
step:848/2160 train_time:31775ms step_avg:37.47ms
step:849/2160 train_time:31834ms step_avg:37.50ms
step:850/2160 train_time:31894ms step_avg:37.52ms
step:851/2160 train_time:31955ms step_avg:37.55ms
step:852/2160 train_time:32016ms step_avg:37.58ms
step:853/2160 train_time:32077ms step_avg:37.60ms
step:854/2160 train_time:32137ms step_avg:37.63ms
step:855/2160 train_time:32198ms step_avg:37.66ms
step:856/2160 train_time:32257ms step_avg:37.68ms
step:857/2160 train_time:32318ms step_avg:37.71ms
step:858/2160 train_time:32378ms step_avg:37.74ms
step:859/2160 train_time:32437ms step_avg:37.76ms
step:860/2160 train_time:32497ms step_avg:37.79ms
step:861/2160 train_time:32556ms step_avg:37.81ms
step:862/2160 train_time:32615ms step_avg:37.84ms
step:863/2160 train_time:32676ms step_avg:37.86ms
step:864/2160 train_time:32735ms step_avg:37.89ms
step:865/2160 train_time:32795ms step_avg:37.91ms
step:866/2160 train_time:32853ms step_avg:37.94ms
step:867/2160 train_time:32914ms step_avg:37.96ms
step:868/2160 train_time:32973ms step_avg:37.99ms
step:869/2160 train_time:33034ms step_avg:38.01ms
step:870/2160 train_time:33094ms step_avg:38.04ms
step:871/2160 train_time:33156ms step_avg:38.07ms
step:872/2160 train_time:33216ms step_avg:38.09ms
step:873/2160 train_time:33277ms step_avg:38.12ms
step:874/2160 train_time:33336ms step_avg:38.14ms
step:875/2160 train_time:33396ms step_avg:38.17ms
step:876/2160 train_time:33455ms step_avg:38.19ms
step:877/2160 train_time:33516ms step_avg:38.22ms
step:878/2160 train_time:33575ms step_avg:38.24ms
step:879/2160 train_time:33635ms step_avg:38.27ms
step:880/2160 train_time:33694ms step_avg:38.29ms
step:881/2160 train_time:33755ms step_avg:38.31ms
step:882/2160 train_time:33814ms step_avg:38.34ms
step:883/2160 train_time:33875ms step_avg:38.36ms
step:884/2160 train_time:33935ms step_avg:38.39ms
step:885/2160 train_time:33996ms step_avg:38.41ms
step:886/2160 train_time:34055ms step_avg:38.44ms
step:887/2160 train_time:34117ms step_avg:38.46ms
step:888/2160 train_time:34177ms step_avg:38.49ms
step:889/2160 train_time:34237ms step_avg:38.51ms
step:890/2160 train_time:34296ms step_avg:38.54ms
step:891/2160 train_time:34357ms step_avg:38.56ms
step:892/2160 train_time:34417ms step_avg:38.58ms
step:893/2160 train_time:34477ms step_avg:38.61ms
step:894/2160 train_time:34537ms step_avg:38.63ms
step:895/2160 train_time:34597ms step_avg:38.66ms
step:896/2160 train_time:34656ms step_avg:38.68ms
step:897/2160 train_time:34716ms step_avg:38.70ms
step:898/2160 train_time:34775ms step_avg:38.73ms
step:899/2160 train_time:34836ms step_avg:38.75ms
step:900/2160 train_time:34895ms step_avg:38.77ms
step:901/2160 train_time:34956ms step_avg:38.80ms
step:902/2160 train_time:35015ms step_avg:38.82ms
step:903/2160 train_time:35077ms step_avg:38.84ms
step:904/2160 train_time:35136ms step_avg:38.87ms
step:905/2160 train_time:35197ms step_avg:38.89ms
step:906/2160 train_time:35256ms step_avg:38.91ms
step:907/2160 train_time:35317ms step_avg:38.94ms
step:908/2160 train_time:35376ms step_avg:38.96ms
step:909/2160 train_time:35437ms step_avg:38.98ms
step:910/2160 train_time:35496ms step_avg:39.01ms
step:911/2160 train_time:35557ms step_avg:39.03ms
step:912/2160 train_time:35616ms step_avg:39.05ms
step:913/2160 train_time:35676ms step_avg:39.08ms
step:914/2160 train_time:35736ms step_avg:39.10ms
step:915/2160 train_time:35797ms step_avg:39.12ms
step:916/2160 train_time:35856ms step_avg:39.14ms
step:917/2160 train_time:35917ms step_avg:39.17ms
step:918/2160 train_time:35976ms step_avg:39.19ms
step:919/2160 train_time:36037ms step_avg:39.21ms
step:920/2160 train_time:36096ms step_avg:39.24ms
step:921/2160 train_time:36158ms step_avg:39.26ms
step:922/2160 train_time:36216ms step_avg:39.28ms
step:923/2160 train_time:36278ms step_avg:39.30ms
step:924/2160 train_time:36337ms step_avg:39.33ms
step:925/2160 train_time:36397ms step_avg:39.35ms
step:926/2160 train_time:36457ms step_avg:39.37ms
step:927/2160 train_time:36518ms step_avg:39.39ms
step:928/2160 train_time:36578ms step_avg:39.42ms
step:929/2160 train_time:36637ms step_avg:39.44ms
step:930/2160 train_time:36696ms step_avg:39.46ms
step:931/2160 train_time:36757ms step_avg:39.48ms
step:932/2160 train_time:36816ms step_avg:39.50ms
step:933/2160 train_time:36876ms step_avg:39.52ms
step:934/2160 train_time:36936ms step_avg:39.55ms
step:935/2160 train_time:36997ms step_avg:39.57ms
step:936/2160 train_time:37057ms step_avg:39.59ms
step:937/2160 train_time:37118ms step_avg:39.61ms
step:938/2160 train_time:37177ms step_avg:39.63ms
step:939/2160 train_time:37237ms step_avg:39.66ms
step:940/2160 train_time:37297ms step_avg:39.68ms
step:941/2160 train_time:37358ms step_avg:39.70ms
step:942/2160 train_time:37417ms step_avg:39.72ms
step:943/2160 train_time:37478ms step_avg:39.74ms
step:944/2160 train_time:37537ms step_avg:39.76ms
step:945/2160 train_time:37597ms step_avg:39.79ms
step:946/2160 train_time:37657ms step_avg:39.81ms
step:947/2160 train_time:37718ms step_avg:39.83ms
step:948/2160 train_time:37778ms step_avg:39.85ms
step:949/2160 train_time:37838ms step_avg:39.87ms
step:950/2160 train_time:37897ms step_avg:39.89ms
step:951/2160 train_time:37958ms step_avg:39.91ms
step:952/2160 train_time:38017ms step_avg:39.93ms
step:953/2160 train_time:38078ms step_avg:39.96ms
step:954/2160 train_time:38137ms step_avg:39.98ms
step:955/2160 train_time:38198ms step_avg:40.00ms
step:956/2160 train_time:38257ms step_avg:40.02ms
step:957/2160 train_time:38317ms step_avg:40.04ms
step:958/2160 train_time:38376ms step_avg:40.06ms
step:959/2160 train_time:38436ms step_avg:40.08ms
step:960/2160 train_time:38496ms step_avg:40.10ms
step:961/2160 train_time:38556ms step_avg:40.12ms
step:962/2160 train_time:38616ms step_avg:40.14ms
step:963/2160 train_time:38677ms step_avg:40.16ms
step:964/2160 train_time:38737ms step_avg:40.18ms
step:965/2160 train_time:38797ms step_avg:40.20ms
step:966/2160 train_time:38857ms step_avg:40.22ms
step:967/2160 train_time:38918ms step_avg:40.25ms
step:968/2160 train_time:38977ms step_avg:40.27ms
step:969/2160 train_time:39037ms step_avg:40.29ms
step:970/2160 train_time:39096ms step_avg:40.30ms
step:971/2160 train_time:39157ms step_avg:40.33ms
step:972/2160 train_time:39216ms step_avg:40.35ms
step:973/2160 train_time:39278ms step_avg:40.37ms
step:974/2160 train_time:39337ms step_avg:40.39ms
step:975/2160 train_time:39398ms step_avg:40.41ms
step:976/2160 train_time:39457ms step_avg:40.43ms
step:977/2160 train_time:39517ms step_avg:40.45ms
step:978/2160 train_time:39577ms step_avg:40.47ms
step:979/2160 train_time:39638ms step_avg:40.49ms
step:980/2160 train_time:39697ms step_avg:40.51ms
step:981/2160 train_time:39758ms step_avg:40.53ms
step:982/2160 train_time:39817ms step_avg:40.55ms
step:983/2160 train_time:39877ms step_avg:40.57ms
step:984/2160 train_time:39937ms step_avg:40.59ms
step:985/2160 train_time:39997ms step_avg:40.61ms
step:986/2160 train_time:40056ms step_avg:40.63ms
step:987/2160 train_time:40118ms step_avg:40.65ms
step:988/2160 train_time:40177ms step_avg:40.67ms
step:989/2160 train_time:40237ms step_avg:40.68ms
step:990/2160 train_time:40297ms step_avg:40.70ms
step:991/2160 train_time:40358ms step_avg:40.72ms
step:992/2160 train_time:40416ms step_avg:40.74ms
step:993/2160 train_time:40477ms step_avg:40.76ms
step:994/2160 train_time:40536ms step_avg:40.78ms
step:995/2160 train_time:40596ms step_avg:40.80ms
step:996/2160 train_time:40656ms step_avg:40.82ms
step:997/2160 train_time:40717ms step_avg:40.84ms
step:998/2160 train_time:40776ms step_avg:40.86ms
step:999/2160 train_time:40837ms step_avg:40.88ms
step:1000/2160 train_time:40896ms step_avg:40.90ms
step:1000/2160 val_loss:3.7094 train_time:40960ms step_avg:40.96ms
step:1001/2160 train_time:40983ms step_avg:40.94ms
step:1002/2160 train_time:41018ms step_avg:40.94ms
step:1003/2160 train_time:41083ms step_avg:40.96ms
step:1004/2160 train_time:41145ms step_avg:40.98ms
step:1005/2160 train_time:41205ms step_avg:41.00ms
step:1006/2160 train_time:41265ms step_avg:41.02ms
step:1007/2160 train_time:41325ms step_avg:41.04ms
step:1008/2160 train_time:41383ms step_avg:41.05ms
step:1009/2160 train_time:41443ms step_avg:41.07ms
step:1010/2160 train_time:41501ms step_avg:41.09ms
step:1011/2160 train_time:41561ms step_avg:41.11ms
step:1012/2160 train_time:41619ms step_avg:41.13ms
step:1013/2160 train_time:41678ms step_avg:41.14ms
step:1014/2160 train_time:41736ms step_avg:41.16ms
step:1015/2160 train_time:41796ms step_avg:41.18ms
step:1016/2160 train_time:41854ms step_avg:41.19ms
step:1017/2160 train_time:41916ms step_avg:41.21ms
step:1018/2160 train_time:41976ms step_avg:41.23ms
step:1019/2160 train_time:42039ms step_avg:41.25ms
step:1020/2160 train_time:42099ms step_avg:41.27ms
step:1021/2160 train_time:42160ms step_avg:41.29ms
step:1022/2160 train_time:42220ms step_avg:41.31ms
step:1023/2160 train_time:42280ms step_avg:41.33ms
step:1024/2160 train_time:42339ms step_avg:41.35ms
step:1025/2160 train_time:42399ms step_avg:41.36ms
step:1026/2160 train_time:42457ms step_avg:41.38ms
step:1027/2160 train_time:42517ms step_avg:41.40ms
step:1028/2160 train_time:42575ms step_avg:41.42ms
step:1029/2160 train_time:42635ms step_avg:41.43ms
step:1030/2160 train_time:42693ms step_avg:41.45ms
step:1031/2160 train_time:42753ms step_avg:41.47ms
step:1032/2160 train_time:42813ms step_avg:41.49ms
step:1033/2160 train_time:42873ms step_avg:41.50ms
step:1034/2160 train_time:42933ms step_avg:41.52ms
step:1035/2160 train_time:42995ms step_avg:41.54ms
step:1036/2160 train_time:43055ms step_avg:41.56ms
step:1037/2160 train_time:43117ms step_avg:41.58ms
step:1038/2160 train_time:43177ms step_avg:41.60ms
step:1039/2160 train_time:43238ms step_avg:41.61ms
step:1040/2160 train_time:43297ms step_avg:41.63ms
step:1041/2160 train_time:43358ms step_avg:41.65ms
step:1042/2160 train_time:43416ms step_avg:41.67ms
step:1043/2160 train_time:43477ms step_avg:41.68ms
step:1044/2160 train_time:43536ms step_avg:41.70ms
step:1045/2160 train_time:43595ms step_avg:41.72ms
step:1046/2160 train_time:43654ms step_avg:41.73ms
step:1047/2160 train_time:43713ms step_avg:41.75ms
step:1048/2160 train_time:43772ms step_avg:41.77ms
step:1049/2160 train_time:43833ms step_avg:41.79ms
step:1050/2160 train_time:43892ms step_avg:41.80ms
step:1051/2160 train_time:43954ms step_avg:41.82ms
step:1052/2160 train_time:44014ms step_avg:41.84ms
step:1053/2160 train_time:44076ms step_avg:41.86ms
step:1054/2160 train_time:44136ms step_avg:41.87ms
step:1055/2160 train_time:44196ms step_avg:41.89ms
step:1056/2160 train_time:44256ms step_avg:41.91ms
step:1057/2160 train_time:44317ms step_avg:41.93ms
step:1058/2160 train_time:44376ms step_avg:41.94ms
step:1059/2160 train_time:44436ms step_avg:41.96ms
step:1060/2160 train_time:44495ms step_avg:41.98ms
step:1061/2160 train_time:44556ms step_avg:41.99ms
step:1062/2160 train_time:44615ms step_avg:42.01ms
step:1063/2160 train_time:44674ms step_avg:42.03ms
step:1064/2160 train_time:44733ms step_avg:42.04ms
step:1065/2160 train_time:44794ms step_avg:42.06ms
step:1066/2160 train_time:44853ms step_avg:42.08ms
step:1067/2160 train_time:44914ms step_avg:42.09ms
step:1068/2160 train_time:44974ms step_avg:42.11ms
step:1069/2160 train_time:45036ms step_avg:42.13ms
step:1070/2160 train_time:45095ms step_avg:42.15ms
step:1071/2160 train_time:45156ms step_avg:42.16ms
step:1072/2160 train_time:45216ms step_avg:42.18ms
step:1073/2160 train_time:45277ms step_avg:42.20ms
step:1074/2160 train_time:45336ms step_avg:42.21ms
step:1075/2160 train_time:45397ms step_avg:42.23ms
step:1076/2160 train_time:45456ms step_avg:42.25ms
step:1077/2160 train_time:45516ms step_avg:42.26ms
step:1078/2160 train_time:45575ms step_avg:42.28ms
step:1079/2160 train_time:45635ms step_avg:42.29ms
step:1080/2160 train_time:45694ms step_avg:42.31ms
step:1081/2160 train_time:45754ms step_avg:42.33ms
step:1082/2160 train_time:45813ms step_avg:42.34ms
step:1083/2160 train_time:45873ms step_avg:42.36ms
step:1084/2160 train_time:45932ms step_avg:42.37ms
step:1085/2160 train_time:45994ms step_avg:42.39ms
step:1086/2160 train_time:46053ms step_avg:42.41ms
step:1087/2160 train_time:46115ms step_avg:42.42ms
step:1088/2160 train_time:46175ms step_avg:42.44ms
step:1089/2160 train_time:46236ms step_avg:42.46ms
step:1090/2160 train_time:46296ms step_avg:42.47ms
step:1091/2160 train_time:46357ms step_avg:42.49ms
step:1092/2160 train_time:46416ms step_avg:42.51ms
step:1093/2160 train_time:46476ms step_avg:42.52ms
step:1094/2160 train_time:46535ms step_avg:42.54ms
step:1095/2160 train_time:46596ms step_avg:42.55ms
step:1096/2160 train_time:46655ms step_avg:42.57ms
step:1097/2160 train_time:46715ms step_avg:42.58ms
step:1098/2160 train_time:46774ms step_avg:42.60ms
step:1099/2160 train_time:46835ms step_avg:42.62ms
step:1100/2160 train_time:46894ms step_avg:42.63ms
step:1101/2160 train_time:46955ms step_avg:42.65ms
step:1102/2160 train_time:47016ms step_avg:42.66ms
step:1103/2160 train_time:47076ms step_avg:42.68ms
step:1104/2160 train_time:47136ms step_avg:42.70ms
step:1105/2160 train_time:47196ms step_avg:42.71ms
step:1106/2160 train_time:47256ms step_avg:42.73ms
step:1107/2160 train_time:47316ms step_avg:42.74ms
step:1108/2160 train_time:47376ms step_avg:42.76ms
step:1109/2160 train_time:47436ms step_avg:42.77ms
step:1110/2160 train_time:47496ms step_avg:42.79ms
step:1111/2160 train_time:47556ms step_avg:42.80ms
step:1112/2160 train_time:47615ms step_avg:42.82ms
step:1113/2160 train_time:47674ms step_avg:42.83ms
step:1114/2160 train_time:47734ms step_avg:42.85ms
step:1115/2160 train_time:47795ms step_avg:42.87ms
step:1116/2160 train_time:47854ms step_avg:42.88ms
step:1117/2160 train_time:47915ms step_avg:42.90ms
step:1118/2160 train_time:47975ms step_avg:42.91ms
step:1119/2160 train_time:48036ms step_avg:42.93ms
step:1120/2160 train_time:48095ms step_avg:42.94ms
step:1121/2160 train_time:48156ms step_avg:42.96ms
step:1122/2160 train_time:48216ms step_avg:42.97ms
step:1123/2160 train_time:48276ms step_avg:42.99ms
step:1124/2160 train_time:48335ms step_avg:43.00ms
step:1125/2160 train_time:48396ms step_avg:43.02ms
step:1126/2160 train_time:48455ms step_avg:43.03ms
step:1127/2160 train_time:48516ms step_avg:43.05ms
step:1128/2160 train_time:48575ms step_avg:43.06ms
step:1129/2160 train_time:48635ms step_avg:43.08ms
step:1130/2160 train_time:48695ms step_avg:43.09ms
step:1131/2160 train_time:48755ms step_avg:43.11ms
step:1132/2160 train_time:48814ms step_avg:43.12ms
step:1133/2160 train_time:48874ms step_avg:43.14ms
step:1134/2160 train_time:48934ms step_avg:43.15ms
step:1135/2160 train_time:48996ms step_avg:43.17ms
step:1136/2160 train_time:49055ms step_avg:43.18ms
step:1137/2160 train_time:49116ms step_avg:43.20ms
step:1138/2160 train_time:49175ms step_avg:43.21ms
step:1139/2160 train_time:49237ms step_avg:43.23ms
step:1140/2160 train_time:49296ms step_avg:43.24ms
step:1141/2160 train_time:49357ms step_avg:43.26ms
step:1142/2160 train_time:49416ms step_avg:43.27ms
step:1143/2160 train_time:49476ms step_avg:43.29ms
step:1144/2160 train_time:49536ms step_avg:43.30ms
step:1145/2160 train_time:49596ms step_avg:43.32ms
step:1146/2160 train_time:49655ms step_avg:43.33ms
step:1147/2160 train_time:49716ms step_avg:43.34ms
step:1148/2160 train_time:49775ms step_avg:43.36ms
step:1149/2160 train_time:49835ms step_avg:43.37ms
step:1150/2160 train_time:49895ms step_avg:43.39ms
step:1151/2160 train_time:49956ms step_avg:43.40ms
step:1152/2160 train_time:50016ms step_avg:43.42ms
step:1153/2160 train_time:50076ms step_avg:43.43ms
step:1154/2160 train_time:50136ms step_avg:43.45ms
step:1155/2160 train_time:50197ms step_avg:43.46ms
step:1156/2160 train_time:50256ms step_avg:43.47ms
step:1157/2160 train_time:50316ms step_avg:43.49ms
step:1158/2160 train_time:50375ms step_avg:43.50ms
step:1159/2160 train_time:50436ms step_avg:43.52ms
step:1160/2160 train_time:50495ms step_avg:43.53ms
step:1161/2160 train_time:50556ms step_avg:43.55ms
step:1162/2160 train_time:50616ms step_avg:43.56ms
step:1163/2160 train_time:50675ms step_avg:43.57ms
step:1164/2160 train_time:50735ms step_avg:43.59ms
step:1165/2160 train_time:50795ms step_avg:43.60ms
step:1166/2160 train_time:50854ms step_avg:43.61ms
step:1167/2160 train_time:50915ms step_avg:43.63ms
step:1168/2160 train_time:50975ms step_avg:43.64ms
step:1169/2160 train_time:51036ms step_avg:43.66ms
step:1170/2160 train_time:51095ms step_avg:43.67ms
step:1171/2160 train_time:51156ms step_avg:43.69ms
step:1172/2160 train_time:51216ms step_avg:43.70ms
step:1173/2160 train_time:51277ms step_avg:43.71ms
step:1174/2160 train_time:51336ms step_avg:43.73ms
step:1175/2160 train_time:51396ms step_avg:43.74ms
step:1176/2160 train_time:51456ms step_avg:43.75ms
step:1177/2160 train_time:51516ms step_avg:43.77ms
step:1178/2160 train_time:51575ms step_avg:43.78ms
step:1179/2160 train_time:51635ms step_avg:43.80ms
step:1180/2160 train_time:51695ms step_avg:43.81ms
step:1181/2160 train_time:51756ms step_avg:43.82ms
step:1182/2160 train_time:51815ms step_avg:43.84ms
step:1183/2160 train_time:51876ms step_avg:43.85ms
step:1184/2160 train_time:51935ms step_avg:43.86ms
step:1185/2160 train_time:51996ms step_avg:43.88ms
step:1186/2160 train_time:52055ms step_avg:43.89ms
step:1187/2160 train_time:52117ms step_avg:43.91ms
step:1188/2160 train_time:52176ms step_avg:43.92ms
step:1189/2160 train_time:52236ms step_avg:43.93ms
step:1190/2160 train_time:52295ms step_avg:43.95ms
step:1191/2160 train_time:52356ms step_avg:43.96ms
step:1192/2160 train_time:52415ms step_avg:43.97ms
step:1193/2160 train_time:52476ms step_avg:43.99ms
step:1194/2160 train_time:52535ms step_avg:44.00ms
step:1195/2160 train_time:52596ms step_avg:44.01ms
step:1196/2160 train_time:52655ms step_avg:44.03ms
step:1197/2160 train_time:52715ms step_avg:44.04ms
step:1198/2160 train_time:52775ms step_avg:44.05ms
step:1199/2160 train_time:52836ms step_avg:44.07ms
step:1200/2160 train_time:52895ms step_avg:44.08ms
step:1201/2160 train_time:52956ms step_avg:44.09ms
step:1202/2160 train_time:53015ms step_avg:44.11ms
step:1203/2160 train_time:53076ms step_avg:44.12ms
step:1204/2160 train_time:53136ms step_avg:44.13ms
step:1205/2160 train_time:53196ms step_avg:44.15ms
step:1206/2160 train_time:53256ms step_avg:44.16ms
step:1207/2160 train_time:53316ms step_avg:44.17ms
step:1208/2160 train_time:53375ms step_avg:44.18ms
step:1209/2160 train_time:53436ms step_avg:44.20ms
step:1210/2160 train_time:53494ms step_avg:44.21ms
step:1211/2160 train_time:53555ms step_avg:44.22ms
step:1212/2160 train_time:53615ms step_avg:44.24ms
step:1213/2160 train_time:53675ms step_avg:44.25ms
step:1214/2160 train_time:53735ms step_avg:44.26ms
step:1215/2160 train_time:53796ms step_avg:44.28ms
step:1216/2160 train_time:53855ms step_avg:44.29ms
step:1217/2160 train_time:53916ms step_avg:44.30ms
step:1218/2160 train_time:53975ms step_avg:44.31ms
step:1219/2160 train_time:54036ms step_avg:44.33ms
step:1220/2160 train_time:54095ms step_avg:44.34ms
step:1221/2160 train_time:54155ms step_avg:44.35ms
step:1222/2160 train_time:54215ms step_avg:44.37ms
step:1223/2160 train_time:54276ms step_avg:44.38ms
step:1224/2160 train_time:54335ms step_avg:44.39ms
step:1225/2160 train_time:54396ms step_avg:44.41ms
step:1226/2160 train_time:54456ms step_avg:44.42ms
step:1227/2160 train_time:54516ms step_avg:44.43ms
step:1228/2160 train_time:54575ms step_avg:44.44ms
step:1229/2160 train_time:54636ms step_avg:44.46ms
step:1230/2160 train_time:54696ms step_avg:44.47ms
step:1231/2160 train_time:54757ms step_avg:44.48ms
step:1232/2160 train_time:54816ms step_avg:44.49ms
step:1233/2160 train_time:54876ms step_avg:44.51ms
step:1234/2160 train_time:54935ms step_avg:44.52ms
step:1235/2160 train_time:54996ms step_avg:44.53ms
step:1236/2160 train_time:55055ms step_avg:44.54ms
step:1237/2160 train_time:55116ms step_avg:44.56ms
step:1238/2160 train_time:55175ms step_avg:44.57ms
step:1239/2160 train_time:55236ms step_avg:44.58ms
step:1240/2160 train_time:55295ms step_avg:44.59ms
step:1241/2160 train_time:55356ms step_avg:44.61ms
step:1242/2160 train_time:55416ms step_avg:44.62ms
step:1243/2160 train_time:55476ms step_avg:44.63ms
step:1244/2160 train_time:55535ms step_avg:44.64ms
step:1245/2160 train_time:55596ms step_avg:44.66ms
step:1246/2160 train_time:55655ms step_avg:44.67ms
step:1247/2160 train_time:55716ms step_avg:44.68ms
step:1248/2160 train_time:55775ms step_avg:44.69ms
step:1249/2160 train_time:55836ms step_avg:44.70ms
step:1250/2160 train_time:55895ms step_avg:44.72ms
step:1250/2160 val_loss:3.5918 train_time:55958ms step_avg:44.77ms
step:1251/2160 train_time:55981ms step_avg:44.75ms
step:1252/2160 train_time:56017ms step_avg:44.74ms
step:1253/2160 train_time:56083ms step_avg:44.76ms
step:1254/2160 train_time:56146ms step_avg:44.77ms
step:1255/2160 train_time:56207ms step_avg:44.79ms
step:1256/2160 train_time:56266ms step_avg:44.80ms
step:1257/2160 train_time:56327ms step_avg:44.81ms
step:1258/2160 train_time:56385ms step_avg:44.82ms
step:1259/2160 train_time:56445ms step_avg:44.83ms
step:1260/2160 train_time:56504ms step_avg:44.84ms
step:1261/2160 train_time:56563ms step_avg:44.86ms
step:1262/2160 train_time:56621ms step_avg:44.87ms
step:1263/2160 train_time:56680ms step_avg:44.88ms
step:1264/2160 train_time:56739ms step_avg:44.89ms
step:1265/2160 train_time:56798ms step_avg:44.90ms
step:1266/2160 train_time:56856ms step_avg:44.91ms
step:1267/2160 train_time:56919ms step_avg:44.92ms
step:1268/2160 train_time:56980ms step_avg:44.94ms
step:1269/2160 train_time:57043ms step_avg:44.95ms
step:1270/2160 train_time:57103ms step_avg:44.96ms
step:1271/2160 train_time:57165ms step_avg:44.98ms
step:1272/2160 train_time:57224ms step_avg:44.99ms
step:1273/2160 train_time:57284ms step_avg:45.00ms
step:1274/2160 train_time:57342ms step_avg:45.01ms
step:1275/2160 train_time:57402ms step_avg:45.02ms
step:1276/2160 train_time:57461ms step_avg:45.03ms
step:1277/2160 train_time:57520ms step_avg:45.04ms
step:1278/2160 train_time:57578ms step_avg:45.05ms
step:1279/2160 train_time:57638ms step_avg:45.06ms
step:1280/2160 train_time:57695ms step_avg:45.07ms
step:1281/2160 train_time:57756ms step_avg:45.09ms
step:1282/2160 train_time:57815ms step_avg:45.10ms
step:1283/2160 train_time:57875ms step_avg:45.11ms
step:1284/2160 train_time:57935ms step_avg:45.12ms
step:1285/2160 train_time:57996ms step_avg:45.13ms
step:1286/2160 train_time:58056ms step_avg:45.14ms
step:1287/2160 train_time:58117ms step_avg:45.16ms
step:1288/2160 train_time:58177ms step_avg:45.17ms
step:1289/2160 train_time:58238ms step_avg:45.18ms
step:1290/2160 train_time:58298ms step_avg:45.19ms
step:1291/2160 train_time:58358ms step_avg:45.20ms
step:1292/2160 train_time:58417ms step_avg:45.21ms
step:1293/2160 train_time:58477ms step_avg:45.23ms
step:1294/2160 train_time:58535ms step_avg:45.24ms
step:1295/2160 train_time:58595ms step_avg:45.25ms
step:1296/2160 train_time:58653ms step_avg:45.26ms
step:1297/2160 train_time:58713ms step_avg:45.27ms
step:1298/2160 train_time:58772ms step_avg:45.28ms
step:1299/2160 train_time:58834ms step_avg:45.29ms
step:1300/2160 train_time:58894ms step_avg:45.30ms
step:1301/2160 train_time:58956ms step_avg:45.32ms
step:1302/2160 train_time:59016ms step_avg:45.33ms
step:1303/2160 train_time:59077ms step_avg:45.34ms
step:1304/2160 train_time:59137ms step_avg:45.35ms
step:1305/2160 train_time:59198ms step_avg:45.36ms
step:1306/2160 train_time:59257ms step_avg:45.37ms
step:1307/2160 train_time:59317ms step_avg:45.38ms
step:1308/2160 train_time:59376ms step_avg:45.39ms
step:1309/2160 train_time:59436ms step_avg:45.41ms
step:1310/2160 train_time:59495ms step_avg:45.42ms
step:1311/2160 train_time:59555ms step_avg:45.43ms
step:1312/2160 train_time:59614ms step_avg:45.44ms
step:1313/2160 train_time:59673ms step_avg:45.45ms
step:1314/2160 train_time:59732ms step_avg:45.46ms
step:1315/2160 train_time:59793ms step_avg:45.47ms
step:1316/2160 train_time:59852ms step_avg:45.48ms
step:1317/2160 train_time:59913ms step_avg:45.49ms
step:1318/2160 train_time:59974ms step_avg:45.50ms
step:1319/2160 train_time:60035ms step_avg:45.52ms
step:1320/2160 train_time:60095ms step_avg:45.53ms
step:1321/2160 train_time:60155ms step_avg:45.54ms
step:1322/2160 train_time:60216ms step_avg:45.55ms
step:1323/2160 train_time:60277ms step_avg:45.56ms
step:1324/2160 train_time:60336ms step_avg:45.57ms
step:1325/2160 train_time:60397ms step_avg:45.58ms
step:1326/2160 train_time:60455ms step_avg:45.59ms
step:1327/2160 train_time:60516ms step_avg:45.60ms
step:1328/2160 train_time:60575ms step_avg:45.61ms
step:1329/2160 train_time:60635ms step_avg:45.62ms
step:1330/2160 train_time:60694ms step_avg:45.63ms
step:1331/2160 train_time:60754ms step_avg:45.65ms
step:1332/2160 train_time:60813ms step_avg:45.66ms
step:1333/2160 train_time:60875ms step_avg:45.67ms
step:1334/2160 train_time:60934ms step_avg:45.68ms
step:1335/2160 train_time:60996ms step_avg:45.69ms
step:1336/2160 train_time:61055ms step_avg:45.70ms
step:1337/2160 train_time:61116ms step_avg:45.71ms
step:1338/2160 train_time:61176ms step_avg:45.72ms
step:1339/2160 train_time:61236ms step_avg:45.73ms
step:1340/2160 train_time:61296ms step_avg:45.74ms
step:1341/2160 train_time:61356ms step_avg:45.75ms
step:1342/2160 train_time:61415ms step_avg:45.76ms
step:1343/2160 train_time:61475ms step_avg:45.77ms
step:1344/2160 train_time:61534ms step_avg:45.78ms
step:1345/2160 train_time:61594ms step_avg:45.79ms
step:1346/2160 train_time:61653ms step_avg:45.80ms
step:1347/2160 train_time:61714ms step_avg:45.82ms
step:1348/2160 train_time:61773ms step_avg:45.83ms
step:1349/2160 train_time:61834ms step_avg:45.84ms
step:1350/2160 train_time:61893ms step_avg:45.85ms
step:1351/2160 train_time:61954ms step_avg:45.86ms
step:1352/2160 train_time:62014ms step_avg:45.87ms
step:1353/2160 train_time:62074ms step_avg:45.88ms
step:1354/2160 train_time:62134ms step_avg:45.89ms
step:1355/2160 train_time:62195ms step_avg:45.90ms
step:1356/2160 train_time:62255ms step_avg:45.91ms
step:1357/2160 train_time:62316ms step_avg:45.92ms
step:1358/2160 train_time:62375ms step_avg:45.93ms
step:1359/2160 train_time:62436ms step_avg:45.94ms
step:1360/2160 train_time:62495ms step_avg:45.95ms
step:1361/2160 train_time:62554ms step_avg:45.96ms
step:1362/2160 train_time:62613ms step_avg:45.97ms
step:1363/2160 train_time:62674ms step_avg:45.98ms
step:1364/2160 train_time:62734ms step_avg:45.99ms
step:1365/2160 train_time:62795ms step_avg:46.00ms
step:1366/2160 train_time:62855ms step_avg:46.01ms
step:1367/2160 train_time:62914ms step_avg:46.02ms
step:1368/2160 train_time:62974ms step_avg:46.03ms
step:1369/2160 train_time:63036ms step_avg:46.05ms
step:1370/2160 train_time:63095ms step_avg:46.05ms
step:1371/2160 train_time:63155ms step_avg:46.07ms
step:1372/2160 train_time:63214ms step_avg:46.07ms
step:1373/2160 train_time:63275ms step_avg:46.08ms
step:1374/2160 train_time:63334ms step_avg:46.09ms
step:1375/2160 train_time:63395ms step_avg:46.11ms
step:1376/2160 train_time:63455ms step_avg:46.12ms
step:1377/2160 train_time:63515ms step_avg:46.13ms
step:1378/2160 train_time:63574ms step_avg:46.13ms
step:1379/2160 train_time:63635ms step_avg:46.15ms
step:1380/2160 train_time:63694ms step_avg:46.16ms
step:1381/2160 train_time:63755ms step_avg:46.17ms
step:1382/2160 train_time:63814ms step_avg:46.18ms
step:1383/2160 train_time:63874ms step_avg:46.19ms
step:1384/2160 train_time:63934ms step_avg:46.19ms
step:1385/2160 train_time:63995ms step_avg:46.21ms
step:1386/2160 train_time:64055ms step_avg:46.22ms
step:1387/2160 train_time:64115ms step_avg:46.23ms
step:1388/2160 train_time:64175ms step_avg:46.24ms
step:1389/2160 train_time:64236ms step_avg:46.25ms
step:1390/2160 train_time:64295ms step_avg:46.26ms
step:1391/2160 train_time:64356ms step_avg:46.27ms
step:1392/2160 train_time:64415ms step_avg:46.27ms
step:1393/2160 train_time:64475ms step_avg:46.29ms
step:1394/2160 train_time:64535ms step_avg:46.29ms
step:1395/2160 train_time:64595ms step_avg:46.30ms
step:1396/2160 train_time:64655ms step_avg:46.31ms
step:1397/2160 train_time:64715ms step_avg:46.32ms
step:1398/2160 train_time:64774ms step_avg:46.33ms
step:1399/2160 train_time:64835ms step_avg:46.34ms
step:1400/2160 train_time:64895ms step_avg:46.35ms
step:1401/2160 train_time:64955ms step_avg:46.36ms
step:1402/2160 train_time:65015ms step_avg:46.37ms
step:1403/2160 train_time:65075ms step_avg:46.38ms
step:1404/2160 train_time:65135ms step_avg:46.39ms
step:1405/2160 train_time:65196ms step_avg:46.40ms
step:1406/2160 train_time:65255ms step_avg:46.41ms
step:1407/2160 train_time:65316ms step_avg:46.42ms
step:1408/2160 train_time:65374ms step_avg:46.43ms
step:1409/2160 train_time:65435ms step_avg:46.44ms
step:1410/2160 train_time:65494ms step_avg:46.45ms
step:1411/2160 train_time:65555ms step_avg:46.46ms
step:1412/2160 train_time:65615ms step_avg:46.47ms
step:1413/2160 train_time:65675ms step_avg:46.48ms
step:1414/2160 train_time:65734ms step_avg:46.49ms
step:1415/2160 train_time:65796ms step_avg:46.50ms
step:1416/2160 train_time:65884ms step_avg:46.53ms
step:1417/2160 train_time:65972ms step_avg:46.56ms
step:1418/2160 train_time:66059ms step_avg:46.59ms
step:1419/2160 train_time:66148ms step_avg:46.62ms
step:1420/2160 train_time:66235ms step_avg:46.64ms
step:1421/2160 train_time:66323ms step_avg:46.67ms
step:1422/2160 train_time:66409ms step_avg:46.70ms
step:1423/2160 train_time:66497ms step_avg:46.73ms
step:1424/2160 train_time:66584ms step_avg:46.76ms
step:1425/2160 train_time:66672ms step_avg:46.79ms
step:1426/2160 train_time:66759ms step_avg:46.82ms
step:1427/2160 train_time:66850ms step_avg:46.85ms
step:1428/2160 train_time:66936ms step_avg:46.87ms
step:1429/2160 train_time:67025ms step_avg:46.90ms
step:1430/2160 train_time:67111ms step_avg:46.93ms
step:1431/2160 train_time:67200ms step_avg:46.96ms
step:1432/2160 train_time:67288ms step_avg:46.99ms
step:1433/2160 train_time:67376ms step_avg:47.02ms
step:1434/2160 train_time:67463ms step_avg:47.05ms
step:1435/2160 train_time:67551ms step_avg:47.07ms
step:1436/2160 train_time:67638ms step_avg:47.10ms
step:1437/2160 train_time:67727ms step_avg:47.13ms
step:1438/2160 train_time:67813ms step_avg:47.16ms
step:1439/2160 train_time:67903ms step_avg:47.19ms
step:1440/2160 train_time:67989ms step_avg:47.21ms
step:1441/2160 train_time:68078ms step_avg:47.24ms
step:1442/2160 train_time:68165ms step_avg:47.27ms
step:1443/2160 train_time:68253ms step_avg:47.30ms
step:1444/2160 train_time:68340ms step_avg:47.33ms
step:1445/2160 train_time:68430ms step_avg:47.36ms
step:1446/2160 train_time:68516ms step_avg:47.38ms
step:1447/2160 train_time:68605ms step_avg:47.41ms
step:1448/2160 train_time:68692ms step_avg:47.44ms
step:1449/2160 train_time:68780ms step_avg:47.47ms
step:1450/2160 train_time:68867ms step_avg:47.49ms
step:1451/2160 train_time:68956ms step_avg:47.52ms
step:1452/2160 train_time:69044ms step_avg:47.55ms
step:1453/2160 train_time:69132ms step_avg:47.58ms
step:1454/2160 train_time:69218ms step_avg:47.61ms
step:1455/2160 train_time:69309ms step_avg:47.63ms
step:1456/2160 train_time:69396ms step_avg:47.66ms
step:1457/2160 train_time:69484ms step_avg:47.69ms
step:1458/2160 train_time:69571ms step_avg:47.72ms
step:1459/2160 train_time:69659ms step_avg:47.74ms
step:1460/2160 train_time:69746ms step_avg:47.77ms
step:1461/2160 train_time:69834ms step_avg:47.80ms
step:1462/2160 train_time:69921ms step_avg:47.83ms
step:1463/2160 train_time:70010ms step_avg:47.85ms
step:1464/2160 train_time:70097ms step_avg:47.88ms
step:1465/2160 train_time:70186ms step_avg:47.91ms
step:1466/2160 train_time:70273ms step_avg:47.94ms
step:1467/2160 train_time:70362ms step_avg:47.96ms
step:1468/2160 train_time:70449ms step_avg:47.99ms
step:1469/2160 train_time:70538ms step_avg:48.02ms
step:1470/2160 train_time:70626ms step_avg:48.04ms
step:1471/2160 train_time:70714ms step_avg:48.07ms
step:1472/2160 train_time:70801ms step_avg:48.10ms
step:1473/2160 train_time:70890ms step_avg:48.13ms
step:1474/2160 train_time:70977ms step_avg:48.15ms
step:1475/2160 train_time:71066ms step_avg:48.18ms
step:1476/2160 train_time:71153ms step_avg:48.21ms
step:1477/2160 train_time:71242ms step_avg:48.23ms
step:1478/2160 train_time:71329ms step_avg:48.26ms
step:1479/2160 train_time:71416ms step_avg:48.29ms
step:1480/2160 train_time:71504ms step_avg:48.31ms
step:1481/2160 train_time:71592ms step_avg:48.34ms
step:1482/2160 train_time:71679ms step_avg:48.37ms
step:1483/2160 train_time:71769ms step_avg:48.39ms
step:1484/2160 train_time:71856ms step_avg:48.42ms
step:1485/2160 train_time:71945ms step_avg:48.45ms
step:1486/2160 train_time:72031ms step_avg:48.47ms
step:1487/2160 train_time:72121ms step_avg:48.50ms
step:1488/2160 train_time:72208ms step_avg:48.53ms
step:1489/2160 train_time:72296ms step_avg:48.55ms
step:1490/2160 train_time:72383ms step_avg:48.58ms
step:1491/2160 train_time:72471ms step_avg:48.61ms
step:1492/2160 train_time:72558ms step_avg:48.63ms
step:1493/2160 train_time:72646ms step_avg:48.66ms
step:1494/2160 train_time:72733ms step_avg:48.68ms
step:1495/2160 train_time:72821ms step_avg:48.71ms
step:1496/2160 train_time:72909ms step_avg:48.74ms
step:1497/2160 train_time:72997ms step_avg:48.76ms
step:1498/2160 train_time:73084ms step_avg:48.79ms
step:1499/2160 train_time:73173ms step_avg:48.81ms
step:1500/2160 train_time:73260ms step_avg:48.84ms
step:1500/2160 val_loss:3.4891 train_time:73351ms step_avg:48.90ms
step:1501/2160 train_time:73374ms step_avg:48.88ms
step:1502/2160 train_time:73440ms step_avg:48.89ms
step:1503/2160 train_time:73531ms step_avg:48.92ms
step:1504/2160 train_time:73619ms step_avg:48.95ms
step:1505/2160 train_time:73706ms step_avg:48.97ms
step:1506/2160 train_time:73792ms step_avg:49.00ms
step:1507/2160 train_time:73879ms step_avg:49.02ms
step:1508/2160 train_time:73965ms step_avg:49.05ms
step:1509/2160 train_time:74053ms step_avg:49.07ms
step:1510/2160 train_time:74139ms step_avg:49.10ms
step:1511/2160 train_time:74227ms step_avg:49.12ms
step:1512/2160 train_time:74314ms step_avg:49.15ms
step:1513/2160 train_time:74405ms step_avg:49.18ms
step:1514/2160 train_time:74494ms step_avg:49.20ms
step:1515/2160 train_time:74584ms step_avg:49.23ms
step:1516/2160 train_time:74672ms step_avg:49.26ms
step:1517/2160 train_time:74760ms step_avg:49.28ms
step:1518/2160 train_time:74846ms step_avg:49.31ms
step:1519/2160 train_time:74934ms step_avg:49.33ms
step:1520/2160 train_time:75019ms step_avg:49.35ms
step:1521/2160 train_time:75108ms step_avg:49.38ms
step:1522/2160 train_time:75195ms step_avg:49.41ms
step:1523/2160 train_time:75283ms step_avg:49.43ms
step:1524/2160 train_time:75372ms step_avg:49.46ms
step:1525/2160 train_time:75462ms step_avg:49.48ms
step:1526/2160 train_time:75551ms step_avg:49.51ms
step:1527/2160 train_time:75640ms step_avg:49.53ms
step:1528/2160 train_time:75727ms step_avg:49.56ms
step:1529/2160 train_time:75814ms step_avg:49.58ms
step:1530/2160 train_time:75900ms step_avg:49.61ms
step:1531/2160 train_time:75988ms step_avg:49.63ms
step:1532/2160 train_time:76074ms step_avg:49.66ms
step:1533/2160 train_time:76162ms step_avg:49.68ms
step:1534/2160 train_time:76249ms step_avg:49.71ms
step:1535/2160 train_time:76339ms step_avg:49.73ms
step:1536/2160 train_time:76429ms step_avg:49.76ms
step:1537/2160 train_time:76520ms step_avg:49.79ms
step:1538/2160 train_time:76607ms step_avg:49.81ms
step:1539/2160 train_time:76696ms step_avg:49.84ms
step:1540/2160 train_time:76782ms step_avg:49.86ms
step:1541/2160 train_time:76870ms step_avg:49.88ms
step:1542/2160 train_time:76956ms step_avg:49.91ms
step:1543/2160 train_time:77043ms step_avg:49.93ms
step:1544/2160 train_time:77130ms step_avg:49.95ms
step:1545/2160 train_time:77219ms step_avg:49.98ms
step:1546/2160 train_time:77306ms step_avg:50.00ms
step:1547/2160 train_time:77397ms step_avg:50.03ms
step:1548/2160 train_time:77485ms step_avg:50.05ms
step:1549/2160 train_time:77574ms step_avg:50.08ms
step:1550/2160 train_time:77661ms step_avg:50.10ms
step:1551/2160 train_time:77748ms step_avg:50.13ms
step:1552/2160 train_time:77836ms step_avg:50.15ms
step:1553/2160 train_time:77924ms step_avg:50.18ms
step:1554/2160 train_time:78010ms step_avg:50.20ms
step:1555/2160 train_time:78099ms step_avg:50.22ms
step:1556/2160 train_time:78186ms step_avg:50.25ms
step:1557/2160 train_time:78275ms step_avg:50.27ms
step:1558/2160 train_time:78362ms step_avg:50.30ms
step:1559/2160 train_time:78451ms step_avg:50.32ms
step:1560/2160 train_time:78538ms step_avg:50.34ms
step:1561/2160 train_time:78627ms step_avg:50.37ms
step:1562/2160 train_time:78714ms step_avg:50.39ms
step:1563/2160 train_time:78802ms step_avg:50.42ms
step:1564/2160 train_time:78889ms step_avg:50.44ms
step:1565/2160 train_time:78978ms step_avg:50.47ms
step:1566/2160 train_time:79064ms step_avg:50.49ms
step:1567/2160 train_time:79152ms step_avg:50.51ms
step:1568/2160 train_time:79239ms step_avg:50.54ms
step:1569/2160 train_time:79328ms step_avg:50.56ms
step:1570/2160 train_time:79416ms step_avg:50.58ms
step:1571/2160 train_time:79507ms step_avg:50.61ms
step:1572/2160 train_time:79594ms step_avg:50.63ms
step:1573/2160 train_time:79681ms step_avg:50.66ms
step:1574/2160 train_time:79768ms step_avg:50.68ms
step:1575/2160 train_time:79856ms step_avg:50.70ms
step:1576/2160 train_time:79942ms step_avg:50.72ms
step:1577/2160 train_time:80031ms step_avg:50.75ms
step:1578/2160 train_time:80118ms step_avg:50.77ms
step:1579/2160 train_time:80206ms step_avg:50.80ms
step:1580/2160 train_time:80293ms step_avg:50.82ms
step:1581/2160 train_time:80382ms step_avg:50.84ms
step:1582/2160 train_time:80468ms step_avg:50.86ms
step:1583/2160 train_time:80558ms step_avg:50.89ms
step:1584/2160 train_time:80644ms step_avg:50.91ms
step:1585/2160 train_time:80734ms step_avg:50.94ms
step:1586/2160 train_time:80820ms step_avg:50.96ms
step:1587/2160 train_time:80909ms step_avg:50.98ms
step:1588/2160 train_time:80996ms step_avg:51.01ms
step:1589/2160 train_time:81085ms step_avg:51.03ms
step:1590/2160 train_time:81172ms step_avg:51.05ms
step:1591/2160 train_time:81260ms step_avg:51.08ms
step:1592/2160 train_time:81347ms step_avg:51.10ms
step:1593/2160 train_time:81437ms step_avg:51.12ms
step:1594/2160 train_time:81523ms step_avg:51.14ms
step:1595/2160 train_time:81611ms step_avg:51.17ms
step:1596/2160 train_time:81698ms step_avg:51.19ms
step:1597/2160 train_time:81786ms step_avg:51.21ms
step:1598/2160 train_time:81873ms step_avg:51.23ms
step:1599/2160 train_time:81962ms step_avg:51.26ms
step:1600/2160 train_time:82048ms step_avg:51.28ms
step:1601/2160 train_time:82137ms step_avg:51.30ms
step:1602/2160 train_time:82223ms step_avg:51.33ms
step:1603/2160 train_time:82312ms step_avg:51.35ms
step:1604/2160 train_time:82399ms step_avg:51.37ms
step:1605/2160 train_time:82489ms step_avg:51.40ms
step:1606/2160 train_time:82576ms step_avg:51.42ms
step:1607/2160 train_time:82665ms step_avg:51.44ms
step:1608/2160 train_time:82751ms step_avg:51.46ms
step:1609/2160 train_time:82840ms step_avg:51.49ms
step:1610/2160 train_time:82927ms step_avg:51.51ms
step:1611/2160 train_time:83016ms step_avg:51.53ms
step:1612/2160 train_time:83102ms step_avg:51.55ms
step:1613/2160 train_time:83190ms step_avg:51.57ms
step:1614/2160 train_time:83277ms step_avg:51.60ms
step:1615/2160 train_time:83366ms step_avg:51.62ms
step:1616/2160 train_time:83454ms step_avg:51.64ms
step:1617/2160 train_time:83543ms step_avg:51.67ms
step:1618/2160 train_time:83629ms step_avg:51.69ms
step:1619/2160 train_time:83718ms step_avg:51.71ms
step:1620/2160 train_time:83804ms step_avg:51.73ms
step:1621/2160 train_time:83894ms step_avg:51.75ms
step:1622/2160 train_time:83980ms step_avg:51.78ms
step:1623/2160 train_time:84068ms step_avg:51.80ms
step:1624/2160 train_time:84155ms step_avg:51.82ms
step:1625/2160 train_time:84242ms step_avg:51.84ms
step:1626/2160 train_time:84329ms step_avg:51.86ms
step:1627/2160 train_time:84419ms step_avg:51.89ms
step:1628/2160 train_time:84506ms step_avg:51.91ms
step:1629/2160 train_time:84595ms step_avg:51.93ms
step:1630/2160 train_time:84681ms step_avg:51.95ms
step:1631/2160 train_time:84770ms step_avg:51.97ms
step:1632/2160 train_time:84856ms step_avg:52.00ms
step:1633/2160 train_time:84944ms step_avg:52.02ms
step:1634/2160 train_time:85031ms step_avg:52.04ms
step:1635/2160 train_time:85120ms step_avg:52.06ms
step:1636/2160 train_time:85207ms step_avg:52.08ms
step:1637/2160 train_time:85296ms step_avg:52.11ms
step:1638/2160 train_time:85382ms step_avg:52.13ms
step:1639/2160 train_time:85473ms step_avg:52.15ms
step:1640/2160 train_time:85559ms step_avg:52.17ms
step:1641/2160 train_time:85648ms step_avg:52.19ms
step:1642/2160 train_time:85735ms step_avg:52.21ms
step:1643/2160 train_time:85823ms step_avg:52.24ms
step:1644/2160 train_time:85910ms step_avg:52.26ms
step:1645/2160 train_time:85999ms step_avg:52.28ms
step:1646/2160 train_time:86086ms step_avg:52.30ms
step:1647/2160 train_time:86174ms step_avg:52.32ms
step:1648/2160 train_time:86261ms step_avg:52.34ms
step:1649/2160 train_time:86349ms step_avg:52.36ms
step:1650/2160 train_time:86436ms step_avg:52.39ms
step:1651/2160 train_time:86526ms step_avg:52.41ms
step:1652/2160 train_time:86613ms step_avg:52.43ms
step:1653/2160 train_time:86701ms step_avg:52.45ms
step:1654/2160 train_time:86788ms step_avg:52.47ms
step:1655/2160 train_time:86877ms step_avg:52.49ms
step:1656/2160 train_time:86963ms step_avg:52.51ms
step:1657/2160 train_time:87051ms step_avg:52.54ms
step:1658/2160 train_time:87138ms step_avg:52.56ms
step:1659/2160 train_time:87227ms step_avg:52.58ms
step:1660/2160 train_time:87315ms step_avg:52.60ms
step:1661/2160 train_time:87403ms step_avg:52.62ms
step:1662/2160 train_time:87490ms step_avg:52.64ms
step:1663/2160 train_time:87579ms step_avg:52.66ms
step:1664/2160 train_time:87666ms step_avg:52.68ms
step:1665/2160 train_time:87755ms step_avg:52.71ms
step:1666/2160 train_time:87841ms step_avg:52.73ms
step:1667/2160 train_time:87929ms step_avg:52.75ms
step:1668/2160 train_time:88017ms step_avg:52.77ms
step:1669/2160 train_time:88105ms step_avg:52.79ms
step:1670/2160 train_time:88191ms step_avg:52.81ms
step:1671/2160 train_time:88281ms step_avg:52.83ms
step:1672/2160 train_time:88368ms step_avg:52.85ms
step:1673/2160 train_time:88458ms step_avg:52.87ms
step:1674/2160 train_time:88544ms step_avg:52.89ms
step:1675/2160 train_time:88633ms step_avg:52.92ms
step:1676/2160 train_time:88719ms step_avg:52.94ms
step:1677/2160 train_time:88808ms step_avg:52.96ms
step:1678/2160 train_time:88896ms step_avg:52.98ms
step:1679/2160 train_time:88985ms step_avg:53.00ms
step:1680/2160 train_time:89072ms step_avg:53.02ms
step:1681/2160 train_time:89161ms step_avg:53.04ms
step:1682/2160 train_time:89248ms step_avg:53.06ms
step:1683/2160 train_time:89338ms step_avg:53.08ms
step:1684/2160 train_time:89425ms step_avg:53.10ms
step:1685/2160 train_time:89513ms step_avg:53.12ms
step:1686/2160 train_time:89600ms step_avg:53.14ms
step:1687/2160 train_time:89688ms step_avg:53.16ms
step:1688/2160 train_time:89775ms step_avg:53.18ms
step:1689/2160 train_time:89864ms step_avg:53.21ms
step:1690/2160 train_time:89950ms step_avg:53.23ms
step:1691/2160 train_time:90040ms step_avg:53.25ms
step:1692/2160 train_time:90126ms step_avg:53.27ms
step:1693/2160 train_time:90215ms step_avg:53.29ms
step:1694/2160 train_time:90301ms step_avg:53.31ms
step:1695/2160 train_time:90390ms step_avg:53.33ms
step:1696/2160 train_time:90477ms step_avg:53.35ms
step:1697/2160 train_time:90566ms step_avg:53.37ms
step:1698/2160 train_time:90653ms step_avg:53.39ms
step:1699/2160 train_time:90742ms step_avg:53.41ms
step:1700/2160 train_time:90829ms step_avg:53.43ms
step:1701/2160 train_time:90918ms step_avg:53.45ms
step:1702/2160 train_time:91005ms step_avg:53.47ms
step:1703/2160 train_time:91094ms step_avg:53.49ms
step:1704/2160 train_time:91180ms step_avg:53.51ms
step:1705/2160 train_time:91269ms step_avg:53.53ms
step:1706/2160 train_time:91356ms step_avg:53.55ms
step:1707/2160 train_time:91444ms step_avg:53.57ms
step:1708/2160 train_time:91531ms step_avg:53.59ms
step:1709/2160 train_time:91620ms step_avg:53.61ms
step:1710/2160 train_time:91706ms step_avg:53.63ms
step:1711/2160 train_time:91796ms step_avg:53.65ms
step:1712/2160 train_time:91881ms step_avg:53.67ms
step:1713/2160 train_time:91970ms step_avg:53.69ms
step:1714/2160 train_time:92057ms step_avg:53.71ms
step:1715/2160 train_time:92144ms step_avg:53.73ms
step:1716/2160 train_time:92232ms step_avg:53.75ms
step:1717/2160 train_time:92321ms step_avg:53.77ms
step:1718/2160 train_time:92408ms step_avg:53.79ms
step:1719/2160 train_time:92497ms step_avg:53.81ms
step:1720/2160 train_time:92584ms step_avg:53.83ms
step:1721/2160 train_time:92673ms step_avg:53.85ms
step:1722/2160 train_time:92759ms step_avg:53.87ms
step:1723/2160 train_time:92847ms step_avg:53.89ms
step:1724/2160 train_time:92936ms step_avg:53.91ms
step:1725/2160 train_time:93024ms step_avg:53.93ms
step:1726/2160 train_time:93110ms step_avg:53.95ms
step:1727/2160 train_time:93199ms step_avg:53.97ms
step:1728/2160 train_time:93285ms step_avg:53.98ms
step:1729/2160 train_time:93373ms step_avg:54.00ms
step:1730/2160 train_time:93459ms step_avg:54.02ms
step:1731/2160 train_time:93547ms step_avg:54.04ms
step:1732/2160 train_time:93634ms step_avg:54.06ms
step:1733/2160 train_time:93722ms step_avg:54.08ms
step:1734/2160 train_time:93809ms step_avg:54.10ms
step:1735/2160 train_time:93899ms step_avg:54.12ms
step:1736/2160 train_time:93986ms step_avg:54.14ms
step:1737/2160 train_time:94075ms step_avg:54.16ms
step:1738/2160 train_time:94161ms step_avg:54.18ms
step:1739/2160 train_time:94249ms step_avg:54.20ms
step:1740/2160 train_time:94337ms step_avg:54.22ms
step:1741/2160 train_time:94426ms step_avg:54.24ms
step:1742/2160 train_time:94512ms step_avg:54.26ms
step:1743/2160 train_time:94601ms step_avg:54.27ms
step:1744/2160 train_time:94687ms step_avg:54.29ms
step:1745/2160 train_time:94776ms step_avg:54.31ms
step:1746/2160 train_time:94862ms step_avg:54.33ms
step:1747/2160 train_time:94950ms step_avg:54.35ms
step:1748/2160 train_time:95038ms step_avg:54.37ms
step:1749/2160 train_time:95126ms step_avg:54.39ms
step:1750/2160 train_time:95214ms step_avg:54.41ms
step:1750/2160 val_loss:3.3905 train_time:95303ms step_avg:54.46ms
step:1751/2160 train_time:95326ms step_avg:54.44ms
step:1752/2160 train_time:95395ms step_avg:54.45ms
step:1753/2160 train_time:95489ms step_avg:54.47ms
step:1754/2160 train_time:95576ms step_avg:54.49ms
step:1755/2160 train_time:95664ms step_avg:54.51ms
step:1756/2160 train_time:95750ms step_avg:54.53ms
step:1757/2160 train_time:95836ms step_avg:54.55ms
step:1758/2160 train_time:95922ms step_avg:54.56ms
step:1759/2160 train_time:96009ms step_avg:54.58ms
step:1760/2160 train_time:96095ms step_avg:54.60ms
step:1761/2160 train_time:96182ms step_avg:54.62ms
step:1762/2160 train_time:96272ms step_avg:54.64ms
step:1763/2160 train_time:96363ms step_avg:54.66ms
step:1764/2160 train_time:96453ms step_avg:54.68ms
step:1765/2160 train_time:96544ms step_avg:54.70ms
step:1766/2160 train_time:96631ms step_avg:54.72ms
step:1767/2160 train_time:96718ms step_avg:54.74ms
step:1768/2160 train_time:96805ms step_avg:54.75ms
step:1769/2160 train_time:96892ms step_avg:54.77ms
step:1770/2160 train_time:96977ms step_avg:54.79ms
step:1771/2160 train_time:97065ms step_avg:54.81ms
step:1772/2160 train_time:97151ms step_avg:54.83ms
step:1773/2160 train_time:97240ms step_avg:54.84ms
step:1774/2160 train_time:97331ms step_avg:54.87ms
step:1775/2160 train_time:97421ms step_avg:54.89ms
step:1776/2160 train_time:97511ms step_avg:54.90ms
step:1777/2160 train_time:97601ms step_avg:54.92ms
step:1778/2160 train_time:97687ms step_avg:54.94ms
step:1779/2160 train_time:97774ms step_avg:54.96ms
step:1780/2160 train_time:97860ms step_avg:54.98ms
step:1781/2160 train_time:97948ms step_avg:55.00ms
step:1782/2160 train_time:98034ms step_avg:55.01ms
step:1783/2160 train_time:98121ms step_avg:55.03ms
step:1784/2160 train_time:98207ms step_avg:55.05ms
step:1785/2160 train_time:98296ms step_avg:55.07ms
step:1786/2160 train_time:98385ms step_avg:55.09ms
step:1787/2160 train_time:98476ms step_avg:55.11ms
step:1788/2160 train_time:98564ms step_avg:55.13ms
step:1789/2160 train_time:98653ms step_avg:55.14ms
step:1790/2160 train_time:98739ms step_avg:55.16ms
step:1791/2160 train_time:98827ms step_avg:55.18ms
step:1792/2160 train_time:98912ms step_avg:55.20ms
step:1793/2160 train_time:99000ms step_avg:55.21ms
step:1794/2160 train_time:99086ms step_avg:55.23ms
step:1795/2160 train_time:99174ms step_avg:55.25ms
step:1796/2160 train_time:99261ms step_avg:55.27ms
step:1797/2160 train_time:99352ms step_avg:55.29ms
step:1798/2160 train_time:99439ms step_avg:55.31ms
step:1799/2160 train_time:99529ms step_avg:55.32ms
step:1800/2160 train_time:99615ms step_avg:55.34ms
step:1801/2160 train_time:99704ms step_avg:55.36ms
step:1802/2160 train_time:99790ms step_avg:55.38ms
step:1803/2160 train_time:99877ms step_avg:55.39ms
step:1804/2160 train_time:99964ms step_avg:55.41ms
step:1805/2160 train_time:100052ms step_avg:55.43ms
step:1806/2160 train_time:100138ms step_avg:55.45ms
step:1807/2160 train_time:100228ms step_avg:55.47ms
step:1808/2160 train_time:100314ms step_avg:55.48ms
step:1809/2160 train_time:100403ms step_avg:55.50ms
step:1810/2160 train_time:100491ms step_avg:55.52ms
step:1811/2160 train_time:100579ms step_avg:55.54ms
step:1812/2160 train_time:100667ms step_avg:55.56ms
step:1813/2160 train_time:100755ms step_avg:55.57ms
step:1814/2160 train_time:100842ms step_avg:55.59ms
step:1815/2160 train_time:100931ms step_avg:55.61ms
step:1816/2160 train_time:101016ms step_avg:55.63ms
step:1817/2160 train_time:101105ms step_avg:55.64ms
step:1818/2160 train_time:101192ms step_avg:55.66ms
step:1819/2160 train_time:101279ms step_avg:55.68ms
step:1820/2160 train_time:101366ms step_avg:55.70ms
step:1821/2160 train_time:101455ms step_avg:55.71ms
step:1822/2160 train_time:101542ms step_avg:55.73ms
step:1823/2160 train_time:101632ms step_avg:55.75ms
step:1824/2160 train_time:101719ms step_avg:55.77ms
step:1825/2160 train_time:101807ms step_avg:55.78ms
step:1826/2160 train_time:101893ms step_avg:55.80ms
step:1827/2160 train_time:101980ms step_avg:55.82ms
step:1828/2160 train_time:102067ms step_avg:55.84ms
step:1829/2160 train_time:102154ms step_avg:55.85ms
step:1830/2160 train_time:102241ms step_avg:55.87ms
step:1831/2160 train_time:102331ms step_avg:55.89ms
step:1832/2160 train_time:102418ms step_avg:55.90ms
step:1833/2160 train_time:102507ms step_avg:55.92ms
step:1834/2160 train_time:102594ms step_avg:55.94ms
step:1835/2160 train_time:102683ms step_avg:55.96ms
step:1836/2160 train_time:102770ms step_avg:55.97ms
step:1837/2160 train_time:102858ms step_avg:55.99ms
step:1838/2160 train_time:102945ms step_avg:56.01ms
step:1839/2160 train_time:103033ms step_avg:56.03ms
step:1840/2160 train_time:103119ms step_avg:56.04ms
step:1841/2160 train_time:103207ms step_avg:56.06ms
step:1842/2160 train_time:103294ms step_avg:56.08ms
step:1843/2160 train_time:103382ms step_avg:56.09ms
step:1844/2160 train_time:103469ms step_avg:56.11ms
step:1845/2160 train_time:103558ms step_avg:56.13ms
step:1846/2160 train_time:103645ms step_avg:56.15ms
step:1847/2160 train_time:103734ms step_avg:56.16ms
step:1848/2160 train_time:103821ms step_avg:56.18ms
step:1849/2160 train_time:103909ms step_avg:56.20ms
step:1850/2160 train_time:103995ms step_avg:56.21ms
step:1851/2160 train_time:104083ms step_avg:56.23ms
step:1852/2160 train_time:104171ms step_avg:56.25ms
step:1853/2160 train_time:104259ms step_avg:56.26ms
step:1854/2160 train_time:104346ms step_avg:56.28ms
step:1855/2160 train_time:104435ms step_avg:56.30ms
step:1856/2160 train_time:104522ms step_avg:56.32ms
step:1857/2160 train_time:104611ms step_avg:56.33ms
step:1858/2160 train_time:104698ms step_avg:56.35ms
step:1859/2160 train_time:104786ms step_avg:56.37ms
step:1860/2160 train_time:104873ms step_avg:56.38ms
step:1861/2160 train_time:104961ms step_avg:56.40ms
step:1862/2160 train_time:105047ms step_avg:56.42ms
step:1863/2160 train_time:105135ms step_avg:56.43ms
step:1864/2160 train_time:105222ms step_avg:56.45ms
step:1865/2160 train_time:105310ms step_avg:56.47ms
step:1866/2160 train_time:105397ms step_avg:56.48ms
step:1867/2160 train_time:105486ms step_avg:56.50ms
step:1868/2160 train_time:105573ms step_avg:56.52ms
step:1869/2160 train_time:105662ms step_avg:56.53ms
step:1870/2160 train_time:105749ms step_avg:56.55ms
step:1871/2160 train_time:105837ms step_avg:56.57ms
step:1872/2160 train_time:105923ms step_avg:56.58ms
step:1873/2160 train_time:106011ms step_avg:56.60ms
step:1874/2160 train_time:106098ms step_avg:56.62ms
step:1875/2160 train_time:106185ms step_avg:56.63ms
step:1876/2160 train_time:106272ms step_avg:56.65ms
step:1877/2160 train_time:106361ms step_avg:56.67ms
step:1878/2160 train_time:106448ms step_avg:56.68ms
step:1879/2160 train_time:106536ms step_avg:56.70ms
step:1880/2160 train_time:106623ms step_avg:56.71ms
step:1881/2160 train_time:106712ms step_avg:56.73ms
step:1882/2160 train_time:106799ms step_avg:56.75ms
step:1883/2160 train_time:106887ms step_avg:56.76ms
step:1884/2160 train_time:106973ms step_avg:56.78ms
step:1885/2160 train_time:107061ms step_avg:56.80ms
step:1886/2160 train_time:107149ms step_avg:56.81ms
step:1887/2160 train_time:107236ms step_avg:56.83ms
step:1888/2160 train_time:107323ms step_avg:56.85ms
step:1889/2160 train_time:107413ms step_avg:56.86ms
step:1890/2160 train_time:107500ms step_avg:56.88ms
step:1891/2160 train_time:107589ms step_avg:56.90ms
step:1892/2160 train_time:107675ms step_avg:56.91ms
step:1893/2160 train_time:107763ms step_avg:56.93ms
step:1894/2160 train_time:107850ms step_avg:56.94ms
step:1895/2160 train_time:107939ms step_avg:56.96ms
step:1896/2160 train_time:108026ms step_avg:56.98ms
step:1897/2160 train_time:108114ms step_avg:56.99ms
step:1898/2160 train_time:108200ms step_avg:57.01ms
step:1899/2160 train_time:108289ms step_avg:57.02ms
step:1900/2160 train_time:108375ms step_avg:57.04ms
step:1901/2160 train_time:108463ms step_avg:57.06ms
step:1902/2160 train_time:108550ms step_avg:57.07ms
step:1903/2160 train_time:108637ms step_avg:57.09ms
step:1904/2160 train_time:108724ms step_avg:57.10ms
step:1905/2160 train_time:108813ms step_avg:57.12ms
step:1906/2160 train_time:108900ms step_avg:57.14ms
step:1907/2160 train_time:108989ms step_avg:57.15ms
step:1908/2160 train_time:109075ms step_avg:57.17ms
step:1909/2160 train_time:109162ms step_avg:57.18ms
step:1910/2160 train_time:109249ms step_avg:57.20ms
step:1911/2160 train_time:109338ms step_avg:57.21ms
step:1912/2160 train_time:109424ms step_avg:57.23ms
step:1913/2160 train_time:109513ms step_avg:57.25ms
step:1914/2160 train_time:109600ms step_avg:57.26ms
step:1915/2160 train_time:109688ms step_avg:57.28ms
step:1916/2160 train_time:109774ms step_avg:57.29ms
step:1917/2160 train_time:109863ms step_avg:57.31ms
step:1918/2160 train_time:109951ms step_avg:57.33ms
step:1919/2160 train_time:110039ms step_avg:57.34ms
step:1920/2160 train_time:110126ms step_avg:57.36ms
step:1921/2160 train_time:110214ms step_avg:57.37ms
step:1922/2160 train_time:110300ms step_avg:57.39ms
step:1923/2160 train_time:110389ms step_avg:57.40ms
step:1924/2160 train_time:110475ms step_avg:57.42ms
step:1925/2160 train_time:110562ms step_avg:57.44ms
step:1926/2160 train_time:110650ms step_avg:57.45ms
step:1927/2160 train_time:110738ms step_avg:57.47ms
step:1928/2160 train_time:110825ms step_avg:57.48ms
step:1929/2160 train_time:110912ms step_avg:57.50ms
step:1930/2160 train_time:111000ms step_avg:57.51ms
step:1931/2160 train_time:111088ms step_avg:57.53ms
step:1932/2160 train_time:111174ms step_avg:57.54ms
step:1933/2160 train_time:111261ms step_avg:57.56ms
step:1934/2160 train_time:111349ms step_avg:57.57ms
step:1935/2160 train_time:111438ms step_avg:57.59ms
step:1936/2160 train_time:111524ms step_avg:57.61ms
step:1937/2160 train_time:111613ms step_avg:57.62ms
step:1938/2160 train_time:111700ms step_avg:57.64ms
step:1939/2160 train_time:111789ms step_avg:57.65ms
step:1940/2160 train_time:111874ms step_avg:57.67ms
step:1941/2160 train_time:111963ms step_avg:57.68ms
step:1942/2160 train_time:112050ms step_avg:57.70ms
step:1943/2160 train_time:112138ms step_avg:57.71ms
step:1944/2160 train_time:112225ms step_avg:57.73ms
step:1945/2160 train_time:112314ms step_avg:57.74ms
step:1946/2160 train_time:112400ms step_avg:57.76ms
step:1947/2160 train_time:112489ms step_avg:57.78ms
step:1948/2160 train_time:112575ms step_avg:57.79ms
step:1949/2160 train_time:112663ms step_avg:57.81ms
step:1950/2160 train_time:112751ms step_avg:57.82ms
step:1951/2160 train_time:112839ms step_avg:57.84ms
step:1952/2160 train_time:112925ms step_avg:57.85ms
step:1953/2160 train_time:113013ms step_avg:57.87ms
step:1954/2160 train_time:113100ms step_avg:57.88ms
step:1955/2160 train_time:113188ms step_avg:57.90ms
step:1956/2160 train_time:113274ms step_avg:57.91ms
step:1957/2160 train_time:113363ms step_avg:57.93ms
step:1958/2160 train_time:113450ms step_avg:57.94ms
step:1959/2160 train_time:113537ms step_avg:57.96ms
step:1960/2160 train_time:113625ms step_avg:57.97ms
step:1961/2160 train_time:113714ms step_avg:57.99ms
step:1962/2160 train_time:113800ms step_avg:58.00ms
step:1963/2160 train_time:113888ms step_avg:58.02ms
step:1964/2160 train_time:113974ms step_avg:58.03ms
step:1965/2160 train_time:114062ms step_avg:58.05ms
step:1966/2160 train_time:114149ms step_avg:58.06ms
step:1967/2160 train_time:114237ms step_avg:58.08ms
step:1968/2160 train_time:114324ms step_avg:58.09ms
step:1969/2160 train_time:114413ms step_avg:58.11ms
step:1970/2160 train_time:114500ms step_avg:58.12ms
step:1971/2160 train_time:114589ms step_avg:58.14ms
step:1972/2160 train_time:114675ms step_avg:58.15ms
step:1973/2160 train_time:114764ms step_avg:58.17ms
step:1974/2160 train_time:114852ms step_avg:58.18ms
step:1975/2160 train_time:114939ms step_avg:58.20ms
step:1976/2160 train_time:115025ms step_avg:58.21ms
step:1977/2160 train_time:115113ms step_avg:58.23ms
step:1978/2160 train_time:115199ms step_avg:58.24ms
step:1979/2160 train_time:115288ms step_avg:58.26ms
step:1980/2160 train_time:115374ms step_avg:58.27ms
step:1981/2160 train_time:115462ms step_avg:58.28ms
step:1982/2160 train_time:115549ms step_avg:58.30ms
step:1983/2160 train_time:115638ms step_avg:58.31ms
step:1984/2160 train_time:115724ms step_avg:58.33ms
step:1985/2160 train_time:115813ms step_avg:58.34ms
step:1986/2160 train_time:115900ms step_avg:58.36ms
step:1987/2160 train_time:115988ms step_avg:58.37ms
step:1988/2160 train_time:116074ms step_avg:58.39ms
step:1989/2160 train_time:116162ms step_avg:58.40ms
step:1990/2160 train_time:116249ms step_avg:58.42ms
step:1991/2160 train_time:116337ms step_avg:58.43ms
step:1992/2160 train_time:116424ms step_avg:58.45ms
step:1993/2160 train_time:116513ms step_avg:58.46ms
step:1994/2160 train_time:116600ms step_avg:58.48ms
step:1995/2160 train_time:116689ms step_avg:58.49ms
step:1996/2160 train_time:116775ms step_avg:58.50ms
step:1997/2160 train_time:116863ms step_avg:58.52ms
step:1998/2160 train_time:116950ms step_avg:58.53ms
step:1999/2160 train_time:117038ms step_avg:58.55ms
step:2000/2160 train_time:117125ms step_avg:58.56ms
step:2000/2160 val_loss:3.3126 train_time:117215ms step_avg:58.61ms
step:2001/2160 train_time:117237ms step_avg:58.59ms
step:2002/2160 train_time:117303ms step_avg:58.59ms
step:2003/2160 train_time:117394ms step_avg:58.61ms
step:2004/2160 train_time:117482ms step_avg:58.62ms
step:2005/2160 train_time:117570ms step_avg:58.64ms
step:2006/2160 train_time:117656ms step_avg:58.65ms
step:2007/2160 train_time:117744ms step_avg:58.67ms
step:2008/2160 train_time:117829ms step_avg:58.68ms
step:2009/2160 train_time:117916ms step_avg:58.69ms
step:2010/2160 train_time:118002ms step_avg:58.71ms
step:2011/2160 train_time:118089ms step_avg:58.72ms
step:2012/2160 train_time:118176ms step_avg:58.74ms
step:2013/2160 train_time:118268ms step_avg:58.75ms
step:2014/2160 train_time:118356ms step_avg:58.77ms
step:2015/2160 train_time:118447ms step_avg:58.78ms
step:2016/2160 train_time:118534ms step_avg:58.80ms
step:2017/2160 train_time:118623ms step_avg:58.81ms
step:2018/2160 train_time:118708ms step_avg:58.82ms
step:2019/2160 train_time:118796ms step_avg:58.84ms
step:2020/2160 train_time:118882ms step_avg:58.85ms
step:2021/2160 train_time:118970ms step_avg:58.87ms
step:2022/2160 train_time:119057ms step_avg:58.88ms
step:2023/2160 train_time:119145ms step_avg:58.90ms
step:2024/2160 train_time:119232ms step_avg:58.91ms
step:2025/2160 train_time:119322ms step_avg:58.92ms
step:2026/2160 train_time:119409ms step_avg:58.94ms
step:2027/2160 train_time:119498ms step_avg:58.95ms
step:2028/2160 train_time:119585ms step_avg:58.97ms
step:2029/2160 train_time:119674ms step_avg:58.98ms
step:2030/2160 train_time:119760ms step_avg:59.00ms
step:2031/2160 train_time:119848ms step_avg:59.01ms
step:2032/2160 train_time:119935ms step_avg:59.02ms
step:2033/2160 train_time:120022ms step_avg:59.04ms
step:2034/2160 train_time:120109ms step_avg:59.05ms
step:2035/2160 train_time:120197ms step_avg:59.06ms
step:2036/2160 train_time:120285ms step_avg:59.08ms
step:2037/2160 train_time:120374ms step_avg:59.09ms
step:2038/2160 train_time:120462ms step_avg:59.11ms
step:2039/2160 train_time:120550ms step_avg:59.12ms
step:2040/2160 train_time:120636ms step_avg:59.14ms
step:2041/2160 train_time:120724ms step_avg:59.15ms
step:2042/2160 train_time:120809ms step_avg:59.16ms
step:2043/2160 train_time:120898ms step_avg:59.18ms
step:2044/2160 train_time:120984ms step_avg:59.19ms
step:2045/2160 train_time:121073ms step_avg:59.20ms
step:2046/2160 train_time:121159ms step_avg:59.22ms
step:2047/2160 train_time:121248ms step_avg:59.23ms
step:2048/2160 train_time:121336ms step_avg:59.25ms
step:2049/2160 train_time:121426ms step_avg:59.26ms
step:2050/2160 train_time:121513ms step_avg:59.27ms
step:2051/2160 train_time:121602ms step_avg:59.29ms
step:2052/2160 train_time:121689ms step_avg:59.30ms
step:2053/2160 train_time:121777ms step_avg:59.32ms
step:2054/2160 train_time:121863ms step_avg:59.33ms
step:2055/2160 train_time:121950ms step_avg:59.34ms
step:2056/2160 train_time:122036ms step_avg:59.36ms
step:2057/2160 train_time:122125ms step_avg:59.37ms
step:2058/2160 train_time:122210ms step_avg:59.38ms
step:2059/2160 train_time:122298ms step_avg:59.40ms
step:2060/2160 train_time:122386ms step_avg:59.41ms
step:2061/2160 train_time:122474ms step_avg:59.42ms
step:2062/2160 train_time:122561ms step_avg:59.44ms
step:2063/2160 train_time:122650ms step_avg:59.45ms
step:2064/2160 train_time:122736ms step_avg:59.47ms
step:2065/2160 train_time:122826ms step_avg:59.48ms
step:2066/2160 train_time:122911ms step_avg:59.49ms
step:2067/2160 train_time:123000ms step_avg:59.51ms
step:2068/2160 train_time:123087ms step_avg:59.52ms
step:2069/2160 train_time:123174ms step_avg:59.53ms
step:2070/2160 train_time:123261ms step_avg:59.55ms
step:2071/2160 train_time:123350ms step_avg:59.56ms
step:2072/2160 train_time:123437ms step_avg:59.57ms
step:2073/2160 train_time:123527ms step_avg:59.59ms
step:2074/2160 train_time:123613ms step_avg:59.60ms
step:2075/2160 train_time:123702ms step_avg:59.62ms
step:2076/2160 train_time:123788ms step_avg:59.63ms
step:2077/2160 train_time:123875ms step_avg:59.64ms
step:2078/2160 train_time:123962ms step_avg:59.65ms
step:2079/2160 train_time:124049ms step_avg:59.67ms
step:2080/2160 train_time:124135ms step_avg:59.68ms
step:2081/2160 train_time:124224ms step_avg:59.69ms
step:2082/2160 train_time:124310ms step_avg:59.71ms
step:2083/2160 train_time:124398ms step_avg:59.72ms
step:2084/2160 train_time:124486ms step_avg:59.73ms
step:2085/2160 train_time:124574ms step_avg:59.75ms
step:2086/2160 train_time:124661ms step_avg:59.76ms
step:2087/2160 train_time:124749ms step_avg:59.77ms
step:2088/2160 train_time:124835ms step_avg:59.79ms
step:2089/2160 train_time:124924ms step_avg:59.80ms
step:2090/2160 train_time:125009ms step_avg:59.81ms
step:2091/2160 train_time:125097ms step_avg:59.83ms
step:2092/2160 train_time:125185ms step_avg:59.84ms
step:2093/2160 train_time:125274ms step_avg:59.85ms
step:2094/2160 train_time:125361ms step_avg:59.87ms
step:2095/2160 train_time:125450ms step_avg:59.88ms
step:2096/2160 train_time:125537ms step_avg:59.89ms
step:2097/2160 train_time:125627ms step_avg:59.91ms
step:2098/2160 train_time:125713ms step_avg:59.92ms
step:2099/2160 train_time:125802ms step_avg:59.93ms
step:2100/2160 train_time:125888ms step_avg:59.95ms
step:2101/2160 train_time:125975ms step_avg:59.96ms
step:2102/2160 train_time:126062ms step_avg:59.97ms
step:2103/2160 train_time:126150ms step_avg:59.99ms
step:2104/2160 train_time:126237ms step_avg:60.00ms
step:2105/2160 train_time:126326ms step_avg:60.01ms
step:2106/2160 train_time:126413ms step_avg:60.02ms
step:2107/2160 train_time:126501ms step_avg:60.04ms
step:2108/2160 train_time:126588ms step_avg:60.05ms
step:2109/2160 train_time:126677ms step_avg:60.06ms
step:2110/2160 train_time:126763ms step_avg:60.08ms
step:2111/2160 train_time:126851ms step_avg:60.09ms
step:2112/2160 train_time:126937ms step_avg:60.10ms
step:2113/2160 train_time:127025ms step_avg:60.12ms
step:2114/2160 train_time:127111ms step_avg:60.13ms
step:2115/2160 train_time:127200ms step_avg:60.14ms
step:2116/2160 train_time:127288ms step_avg:60.15ms
step:2117/2160 train_time:127375ms step_avg:60.17ms
step:2118/2160 train_time:127462ms step_avg:60.18ms
step:2119/2160 train_time:127550ms step_avg:60.19ms
step:2120/2160 train_time:127637ms step_avg:60.21ms
step:2121/2160 train_time:127727ms step_avg:60.22ms
step:2122/2160 train_time:127813ms step_avg:60.23ms
step:2123/2160 train_time:127902ms step_avg:60.25ms
step:2124/2160 train_time:127988ms step_avg:60.26ms
step:2125/2160 train_time:128077ms step_avg:60.27ms
step:2126/2160 train_time:128165ms step_avg:60.28ms
step:2127/2160 train_time:128254ms step_avg:60.30ms
step:2128/2160 train_time:128340ms step_avg:60.31ms
step:2129/2160 train_time:128429ms step_avg:60.32ms
step:2130/2160 train_time:128516ms step_avg:60.34ms
step:2131/2160 train_time:128605ms step_avg:60.35ms
step:2132/2160 train_time:128692ms step_avg:60.36ms
step:2133/2160 train_time:128781ms step_avg:60.38ms
step:2134/2160 train_time:128869ms step_avg:60.39ms
step:2135/2160 train_time:128956ms step_avg:60.40ms
step:2136/2160 train_time:129042ms step_avg:60.41ms
step:2137/2160 train_time:129130ms step_avg:60.43ms
step:2138/2160 train_time:129217ms step_avg:60.44ms
step:2139/2160 train_time:129306ms step_avg:60.45ms
step:2140/2160 train_time:129393ms step_avg:60.46ms
step:2141/2160 train_time:129482ms step_avg:60.48ms
step:2142/2160 train_time:129568ms step_avg:60.49ms
step:2143/2160 train_time:129657ms step_avg:60.50ms
step:2144/2160 train_time:129745ms step_avg:60.52ms
step:2145/2160 train_time:129834ms step_avg:60.53ms
step:2146/2160 train_time:129921ms step_avg:60.54ms
step:2147/2160 train_time:130009ms step_avg:60.55ms
step:2148/2160 train_time:130096ms step_avg:60.57ms
step:2149/2160 train_time:130185ms step_avg:60.58ms
step:2150/2160 train_time:130271ms step_avg:60.59ms
step:2151/2160 train_time:130359ms step_avg:60.60ms
step:2152/2160 train_time:130445ms step_avg:60.62ms
step:2153/2160 train_time:130534ms step_avg:60.63ms
step:2154/2160 train_time:130621ms step_avg:60.64ms
step:2155/2160 train_time:130710ms step_avg:60.65ms
step:2156/2160 train_time:130797ms step_avg:60.67ms
step:2157/2160 train_time:130886ms step_avg:60.68ms
step:2158/2160 train_time:130972ms step_avg:60.69ms
step:2159/2160 train_time:131061ms step_avg:60.70ms
step:2160/2160 train_time:131148ms step_avg:60.72ms
step:2160/2160 val_loss:3.2758 train_time:131238ms step_avg:60.76ms
peak memory allocated: 29862 MiB reserved: 44396 MiB

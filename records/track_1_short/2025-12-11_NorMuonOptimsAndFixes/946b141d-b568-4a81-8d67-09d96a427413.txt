import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 08:36:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   41C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           17187      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           17188      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17189      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17190      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17191      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17192      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17193      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           17194      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           17188      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           17189      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           17190      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           17191      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           17192      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           17193      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           17194      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:102ms step_avg:102.42ms
step:2/2160 train_time:130ms step_avg:65.09ms
step:3/2160 train_time:154ms step_avg:51.37ms
step:4/2160 train_time:180ms step_avg:44.98ms
step:5/2160 train_time:203ms step_avg:40.61ms
step:6/2160 train_time:354ms step_avg:59.07ms
step:7/2160 train_time:389ms step_avg:55.61ms
step:8/2160 train_time:423ms step_avg:52.83ms
step:9/2160 train_time:456ms step_avg:50.66ms
step:10/2160 train_time:489ms step_avg:48.91ms
step:11/2160 train_time:522ms step_avg:47.50ms
step:12/2160 train_time:556ms step_avg:46.32ms
step:13/2160 train_time:589ms step_avg:45.33ms
step:14/2160 train_time:623ms step_avg:44.47ms
step:15/2160 train_time:656ms step_avg:43.76ms
step:16/2160 train_time:690ms step_avg:43.11ms
step:17/2160 train_time:723ms step_avg:42.55ms
step:18/2160 train_time:757ms step_avg:42.04ms
step:19/2160 train_time:790ms step_avg:41.59ms
step:20/2160 train_time:824ms step_avg:41.18ms
step:21/2160 train_time:857ms step_avg:40.82ms
step:22/2160 train_time:891ms step_avg:40.49ms
step:23/2160 train_time:924ms step_avg:40.18ms
step:24/2160 train_time:958ms step_avg:39.90ms
step:25/2160 train_time:991ms step_avg:39.65ms
step:26/2160 train_time:1025ms step_avg:39.41ms
step:27/2160 train_time:1058ms step_avg:39.19ms
step:28/2160 train_time:1092ms step_avg:38.99ms
step:29/2160 train_time:1125ms step_avg:38.80ms
step:30/2160 train_time:1159ms step_avg:38.63ms
step:31/2160 train_time:1192ms step_avg:38.46ms
step:32/2160 train_time:1226ms step_avg:38.30ms
step:33/2160 train_time:1260ms step_avg:38.17ms
step:34/2160 train_time:1293ms step_avg:38.04ms
step:35/2160 train_time:1329ms step_avg:37.96ms
step:36/2160 train_time:1362ms step_avg:37.84ms
step:37/2160 train_time:1396ms step_avg:37.74ms
step:38/2160 train_time:1430ms step_avg:37.64ms
step:39/2160 train_time:1464ms step_avg:37.54ms
step:40/2160 train_time:1498ms step_avg:37.44ms
step:41/2160 train_time:1531ms step_avg:37.34ms
step:42/2160 train_time:1565ms step_avg:37.25ms
step:43/2160 train_time:1598ms step_avg:37.17ms
step:44/2160 train_time:1632ms step_avg:37.08ms
step:45/2160 train_time:1665ms step_avg:37.01ms
step:46/2160 train_time:1699ms step_avg:36.94ms
step:47/2160 train_time:1733ms step_avg:36.87ms
step:48/2160 train_time:1766ms step_avg:36.80ms
step:49/2160 train_time:1800ms step_avg:36.74ms
step:50/2160 train_time:1834ms step_avg:36.67ms
step:51/2160 train_time:1867ms step_avg:36.61ms
step:52/2160 train_time:1901ms step_avg:36.55ms
step:53/2160 train_time:1934ms step_avg:36.49ms
step:54/2160 train_time:1967ms step_avg:36.43ms
step:55/2160 train_time:2001ms step_avg:36.39ms
step:56/2160 train_time:2035ms step_avg:36.33ms
step:57/2160 train_time:2068ms step_avg:36.29ms
step:58/2160 train_time:2102ms step_avg:36.24ms
step:59/2160 train_time:2135ms step_avg:36.19ms
step:60/2160 train_time:2169ms step_avg:36.14ms
step:61/2160 train_time:2202ms step_avg:36.11ms
step:62/2160 train_time:2236ms step_avg:36.06ms
step:63/2160 train_time:2270ms step_avg:36.02ms
step:64/2160 train_time:2303ms step_avg:35.99ms
step:65/2160 train_time:2337ms step_avg:35.95ms
step:66/2160 train_time:2371ms step_avg:35.92ms
step:67/2160 train_time:2404ms step_avg:35.89ms
step:68/2160 train_time:2438ms step_avg:35.85ms
step:69/2160 train_time:2472ms step_avg:35.82ms
step:70/2160 train_time:2505ms step_avg:35.79ms
step:71/2160 train_time:2539ms step_avg:35.76ms
step:72/2160 train_time:2573ms step_avg:35.74ms
step:73/2160 train_time:2606ms step_avg:35.70ms
step:74/2160 train_time:2640ms step_avg:35.67ms
step:75/2160 train_time:2674ms step_avg:35.65ms
step:76/2160 train_time:2707ms step_avg:35.62ms
step:77/2160 train_time:2741ms step_avg:35.60ms
step:78/2160 train_time:2775ms step_avg:35.58ms
step:79/2160 train_time:2808ms step_avg:35.55ms
step:80/2160 train_time:2842ms step_avg:35.52ms
step:81/2160 train_time:2876ms step_avg:35.50ms
step:82/2160 train_time:2909ms step_avg:35.48ms
step:83/2160 train_time:2943ms step_avg:35.46ms
step:84/2160 train_time:2977ms step_avg:35.44ms
step:85/2160 train_time:3010ms step_avg:35.41ms
step:86/2160 train_time:3044ms step_avg:35.39ms
step:87/2160 train_time:3078ms step_avg:35.38ms
step:88/2160 train_time:3111ms step_avg:35.35ms
step:89/2160 train_time:3144ms step_avg:35.33ms
step:90/2160 train_time:3178ms step_avg:35.31ms
step:91/2160 train_time:3211ms step_avg:35.29ms
step:92/2160 train_time:3245ms step_avg:35.27ms
step:93/2160 train_time:3279ms step_avg:35.26ms
step:94/2160 train_time:3312ms step_avg:35.24ms
step:95/2160 train_time:3346ms step_avg:35.22ms
step:96/2160 train_time:3379ms step_avg:35.20ms
step:97/2160 train_time:3413ms step_avg:35.19ms
step:98/2160 train_time:3446ms step_avg:35.17ms
step:99/2160 train_time:3480ms step_avg:35.16ms
step:100/2160 train_time:3514ms step_avg:35.14ms
step:101/2160 train_time:3548ms step_avg:35.13ms
step:102/2160 train_time:3581ms step_avg:35.11ms
step:103/2160 train_time:3615ms step_avg:35.09ms
step:104/2160 train_time:3648ms step_avg:35.07ms
step:105/2160 train_time:3682ms step_avg:35.07ms
step:106/2160 train_time:3715ms step_avg:35.05ms
step:107/2160 train_time:3749ms step_avg:35.04ms
step:108/2160 train_time:3783ms step_avg:35.02ms
step:109/2160 train_time:3816ms step_avg:35.01ms
step:110/2160 train_time:3850ms step_avg:35.00ms
step:111/2160 train_time:3883ms step_avg:34.98ms
step:112/2160 train_time:3917ms step_avg:34.98ms
step:113/2160 train_time:3951ms step_avg:34.96ms
step:114/2160 train_time:3984ms step_avg:34.95ms
step:115/2160 train_time:4018ms step_avg:34.94ms
step:116/2160 train_time:4051ms step_avg:34.93ms
step:117/2160 train_time:4085ms step_avg:34.92ms
step:118/2160 train_time:4119ms step_avg:34.90ms
step:119/2160 train_time:4152ms step_avg:34.89ms
step:120/2160 train_time:4185ms step_avg:34.88ms
step:121/2160 train_time:4219ms step_avg:34.87ms
step:122/2160 train_time:4252ms step_avg:34.86ms
step:123/2160 train_time:4286ms step_avg:34.85ms
step:124/2160 train_time:4319ms step_avg:34.83ms
step:125/2160 train_time:4353ms step_avg:34.82ms
step:126/2160 train_time:4386ms step_avg:34.81ms
step:127/2160 train_time:4419ms step_avg:34.80ms
step:128/2160 train_time:4453ms step_avg:34.79ms
step:129/2160 train_time:4486ms step_avg:34.78ms
step:130/2160 train_time:4520ms step_avg:34.77ms
step:131/2160 train_time:4553ms step_avg:34.76ms
step:132/2160 train_time:4586ms step_avg:34.75ms
step:133/2160 train_time:4621ms step_avg:34.74ms
step:134/2160 train_time:4654ms step_avg:34.73ms
step:135/2160 train_time:4688ms step_avg:34.72ms
step:136/2160 train_time:4721ms step_avg:34.71ms
step:137/2160 train_time:4754ms step_avg:34.70ms
step:138/2160 train_time:4787ms step_avg:34.69ms
step:139/2160 train_time:4821ms step_avg:34.68ms
step:140/2160 train_time:4854ms step_avg:34.67ms
step:141/2160 train_time:4888ms step_avg:34.67ms
step:142/2160 train_time:4922ms step_avg:34.66ms
step:143/2160 train_time:4955ms step_avg:34.65ms
step:144/2160 train_time:4988ms step_avg:34.64ms
step:145/2160 train_time:5023ms step_avg:34.64ms
step:146/2160 train_time:5056ms step_avg:34.63ms
step:147/2160 train_time:5090ms step_avg:34.62ms
step:148/2160 train_time:5123ms step_avg:34.62ms
step:149/2160 train_time:5157ms step_avg:34.61ms
step:150/2160 train_time:5190ms step_avg:34.60ms
step:151/2160 train_time:5224ms step_avg:34.60ms
step:152/2160 train_time:5258ms step_avg:34.59ms
step:153/2160 train_time:5291ms step_avg:34.58ms
step:154/2160 train_time:5325ms step_avg:34.58ms
step:155/2160 train_time:5358ms step_avg:34.57ms
step:156/2160 train_time:5392ms step_avg:34.56ms
step:157/2160 train_time:5426ms step_avg:34.56ms
step:158/2160 train_time:5459ms step_avg:34.55ms
step:159/2160 train_time:5493ms step_avg:34.54ms
step:160/2160 train_time:5526ms step_avg:34.54ms
step:161/2160 train_time:5560ms step_avg:34.53ms
step:162/2160 train_time:5593ms step_avg:34.53ms
step:163/2160 train_time:5627ms step_avg:34.52ms
step:164/2160 train_time:5660ms step_avg:34.51ms
step:165/2160 train_time:5694ms step_avg:34.51ms
step:166/2160 train_time:5727ms step_avg:34.50ms
step:167/2160 train_time:5761ms step_avg:34.50ms
step:168/2160 train_time:5795ms step_avg:34.49ms
step:169/2160 train_time:5828ms step_avg:34.49ms
step:170/2160 train_time:5862ms step_avg:34.48ms
step:171/2160 train_time:5895ms step_avg:34.48ms
step:172/2160 train_time:5929ms step_avg:34.47ms
step:173/2160 train_time:5963ms step_avg:34.47ms
step:174/2160 train_time:5996ms step_avg:34.46ms
step:175/2160 train_time:6030ms step_avg:34.46ms
step:176/2160 train_time:6063ms step_avg:34.45ms
step:177/2160 train_time:6097ms step_avg:34.45ms
step:178/2160 train_time:6130ms step_avg:34.44ms
step:179/2160 train_time:6164ms step_avg:34.43ms
step:180/2160 train_time:6198ms step_avg:34.43ms
step:181/2160 train_time:6231ms step_avg:34.43ms
step:182/2160 train_time:6264ms step_avg:34.42ms
step:183/2160 train_time:6298ms step_avg:34.42ms
step:184/2160 train_time:6331ms step_avg:34.41ms
step:185/2160 train_time:6365ms step_avg:34.40ms
step:186/2160 train_time:6398ms step_avg:34.40ms
step:187/2160 train_time:6432ms step_avg:34.40ms
step:188/2160 train_time:6465ms step_avg:34.39ms
step:189/2160 train_time:6499ms step_avg:34.39ms
step:190/2160 train_time:6532ms step_avg:34.38ms
step:191/2160 train_time:6566ms step_avg:34.38ms
step:192/2160 train_time:6599ms step_avg:34.37ms
step:193/2160 train_time:6633ms step_avg:34.37ms
step:194/2160 train_time:6666ms step_avg:34.36ms
step:195/2160 train_time:6701ms step_avg:34.36ms
step:196/2160 train_time:6734ms step_avg:34.36ms
step:197/2160 train_time:6768ms step_avg:34.35ms
step:198/2160 train_time:6801ms step_avg:34.35ms
step:199/2160 train_time:6834ms step_avg:34.34ms
step:200/2160 train_time:6867ms step_avg:34.34ms
step:201/2160 train_time:6901ms step_avg:34.33ms
step:202/2160 train_time:6935ms step_avg:34.33ms
step:203/2160 train_time:6968ms step_avg:34.33ms
step:204/2160 train_time:7002ms step_avg:34.32ms
step:205/2160 train_time:7035ms step_avg:34.32ms
step:206/2160 train_time:7069ms step_avg:34.31ms
step:207/2160 train_time:7102ms step_avg:34.31ms
step:208/2160 train_time:7136ms step_avg:34.31ms
step:209/2160 train_time:7170ms step_avg:34.30ms
step:210/2160 train_time:7205ms step_avg:34.31ms
step:211/2160 train_time:7237ms step_avg:34.30ms
step:212/2160 train_time:7270ms step_avg:34.29ms
step:213/2160 train_time:7304ms step_avg:34.29ms
step:214/2160 train_time:7337ms step_avg:34.28ms
step:215/2160 train_time:7371ms step_avg:34.28ms
step:216/2160 train_time:7404ms step_avg:34.28ms
step:217/2160 train_time:7438ms step_avg:34.27ms
step:218/2160 train_time:7471ms step_avg:34.27ms
step:219/2160 train_time:7504ms step_avg:34.27ms
step:220/2160 train_time:7538ms step_avg:34.26ms
step:221/2160 train_time:7571ms step_avg:34.26ms
step:222/2160 train_time:7605ms step_avg:34.26ms
step:223/2160 train_time:7638ms step_avg:34.25ms
step:224/2160 train_time:7672ms step_avg:34.25ms
step:225/2160 train_time:7705ms step_avg:34.25ms
step:226/2160 train_time:7739ms step_avg:34.24ms
step:227/2160 train_time:7772ms step_avg:34.24ms
step:228/2160 train_time:7805ms step_avg:34.23ms
step:229/2160 train_time:7839ms step_avg:34.23ms
step:230/2160 train_time:7872ms step_avg:34.23ms
step:231/2160 train_time:7906ms step_avg:34.23ms
step:232/2160 train_time:7939ms step_avg:34.22ms
step:233/2160 train_time:7973ms step_avg:34.22ms
step:234/2160 train_time:8006ms step_avg:34.21ms
step:235/2160 train_time:8040ms step_avg:34.21ms
step:236/2160 train_time:8073ms step_avg:34.21ms
step:237/2160 train_time:8107ms step_avg:34.21ms
step:238/2160 train_time:8140ms step_avg:34.20ms
step:239/2160 train_time:8173ms step_avg:34.20ms
step:240/2160 train_time:8207ms step_avg:34.19ms
step:241/2160 train_time:8241ms step_avg:34.19ms
step:242/2160 train_time:8274ms step_avg:34.19ms
step:243/2160 train_time:8307ms step_avg:34.18ms
step:244/2160 train_time:8340ms step_avg:34.18ms
step:245/2160 train_time:8374ms step_avg:34.18ms
step:246/2160 train_time:8407ms step_avg:34.18ms
step:247/2160 train_time:8441ms step_avg:34.17ms
step:248/2160 train_time:8474ms step_avg:34.17ms
step:249/2160 train_time:8508ms step_avg:34.17ms
step:250/2160 train_time:8541ms step_avg:34.16ms
step:250/2160 val_loss:4.3125 train_time:8576ms step_avg:34.31ms
step:251/2160 train_time:8599ms step_avg:34.26ms
step:252/2160 train_time:8622ms step_avg:34.21ms
step:253/2160 train_time:8646ms step_avg:34.17ms
step:254/2160 train_time:8680ms step_avg:34.17ms
step:255/2160 train_time:8715ms step_avg:34.18ms
step:256/2160 train_time:8750ms step_avg:34.18ms
step:257/2160 train_time:8785ms step_avg:34.18ms
step:258/2160 train_time:8818ms step_avg:34.18ms
step:259/2160 train_time:8852ms step_avg:34.18ms
step:260/2160 train_time:8886ms step_avg:34.18ms
step:261/2160 train_time:8920ms step_avg:34.17ms
step:262/2160 train_time:8953ms step_avg:34.17ms
step:263/2160 train_time:8986ms step_avg:34.17ms
step:264/2160 train_time:9020ms step_avg:34.17ms
step:265/2160 train_time:9053ms step_avg:34.16ms
step:266/2160 train_time:9086ms step_avg:34.16ms
step:267/2160 train_time:9120ms step_avg:34.16ms
step:268/2160 train_time:9153ms step_avg:34.15ms
step:269/2160 train_time:9186ms step_avg:34.15ms
step:270/2160 train_time:9220ms step_avg:34.15ms
step:271/2160 train_time:9253ms step_avg:34.14ms
step:272/2160 train_time:9287ms step_avg:34.14ms
step:273/2160 train_time:9320ms step_avg:34.14ms
step:274/2160 train_time:9353ms step_avg:34.14ms
step:275/2160 train_time:9387ms step_avg:34.13ms
step:276/2160 train_time:9420ms step_avg:34.13ms
step:277/2160 train_time:9453ms step_avg:34.13ms
step:278/2160 train_time:9486ms step_avg:34.12ms
step:279/2160 train_time:9520ms step_avg:34.12ms
step:280/2160 train_time:9553ms step_avg:34.12ms
step:281/2160 train_time:9587ms step_avg:34.12ms
step:282/2160 train_time:9620ms step_avg:34.12ms
step:283/2160 train_time:9654ms step_avg:34.11ms
step:284/2160 train_time:9688ms step_avg:34.11ms
step:285/2160 train_time:9722ms step_avg:34.11ms
step:286/2160 train_time:9756ms step_avg:34.11ms
step:287/2160 train_time:9790ms step_avg:34.11ms
step:288/2160 train_time:9823ms step_avg:34.11ms
step:289/2160 train_time:9857ms step_avg:34.11ms
step:290/2160 train_time:9890ms step_avg:34.10ms
step:291/2160 train_time:9924ms step_avg:34.10ms
step:292/2160 train_time:9958ms step_avg:34.10ms
step:293/2160 train_time:9991ms step_avg:34.10ms
step:294/2160 train_time:10024ms step_avg:34.10ms
step:295/2160 train_time:10059ms step_avg:34.10ms
step:296/2160 train_time:10092ms step_avg:34.09ms
step:297/2160 train_time:10126ms step_avg:34.09ms
step:298/2160 train_time:10159ms step_avg:34.09ms
step:299/2160 train_time:10192ms step_avg:34.09ms
step:300/2160 train_time:10226ms step_avg:34.09ms
step:301/2160 train_time:10259ms step_avg:34.08ms
step:302/2160 train_time:10292ms step_avg:34.08ms
step:303/2160 train_time:10326ms step_avg:34.08ms
step:304/2160 train_time:10360ms step_avg:34.08ms
step:305/2160 train_time:10393ms step_avg:34.08ms
step:306/2160 train_time:10426ms step_avg:34.07ms
step:307/2160 train_time:10460ms step_avg:34.07ms
step:308/2160 train_time:10493ms step_avg:34.07ms
step:309/2160 train_time:10526ms step_avg:34.07ms
step:310/2160 train_time:10560ms step_avg:34.06ms
step:311/2160 train_time:10593ms step_avg:34.06ms
step:312/2160 train_time:10627ms step_avg:34.06ms
step:313/2160 train_time:10660ms step_avg:34.06ms
step:314/2160 train_time:10694ms step_avg:34.06ms
step:315/2160 train_time:10728ms step_avg:34.06ms
step:316/2160 train_time:10761ms step_avg:34.06ms
step:317/2160 train_time:10795ms step_avg:34.05ms
step:318/2160 train_time:10828ms step_avg:34.05ms
step:319/2160 train_time:10862ms step_avg:34.05ms
step:320/2160 train_time:10895ms step_avg:34.05ms
step:321/2160 train_time:10929ms step_avg:34.05ms
step:322/2160 train_time:10962ms step_avg:34.04ms
step:323/2160 train_time:10996ms step_avg:34.04ms
step:324/2160 train_time:11029ms step_avg:34.04ms
step:325/2160 train_time:11063ms step_avg:34.04ms
step:326/2160 train_time:11097ms step_avg:34.04ms
step:327/2160 train_time:11130ms step_avg:34.04ms
step:328/2160 train_time:11164ms step_avg:34.04ms
step:329/2160 train_time:11197ms step_avg:34.03ms
step:330/2160 train_time:11230ms step_avg:34.03ms
step:331/2160 train_time:11264ms step_avg:34.03ms
step:332/2160 train_time:11298ms step_avg:34.03ms
step:333/2160 train_time:11331ms step_avg:34.03ms
step:334/2160 train_time:11365ms step_avg:34.03ms
step:335/2160 train_time:11398ms step_avg:34.02ms
step:336/2160 train_time:11431ms step_avg:34.02ms
step:337/2160 train_time:11465ms step_avg:34.02ms
step:338/2160 train_time:11499ms step_avg:34.02ms
step:339/2160 train_time:11532ms step_avg:34.02ms
step:340/2160 train_time:11566ms step_avg:34.02ms
step:341/2160 train_time:11599ms step_avg:34.01ms
step:342/2160 train_time:11632ms step_avg:34.01ms
step:343/2160 train_time:11666ms step_avg:34.01ms
step:344/2160 train_time:11699ms step_avg:34.01ms
step:345/2160 train_time:11733ms step_avg:34.01ms
step:346/2160 train_time:11766ms step_avg:34.01ms
step:347/2160 train_time:11800ms step_avg:34.01ms
step:348/2160 train_time:11834ms step_avg:34.00ms
step:349/2160 train_time:11867ms step_avg:34.00ms
step:350/2160 train_time:11901ms step_avg:34.00ms
step:351/2160 train_time:11934ms step_avg:34.00ms
step:352/2160 train_time:11968ms step_avg:34.00ms
step:353/2160 train_time:12002ms step_avg:34.00ms
step:354/2160 train_time:12036ms step_avg:34.00ms
step:355/2160 train_time:12069ms step_avg:34.00ms
step:356/2160 train_time:12103ms step_avg:34.00ms
step:357/2160 train_time:12136ms step_avg:33.99ms
step:358/2160 train_time:12170ms step_avg:33.99ms
step:359/2160 train_time:12203ms step_avg:33.99ms
step:360/2160 train_time:12236ms step_avg:33.99ms
step:361/2160 train_time:12270ms step_avg:33.99ms
step:362/2160 train_time:12303ms step_avg:33.99ms
step:363/2160 train_time:12337ms step_avg:33.99ms
step:364/2160 train_time:12371ms step_avg:33.98ms
step:365/2160 train_time:12404ms step_avg:33.98ms
step:366/2160 train_time:12438ms step_avg:33.98ms
step:367/2160 train_time:12471ms step_avg:33.98ms
step:368/2160 train_time:12505ms step_avg:33.98ms
step:369/2160 train_time:12538ms step_avg:33.98ms
step:370/2160 train_time:12572ms step_avg:33.98ms
step:371/2160 train_time:12605ms step_avg:33.98ms
step:372/2160 train_time:12639ms step_avg:33.98ms
step:373/2160 train_time:12672ms step_avg:33.97ms
step:374/2160 train_time:12706ms step_avg:33.97ms
step:375/2160 train_time:12740ms step_avg:33.97ms
step:376/2160 train_time:12773ms step_avg:33.97ms
step:377/2160 train_time:12807ms step_avg:33.97ms
step:378/2160 train_time:12840ms step_avg:33.97ms
step:379/2160 train_time:12873ms step_avg:33.97ms
step:380/2160 train_time:12907ms step_avg:33.97ms
step:381/2160 train_time:12940ms step_avg:33.96ms
step:382/2160 train_time:12974ms step_avg:33.96ms
step:383/2160 train_time:13007ms step_avg:33.96ms
step:384/2160 train_time:13041ms step_avg:33.96ms
step:385/2160 train_time:13074ms step_avg:33.96ms
step:386/2160 train_time:13108ms step_avg:33.96ms
step:387/2160 train_time:13142ms step_avg:33.96ms
step:388/2160 train_time:13175ms step_avg:33.96ms
step:389/2160 train_time:13209ms step_avg:33.96ms
step:390/2160 train_time:13242ms step_avg:33.95ms
step:391/2160 train_time:13276ms step_avg:33.95ms
step:392/2160 train_time:13309ms step_avg:33.95ms
step:393/2160 train_time:13343ms step_avg:33.95ms
step:394/2160 train_time:13376ms step_avg:33.95ms
step:395/2160 train_time:13410ms step_avg:33.95ms
step:396/2160 train_time:13443ms step_avg:33.95ms
step:397/2160 train_time:13477ms step_avg:33.95ms
step:398/2160 train_time:13510ms step_avg:33.95ms
step:399/2160 train_time:13544ms step_avg:33.94ms
step:400/2160 train_time:13577ms step_avg:33.94ms
step:401/2160 train_time:13610ms step_avg:33.94ms
step:402/2160 train_time:13643ms step_avg:33.94ms
step:403/2160 train_time:13677ms step_avg:33.94ms
step:404/2160 train_time:13710ms step_avg:33.94ms
step:405/2160 train_time:13744ms step_avg:33.94ms
step:406/2160 train_time:13778ms step_avg:33.93ms
step:407/2160 train_time:13811ms step_avg:33.93ms
step:408/2160 train_time:13844ms step_avg:33.93ms
step:409/2160 train_time:13878ms step_avg:33.93ms
step:410/2160 train_time:13911ms step_avg:33.93ms
step:411/2160 train_time:13945ms step_avg:33.93ms
step:412/2160 train_time:13978ms step_avg:33.93ms
step:413/2160 train_time:14012ms step_avg:33.93ms
step:414/2160 train_time:14045ms step_avg:33.93ms
step:415/2160 train_time:14079ms step_avg:33.93ms
step:416/2160 train_time:14113ms step_avg:33.92ms
step:417/2160 train_time:14147ms step_avg:33.92ms
step:418/2160 train_time:14180ms step_avg:33.92ms
step:419/2160 train_time:14214ms step_avg:33.92ms
step:420/2160 train_time:14247ms step_avg:33.92ms
step:421/2160 train_time:14281ms step_avg:33.92ms
step:422/2160 train_time:14314ms step_avg:33.92ms
step:423/2160 train_time:14348ms step_avg:33.92ms
step:424/2160 train_time:14381ms step_avg:33.92ms
step:425/2160 train_time:14415ms step_avg:33.92ms
step:426/2160 train_time:14448ms step_avg:33.92ms
step:427/2160 train_time:14483ms step_avg:33.92ms
step:428/2160 train_time:14516ms step_avg:33.92ms
step:429/2160 train_time:14550ms step_avg:33.92ms
step:430/2160 train_time:14583ms step_avg:33.91ms
step:431/2160 train_time:14616ms step_avg:33.91ms
step:432/2160 train_time:14649ms step_avg:33.91ms
step:433/2160 train_time:14683ms step_avg:33.91ms
step:434/2160 train_time:14716ms step_avg:33.91ms
step:435/2160 train_time:14750ms step_avg:33.91ms
step:436/2160 train_time:14783ms step_avg:33.91ms
step:437/2160 train_time:14817ms step_avg:33.91ms
step:438/2160 train_time:14851ms step_avg:33.91ms
step:439/2160 train_time:14885ms step_avg:33.91ms
step:440/2160 train_time:14918ms step_avg:33.91ms
step:441/2160 train_time:14952ms step_avg:33.91ms
step:442/2160 train_time:14986ms step_avg:33.90ms
step:443/2160 train_time:15020ms step_avg:33.90ms
step:444/2160 train_time:15053ms step_avg:33.90ms
step:445/2160 train_time:15087ms step_avg:33.90ms
step:446/2160 train_time:15120ms step_avg:33.90ms
step:447/2160 train_time:15154ms step_avg:33.90ms
step:448/2160 train_time:15187ms step_avg:33.90ms
step:449/2160 train_time:15221ms step_avg:33.90ms
step:450/2160 train_time:15254ms step_avg:33.90ms
step:451/2160 train_time:15288ms step_avg:33.90ms
step:452/2160 train_time:15321ms step_avg:33.90ms
step:453/2160 train_time:15355ms step_avg:33.90ms
step:454/2160 train_time:15388ms step_avg:33.89ms
step:455/2160 train_time:15422ms step_avg:33.90ms
step:456/2160 train_time:15456ms step_avg:33.89ms
step:457/2160 train_time:15489ms step_avg:33.89ms
step:458/2160 train_time:15522ms step_avg:33.89ms
step:459/2160 train_time:15556ms step_avg:33.89ms
step:460/2160 train_time:15589ms step_avg:33.89ms
step:461/2160 train_time:15623ms step_avg:33.89ms
step:462/2160 train_time:15656ms step_avg:33.89ms
step:463/2160 train_time:15690ms step_avg:33.89ms
step:464/2160 train_time:15723ms step_avg:33.89ms
step:465/2160 train_time:15757ms step_avg:33.89ms
step:466/2160 train_time:15790ms step_avg:33.89ms
step:467/2160 train_time:15825ms step_avg:33.89ms
step:468/2160 train_time:15858ms step_avg:33.88ms
step:469/2160 train_time:15892ms step_avg:33.88ms
step:470/2160 train_time:15925ms step_avg:33.88ms
step:471/2160 train_time:15959ms step_avg:33.88ms
step:472/2160 train_time:15992ms step_avg:33.88ms
step:473/2160 train_time:16026ms step_avg:33.88ms
step:474/2160 train_time:16059ms step_avg:33.88ms
step:475/2160 train_time:16093ms step_avg:33.88ms
step:476/2160 train_time:16126ms step_avg:33.88ms
step:477/2160 train_time:16160ms step_avg:33.88ms
step:478/2160 train_time:16193ms step_avg:33.88ms
step:479/2160 train_time:16227ms step_avg:33.88ms
step:480/2160 train_time:16260ms step_avg:33.88ms
step:481/2160 train_time:16293ms step_avg:33.87ms
step:482/2160 train_time:16327ms step_avg:33.87ms
step:483/2160 train_time:16361ms step_avg:33.87ms
step:484/2160 train_time:16394ms step_avg:33.87ms
step:485/2160 train_time:16428ms step_avg:33.87ms
step:486/2160 train_time:16461ms step_avg:33.87ms
step:487/2160 train_time:16495ms step_avg:33.87ms
step:488/2160 train_time:16528ms step_avg:33.87ms
step:489/2160 train_time:16562ms step_avg:33.87ms
step:490/2160 train_time:16595ms step_avg:33.87ms
step:491/2160 train_time:16629ms step_avg:33.87ms
step:492/2160 train_time:16662ms step_avg:33.87ms
step:493/2160 train_time:16696ms step_avg:33.87ms
step:494/2160 train_time:16729ms step_avg:33.86ms
step:495/2160 train_time:16763ms step_avg:33.86ms
step:496/2160 train_time:16796ms step_avg:33.86ms
step:497/2160 train_time:16829ms step_avg:33.86ms
step:498/2160 train_time:16863ms step_avg:33.86ms
step:499/2160 train_time:16896ms step_avg:33.86ms
step:500/2160 train_time:16929ms step_avg:33.86ms
step:500/2160 val_loss:4.0138 train_time:16964ms step_avg:33.93ms
step:501/2160 train_time:16987ms step_avg:33.91ms
step:502/2160 train_time:17010ms step_avg:33.88ms
step:503/2160 train_time:17033ms step_avg:33.86ms
step:504/2160 train_time:17067ms step_avg:33.86ms
step:505/2160 train_time:17102ms step_avg:33.87ms
step:506/2160 train_time:17136ms step_avg:33.87ms
step:507/2160 train_time:17171ms step_avg:33.87ms
step:508/2160 train_time:17204ms step_avg:33.87ms
step:509/2160 train_time:17238ms step_avg:33.87ms
step:510/2160 train_time:17272ms step_avg:33.87ms
step:511/2160 train_time:17305ms step_avg:33.87ms
step:512/2160 train_time:17339ms step_avg:33.86ms
step:513/2160 train_time:17372ms step_avg:33.86ms
step:514/2160 train_time:17408ms step_avg:33.87ms
step:515/2160 train_time:17439ms step_avg:33.86ms
step:516/2160 train_time:17472ms step_avg:33.86ms
step:517/2160 train_time:17505ms step_avg:33.86ms
step:518/2160 train_time:17539ms step_avg:33.86ms
step:519/2160 train_time:17572ms step_avg:33.86ms
step:520/2160 train_time:17605ms step_avg:33.86ms
step:521/2160 train_time:17639ms step_avg:33.86ms
step:522/2160 train_time:17672ms step_avg:33.85ms
step:523/2160 train_time:17706ms step_avg:33.85ms
step:524/2160 train_time:17739ms step_avg:33.85ms
step:525/2160 train_time:17773ms step_avg:33.85ms
step:526/2160 train_time:17806ms step_avg:33.85ms
step:527/2160 train_time:17839ms step_avg:33.85ms
step:528/2160 train_time:17872ms step_avg:33.85ms
step:529/2160 train_time:17906ms step_avg:33.85ms
step:530/2160 train_time:17939ms step_avg:33.85ms
step:531/2160 train_time:17974ms step_avg:33.85ms
step:532/2160 train_time:18008ms step_avg:33.85ms
step:533/2160 train_time:18042ms step_avg:33.85ms
step:534/2160 train_time:18075ms step_avg:33.85ms
step:535/2160 train_time:18109ms step_avg:33.85ms
step:536/2160 train_time:18143ms step_avg:33.85ms
step:537/2160 train_time:18177ms step_avg:33.85ms
step:538/2160 train_time:18210ms step_avg:33.85ms
step:539/2160 train_time:18244ms step_avg:33.85ms
step:540/2160 train_time:18277ms step_avg:33.85ms
step:541/2160 train_time:18311ms step_avg:33.85ms
step:542/2160 train_time:18344ms step_avg:33.85ms
step:543/2160 train_time:18378ms step_avg:33.85ms
step:544/2160 train_time:18411ms step_avg:33.84ms
step:545/2160 train_time:18445ms step_avg:33.84ms
step:546/2160 train_time:18478ms step_avg:33.84ms
step:547/2160 train_time:18512ms step_avg:33.84ms
step:548/2160 train_time:18545ms step_avg:33.84ms
step:549/2160 train_time:18579ms step_avg:33.84ms
step:550/2160 train_time:18612ms step_avg:33.84ms
step:551/2160 train_time:18646ms step_avg:33.84ms
step:552/2160 train_time:18679ms step_avg:33.84ms
step:553/2160 train_time:18713ms step_avg:33.84ms
step:554/2160 train_time:18746ms step_avg:33.84ms
step:555/2160 train_time:18780ms step_avg:33.84ms
step:556/2160 train_time:18813ms step_avg:33.84ms
step:557/2160 train_time:18847ms step_avg:33.84ms
step:558/2160 train_time:18880ms step_avg:33.84ms
step:559/2160 train_time:18914ms step_avg:33.83ms
step:560/2160 train_time:18947ms step_avg:33.83ms
step:561/2160 train_time:18980ms step_avg:33.83ms
step:562/2160 train_time:19014ms step_avg:33.83ms
step:563/2160 train_time:19048ms step_avg:33.83ms
step:564/2160 train_time:19082ms step_avg:33.83ms
step:565/2160 train_time:19115ms step_avg:33.83ms
step:566/2160 train_time:19149ms step_avg:33.83ms
step:567/2160 train_time:19182ms step_avg:33.83ms
step:568/2160 train_time:19216ms step_avg:33.83ms
step:569/2160 train_time:19250ms step_avg:33.83ms
step:570/2160 train_time:19283ms step_avg:33.83ms
step:571/2160 train_time:19317ms step_avg:33.83ms
step:572/2160 train_time:19350ms step_avg:33.83ms
step:573/2160 train_time:19384ms step_avg:33.83ms
step:574/2160 train_time:19417ms step_avg:33.83ms
step:575/2160 train_time:19451ms step_avg:33.83ms
step:576/2160 train_time:19484ms step_avg:33.83ms
step:577/2160 train_time:19518ms step_avg:33.83ms
step:578/2160 train_time:19551ms step_avg:33.83ms
step:579/2160 train_time:19584ms step_avg:33.82ms
step:580/2160 train_time:19618ms step_avg:33.82ms
step:581/2160 train_time:19651ms step_avg:33.82ms
step:582/2160 train_time:19685ms step_avg:33.82ms
step:583/2160 train_time:19718ms step_avg:33.82ms
step:584/2160 train_time:19752ms step_avg:33.82ms
step:585/2160 train_time:19785ms step_avg:33.82ms
step:586/2160 train_time:19819ms step_avg:33.82ms
step:587/2160 train_time:19852ms step_avg:33.82ms
step:588/2160 train_time:19886ms step_avg:33.82ms
step:589/2160 train_time:19919ms step_avg:33.82ms
step:590/2160 train_time:19953ms step_avg:33.82ms
step:591/2160 train_time:19987ms step_avg:33.82ms
step:592/2160 train_time:20020ms step_avg:33.82ms
step:593/2160 train_time:20054ms step_avg:33.82ms
step:594/2160 train_time:20087ms step_avg:33.82ms
step:595/2160 train_time:20121ms step_avg:33.82ms
step:596/2160 train_time:20154ms step_avg:33.82ms
step:597/2160 train_time:20188ms step_avg:33.82ms
step:598/2160 train_time:20222ms step_avg:33.82ms
step:599/2160 train_time:20256ms step_avg:33.82ms
step:600/2160 train_time:20289ms step_avg:33.82ms
step:601/2160 train_time:20323ms step_avg:33.81ms
step:602/2160 train_time:20356ms step_avg:33.81ms
step:603/2160 train_time:20390ms step_avg:33.81ms
step:604/2160 train_time:20424ms step_avg:33.81ms
step:605/2160 train_time:20457ms step_avg:33.81ms
step:606/2160 train_time:20491ms step_avg:33.81ms
step:607/2160 train_time:20524ms step_avg:33.81ms
step:608/2160 train_time:20557ms step_avg:33.81ms
step:609/2160 train_time:20591ms step_avg:33.81ms
step:610/2160 train_time:20625ms step_avg:33.81ms
step:611/2160 train_time:20658ms step_avg:33.81ms
step:612/2160 train_time:20692ms step_avg:33.81ms
step:613/2160 train_time:20725ms step_avg:33.81ms
step:614/2160 train_time:20758ms step_avg:33.81ms
step:615/2160 train_time:20792ms step_avg:33.81ms
step:616/2160 train_time:20825ms step_avg:33.81ms
step:617/2160 train_time:20859ms step_avg:33.81ms
step:618/2160 train_time:20892ms step_avg:33.81ms
step:619/2160 train_time:20925ms step_avg:33.80ms
step:620/2160 train_time:20958ms step_avg:33.80ms
step:621/2160 train_time:20993ms step_avg:33.80ms
step:622/2160 train_time:21027ms step_avg:33.80ms
step:623/2160 train_time:21060ms step_avg:33.80ms
step:624/2160 train_time:21094ms step_avg:33.80ms
step:625/2160 train_time:21127ms step_avg:33.80ms
step:626/2160 train_time:21161ms step_avg:33.80ms
step:627/2160 train_time:21194ms step_avg:33.80ms
step:628/2160 train_time:21228ms step_avg:33.80ms
step:629/2160 train_time:21262ms step_avg:33.80ms
step:630/2160 train_time:21295ms step_avg:33.80ms
step:631/2160 train_time:21329ms step_avg:33.80ms
step:632/2160 train_time:21363ms step_avg:33.80ms
step:633/2160 train_time:21396ms step_avg:33.80ms
step:634/2160 train_time:21430ms step_avg:33.80ms
step:635/2160 train_time:21463ms step_avg:33.80ms
step:636/2160 train_time:21496ms step_avg:33.80ms
step:637/2160 train_time:21530ms step_avg:33.80ms
step:638/2160 train_time:21564ms step_avg:33.80ms
step:639/2160 train_time:21598ms step_avg:33.80ms
step:640/2160 train_time:21631ms step_avg:33.80ms
step:641/2160 train_time:21665ms step_avg:33.80ms
step:642/2160 train_time:21698ms step_avg:33.80ms
step:643/2160 train_time:21732ms step_avg:33.80ms
step:644/2160 train_time:21765ms step_avg:33.80ms
step:645/2160 train_time:21799ms step_avg:33.80ms
step:646/2160 train_time:21833ms step_avg:33.80ms
step:647/2160 train_time:21866ms step_avg:33.80ms
step:648/2160 train_time:21899ms step_avg:33.80ms
step:649/2160 train_time:21933ms step_avg:33.80ms
step:650/2160 train_time:21967ms step_avg:33.80ms
step:651/2160 train_time:22000ms step_avg:33.79ms
step:652/2160 train_time:22034ms step_avg:33.79ms
step:653/2160 train_time:22068ms step_avg:33.79ms
step:654/2160 train_time:22101ms step_avg:33.79ms
step:655/2160 train_time:22135ms step_avg:33.79ms
step:656/2160 train_time:22168ms step_avg:33.79ms
step:657/2160 train_time:22202ms step_avg:33.79ms
step:658/2160 train_time:22235ms step_avg:33.79ms
step:659/2160 train_time:22269ms step_avg:33.79ms
step:660/2160 train_time:22303ms step_avg:33.79ms
step:661/2160 train_time:22336ms step_avg:33.79ms
step:662/2160 train_time:22370ms step_avg:33.79ms
step:663/2160 train_time:22404ms step_avg:33.79ms
step:664/2160 train_time:22438ms step_avg:33.79ms
step:665/2160 train_time:22471ms step_avg:33.79ms
step:666/2160 train_time:22505ms step_avg:33.79ms
step:667/2160 train_time:22539ms step_avg:33.79ms
step:668/2160 train_time:22572ms step_avg:33.79ms
step:669/2160 train_time:22606ms step_avg:33.79ms
step:670/2160 train_time:22639ms step_avg:33.79ms
step:671/2160 train_time:22673ms step_avg:33.79ms
step:672/2160 train_time:22706ms step_avg:33.79ms
step:673/2160 train_time:22740ms step_avg:33.79ms
step:674/2160 train_time:22773ms step_avg:33.79ms
step:675/2160 train_time:22807ms step_avg:33.79ms
step:676/2160 train_time:22840ms step_avg:33.79ms
step:677/2160 train_time:22874ms step_avg:33.79ms
step:678/2160 train_time:22908ms step_avg:33.79ms
step:679/2160 train_time:22942ms step_avg:33.79ms
step:680/2160 train_time:22975ms step_avg:33.79ms
step:681/2160 train_time:23008ms step_avg:33.79ms
step:682/2160 train_time:23042ms step_avg:33.79ms
step:683/2160 train_time:23075ms step_avg:33.79ms
step:684/2160 train_time:23109ms step_avg:33.78ms
step:685/2160 train_time:23142ms step_avg:33.78ms
step:686/2160 train_time:23176ms step_avg:33.78ms
step:687/2160 train_time:23210ms step_avg:33.78ms
step:688/2160 train_time:23244ms step_avg:33.78ms
step:689/2160 train_time:23277ms step_avg:33.78ms
step:690/2160 train_time:23310ms step_avg:33.78ms
step:691/2160 train_time:23344ms step_avg:33.78ms
step:692/2160 train_time:23377ms step_avg:33.78ms
step:693/2160 train_time:23412ms step_avg:33.78ms
step:694/2160 train_time:23445ms step_avg:33.78ms
step:695/2160 train_time:23479ms step_avg:33.78ms
step:696/2160 train_time:23512ms step_avg:33.78ms
step:697/2160 train_time:23546ms step_avg:33.78ms
step:698/2160 train_time:23579ms step_avg:33.78ms
step:699/2160 train_time:23613ms step_avg:33.78ms
step:700/2160 train_time:23646ms step_avg:33.78ms
step:701/2160 train_time:23680ms step_avg:33.78ms
step:702/2160 train_time:23713ms step_avg:33.78ms
step:703/2160 train_time:23747ms step_avg:33.78ms
step:704/2160 train_time:23781ms step_avg:33.78ms
step:705/2160 train_time:23815ms step_avg:33.78ms
step:706/2160 train_time:23848ms step_avg:33.78ms
step:707/2160 train_time:23882ms step_avg:33.78ms
step:708/2160 train_time:23916ms step_avg:33.78ms
step:709/2160 train_time:23975ms step_avg:33.82ms
step:710/2160 train_time:24035ms step_avg:33.85ms
step:711/2160 train_time:24096ms step_avg:33.89ms
step:712/2160 train_time:24155ms step_avg:33.93ms
step:713/2160 train_time:24216ms step_avg:33.96ms
step:714/2160 train_time:24276ms step_avg:34.00ms
step:715/2160 train_time:24338ms step_avg:34.04ms
step:716/2160 train_time:24398ms step_avg:34.07ms
step:717/2160 train_time:24460ms step_avg:34.11ms
step:718/2160 train_time:24520ms step_avg:34.15ms
step:719/2160 train_time:24582ms step_avg:34.19ms
step:720/2160 train_time:24642ms step_avg:34.22ms
step:721/2160 train_time:24703ms step_avg:34.26ms
step:722/2160 train_time:24762ms step_avg:34.30ms
step:723/2160 train_time:24824ms step_avg:34.33ms
step:724/2160 train_time:24883ms step_avg:34.37ms
step:725/2160 train_time:24944ms step_avg:34.41ms
step:726/2160 train_time:25003ms step_avg:34.44ms
step:727/2160 train_time:25065ms step_avg:34.48ms
step:728/2160 train_time:25124ms step_avg:34.51ms
step:729/2160 train_time:25186ms step_avg:34.55ms
step:730/2160 train_time:25245ms step_avg:34.58ms
step:731/2160 train_time:25306ms step_avg:34.62ms
step:732/2160 train_time:25366ms step_avg:34.65ms
step:733/2160 train_time:25427ms step_avg:34.69ms
step:734/2160 train_time:25487ms step_avg:34.72ms
step:735/2160 train_time:25549ms step_avg:34.76ms
step:736/2160 train_time:25608ms step_avg:34.79ms
step:737/2160 train_time:25669ms step_avg:34.83ms
step:738/2160 train_time:25728ms step_avg:34.86ms
step:739/2160 train_time:25789ms step_avg:34.90ms
step:740/2160 train_time:25849ms step_avg:34.93ms
step:741/2160 train_time:25910ms step_avg:34.97ms
step:742/2160 train_time:25969ms step_avg:35.00ms
step:743/2160 train_time:26030ms step_avg:35.03ms
step:744/2160 train_time:26090ms step_avg:35.07ms
step:745/2160 train_time:26152ms step_avg:35.10ms
step:746/2160 train_time:26211ms step_avg:35.14ms
step:747/2160 train_time:26272ms step_avg:35.17ms
step:748/2160 train_time:26331ms step_avg:35.20ms
step:749/2160 train_time:26393ms step_avg:35.24ms
step:750/2160 train_time:26453ms step_avg:35.27ms
step:750/2160 val_loss:3.8511 train_time:26515ms step_avg:35.35ms
step:751/2160 train_time:26538ms step_avg:35.34ms
step:752/2160 train_time:26578ms step_avg:35.34ms
step:753/2160 train_time:26643ms step_avg:35.38ms
step:754/2160 train_time:26707ms step_avg:35.42ms
step:755/2160 train_time:26770ms step_avg:35.46ms
step:756/2160 train_time:26829ms step_avg:35.49ms
step:757/2160 train_time:26890ms step_avg:35.52ms
step:758/2160 train_time:26949ms step_avg:35.55ms
step:759/2160 train_time:27010ms step_avg:35.59ms
step:760/2160 train_time:27068ms step_avg:35.62ms
step:761/2160 train_time:27129ms step_avg:35.65ms
step:762/2160 train_time:27187ms step_avg:35.68ms
step:763/2160 train_time:27247ms step_avg:35.71ms
step:764/2160 train_time:27306ms step_avg:35.74ms
step:765/2160 train_time:27366ms step_avg:35.77ms
step:766/2160 train_time:27425ms step_avg:35.80ms
step:767/2160 train_time:27488ms step_avg:35.84ms
step:768/2160 train_time:27548ms step_avg:35.87ms
step:769/2160 train_time:27612ms step_avg:35.91ms
step:770/2160 train_time:27674ms step_avg:35.94ms
step:771/2160 train_time:27736ms step_avg:35.97ms
step:772/2160 train_time:27795ms step_avg:36.00ms
step:773/2160 train_time:27856ms step_avg:36.04ms
step:774/2160 train_time:27915ms step_avg:36.07ms
step:775/2160 train_time:27976ms step_avg:36.10ms
step:776/2160 train_time:28035ms step_avg:36.13ms
step:777/2160 train_time:28096ms step_avg:36.16ms
step:778/2160 train_time:28155ms step_avg:36.19ms
step:779/2160 train_time:28216ms step_avg:36.22ms
step:780/2160 train_time:28274ms step_avg:36.25ms
step:781/2160 train_time:28336ms step_avg:36.28ms
step:782/2160 train_time:28395ms step_avg:36.31ms
step:783/2160 train_time:28457ms step_avg:36.34ms
step:784/2160 train_time:28517ms step_avg:36.37ms
step:785/2160 train_time:28579ms step_avg:36.41ms
step:786/2160 train_time:28640ms step_avg:36.44ms
step:787/2160 train_time:28702ms step_avg:36.47ms
step:788/2160 train_time:28762ms step_avg:36.50ms
step:789/2160 train_time:28824ms step_avg:36.53ms
step:790/2160 train_time:28884ms step_avg:36.56ms
step:791/2160 train_time:28946ms step_avg:36.59ms
step:792/2160 train_time:29006ms step_avg:36.62ms
step:793/2160 train_time:29067ms step_avg:36.66ms
step:794/2160 train_time:29127ms step_avg:36.68ms
step:795/2160 train_time:29187ms step_avg:36.71ms
step:796/2160 train_time:29246ms step_avg:36.74ms
step:797/2160 train_time:29308ms step_avg:36.77ms
step:798/2160 train_time:29367ms step_avg:36.80ms
step:799/2160 train_time:29429ms step_avg:36.83ms
step:800/2160 train_time:29488ms step_avg:36.86ms
step:801/2160 train_time:29551ms step_avg:36.89ms
step:802/2160 train_time:29610ms step_avg:36.92ms
step:803/2160 train_time:29671ms step_avg:36.95ms
step:804/2160 train_time:29731ms step_avg:36.98ms
step:805/2160 train_time:29792ms step_avg:37.01ms
step:806/2160 train_time:29852ms step_avg:37.04ms
step:807/2160 train_time:29914ms step_avg:37.07ms
step:808/2160 train_time:29973ms step_avg:37.10ms
step:809/2160 train_time:30034ms step_avg:37.13ms
step:810/2160 train_time:30094ms step_avg:37.15ms
step:811/2160 train_time:30155ms step_avg:37.18ms
step:812/2160 train_time:30214ms step_avg:37.21ms
step:813/2160 train_time:30274ms step_avg:37.24ms
step:814/2160 train_time:30334ms step_avg:37.27ms
step:815/2160 train_time:30395ms step_avg:37.29ms
step:816/2160 train_time:30454ms step_avg:37.32ms
step:817/2160 train_time:30515ms step_avg:37.35ms
step:818/2160 train_time:30575ms step_avg:37.38ms
step:819/2160 train_time:30636ms step_avg:37.41ms
step:820/2160 train_time:30696ms step_avg:37.43ms
step:821/2160 train_time:30757ms step_avg:37.46ms
step:822/2160 train_time:30817ms step_avg:37.49ms
step:823/2160 train_time:30878ms step_avg:37.52ms
step:824/2160 train_time:30939ms step_avg:37.55ms
step:825/2160 train_time:31000ms step_avg:37.58ms
step:826/2160 train_time:31059ms step_avg:37.60ms
step:827/2160 train_time:31121ms step_avg:37.63ms
step:828/2160 train_time:31180ms step_avg:37.66ms
step:829/2160 train_time:31242ms step_avg:37.69ms
step:830/2160 train_time:31302ms step_avg:37.71ms
step:831/2160 train_time:31363ms step_avg:37.74ms
step:832/2160 train_time:31423ms step_avg:37.77ms
step:833/2160 train_time:31485ms step_avg:37.80ms
step:834/2160 train_time:31544ms step_avg:37.82ms
step:835/2160 train_time:31607ms step_avg:37.85ms
step:836/2160 train_time:31667ms step_avg:37.88ms
step:837/2160 train_time:31728ms step_avg:37.91ms
step:838/2160 train_time:31788ms step_avg:37.93ms
step:839/2160 train_time:31849ms step_avg:37.96ms
step:840/2160 train_time:31909ms step_avg:37.99ms
step:841/2160 train_time:31971ms step_avg:38.02ms
step:842/2160 train_time:32030ms step_avg:38.04ms
step:843/2160 train_time:32091ms step_avg:38.07ms
step:844/2160 train_time:32150ms step_avg:38.09ms
step:845/2160 train_time:32212ms step_avg:38.12ms
step:846/2160 train_time:32271ms step_avg:38.15ms
step:847/2160 train_time:32332ms step_avg:38.17ms
step:848/2160 train_time:32393ms step_avg:38.20ms
step:849/2160 train_time:32454ms step_avg:38.23ms
step:850/2160 train_time:32515ms step_avg:38.25ms
step:851/2160 train_time:32576ms step_avg:38.28ms
step:852/2160 train_time:32637ms step_avg:38.31ms
step:853/2160 train_time:32698ms step_avg:38.33ms
step:854/2160 train_time:32758ms step_avg:38.36ms
step:855/2160 train_time:32819ms step_avg:38.38ms
step:856/2160 train_time:32879ms step_avg:38.41ms
step:857/2160 train_time:32940ms step_avg:38.44ms
step:858/2160 train_time:33000ms step_avg:38.46ms
step:859/2160 train_time:33061ms step_avg:38.49ms
step:860/2160 train_time:33121ms step_avg:38.51ms
step:861/2160 train_time:33183ms step_avg:38.54ms
step:862/2160 train_time:33243ms step_avg:38.56ms
step:863/2160 train_time:33304ms step_avg:38.59ms
step:864/2160 train_time:33364ms step_avg:38.62ms
step:865/2160 train_time:33426ms step_avg:38.64ms
step:866/2160 train_time:33486ms step_avg:38.67ms
step:867/2160 train_time:33548ms step_avg:38.69ms
step:868/2160 train_time:33608ms step_avg:38.72ms
step:869/2160 train_time:33669ms step_avg:38.74ms
step:870/2160 train_time:33728ms step_avg:38.77ms
step:871/2160 train_time:33790ms step_avg:38.79ms
step:872/2160 train_time:33850ms step_avg:38.82ms
step:873/2160 train_time:33911ms step_avg:38.84ms
step:874/2160 train_time:33970ms step_avg:38.87ms
step:875/2160 train_time:34031ms step_avg:38.89ms
step:876/2160 train_time:34091ms step_avg:38.92ms
step:877/2160 train_time:34152ms step_avg:38.94ms
step:878/2160 train_time:34212ms step_avg:38.97ms
step:879/2160 train_time:34273ms step_avg:38.99ms
step:880/2160 train_time:34333ms step_avg:39.01ms
step:881/2160 train_time:34395ms step_avg:39.04ms
step:882/2160 train_time:34454ms step_avg:39.06ms
step:883/2160 train_time:34515ms step_avg:39.09ms
step:884/2160 train_time:34574ms step_avg:39.11ms
step:885/2160 train_time:34635ms step_avg:39.14ms
step:886/2160 train_time:34695ms step_avg:39.16ms
step:887/2160 train_time:34756ms step_avg:39.18ms
step:888/2160 train_time:34816ms step_avg:39.21ms
step:889/2160 train_time:34877ms step_avg:39.23ms
step:890/2160 train_time:34936ms step_avg:39.25ms
step:891/2160 train_time:34997ms step_avg:39.28ms
step:892/2160 train_time:35057ms step_avg:39.30ms
step:893/2160 train_time:35118ms step_avg:39.33ms
step:894/2160 train_time:35177ms step_avg:39.35ms
step:895/2160 train_time:35238ms step_avg:39.37ms
step:896/2160 train_time:35298ms step_avg:39.40ms
step:897/2160 train_time:35360ms step_avg:39.42ms
step:898/2160 train_time:35419ms step_avg:39.44ms
step:899/2160 train_time:35482ms step_avg:39.47ms
step:900/2160 train_time:35542ms step_avg:39.49ms
step:901/2160 train_time:35604ms step_avg:39.52ms
step:902/2160 train_time:35664ms step_avg:39.54ms
step:903/2160 train_time:35726ms step_avg:39.56ms
step:904/2160 train_time:35786ms step_avg:39.59ms
step:905/2160 train_time:35847ms step_avg:39.61ms
step:906/2160 train_time:35908ms step_avg:39.63ms
step:907/2160 train_time:35970ms step_avg:39.66ms
step:908/2160 train_time:36029ms step_avg:39.68ms
step:909/2160 train_time:36090ms step_avg:39.70ms
step:910/2160 train_time:36149ms step_avg:39.72ms
step:911/2160 train_time:36210ms step_avg:39.75ms
step:912/2160 train_time:36270ms step_avg:39.77ms
step:913/2160 train_time:36331ms step_avg:39.79ms
step:914/2160 train_time:36391ms step_avg:39.81ms
step:915/2160 train_time:36453ms step_avg:39.84ms
step:916/2160 train_time:36512ms step_avg:39.86ms
step:917/2160 train_time:36573ms step_avg:39.88ms
step:918/2160 train_time:36633ms step_avg:39.90ms
step:919/2160 train_time:36694ms step_avg:39.93ms
step:920/2160 train_time:36755ms step_avg:39.95ms
step:921/2160 train_time:36816ms step_avg:39.97ms
step:922/2160 train_time:36876ms step_avg:40.00ms
step:923/2160 train_time:36937ms step_avg:40.02ms
step:924/2160 train_time:36996ms step_avg:40.04ms
step:925/2160 train_time:37057ms step_avg:40.06ms
step:926/2160 train_time:37116ms step_avg:40.08ms
step:927/2160 train_time:37177ms step_avg:40.10ms
step:928/2160 train_time:37237ms step_avg:40.13ms
step:929/2160 train_time:37299ms step_avg:40.15ms
step:930/2160 train_time:37359ms step_avg:40.17ms
step:931/2160 train_time:37421ms step_avg:40.19ms
step:932/2160 train_time:37481ms step_avg:40.22ms
step:933/2160 train_time:37542ms step_avg:40.24ms
step:934/2160 train_time:37602ms step_avg:40.26ms
step:935/2160 train_time:37664ms step_avg:40.28ms
step:936/2160 train_time:37724ms step_avg:40.30ms
step:937/2160 train_time:37785ms step_avg:40.33ms
step:938/2160 train_time:37846ms step_avg:40.35ms
step:939/2160 train_time:37908ms step_avg:40.37ms
step:940/2160 train_time:37968ms step_avg:40.39ms
step:941/2160 train_time:38029ms step_avg:40.41ms
step:942/2160 train_time:38088ms step_avg:40.43ms
step:943/2160 train_time:38150ms step_avg:40.46ms
step:944/2160 train_time:38209ms step_avg:40.48ms
step:945/2160 train_time:38271ms step_avg:40.50ms
step:946/2160 train_time:38330ms step_avg:40.52ms
step:947/2160 train_time:38391ms step_avg:40.54ms
step:948/2160 train_time:38451ms step_avg:40.56ms
step:949/2160 train_time:38513ms step_avg:40.58ms
step:950/2160 train_time:38573ms step_avg:40.60ms
step:951/2160 train_time:38634ms step_avg:40.62ms
step:952/2160 train_time:38694ms step_avg:40.65ms
step:953/2160 train_time:38756ms step_avg:40.67ms
step:954/2160 train_time:38816ms step_avg:40.69ms
step:955/2160 train_time:38877ms step_avg:40.71ms
step:956/2160 train_time:38937ms step_avg:40.73ms
step:957/2160 train_time:38999ms step_avg:40.75ms
step:958/2160 train_time:39058ms step_avg:40.77ms
step:959/2160 train_time:39119ms step_avg:40.79ms
step:960/2160 train_time:39179ms step_avg:40.81ms
step:961/2160 train_time:39241ms step_avg:40.83ms
step:962/2160 train_time:39300ms step_avg:40.85ms
step:963/2160 train_time:39362ms step_avg:40.87ms
step:964/2160 train_time:39422ms step_avg:40.89ms
step:965/2160 train_time:39484ms step_avg:40.92ms
step:966/2160 train_time:39544ms step_avg:40.94ms
step:967/2160 train_time:39607ms step_avg:40.96ms
step:968/2160 train_time:39668ms step_avg:40.98ms
step:969/2160 train_time:39730ms step_avg:41.00ms
step:970/2160 train_time:39790ms step_avg:41.02ms
step:971/2160 train_time:39851ms step_avg:41.04ms
step:972/2160 train_time:39910ms step_avg:41.06ms
step:973/2160 train_time:39971ms step_avg:41.08ms
step:974/2160 train_time:40030ms step_avg:41.10ms
step:975/2160 train_time:40092ms step_avg:41.12ms
step:976/2160 train_time:40152ms step_avg:41.14ms
step:977/2160 train_time:40213ms step_avg:41.16ms
step:978/2160 train_time:40272ms step_avg:41.18ms
step:979/2160 train_time:40334ms step_avg:41.20ms
step:980/2160 train_time:40394ms step_avg:41.22ms
step:981/2160 train_time:40455ms step_avg:41.24ms
step:982/2160 train_time:40515ms step_avg:41.26ms
step:983/2160 train_time:40576ms step_avg:41.28ms
step:984/2160 train_time:40636ms step_avg:41.30ms
step:985/2160 train_time:40698ms step_avg:41.32ms
step:986/2160 train_time:40757ms step_avg:41.34ms
step:987/2160 train_time:40818ms step_avg:41.36ms
step:988/2160 train_time:40878ms step_avg:41.37ms
step:989/2160 train_time:40939ms step_avg:41.39ms
step:990/2160 train_time:40999ms step_avg:41.41ms
step:991/2160 train_time:41061ms step_avg:41.43ms
step:992/2160 train_time:41120ms step_avg:41.45ms
step:993/2160 train_time:41182ms step_avg:41.47ms
step:994/2160 train_time:41242ms step_avg:41.49ms
step:995/2160 train_time:41304ms step_avg:41.51ms
step:996/2160 train_time:41364ms step_avg:41.53ms
step:997/2160 train_time:41426ms step_avg:41.55ms
step:998/2160 train_time:41487ms step_avg:41.57ms
step:999/2160 train_time:41549ms step_avg:41.59ms
step:1000/2160 train_time:41609ms step_avg:41.61ms
step:1000/2160 val_loss:3.6904 train_time:41671ms step_avg:41.67ms
step:1001/2160 train_time:41694ms step_avg:41.65ms
step:1002/2160 train_time:41733ms step_avg:41.65ms
step:1003/2160 train_time:41797ms step_avg:41.67ms
step:1004/2160 train_time:41861ms step_avg:41.69ms
step:1005/2160 train_time:41922ms step_avg:41.71ms
step:1006/2160 train_time:41983ms step_avg:41.73ms
step:1007/2160 train_time:42044ms step_avg:41.75ms
step:1008/2160 train_time:42103ms step_avg:41.77ms
step:1009/2160 train_time:42164ms step_avg:41.79ms
step:1010/2160 train_time:42223ms step_avg:41.80ms
step:1011/2160 train_time:42284ms step_avg:41.82ms
step:1012/2160 train_time:42343ms step_avg:41.84ms
step:1013/2160 train_time:42404ms step_avg:41.86ms
step:1014/2160 train_time:42463ms step_avg:41.88ms
step:1015/2160 train_time:42523ms step_avg:41.90ms
step:1016/2160 train_time:42583ms step_avg:41.91ms
step:1017/2160 train_time:42647ms step_avg:41.93ms
step:1018/2160 train_time:42709ms step_avg:41.95ms
step:1019/2160 train_time:42775ms step_avg:41.98ms
step:1020/2160 train_time:42835ms step_avg:42.00ms
step:1021/2160 train_time:42896ms step_avg:42.01ms
step:1022/2160 train_time:42956ms step_avg:42.03ms
step:1023/2160 train_time:43018ms step_avg:42.05ms
step:1024/2160 train_time:43077ms step_avg:42.07ms
step:1025/2160 train_time:43137ms step_avg:42.09ms
step:1026/2160 train_time:43197ms step_avg:42.10ms
step:1027/2160 train_time:43258ms step_avg:42.12ms
step:1028/2160 train_time:43316ms step_avg:42.14ms
step:1029/2160 train_time:43377ms step_avg:42.15ms
step:1030/2160 train_time:43436ms step_avg:42.17ms
step:1031/2160 train_time:43497ms step_avg:42.19ms
step:1032/2160 train_time:43557ms step_avg:42.21ms
step:1033/2160 train_time:43618ms step_avg:42.22ms
step:1034/2160 train_time:43678ms step_avg:42.24ms
step:1035/2160 train_time:43741ms step_avg:42.26ms
step:1036/2160 train_time:43802ms step_avg:42.28ms
step:1037/2160 train_time:43865ms step_avg:42.30ms
step:1038/2160 train_time:43925ms step_avg:42.32ms
step:1039/2160 train_time:43988ms step_avg:42.34ms
step:1040/2160 train_time:44048ms step_avg:42.35ms
step:1041/2160 train_time:44109ms step_avg:42.37ms
step:1042/2160 train_time:44168ms step_avg:42.39ms
step:1043/2160 train_time:44230ms step_avg:42.41ms
step:1044/2160 train_time:44289ms step_avg:42.42ms
step:1045/2160 train_time:44350ms step_avg:42.44ms
step:1046/2160 train_time:44410ms step_avg:42.46ms
step:1047/2160 train_time:44472ms step_avg:42.48ms
step:1048/2160 train_time:44532ms step_avg:42.49ms
step:1049/2160 train_time:44594ms step_avg:42.51ms
step:1050/2160 train_time:44654ms step_avg:42.53ms
step:1051/2160 train_time:44716ms step_avg:42.55ms
step:1052/2160 train_time:44776ms step_avg:42.56ms
step:1053/2160 train_time:44838ms step_avg:42.58ms
step:1054/2160 train_time:44898ms step_avg:42.60ms
step:1055/2160 train_time:44960ms step_avg:42.62ms
step:1056/2160 train_time:45021ms step_avg:42.63ms
step:1057/2160 train_time:45082ms step_avg:42.65ms
step:1058/2160 train_time:45142ms step_avg:42.67ms
step:1059/2160 train_time:45203ms step_avg:42.68ms
step:1060/2160 train_time:45262ms step_avg:42.70ms
step:1061/2160 train_time:45324ms step_avg:42.72ms
step:1062/2160 train_time:45384ms step_avg:42.73ms
step:1063/2160 train_time:45445ms step_avg:42.75ms
step:1064/2160 train_time:45506ms step_avg:42.77ms
step:1065/2160 train_time:45568ms step_avg:42.79ms
step:1066/2160 train_time:45630ms step_avg:42.80ms
step:1067/2160 train_time:45692ms step_avg:42.82ms
step:1068/2160 train_time:45752ms step_avg:42.84ms
step:1069/2160 train_time:45814ms step_avg:42.86ms
step:1070/2160 train_time:45874ms step_avg:42.87ms
step:1071/2160 train_time:45935ms step_avg:42.89ms
step:1072/2160 train_time:45994ms step_avg:42.91ms
step:1073/2160 train_time:46056ms step_avg:42.92ms
step:1074/2160 train_time:46116ms step_avg:42.94ms
step:1075/2160 train_time:46176ms step_avg:42.95ms
step:1076/2160 train_time:46236ms step_avg:42.97ms
step:1077/2160 train_time:46298ms step_avg:42.99ms
step:1078/2160 train_time:46358ms step_avg:43.00ms
step:1079/2160 train_time:46419ms step_avg:43.02ms
step:1080/2160 train_time:46478ms step_avg:43.04ms
step:1081/2160 train_time:46540ms step_avg:43.05ms
step:1082/2160 train_time:46601ms step_avg:43.07ms
step:1083/2160 train_time:46662ms step_avg:43.09ms
step:1084/2160 train_time:46723ms step_avg:43.10ms
step:1085/2160 train_time:46785ms step_avg:43.12ms
step:1086/2160 train_time:46845ms step_avg:43.14ms
step:1087/2160 train_time:46908ms step_avg:43.15ms
step:1088/2160 train_time:46968ms step_avg:43.17ms
step:1089/2160 train_time:47030ms step_avg:43.19ms
step:1090/2160 train_time:47090ms step_avg:43.20ms
step:1091/2160 train_time:47152ms step_avg:43.22ms
step:1092/2160 train_time:47211ms step_avg:43.23ms
step:1093/2160 train_time:47273ms step_avg:43.25ms
step:1094/2160 train_time:47332ms step_avg:43.27ms
step:1095/2160 train_time:47393ms step_avg:43.28ms
step:1096/2160 train_time:47453ms step_avg:43.30ms
step:1097/2160 train_time:47514ms step_avg:43.31ms
step:1098/2160 train_time:47573ms step_avg:43.33ms
step:1099/2160 train_time:47635ms step_avg:43.34ms
step:1100/2160 train_time:47695ms step_avg:43.36ms
step:1101/2160 train_time:47756ms step_avg:43.38ms
step:1102/2160 train_time:47816ms step_avg:43.39ms
step:1103/2160 train_time:47878ms step_avg:43.41ms
step:1104/2160 train_time:47938ms step_avg:43.42ms
step:1105/2160 train_time:48000ms step_avg:43.44ms
step:1106/2160 train_time:48060ms step_avg:43.45ms
step:1107/2160 train_time:48120ms step_avg:43.47ms
step:1108/2160 train_time:48180ms step_avg:43.48ms
step:1109/2160 train_time:48242ms step_avg:43.50ms
step:1110/2160 train_time:48301ms step_avg:43.51ms
step:1111/2160 train_time:48363ms step_avg:43.53ms
step:1112/2160 train_time:48423ms step_avg:43.55ms
step:1113/2160 train_time:48484ms step_avg:43.56ms
step:1114/2160 train_time:48544ms step_avg:43.58ms
step:1115/2160 train_time:48606ms step_avg:43.59ms
step:1116/2160 train_time:48666ms step_avg:43.61ms
step:1117/2160 train_time:48728ms step_avg:43.62ms
step:1118/2160 train_time:48788ms step_avg:43.64ms
step:1119/2160 train_time:48851ms step_avg:43.66ms
step:1120/2160 train_time:48911ms step_avg:43.67ms
step:1121/2160 train_time:48974ms step_avg:43.69ms
step:1122/2160 train_time:49033ms step_avg:43.70ms
step:1123/2160 train_time:49095ms step_avg:43.72ms
step:1124/2160 train_time:49154ms step_avg:43.73ms
step:1125/2160 train_time:49214ms step_avg:43.75ms
step:1126/2160 train_time:49273ms step_avg:43.76ms
step:1127/2160 train_time:49335ms step_avg:43.78ms
step:1128/2160 train_time:49395ms step_avg:43.79ms
step:1129/2160 train_time:49456ms step_avg:43.80ms
step:1130/2160 train_time:49515ms step_avg:43.82ms
step:1131/2160 train_time:49577ms step_avg:43.83ms
step:1132/2160 train_time:49638ms step_avg:43.85ms
step:1133/2160 train_time:49699ms step_avg:43.87ms
step:1134/2160 train_time:49760ms step_avg:43.88ms
step:1135/2160 train_time:49821ms step_avg:43.90ms
step:1136/2160 train_time:49881ms step_avg:43.91ms
step:1137/2160 train_time:49943ms step_avg:43.93ms
step:1138/2160 train_time:50003ms step_avg:43.94ms
step:1139/2160 train_time:50066ms step_avg:43.96ms
step:1140/2160 train_time:50126ms step_avg:43.97ms
step:1141/2160 train_time:50188ms step_avg:43.99ms
step:1142/2160 train_time:50248ms step_avg:44.00ms
step:1143/2160 train_time:50310ms step_avg:44.02ms
step:1144/2160 train_time:50370ms step_avg:44.03ms
step:1145/2160 train_time:50432ms step_avg:44.05ms
step:1146/2160 train_time:50492ms step_avg:44.06ms
step:1147/2160 train_time:50554ms step_avg:44.07ms
step:1148/2160 train_time:50613ms step_avg:44.09ms
step:1149/2160 train_time:50675ms step_avg:44.10ms
step:1150/2160 train_time:50734ms step_avg:44.12ms
step:1151/2160 train_time:50796ms step_avg:44.13ms
step:1152/2160 train_time:50856ms step_avg:44.15ms
step:1153/2160 train_time:50917ms step_avg:44.16ms
step:1154/2160 train_time:50977ms step_avg:44.17ms
step:1155/2160 train_time:51039ms step_avg:44.19ms
step:1156/2160 train_time:51098ms step_avg:44.20ms
step:1157/2160 train_time:51160ms step_avg:44.22ms
step:1158/2160 train_time:51220ms step_avg:44.23ms
step:1159/2160 train_time:51282ms step_avg:44.25ms
step:1160/2160 train_time:51341ms step_avg:44.26ms
step:1161/2160 train_time:51403ms step_avg:44.28ms
step:1162/2160 train_time:51464ms step_avg:44.29ms
step:1163/2160 train_time:51526ms step_avg:44.30ms
step:1164/2160 train_time:51587ms step_avg:44.32ms
step:1165/2160 train_time:51648ms step_avg:44.33ms
step:1166/2160 train_time:51708ms step_avg:44.35ms
step:1167/2160 train_time:51771ms step_avg:44.36ms
step:1168/2160 train_time:51831ms step_avg:44.38ms
step:1169/2160 train_time:51892ms step_avg:44.39ms
step:1170/2160 train_time:51952ms step_avg:44.40ms
step:1171/2160 train_time:52015ms step_avg:44.42ms
step:1172/2160 train_time:52074ms step_avg:44.43ms
step:1173/2160 train_time:52136ms step_avg:44.45ms
step:1174/2160 train_time:52196ms step_avg:44.46ms
step:1175/2160 train_time:52257ms step_avg:44.47ms
step:1176/2160 train_time:52317ms step_avg:44.49ms
step:1177/2160 train_time:52378ms step_avg:44.50ms
step:1178/2160 train_time:52439ms step_avg:44.51ms
step:1179/2160 train_time:52500ms step_avg:44.53ms
step:1180/2160 train_time:52560ms step_avg:44.54ms
step:1181/2160 train_time:52621ms step_avg:44.56ms
step:1182/2160 train_time:52681ms step_avg:44.57ms
step:1183/2160 train_time:52742ms step_avg:44.58ms
step:1184/2160 train_time:52802ms step_avg:44.60ms
step:1185/2160 train_time:52865ms step_avg:44.61ms
step:1186/2160 train_time:52927ms step_avg:44.63ms
step:1187/2160 train_time:52987ms step_avg:44.64ms
step:1188/2160 train_time:53047ms step_avg:44.65ms
step:1189/2160 train_time:53110ms step_avg:44.67ms
step:1190/2160 train_time:53171ms step_avg:44.68ms
step:1191/2160 train_time:53233ms step_avg:44.70ms
step:1192/2160 train_time:53292ms step_avg:44.71ms
step:1193/2160 train_time:53354ms step_avg:44.72ms
step:1194/2160 train_time:53414ms step_avg:44.74ms
step:1195/2160 train_time:53475ms step_avg:44.75ms
step:1196/2160 train_time:53535ms step_avg:44.76ms
step:1197/2160 train_time:53596ms step_avg:44.78ms
step:1198/2160 train_time:53656ms step_avg:44.79ms
step:1199/2160 train_time:53717ms step_avg:44.80ms
step:1200/2160 train_time:53777ms step_avg:44.81ms
step:1201/2160 train_time:53838ms step_avg:44.83ms
step:1202/2160 train_time:53899ms step_avg:44.84ms
step:1203/2160 train_time:53961ms step_avg:44.86ms
step:1204/2160 train_time:54020ms step_avg:44.87ms
step:1205/2160 train_time:54082ms step_avg:44.88ms
step:1206/2160 train_time:54141ms step_avg:44.89ms
step:1207/2160 train_time:54203ms step_avg:44.91ms
step:1208/2160 train_time:54263ms step_avg:44.92ms
step:1209/2160 train_time:54325ms step_avg:44.93ms
step:1210/2160 train_time:54385ms step_avg:44.95ms
step:1211/2160 train_time:54447ms step_avg:44.96ms
step:1212/2160 train_time:54507ms step_avg:44.97ms
step:1213/2160 train_time:54569ms step_avg:44.99ms
step:1214/2160 train_time:54629ms step_avg:45.00ms
step:1215/2160 train_time:54692ms step_avg:45.01ms
step:1216/2160 train_time:54752ms step_avg:45.03ms
step:1217/2160 train_time:54815ms step_avg:45.04ms
step:1218/2160 train_time:54873ms step_avg:45.05ms
step:1219/2160 train_time:54935ms step_avg:45.07ms
step:1220/2160 train_time:54994ms step_avg:45.08ms
step:1221/2160 train_time:55056ms step_avg:45.09ms
step:1222/2160 train_time:55115ms step_avg:45.10ms
step:1223/2160 train_time:55176ms step_avg:45.12ms
step:1224/2160 train_time:55236ms step_avg:45.13ms
step:1225/2160 train_time:55297ms step_avg:45.14ms
step:1226/2160 train_time:55356ms step_avg:45.15ms
step:1227/2160 train_time:55418ms step_avg:45.17ms
step:1228/2160 train_time:55478ms step_avg:45.18ms
step:1229/2160 train_time:55539ms step_avg:45.19ms
step:1230/2160 train_time:55599ms step_avg:45.20ms
step:1231/2160 train_time:55660ms step_avg:45.22ms
step:1232/2160 train_time:55720ms step_avg:45.23ms
step:1233/2160 train_time:55781ms step_avg:45.24ms
step:1234/2160 train_time:55841ms step_avg:45.25ms
step:1235/2160 train_time:55902ms step_avg:45.26ms
step:1236/2160 train_time:55961ms step_avg:45.28ms
step:1237/2160 train_time:56024ms step_avg:45.29ms
step:1238/2160 train_time:56083ms step_avg:45.30ms
step:1239/2160 train_time:56146ms step_avg:45.32ms
step:1240/2160 train_time:56206ms step_avg:45.33ms
step:1241/2160 train_time:56268ms step_avg:45.34ms
step:1242/2160 train_time:56327ms step_avg:45.35ms
step:1243/2160 train_time:56389ms step_avg:45.37ms
step:1244/2160 train_time:56449ms step_avg:45.38ms
step:1245/2160 train_time:56512ms step_avg:45.39ms
step:1246/2160 train_time:56572ms step_avg:45.40ms
step:1247/2160 train_time:56634ms step_avg:45.42ms
step:1248/2160 train_time:56693ms step_avg:45.43ms
step:1249/2160 train_time:56754ms step_avg:45.44ms
step:1250/2160 train_time:56814ms step_avg:45.45ms
step:1250/2160 val_loss:3.5708 train_time:56875ms step_avg:45.50ms
step:1251/2160 train_time:56899ms step_avg:45.48ms
step:1252/2160 train_time:56938ms step_avg:45.48ms
step:1253/2160 train_time:57004ms step_avg:45.49ms
step:1254/2160 train_time:57066ms step_avg:45.51ms
step:1255/2160 train_time:57127ms step_avg:45.52ms
step:1256/2160 train_time:57187ms step_avg:45.53ms
step:1257/2160 train_time:57248ms step_avg:45.54ms
step:1258/2160 train_time:57307ms step_avg:45.55ms
step:1259/2160 train_time:57367ms step_avg:45.57ms
step:1260/2160 train_time:57426ms step_avg:45.58ms
step:1261/2160 train_time:57487ms step_avg:45.59ms
step:1262/2160 train_time:57546ms step_avg:45.60ms
step:1263/2160 train_time:57607ms step_avg:45.61ms
step:1264/2160 train_time:57666ms step_avg:45.62ms
step:1265/2160 train_time:57727ms step_avg:45.63ms
step:1266/2160 train_time:57787ms step_avg:45.65ms
step:1267/2160 train_time:57850ms step_avg:45.66ms
step:1268/2160 train_time:57911ms step_avg:45.67ms
step:1269/2160 train_time:57975ms step_avg:45.69ms
step:1270/2160 train_time:58035ms step_avg:45.70ms
step:1271/2160 train_time:58098ms step_avg:45.71ms
step:1272/2160 train_time:58158ms step_avg:45.72ms
step:1273/2160 train_time:58219ms step_avg:45.73ms
step:1274/2160 train_time:58279ms step_avg:45.75ms
step:1275/2160 train_time:58341ms step_avg:45.76ms
step:1276/2160 train_time:58400ms step_avg:45.77ms
step:1277/2160 train_time:58461ms step_avg:45.78ms
step:1278/2160 train_time:58521ms step_avg:45.79ms
step:1279/2160 train_time:58582ms step_avg:45.80ms
step:1280/2160 train_time:58642ms step_avg:45.81ms
step:1281/2160 train_time:58704ms step_avg:45.83ms
step:1282/2160 train_time:58764ms step_avg:45.84ms
step:1283/2160 train_time:58827ms step_avg:45.85ms
step:1284/2160 train_time:58887ms step_avg:45.86ms
step:1285/2160 train_time:58950ms step_avg:45.88ms
step:1286/2160 train_time:59010ms step_avg:45.89ms
step:1287/2160 train_time:59071ms step_avg:45.90ms
step:1288/2160 train_time:59132ms step_avg:45.91ms
step:1289/2160 train_time:59193ms step_avg:45.92ms
step:1290/2160 train_time:59253ms step_avg:45.93ms
step:1291/2160 train_time:59313ms step_avg:45.94ms
step:1292/2160 train_time:59374ms step_avg:45.95ms
step:1293/2160 train_time:59435ms step_avg:45.97ms
step:1294/2160 train_time:59495ms step_avg:45.98ms
step:1295/2160 train_time:59555ms step_avg:45.99ms
step:1296/2160 train_time:59615ms step_avg:46.00ms
step:1297/2160 train_time:59677ms step_avg:46.01ms
step:1298/2160 train_time:59736ms step_avg:46.02ms
step:1299/2160 train_time:59798ms step_avg:46.03ms
step:1300/2160 train_time:59859ms step_avg:46.05ms
step:1301/2160 train_time:59922ms step_avg:46.06ms
step:1302/2160 train_time:59983ms step_avg:46.07ms
step:1303/2160 train_time:60045ms step_avg:46.08ms
step:1304/2160 train_time:60105ms step_avg:46.09ms
step:1305/2160 train_time:60168ms step_avg:46.11ms
step:1306/2160 train_time:60228ms step_avg:46.12ms
step:1307/2160 train_time:60289ms step_avg:46.13ms
step:1308/2160 train_time:60349ms step_avg:46.14ms
step:1309/2160 train_time:60410ms step_avg:46.15ms
step:1310/2160 train_time:60471ms step_avg:46.16ms
step:1311/2160 train_time:60532ms step_avg:46.17ms
step:1312/2160 train_time:60592ms step_avg:46.18ms
step:1313/2160 train_time:60652ms step_avg:46.19ms
step:1314/2160 train_time:60712ms step_avg:46.20ms
step:1315/2160 train_time:60774ms step_avg:46.22ms
step:1316/2160 train_time:60834ms step_avg:46.23ms
step:1317/2160 train_time:60896ms step_avg:46.24ms
step:1318/2160 train_time:60956ms step_avg:46.25ms
step:1319/2160 train_time:61018ms step_avg:46.26ms
step:1320/2160 train_time:61078ms step_avg:46.27ms
step:1321/2160 train_time:61141ms step_avg:46.28ms
step:1322/2160 train_time:61201ms step_avg:46.29ms
step:1323/2160 train_time:61264ms step_avg:46.31ms
step:1324/2160 train_time:61325ms step_avg:46.32ms
step:1325/2160 train_time:61387ms step_avg:46.33ms
step:1326/2160 train_time:61447ms step_avg:46.34ms
step:1327/2160 train_time:61508ms step_avg:46.35ms
step:1328/2160 train_time:61567ms step_avg:46.36ms
step:1329/2160 train_time:61628ms step_avg:46.37ms
step:1330/2160 train_time:61688ms step_avg:46.38ms
step:1331/2160 train_time:61749ms step_avg:46.39ms
step:1332/2160 train_time:61809ms step_avg:46.40ms
step:1333/2160 train_time:61870ms step_avg:46.41ms
step:1334/2160 train_time:61931ms step_avg:46.42ms
step:1335/2160 train_time:61992ms step_avg:46.44ms
step:1336/2160 train_time:62052ms step_avg:46.45ms
step:1337/2160 train_time:62115ms step_avg:46.46ms
step:1338/2160 train_time:62176ms step_avg:46.47ms
step:1339/2160 train_time:62237ms step_avg:46.48ms
step:1340/2160 train_time:62297ms step_avg:46.49ms
step:1341/2160 train_time:62358ms step_avg:46.50ms
step:1342/2160 train_time:62418ms step_avg:46.51ms
step:1343/2160 train_time:62480ms step_avg:46.52ms
step:1344/2160 train_time:62540ms step_avg:46.53ms
step:1345/2160 train_time:62603ms step_avg:46.54ms
step:1346/2160 train_time:62662ms step_avg:46.55ms
step:1347/2160 train_time:62725ms step_avg:46.57ms
step:1348/2160 train_time:62785ms step_avg:46.58ms
step:1349/2160 train_time:62847ms step_avg:46.59ms
step:1350/2160 train_time:62906ms step_avg:46.60ms
step:1351/2160 train_time:62968ms step_avg:46.61ms
step:1352/2160 train_time:63027ms step_avg:46.62ms
step:1353/2160 train_time:63088ms step_avg:46.63ms
step:1354/2160 train_time:63148ms step_avg:46.64ms
step:1355/2160 train_time:63209ms step_avg:46.65ms
step:1356/2160 train_time:63269ms step_avg:46.66ms
step:1357/2160 train_time:63331ms step_avg:46.67ms
step:1358/2160 train_time:63391ms step_avg:46.68ms
step:1359/2160 train_time:63453ms step_avg:46.69ms
step:1360/2160 train_time:63513ms step_avg:46.70ms
step:1361/2160 train_time:63574ms step_avg:46.71ms
step:1362/2160 train_time:63635ms step_avg:46.72ms
step:1363/2160 train_time:63696ms step_avg:46.73ms
step:1364/2160 train_time:63756ms step_avg:46.74ms
step:1365/2160 train_time:63817ms step_avg:46.75ms
step:1366/2160 train_time:63877ms step_avg:46.76ms
step:1367/2160 train_time:63939ms step_avg:46.77ms
step:1368/2160 train_time:63999ms step_avg:46.78ms
step:1369/2160 train_time:64061ms step_avg:46.79ms
step:1370/2160 train_time:64121ms step_avg:46.80ms
step:1371/2160 train_time:64183ms step_avg:46.81ms
step:1372/2160 train_time:64244ms step_avg:46.82ms
step:1373/2160 train_time:64306ms step_avg:46.84ms
step:1374/2160 train_time:64365ms step_avg:46.85ms
step:1375/2160 train_time:64428ms step_avg:46.86ms
step:1376/2160 train_time:64487ms step_avg:46.87ms
step:1377/2160 train_time:64549ms step_avg:46.88ms
step:1378/2160 train_time:64608ms step_avg:46.89ms
step:1379/2160 train_time:64669ms step_avg:46.90ms
step:1380/2160 train_time:64729ms step_avg:46.90ms
step:1381/2160 train_time:64790ms step_avg:46.92ms
step:1382/2160 train_time:64851ms step_avg:46.93ms
step:1383/2160 train_time:64912ms step_avg:46.94ms
step:1384/2160 train_time:64972ms step_avg:46.95ms
step:1385/2160 train_time:65033ms step_avg:46.96ms
step:1386/2160 train_time:65094ms step_avg:46.97ms
step:1387/2160 train_time:65156ms step_avg:46.98ms
step:1388/2160 train_time:65216ms step_avg:46.99ms
step:1389/2160 train_time:65278ms step_avg:47.00ms
step:1390/2160 train_time:65337ms step_avg:47.01ms
step:1391/2160 train_time:65401ms step_avg:47.02ms
step:1392/2160 train_time:65461ms step_avg:47.03ms
step:1393/2160 train_time:65523ms step_avg:47.04ms
step:1394/2160 train_time:65583ms step_avg:47.05ms
step:1395/2160 train_time:65646ms step_avg:47.06ms
step:1396/2160 train_time:65706ms step_avg:47.07ms
step:1397/2160 train_time:65767ms step_avg:47.08ms
step:1398/2160 train_time:65826ms step_avg:47.09ms
step:1399/2160 train_time:65887ms step_avg:47.10ms
step:1400/2160 train_time:65947ms step_avg:47.10ms
step:1401/2160 train_time:66009ms step_avg:47.12ms
step:1402/2160 train_time:66068ms step_avg:47.12ms
step:1403/2160 train_time:66130ms step_avg:47.13ms
step:1404/2160 train_time:66190ms step_avg:47.14ms
step:1405/2160 train_time:66251ms step_avg:47.15ms
step:1406/2160 train_time:66311ms step_avg:47.16ms
step:1407/2160 train_time:66373ms step_avg:47.17ms
step:1408/2160 train_time:66433ms step_avg:47.18ms
step:1409/2160 train_time:66494ms step_avg:47.19ms
step:1410/2160 train_time:66555ms step_avg:47.20ms
step:1411/2160 train_time:66616ms step_avg:47.21ms
step:1412/2160 train_time:66676ms step_avg:47.22ms
step:1413/2160 train_time:66737ms step_avg:47.23ms
step:1414/2160 train_time:66797ms step_avg:47.24ms
step:1415/2160 train_time:66859ms step_avg:47.25ms
step:1416/2160 train_time:66947ms step_avg:47.28ms
step:1417/2160 train_time:67037ms step_avg:47.31ms
step:1418/2160 train_time:67123ms step_avg:47.34ms
step:1419/2160 train_time:67213ms step_avg:47.37ms
step:1420/2160 train_time:67300ms step_avg:47.39ms
step:1421/2160 train_time:67390ms step_avg:47.42ms
step:1422/2160 train_time:67479ms step_avg:47.45ms
step:1423/2160 train_time:67568ms step_avg:47.48ms
step:1424/2160 train_time:67655ms step_avg:47.51ms
step:1425/2160 train_time:67743ms step_avg:47.54ms
step:1426/2160 train_time:67831ms step_avg:47.57ms
step:1427/2160 train_time:67922ms step_avg:47.60ms
step:1428/2160 train_time:68010ms step_avg:47.63ms
step:1429/2160 train_time:68100ms step_avg:47.66ms
step:1430/2160 train_time:68187ms step_avg:47.68ms
step:1431/2160 train_time:68278ms step_avg:47.71ms
step:1432/2160 train_time:68365ms step_avg:47.74ms
step:1433/2160 train_time:68455ms step_avg:47.77ms
step:1434/2160 train_time:68542ms step_avg:47.80ms
step:1435/2160 train_time:68631ms step_avg:47.83ms
step:1436/2160 train_time:68720ms step_avg:47.85ms
step:1437/2160 train_time:68809ms step_avg:47.88ms
step:1438/2160 train_time:68897ms step_avg:47.91ms
step:1439/2160 train_time:68985ms step_avg:47.94ms
step:1440/2160 train_time:69074ms step_avg:47.97ms
step:1441/2160 train_time:69163ms step_avg:48.00ms
step:1442/2160 train_time:69251ms step_avg:48.02ms
step:1443/2160 train_time:69340ms step_avg:48.05ms
step:1444/2160 train_time:69428ms step_avg:48.08ms
step:1445/2160 train_time:69518ms step_avg:48.11ms
step:1446/2160 train_time:69605ms step_avg:48.14ms
step:1447/2160 train_time:69695ms step_avg:48.17ms
step:1448/2160 train_time:69782ms step_avg:48.19ms
step:1449/2160 train_time:69872ms step_avg:48.22ms
step:1450/2160 train_time:69959ms step_avg:48.25ms
step:1451/2160 train_time:70048ms step_avg:48.28ms
step:1452/2160 train_time:70137ms step_avg:48.30ms
step:1453/2160 train_time:70226ms step_avg:48.33ms
step:1454/2160 train_time:70313ms step_avg:48.36ms
step:1455/2160 train_time:70403ms step_avg:48.39ms
step:1456/2160 train_time:70491ms step_avg:48.41ms
step:1457/2160 train_time:70581ms step_avg:48.44ms
step:1458/2160 train_time:70669ms step_avg:48.47ms
step:1459/2160 train_time:70760ms step_avg:48.50ms
step:1460/2160 train_time:70847ms step_avg:48.53ms
step:1461/2160 train_time:70937ms step_avg:48.55ms
step:1462/2160 train_time:71023ms step_avg:48.58ms
step:1463/2160 train_time:71113ms step_avg:48.61ms
step:1464/2160 train_time:71201ms step_avg:48.63ms
step:1465/2160 train_time:71290ms step_avg:48.66ms
step:1466/2160 train_time:71377ms step_avg:48.69ms
step:1467/2160 train_time:71467ms step_avg:48.72ms
step:1468/2160 train_time:71555ms step_avg:48.74ms
step:1469/2160 train_time:71644ms step_avg:48.77ms
step:1470/2160 train_time:71731ms step_avg:48.80ms
step:1471/2160 train_time:71821ms step_avg:48.82ms
step:1472/2160 train_time:71909ms step_avg:48.85ms
step:1473/2160 train_time:72001ms step_avg:48.88ms
step:1474/2160 train_time:72088ms step_avg:48.91ms
step:1475/2160 train_time:72179ms step_avg:48.93ms
step:1476/2160 train_time:72267ms step_avg:48.96ms
step:1477/2160 train_time:72356ms step_avg:48.99ms
step:1478/2160 train_time:72443ms step_avg:49.01ms
step:1479/2160 train_time:72532ms step_avg:49.04ms
step:1480/2160 train_time:72620ms step_avg:49.07ms
step:1481/2160 train_time:72709ms step_avg:49.09ms
step:1482/2160 train_time:72798ms step_avg:49.12ms
step:1483/2160 train_time:72887ms step_avg:49.15ms
step:1484/2160 train_time:72976ms step_avg:49.17ms
step:1485/2160 train_time:73064ms step_avg:49.20ms
step:1486/2160 train_time:73152ms step_avg:49.23ms
step:1487/2160 train_time:73242ms step_avg:49.25ms
step:1488/2160 train_time:73330ms step_avg:49.28ms
step:1489/2160 train_time:73420ms step_avg:49.31ms
step:1490/2160 train_time:73508ms step_avg:49.33ms
step:1491/2160 train_time:73597ms step_avg:49.36ms
step:1492/2160 train_time:73685ms step_avg:49.39ms
step:1493/2160 train_time:73774ms step_avg:49.41ms
step:1494/2160 train_time:73860ms step_avg:49.44ms
step:1495/2160 train_time:73950ms step_avg:49.46ms
step:1496/2160 train_time:74038ms step_avg:49.49ms
step:1497/2160 train_time:74127ms step_avg:49.52ms
step:1498/2160 train_time:74214ms step_avg:49.54ms
step:1499/2160 train_time:74302ms step_avg:49.57ms
step:1500/2160 train_time:74390ms step_avg:49.59ms
step:1500/2160 val_loss:3.4697 train_time:74482ms step_avg:49.65ms
step:1501/2160 train_time:74506ms step_avg:49.64ms
step:1502/2160 train_time:74574ms step_avg:49.65ms
step:1503/2160 train_time:74668ms step_avg:49.68ms
step:1504/2160 train_time:74757ms step_avg:49.71ms
step:1505/2160 train_time:74847ms step_avg:49.73ms
step:1506/2160 train_time:74933ms step_avg:49.76ms
step:1507/2160 train_time:75021ms step_avg:49.78ms
step:1508/2160 train_time:75107ms step_avg:49.81ms
step:1509/2160 train_time:75195ms step_avg:49.83ms
step:1510/2160 train_time:75282ms step_avg:49.86ms
step:1511/2160 train_time:75370ms step_avg:49.88ms
step:1512/2160 train_time:75459ms step_avg:49.91ms
step:1513/2160 train_time:75553ms step_avg:49.94ms
step:1514/2160 train_time:75644ms step_avg:49.96ms
step:1515/2160 train_time:75735ms step_avg:49.99ms
step:1516/2160 train_time:75822ms step_avg:50.01ms
step:1517/2160 train_time:75910ms step_avg:50.04ms
step:1518/2160 train_time:75997ms step_avg:50.06ms
step:1519/2160 train_time:76086ms step_avg:50.09ms
step:1520/2160 train_time:76171ms step_avg:50.11ms
step:1521/2160 train_time:76259ms step_avg:50.14ms
step:1522/2160 train_time:76346ms step_avg:50.16ms
step:1523/2160 train_time:76436ms step_avg:50.19ms
step:1524/2160 train_time:76526ms step_avg:50.21ms
step:1525/2160 train_time:76618ms step_avg:50.24ms
step:1526/2160 train_time:76706ms step_avg:50.27ms
step:1527/2160 train_time:76796ms step_avg:50.29ms
step:1528/2160 train_time:76884ms step_avg:50.32ms
step:1529/2160 train_time:76972ms step_avg:50.34ms
step:1530/2160 train_time:77059ms step_avg:50.37ms
step:1531/2160 train_time:77147ms step_avg:50.39ms
step:1532/2160 train_time:77234ms step_avg:50.41ms
step:1533/2160 train_time:77323ms step_avg:50.44ms
step:1534/2160 train_time:77410ms step_avg:50.46ms
step:1535/2160 train_time:77500ms step_avg:50.49ms
step:1536/2160 train_time:77590ms step_avg:50.51ms
step:1537/2160 train_time:77682ms step_avg:50.54ms
step:1538/2160 train_time:77770ms step_avg:50.57ms
step:1539/2160 train_time:77859ms step_avg:50.59ms
step:1540/2160 train_time:77947ms step_avg:50.62ms
step:1541/2160 train_time:78036ms step_avg:50.64ms
step:1542/2160 train_time:78123ms step_avg:50.66ms
step:1543/2160 train_time:78211ms step_avg:50.69ms
step:1544/2160 train_time:78298ms step_avg:50.71ms
step:1545/2160 train_time:78388ms step_avg:50.74ms
step:1546/2160 train_time:78475ms step_avg:50.76ms
step:1547/2160 train_time:78567ms step_avg:50.79ms
step:1548/2160 train_time:78656ms step_avg:50.81ms
step:1549/2160 train_time:78747ms step_avg:50.84ms
step:1550/2160 train_time:78835ms step_avg:50.86ms
step:1551/2160 train_time:78924ms step_avg:50.89ms
step:1552/2160 train_time:79011ms step_avg:50.91ms
step:1553/2160 train_time:79101ms step_avg:50.93ms
step:1554/2160 train_time:79186ms step_avg:50.96ms
step:1555/2160 train_time:79275ms step_avg:50.98ms
step:1556/2160 train_time:79363ms step_avg:51.00ms
step:1557/2160 train_time:79453ms step_avg:51.03ms
step:1558/2160 train_time:79542ms step_avg:51.05ms
step:1559/2160 train_time:79632ms step_avg:51.08ms
step:1560/2160 train_time:79720ms step_avg:51.10ms
step:1561/2160 train_time:79810ms step_avg:51.13ms
step:1562/2160 train_time:79897ms step_avg:51.15ms
step:1563/2160 train_time:79986ms step_avg:51.17ms
step:1564/2160 train_time:80073ms step_avg:51.20ms
step:1565/2160 train_time:80162ms step_avg:51.22ms
step:1566/2160 train_time:80249ms step_avg:51.24ms
step:1567/2160 train_time:80338ms step_avg:51.27ms
step:1568/2160 train_time:80426ms step_avg:51.29ms
step:1569/2160 train_time:80516ms step_avg:51.32ms
step:1570/2160 train_time:80604ms step_avg:51.34ms
step:1571/2160 train_time:80694ms step_avg:51.36ms
step:1572/2160 train_time:80783ms step_avg:51.39ms
step:1573/2160 train_time:80871ms step_avg:51.41ms
step:1574/2160 train_time:80959ms step_avg:51.43ms
step:1575/2160 train_time:81048ms step_avg:51.46ms
step:1576/2160 train_time:81136ms step_avg:51.48ms
step:1577/2160 train_time:81225ms step_avg:51.51ms
step:1578/2160 train_time:81311ms step_avg:51.53ms
step:1579/2160 train_time:81401ms step_avg:51.55ms
step:1580/2160 train_time:81487ms step_avg:51.57ms
step:1581/2160 train_time:81577ms step_avg:51.60ms
step:1582/2160 train_time:81664ms step_avg:51.62ms
step:1583/2160 train_time:81753ms step_avg:51.64ms
step:1584/2160 train_time:81842ms step_avg:51.67ms
step:1585/2160 train_time:81932ms step_avg:51.69ms
step:1586/2160 train_time:82019ms step_avg:51.71ms
step:1587/2160 train_time:82109ms step_avg:51.74ms
step:1588/2160 train_time:82196ms step_avg:51.76ms
step:1589/2160 train_time:82286ms step_avg:51.78ms
step:1590/2160 train_time:82373ms step_avg:51.81ms
step:1591/2160 train_time:82463ms step_avg:51.83ms
step:1592/2160 train_time:82550ms step_avg:51.85ms
step:1593/2160 train_time:82640ms step_avg:51.88ms
step:1594/2160 train_time:82728ms step_avg:51.90ms
step:1595/2160 train_time:82818ms step_avg:51.92ms
step:1596/2160 train_time:82906ms step_avg:51.95ms
step:1597/2160 train_time:82996ms step_avg:51.97ms
step:1598/2160 train_time:83084ms step_avg:51.99ms
step:1599/2160 train_time:83173ms step_avg:52.02ms
step:1600/2160 train_time:83261ms step_avg:52.04ms
step:1601/2160 train_time:83350ms step_avg:52.06ms
step:1602/2160 train_time:83437ms step_avg:52.08ms
step:1603/2160 train_time:83527ms step_avg:52.11ms
step:1604/2160 train_time:83614ms step_avg:52.13ms
step:1605/2160 train_time:83706ms step_avg:52.15ms
step:1606/2160 train_time:83793ms step_avg:52.18ms
step:1607/2160 train_time:83882ms step_avg:52.20ms
step:1608/2160 train_time:83969ms step_avg:52.22ms
step:1609/2160 train_time:84058ms step_avg:52.24ms
step:1610/2160 train_time:84145ms step_avg:52.26ms
step:1611/2160 train_time:84235ms step_avg:52.29ms
step:1612/2160 train_time:84321ms step_avg:52.31ms
step:1613/2160 train_time:84410ms step_avg:52.33ms
step:1614/2160 train_time:84498ms step_avg:52.35ms
step:1615/2160 train_time:84589ms step_avg:52.38ms
step:1616/2160 train_time:84677ms step_avg:52.40ms
step:1617/2160 train_time:84767ms step_avg:52.42ms
step:1618/2160 train_time:84855ms step_avg:52.44ms
step:1619/2160 train_time:84945ms step_avg:52.47ms
step:1620/2160 train_time:85032ms step_avg:52.49ms
step:1621/2160 train_time:85121ms step_avg:52.51ms
step:1622/2160 train_time:85208ms step_avg:52.53ms
step:1623/2160 train_time:85297ms step_avg:52.56ms
step:1624/2160 train_time:85384ms step_avg:52.58ms
step:1625/2160 train_time:85472ms step_avg:52.60ms
step:1626/2160 train_time:85560ms step_avg:52.62ms
step:1627/2160 train_time:85651ms step_avg:52.64ms
step:1628/2160 train_time:85739ms step_avg:52.67ms
step:1629/2160 train_time:85830ms step_avg:52.69ms
step:1630/2160 train_time:85919ms step_avg:52.71ms
step:1631/2160 train_time:86010ms step_avg:52.73ms
step:1632/2160 train_time:86098ms step_avg:52.76ms
step:1633/2160 train_time:86187ms step_avg:52.78ms
step:1634/2160 train_time:86274ms step_avg:52.80ms
step:1635/2160 train_time:86363ms step_avg:52.82ms
step:1636/2160 train_time:86450ms step_avg:52.84ms
step:1637/2160 train_time:86539ms step_avg:52.86ms
step:1638/2160 train_time:86627ms step_avg:52.89ms
step:1639/2160 train_time:86717ms step_avg:52.91ms
step:1640/2160 train_time:86805ms step_avg:52.93ms
step:1641/2160 train_time:86895ms step_avg:52.95ms
step:1642/2160 train_time:86983ms step_avg:52.97ms
step:1643/2160 train_time:87071ms step_avg:53.00ms
step:1644/2160 train_time:87159ms step_avg:53.02ms
step:1645/2160 train_time:87248ms step_avg:53.04ms
step:1646/2160 train_time:87335ms step_avg:53.06ms
step:1647/2160 train_time:87425ms step_avg:53.08ms
step:1648/2160 train_time:87512ms step_avg:53.10ms
step:1649/2160 train_time:87601ms step_avg:53.12ms
step:1650/2160 train_time:87688ms step_avg:53.14ms
step:1651/2160 train_time:87779ms step_avg:53.17ms
step:1652/2160 train_time:87868ms step_avg:53.19ms
step:1653/2160 train_time:87957ms step_avg:53.21ms
step:1654/2160 train_time:88046ms step_avg:53.23ms
step:1655/2160 train_time:88135ms step_avg:53.25ms
step:1656/2160 train_time:88222ms step_avg:53.27ms
step:1657/2160 train_time:88311ms step_avg:53.30ms
step:1658/2160 train_time:88398ms step_avg:53.32ms
step:1659/2160 train_time:88488ms step_avg:53.34ms
step:1660/2160 train_time:88575ms step_avg:53.36ms
step:1661/2160 train_time:88666ms step_avg:53.38ms
step:1662/2160 train_time:88753ms step_avg:53.40ms
step:1663/2160 train_time:88843ms step_avg:53.42ms
step:1664/2160 train_time:88930ms step_avg:53.44ms
step:1665/2160 train_time:89020ms step_avg:53.47ms
step:1666/2160 train_time:89108ms step_avg:53.49ms
step:1667/2160 train_time:89197ms step_avg:53.51ms
step:1668/2160 train_time:89284ms step_avg:53.53ms
step:1669/2160 train_time:89372ms step_avg:53.55ms
step:1670/2160 train_time:89460ms step_avg:53.57ms
step:1671/2160 train_time:89550ms step_avg:53.59ms
step:1672/2160 train_time:89637ms step_avg:53.61ms
step:1673/2160 train_time:89728ms step_avg:53.63ms
step:1674/2160 train_time:89815ms step_avg:53.65ms
step:1675/2160 train_time:89904ms step_avg:53.67ms
step:1676/2160 train_time:89991ms step_avg:53.69ms
step:1677/2160 train_time:90081ms step_avg:53.72ms
step:1678/2160 train_time:90167ms step_avg:53.74ms
step:1679/2160 train_time:90256ms step_avg:53.76ms
step:1680/2160 train_time:90346ms step_avg:53.78ms
step:1681/2160 train_time:90434ms step_avg:53.80ms
step:1682/2160 train_time:90522ms step_avg:53.82ms
step:1683/2160 train_time:90611ms step_avg:53.84ms
step:1684/2160 train_time:90700ms step_avg:53.86ms
step:1685/2160 train_time:90790ms step_avg:53.88ms
step:1686/2160 train_time:90877ms step_avg:53.90ms
step:1687/2160 train_time:90968ms step_avg:53.92ms
step:1688/2160 train_time:91055ms step_avg:53.94ms
step:1689/2160 train_time:91145ms step_avg:53.96ms
step:1690/2160 train_time:91232ms step_avg:53.98ms
step:1691/2160 train_time:91321ms step_avg:54.00ms
step:1692/2160 train_time:91408ms step_avg:54.02ms
step:1693/2160 train_time:91497ms step_avg:54.04ms
step:1694/2160 train_time:91584ms step_avg:54.06ms
step:1695/2160 train_time:91673ms step_avg:54.08ms
step:1696/2160 train_time:91761ms step_avg:54.10ms
step:1697/2160 train_time:91851ms step_avg:54.13ms
step:1698/2160 train_time:91939ms step_avg:54.15ms
step:1699/2160 train_time:92028ms step_avg:54.17ms
step:1700/2160 train_time:92116ms step_avg:54.19ms
step:1701/2160 train_time:92205ms step_avg:54.21ms
step:1702/2160 train_time:92292ms step_avg:54.23ms
step:1703/2160 train_time:92381ms step_avg:54.25ms
step:1704/2160 train_time:92468ms step_avg:54.27ms
step:1705/2160 train_time:92558ms step_avg:54.29ms
step:1706/2160 train_time:92645ms step_avg:54.31ms
step:1707/2160 train_time:92734ms step_avg:54.33ms
step:1708/2160 train_time:92821ms step_avg:54.35ms
step:1709/2160 train_time:92911ms step_avg:54.37ms
step:1710/2160 train_time:92999ms step_avg:54.39ms
step:1711/2160 train_time:93088ms step_avg:54.41ms
step:1712/2160 train_time:93176ms step_avg:54.43ms
step:1713/2160 train_time:93266ms step_avg:54.45ms
step:1714/2160 train_time:93352ms step_avg:54.46ms
step:1715/2160 train_time:93441ms step_avg:54.48ms
step:1716/2160 train_time:93528ms step_avg:54.50ms
step:1717/2160 train_time:93617ms step_avg:54.52ms
step:1718/2160 train_time:93705ms step_avg:54.54ms
step:1719/2160 train_time:93794ms step_avg:54.56ms
step:1720/2160 train_time:93882ms step_avg:54.58ms
step:1721/2160 train_time:93972ms step_avg:54.60ms
step:1722/2160 train_time:94059ms step_avg:54.62ms
step:1723/2160 train_time:94149ms step_avg:54.64ms
step:1724/2160 train_time:94235ms step_avg:54.66ms
step:1725/2160 train_time:94325ms step_avg:54.68ms
step:1726/2160 train_time:94412ms step_avg:54.70ms
step:1727/2160 train_time:94502ms step_avg:54.72ms
step:1728/2160 train_time:94589ms step_avg:54.74ms
step:1729/2160 train_time:94678ms step_avg:54.76ms
step:1730/2160 train_time:94765ms step_avg:54.78ms
step:1731/2160 train_time:94854ms step_avg:54.80ms
step:1732/2160 train_time:94942ms step_avg:54.82ms
step:1733/2160 train_time:95031ms step_avg:54.84ms
step:1734/2160 train_time:95119ms step_avg:54.86ms
step:1735/2160 train_time:95208ms step_avg:54.88ms
step:1736/2160 train_time:95296ms step_avg:54.89ms
step:1737/2160 train_time:95386ms step_avg:54.91ms
step:1738/2160 train_time:95473ms step_avg:54.93ms
step:1739/2160 train_time:95562ms step_avg:54.95ms
step:1740/2160 train_time:95649ms step_avg:54.97ms
step:1741/2160 train_time:95739ms step_avg:54.99ms
step:1742/2160 train_time:95827ms step_avg:55.01ms
step:1743/2160 train_time:95917ms step_avg:55.03ms
step:1744/2160 train_time:96006ms step_avg:55.05ms
step:1745/2160 train_time:96095ms step_avg:55.07ms
step:1746/2160 train_time:96182ms step_avg:55.09ms
step:1747/2160 train_time:96271ms step_avg:55.11ms
step:1748/2160 train_time:96358ms step_avg:55.12ms
step:1749/2160 train_time:96448ms step_avg:55.14ms
step:1750/2160 train_time:96535ms step_avg:55.16ms
step:1750/2160 val_loss:3.3788 train_time:96626ms step_avg:55.21ms
step:1751/2160 train_time:96650ms step_avg:55.20ms
step:1752/2160 train_time:96720ms step_avg:55.21ms
step:1753/2160 train_time:96816ms step_avg:55.23ms
step:1754/2160 train_time:96904ms step_avg:55.25ms
step:1755/2160 train_time:96992ms step_avg:55.27ms
step:1756/2160 train_time:97078ms step_avg:55.28ms
step:1757/2160 train_time:97166ms step_avg:55.30ms
step:1758/2160 train_time:97251ms step_avg:55.32ms
step:1759/2160 train_time:97339ms step_avg:55.34ms
step:1760/2160 train_time:97426ms step_avg:55.36ms
step:1761/2160 train_time:97515ms step_avg:55.37ms
step:1762/2160 train_time:97604ms step_avg:55.39ms
step:1763/2160 train_time:97698ms step_avg:55.42ms
step:1764/2160 train_time:97789ms step_avg:55.44ms
step:1765/2160 train_time:97881ms step_avg:55.46ms
step:1766/2160 train_time:97969ms step_avg:55.48ms
step:1767/2160 train_time:98058ms step_avg:55.49ms
step:1768/2160 train_time:98145ms step_avg:55.51ms
step:1769/2160 train_time:98232ms step_avg:55.53ms
step:1770/2160 train_time:98318ms step_avg:55.55ms
step:1771/2160 train_time:98406ms step_avg:55.57ms
step:1772/2160 train_time:98493ms step_avg:55.58ms
step:1773/2160 train_time:98582ms step_avg:55.60ms
step:1774/2160 train_time:98672ms step_avg:55.62ms
step:1775/2160 train_time:98764ms step_avg:55.64ms
step:1776/2160 train_time:98854ms step_avg:55.66ms
step:1777/2160 train_time:98942ms step_avg:55.68ms
step:1778/2160 train_time:99029ms step_avg:55.70ms
step:1779/2160 train_time:99118ms step_avg:55.72ms
step:1780/2160 train_time:99205ms step_avg:55.73ms
step:1781/2160 train_time:99294ms step_avg:55.75ms
step:1782/2160 train_time:99380ms step_avg:55.77ms
step:1783/2160 train_time:99468ms step_avg:55.79ms
step:1784/2160 train_time:99555ms step_avg:55.80ms
step:1785/2160 train_time:99644ms step_avg:55.82ms
step:1786/2160 train_time:99734ms step_avg:55.84ms
step:1787/2160 train_time:99824ms step_avg:55.86ms
step:1788/2160 train_time:99913ms step_avg:55.88ms
step:1789/2160 train_time:100002ms step_avg:55.90ms
step:1790/2160 train_time:100090ms step_avg:55.92ms
step:1791/2160 train_time:100179ms step_avg:55.93ms
step:1792/2160 train_time:100265ms step_avg:55.95ms
step:1793/2160 train_time:100353ms step_avg:55.97ms
step:1794/2160 train_time:100440ms step_avg:55.99ms
step:1795/2160 train_time:100528ms step_avg:56.00ms
step:1796/2160 train_time:100615ms step_avg:56.02ms
step:1797/2160 train_time:100705ms step_avg:56.04ms
step:1798/2160 train_time:100794ms step_avg:56.06ms
step:1799/2160 train_time:100884ms step_avg:56.08ms
step:1800/2160 train_time:100971ms step_avg:56.10ms
step:1801/2160 train_time:101061ms step_avg:56.11ms
step:1802/2160 train_time:101148ms step_avg:56.13ms
step:1803/2160 train_time:101237ms step_avg:56.15ms
step:1804/2160 train_time:101323ms step_avg:56.17ms
step:1805/2160 train_time:101412ms step_avg:56.18ms
step:1806/2160 train_time:101499ms step_avg:56.20ms
step:1807/2160 train_time:101588ms step_avg:56.22ms
step:1808/2160 train_time:101677ms step_avg:56.24ms
step:1809/2160 train_time:101766ms step_avg:56.26ms
step:1810/2160 train_time:101854ms step_avg:56.27ms
step:1811/2160 train_time:101943ms step_avg:56.29ms
step:1812/2160 train_time:102030ms step_avg:56.31ms
step:1813/2160 train_time:102119ms step_avg:56.33ms
step:1814/2160 train_time:102207ms step_avg:56.34ms
step:1815/2160 train_time:102297ms step_avg:56.36ms
step:1816/2160 train_time:102383ms step_avg:56.38ms
step:1817/2160 train_time:102471ms step_avg:56.40ms
step:1818/2160 train_time:102559ms step_avg:56.41ms
step:1819/2160 train_time:102648ms step_avg:56.43ms
step:1820/2160 train_time:102736ms step_avg:56.45ms
step:1821/2160 train_time:102825ms step_avg:56.47ms
step:1822/2160 train_time:102912ms step_avg:56.48ms
step:1823/2160 train_time:103001ms step_avg:56.50ms
step:1824/2160 train_time:103089ms step_avg:56.52ms
step:1825/2160 train_time:103179ms step_avg:56.54ms
step:1826/2160 train_time:103266ms step_avg:56.55ms
step:1827/2160 train_time:103354ms step_avg:56.57ms
step:1828/2160 train_time:103440ms step_avg:56.59ms
step:1829/2160 train_time:103530ms step_avg:56.60ms
step:1830/2160 train_time:103618ms step_avg:56.62ms
step:1831/2160 train_time:103707ms step_avg:56.64ms
step:1832/2160 train_time:103795ms step_avg:56.66ms
step:1833/2160 train_time:103884ms step_avg:56.67ms
step:1834/2160 train_time:103972ms step_avg:56.69ms
step:1835/2160 train_time:104061ms step_avg:56.71ms
step:1836/2160 train_time:104148ms step_avg:56.73ms
step:1837/2160 train_time:104239ms step_avg:56.74ms
step:1838/2160 train_time:104326ms step_avg:56.76ms
step:1839/2160 train_time:104416ms step_avg:56.78ms
step:1840/2160 train_time:104502ms step_avg:56.79ms
step:1841/2160 train_time:104591ms step_avg:56.81ms
step:1842/2160 train_time:104679ms step_avg:56.83ms
step:1843/2160 train_time:104769ms step_avg:56.85ms
step:1844/2160 train_time:104856ms step_avg:56.86ms
step:1845/2160 train_time:104945ms step_avg:56.88ms
step:1846/2160 train_time:105033ms step_avg:56.90ms
step:1847/2160 train_time:105122ms step_avg:56.92ms
step:1848/2160 train_time:105211ms step_avg:56.93ms
step:1849/2160 train_time:105300ms step_avg:56.95ms
step:1850/2160 train_time:105387ms step_avg:56.97ms
step:1851/2160 train_time:105477ms step_avg:56.98ms
step:1852/2160 train_time:105564ms step_avg:57.00ms
step:1853/2160 train_time:105654ms step_avg:57.02ms
step:1854/2160 train_time:105741ms step_avg:57.03ms
step:1855/2160 train_time:105830ms step_avg:57.05ms
step:1856/2160 train_time:105917ms step_avg:57.07ms
step:1857/2160 train_time:106006ms step_avg:57.08ms
step:1858/2160 train_time:106093ms step_avg:57.10ms
step:1859/2160 train_time:106183ms step_avg:57.12ms
step:1860/2160 train_time:106271ms step_avg:57.14ms
step:1861/2160 train_time:106361ms step_avg:57.15ms
step:1862/2160 train_time:106448ms step_avg:57.17ms
step:1863/2160 train_time:106538ms step_avg:57.19ms
step:1864/2160 train_time:106625ms step_avg:57.20ms
step:1865/2160 train_time:106714ms step_avg:57.22ms
step:1866/2160 train_time:106801ms step_avg:57.24ms
step:1867/2160 train_time:106890ms step_avg:57.25ms
step:1868/2160 train_time:106976ms step_avg:57.27ms
step:1869/2160 train_time:107065ms step_avg:57.28ms
step:1870/2160 train_time:107153ms step_avg:57.30ms
step:1871/2160 train_time:107242ms step_avg:57.32ms
step:1872/2160 train_time:107330ms step_avg:57.33ms
step:1873/2160 train_time:107420ms step_avg:57.35ms
step:1874/2160 train_time:107508ms step_avg:57.37ms
step:1875/2160 train_time:107598ms step_avg:57.39ms
step:1876/2160 train_time:107684ms step_avg:57.40ms
step:1877/2160 train_time:107774ms step_avg:57.42ms
step:1878/2160 train_time:107860ms step_avg:57.43ms
step:1879/2160 train_time:107949ms step_avg:57.45ms
step:1880/2160 train_time:108036ms step_avg:57.47ms
step:1881/2160 train_time:108125ms step_avg:57.48ms
step:1882/2160 train_time:108212ms step_avg:57.50ms
step:1883/2160 train_time:108301ms step_avg:57.52ms
step:1884/2160 train_time:108388ms step_avg:57.53ms
step:1885/2160 train_time:108479ms step_avg:57.55ms
step:1886/2160 train_time:108566ms step_avg:57.56ms
step:1887/2160 train_time:108655ms step_avg:57.58ms
step:1888/2160 train_time:108742ms step_avg:57.60ms
step:1889/2160 train_time:108831ms step_avg:57.61ms
step:1890/2160 train_time:108918ms step_avg:57.63ms
step:1891/2160 train_time:109008ms step_avg:57.65ms
step:1892/2160 train_time:109095ms step_avg:57.66ms
step:1893/2160 train_time:109183ms step_avg:57.68ms
step:1894/2160 train_time:109270ms step_avg:57.69ms
step:1895/2160 train_time:109360ms step_avg:57.71ms
step:1896/2160 train_time:109447ms step_avg:57.73ms
step:1897/2160 train_time:109537ms step_avg:57.74ms
step:1898/2160 train_time:109624ms step_avg:57.76ms
step:1899/2160 train_time:109712ms step_avg:57.77ms
step:1900/2160 train_time:109799ms step_avg:57.79ms
step:1901/2160 train_time:109888ms step_avg:57.81ms
step:1902/2160 train_time:109975ms step_avg:57.82ms
step:1903/2160 train_time:110064ms step_avg:57.84ms
step:1904/2160 train_time:110151ms step_avg:57.85ms
step:1905/2160 train_time:110240ms step_avg:57.87ms
step:1906/2160 train_time:110328ms step_avg:57.88ms
step:1907/2160 train_time:110417ms step_avg:57.90ms
step:1908/2160 train_time:110504ms step_avg:57.92ms
step:1909/2160 train_time:110593ms step_avg:57.93ms
step:1910/2160 train_time:110681ms step_avg:57.95ms
step:1911/2160 train_time:110770ms step_avg:57.96ms
step:1912/2160 train_time:110858ms step_avg:57.98ms
step:1913/2160 train_time:110947ms step_avg:58.00ms
step:1914/2160 train_time:111033ms step_avg:58.01ms
step:1915/2160 train_time:111122ms step_avg:58.03ms
step:1916/2160 train_time:111210ms step_avg:58.04ms
step:1917/2160 train_time:111300ms step_avg:58.06ms
step:1918/2160 train_time:111387ms step_avg:58.07ms
step:1919/2160 train_time:111476ms step_avg:58.09ms
step:1920/2160 train_time:111563ms step_avg:58.11ms
step:1921/2160 train_time:111652ms step_avg:58.12ms
step:1922/2160 train_time:111739ms step_avg:58.14ms
step:1923/2160 train_time:111828ms step_avg:58.15ms
step:1924/2160 train_time:111915ms step_avg:58.17ms
step:1925/2160 train_time:112004ms step_avg:58.18ms
step:1926/2160 train_time:112091ms step_avg:58.20ms
step:1927/2160 train_time:112181ms step_avg:58.22ms
step:1928/2160 train_time:112268ms step_avg:58.23ms
step:1929/2160 train_time:112358ms step_avg:58.25ms
step:1930/2160 train_time:112446ms step_avg:58.26ms
step:1931/2160 train_time:112536ms step_avg:58.28ms
step:1932/2160 train_time:112623ms step_avg:58.29ms
step:1933/2160 train_time:112712ms step_avg:58.31ms
step:1934/2160 train_time:112798ms step_avg:58.32ms
step:1935/2160 train_time:112886ms step_avg:58.34ms
step:1936/2160 train_time:112974ms step_avg:58.35ms
step:1937/2160 train_time:113064ms step_avg:58.37ms
step:1938/2160 train_time:113153ms step_avg:58.39ms
step:1939/2160 train_time:113242ms step_avg:58.40ms
step:1940/2160 train_time:113329ms step_avg:58.42ms
step:1941/2160 train_time:113418ms step_avg:58.43ms
step:1942/2160 train_time:113505ms step_avg:58.45ms
step:1943/2160 train_time:113594ms step_avg:58.46ms
step:1944/2160 train_time:113681ms step_avg:58.48ms
step:1945/2160 train_time:113770ms step_avg:58.49ms
step:1946/2160 train_time:113856ms step_avg:58.51ms
step:1947/2160 train_time:113945ms step_avg:58.52ms
step:1948/2160 train_time:114032ms step_avg:58.54ms
step:1949/2160 train_time:114121ms step_avg:58.55ms
step:1950/2160 train_time:114208ms step_avg:58.57ms
step:1951/2160 train_time:114297ms step_avg:58.58ms
step:1952/2160 train_time:114384ms step_avg:58.60ms
step:1953/2160 train_time:114473ms step_avg:58.61ms
step:1954/2160 train_time:114560ms step_avg:58.63ms
step:1955/2160 train_time:114649ms step_avg:58.64ms
step:1956/2160 train_time:114738ms step_avg:58.66ms
step:1957/2160 train_time:114827ms step_avg:58.67ms
step:1958/2160 train_time:114913ms step_avg:58.69ms
step:1959/2160 train_time:115002ms step_avg:58.70ms
step:1960/2160 train_time:115089ms step_avg:58.72ms
step:1961/2160 train_time:115178ms step_avg:58.73ms
step:1962/2160 train_time:115266ms step_avg:58.75ms
step:1963/2160 train_time:115354ms step_avg:58.76ms
step:1964/2160 train_time:115441ms step_avg:58.78ms
step:1965/2160 train_time:115530ms step_avg:58.79ms
step:1966/2160 train_time:115618ms step_avg:58.81ms
step:1967/2160 train_time:115707ms step_avg:58.82ms
step:1968/2160 train_time:115795ms step_avg:58.84ms
step:1969/2160 train_time:115883ms step_avg:58.85ms
step:1970/2160 train_time:115970ms step_avg:58.87ms
step:1971/2160 train_time:116059ms step_avg:58.88ms
step:1972/2160 train_time:116146ms step_avg:58.90ms
step:1973/2160 train_time:116235ms step_avg:58.91ms
step:1974/2160 train_time:116321ms step_avg:58.93ms
step:1975/2160 train_time:116411ms step_avg:58.94ms
step:1976/2160 train_time:116498ms step_avg:58.96ms
step:1977/2160 train_time:116587ms step_avg:58.97ms
step:1978/2160 train_time:116674ms step_avg:58.99ms
step:1979/2160 train_time:116763ms step_avg:59.00ms
step:1980/2160 train_time:116850ms step_avg:59.02ms
step:1981/2160 train_time:116939ms step_avg:59.03ms
step:1982/2160 train_time:117026ms step_avg:59.04ms
step:1983/2160 train_time:117115ms step_avg:59.06ms
step:1984/2160 train_time:117202ms step_avg:59.07ms
step:1985/2160 train_time:117291ms step_avg:59.09ms
step:1986/2160 train_time:117380ms step_avg:59.10ms
step:1987/2160 train_time:117470ms step_avg:59.12ms
step:1988/2160 train_time:117558ms step_avg:59.13ms
step:1989/2160 train_time:117647ms step_avg:59.15ms
step:1990/2160 train_time:117735ms step_avg:59.16ms
step:1991/2160 train_time:117823ms step_avg:59.18ms
step:1992/2160 train_time:117911ms step_avg:59.19ms
step:1993/2160 train_time:118001ms step_avg:59.21ms
step:1994/2160 train_time:118088ms step_avg:59.22ms
step:1995/2160 train_time:118177ms step_avg:59.24ms
step:1996/2160 train_time:118264ms step_avg:59.25ms
step:1997/2160 train_time:118353ms step_avg:59.27ms
step:1998/2160 train_time:118440ms step_avg:59.28ms
step:1999/2160 train_time:118530ms step_avg:59.29ms
step:2000/2160 train_time:118618ms step_avg:59.31ms
step:2000/2160 val_loss:3.3101 train_time:118707ms step_avg:59.35ms
step:2001/2160 train_time:118730ms step_avg:59.34ms
step:2002/2160 train_time:118799ms step_avg:59.34ms
step:2003/2160 train_time:118892ms step_avg:59.36ms
step:2004/2160 train_time:118979ms step_avg:59.37ms
step:2005/2160 train_time:119068ms step_avg:59.39ms
step:2006/2160 train_time:119154ms step_avg:59.40ms
step:2007/2160 train_time:119241ms step_avg:59.41ms
step:2008/2160 train_time:119326ms step_avg:59.43ms
step:2009/2160 train_time:119414ms step_avg:59.44ms
step:2010/2160 train_time:119501ms step_avg:59.45ms
step:2011/2160 train_time:119589ms step_avg:59.47ms
step:2012/2160 train_time:119679ms step_avg:59.48ms
step:2013/2160 train_time:119772ms step_avg:59.50ms
step:2014/2160 train_time:119861ms step_avg:59.51ms
step:2015/2160 train_time:119950ms step_avg:59.53ms
step:2016/2160 train_time:120037ms step_avg:59.54ms
step:2017/2160 train_time:120125ms step_avg:59.56ms
step:2018/2160 train_time:120212ms step_avg:59.57ms
step:2019/2160 train_time:120299ms step_avg:59.58ms
step:2020/2160 train_time:120385ms step_avg:59.60ms
step:2021/2160 train_time:120474ms step_avg:59.61ms
step:2022/2160 train_time:120560ms step_avg:59.62ms
step:2023/2160 train_time:120650ms step_avg:59.64ms
step:2024/2160 train_time:120738ms step_avg:59.65ms
step:2025/2160 train_time:120828ms step_avg:59.67ms
step:2026/2160 train_time:120916ms step_avg:59.68ms
step:2027/2160 train_time:121004ms step_avg:59.70ms
step:2028/2160 train_time:121090ms step_avg:59.71ms
step:2029/2160 train_time:121179ms step_avg:59.72ms
step:2030/2160 train_time:121265ms step_avg:59.74ms
step:2031/2160 train_time:121354ms step_avg:59.75ms
step:2032/2160 train_time:121441ms step_avg:59.76ms
step:2033/2160 train_time:121529ms step_avg:59.78ms
step:2034/2160 train_time:121616ms step_avg:59.79ms
step:2035/2160 train_time:121707ms step_avg:59.81ms
step:2036/2160 train_time:121795ms step_avg:59.82ms
step:2037/2160 train_time:121885ms step_avg:59.84ms
step:2038/2160 train_time:121973ms step_avg:59.85ms
step:2039/2160 train_time:122062ms step_avg:59.86ms
step:2040/2160 train_time:122149ms step_avg:59.88ms
step:2041/2160 train_time:122238ms step_avg:59.89ms
step:2042/2160 train_time:122324ms step_avg:59.90ms
step:2043/2160 train_time:122414ms step_avg:59.92ms
step:2044/2160 train_time:122499ms step_avg:59.93ms
step:2045/2160 train_time:122587ms step_avg:59.94ms
step:2046/2160 train_time:122676ms step_avg:59.96ms
step:2047/2160 train_time:122765ms step_avg:59.97ms
step:2048/2160 train_time:122854ms step_avg:59.99ms
step:2049/2160 train_time:122944ms step_avg:60.00ms
step:2050/2160 train_time:123031ms step_avg:60.02ms
step:2051/2160 train_time:123120ms step_avg:60.03ms
step:2052/2160 train_time:123207ms step_avg:60.04ms
step:2053/2160 train_time:123295ms step_avg:60.06ms
step:2054/2160 train_time:123381ms step_avg:60.07ms
step:2055/2160 train_time:123470ms step_avg:60.08ms
step:2056/2160 train_time:123556ms step_avg:60.10ms
step:2057/2160 train_time:123646ms step_avg:60.11ms
step:2058/2160 train_time:123733ms step_avg:60.12ms
step:2059/2160 train_time:123822ms step_avg:60.14ms
step:2060/2160 train_time:123911ms step_avg:60.15ms
step:2061/2160 train_time:124000ms step_avg:60.17ms
step:2062/2160 train_time:124087ms step_avg:60.18ms
step:2063/2160 train_time:124175ms step_avg:60.19ms
step:2064/2160 train_time:124262ms step_avg:60.20ms
step:2065/2160 train_time:124351ms step_avg:60.22ms
step:2066/2160 train_time:124437ms step_avg:60.23ms
step:2067/2160 train_time:124526ms step_avg:60.24ms
step:2068/2160 train_time:124614ms step_avg:60.26ms
step:2069/2160 train_time:124703ms step_avg:60.27ms
step:2070/2160 train_time:124791ms step_avg:60.29ms
step:2071/2160 train_time:124880ms step_avg:60.30ms
step:2072/2160 train_time:124968ms step_avg:60.31ms
step:2073/2160 train_time:125056ms step_avg:60.33ms
step:2074/2160 train_time:125143ms step_avg:60.34ms
step:2075/2160 train_time:125231ms step_avg:60.35ms
step:2076/2160 train_time:125318ms step_avg:60.37ms
step:2077/2160 train_time:125407ms step_avg:60.38ms
step:2078/2160 train_time:125494ms step_avg:60.39ms
step:2079/2160 train_time:125583ms step_avg:60.41ms
step:2080/2160 train_time:125670ms step_avg:60.42ms
step:2081/2160 train_time:125760ms step_avg:60.43ms
step:2082/2160 train_time:125847ms step_avg:60.45ms
step:2083/2160 train_time:125937ms step_avg:60.46ms
step:2084/2160 train_time:126024ms step_avg:60.47ms
step:2085/2160 train_time:126114ms step_avg:60.49ms
step:2086/2160 train_time:126200ms step_avg:60.50ms
step:2087/2160 train_time:126288ms step_avg:60.51ms
step:2088/2160 train_time:126376ms step_avg:60.52ms
step:2089/2160 train_time:126464ms step_avg:60.54ms
step:2090/2160 train_time:126551ms step_avg:60.55ms
step:2091/2160 train_time:126641ms step_avg:60.56ms
step:2092/2160 train_time:126728ms step_avg:60.58ms
step:2093/2160 train_time:126817ms step_avg:60.59ms
step:2094/2160 train_time:126904ms step_avg:60.60ms
step:2095/2160 train_time:126994ms step_avg:60.62ms
step:2096/2160 train_time:127082ms step_avg:60.63ms
step:2097/2160 train_time:127170ms step_avg:60.64ms
step:2098/2160 train_time:127256ms step_avg:60.66ms
step:2099/2160 train_time:127346ms step_avg:60.67ms
step:2100/2160 train_time:127432ms step_avg:60.68ms
step:2101/2160 train_time:127521ms step_avg:60.70ms
step:2102/2160 train_time:127609ms step_avg:60.71ms
step:2103/2160 train_time:127698ms step_avg:60.72ms
step:2104/2160 train_time:127786ms step_avg:60.73ms
step:2105/2160 train_time:127875ms step_avg:60.75ms
step:2106/2160 train_time:127962ms step_avg:60.76ms
step:2107/2160 train_time:128051ms step_avg:60.77ms
step:2108/2160 train_time:128139ms step_avg:60.79ms
step:2109/2160 train_time:128227ms step_avg:60.80ms
step:2110/2160 train_time:128314ms step_avg:60.81ms
step:2111/2160 train_time:128402ms step_avg:60.83ms
step:2112/2160 train_time:128489ms step_avg:60.84ms
step:2113/2160 train_time:128578ms step_avg:60.85ms
step:2114/2160 train_time:128665ms step_avg:60.86ms
step:2115/2160 train_time:128755ms step_avg:60.88ms
step:2116/2160 train_time:128842ms step_avg:60.89ms
step:2117/2160 train_time:128931ms step_avg:60.90ms
step:2118/2160 train_time:129018ms step_avg:60.91ms
step:2119/2160 train_time:129107ms step_avg:60.93ms
step:2120/2160 train_time:129194ms step_avg:60.94ms
step:2121/2160 train_time:129282ms step_avg:60.95ms
step:2122/2160 train_time:129370ms step_avg:60.97ms
step:2123/2160 train_time:129459ms step_avg:60.98ms
step:2124/2160 train_time:129547ms step_avg:60.99ms
step:2125/2160 train_time:129636ms step_avg:61.01ms
step:2126/2160 train_time:129724ms step_avg:61.02ms
step:2127/2160 train_time:129814ms step_avg:61.03ms
step:2128/2160 train_time:129901ms step_avg:61.04ms
step:2129/2160 train_time:129991ms step_avg:61.06ms
step:2130/2160 train_time:130077ms step_avg:61.07ms
step:2131/2160 train_time:130167ms step_avg:61.08ms
step:2132/2160 train_time:130254ms step_avg:61.09ms
step:2133/2160 train_time:130343ms step_avg:61.11ms
step:2134/2160 train_time:130431ms step_avg:61.12ms
step:2135/2160 train_time:130520ms step_avg:61.13ms
step:2136/2160 train_time:130606ms step_avg:61.15ms
step:2137/2160 train_time:130695ms step_avg:61.16ms
step:2138/2160 train_time:130782ms step_avg:61.17ms
step:2139/2160 train_time:130873ms step_avg:61.18ms
step:2140/2160 train_time:130960ms step_avg:61.20ms
step:2141/2160 train_time:131050ms step_avg:61.21ms
step:2142/2160 train_time:131136ms step_avg:61.22ms
step:2143/2160 train_time:131225ms step_avg:61.23ms
step:2144/2160 train_time:131313ms step_avg:61.25ms
step:2145/2160 train_time:131403ms step_avg:61.26ms
step:2146/2160 train_time:131491ms step_avg:61.27ms
step:2147/2160 train_time:131580ms step_avg:61.29ms
step:2148/2160 train_time:131668ms step_avg:61.30ms
step:2149/2160 train_time:131757ms step_avg:61.31ms
step:2150/2160 train_time:131844ms step_avg:61.32ms
step:2151/2160 train_time:131935ms step_avg:61.34ms
step:2152/2160 train_time:132022ms step_avg:61.35ms
step:2153/2160 train_time:132111ms step_avg:61.36ms
step:2154/2160 train_time:132198ms step_avg:61.37ms
step:2155/2160 train_time:132288ms step_avg:61.39ms
step:2156/2160 train_time:132375ms step_avg:61.40ms
step:2157/2160 train_time:132464ms step_avg:61.41ms
step:2158/2160 train_time:132551ms step_avg:61.42ms
step:2159/2160 train_time:132640ms step_avg:61.44ms
step:2160/2160 train_time:132728ms step_avg:61.45ms
step:2160/2160 val_loss:3.2781 train_time:132817ms step_avg:61.49ms
peak memory allocated: 30078 MiB reserved: 44936 MiB

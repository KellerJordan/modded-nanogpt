import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:48:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   36C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              75      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A              76      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              77      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              78      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              79      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              80      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A              76      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A              77      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A              78      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A              79      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A              80      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A              81      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A              82      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:94ms step_avg:93.65ms
step:2/2160 train_time:120ms step_avg:60.15ms
step:3/2160 train_time:153ms step_avg:51.12ms
step:4/2160 train_time:187ms step_avg:46.70ms
step:5/2160 train_time:220ms step_avg:44.09ms
step:6/2160 train_time:342ms step_avg:56.93ms
step:7/2160 train_time:362ms step_avg:51.76ms
step:8/2160 train_time:394ms step_avg:49.23ms
step:9/2160 train_time:427ms step_avg:47.41ms
step:10/2160 train_time:459ms step_avg:45.88ms
step:11/2160 train_time:492ms step_avg:44.75ms
step:12/2160 train_time:525ms step_avg:43.72ms
step:13/2160 train_time:558ms step_avg:42.92ms
step:14/2160 train_time:590ms step_avg:42.16ms
step:15/2160 train_time:624ms step_avg:41.59ms
step:16/2160 train_time:656ms step_avg:41.01ms
step:17/2160 train_time:689ms step_avg:40.56ms
step:18/2160 train_time:722ms step_avg:40.10ms
step:19/2160 train_time:755ms step_avg:39.73ms
step:20/2160 train_time:787ms step_avg:39.37ms
step:21/2160 train_time:820ms step_avg:39.07ms
step:22/2160 train_time:853ms step_avg:38.77ms
step:23/2160 train_time:886ms step_avg:38.52ms
step:24/2160 train_time:919ms step_avg:38.27ms
step:25/2160 train_time:952ms step_avg:38.07ms
step:26/2160 train_time:984ms step_avg:37.85ms
step:27/2160 train_time:1017ms step_avg:37.68ms
step:28/2160 train_time:1050ms step_avg:37.51ms
step:29/2160 train_time:1083ms step_avg:37.34ms
step:30/2160 train_time:1115ms step_avg:37.18ms
step:31/2160 train_time:1149ms step_avg:37.06ms
step:32/2160 train_time:1181ms step_avg:36.91ms
step:33/2160 train_time:1215ms step_avg:36.80ms
step:34/2160 train_time:1249ms step_avg:36.73ms
step:35/2160 train_time:1285ms step_avg:36.71ms
step:36/2160 train_time:1318ms step_avg:36.61ms
step:37/2160 train_time:1353ms step_avg:36.57ms
step:38/2160 train_time:1386ms step_avg:36.47ms
step:39/2160 train_time:1419ms step_avg:36.39ms
step:40/2160 train_time:1452ms step_avg:36.30ms
step:41/2160 train_time:1486ms step_avg:36.23ms
step:42/2160 train_time:1518ms step_avg:36.15ms
step:43/2160 train_time:1552ms step_avg:36.09ms
step:44/2160 train_time:1585ms step_avg:36.01ms
step:45/2160 train_time:1618ms step_avg:35.95ms
step:46/2160 train_time:1650ms step_avg:35.88ms
step:47/2160 train_time:1684ms step_avg:35.82ms
step:48/2160 train_time:1716ms step_avg:35.75ms
step:49/2160 train_time:1749ms step_avg:35.70ms
step:50/2160 train_time:1781ms step_avg:35.63ms
step:51/2160 train_time:1815ms step_avg:35.59ms
step:52/2160 train_time:1847ms step_avg:35.52ms
step:53/2160 train_time:1881ms step_avg:35.49ms
step:54/2160 train_time:1913ms step_avg:35.43ms
step:55/2160 train_time:1947ms step_avg:35.39ms
step:56/2160 train_time:1979ms step_avg:35.34ms
step:57/2160 train_time:2012ms step_avg:35.30ms
step:58/2160 train_time:2045ms step_avg:35.25ms
step:59/2160 train_time:2078ms step_avg:35.22ms
step:60/2160 train_time:2110ms step_avg:35.17ms
step:61/2160 train_time:2144ms step_avg:35.14ms
step:62/2160 train_time:2176ms step_avg:35.09ms
step:63/2160 train_time:2210ms step_avg:35.07ms
step:64/2160 train_time:2242ms step_avg:35.03ms
step:65/2160 train_time:2276ms step_avg:35.01ms
step:66/2160 train_time:2308ms step_avg:34.98ms
step:67/2160 train_time:2343ms step_avg:34.97ms
step:68/2160 train_time:2375ms step_avg:34.93ms
step:69/2160 train_time:2409ms step_avg:34.92ms
step:70/2160 train_time:2442ms step_avg:34.88ms
step:71/2160 train_time:2476ms step_avg:34.87ms
step:72/2160 train_time:2508ms step_avg:34.84ms
step:73/2160 train_time:2542ms step_avg:34.82ms
step:74/2160 train_time:2574ms step_avg:34.79ms
step:75/2160 train_time:2608ms step_avg:34.77ms
step:76/2160 train_time:2641ms step_avg:34.74ms
step:77/2160 train_time:2673ms step_avg:34.72ms
step:78/2160 train_time:2706ms step_avg:34.69ms
step:79/2160 train_time:2739ms step_avg:34.67ms
step:80/2160 train_time:2771ms step_avg:34.64ms
step:81/2160 train_time:2805ms step_avg:34.63ms
step:82/2160 train_time:2838ms step_avg:34.61ms
step:83/2160 train_time:2871ms step_avg:34.59ms
step:84/2160 train_time:2903ms step_avg:34.56ms
step:85/2160 train_time:2936ms step_avg:34.55ms
step:86/2160 train_time:2969ms step_avg:34.53ms
step:87/2160 train_time:3003ms step_avg:34.51ms
step:88/2160 train_time:3035ms step_avg:34.49ms
step:89/2160 train_time:3068ms step_avg:34.47ms
step:90/2160 train_time:3100ms step_avg:34.45ms
step:91/2160 train_time:3134ms step_avg:34.43ms
step:92/2160 train_time:3166ms step_avg:34.41ms
step:93/2160 train_time:3199ms step_avg:34.40ms
step:94/2160 train_time:3232ms step_avg:34.38ms
step:95/2160 train_time:3265ms step_avg:34.37ms
step:96/2160 train_time:3297ms step_avg:34.35ms
step:97/2160 train_time:3331ms step_avg:34.34ms
step:98/2160 train_time:3364ms step_avg:34.32ms
step:99/2160 train_time:3397ms step_avg:34.32ms
step:100/2160 train_time:3430ms step_avg:34.30ms
step:101/2160 train_time:3463ms step_avg:34.29ms
step:102/2160 train_time:3496ms step_avg:34.27ms
step:103/2160 train_time:3529ms step_avg:34.27ms
step:104/2160 train_time:3562ms step_avg:34.25ms
step:105/2160 train_time:3595ms step_avg:34.24ms
step:106/2160 train_time:3628ms step_avg:34.22ms
step:107/2160 train_time:3661ms step_avg:34.21ms
step:108/2160 train_time:3693ms step_avg:34.20ms
step:109/2160 train_time:3727ms step_avg:34.19ms
step:110/2160 train_time:3759ms step_avg:34.17ms
step:111/2160 train_time:3792ms step_avg:34.17ms
step:112/2160 train_time:3825ms step_avg:34.15ms
step:113/2160 train_time:3858ms step_avg:34.14ms
step:114/2160 train_time:3890ms step_avg:34.12ms
step:115/2160 train_time:3923ms step_avg:34.12ms
step:116/2160 train_time:3956ms step_avg:34.10ms
step:117/2160 train_time:3989ms step_avg:34.10ms
step:118/2160 train_time:4022ms step_avg:34.08ms
step:119/2160 train_time:4055ms step_avg:34.08ms
step:120/2160 train_time:4088ms step_avg:34.06ms
step:121/2160 train_time:4121ms step_avg:34.06ms
step:122/2160 train_time:4153ms step_avg:34.04ms
step:123/2160 train_time:4186ms step_avg:34.04ms
step:124/2160 train_time:4219ms step_avg:34.02ms
step:125/2160 train_time:4252ms step_avg:34.02ms
step:126/2160 train_time:4284ms step_avg:34.00ms
step:127/2160 train_time:4317ms step_avg:34.00ms
step:128/2160 train_time:4350ms step_avg:33.98ms
step:129/2160 train_time:4383ms step_avg:33.98ms
step:130/2160 train_time:4416ms step_avg:33.97ms
step:131/2160 train_time:4449ms step_avg:33.96ms
step:132/2160 train_time:4482ms step_avg:33.95ms
step:133/2160 train_time:4515ms step_avg:33.94ms
step:134/2160 train_time:4547ms step_avg:33.93ms
step:135/2160 train_time:4580ms step_avg:33.93ms
step:136/2160 train_time:4613ms step_avg:33.92ms
step:137/2160 train_time:4646ms step_avg:33.91ms
step:138/2160 train_time:4678ms step_avg:33.90ms
step:139/2160 train_time:4711ms step_avg:33.89ms
step:140/2160 train_time:4744ms step_avg:33.88ms
step:141/2160 train_time:4777ms step_avg:33.88ms
step:142/2160 train_time:4809ms step_avg:33.87ms
step:143/2160 train_time:4842ms step_avg:33.86ms
step:144/2160 train_time:4875ms step_avg:33.85ms
step:145/2160 train_time:4908ms step_avg:33.85ms
step:146/2160 train_time:4940ms step_avg:33.83ms
step:147/2160 train_time:4973ms step_avg:33.83ms
step:148/2160 train_time:5005ms step_avg:33.82ms
step:149/2160 train_time:5038ms step_avg:33.81ms
step:150/2160 train_time:5071ms step_avg:33.81ms
step:151/2160 train_time:5104ms step_avg:33.80ms
step:152/2160 train_time:5137ms step_avg:33.80ms
step:153/2160 train_time:5170ms step_avg:33.79ms
step:154/2160 train_time:5202ms step_avg:33.78ms
step:155/2160 train_time:5236ms step_avg:33.78ms
step:156/2160 train_time:5268ms step_avg:33.77ms
step:157/2160 train_time:5301ms step_avg:33.77ms
step:158/2160 train_time:5334ms step_avg:33.76ms
step:159/2160 train_time:5367ms step_avg:33.76ms
step:160/2160 train_time:5400ms step_avg:33.75ms
step:161/2160 train_time:5433ms step_avg:33.74ms
step:162/2160 train_time:5465ms step_avg:33.73ms
step:163/2160 train_time:5498ms step_avg:33.73ms
step:164/2160 train_time:5531ms step_avg:33.72ms
step:165/2160 train_time:5564ms step_avg:33.72ms
step:166/2160 train_time:5597ms step_avg:33.71ms
step:167/2160 train_time:5630ms step_avg:33.71ms
step:168/2160 train_time:5662ms step_avg:33.70ms
step:169/2160 train_time:5696ms step_avg:33.70ms
step:170/2160 train_time:5728ms step_avg:33.69ms
step:171/2160 train_time:5761ms step_avg:33.69ms
step:172/2160 train_time:5793ms step_avg:33.68ms
step:173/2160 train_time:5826ms step_avg:33.68ms
step:174/2160 train_time:5859ms step_avg:33.67ms
step:175/2160 train_time:5892ms step_avg:33.67ms
step:176/2160 train_time:5924ms step_avg:33.66ms
step:177/2160 train_time:5957ms step_avg:33.66ms
step:178/2160 train_time:5989ms step_avg:33.65ms
step:179/2160 train_time:6023ms step_avg:33.65ms
step:180/2160 train_time:6055ms step_avg:33.64ms
step:181/2160 train_time:6089ms step_avg:33.64ms
step:182/2160 train_time:6121ms step_avg:33.63ms
step:183/2160 train_time:6154ms step_avg:33.63ms
step:184/2160 train_time:6187ms step_avg:33.62ms
step:185/2160 train_time:6220ms step_avg:33.62ms
step:186/2160 train_time:6252ms step_avg:33.62ms
step:187/2160 train_time:6286ms step_avg:33.61ms
step:188/2160 train_time:6318ms step_avg:33.61ms
step:189/2160 train_time:6351ms step_avg:33.60ms
step:190/2160 train_time:6384ms step_avg:33.60ms
step:191/2160 train_time:6417ms step_avg:33.60ms
step:192/2160 train_time:6449ms step_avg:33.59ms
step:193/2160 train_time:6482ms step_avg:33.59ms
step:194/2160 train_time:6515ms step_avg:33.58ms
step:195/2160 train_time:6548ms step_avg:33.58ms
step:196/2160 train_time:6580ms step_avg:33.57ms
step:197/2160 train_time:6614ms step_avg:33.57ms
step:198/2160 train_time:6646ms step_avg:33.56ms
step:199/2160 train_time:6679ms step_avg:33.56ms
step:200/2160 train_time:6711ms step_avg:33.56ms
step:201/2160 train_time:6745ms step_avg:33.56ms
step:202/2160 train_time:6777ms step_avg:33.55ms
step:203/2160 train_time:6810ms step_avg:33.55ms
step:204/2160 train_time:6842ms step_avg:33.54ms
step:205/2160 train_time:6876ms step_avg:33.54ms
step:206/2160 train_time:6908ms step_avg:33.54ms
step:207/2160 train_time:6941ms step_avg:33.53ms
step:208/2160 train_time:6974ms step_avg:33.53ms
step:209/2160 train_time:7007ms step_avg:33.53ms
step:210/2160 train_time:7039ms step_avg:33.52ms
step:211/2160 train_time:7073ms step_avg:33.52ms
step:212/2160 train_time:7105ms step_avg:33.51ms
step:213/2160 train_time:7138ms step_avg:33.51ms
step:214/2160 train_time:7170ms step_avg:33.50ms
step:215/2160 train_time:7203ms step_avg:33.50ms
step:216/2160 train_time:7236ms step_avg:33.50ms
step:217/2160 train_time:7269ms step_avg:33.50ms
step:218/2160 train_time:7302ms step_avg:33.49ms
step:219/2160 train_time:7335ms step_avg:33.49ms
step:220/2160 train_time:7367ms step_avg:33.49ms
step:221/2160 train_time:7400ms step_avg:33.48ms
step:222/2160 train_time:7433ms step_avg:33.48ms
step:223/2160 train_time:7466ms step_avg:33.48ms
step:224/2160 train_time:7498ms step_avg:33.47ms
step:225/2160 train_time:7531ms step_avg:33.47ms
step:226/2160 train_time:7563ms step_avg:33.47ms
step:227/2160 train_time:7596ms step_avg:33.46ms
step:228/2160 train_time:7629ms step_avg:33.46ms
step:229/2160 train_time:7662ms step_avg:33.46ms
step:230/2160 train_time:7694ms step_avg:33.45ms
step:231/2160 train_time:7728ms step_avg:33.45ms
step:232/2160 train_time:7760ms step_avg:33.45ms
step:233/2160 train_time:7793ms step_avg:33.45ms
step:234/2160 train_time:7825ms step_avg:33.44ms
step:235/2160 train_time:7859ms step_avg:33.44ms
step:236/2160 train_time:7891ms step_avg:33.44ms
step:237/2160 train_time:7924ms step_avg:33.44ms
step:238/2160 train_time:7956ms step_avg:33.43ms
step:239/2160 train_time:7990ms step_avg:33.43ms
step:240/2160 train_time:8022ms step_avg:33.42ms
step:241/2160 train_time:8055ms step_avg:33.42ms
step:242/2160 train_time:8087ms step_avg:33.42ms
step:243/2160 train_time:8120ms step_avg:33.42ms
step:244/2160 train_time:8152ms step_avg:33.41ms
step:245/2160 train_time:8186ms step_avg:33.41ms
step:246/2160 train_time:8218ms step_avg:33.41ms
step:247/2160 train_time:8251ms step_avg:33.41ms
step:248/2160 train_time:8284ms step_avg:33.40ms
step:249/2160 train_time:8317ms step_avg:33.40ms
step:250/2160 train_time:8349ms step_avg:33.40ms
step:250/2160 val_loss:4.3032 train_time:8385ms step_avg:33.54ms
step:251/2160 train_time:8407ms step_avg:33.49ms
step:252/2160 train_time:8429ms step_avg:33.45ms
step:253/2160 train_time:8453ms step_avg:33.41ms
step:254/2160 train_time:8485ms step_avg:33.41ms
step:255/2160 train_time:8520ms step_avg:33.41ms
step:256/2160 train_time:8554ms step_avg:33.41ms
step:257/2160 train_time:8588ms step_avg:33.42ms
step:258/2160 train_time:8621ms step_avg:33.41ms
step:259/2160 train_time:8655ms step_avg:33.42ms
step:260/2160 train_time:8687ms step_avg:33.41ms
step:261/2160 train_time:8720ms step_avg:33.41ms
step:262/2160 train_time:8752ms step_avg:33.41ms
step:263/2160 train_time:8785ms step_avg:33.40ms
step:264/2160 train_time:8817ms step_avg:33.40ms
step:265/2160 train_time:8850ms step_avg:33.40ms
step:266/2160 train_time:8882ms step_avg:33.39ms
step:267/2160 train_time:8915ms step_avg:33.39ms
step:268/2160 train_time:8947ms step_avg:33.38ms
step:269/2160 train_time:8980ms step_avg:33.38ms
step:270/2160 train_time:9012ms step_avg:33.38ms
step:271/2160 train_time:9045ms step_avg:33.38ms
step:272/2160 train_time:9077ms step_avg:33.37ms
step:273/2160 train_time:9110ms step_avg:33.37ms
step:274/2160 train_time:9142ms step_avg:33.37ms
step:275/2160 train_time:9175ms step_avg:33.37ms
step:276/2160 train_time:9207ms step_avg:33.36ms
step:277/2160 train_time:9240ms step_avg:33.36ms
step:278/2160 train_time:9273ms step_avg:33.35ms
step:279/2160 train_time:9305ms step_avg:33.35ms
step:280/2160 train_time:9337ms step_avg:33.35ms
step:281/2160 train_time:9370ms step_avg:33.35ms
step:282/2160 train_time:9403ms step_avg:33.34ms
step:283/2160 train_time:9436ms step_avg:33.34ms
step:284/2160 train_time:9468ms step_avg:33.34ms
step:285/2160 train_time:9502ms step_avg:33.34ms
step:286/2160 train_time:9534ms step_avg:33.34ms
step:287/2160 train_time:9568ms step_avg:33.34ms
step:288/2160 train_time:9600ms step_avg:33.33ms
step:289/2160 train_time:9634ms step_avg:33.34ms
step:290/2160 train_time:9666ms step_avg:33.33ms
step:291/2160 train_time:9700ms step_avg:33.33ms
step:292/2160 train_time:9732ms step_avg:33.33ms
step:293/2160 train_time:9765ms step_avg:33.33ms
step:294/2160 train_time:9797ms step_avg:33.32ms
step:295/2160 train_time:9831ms step_avg:33.32ms
step:296/2160 train_time:9863ms step_avg:33.32ms
step:297/2160 train_time:9896ms step_avg:33.32ms
step:298/2160 train_time:9928ms step_avg:33.31ms
step:299/2160 train_time:9961ms step_avg:33.31ms
step:300/2160 train_time:9993ms step_avg:33.31ms
step:301/2160 train_time:10026ms step_avg:33.31ms
step:302/2160 train_time:10058ms step_avg:33.31ms
step:303/2160 train_time:10091ms step_avg:33.30ms
step:304/2160 train_time:10124ms step_avg:33.30ms
step:305/2160 train_time:10157ms step_avg:33.30ms
step:306/2160 train_time:10189ms step_avg:33.30ms
step:307/2160 train_time:10222ms step_avg:33.30ms
step:308/2160 train_time:10254ms step_avg:33.29ms
step:309/2160 train_time:10287ms step_avg:33.29ms
step:310/2160 train_time:10319ms step_avg:33.29ms
step:311/2160 train_time:10352ms step_avg:33.29ms
step:312/2160 train_time:10385ms step_avg:33.28ms
step:313/2160 train_time:10418ms step_avg:33.28ms
step:314/2160 train_time:10451ms step_avg:33.28ms
step:315/2160 train_time:10484ms step_avg:33.28ms
step:316/2160 train_time:10516ms step_avg:33.28ms
step:317/2160 train_time:10549ms step_avg:33.28ms
step:318/2160 train_time:10582ms step_avg:33.28ms
step:319/2160 train_time:10615ms step_avg:33.28ms
step:320/2160 train_time:10647ms step_avg:33.27ms
step:321/2160 train_time:10681ms step_avg:33.27ms
step:322/2160 train_time:10713ms step_avg:33.27ms
step:323/2160 train_time:10746ms step_avg:33.27ms
step:324/2160 train_time:10779ms step_avg:33.27ms
step:325/2160 train_time:10812ms step_avg:33.27ms
step:326/2160 train_time:10844ms step_avg:33.26ms
step:327/2160 train_time:10877ms step_avg:33.26ms
step:328/2160 train_time:10910ms step_avg:33.26ms
step:329/2160 train_time:10943ms step_avg:33.26ms
step:330/2160 train_time:10975ms step_avg:33.26ms
step:331/2160 train_time:11008ms step_avg:33.26ms
step:332/2160 train_time:11040ms step_avg:33.25ms
step:333/2160 train_time:11073ms step_avg:33.25ms
step:334/2160 train_time:11106ms step_avg:33.25ms
step:335/2160 train_time:11139ms step_avg:33.25ms
step:336/2160 train_time:11171ms step_avg:33.25ms
step:337/2160 train_time:11204ms step_avg:33.25ms
step:338/2160 train_time:11236ms step_avg:33.24ms
step:339/2160 train_time:11269ms step_avg:33.24ms
step:340/2160 train_time:11302ms step_avg:33.24ms
step:341/2160 train_time:11335ms step_avg:33.24ms
step:342/2160 train_time:11367ms step_avg:33.24ms
step:343/2160 train_time:11400ms step_avg:33.24ms
step:344/2160 train_time:11432ms step_avg:33.23ms
step:345/2160 train_time:11465ms step_avg:33.23ms
step:346/2160 train_time:11498ms step_avg:33.23ms
step:347/2160 train_time:11531ms step_avg:33.23ms
step:348/2160 train_time:11563ms step_avg:33.23ms
step:349/2160 train_time:11596ms step_avg:33.23ms
step:350/2160 train_time:11628ms step_avg:33.22ms
step:351/2160 train_time:11662ms step_avg:33.22ms
step:352/2160 train_time:11695ms step_avg:33.22ms
step:353/2160 train_time:11728ms step_avg:33.22ms
step:354/2160 train_time:11760ms step_avg:33.22ms
step:355/2160 train_time:11793ms step_avg:33.22ms
step:356/2160 train_time:11826ms step_avg:33.22ms
step:357/2160 train_time:11859ms step_avg:33.22ms
step:358/2160 train_time:11891ms step_avg:33.22ms
step:359/2160 train_time:11924ms step_avg:33.22ms
step:360/2160 train_time:11957ms step_avg:33.21ms
step:361/2160 train_time:11990ms step_avg:33.21ms
step:362/2160 train_time:12022ms step_avg:33.21ms
step:363/2160 train_time:12055ms step_avg:33.21ms
step:364/2160 train_time:12087ms step_avg:33.21ms
step:365/2160 train_time:12120ms step_avg:33.21ms
step:366/2160 train_time:12152ms step_avg:33.20ms
step:367/2160 train_time:12185ms step_avg:33.20ms
step:368/2160 train_time:12218ms step_avg:33.20ms
step:369/2160 train_time:12251ms step_avg:33.20ms
step:370/2160 train_time:12283ms step_avg:33.20ms
step:371/2160 train_time:12316ms step_avg:33.20ms
step:372/2160 train_time:12348ms step_avg:33.19ms
step:373/2160 train_time:12381ms step_avg:33.19ms
step:374/2160 train_time:12414ms step_avg:33.19ms
step:375/2160 train_time:12447ms step_avg:33.19ms
step:376/2160 train_time:12479ms step_avg:33.19ms
step:377/2160 train_time:12512ms step_avg:33.19ms
step:378/2160 train_time:12545ms step_avg:33.19ms
step:379/2160 train_time:12578ms step_avg:33.19ms
step:380/2160 train_time:12610ms step_avg:33.19ms
step:381/2160 train_time:12644ms step_avg:33.19ms
step:382/2160 train_time:12676ms step_avg:33.18ms
step:383/2160 train_time:12709ms step_avg:33.18ms
step:384/2160 train_time:12742ms step_avg:33.18ms
step:385/2160 train_time:12775ms step_avg:33.18ms
step:386/2160 train_time:12807ms step_avg:33.18ms
step:387/2160 train_time:12840ms step_avg:33.18ms
step:388/2160 train_time:12873ms step_avg:33.18ms
step:389/2160 train_time:12906ms step_avg:33.18ms
step:390/2160 train_time:12938ms step_avg:33.17ms
step:391/2160 train_time:12971ms step_avg:33.17ms
step:392/2160 train_time:13003ms step_avg:33.17ms
step:393/2160 train_time:13037ms step_avg:33.17ms
step:394/2160 train_time:13069ms step_avg:33.17ms
step:395/2160 train_time:13102ms step_avg:33.17ms
step:396/2160 train_time:13134ms step_avg:33.17ms
step:397/2160 train_time:13168ms step_avg:33.17ms
step:398/2160 train_time:13200ms step_avg:33.17ms
step:399/2160 train_time:13233ms step_avg:33.17ms
step:400/2160 train_time:13265ms step_avg:33.16ms
step:401/2160 train_time:13298ms step_avg:33.16ms
step:402/2160 train_time:13331ms step_avg:33.16ms
step:403/2160 train_time:13364ms step_avg:33.16ms
step:404/2160 train_time:13396ms step_avg:33.16ms
step:405/2160 train_time:13430ms step_avg:33.16ms
step:406/2160 train_time:13462ms step_avg:33.16ms
step:407/2160 train_time:13495ms step_avg:33.16ms
step:408/2160 train_time:13527ms step_avg:33.15ms
step:409/2160 train_time:13560ms step_avg:33.15ms
step:410/2160 train_time:13592ms step_avg:33.15ms
step:411/2160 train_time:13625ms step_avg:33.15ms
step:412/2160 train_time:13658ms step_avg:33.15ms
step:413/2160 train_time:13691ms step_avg:33.15ms
step:414/2160 train_time:13723ms step_avg:33.15ms
step:415/2160 train_time:13757ms step_avg:33.15ms
step:416/2160 train_time:13789ms step_avg:33.15ms
step:417/2160 train_time:13822ms step_avg:33.15ms
step:418/2160 train_time:13854ms step_avg:33.14ms
step:419/2160 train_time:13888ms step_avg:33.14ms
step:420/2160 train_time:13920ms step_avg:33.14ms
step:421/2160 train_time:13953ms step_avg:33.14ms
step:422/2160 train_time:13985ms step_avg:33.14ms
step:423/2160 train_time:14019ms step_avg:33.14ms
step:424/2160 train_time:14051ms step_avg:33.14ms
step:425/2160 train_time:14084ms step_avg:33.14ms
step:426/2160 train_time:14116ms step_avg:33.14ms
step:427/2160 train_time:14150ms step_avg:33.14ms
step:428/2160 train_time:14182ms step_avg:33.14ms
step:429/2160 train_time:14215ms step_avg:33.14ms
step:430/2160 train_time:14247ms step_avg:33.13ms
step:431/2160 train_time:14281ms step_avg:33.13ms
step:432/2160 train_time:14313ms step_avg:33.13ms
step:433/2160 train_time:14346ms step_avg:33.13ms
step:434/2160 train_time:14378ms step_avg:33.13ms
step:435/2160 train_time:14411ms step_avg:33.13ms
step:436/2160 train_time:14444ms step_avg:33.13ms
step:437/2160 train_time:14477ms step_avg:33.13ms
step:438/2160 train_time:14509ms step_avg:33.12ms
step:439/2160 train_time:14542ms step_avg:33.13ms
step:440/2160 train_time:14575ms step_avg:33.12ms
step:441/2160 train_time:14607ms step_avg:33.12ms
step:442/2160 train_time:14640ms step_avg:33.12ms
step:443/2160 train_time:14673ms step_avg:33.12ms
step:444/2160 train_time:14705ms step_avg:33.12ms
step:445/2160 train_time:14738ms step_avg:33.12ms
step:446/2160 train_time:14771ms step_avg:33.12ms
step:447/2160 train_time:14804ms step_avg:33.12ms
step:448/2160 train_time:14836ms step_avg:33.12ms
step:449/2160 train_time:14869ms step_avg:33.12ms
step:450/2160 train_time:14902ms step_avg:33.12ms
step:451/2160 train_time:14935ms step_avg:33.12ms
step:452/2160 train_time:14967ms step_avg:33.11ms
step:453/2160 train_time:15000ms step_avg:33.11ms
step:454/2160 train_time:15033ms step_avg:33.11ms
step:455/2160 train_time:15066ms step_avg:33.11ms
step:456/2160 train_time:15098ms step_avg:33.11ms
step:457/2160 train_time:15132ms step_avg:33.11ms
step:458/2160 train_time:15164ms step_avg:33.11ms
step:459/2160 train_time:15197ms step_avg:33.11ms
step:460/2160 train_time:15230ms step_avg:33.11ms
step:461/2160 train_time:15262ms step_avg:33.11ms
step:462/2160 train_time:15295ms step_avg:33.11ms
step:463/2160 train_time:15328ms step_avg:33.11ms
step:464/2160 train_time:15360ms step_avg:33.10ms
step:465/2160 train_time:15393ms step_avg:33.10ms
step:466/2160 train_time:15426ms step_avg:33.10ms
step:467/2160 train_time:15459ms step_avg:33.10ms
step:468/2160 train_time:15491ms step_avg:33.10ms
step:469/2160 train_time:15524ms step_avg:33.10ms
step:470/2160 train_time:15556ms step_avg:33.10ms
step:471/2160 train_time:15589ms step_avg:33.10ms
step:472/2160 train_time:15622ms step_avg:33.10ms
step:473/2160 train_time:15655ms step_avg:33.10ms
step:474/2160 train_time:15687ms step_avg:33.09ms
step:475/2160 train_time:15720ms step_avg:33.10ms
step:476/2160 train_time:15752ms step_avg:33.09ms
step:477/2160 train_time:15786ms step_avg:33.09ms
step:478/2160 train_time:15818ms step_avg:33.09ms
step:479/2160 train_time:15851ms step_avg:33.09ms
step:480/2160 train_time:15884ms step_avg:33.09ms
step:481/2160 train_time:15917ms step_avg:33.09ms
step:482/2160 train_time:15949ms step_avg:33.09ms
step:483/2160 train_time:15982ms step_avg:33.09ms
step:484/2160 train_time:16015ms step_avg:33.09ms
step:485/2160 train_time:16048ms step_avg:33.09ms
step:486/2160 train_time:16080ms step_avg:33.09ms
step:487/2160 train_time:16113ms step_avg:33.09ms
step:488/2160 train_time:16146ms step_avg:33.09ms
step:489/2160 train_time:16179ms step_avg:33.09ms
step:490/2160 train_time:16211ms step_avg:33.08ms
step:491/2160 train_time:16245ms step_avg:33.08ms
step:492/2160 train_time:16277ms step_avg:33.08ms
step:493/2160 train_time:16310ms step_avg:33.08ms
step:494/2160 train_time:16342ms step_avg:33.08ms
step:495/2160 train_time:16376ms step_avg:33.08ms
step:496/2160 train_time:16408ms step_avg:33.08ms
step:497/2160 train_time:16441ms step_avg:33.08ms
step:498/2160 train_time:16473ms step_avg:33.08ms
step:499/2160 train_time:16506ms step_avg:33.08ms
step:500/2160 train_time:16538ms step_avg:33.08ms
step:500/2160 val_loss:4.0184 train_time:16574ms step_avg:33.15ms
step:501/2160 train_time:16596ms step_avg:33.13ms
step:502/2160 train_time:16618ms step_avg:33.10ms
step:503/2160 train_time:16641ms step_avg:33.08ms
step:504/2160 train_time:16674ms step_avg:33.08ms
step:505/2160 train_time:16708ms step_avg:33.08ms
step:506/2160 train_time:16742ms step_avg:33.09ms
step:507/2160 train_time:16777ms step_avg:33.09ms
step:508/2160 train_time:16811ms step_avg:33.09ms
step:509/2160 train_time:16846ms step_avg:33.10ms
step:510/2160 train_time:16878ms step_avg:33.09ms
step:511/2160 train_time:16912ms step_avg:33.10ms
step:512/2160 train_time:16944ms step_avg:33.09ms
step:513/2160 train_time:16978ms step_avg:33.09ms
step:514/2160 train_time:17010ms step_avg:33.09ms
step:515/2160 train_time:17043ms step_avg:33.09ms
step:516/2160 train_time:17075ms step_avg:33.09ms
step:517/2160 train_time:17108ms step_avg:33.09ms
step:518/2160 train_time:17140ms step_avg:33.09ms
step:519/2160 train_time:17173ms step_avg:33.09ms
step:520/2160 train_time:17205ms step_avg:33.09ms
step:521/2160 train_time:17238ms step_avg:33.09ms
step:522/2160 train_time:17270ms step_avg:33.08ms
step:523/2160 train_time:17303ms step_avg:33.08ms
step:524/2160 train_time:17335ms step_avg:33.08ms
step:525/2160 train_time:17369ms step_avg:33.08ms
step:526/2160 train_time:17401ms step_avg:33.08ms
step:527/2160 train_time:17434ms step_avg:33.08ms
step:528/2160 train_time:17466ms step_avg:33.08ms
step:529/2160 train_time:17499ms step_avg:33.08ms
step:530/2160 train_time:17531ms step_avg:33.08ms
step:531/2160 train_time:17564ms step_avg:33.08ms
step:532/2160 train_time:17596ms step_avg:33.08ms
step:533/2160 train_time:17630ms step_avg:33.08ms
step:534/2160 train_time:17662ms step_avg:33.07ms
step:535/2160 train_time:17697ms step_avg:33.08ms
step:536/2160 train_time:17729ms step_avg:33.08ms
step:537/2160 train_time:17763ms step_avg:33.08ms
step:538/2160 train_time:17796ms step_avg:33.08ms
step:539/2160 train_time:17830ms step_avg:33.08ms
step:540/2160 train_time:17862ms step_avg:33.08ms
step:541/2160 train_time:17896ms step_avg:33.08ms
step:542/2160 train_time:17929ms step_avg:33.08ms
step:543/2160 train_time:17962ms step_avg:33.08ms
step:544/2160 train_time:17994ms step_avg:33.08ms
step:545/2160 train_time:18028ms step_avg:33.08ms
step:546/2160 train_time:18060ms step_avg:33.08ms
step:547/2160 train_time:18093ms step_avg:33.08ms
step:548/2160 train_time:18125ms step_avg:33.08ms
step:549/2160 train_time:18159ms step_avg:33.08ms
step:550/2160 train_time:18191ms step_avg:33.07ms
step:551/2160 train_time:18224ms step_avg:33.08ms
step:552/2160 train_time:18257ms step_avg:33.07ms
step:553/2160 train_time:18290ms step_avg:33.07ms
step:554/2160 train_time:18322ms step_avg:33.07ms
step:555/2160 train_time:18355ms step_avg:33.07ms
step:556/2160 train_time:18388ms step_avg:33.07ms
step:557/2160 train_time:18421ms step_avg:33.07ms
step:558/2160 train_time:18453ms step_avg:33.07ms
step:559/2160 train_time:18486ms step_avg:33.07ms
step:560/2160 train_time:18518ms step_avg:33.07ms
step:561/2160 train_time:18551ms step_avg:33.07ms
step:562/2160 train_time:18583ms step_avg:33.07ms
step:563/2160 train_time:18616ms step_avg:33.07ms
step:564/2160 train_time:18649ms step_avg:33.07ms
step:565/2160 train_time:18682ms step_avg:33.07ms
step:566/2160 train_time:18715ms step_avg:33.06ms
step:567/2160 train_time:18748ms step_avg:33.07ms
step:568/2160 train_time:18781ms step_avg:33.06ms
step:569/2160 train_time:18815ms step_avg:33.07ms
step:570/2160 train_time:18847ms step_avg:33.06ms
step:571/2160 train_time:18881ms step_avg:33.07ms
step:572/2160 train_time:18913ms step_avg:33.07ms
step:573/2160 train_time:18947ms step_avg:33.07ms
step:574/2160 train_time:18979ms step_avg:33.06ms
step:575/2160 train_time:19012ms step_avg:33.06ms
step:576/2160 train_time:19044ms step_avg:33.06ms
step:577/2160 train_time:19078ms step_avg:33.06ms
step:578/2160 train_time:19110ms step_avg:33.06ms
step:579/2160 train_time:19143ms step_avg:33.06ms
step:580/2160 train_time:19176ms step_avg:33.06ms
step:581/2160 train_time:19209ms step_avg:33.06ms
step:582/2160 train_time:19241ms step_avg:33.06ms
step:583/2160 train_time:19274ms step_avg:33.06ms
step:584/2160 train_time:19306ms step_avg:33.06ms
step:585/2160 train_time:19339ms step_avg:33.06ms
step:586/2160 train_time:19372ms step_avg:33.06ms
step:587/2160 train_time:19405ms step_avg:33.06ms
step:588/2160 train_time:19437ms step_avg:33.06ms
step:589/2160 train_time:19470ms step_avg:33.06ms
step:590/2160 train_time:19502ms step_avg:33.05ms
step:591/2160 train_time:19536ms step_avg:33.06ms
step:592/2160 train_time:19568ms step_avg:33.05ms
step:593/2160 train_time:19602ms step_avg:33.05ms
step:594/2160 train_time:19633ms step_avg:33.05ms
step:595/2160 train_time:19667ms step_avg:33.05ms
step:596/2160 train_time:19699ms step_avg:33.05ms
step:597/2160 train_time:19733ms step_avg:33.05ms
step:598/2160 train_time:19766ms step_avg:33.05ms
step:599/2160 train_time:19799ms step_avg:33.05ms
step:600/2160 train_time:19831ms step_avg:33.05ms
step:601/2160 train_time:19865ms step_avg:33.05ms
step:602/2160 train_time:19897ms step_avg:33.05ms
step:603/2160 train_time:19930ms step_avg:33.05ms
step:604/2160 train_time:19963ms step_avg:33.05ms
step:605/2160 train_time:19996ms step_avg:33.05ms
step:606/2160 train_time:20029ms step_avg:33.05ms
step:607/2160 train_time:20062ms step_avg:33.05ms
step:608/2160 train_time:20094ms step_avg:33.05ms
step:609/2160 train_time:20128ms step_avg:33.05ms
step:610/2160 train_time:20161ms step_avg:33.05ms
step:611/2160 train_time:20193ms step_avg:33.05ms
step:612/2160 train_time:20226ms step_avg:33.05ms
step:613/2160 train_time:20259ms step_avg:33.05ms
step:614/2160 train_time:20291ms step_avg:33.05ms
step:615/2160 train_time:20324ms step_avg:33.05ms
step:616/2160 train_time:20357ms step_avg:33.05ms
step:617/2160 train_time:20390ms step_avg:33.05ms
step:618/2160 train_time:20422ms step_avg:33.05ms
step:619/2160 train_time:20456ms step_avg:33.05ms
step:620/2160 train_time:20488ms step_avg:33.04ms
step:621/2160 train_time:20521ms step_avg:33.05ms
step:622/2160 train_time:20553ms step_avg:33.04ms
step:623/2160 train_time:20586ms step_avg:33.04ms
step:624/2160 train_time:20619ms step_avg:33.04ms
step:625/2160 train_time:20652ms step_avg:33.04ms
step:626/2160 train_time:20684ms step_avg:33.04ms
step:627/2160 train_time:20718ms step_avg:33.04ms
step:628/2160 train_time:20750ms step_avg:33.04ms
step:629/2160 train_time:20784ms step_avg:33.04ms
step:630/2160 train_time:20816ms step_avg:33.04ms
step:631/2160 train_time:20849ms step_avg:33.04ms
step:632/2160 train_time:20881ms step_avg:33.04ms
step:633/2160 train_time:20915ms step_avg:33.04ms
step:634/2160 train_time:20948ms step_avg:33.04ms
step:635/2160 train_time:20981ms step_avg:33.04ms
step:636/2160 train_time:21013ms step_avg:33.04ms
step:637/2160 train_time:21047ms step_avg:33.04ms
step:638/2160 train_time:21079ms step_avg:33.04ms
step:639/2160 train_time:21112ms step_avg:33.04ms
step:640/2160 train_time:21145ms step_avg:33.04ms
step:641/2160 train_time:21179ms step_avg:33.04ms
step:642/2160 train_time:21211ms step_avg:33.04ms
step:643/2160 train_time:21244ms step_avg:33.04ms
step:644/2160 train_time:21276ms step_avg:33.04ms
step:645/2160 train_time:21309ms step_avg:33.04ms
step:646/2160 train_time:21342ms step_avg:33.04ms
step:647/2160 train_time:21375ms step_avg:33.04ms
step:648/2160 train_time:21407ms step_avg:33.04ms
step:649/2160 train_time:21440ms step_avg:33.04ms
step:650/2160 train_time:21473ms step_avg:33.03ms
step:651/2160 train_time:21506ms step_avg:33.03ms
step:652/2160 train_time:21538ms step_avg:33.03ms
step:653/2160 train_time:21572ms step_avg:33.03ms
step:654/2160 train_time:21604ms step_avg:33.03ms
step:655/2160 train_time:21638ms step_avg:33.03ms
step:656/2160 train_time:21670ms step_avg:33.03ms
step:657/2160 train_time:21704ms step_avg:33.03ms
step:658/2160 train_time:21736ms step_avg:33.03ms
step:659/2160 train_time:21769ms step_avg:33.03ms
step:660/2160 train_time:21802ms step_avg:33.03ms
step:661/2160 train_time:21836ms step_avg:33.03ms
step:662/2160 train_time:21868ms step_avg:33.03ms
step:663/2160 train_time:21901ms step_avg:33.03ms
step:664/2160 train_time:21933ms step_avg:33.03ms
step:665/2160 train_time:21967ms step_avg:33.03ms
step:666/2160 train_time:21999ms step_avg:33.03ms
step:667/2160 train_time:22033ms step_avg:33.03ms
step:668/2160 train_time:22065ms step_avg:33.03ms
step:669/2160 train_time:22098ms step_avg:33.03ms
step:670/2160 train_time:22130ms step_avg:33.03ms
step:671/2160 train_time:22164ms step_avg:33.03ms
step:672/2160 train_time:22196ms step_avg:33.03ms
step:673/2160 train_time:22230ms step_avg:33.03ms
step:674/2160 train_time:22262ms step_avg:33.03ms
step:675/2160 train_time:22295ms step_avg:33.03ms
step:676/2160 train_time:22328ms step_avg:33.03ms
step:677/2160 train_time:22361ms step_avg:33.03ms
step:678/2160 train_time:22393ms step_avg:33.03ms
step:679/2160 train_time:22427ms step_avg:33.03ms
step:680/2160 train_time:22459ms step_avg:33.03ms
step:681/2160 train_time:22492ms step_avg:33.03ms
step:682/2160 train_time:22525ms step_avg:33.03ms
step:683/2160 train_time:22558ms step_avg:33.03ms
step:684/2160 train_time:22590ms step_avg:33.03ms
step:685/2160 train_time:22623ms step_avg:33.03ms
step:686/2160 train_time:22655ms step_avg:33.03ms
step:687/2160 train_time:22689ms step_avg:33.03ms
step:688/2160 train_time:22721ms step_avg:33.02ms
step:689/2160 train_time:22755ms step_avg:33.03ms
step:690/2160 train_time:22788ms step_avg:33.03ms
step:691/2160 train_time:22821ms step_avg:33.03ms
step:692/2160 train_time:22853ms step_avg:33.03ms
step:693/2160 train_time:22887ms step_avg:33.03ms
step:694/2160 train_time:22919ms step_avg:33.02ms
step:695/2160 train_time:22952ms step_avg:33.02ms
step:696/2160 train_time:22985ms step_avg:33.02ms
step:697/2160 train_time:23018ms step_avg:33.02ms
step:698/2160 train_time:23050ms step_avg:33.02ms
step:699/2160 train_time:23084ms step_avg:33.02ms
step:700/2160 train_time:23116ms step_avg:33.02ms
step:701/2160 train_time:23149ms step_avg:33.02ms
step:702/2160 train_time:23182ms step_avg:33.02ms
step:703/2160 train_time:23215ms step_avg:33.02ms
step:704/2160 train_time:23247ms step_avg:33.02ms
step:705/2160 train_time:23281ms step_avg:33.02ms
step:706/2160 train_time:23313ms step_avg:33.02ms
step:707/2160 train_time:23346ms step_avg:33.02ms
step:708/2160 train_time:23380ms step_avg:33.02ms
step:709/2160 train_time:23438ms step_avg:33.06ms
step:710/2160 train_time:23497ms step_avg:33.09ms
step:711/2160 train_time:23556ms step_avg:33.13ms
step:712/2160 train_time:23615ms step_avg:33.17ms
step:713/2160 train_time:23676ms step_avg:33.21ms
step:714/2160 train_time:23735ms step_avg:33.24ms
step:715/2160 train_time:23795ms step_avg:33.28ms
step:716/2160 train_time:23854ms step_avg:33.32ms
step:717/2160 train_time:23914ms step_avg:33.35ms
step:718/2160 train_time:23973ms step_avg:33.39ms
step:719/2160 train_time:24033ms step_avg:33.43ms
step:720/2160 train_time:24092ms step_avg:33.46ms
step:721/2160 train_time:24152ms step_avg:33.50ms
step:722/2160 train_time:24211ms step_avg:33.53ms
step:723/2160 train_time:24272ms step_avg:33.57ms
step:724/2160 train_time:24331ms step_avg:33.61ms
step:725/2160 train_time:24391ms step_avg:33.64ms
step:726/2160 train_time:24450ms step_avg:33.68ms
step:727/2160 train_time:24510ms step_avg:33.71ms
step:728/2160 train_time:24570ms step_avg:33.75ms
step:729/2160 train_time:24630ms step_avg:33.79ms
step:730/2160 train_time:24690ms step_avg:33.82ms
step:731/2160 train_time:24750ms step_avg:33.86ms
step:732/2160 train_time:24809ms step_avg:33.89ms
step:733/2160 train_time:24870ms step_avg:33.93ms
step:734/2160 train_time:24929ms step_avg:33.96ms
step:735/2160 train_time:24990ms step_avg:34.00ms
step:736/2160 train_time:25049ms step_avg:34.03ms
step:737/2160 train_time:25110ms step_avg:34.07ms
step:738/2160 train_time:25170ms step_avg:34.11ms
step:739/2160 train_time:25230ms step_avg:34.14ms
step:740/2160 train_time:25289ms step_avg:34.17ms
step:741/2160 train_time:25350ms step_avg:34.21ms
step:742/2160 train_time:25409ms step_avg:34.24ms
step:743/2160 train_time:25470ms step_avg:34.28ms
step:744/2160 train_time:25530ms step_avg:34.31ms
step:745/2160 train_time:25591ms step_avg:34.35ms
step:746/2160 train_time:25650ms step_avg:34.38ms
step:747/2160 train_time:25711ms step_avg:34.42ms
step:748/2160 train_time:25770ms step_avg:34.45ms
step:749/2160 train_time:25830ms step_avg:34.49ms
step:750/2160 train_time:25889ms step_avg:34.52ms
step:750/2160 val_loss:3.8665 train_time:25952ms step_avg:34.60ms
step:751/2160 train_time:25974ms step_avg:34.59ms
step:752/2160 train_time:26011ms step_avg:34.59ms
step:753/2160 train_time:26074ms step_avg:34.63ms
step:754/2160 train_time:26136ms step_avg:34.66ms
step:755/2160 train_time:26198ms step_avg:34.70ms
step:756/2160 train_time:26256ms step_avg:34.73ms
step:757/2160 train_time:26316ms step_avg:34.76ms
step:758/2160 train_time:26375ms step_avg:34.80ms
step:759/2160 train_time:26436ms step_avg:34.83ms
step:760/2160 train_time:26494ms step_avg:34.86ms
step:761/2160 train_time:26554ms step_avg:34.89ms
step:762/2160 train_time:26613ms step_avg:34.93ms
step:763/2160 train_time:26673ms step_avg:34.96ms
step:764/2160 train_time:26732ms step_avg:34.99ms
step:765/2160 train_time:26791ms step_avg:35.02ms
step:766/2160 train_time:26850ms step_avg:35.05ms
step:767/2160 train_time:26912ms step_avg:35.09ms
step:768/2160 train_time:26973ms step_avg:35.12ms
step:769/2160 train_time:27035ms step_avg:35.16ms
step:770/2160 train_time:27096ms step_avg:35.19ms
step:771/2160 train_time:27157ms step_avg:35.22ms
step:772/2160 train_time:27216ms step_avg:35.25ms
step:773/2160 train_time:27276ms step_avg:35.29ms
step:774/2160 train_time:27336ms step_avg:35.32ms
step:775/2160 train_time:27395ms step_avg:35.35ms
step:776/2160 train_time:27454ms step_avg:35.38ms
step:777/2160 train_time:27514ms step_avg:35.41ms
step:778/2160 train_time:27573ms step_avg:35.44ms
step:779/2160 train_time:27633ms step_avg:35.47ms
step:780/2160 train_time:27691ms step_avg:35.50ms
step:781/2160 train_time:27751ms step_avg:35.53ms
step:782/2160 train_time:27810ms step_avg:35.56ms
step:783/2160 train_time:27871ms step_avg:35.59ms
step:784/2160 train_time:27931ms step_avg:35.63ms
step:785/2160 train_time:27992ms step_avg:35.66ms
step:786/2160 train_time:28053ms step_avg:35.69ms
step:787/2160 train_time:28115ms step_avg:35.72ms
step:788/2160 train_time:28175ms step_avg:35.75ms
step:789/2160 train_time:28235ms step_avg:35.79ms
step:790/2160 train_time:28295ms step_avg:35.82ms
step:791/2160 train_time:28354ms step_avg:35.85ms
step:792/2160 train_time:28413ms step_avg:35.88ms
step:793/2160 train_time:28473ms step_avg:35.91ms
step:794/2160 train_time:28532ms step_avg:35.93ms
step:795/2160 train_time:28592ms step_avg:35.96ms
step:796/2160 train_time:28651ms step_avg:35.99ms
step:797/2160 train_time:28711ms step_avg:36.02ms
step:798/2160 train_time:28769ms step_avg:36.05ms
step:799/2160 train_time:28830ms step_avg:36.08ms
step:800/2160 train_time:28888ms step_avg:36.11ms
step:801/2160 train_time:28949ms step_avg:36.14ms
step:802/2160 train_time:29009ms step_avg:36.17ms
step:803/2160 train_time:29072ms step_avg:36.20ms
step:804/2160 train_time:29131ms step_avg:36.23ms
step:805/2160 train_time:29192ms step_avg:36.26ms
step:806/2160 train_time:29252ms step_avg:36.29ms
step:807/2160 train_time:29313ms step_avg:36.32ms
step:808/2160 train_time:29372ms step_avg:36.35ms
step:809/2160 train_time:29432ms step_avg:36.38ms
step:810/2160 train_time:29491ms step_avg:36.41ms
step:811/2160 train_time:29551ms step_avg:36.44ms
step:812/2160 train_time:29610ms step_avg:36.47ms
step:813/2160 train_time:29670ms step_avg:36.49ms
step:814/2160 train_time:29730ms step_avg:36.52ms
step:815/2160 train_time:29789ms step_avg:36.55ms
step:816/2160 train_time:29848ms step_avg:36.58ms
step:817/2160 train_time:29909ms step_avg:36.61ms
step:818/2160 train_time:29969ms step_avg:36.64ms
step:819/2160 train_time:30030ms step_avg:36.67ms
step:820/2160 train_time:30090ms step_avg:36.69ms
step:821/2160 train_time:30151ms step_avg:36.72ms
step:822/2160 train_time:30210ms step_avg:36.75ms
step:823/2160 train_time:30272ms step_avg:36.78ms
step:824/2160 train_time:30331ms step_avg:36.81ms
step:825/2160 train_time:30392ms step_avg:36.84ms
step:826/2160 train_time:30450ms step_avg:36.86ms
step:827/2160 train_time:30511ms step_avg:36.89ms
step:828/2160 train_time:30569ms step_avg:36.92ms
step:829/2160 train_time:30629ms step_avg:36.95ms
step:830/2160 train_time:30688ms step_avg:36.97ms
step:831/2160 train_time:30748ms step_avg:37.00ms
step:832/2160 train_time:30807ms step_avg:37.03ms
step:833/2160 train_time:30868ms step_avg:37.06ms
step:834/2160 train_time:30927ms step_avg:37.08ms
step:835/2160 train_time:30989ms step_avg:37.11ms
step:836/2160 train_time:31049ms step_avg:37.14ms
step:837/2160 train_time:31111ms step_avg:37.17ms
step:838/2160 train_time:31170ms step_avg:37.20ms
step:839/2160 train_time:31231ms step_avg:37.22ms
step:840/2160 train_time:31290ms step_avg:37.25ms
step:841/2160 train_time:31350ms step_avg:37.28ms
step:842/2160 train_time:31409ms step_avg:37.30ms
step:843/2160 train_time:31470ms step_avg:37.33ms
step:844/2160 train_time:31529ms step_avg:37.36ms
step:845/2160 train_time:31589ms step_avg:37.38ms
step:846/2160 train_time:31648ms step_avg:37.41ms
step:847/2160 train_time:31708ms step_avg:37.44ms
step:848/2160 train_time:31767ms step_avg:37.46ms
step:849/2160 train_time:31829ms step_avg:37.49ms
step:850/2160 train_time:31888ms step_avg:37.52ms
step:851/2160 train_time:31949ms step_avg:37.54ms
step:852/2160 train_time:32009ms step_avg:37.57ms
step:853/2160 train_time:32071ms step_avg:37.60ms
step:854/2160 train_time:32130ms step_avg:37.62ms
step:855/2160 train_time:32190ms step_avg:37.65ms
step:856/2160 train_time:32250ms step_avg:37.67ms
step:857/2160 train_time:32310ms step_avg:37.70ms
step:858/2160 train_time:32369ms step_avg:37.73ms
step:859/2160 train_time:32430ms step_avg:37.75ms
step:860/2160 train_time:32489ms step_avg:37.78ms
step:861/2160 train_time:32549ms step_avg:37.80ms
step:862/2160 train_time:32608ms step_avg:37.83ms
step:863/2160 train_time:32669ms step_avg:37.86ms
step:864/2160 train_time:32728ms step_avg:37.88ms
step:865/2160 train_time:32788ms step_avg:37.91ms
step:866/2160 train_time:32848ms step_avg:37.93ms
step:867/2160 train_time:32909ms step_avg:37.96ms
step:868/2160 train_time:32968ms step_avg:37.98ms
step:869/2160 train_time:33028ms step_avg:38.01ms
step:870/2160 train_time:33088ms step_avg:38.03ms
step:871/2160 train_time:33149ms step_avg:38.06ms
step:872/2160 train_time:33208ms step_avg:38.08ms
step:873/2160 train_time:33269ms step_avg:38.11ms
step:874/2160 train_time:33329ms step_avg:38.13ms
step:875/2160 train_time:33390ms step_avg:38.16ms
step:876/2160 train_time:33449ms step_avg:38.18ms
step:877/2160 train_time:33509ms step_avg:38.21ms
step:878/2160 train_time:33567ms step_avg:38.23ms
step:879/2160 train_time:33628ms step_avg:38.26ms
step:880/2160 train_time:33687ms step_avg:38.28ms
step:881/2160 train_time:33747ms step_avg:38.31ms
step:882/2160 train_time:33806ms step_avg:38.33ms
step:883/2160 train_time:33867ms step_avg:38.35ms
step:884/2160 train_time:33927ms step_avg:38.38ms
step:885/2160 train_time:33988ms step_avg:38.40ms
step:886/2160 train_time:34047ms step_avg:38.43ms
step:887/2160 train_time:34109ms step_avg:38.45ms
step:888/2160 train_time:34169ms step_avg:38.48ms
step:889/2160 train_time:34229ms step_avg:38.50ms
step:890/2160 train_time:34288ms step_avg:38.53ms
step:891/2160 train_time:34349ms step_avg:38.55ms
step:892/2160 train_time:34408ms step_avg:38.57ms
step:893/2160 train_time:34469ms step_avg:38.60ms
step:894/2160 train_time:34529ms step_avg:38.62ms
step:895/2160 train_time:34589ms step_avg:38.65ms
step:896/2160 train_time:34648ms step_avg:38.67ms
step:897/2160 train_time:34709ms step_avg:38.69ms
step:898/2160 train_time:34767ms step_avg:38.72ms
step:899/2160 train_time:34828ms step_avg:38.74ms
step:900/2160 train_time:34887ms step_avg:38.76ms
step:901/2160 train_time:34948ms step_avg:38.79ms
step:902/2160 train_time:35008ms step_avg:38.81ms
step:903/2160 train_time:35069ms step_avg:38.84ms
step:904/2160 train_time:35128ms step_avg:38.86ms
step:905/2160 train_time:35190ms step_avg:38.88ms
step:906/2160 train_time:35249ms step_avg:38.91ms
step:907/2160 train_time:35310ms step_avg:38.93ms
step:908/2160 train_time:35369ms step_avg:38.95ms
step:909/2160 train_time:35430ms step_avg:38.98ms
step:910/2160 train_time:35490ms step_avg:39.00ms
step:911/2160 train_time:35550ms step_avg:39.02ms
step:912/2160 train_time:35609ms step_avg:39.05ms
step:913/2160 train_time:35670ms step_avg:39.07ms
step:914/2160 train_time:35729ms step_avg:39.09ms
step:915/2160 train_time:35790ms step_avg:39.11ms
step:916/2160 train_time:35849ms step_avg:39.14ms
step:917/2160 train_time:35910ms step_avg:39.16ms
step:918/2160 train_time:35969ms step_avg:39.18ms
step:919/2160 train_time:36030ms step_avg:39.21ms
step:920/2160 train_time:36090ms step_avg:39.23ms
step:921/2160 train_time:36151ms step_avg:39.25ms
step:922/2160 train_time:36210ms step_avg:39.27ms
step:923/2160 train_time:36271ms step_avg:39.30ms
step:924/2160 train_time:36331ms step_avg:39.32ms
step:925/2160 train_time:36391ms step_avg:39.34ms
step:926/2160 train_time:36450ms step_avg:39.36ms
step:927/2160 train_time:36511ms step_avg:39.39ms
step:928/2160 train_time:36570ms step_avg:39.41ms
step:929/2160 train_time:36630ms step_avg:39.43ms
step:930/2160 train_time:36689ms step_avg:39.45ms
step:931/2160 train_time:36749ms step_avg:39.47ms
step:932/2160 train_time:36809ms step_avg:39.49ms
step:933/2160 train_time:36869ms step_avg:39.52ms
step:934/2160 train_time:36928ms step_avg:39.54ms
step:935/2160 train_time:36988ms step_avg:39.56ms
step:936/2160 train_time:37047ms step_avg:39.58ms
step:937/2160 train_time:37108ms step_avg:39.60ms
step:938/2160 train_time:37168ms step_avg:39.62ms
step:939/2160 train_time:37229ms step_avg:39.65ms
step:940/2160 train_time:37289ms step_avg:39.67ms
step:941/2160 train_time:37349ms step_avg:39.69ms
step:942/2160 train_time:37409ms step_avg:39.71ms
step:943/2160 train_time:37469ms step_avg:39.73ms
step:944/2160 train_time:37529ms step_avg:39.76ms
step:945/2160 train_time:37589ms step_avg:39.78ms
step:946/2160 train_time:37649ms step_avg:39.80ms
step:947/2160 train_time:37710ms step_avg:39.82ms
step:948/2160 train_time:37768ms step_avg:39.84ms
step:949/2160 train_time:37829ms step_avg:39.86ms
step:950/2160 train_time:37889ms step_avg:39.88ms
step:951/2160 train_time:37950ms step_avg:39.90ms
step:952/2160 train_time:38009ms step_avg:39.93ms
step:953/2160 train_time:38070ms step_avg:39.95ms
step:954/2160 train_time:38129ms step_avg:39.97ms
step:955/2160 train_time:38191ms step_avg:39.99ms
step:956/2160 train_time:38250ms step_avg:40.01ms
step:957/2160 train_time:38311ms step_avg:40.03ms
step:958/2160 train_time:38370ms step_avg:40.05ms
step:959/2160 train_time:38430ms step_avg:40.07ms
step:960/2160 train_time:38490ms step_avg:40.09ms
step:961/2160 train_time:38550ms step_avg:40.11ms
step:962/2160 train_time:38609ms step_avg:40.13ms
step:963/2160 train_time:38670ms step_avg:40.16ms
step:964/2160 train_time:38729ms step_avg:40.18ms
step:965/2160 train_time:38789ms step_avg:40.20ms
step:966/2160 train_time:38849ms step_avg:40.22ms
step:967/2160 train_time:38909ms step_avg:40.24ms
step:968/2160 train_time:38969ms step_avg:40.26ms
step:969/2160 train_time:39029ms step_avg:40.28ms
step:970/2160 train_time:39089ms step_avg:40.30ms
step:971/2160 train_time:39150ms step_avg:40.32ms
step:972/2160 train_time:39209ms step_avg:40.34ms
step:973/2160 train_time:39270ms step_avg:40.36ms
step:974/2160 train_time:39329ms step_avg:40.38ms
step:975/2160 train_time:39390ms step_avg:40.40ms
step:976/2160 train_time:39449ms step_avg:40.42ms
step:977/2160 train_time:39509ms step_avg:40.44ms
step:978/2160 train_time:39569ms step_avg:40.46ms
step:979/2160 train_time:39629ms step_avg:40.48ms
step:980/2160 train_time:39689ms step_avg:40.50ms
step:981/2160 train_time:39749ms step_avg:40.52ms
step:982/2160 train_time:39809ms step_avg:40.54ms
step:983/2160 train_time:39869ms step_avg:40.56ms
step:984/2160 train_time:39930ms step_avg:40.58ms
step:985/2160 train_time:39990ms step_avg:40.60ms
step:986/2160 train_time:40049ms step_avg:40.62ms
step:987/2160 train_time:40110ms step_avg:40.64ms
step:988/2160 train_time:40170ms step_avg:40.66ms
step:989/2160 train_time:40231ms step_avg:40.68ms
step:990/2160 train_time:40290ms step_avg:40.70ms
step:991/2160 train_time:40352ms step_avg:40.72ms
step:992/2160 train_time:40411ms step_avg:40.74ms
step:993/2160 train_time:40472ms step_avg:40.76ms
step:994/2160 train_time:40531ms step_avg:40.78ms
step:995/2160 train_time:40592ms step_avg:40.80ms
step:996/2160 train_time:40651ms step_avg:40.81ms
step:997/2160 train_time:40712ms step_avg:40.83ms
step:998/2160 train_time:40771ms step_avg:40.85ms
step:999/2160 train_time:40832ms step_avg:40.87ms
step:1000/2160 train_time:40891ms step_avg:40.89ms
step:1000/2160 val_loss:3.7122 train_time:40954ms step_avg:40.95ms
step:1001/2160 train_time:40977ms step_avg:40.94ms
step:1002/2160 train_time:41014ms step_avg:40.93ms
step:1003/2160 train_time:41079ms step_avg:40.96ms
step:1004/2160 train_time:41143ms step_avg:40.98ms
step:1005/2160 train_time:41205ms step_avg:41.00ms
step:1006/2160 train_time:41265ms step_avg:41.02ms
step:1007/2160 train_time:41325ms step_avg:41.04ms
step:1008/2160 train_time:41384ms step_avg:41.06ms
step:1009/2160 train_time:41444ms step_avg:41.07ms
step:1010/2160 train_time:41503ms step_avg:41.09ms
step:1011/2160 train_time:41563ms step_avg:41.11ms
step:1012/2160 train_time:41622ms step_avg:41.13ms
step:1013/2160 train_time:41681ms step_avg:41.15ms
step:1014/2160 train_time:41740ms step_avg:41.16ms
step:1015/2160 train_time:41800ms step_avg:41.18ms
step:1016/2160 train_time:41858ms step_avg:41.20ms
step:1017/2160 train_time:41919ms step_avg:41.22ms
step:1018/2160 train_time:41980ms step_avg:41.24ms
step:1019/2160 train_time:42043ms step_avg:41.26ms
step:1020/2160 train_time:42106ms step_avg:41.28ms
step:1021/2160 train_time:42169ms step_avg:41.30ms
step:1022/2160 train_time:42227ms step_avg:41.32ms
step:1023/2160 train_time:42288ms step_avg:41.34ms
step:1024/2160 train_time:42346ms step_avg:41.35ms
step:1025/2160 train_time:42406ms step_avg:41.37ms
step:1026/2160 train_time:42465ms step_avg:41.39ms
step:1027/2160 train_time:42524ms step_avg:41.41ms
step:1028/2160 train_time:42584ms step_avg:41.42ms
step:1029/2160 train_time:42643ms step_avg:41.44ms
step:1030/2160 train_time:42702ms step_avg:41.46ms
step:1031/2160 train_time:42761ms step_avg:41.48ms
step:1032/2160 train_time:42820ms step_avg:41.49ms
step:1033/2160 train_time:42881ms step_avg:41.51ms
step:1034/2160 train_time:42941ms step_avg:41.53ms
step:1035/2160 train_time:43002ms step_avg:41.55ms
step:1036/2160 train_time:43063ms step_avg:41.57ms
step:1037/2160 train_time:43126ms step_avg:41.59ms
step:1038/2160 train_time:43185ms step_avg:41.60ms
step:1039/2160 train_time:43246ms step_avg:41.62ms
step:1040/2160 train_time:43306ms step_avg:41.64ms
step:1041/2160 train_time:43366ms step_avg:41.66ms
step:1042/2160 train_time:43425ms step_avg:41.67ms
step:1043/2160 train_time:43484ms step_avg:41.69ms
step:1044/2160 train_time:43543ms step_avg:41.71ms
step:1045/2160 train_time:43603ms step_avg:41.73ms
step:1046/2160 train_time:43662ms step_avg:41.74ms
step:1047/2160 train_time:43722ms step_avg:41.76ms
step:1048/2160 train_time:43782ms step_avg:41.78ms
step:1049/2160 train_time:43841ms step_avg:41.79ms
step:1050/2160 train_time:43901ms step_avg:41.81ms
step:1051/2160 train_time:43963ms step_avg:41.83ms
step:1052/2160 train_time:44023ms step_avg:41.85ms
step:1053/2160 train_time:44086ms step_avg:41.87ms
step:1054/2160 train_time:44146ms step_avg:41.88ms
step:1055/2160 train_time:44206ms step_avg:41.90ms
step:1056/2160 train_time:44266ms step_avg:41.92ms
step:1057/2160 train_time:44327ms step_avg:41.94ms
step:1058/2160 train_time:44386ms step_avg:41.95ms
step:1059/2160 train_time:44446ms step_avg:41.97ms
step:1060/2160 train_time:44504ms step_avg:41.99ms
step:1061/2160 train_time:44565ms step_avg:42.00ms
step:1062/2160 train_time:44623ms step_avg:42.02ms
step:1063/2160 train_time:44683ms step_avg:42.03ms
step:1064/2160 train_time:44742ms step_avg:42.05ms
step:1065/2160 train_time:44803ms step_avg:42.07ms
step:1066/2160 train_time:44862ms step_avg:42.08ms
step:1067/2160 train_time:44924ms step_avg:42.10ms
step:1068/2160 train_time:44984ms step_avg:42.12ms
step:1069/2160 train_time:45046ms step_avg:42.14ms
step:1070/2160 train_time:45106ms step_avg:42.15ms
step:1071/2160 train_time:45168ms step_avg:42.17ms
step:1072/2160 train_time:45227ms step_avg:42.19ms
step:1073/2160 train_time:45287ms step_avg:42.21ms
step:1074/2160 train_time:45346ms step_avg:42.22ms
step:1075/2160 train_time:45406ms step_avg:42.24ms
step:1076/2160 train_time:45464ms step_avg:42.25ms
step:1077/2160 train_time:45525ms step_avg:42.27ms
step:1078/2160 train_time:45584ms step_avg:42.29ms
step:1079/2160 train_time:45644ms step_avg:42.30ms
step:1080/2160 train_time:45703ms step_avg:42.32ms
step:1081/2160 train_time:45764ms step_avg:42.33ms
step:1082/2160 train_time:45823ms step_avg:42.35ms
step:1083/2160 train_time:45884ms step_avg:42.37ms
step:1084/2160 train_time:45943ms step_avg:42.38ms
step:1085/2160 train_time:46005ms step_avg:42.40ms
step:1086/2160 train_time:46065ms step_avg:42.42ms
step:1087/2160 train_time:46126ms step_avg:42.43ms
step:1088/2160 train_time:46186ms step_avg:42.45ms
step:1089/2160 train_time:46246ms step_avg:42.47ms
step:1090/2160 train_time:46305ms step_avg:42.48ms
step:1091/2160 train_time:46365ms step_avg:42.50ms
step:1092/2160 train_time:46425ms step_avg:42.51ms
step:1093/2160 train_time:46485ms step_avg:42.53ms
step:1094/2160 train_time:46543ms step_avg:42.54ms
step:1095/2160 train_time:46604ms step_avg:42.56ms
step:1096/2160 train_time:46664ms step_avg:42.58ms
step:1097/2160 train_time:46725ms step_avg:42.59ms
step:1098/2160 train_time:46784ms step_avg:42.61ms
step:1099/2160 train_time:46844ms step_avg:42.62ms
step:1100/2160 train_time:46903ms step_avg:42.64ms
step:1101/2160 train_time:46964ms step_avg:42.66ms
step:1102/2160 train_time:47024ms step_avg:42.67ms
step:1103/2160 train_time:47085ms step_avg:42.69ms
step:1104/2160 train_time:47144ms step_avg:42.70ms
step:1105/2160 train_time:47205ms step_avg:42.72ms
step:1106/2160 train_time:47265ms step_avg:42.74ms
step:1107/2160 train_time:47326ms step_avg:42.75ms
step:1108/2160 train_time:47386ms step_avg:42.77ms
step:1109/2160 train_time:47445ms step_avg:42.78ms
step:1110/2160 train_time:47505ms step_avg:42.80ms
step:1111/2160 train_time:47567ms step_avg:42.81ms
step:1112/2160 train_time:47625ms step_avg:42.83ms
step:1113/2160 train_time:47686ms step_avg:42.84ms
step:1114/2160 train_time:47744ms step_avg:42.86ms
step:1115/2160 train_time:47804ms step_avg:42.87ms
step:1116/2160 train_time:47864ms step_avg:42.89ms
step:1117/2160 train_time:47925ms step_avg:42.90ms
step:1118/2160 train_time:47984ms step_avg:42.92ms
step:1119/2160 train_time:48045ms step_avg:42.94ms
step:1120/2160 train_time:48104ms step_avg:42.95ms
step:1121/2160 train_time:48165ms step_avg:42.97ms
step:1122/2160 train_time:48224ms step_avg:42.98ms
step:1123/2160 train_time:48285ms step_avg:43.00ms
step:1124/2160 train_time:48344ms step_avg:43.01ms
step:1125/2160 train_time:48404ms step_avg:43.03ms
step:1126/2160 train_time:48464ms step_avg:43.04ms
step:1127/2160 train_time:48525ms step_avg:43.06ms
step:1128/2160 train_time:48585ms step_avg:43.07ms
step:1129/2160 train_time:48645ms step_avg:43.09ms
step:1130/2160 train_time:48704ms step_avg:43.10ms
step:1131/2160 train_time:48764ms step_avg:43.12ms
step:1132/2160 train_time:48824ms step_avg:43.13ms
step:1133/2160 train_time:48885ms step_avg:43.15ms
step:1134/2160 train_time:48944ms step_avg:43.16ms
step:1135/2160 train_time:49007ms step_avg:43.18ms
step:1136/2160 train_time:49066ms step_avg:43.19ms
step:1137/2160 train_time:49127ms step_avg:43.21ms
step:1138/2160 train_time:49186ms step_avg:43.22ms
step:1139/2160 train_time:49246ms step_avg:43.24ms
step:1140/2160 train_time:49305ms step_avg:43.25ms
step:1141/2160 train_time:49366ms step_avg:43.27ms
step:1142/2160 train_time:49425ms step_avg:43.28ms
step:1143/2160 train_time:49486ms step_avg:43.30ms
step:1144/2160 train_time:49546ms step_avg:43.31ms
step:1145/2160 train_time:49606ms step_avg:43.32ms
step:1146/2160 train_time:49665ms step_avg:43.34ms
step:1147/2160 train_time:49725ms step_avg:43.35ms
step:1148/2160 train_time:49785ms step_avg:43.37ms
step:1149/2160 train_time:49845ms step_avg:43.38ms
step:1150/2160 train_time:49904ms step_avg:43.39ms
step:1151/2160 train_time:49966ms step_avg:43.41ms
step:1152/2160 train_time:50025ms step_avg:43.42ms
step:1153/2160 train_time:50086ms step_avg:43.44ms
step:1154/2160 train_time:50145ms step_avg:43.45ms
step:1155/2160 train_time:50206ms step_avg:43.47ms
step:1156/2160 train_time:50266ms step_avg:43.48ms
step:1157/2160 train_time:50326ms step_avg:43.50ms
step:1158/2160 train_time:50386ms step_avg:43.51ms
step:1159/2160 train_time:50446ms step_avg:43.53ms
step:1160/2160 train_time:50505ms step_avg:43.54ms
step:1161/2160 train_time:50566ms step_avg:43.55ms
step:1162/2160 train_time:50625ms step_avg:43.57ms
step:1163/2160 train_time:50685ms step_avg:43.58ms
step:1164/2160 train_time:50745ms step_avg:43.60ms
step:1165/2160 train_time:50805ms step_avg:43.61ms
step:1166/2160 train_time:50864ms step_avg:43.62ms
step:1167/2160 train_time:50926ms step_avg:43.64ms
step:1168/2160 train_time:50985ms step_avg:43.65ms
step:1169/2160 train_time:51045ms step_avg:43.67ms
step:1170/2160 train_time:51104ms step_avg:43.68ms
step:1171/2160 train_time:51165ms step_avg:43.69ms
step:1172/2160 train_time:51225ms step_avg:43.71ms
step:1173/2160 train_time:51286ms step_avg:43.72ms
step:1174/2160 train_time:51345ms step_avg:43.73ms
step:1175/2160 train_time:51405ms step_avg:43.75ms
step:1176/2160 train_time:51465ms step_avg:43.76ms
step:1177/2160 train_time:51526ms step_avg:43.78ms
step:1178/2160 train_time:51585ms step_avg:43.79ms
step:1179/2160 train_time:51645ms step_avg:43.80ms
step:1180/2160 train_time:51705ms step_avg:43.82ms
step:1181/2160 train_time:51765ms step_avg:43.83ms
step:1182/2160 train_time:51824ms step_avg:43.84ms
step:1183/2160 train_time:51885ms step_avg:43.86ms
step:1184/2160 train_time:51944ms step_avg:43.87ms
step:1185/2160 train_time:52006ms step_avg:43.89ms
step:1186/2160 train_time:52065ms step_avg:43.90ms
step:1187/2160 train_time:52125ms step_avg:43.91ms
step:1188/2160 train_time:52185ms step_avg:43.93ms
step:1189/2160 train_time:52246ms step_avg:43.94ms
step:1190/2160 train_time:52305ms step_avg:43.95ms
step:1191/2160 train_time:52367ms step_avg:43.97ms
step:1192/2160 train_time:52425ms step_avg:43.98ms
step:1193/2160 train_time:52486ms step_avg:44.00ms
step:1194/2160 train_time:52546ms step_avg:44.01ms
step:1195/2160 train_time:52606ms step_avg:44.02ms
step:1196/2160 train_time:52665ms step_avg:44.03ms
step:1197/2160 train_time:52726ms step_avg:44.05ms
step:1198/2160 train_time:52786ms step_avg:44.06ms
step:1199/2160 train_time:52846ms step_avg:44.08ms
step:1200/2160 train_time:52905ms step_avg:44.09ms
step:1201/2160 train_time:52965ms step_avg:44.10ms
step:1202/2160 train_time:53025ms step_avg:44.11ms
step:1203/2160 train_time:53086ms step_avg:44.13ms
step:1204/2160 train_time:53145ms step_avg:44.14ms
step:1205/2160 train_time:53205ms step_avg:44.15ms
step:1206/2160 train_time:53265ms step_avg:44.17ms
step:1207/2160 train_time:53326ms step_avg:44.18ms
step:1208/2160 train_time:53386ms step_avg:44.19ms
step:1209/2160 train_time:53446ms step_avg:44.21ms
step:1210/2160 train_time:53506ms step_avg:44.22ms
step:1211/2160 train_time:53566ms step_avg:44.23ms
step:1212/2160 train_time:53625ms step_avg:44.25ms
step:1213/2160 train_time:53686ms step_avg:44.26ms
step:1214/2160 train_time:53745ms step_avg:44.27ms
step:1215/2160 train_time:53806ms step_avg:44.28ms
step:1216/2160 train_time:53866ms step_avg:44.30ms
step:1217/2160 train_time:53926ms step_avg:44.31ms
step:1218/2160 train_time:53985ms step_avg:44.32ms
step:1219/2160 train_time:54046ms step_avg:44.34ms
step:1220/2160 train_time:54105ms step_avg:44.35ms
step:1221/2160 train_time:54166ms step_avg:44.36ms
step:1222/2160 train_time:54225ms step_avg:44.37ms
step:1223/2160 train_time:54285ms step_avg:44.39ms
step:1224/2160 train_time:54344ms step_avg:44.40ms
step:1225/2160 train_time:54405ms step_avg:44.41ms
step:1226/2160 train_time:54465ms step_avg:44.43ms
step:1227/2160 train_time:54526ms step_avg:44.44ms
step:1228/2160 train_time:54585ms step_avg:44.45ms
step:1229/2160 train_time:54645ms step_avg:44.46ms
step:1230/2160 train_time:54705ms step_avg:44.48ms
step:1231/2160 train_time:54766ms step_avg:44.49ms
step:1232/2160 train_time:54825ms step_avg:44.50ms
step:1233/2160 train_time:54886ms step_avg:44.51ms
step:1234/2160 train_time:54945ms step_avg:44.53ms
step:1235/2160 train_time:55005ms step_avg:44.54ms
step:1236/2160 train_time:55065ms step_avg:44.55ms
step:1237/2160 train_time:55126ms step_avg:44.56ms
step:1238/2160 train_time:55185ms step_avg:44.58ms
step:1239/2160 train_time:55245ms step_avg:44.59ms
step:1240/2160 train_time:55304ms step_avg:44.60ms
step:1241/2160 train_time:55365ms step_avg:44.61ms
step:1242/2160 train_time:55424ms step_avg:44.63ms
step:1243/2160 train_time:55486ms step_avg:44.64ms
step:1244/2160 train_time:55545ms step_avg:44.65ms
step:1245/2160 train_time:55605ms step_avg:44.66ms
step:1246/2160 train_time:55664ms step_avg:44.67ms
step:1247/2160 train_time:55725ms step_avg:44.69ms
step:1248/2160 train_time:55785ms step_avg:44.70ms
step:1249/2160 train_time:55845ms step_avg:44.71ms
step:1250/2160 train_time:55905ms step_avg:44.72ms
step:1250/2160 val_loss:3.5940 train_time:55967ms step_avg:44.77ms
step:1251/2160 train_time:55990ms step_avg:44.76ms
step:1252/2160 train_time:56028ms step_avg:44.75ms
step:1253/2160 train_time:56090ms step_avg:44.76ms
step:1254/2160 train_time:56154ms step_avg:44.78ms
step:1255/2160 train_time:56214ms step_avg:44.79ms
step:1256/2160 train_time:56273ms step_avg:44.80ms
step:1257/2160 train_time:56333ms step_avg:44.82ms
step:1258/2160 train_time:56391ms step_avg:44.83ms
step:1259/2160 train_time:56450ms step_avg:44.84ms
step:1260/2160 train_time:56508ms step_avg:44.85ms
step:1261/2160 train_time:56568ms step_avg:44.86ms
step:1262/2160 train_time:56627ms step_avg:44.87ms
step:1263/2160 train_time:56687ms step_avg:44.88ms
step:1264/2160 train_time:56745ms step_avg:44.89ms
step:1265/2160 train_time:56805ms step_avg:44.91ms
step:1266/2160 train_time:56864ms step_avg:44.92ms
step:1267/2160 train_time:56926ms step_avg:44.93ms
step:1268/2160 train_time:56987ms step_avg:44.94ms
step:1269/2160 train_time:57050ms step_avg:44.96ms
step:1270/2160 train_time:57112ms step_avg:44.97ms
step:1271/2160 train_time:57174ms step_avg:44.98ms
step:1272/2160 train_time:57232ms step_avg:44.99ms
step:1273/2160 train_time:57292ms step_avg:45.01ms
step:1274/2160 train_time:57351ms step_avg:45.02ms
step:1275/2160 train_time:57411ms step_avg:45.03ms
step:1276/2160 train_time:57469ms step_avg:45.04ms
step:1277/2160 train_time:57529ms step_avg:45.05ms
step:1278/2160 train_time:57588ms step_avg:45.06ms
step:1279/2160 train_time:57647ms step_avg:45.07ms
step:1280/2160 train_time:57706ms step_avg:45.08ms
step:1281/2160 train_time:57766ms step_avg:45.09ms
step:1282/2160 train_time:57825ms step_avg:45.11ms
step:1283/2160 train_time:57886ms step_avg:45.12ms
step:1284/2160 train_time:57946ms step_avg:45.13ms
step:1285/2160 train_time:58009ms step_avg:45.14ms
step:1286/2160 train_time:58070ms step_avg:45.16ms
step:1287/2160 train_time:58133ms step_avg:45.17ms
step:1288/2160 train_time:58192ms step_avg:45.18ms
step:1289/2160 train_time:58253ms step_avg:45.19ms
step:1290/2160 train_time:58312ms step_avg:45.20ms
step:1291/2160 train_time:58372ms step_avg:45.21ms
step:1292/2160 train_time:58431ms step_avg:45.23ms
step:1293/2160 train_time:58490ms step_avg:45.24ms
step:1294/2160 train_time:58549ms step_avg:45.25ms
step:1295/2160 train_time:58609ms step_avg:45.26ms
step:1296/2160 train_time:58668ms step_avg:45.27ms
step:1297/2160 train_time:58728ms step_avg:45.28ms
step:1298/2160 train_time:58787ms step_avg:45.29ms
step:1299/2160 train_time:58848ms step_avg:45.30ms
step:1300/2160 train_time:58907ms step_avg:45.31ms
step:1301/2160 train_time:58969ms step_avg:45.33ms
step:1302/2160 train_time:59030ms step_avg:45.34ms
step:1303/2160 train_time:59091ms step_avg:45.35ms
step:1304/2160 train_time:59151ms step_avg:45.36ms
step:1305/2160 train_time:59212ms step_avg:45.37ms
step:1306/2160 train_time:59271ms step_avg:45.38ms
step:1307/2160 train_time:59332ms step_avg:45.40ms
step:1308/2160 train_time:59391ms step_avg:45.41ms
step:1309/2160 train_time:59451ms step_avg:45.42ms
step:1310/2160 train_time:59510ms step_avg:45.43ms
step:1311/2160 train_time:59570ms step_avg:45.44ms
step:1312/2160 train_time:59629ms step_avg:45.45ms
step:1313/2160 train_time:59688ms step_avg:45.46ms
step:1314/2160 train_time:59748ms step_avg:45.47ms
step:1315/2160 train_time:59808ms step_avg:45.48ms
step:1316/2160 train_time:59867ms step_avg:45.49ms
step:1317/2160 train_time:59928ms step_avg:45.50ms
step:1318/2160 train_time:59989ms step_avg:45.51ms
step:1319/2160 train_time:60051ms step_avg:45.53ms
step:1320/2160 train_time:60110ms step_avg:45.54ms
step:1321/2160 train_time:60171ms step_avg:45.55ms
step:1322/2160 train_time:60230ms step_avg:45.56ms
step:1323/2160 train_time:60291ms step_avg:45.57ms
step:1324/2160 train_time:60351ms step_avg:45.58ms
step:1325/2160 train_time:60411ms step_avg:45.59ms
step:1326/2160 train_time:60470ms step_avg:45.60ms
step:1327/2160 train_time:60529ms step_avg:45.61ms
step:1328/2160 train_time:60588ms step_avg:45.62ms
step:1329/2160 train_time:60648ms step_avg:45.63ms
step:1330/2160 train_time:60706ms step_avg:45.64ms
step:1331/2160 train_time:60767ms step_avg:45.65ms
step:1332/2160 train_time:60826ms step_avg:45.67ms
step:1333/2160 train_time:60886ms step_avg:45.68ms
step:1334/2160 train_time:60946ms step_avg:45.69ms
step:1335/2160 train_time:61009ms step_avg:45.70ms
step:1336/2160 train_time:61069ms step_avg:45.71ms
step:1337/2160 train_time:61131ms step_avg:45.72ms
step:1338/2160 train_time:61190ms step_avg:45.73ms
step:1339/2160 train_time:61251ms step_avg:45.74ms
step:1340/2160 train_time:61310ms step_avg:45.75ms
step:1341/2160 train_time:61371ms step_avg:45.76ms
step:1342/2160 train_time:61430ms step_avg:45.78ms
step:1343/2160 train_time:61490ms step_avg:45.79ms
step:1344/2160 train_time:61549ms step_avg:45.80ms
step:1345/2160 train_time:61610ms step_avg:45.81ms
step:1346/2160 train_time:61668ms step_avg:45.82ms
step:1347/2160 train_time:61728ms step_avg:45.83ms
step:1348/2160 train_time:61787ms step_avg:45.84ms
step:1349/2160 train_time:61848ms step_avg:45.85ms
step:1350/2160 train_time:61909ms step_avg:45.86ms
step:1351/2160 train_time:61971ms step_avg:45.87ms
step:1352/2160 train_time:62031ms step_avg:45.88ms
step:1353/2160 train_time:62091ms step_avg:45.89ms
step:1354/2160 train_time:62150ms step_avg:45.90ms
step:1355/2160 train_time:62211ms step_avg:45.91ms
step:1356/2160 train_time:62270ms step_avg:45.92ms
step:1357/2160 train_time:62332ms step_avg:45.93ms
step:1358/2160 train_time:62390ms step_avg:45.94ms
step:1359/2160 train_time:62450ms step_avg:45.95ms
step:1360/2160 train_time:62509ms step_avg:45.96ms
step:1361/2160 train_time:62570ms step_avg:45.97ms
step:1362/2160 train_time:62628ms step_avg:45.98ms
step:1363/2160 train_time:62688ms step_avg:45.99ms
step:1364/2160 train_time:62747ms step_avg:46.00ms
step:1365/2160 train_time:62808ms step_avg:46.01ms
step:1366/2160 train_time:62867ms step_avg:46.02ms
step:1367/2160 train_time:62928ms step_avg:46.03ms
step:1368/2160 train_time:62987ms step_avg:46.04ms
step:1369/2160 train_time:63048ms step_avg:46.05ms
step:1370/2160 train_time:63108ms step_avg:46.06ms
step:1371/2160 train_time:63169ms step_avg:46.08ms
step:1372/2160 train_time:63229ms step_avg:46.09ms
step:1373/2160 train_time:63289ms step_avg:46.10ms
step:1374/2160 train_time:63349ms step_avg:46.11ms
step:1375/2160 train_time:63409ms step_avg:46.12ms
step:1376/2160 train_time:63468ms step_avg:46.13ms
step:1377/2160 train_time:63529ms step_avg:46.14ms
step:1378/2160 train_time:63587ms step_avg:46.14ms
step:1379/2160 train_time:63648ms step_avg:46.15ms
step:1380/2160 train_time:63707ms step_avg:46.16ms
step:1381/2160 train_time:63768ms step_avg:46.18ms
step:1382/2160 train_time:63828ms step_avg:46.18ms
step:1383/2160 train_time:63888ms step_avg:46.20ms
step:1384/2160 train_time:63948ms step_avg:46.21ms
step:1385/2160 train_time:64010ms step_avg:46.22ms
step:1386/2160 train_time:64070ms step_avg:46.23ms
step:1387/2160 train_time:64131ms step_avg:46.24ms
step:1388/2160 train_time:64191ms step_avg:46.25ms
step:1389/2160 train_time:64252ms step_avg:46.26ms
step:1390/2160 train_time:64311ms step_avg:46.27ms
step:1391/2160 train_time:64371ms step_avg:46.28ms
step:1392/2160 train_time:64430ms step_avg:46.29ms
step:1393/2160 train_time:64490ms step_avg:46.30ms
step:1394/2160 train_time:64549ms step_avg:46.31ms
step:1395/2160 train_time:64609ms step_avg:46.31ms
step:1396/2160 train_time:64668ms step_avg:46.32ms
step:1397/2160 train_time:64729ms step_avg:46.33ms
step:1398/2160 train_time:64788ms step_avg:46.34ms
step:1399/2160 train_time:64849ms step_avg:46.35ms
step:1400/2160 train_time:64909ms step_avg:46.36ms
step:1401/2160 train_time:64971ms step_avg:46.37ms
step:1402/2160 train_time:65030ms step_avg:46.38ms
step:1403/2160 train_time:65091ms step_avg:46.39ms
step:1404/2160 train_time:65150ms step_avg:46.40ms
step:1405/2160 train_time:65211ms step_avg:46.41ms
step:1406/2160 train_time:65270ms step_avg:46.42ms
step:1407/2160 train_time:65331ms step_avg:46.43ms
step:1408/2160 train_time:65390ms step_avg:46.44ms
step:1409/2160 train_time:65450ms step_avg:46.45ms
step:1410/2160 train_time:65509ms step_avg:46.46ms
step:1411/2160 train_time:65570ms step_avg:46.47ms
step:1412/2160 train_time:65629ms step_avg:46.48ms
step:1413/2160 train_time:65689ms step_avg:46.49ms
step:1414/2160 train_time:65748ms step_avg:46.50ms
step:1415/2160 train_time:65809ms step_avg:46.51ms
step:1416/2160 train_time:65897ms step_avg:46.54ms
step:1417/2160 train_time:65986ms step_avg:46.57ms
step:1418/2160 train_time:66073ms step_avg:46.60ms
step:1419/2160 train_time:66163ms step_avg:46.63ms
step:1420/2160 train_time:66251ms step_avg:46.66ms
step:1421/2160 train_time:66341ms step_avg:46.69ms
step:1422/2160 train_time:66426ms step_avg:46.71ms
step:1423/2160 train_time:66513ms step_avg:46.74ms
step:1424/2160 train_time:66600ms step_avg:46.77ms
step:1425/2160 train_time:66688ms step_avg:46.80ms
step:1426/2160 train_time:66774ms step_avg:46.83ms
step:1427/2160 train_time:66863ms step_avg:46.86ms
step:1428/2160 train_time:66950ms step_avg:46.88ms
step:1429/2160 train_time:67040ms step_avg:46.91ms
step:1430/2160 train_time:67125ms step_avg:46.94ms
step:1431/2160 train_time:67215ms step_avg:46.97ms
step:1432/2160 train_time:67302ms step_avg:47.00ms
step:1433/2160 train_time:67390ms step_avg:47.03ms
step:1434/2160 train_time:67477ms step_avg:47.06ms
step:1435/2160 train_time:67566ms step_avg:47.08ms
step:1436/2160 train_time:67652ms step_avg:47.11ms
step:1437/2160 train_time:67740ms step_avg:47.14ms
step:1438/2160 train_time:67826ms step_avg:47.17ms
step:1439/2160 train_time:67915ms step_avg:47.20ms
step:1440/2160 train_time:68003ms step_avg:47.22ms
step:1441/2160 train_time:68091ms step_avg:47.25ms
step:1442/2160 train_time:68179ms step_avg:47.28ms
step:1443/2160 train_time:68267ms step_avg:47.31ms
step:1444/2160 train_time:68354ms step_avg:47.34ms
step:1445/2160 train_time:68444ms step_avg:47.37ms
step:1446/2160 train_time:68530ms step_avg:47.39ms
step:1447/2160 train_time:68618ms step_avg:47.42ms
step:1448/2160 train_time:68704ms step_avg:47.45ms
step:1449/2160 train_time:68792ms step_avg:47.48ms
step:1450/2160 train_time:68880ms step_avg:47.50ms
step:1451/2160 train_time:68968ms step_avg:47.53ms
step:1452/2160 train_time:69055ms step_avg:47.56ms
step:1453/2160 train_time:69146ms step_avg:47.59ms
step:1454/2160 train_time:69232ms step_avg:47.61ms
step:1455/2160 train_time:69321ms step_avg:47.64ms
step:1456/2160 train_time:69407ms step_avg:47.67ms
step:1457/2160 train_time:69496ms step_avg:47.70ms
step:1458/2160 train_time:69583ms step_avg:47.73ms
step:1459/2160 train_time:69671ms step_avg:47.75ms
step:1460/2160 train_time:69758ms step_avg:47.78ms
step:1461/2160 train_time:69847ms step_avg:47.81ms
step:1462/2160 train_time:69934ms step_avg:47.83ms
step:1463/2160 train_time:70022ms step_avg:47.86ms
step:1464/2160 train_time:70109ms step_avg:47.89ms
step:1465/2160 train_time:70198ms step_avg:47.92ms
step:1466/2160 train_time:70285ms step_avg:47.94ms
step:1467/2160 train_time:70373ms step_avg:47.97ms
step:1468/2160 train_time:70460ms step_avg:48.00ms
step:1469/2160 train_time:70548ms step_avg:48.02ms
step:1470/2160 train_time:70634ms step_avg:48.05ms
step:1471/2160 train_time:70722ms step_avg:48.08ms
step:1472/2160 train_time:70809ms step_avg:48.10ms
step:1473/2160 train_time:70898ms step_avg:48.13ms
step:1474/2160 train_time:70984ms step_avg:48.16ms
step:1475/2160 train_time:71072ms step_avg:48.18ms
step:1476/2160 train_time:71160ms step_avg:48.21ms
step:1477/2160 train_time:71248ms step_avg:48.24ms
step:1478/2160 train_time:71335ms step_avg:48.26ms
step:1479/2160 train_time:71424ms step_avg:48.29ms
step:1480/2160 train_time:71510ms step_avg:48.32ms
step:1481/2160 train_time:71599ms step_avg:48.35ms
step:1482/2160 train_time:71685ms step_avg:48.37ms
step:1483/2160 train_time:71773ms step_avg:48.40ms
step:1484/2160 train_time:71860ms step_avg:48.42ms
step:1485/2160 train_time:71948ms step_avg:48.45ms
step:1486/2160 train_time:72035ms step_avg:48.48ms
step:1487/2160 train_time:72124ms step_avg:48.50ms
step:1488/2160 train_time:72212ms step_avg:48.53ms
step:1489/2160 train_time:72300ms step_avg:48.56ms
step:1490/2160 train_time:72386ms step_avg:48.58ms
step:1491/2160 train_time:72474ms step_avg:48.61ms
step:1492/2160 train_time:72561ms step_avg:48.63ms
step:1493/2160 train_time:72649ms step_avg:48.66ms
step:1494/2160 train_time:72736ms step_avg:48.69ms
step:1495/2160 train_time:72825ms step_avg:48.71ms
step:1496/2160 train_time:72912ms step_avg:48.74ms
step:1497/2160 train_time:73000ms step_avg:48.76ms
step:1498/2160 train_time:73086ms step_avg:48.79ms
step:1499/2160 train_time:73175ms step_avg:48.82ms
step:1500/2160 train_time:73262ms step_avg:48.84ms
step:1500/2160 val_loss:3.4965 train_time:73352ms step_avg:48.90ms
step:1501/2160 train_time:73375ms step_avg:48.88ms
step:1502/2160 train_time:73441ms step_avg:48.90ms
step:1503/2160 train_time:73535ms step_avg:48.93ms
step:1504/2160 train_time:73622ms step_avg:48.95ms
step:1505/2160 train_time:73711ms step_avg:48.98ms
step:1506/2160 train_time:73797ms step_avg:49.00ms
step:1507/2160 train_time:73884ms step_avg:49.03ms
step:1508/2160 train_time:73970ms step_avg:49.05ms
step:1509/2160 train_time:74057ms step_avg:49.08ms
step:1510/2160 train_time:74142ms step_avg:49.10ms
step:1511/2160 train_time:74229ms step_avg:49.13ms
step:1512/2160 train_time:74319ms step_avg:49.15ms
step:1513/2160 train_time:74410ms step_avg:49.18ms
step:1514/2160 train_time:74498ms step_avg:49.21ms
step:1515/2160 train_time:74589ms step_avg:49.23ms
step:1516/2160 train_time:74675ms step_avg:49.26ms
step:1517/2160 train_time:74763ms step_avg:49.28ms
step:1518/2160 train_time:74849ms step_avg:49.31ms
step:1519/2160 train_time:74937ms step_avg:49.33ms
step:1520/2160 train_time:75022ms step_avg:49.36ms
step:1521/2160 train_time:75109ms step_avg:49.38ms
step:1522/2160 train_time:75195ms step_avg:49.41ms
step:1523/2160 train_time:75284ms step_avg:49.43ms
step:1524/2160 train_time:75372ms step_avg:49.46ms
step:1525/2160 train_time:75461ms step_avg:49.48ms
step:1526/2160 train_time:75549ms step_avg:49.51ms
step:1527/2160 train_time:75639ms step_avg:49.53ms
step:1528/2160 train_time:75725ms step_avg:49.56ms
step:1529/2160 train_time:75815ms step_avg:49.58ms
step:1530/2160 train_time:75900ms step_avg:49.61ms
step:1531/2160 train_time:75989ms step_avg:49.63ms
step:1532/2160 train_time:76074ms step_avg:49.66ms
step:1533/2160 train_time:76161ms step_avg:49.68ms
step:1534/2160 train_time:76248ms step_avg:49.71ms
step:1535/2160 train_time:76337ms step_avg:49.73ms
step:1536/2160 train_time:76425ms step_avg:49.76ms
step:1537/2160 train_time:76516ms step_avg:49.78ms
step:1538/2160 train_time:76603ms step_avg:49.81ms
step:1539/2160 train_time:76693ms step_avg:49.83ms
step:1540/2160 train_time:76779ms step_avg:49.86ms
step:1541/2160 train_time:76866ms step_avg:49.88ms
step:1542/2160 train_time:76953ms step_avg:49.90ms
step:1543/2160 train_time:77040ms step_avg:49.93ms
step:1544/2160 train_time:77125ms step_avg:49.95ms
step:1545/2160 train_time:77215ms step_avg:49.98ms
step:1546/2160 train_time:77301ms step_avg:50.00ms
step:1547/2160 train_time:77390ms step_avg:50.03ms
step:1548/2160 train_time:77478ms step_avg:50.05ms
step:1549/2160 train_time:77566ms step_avg:50.07ms
step:1550/2160 train_time:77654ms step_avg:50.10ms
step:1551/2160 train_time:77743ms step_avg:50.12ms
step:1552/2160 train_time:77830ms step_avg:50.15ms
step:1553/2160 train_time:77918ms step_avg:50.17ms
step:1554/2160 train_time:78003ms step_avg:50.20ms
step:1555/2160 train_time:78091ms step_avg:50.22ms
step:1556/2160 train_time:78177ms step_avg:50.24ms
step:1557/2160 train_time:78266ms step_avg:50.27ms
step:1558/2160 train_time:78353ms step_avg:50.29ms
step:1559/2160 train_time:78442ms step_avg:50.32ms
step:1560/2160 train_time:78529ms step_avg:50.34ms
step:1561/2160 train_time:78618ms step_avg:50.36ms
step:1562/2160 train_time:78705ms step_avg:50.39ms
step:1563/2160 train_time:78795ms step_avg:50.41ms
step:1564/2160 train_time:78881ms step_avg:50.44ms
step:1565/2160 train_time:78969ms step_avg:50.46ms
step:1566/2160 train_time:79055ms step_avg:50.48ms
step:1567/2160 train_time:79143ms step_avg:50.51ms
step:1568/2160 train_time:79229ms step_avg:50.53ms
step:1569/2160 train_time:79318ms step_avg:50.55ms
step:1570/2160 train_time:79405ms step_avg:50.58ms
step:1571/2160 train_time:79495ms step_avg:50.60ms
step:1572/2160 train_time:79581ms step_avg:50.62ms
step:1573/2160 train_time:79671ms step_avg:50.65ms
step:1574/2160 train_time:79756ms step_avg:50.67ms
step:1575/2160 train_time:79844ms step_avg:50.69ms
step:1576/2160 train_time:79931ms step_avg:50.72ms
step:1577/2160 train_time:80019ms step_avg:50.74ms
step:1578/2160 train_time:80105ms step_avg:50.76ms
step:1579/2160 train_time:80194ms step_avg:50.79ms
step:1580/2160 train_time:80280ms step_avg:50.81ms
step:1581/2160 train_time:80369ms step_avg:50.83ms
step:1582/2160 train_time:80455ms step_avg:50.86ms
step:1583/2160 train_time:80544ms step_avg:50.88ms
step:1584/2160 train_time:80632ms step_avg:50.90ms
step:1585/2160 train_time:80720ms step_avg:50.93ms
step:1586/2160 train_time:80807ms step_avg:50.95ms
step:1587/2160 train_time:80896ms step_avg:50.97ms
step:1588/2160 train_time:80983ms step_avg:51.00ms
step:1589/2160 train_time:81072ms step_avg:51.02ms
step:1590/2160 train_time:81158ms step_avg:51.04ms
step:1591/2160 train_time:81247ms step_avg:51.07ms
step:1592/2160 train_time:81334ms step_avg:51.09ms
step:1593/2160 train_time:81423ms step_avg:51.11ms
step:1594/2160 train_time:81510ms step_avg:51.14ms
step:1595/2160 train_time:81599ms step_avg:51.16ms
step:1596/2160 train_time:81685ms step_avg:51.18ms
step:1597/2160 train_time:81774ms step_avg:51.20ms
step:1598/2160 train_time:81860ms step_avg:51.23ms
step:1599/2160 train_time:81949ms step_avg:51.25ms
step:1600/2160 train_time:82036ms step_avg:51.27ms
step:1601/2160 train_time:82124ms step_avg:51.30ms
step:1602/2160 train_time:82212ms step_avg:51.32ms
step:1603/2160 train_time:82300ms step_avg:51.34ms
step:1604/2160 train_time:82387ms step_avg:51.36ms
step:1605/2160 train_time:82476ms step_avg:51.39ms
step:1606/2160 train_time:82563ms step_avg:51.41ms
step:1607/2160 train_time:82651ms step_avg:51.43ms
step:1608/2160 train_time:82737ms step_avg:51.45ms
step:1609/2160 train_time:82825ms step_avg:51.48ms
step:1610/2160 train_time:82912ms step_avg:51.50ms
step:1611/2160 train_time:83000ms step_avg:51.52ms
step:1612/2160 train_time:83087ms step_avg:51.54ms
step:1613/2160 train_time:83176ms step_avg:51.57ms
step:1614/2160 train_time:83262ms step_avg:51.59ms
step:1615/2160 train_time:83351ms step_avg:51.61ms
step:1616/2160 train_time:83437ms step_avg:51.63ms
step:1617/2160 train_time:83526ms step_avg:51.65ms
step:1618/2160 train_time:83613ms step_avg:51.68ms
step:1619/2160 train_time:83701ms step_avg:51.70ms
step:1620/2160 train_time:83788ms step_avg:51.72ms
step:1621/2160 train_time:83877ms step_avg:51.74ms
step:1622/2160 train_time:83963ms step_avg:51.77ms
step:1623/2160 train_time:84052ms step_avg:51.79ms
step:1624/2160 train_time:84138ms step_avg:51.81ms
step:1625/2160 train_time:84225ms step_avg:51.83ms
step:1626/2160 train_time:84315ms step_avg:51.85ms
step:1627/2160 train_time:84403ms step_avg:51.88ms
step:1628/2160 train_time:84490ms step_avg:51.90ms
step:1629/2160 train_time:84579ms step_avg:51.92ms
step:1630/2160 train_time:84666ms step_avg:51.94ms
step:1631/2160 train_time:84755ms step_avg:51.97ms
step:1632/2160 train_time:84841ms step_avg:51.99ms
step:1633/2160 train_time:84930ms step_avg:52.01ms
step:1634/2160 train_time:85016ms step_avg:52.03ms
step:1635/2160 train_time:85103ms step_avg:52.05ms
step:1636/2160 train_time:85190ms step_avg:52.07ms
step:1637/2160 train_time:85278ms step_avg:52.09ms
step:1638/2160 train_time:85365ms step_avg:52.12ms
step:1639/2160 train_time:85455ms step_avg:52.14ms
step:1640/2160 train_time:85541ms step_avg:52.16ms
step:1641/2160 train_time:85630ms step_avg:52.18ms
step:1642/2160 train_time:85716ms step_avg:52.20ms
step:1643/2160 train_time:85804ms step_avg:52.22ms
step:1644/2160 train_time:85892ms step_avg:52.25ms
step:1645/2160 train_time:85980ms step_avg:52.27ms
step:1646/2160 train_time:86066ms step_avg:52.29ms
step:1647/2160 train_time:86154ms step_avg:52.31ms
step:1648/2160 train_time:86240ms step_avg:52.33ms
step:1649/2160 train_time:86329ms step_avg:52.35ms
step:1650/2160 train_time:86416ms step_avg:52.37ms
step:1651/2160 train_time:86505ms step_avg:52.40ms
step:1652/2160 train_time:86591ms step_avg:52.42ms
step:1653/2160 train_time:86680ms step_avg:52.44ms
step:1654/2160 train_time:86766ms step_avg:52.46ms
step:1655/2160 train_time:86855ms step_avg:52.48ms
step:1656/2160 train_time:86941ms step_avg:52.50ms
step:1657/2160 train_time:87029ms step_avg:52.52ms
step:1658/2160 train_time:87116ms step_avg:52.54ms
step:1659/2160 train_time:87204ms step_avg:52.56ms
step:1660/2160 train_time:87291ms step_avg:52.59ms
step:1661/2160 train_time:87381ms step_avg:52.61ms
step:1662/2160 train_time:87468ms step_avg:52.63ms
step:1663/2160 train_time:87557ms step_avg:52.65ms
step:1664/2160 train_time:87643ms step_avg:52.67ms
step:1665/2160 train_time:87732ms step_avg:52.69ms
step:1666/2160 train_time:87818ms step_avg:52.71ms
step:1667/2160 train_time:87906ms step_avg:52.73ms
step:1668/2160 train_time:87993ms step_avg:52.75ms
step:1669/2160 train_time:88080ms step_avg:52.77ms
step:1670/2160 train_time:88166ms step_avg:52.79ms
step:1671/2160 train_time:88255ms step_avg:52.82ms
step:1672/2160 train_time:88342ms step_avg:52.84ms
step:1673/2160 train_time:88430ms step_avg:52.86ms
step:1674/2160 train_time:88516ms step_avg:52.88ms
step:1675/2160 train_time:88603ms step_avg:52.90ms
step:1676/2160 train_time:88691ms step_avg:52.92ms
step:1677/2160 train_time:88779ms step_avg:52.94ms
step:1678/2160 train_time:88866ms step_avg:52.96ms
step:1679/2160 train_time:88956ms step_avg:52.98ms
step:1680/2160 train_time:89043ms step_avg:53.00ms
step:1681/2160 train_time:89131ms step_avg:53.02ms
step:1682/2160 train_time:89218ms step_avg:53.04ms
step:1683/2160 train_time:89306ms step_avg:53.06ms
step:1684/2160 train_time:89393ms step_avg:53.08ms
step:1685/2160 train_time:89480ms step_avg:53.10ms
step:1686/2160 train_time:89567ms step_avg:53.12ms
step:1687/2160 train_time:89656ms step_avg:53.15ms
step:1688/2160 train_time:89743ms step_avg:53.17ms
step:1689/2160 train_time:89832ms step_avg:53.19ms
step:1690/2160 train_time:89918ms step_avg:53.21ms
step:1691/2160 train_time:90007ms step_avg:53.23ms
step:1692/2160 train_time:90095ms step_avg:53.25ms
step:1693/2160 train_time:90183ms step_avg:53.27ms
step:1694/2160 train_time:90270ms step_avg:53.29ms
step:1695/2160 train_time:90358ms step_avg:53.31ms
step:1696/2160 train_time:90445ms step_avg:53.33ms
step:1697/2160 train_time:90534ms step_avg:53.35ms
step:1698/2160 train_time:90620ms step_avg:53.37ms
step:1699/2160 train_time:90709ms step_avg:53.39ms
step:1700/2160 train_time:90795ms step_avg:53.41ms
step:1701/2160 train_time:90883ms step_avg:53.43ms
step:1702/2160 train_time:90971ms step_avg:53.45ms
step:1703/2160 train_time:91059ms step_avg:53.47ms
step:1704/2160 train_time:91146ms step_avg:53.49ms
step:1705/2160 train_time:91236ms step_avg:53.51ms
step:1706/2160 train_time:91322ms step_avg:53.53ms
step:1707/2160 train_time:91411ms step_avg:53.55ms
step:1708/2160 train_time:91497ms step_avg:53.57ms
step:1709/2160 train_time:91585ms step_avg:53.59ms
step:1710/2160 train_time:91672ms step_avg:53.61ms
step:1711/2160 train_time:91759ms step_avg:53.63ms
step:1712/2160 train_time:91846ms step_avg:53.65ms
step:1713/2160 train_time:91935ms step_avg:53.67ms
step:1714/2160 train_time:92022ms step_avg:53.69ms
step:1715/2160 train_time:92111ms step_avg:53.71ms
step:1716/2160 train_time:92197ms step_avg:53.73ms
step:1717/2160 train_time:92285ms step_avg:53.75ms
step:1718/2160 train_time:92372ms step_avg:53.77ms
step:1719/2160 train_time:92460ms step_avg:53.79ms
step:1720/2160 train_time:92546ms step_avg:53.81ms
step:1721/2160 train_time:92636ms step_avg:53.83ms
step:1722/2160 train_time:92722ms step_avg:53.85ms
step:1723/2160 train_time:92810ms step_avg:53.87ms
step:1724/2160 train_time:92896ms step_avg:53.88ms
step:1725/2160 train_time:92985ms step_avg:53.90ms
step:1726/2160 train_time:93071ms step_avg:53.92ms
step:1727/2160 train_time:93159ms step_avg:53.94ms
step:1728/2160 train_time:93245ms step_avg:53.96ms
step:1729/2160 train_time:93334ms step_avg:53.98ms
step:1730/2160 train_time:93420ms step_avg:54.00ms
step:1731/2160 train_time:93508ms step_avg:54.02ms
step:1732/2160 train_time:93596ms step_avg:54.04ms
step:1733/2160 train_time:93684ms step_avg:54.06ms
step:1734/2160 train_time:93771ms step_avg:54.08ms
step:1735/2160 train_time:93859ms step_avg:54.10ms
step:1736/2160 train_time:93945ms step_avg:54.12ms
step:1737/2160 train_time:94034ms step_avg:54.14ms
step:1738/2160 train_time:94120ms step_avg:54.15ms
step:1739/2160 train_time:94209ms step_avg:54.17ms
step:1740/2160 train_time:94296ms step_avg:54.19ms
step:1741/2160 train_time:94384ms step_avg:54.21ms
step:1742/2160 train_time:94471ms step_avg:54.23ms
step:1743/2160 train_time:94559ms step_avg:54.25ms
step:1744/2160 train_time:94646ms step_avg:54.27ms
step:1745/2160 train_time:94737ms step_avg:54.29ms
step:1746/2160 train_time:94824ms step_avg:54.31ms
step:1747/2160 train_time:94912ms step_avg:54.33ms
step:1748/2160 train_time:94998ms step_avg:54.35ms
step:1749/2160 train_time:95086ms step_avg:54.37ms
step:1750/2160 train_time:95173ms step_avg:54.38ms
step:1750/2160 val_loss:3.3913 train_time:95262ms step_avg:54.44ms
step:1751/2160 train_time:95285ms step_avg:54.42ms
step:1752/2160 train_time:95353ms step_avg:54.42ms
step:1753/2160 train_time:95444ms step_avg:54.45ms
step:1754/2160 train_time:95532ms step_avg:54.47ms
step:1755/2160 train_time:95619ms step_avg:54.48ms
step:1756/2160 train_time:95705ms step_avg:54.50ms
step:1757/2160 train_time:95791ms step_avg:54.52ms
step:1758/2160 train_time:95877ms step_avg:54.54ms
step:1759/2160 train_time:95965ms step_avg:54.56ms
step:1760/2160 train_time:96051ms step_avg:54.57ms
step:1761/2160 train_time:96139ms step_avg:54.59ms
step:1762/2160 train_time:96227ms step_avg:54.61ms
step:1763/2160 train_time:96318ms step_avg:54.63ms
step:1764/2160 train_time:96408ms step_avg:54.65ms
step:1765/2160 train_time:96497ms step_avg:54.67ms
step:1766/2160 train_time:96584ms step_avg:54.69ms
step:1767/2160 train_time:96671ms step_avg:54.71ms
step:1768/2160 train_time:96757ms step_avg:54.73ms
step:1769/2160 train_time:96845ms step_avg:54.75ms
step:1770/2160 train_time:96930ms step_avg:54.76ms
step:1771/2160 train_time:97018ms step_avg:54.78ms
step:1772/2160 train_time:97105ms step_avg:54.80ms
step:1773/2160 train_time:97193ms step_avg:54.82ms
step:1774/2160 train_time:97281ms step_avg:54.84ms
step:1775/2160 train_time:97372ms step_avg:54.86ms
step:1776/2160 train_time:97461ms step_avg:54.88ms
step:1777/2160 train_time:97549ms step_avg:54.90ms
step:1778/2160 train_time:97635ms step_avg:54.91ms
step:1779/2160 train_time:97722ms step_avg:54.93ms
step:1780/2160 train_time:97808ms step_avg:54.95ms
step:1781/2160 train_time:97895ms step_avg:54.97ms
step:1782/2160 train_time:97981ms step_avg:54.98ms
step:1783/2160 train_time:98069ms step_avg:55.00ms
step:1784/2160 train_time:98155ms step_avg:55.02ms
step:1785/2160 train_time:98244ms step_avg:55.04ms
step:1786/2160 train_time:98331ms step_avg:55.06ms
step:1787/2160 train_time:98422ms step_avg:55.08ms
step:1788/2160 train_time:98509ms step_avg:55.09ms
step:1789/2160 train_time:98597ms step_avg:55.11ms
step:1790/2160 train_time:98684ms step_avg:55.13ms
step:1791/2160 train_time:98771ms step_avg:55.15ms
step:1792/2160 train_time:98856ms step_avg:55.17ms
step:1793/2160 train_time:98945ms step_avg:55.18ms
step:1794/2160 train_time:99030ms step_avg:55.20ms
step:1795/2160 train_time:99118ms step_avg:55.22ms
step:1796/2160 train_time:99204ms step_avg:55.24ms
step:1797/2160 train_time:99293ms step_avg:55.25ms
step:1798/2160 train_time:99381ms step_avg:55.27ms
step:1799/2160 train_time:99470ms step_avg:55.29ms
step:1800/2160 train_time:99556ms step_avg:55.31ms
step:1801/2160 train_time:99645ms step_avg:55.33ms
step:1802/2160 train_time:99731ms step_avg:55.34ms
step:1803/2160 train_time:99818ms step_avg:55.36ms
step:1804/2160 train_time:99905ms step_avg:55.38ms
step:1805/2160 train_time:99993ms step_avg:55.40ms
step:1806/2160 train_time:100079ms step_avg:55.41ms
step:1807/2160 train_time:100168ms step_avg:55.43ms
step:1808/2160 train_time:100254ms step_avg:55.45ms
step:1809/2160 train_time:100344ms step_avg:55.47ms
step:1810/2160 train_time:100430ms step_avg:55.49ms
step:1811/2160 train_time:100519ms step_avg:55.50ms
step:1812/2160 train_time:100606ms step_avg:55.52ms
step:1813/2160 train_time:100694ms step_avg:55.54ms
step:1814/2160 train_time:100780ms step_avg:55.56ms
step:1815/2160 train_time:100868ms step_avg:55.57ms
step:1816/2160 train_time:100954ms step_avg:55.59ms
step:1817/2160 train_time:101043ms step_avg:55.61ms
step:1818/2160 train_time:101129ms step_avg:55.63ms
step:1819/2160 train_time:101218ms step_avg:55.64ms
step:1820/2160 train_time:101305ms step_avg:55.66ms
step:1821/2160 train_time:101392ms step_avg:55.68ms
step:1822/2160 train_time:101479ms step_avg:55.70ms
step:1823/2160 train_time:101568ms step_avg:55.71ms
step:1824/2160 train_time:101654ms step_avg:55.73ms
step:1825/2160 train_time:101743ms step_avg:55.75ms
step:1826/2160 train_time:101829ms step_avg:55.77ms
step:1827/2160 train_time:101917ms step_avg:55.78ms
step:1828/2160 train_time:102004ms step_avg:55.80ms
step:1829/2160 train_time:102091ms step_avg:55.82ms
step:1830/2160 train_time:102177ms step_avg:55.83ms
step:1831/2160 train_time:102266ms step_avg:55.85ms
step:1832/2160 train_time:102352ms step_avg:55.87ms
step:1833/2160 train_time:102441ms step_avg:55.89ms
step:1834/2160 train_time:102529ms step_avg:55.90ms
step:1835/2160 train_time:102617ms step_avg:55.92ms
step:1836/2160 train_time:102704ms step_avg:55.94ms
step:1837/2160 train_time:102792ms step_avg:55.96ms
step:1838/2160 train_time:102878ms step_avg:55.97ms
step:1839/2160 train_time:102967ms step_avg:55.99ms
step:1840/2160 train_time:103052ms step_avg:56.01ms
step:1841/2160 train_time:103142ms step_avg:56.02ms
step:1842/2160 train_time:103228ms step_avg:56.04ms
step:1843/2160 train_time:103316ms step_avg:56.06ms
step:1844/2160 train_time:103403ms step_avg:56.08ms
step:1845/2160 train_time:103491ms step_avg:56.09ms
step:1846/2160 train_time:103578ms step_avg:56.11ms
step:1847/2160 train_time:103668ms step_avg:56.13ms
step:1848/2160 train_time:103754ms step_avg:56.14ms
step:1849/2160 train_time:103842ms step_avg:56.16ms
step:1850/2160 train_time:103929ms step_avg:56.18ms
step:1851/2160 train_time:104017ms step_avg:56.19ms
step:1852/2160 train_time:104104ms step_avg:56.21ms
step:1853/2160 train_time:104191ms step_avg:56.23ms
step:1854/2160 train_time:104278ms step_avg:56.25ms
step:1855/2160 train_time:104367ms step_avg:56.26ms
step:1856/2160 train_time:104453ms step_avg:56.28ms
step:1857/2160 train_time:104542ms step_avg:56.30ms
step:1858/2160 train_time:104629ms step_avg:56.31ms
step:1859/2160 train_time:104716ms step_avg:56.33ms
step:1860/2160 train_time:104803ms step_avg:56.35ms
step:1861/2160 train_time:104892ms step_avg:56.36ms
step:1862/2160 train_time:104979ms step_avg:56.38ms
step:1863/2160 train_time:105068ms step_avg:56.40ms
step:1864/2160 train_time:105153ms step_avg:56.41ms
step:1865/2160 train_time:105242ms step_avg:56.43ms
step:1866/2160 train_time:105329ms step_avg:56.45ms
step:1867/2160 train_time:105416ms step_avg:56.46ms
step:1868/2160 train_time:105504ms step_avg:56.48ms
step:1869/2160 train_time:105591ms step_avg:56.50ms
step:1870/2160 train_time:105679ms step_avg:56.51ms
step:1871/2160 train_time:105768ms step_avg:56.53ms
step:1872/2160 train_time:105854ms step_avg:56.55ms
step:1873/2160 train_time:105943ms step_avg:56.56ms
step:1874/2160 train_time:106029ms step_avg:56.58ms
step:1875/2160 train_time:106116ms step_avg:56.60ms
step:1876/2160 train_time:106204ms step_avg:56.61ms
step:1877/2160 train_time:106292ms step_avg:56.63ms
step:1878/2160 train_time:106378ms step_avg:56.64ms
step:1879/2160 train_time:106468ms step_avg:56.66ms
step:1880/2160 train_time:106554ms step_avg:56.68ms
step:1881/2160 train_time:106643ms step_avg:56.69ms
step:1882/2160 train_time:106730ms step_avg:56.71ms
step:1883/2160 train_time:106818ms step_avg:56.73ms
step:1884/2160 train_time:106906ms step_avg:56.74ms
step:1885/2160 train_time:106994ms step_avg:56.76ms
step:1886/2160 train_time:107080ms step_avg:56.78ms
step:1887/2160 train_time:107169ms step_avg:56.79ms
step:1888/2160 train_time:107255ms step_avg:56.81ms
step:1889/2160 train_time:107343ms step_avg:56.83ms
step:1890/2160 train_time:107430ms step_avg:56.84ms
step:1891/2160 train_time:107517ms step_avg:56.86ms
step:1892/2160 train_time:107604ms step_avg:56.87ms
step:1893/2160 train_time:107692ms step_avg:56.89ms
step:1894/2160 train_time:107779ms step_avg:56.91ms
step:1895/2160 train_time:107867ms step_avg:56.92ms
step:1896/2160 train_time:107953ms step_avg:56.94ms
step:1897/2160 train_time:108042ms step_avg:56.95ms
step:1898/2160 train_time:108129ms step_avg:56.97ms
step:1899/2160 train_time:108218ms step_avg:56.99ms
step:1900/2160 train_time:108306ms step_avg:57.00ms
step:1901/2160 train_time:108394ms step_avg:57.02ms
step:1902/2160 train_time:108480ms step_avg:57.03ms
step:1903/2160 train_time:108570ms step_avg:57.05ms
step:1904/2160 train_time:108656ms step_avg:57.07ms
step:1905/2160 train_time:108743ms step_avg:57.08ms
step:1906/2160 train_time:108830ms step_avg:57.10ms
step:1907/2160 train_time:108918ms step_avg:57.11ms
step:1908/2160 train_time:109006ms step_avg:57.13ms
step:1909/2160 train_time:109094ms step_avg:57.15ms
step:1910/2160 train_time:109180ms step_avg:57.16ms
step:1911/2160 train_time:109270ms step_avg:57.18ms
step:1912/2160 train_time:109356ms step_avg:57.19ms
step:1913/2160 train_time:109445ms step_avg:57.21ms
step:1914/2160 train_time:109531ms step_avg:57.23ms
step:1915/2160 train_time:109619ms step_avg:57.24ms
step:1916/2160 train_time:109706ms step_avg:57.26ms
step:1917/2160 train_time:109793ms step_avg:57.27ms
step:1918/2160 train_time:109881ms step_avg:57.29ms
step:1919/2160 train_time:109970ms step_avg:57.31ms
step:1920/2160 train_time:110057ms step_avg:57.32ms
step:1921/2160 train_time:110146ms step_avg:57.34ms
step:1922/2160 train_time:110232ms step_avg:57.35ms
step:1923/2160 train_time:110320ms step_avg:57.37ms
step:1924/2160 train_time:110407ms step_avg:57.38ms
step:1925/2160 train_time:110496ms step_avg:57.40ms
step:1926/2160 train_time:110582ms step_avg:57.42ms
step:1927/2160 train_time:110670ms step_avg:57.43ms
step:1928/2160 train_time:110757ms step_avg:57.45ms
step:1929/2160 train_time:110846ms step_avg:57.46ms
step:1930/2160 train_time:110932ms step_avg:57.48ms
step:1931/2160 train_time:111020ms step_avg:57.49ms
step:1932/2160 train_time:111107ms step_avg:57.51ms
step:1933/2160 train_time:111195ms step_avg:57.52ms
step:1934/2160 train_time:111282ms step_avg:57.54ms
step:1935/2160 train_time:111370ms step_avg:57.56ms
step:1936/2160 train_time:111456ms step_avg:57.57ms
step:1937/2160 train_time:111544ms step_avg:57.59ms
step:1938/2160 train_time:111631ms step_avg:57.60ms
step:1939/2160 train_time:111717ms step_avg:57.62ms
step:1940/2160 train_time:111804ms step_avg:57.63ms
step:1941/2160 train_time:111891ms step_avg:57.65ms
step:1942/2160 train_time:111978ms step_avg:57.66ms
step:1943/2160 train_time:112068ms step_avg:57.68ms
step:1944/2160 train_time:112154ms step_avg:57.69ms
step:1945/2160 train_time:112244ms step_avg:57.71ms
step:1946/2160 train_time:112330ms step_avg:57.72ms
step:1947/2160 train_time:112418ms step_avg:57.74ms
step:1948/2160 train_time:112504ms step_avg:57.75ms
step:1949/2160 train_time:112591ms step_avg:57.77ms
step:1950/2160 train_time:112677ms step_avg:57.78ms
step:1951/2160 train_time:112767ms step_avg:57.80ms
step:1952/2160 train_time:112852ms step_avg:57.81ms
step:1953/2160 train_time:112941ms step_avg:57.83ms
step:1954/2160 train_time:113029ms step_avg:57.84ms
step:1955/2160 train_time:113116ms step_avg:57.86ms
step:1956/2160 train_time:113204ms step_avg:57.88ms
step:1957/2160 train_time:113292ms step_avg:57.89ms
step:1958/2160 train_time:113380ms step_avg:57.91ms
step:1959/2160 train_time:113469ms step_avg:57.92ms
step:1960/2160 train_time:113554ms step_avg:57.94ms
step:1961/2160 train_time:113643ms step_avg:57.95ms
step:1962/2160 train_time:113729ms step_avg:57.97ms
step:1963/2160 train_time:113817ms step_avg:57.98ms
step:1964/2160 train_time:113904ms step_avg:58.00ms
step:1965/2160 train_time:113992ms step_avg:58.01ms
step:1966/2160 train_time:114079ms step_avg:58.03ms
step:1967/2160 train_time:114167ms step_avg:58.04ms
step:1968/2160 train_time:114253ms step_avg:58.06ms
step:1969/2160 train_time:114342ms step_avg:58.07ms
step:1970/2160 train_time:114428ms step_avg:58.09ms
step:1971/2160 train_time:114516ms step_avg:58.10ms
step:1972/2160 train_time:114603ms step_avg:58.12ms
step:1973/2160 train_time:114691ms step_avg:58.13ms
step:1974/2160 train_time:114777ms step_avg:58.14ms
step:1975/2160 train_time:114866ms step_avg:58.16ms
step:1976/2160 train_time:114953ms step_avg:58.17ms
step:1977/2160 train_time:115041ms step_avg:58.19ms
step:1978/2160 train_time:115127ms step_avg:58.20ms
step:1979/2160 train_time:115215ms step_avg:58.22ms
step:1980/2160 train_time:115302ms step_avg:58.23ms
step:1981/2160 train_time:115391ms step_avg:58.25ms
step:1982/2160 train_time:115477ms step_avg:58.26ms
step:1983/2160 train_time:115565ms step_avg:58.28ms
step:1984/2160 train_time:115651ms step_avg:58.29ms
step:1985/2160 train_time:115739ms step_avg:58.31ms
step:1986/2160 train_time:115826ms step_avg:58.32ms
step:1987/2160 train_time:115913ms step_avg:58.34ms
step:1988/2160 train_time:116000ms step_avg:58.35ms
step:1989/2160 train_time:116088ms step_avg:58.37ms
step:1990/2160 train_time:116175ms step_avg:58.38ms
step:1991/2160 train_time:116264ms step_avg:58.39ms
step:1992/2160 train_time:116350ms step_avg:58.41ms
step:1993/2160 train_time:116437ms step_avg:58.42ms
step:1994/2160 train_time:116525ms step_avg:58.44ms
step:1995/2160 train_time:116613ms step_avg:58.45ms
step:1996/2160 train_time:116700ms step_avg:58.47ms
step:1997/2160 train_time:116789ms step_avg:58.48ms
step:1998/2160 train_time:116875ms step_avg:58.50ms
step:1999/2160 train_time:116963ms step_avg:58.51ms
step:2000/2160 train_time:117050ms step_avg:58.52ms
step:2000/2160 val_loss:3.3144 train_time:117139ms step_avg:58.57ms
step:2001/2160 train_time:117161ms step_avg:58.55ms
step:2002/2160 train_time:117227ms step_avg:58.56ms
step:2003/2160 train_time:117319ms step_avg:58.57ms
step:2004/2160 train_time:117406ms step_avg:58.59ms
step:2005/2160 train_time:117493ms step_avg:58.60ms
step:2006/2160 train_time:117578ms step_avg:58.61ms
step:2007/2160 train_time:117665ms step_avg:58.63ms
step:2008/2160 train_time:117751ms step_avg:58.64ms
step:2009/2160 train_time:117838ms step_avg:58.66ms
step:2010/2160 train_time:117925ms step_avg:58.67ms
step:2011/2160 train_time:118013ms step_avg:58.68ms
step:2012/2160 train_time:118100ms step_avg:58.70ms
step:2013/2160 train_time:118191ms step_avg:58.71ms
step:2014/2160 train_time:118280ms step_avg:58.73ms
step:2015/2160 train_time:118369ms step_avg:58.74ms
step:2016/2160 train_time:118456ms step_avg:58.76ms
step:2017/2160 train_time:118544ms step_avg:58.77ms
step:2018/2160 train_time:118630ms step_avg:58.79ms
step:2019/2160 train_time:118717ms step_avg:58.80ms
step:2020/2160 train_time:118803ms step_avg:58.81ms
step:2021/2160 train_time:118891ms step_avg:58.83ms
step:2022/2160 train_time:118978ms step_avg:58.84ms
step:2023/2160 train_time:119067ms step_avg:58.86ms
step:2024/2160 train_time:119155ms step_avg:58.87ms
step:2025/2160 train_time:119245ms step_avg:58.89ms
step:2026/2160 train_time:119332ms step_avg:58.90ms
step:2027/2160 train_time:119421ms step_avg:58.91ms
step:2028/2160 train_time:119507ms step_avg:58.93ms
step:2029/2160 train_time:119596ms step_avg:58.94ms
step:2030/2160 train_time:119682ms step_avg:58.96ms
step:2031/2160 train_time:119769ms step_avg:58.97ms
step:2032/2160 train_time:119855ms step_avg:58.98ms
step:2033/2160 train_time:119942ms step_avg:59.00ms
step:2034/2160 train_time:120029ms step_avg:59.01ms
step:2035/2160 train_time:120118ms step_avg:59.03ms
step:2036/2160 train_time:120206ms step_avg:59.04ms
step:2037/2160 train_time:120296ms step_avg:59.06ms
step:2038/2160 train_time:120383ms step_avg:59.07ms
step:2039/2160 train_time:120471ms step_avg:59.08ms
step:2040/2160 train_time:120557ms step_avg:59.10ms
step:2041/2160 train_time:120645ms step_avg:59.11ms
step:2042/2160 train_time:120731ms step_avg:59.12ms
step:2043/2160 train_time:120819ms step_avg:59.14ms
step:2044/2160 train_time:120906ms step_avg:59.15ms
step:2045/2160 train_time:120994ms step_avg:59.17ms
step:2046/2160 train_time:121081ms step_avg:59.18ms
step:2047/2160 train_time:121169ms step_avg:59.19ms
step:2048/2160 train_time:121259ms step_avg:59.21ms
step:2049/2160 train_time:121347ms step_avg:59.22ms
step:2050/2160 train_time:121435ms step_avg:59.24ms
step:2051/2160 train_time:121523ms step_avg:59.25ms
step:2052/2160 train_time:121610ms step_avg:59.26ms
step:2053/2160 train_time:121697ms step_avg:59.28ms
step:2054/2160 train_time:121783ms step_avg:59.29ms
step:2055/2160 train_time:121871ms step_avg:59.30ms
step:2056/2160 train_time:121958ms step_avg:59.32ms
step:2057/2160 train_time:122044ms step_avg:59.33ms
step:2058/2160 train_time:122131ms step_avg:59.34ms
step:2059/2160 train_time:122220ms step_avg:59.36ms
step:2060/2160 train_time:122308ms step_avg:59.37ms
step:2061/2160 train_time:122397ms step_avg:59.39ms
step:2062/2160 train_time:122484ms step_avg:59.40ms
step:2063/2160 train_time:122572ms step_avg:59.41ms
step:2064/2160 train_time:122659ms step_avg:59.43ms
step:2065/2160 train_time:122745ms step_avg:59.44ms
step:2066/2160 train_time:122832ms step_avg:59.45ms
step:2067/2160 train_time:122921ms step_avg:59.47ms
step:2068/2160 train_time:123007ms step_avg:59.48ms
step:2069/2160 train_time:123096ms step_avg:59.50ms
step:2070/2160 train_time:123182ms step_avg:59.51ms
step:2071/2160 train_time:123271ms step_avg:59.52ms
step:2072/2160 train_time:123358ms step_avg:59.54ms
step:2073/2160 train_time:123446ms step_avg:59.55ms
step:2074/2160 train_time:123534ms step_avg:59.56ms
step:2075/2160 train_time:123621ms step_avg:59.58ms
step:2076/2160 train_time:123707ms step_avg:59.59ms
step:2077/2160 train_time:123795ms step_avg:59.60ms
step:2078/2160 train_time:123881ms step_avg:59.62ms
step:2079/2160 train_time:123969ms step_avg:59.63ms
step:2080/2160 train_time:124056ms step_avg:59.64ms
step:2081/2160 train_time:124144ms step_avg:59.66ms
step:2082/2160 train_time:124232ms step_avg:59.67ms
step:2083/2160 train_time:124320ms step_avg:59.68ms
step:2084/2160 train_time:124407ms step_avg:59.70ms
step:2085/2160 train_time:124496ms step_avg:59.71ms
step:2086/2160 train_time:124581ms step_avg:59.72ms
step:2087/2160 train_time:124669ms step_avg:59.74ms
step:2088/2160 train_time:124756ms step_avg:59.75ms
step:2089/2160 train_time:124843ms step_avg:59.76ms
step:2090/2160 train_time:124929ms step_avg:59.77ms
step:2091/2160 train_time:125018ms step_avg:59.79ms
step:2092/2160 train_time:125105ms step_avg:59.80ms
step:2093/2160 train_time:125194ms step_avg:59.82ms
step:2094/2160 train_time:125280ms step_avg:59.83ms
step:2095/2160 train_time:125368ms step_avg:59.84ms
step:2096/2160 train_time:125456ms step_avg:59.85ms
step:2097/2160 train_time:125543ms step_avg:59.87ms
step:2098/2160 train_time:125630ms step_avg:59.88ms
step:2099/2160 train_time:125717ms step_avg:59.89ms
step:2100/2160 train_time:125803ms step_avg:59.91ms
step:2101/2160 train_time:125891ms step_avg:59.92ms
step:2102/2160 train_time:125978ms step_avg:59.93ms
step:2103/2160 train_time:126065ms step_avg:59.95ms
step:2104/2160 train_time:126152ms step_avg:59.96ms
step:2105/2160 train_time:126241ms step_avg:59.97ms
step:2106/2160 train_time:126328ms step_avg:59.98ms
step:2107/2160 train_time:126418ms step_avg:60.00ms
step:2108/2160 train_time:126505ms step_avg:60.01ms
step:2109/2160 train_time:126594ms step_avg:60.03ms
step:2110/2160 train_time:126680ms step_avg:60.04ms
step:2111/2160 train_time:126767ms step_avg:60.05ms
step:2112/2160 train_time:126855ms step_avg:60.06ms
step:2113/2160 train_time:126942ms step_avg:60.08ms
step:2114/2160 train_time:127029ms step_avg:60.09ms
step:2115/2160 train_time:127117ms step_avg:60.10ms
step:2116/2160 train_time:127204ms step_avg:60.12ms
step:2117/2160 train_time:127292ms step_avg:60.13ms
step:2118/2160 train_time:127378ms step_avg:60.14ms
step:2119/2160 train_time:127466ms step_avg:60.15ms
step:2120/2160 train_time:127554ms step_avg:60.17ms
step:2121/2160 train_time:127642ms step_avg:60.18ms
step:2122/2160 train_time:127729ms step_avg:60.19ms
step:2123/2160 train_time:127818ms step_avg:60.21ms
step:2124/2160 train_time:127904ms step_avg:60.22ms
step:2125/2160 train_time:127992ms step_avg:60.23ms
step:2126/2160 train_time:128079ms step_avg:60.24ms
step:2127/2160 train_time:128168ms step_avg:60.26ms
step:2128/2160 train_time:128256ms step_avg:60.27ms
step:2129/2160 train_time:128345ms step_avg:60.28ms
step:2130/2160 train_time:128432ms step_avg:60.30ms
step:2131/2160 train_time:128521ms step_avg:60.31ms
step:2132/2160 train_time:128608ms step_avg:60.32ms
step:2133/2160 train_time:128697ms step_avg:60.34ms
step:2134/2160 train_time:128783ms step_avg:60.35ms
step:2135/2160 train_time:128872ms step_avg:60.36ms
step:2136/2160 train_time:128958ms step_avg:60.37ms
step:2137/2160 train_time:129046ms step_avg:60.39ms
step:2138/2160 train_time:129133ms step_avg:60.40ms
step:2139/2160 train_time:129221ms step_avg:60.41ms
step:2140/2160 train_time:129309ms step_avg:60.42ms
step:2141/2160 train_time:129398ms step_avg:60.44ms
step:2142/2160 train_time:129484ms step_avg:60.45ms
step:2143/2160 train_time:129574ms step_avg:60.46ms
step:2144/2160 train_time:129660ms step_avg:60.48ms
step:2145/2160 train_time:129748ms step_avg:60.49ms
step:2146/2160 train_time:129835ms step_avg:60.50ms
step:2147/2160 train_time:129923ms step_avg:60.51ms
step:2148/2160 train_time:130009ms step_avg:60.53ms
step:2149/2160 train_time:130098ms step_avg:60.54ms
step:2150/2160 train_time:130185ms step_avg:60.55ms
step:2151/2160 train_time:130275ms step_avg:60.56ms
step:2152/2160 train_time:130361ms step_avg:60.58ms
step:2153/2160 train_time:130451ms step_avg:60.59ms
step:2154/2160 train_time:130538ms step_avg:60.60ms
step:2155/2160 train_time:130627ms step_avg:60.62ms
step:2156/2160 train_time:130714ms step_avg:60.63ms
step:2157/2160 train_time:130802ms step_avg:60.64ms
step:2158/2160 train_time:130890ms step_avg:60.65ms
step:2159/2160 train_time:130978ms step_avg:60.67ms
step:2160/2160 train_time:131065ms step_avg:60.68ms
step:2160/2160 val_loss:3.2776 train_time:131155ms step_avg:60.72ms
peak memory allocated: 30540 MiB reserved: 44496 MiB

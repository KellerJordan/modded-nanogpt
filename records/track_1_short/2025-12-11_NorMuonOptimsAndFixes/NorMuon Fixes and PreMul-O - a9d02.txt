import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:57:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   42C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   41C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           13350      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           13351      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13352      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13353      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13354      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13355      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13356      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13357      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           13351      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           13352      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           13353      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           13354      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           13355      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           13356      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           13357      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:91ms step_avg:90.50ms
step:2/2160 train_time:116ms step_avg:57.81ms
step:3/2160 train_time:138ms step_avg:46.05ms
step:4/2160 train_time:161ms step_avg:40.37ms
step:5/2160 train_time:194ms step_avg:38.71ms
step:6/2160 train_time:343ms step_avg:57.09ms
step:7/2160 train_time:377ms step_avg:53.80ms
step:8/2160 train_time:409ms step_avg:51.11ms
step:9/2160 train_time:442ms step_avg:49.08ms
step:10/2160 train_time:474ms step_avg:47.40ms
step:11/2160 train_time:507ms step_avg:46.13ms
step:12/2160 train_time:540ms step_avg:45.01ms
step:13/2160 train_time:573ms step_avg:44.11ms
step:14/2160 train_time:606ms step_avg:43.27ms
step:15/2160 train_time:639ms step_avg:42.63ms
step:16/2160 train_time:672ms step_avg:42.00ms
step:17/2160 train_time:705ms step_avg:41.48ms
step:18/2160 train_time:738ms step_avg:40.98ms
step:19/2160 train_time:771ms step_avg:40.57ms
step:20/2160 train_time:803ms step_avg:40.16ms
step:21/2160 train_time:836ms step_avg:39.83ms
step:22/2160 train_time:869ms step_avg:39.49ms
step:23/2160 train_time:902ms step_avg:39.21ms
step:24/2160 train_time:935ms step_avg:38.95ms
step:25/2160 train_time:968ms step_avg:38.70ms
step:26/2160 train_time:1000ms step_avg:38.45ms
step:27/2160 train_time:1033ms step_avg:38.27ms
step:28/2160 train_time:1066ms step_avg:38.06ms
step:29/2160 train_time:1099ms step_avg:37.90ms
step:30/2160 train_time:1132ms step_avg:37.72ms
step:31/2160 train_time:1165ms step_avg:37.57ms
step:32/2160 train_time:1197ms step_avg:37.41ms
step:33/2160 train_time:1231ms step_avg:37.29ms
step:34/2160 train_time:1263ms step_avg:37.16ms
step:35/2160 train_time:1298ms step_avg:37.09ms
step:36/2160 train_time:1331ms step_avg:36.98ms
step:37/2160 train_time:1365ms step_avg:36.90ms
step:38/2160 train_time:1398ms step_avg:36.79ms
step:39/2160 train_time:1432ms step_avg:36.71ms
step:40/2160 train_time:1464ms step_avg:36.61ms
step:41/2160 train_time:1498ms step_avg:36.53ms
step:42/2160 train_time:1530ms step_avg:36.44ms
step:43/2160 train_time:1564ms step_avg:36.37ms
step:44/2160 train_time:1596ms step_avg:36.28ms
step:45/2160 train_time:1630ms step_avg:36.21ms
step:46/2160 train_time:1662ms step_avg:36.12ms
step:47/2160 train_time:1695ms step_avg:36.07ms
step:48/2160 train_time:1728ms step_avg:35.99ms
step:49/2160 train_time:1761ms step_avg:35.94ms
step:50/2160 train_time:1793ms step_avg:35.87ms
step:51/2160 train_time:1827ms step_avg:35.82ms
step:52/2160 train_time:1859ms step_avg:35.75ms
step:53/2160 train_time:1893ms step_avg:35.71ms
step:54/2160 train_time:1925ms step_avg:35.65ms
step:55/2160 train_time:1958ms step_avg:35.60ms
step:56/2160 train_time:1990ms step_avg:35.54ms
step:57/2160 train_time:2024ms step_avg:35.51ms
step:58/2160 train_time:2056ms step_avg:35.45ms
step:59/2160 train_time:2090ms step_avg:35.42ms
step:60/2160 train_time:2122ms step_avg:35.36ms
step:61/2160 train_time:2155ms step_avg:35.33ms
step:62/2160 train_time:2187ms step_avg:35.28ms
step:63/2160 train_time:2221ms step_avg:35.25ms
step:64/2160 train_time:2253ms step_avg:35.21ms
step:65/2160 train_time:2287ms step_avg:35.18ms
step:66/2160 train_time:2319ms step_avg:35.14ms
step:67/2160 train_time:2353ms step_avg:35.12ms
step:68/2160 train_time:2385ms step_avg:35.08ms
step:69/2160 train_time:2419ms step_avg:35.05ms
step:70/2160 train_time:2451ms step_avg:35.02ms
step:71/2160 train_time:2485ms step_avg:35.00ms
step:72/2160 train_time:2517ms step_avg:34.96ms
step:73/2160 train_time:2551ms step_avg:34.94ms
step:74/2160 train_time:2583ms step_avg:34.91ms
step:75/2160 train_time:2616ms step_avg:34.89ms
step:76/2160 train_time:2649ms step_avg:34.86ms
step:77/2160 train_time:2682ms step_avg:34.84ms
step:78/2160 train_time:2715ms step_avg:34.80ms
step:79/2160 train_time:2748ms step_avg:34.79ms
step:80/2160 train_time:2781ms step_avg:34.76ms
step:81/2160 train_time:2814ms step_avg:34.74ms
step:82/2160 train_time:2846ms step_avg:34.71ms
step:83/2160 train_time:2879ms step_avg:34.69ms
step:84/2160 train_time:2912ms step_avg:34.66ms
step:85/2160 train_time:2945ms step_avg:34.65ms
step:86/2160 train_time:2978ms step_avg:34.62ms
step:87/2160 train_time:3011ms step_avg:34.61ms
step:88/2160 train_time:3043ms step_avg:34.58ms
step:89/2160 train_time:3076ms step_avg:34.56ms
step:90/2160 train_time:3109ms step_avg:34.54ms
step:91/2160 train_time:3142ms step_avg:34.53ms
step:92/2160 train_time:3174ms step_avg:34.50ms
step:93/2160 train_time:3208ms step_avg:34.49ms
step:94/2160 train_time:3240ms step_avg:34.47ms
step:95/2160 train_time:3273ms step_avg:34.46ms
step:96/2160 train_time:3305ms step_avg:34.43ms
step:97/2160 train_time:3339ms step_avg:34.43ms
step:98/2160 train_time:3372ms step_avg:34.40ms
step:99/2160 train_time:3406ms step_avg:34.40ms
step:100/2160 train_time:3438ms step_avg:34.38ms
step:101/2160 train_time:3472ms step_avg:34.37ms
step:102/2160 train_time:3504ms step_avg:34.35ms
step:103/2160 train_time:3537ms step_avg:34.34ms
step:104/2160 train_time:3569ms step_avg:34.32ms
step:105/2160 train_time:3603ms step_avg:34.31ms
step:106/2160 train_time:3635ms step_avg:34.29ms
step:107/2160 train_time:3668ms step_avg:34.28ms
step:108/2160 train_time:3700ms step_avg:34.26ms
step:109/2160 train_time:3734ms step_avg:34.25ms
step:110/2160 train_time:3766ms step_avg:34.23ms
step:111/2160 train_time:3799ms step_avg:34.23ms
step:112/2160 train_time:3831ms step_avg:34.21ms
step:113/2160 train_time:3865ms step_avg:34.20ms
step:114/2160 train_time:3897ms step_avg:34.19ms
step:115/2160 train_time:3931ms step_avg:34.18ms
step:116/2160 train_time:3963ms step_avg:34.16ms
step:117/2160 train_time:3996ms step_avg:34.15ms
step:118/2160 train_time:4028ms step_avg:34.14ms
step:119/2160 train_time:4062ms step_avg:34.13ms
step:120/2160 train_time:4094ms step_avg:34.12ms
step:121/2160 train_time:4128ms step_avg:34.11ms
step:122/2160 train_time:4160ms step_avg:34.10ms
step:123/2160 train_time:4193ms step_avg:34.09ms
step:124/2160 train_time:4225ms step_avg:34.08ms
step:125/2160 train_time:4258ms step_avg:34.07ms
step:126/2160 train_time:4291ms step_avg:34.05ms
step:127/2160 train_time:4324ms step_avg:34.05ms
step:128/2160 train_time:4356ms step_avg:34.03ms
step:129/2160 train_time:4389ms step_avg:34.03ms
step:130/2160 train_time:4422ms step_avg:34.01ms
step:131/2160 train_time:4455ms step_avg:34.01ms
step:132/2160 train_time:4488ms step_avg:34.00ms
step:133/2160 train_time:4521ms step_avg:33.99ms
step:134/2160 train_time:4553ms step_avg:33.98ms
step:135/2160 train_time:4586ms step_avg:33.97ms
step:136/2160 train_time:4619ms step_avg:33.96ms
step:137/2160 train_time:4652ms step_avg:33.95ms
step:138/2160 train_time:4684ms step_avg:33.94ms
step:139/2160 train_time:4718ms step_avg:33.94ms
step:140/2160 train_time:4750ms step_avg:33.93ms
step:141/2160 train_time:4783ms step_avg:33.92ms
step:142/2160 train_time:4816ms step_avg:33.91ms
step:143/2160 train_time:4849ms step_avg:33.91ms
step:144/2160 train_time:4881ms step_avg:33.90ms
step:145/2160 train_time:4915ms step_avg:33.89ms
step:146/2160 train_time:4947ms step_avg:33.88ms
step:147/2160 train_time:4980ms step_avg:33.88ms
step:148/2160 train_time:5013ms step_avg:33.87ms
step:149/2160 train_time:5047ms step_avg:33.87ms
step:150/2160 train_time:5079ms step_avg:33.86ms
step:151/2160 train_time:5112ms step_avg:33.85ms
step:152/2160 train_time:5144ms step_avg:33.84ms
step:153/2160 train_time:5177ms step_avg:33.84ms
step:154/2160 train_time:5210ms step_avg:33.83ms
step:155/2160 train_time:5243ms step_avg:33.83ms
step:156/2160 train_time:5275ms step_avg:33.82ms
step:157/2160 train_time:5309ms step_avg:33.81ms
step:158/2160 train_time:5341ms step_avg:33.80ms
step:159/2160 train_time:5374ms step_avg:33.80ms
step:160/2160 train_time:5406ms step_avg:33.79ms
step:161/2160 train_time:5440ms step_avg:33.79ms
step:162/2160 train_time:5472ms step_avg:33.78ms
step:163/2160 train_time:5505ms step_avg:33.77ms
step:164/2160 train_time:5538ms step_avg:33.77ms
step:165/2160 train_time:5571ms step_avg:33.76ms
step:166/2160 train_time:5603ms step_avg:33.75ms
step:167/2160 train_time:5636ms step_avg:33.75ms
step:168/2160 train_time:5669ms step_avg:33.74ms
step:169/2160 train_time:5702ms step_avg:33.74ms
step:170/2160 train_time:5734ms step_avg:33.73ms
step:171/2160 train_time:5768ms step_avg:33.73ms
step:172/2160 train_time:5800ms step_avg:33.72ms
step:173/2160 train_time:5833ms step_avg:33.72ms
step:174/2160 train_time:5865ms step_avg:33.71ms
step:175/2160 train_time:5898ms step_avg:33.70ms
step:176/2160 train_time:5930ms step_avg:33.70ms
step:177/2160 train_time:5964ms step_avg:33.69ms
step:178/2160 train_time:5996ms step_avg:33.69ms
step:179/2160 train_time:6029ms step_avg:33.68ms
step:180/2160 train_time:6061ms step_avg:33.67ms
step:181/2160 train_time:6094ms step_avg:33.67ms
step:182/2160 train_time:6127ms step_avg:33.66ms
step:183/2160 train_time:6160ms step_avg:33.66ms
step:184/2160 train_time:6192ms step_avg:33.65ms
step:185/2160 train_time:6225ms step_avg:33.65ms
step:186/2160 train_time:6257ms step_avg:33.64ms
step:187/2160 train_time:6291ms step_avg:33.64ms
step:188/2160 train_time:6323ms step_avg:33.63ms
step:189/2160 train_time:6356ms step_avg:33.63ms
step:190/2160 train_time:6388ms step_avg:33.62ms
step:191/2160 train_time:6422ms step_avg:33.62ms
step:192/2160 train_time:6454ms step_avg:33.62ms
step:193/2160 train_time:6487ms step_avg:33.61ms
step:194/2160 train_time:6519ms step_avg:33.60ms
step:195/2160 train_time:6553ms step_avg:33.60ms
step:196/2160 train_time:6585ms step_avg:33.60ms
step:197/2160 train_time:6618ms step_avg:33.59ms
step:198/2160 train_time:6650ms step_avg:33.59ms
step:199/2160 train_time:6683ms step_avg:33.58ms
step:200/2160 train_time:6716ms step_avg:33.58ms
step:201/2160 train_time:6749ms step_avg:33.58ms
step:202/2160 train_time:6781ms step_avg:33.57ms
step:203/2160 train_time:6814ms step_avg:33.56ms
step:204/2160 train_time:6846ms step_avg:33.56ms
step:205/2160 train_time:6879ms step_avg:33.56ms
step:206/2160 train_time:6912ms step_avg:33.55ms
step:207/2160 train_time:6945ms step_avg:33.55ms
step:208/2160 train_time:6977ms step_avg:33.55ms
step:209/2160 train_time:7011ms step_avg:33.54ms
step:210/2160 train_time:7043ms step_avg:33.54ms
step:211/2160 train_time:7076ms step_avg:33.53ms
step:212/2160 train_time:7108ms step_avg:33.53ms
step:213/2160 train_time:7141ms step_avg:33.53ms
step:214/2160 train_time:7173ms step_avg:33.52ms
step:215/2160 train_time:7207ms step_avg:33.52ms
step:216/2160 train_time:7239ms step_avg:33.51ms
step:217/2160 train_time:7272ms step_avg:33.51ms
step:218/2160 train_time:7305ms step_avg:33.51ms
step:219/2160 train_time:7338ms step_avg:33.51ms
step:220/2160 train_time:7370ms step_avg:33.50ms
step:221/2160 train_time:7403ms step_avg:33.50ms
step:222/2160 train_time:7435ms step_avg:33.49ms
step:223/2160 train_time:7468ms step_avg:33.49ms
step:224/2160 train_time:7500ms step_avg:33.48ms
step:225/2160 train_time:7534ms step_avg:33.48ms
step:226/2160 train_time:7566ms step_avg:33.48ms
step:227/2160 train_time:7599ms step_avg:33.48ms
step:228/2160 train_time:7632ms step_avg:33.47ms
step:229/2160 train_time:7665ms step_avg:33.47ms
step:230/2160 train_time:7698ms step_avg:33.47ms
step:231/2160 train_time:7730ms step_avg:33.47ms
step:232/2160 train_time:7763ms step_avg:33.46ms
step:233/2160 train_time:7796ms step_avg:33.46ms
step:234/2160 train_time:7828ms step_avg:33.45ms
step:235/2160 train_time:7861ms step_avg:33.45ms
step:236/2160 train_time:7894ms step_avg:33.45ms
step:237/2160 train_time:7927ms step_avg:33.45ms
step:238/2160 train_time:7959ms step_avg:33.44ms
step:239/2160 train_time:7992ms step_avg:33.44ms
step:240/2160 train_time:8024ms step_avg:33.44ms
step:241/2160 train_time:8057ms step_avg:33.43ms
step:242/2160 train_time:8090ms step_avg:33.43ms
step:243/2160 train_time:8123ms step_avg:33.43ms
step:244/2160 train_time:8156ms step_avg:33.43ms
step:245/2160 train_time:8189ms step_avg:33.42ms
step:246/2160 train_time:8221ms step_avg:33.42ms
step:247/2160 train_time:8254ms step_avg:33.42ms
step:248/2160 train_time:8286ms step_avg:33.41ms
step:249/2160 train_time:8320ms step_avg:33.41ms
step:250/2160 train_time:8352ms step_avg:33.41ms
step:250/2160 val_loss:4.2948 train_time:8387ms step_avg:33.55ms
step:251/2160 train_time:8409ms step_avg:33.50ms
step:252/2160 train_time:8429ms step_avg:33.45ms
step:253/2160 train_time:8453ms step_avg:33.41ms
step:254/2160 train_time:8486ms step_avg:33.41ms
step:255/2160 train_time:8522ms step_avg:33.42ms
step:256/2160 train_time:8555ms step_avg:33.42ms
step:257/2160 train_time:8590ms step_avg:33.42ms
step:258/2160 train_time:8622ms step_avg:33.42ms
step:259/2160 train_time:8655ms step_avg:33.42ms
step:260/2160 train_time:8687ms step_avg:33.41ms
step:261/2160 train_time:8721ms step_avg:33.41ms
step:262/2160 train_time:8753ms step_avg:33.41ms
step:263/2160 train_time:8787ms step_avg:33.41ms
step:264/2160 train_time:8819ms step_avg:33.40ms
step:265/2160 train_time:8852ms step_avg:33.40ms
step:266/2160 train_time:8884ms step_avg:33.40ms
step:267/2160 train_time:8917ms step_avg:33.40ms
step:268/2160 train_time:8949ms step_avg:33.39ms
step:269/2160 train_time:8982ms step_avg:33.39ms
step:270/2160 train_time:9014ms step_avg:33.38ms
step:271/2160 train_time:9047ms step_avg:33.38ms
step:272/2160 train_time:9079ms step_avg:33.38ms
step:273/2160 train_time:9112ms step_avg:33.38ms
step:274/2160 train_time:9144ms step_avg:33.37ms
step:275/2160 train_time:9177ms step_avg:33.37ms
step:276/2160 train_time:9209ms step_avg:33.37ms
step:277/2160 train_time:9242ms step_avg:33.36ms
step:278/2160 train_time:9274ms step_avg:33.36ms
step:279/2160 train_time:9307ms step_avg:33.36ms
step:280/2160 train_time:9339ms step_avg:33.35ms
step:281/2160 train_time:9372ms step_avg:33.35ms
step:282/2160 train_time:9404ms step_avg:33.35ms
step:283/2160 train_time:9437ms step_avg:33.35ms
step:284/2160 train_time:9470ms step_avg:33.35ms
step:285/2160 train_time:9505ms step_avg:33.35ms
step:286/2160 train_time:9537ms step_avg:33.35ms
step:287/2160 train_time:9570ms step_avg:33.35ms
step:288/2160 train_time:9603ms step_avg:33.34ms
step:289/2160 train_time:9636ms step_avg:33.34ms
step:290/2160 train_time:9669ms step_avg:33.34ms
step:291/2160 train_time:9702ms step_avg:33.34ms
step:292/2160 train_time:9735ms step_avg:33.34ms
step:293/2160 train_time:9768ms step_avg:33.34ms
step:294/2160 train_time:9800ms step_avg:33.33ms
step:295/2160 train_time:9833ms step_avg:33.33ms
step:296/2160 train_time:9865ms step_avg:33.33ms
step:297/2160 train_time:9898ms step_avg:33.33ms
step:298/2160 train_time:9930ms step_avg:33.32ms
step:299/2160 train_time:9964ms step_avg:33.32ms
step:300/2160 train_time:9996ms step_avg:33.32ms
step:301/2160 train_time:10029ms step_avg:33.32ms
step:302/2160 train_time:10061ms step_avg:33.31ms
step:303/2160 train_time:10094ms step_avg:33.31ms
step:304/2160 train_time:10126ms step_avg:33.31ms
step:305/2160 train_time:10159ms step_avg:33.31ms
step:306/2160 train_time:10191ms step_avg:33.30ms
step:307/2160 train_time:10224ms step_avg:33.30ms
step:308/2160 train_time:10257ms step_avg:33.30ms
step:309/2160 train_time:10290ms step_avg:33.30ms
step:310/2160 train_time:10322ms step_avg:33.30ms
step:311/2160 train_time:10355ms step_avg:33.29ms
step:312/2160 train_time:10387ms step_avg:33.29ms
step:313/2160 train_time:10420ms step_avg:33.29ms
step:314/2160 train_time:10452ms step_avg:33.29ms
step:315/2160 train_time:10486ms step_avg:33.29ms
step:316/2160 train_time:10518ms step_avg:33.28ms
step:317/2160 train_time:10552ms step_avg:33.29ms
step:318/2160 train_time:10584ms step_avg:33.28ms
step:319/2160 train_time:10617ms step_avg:33.28ms
step:320/2160 train_time:10649ms step_avg:33.28ms
step:321/2160 train_time:10682ms step_avg:33.28ms
step:322/2160 train_time:10714ms step_avg:33.27ms
step:323/2160 train_time:10748ms step_avg:33.27ms
step:324/2160 train_time:10780ms step_avg:33.27ms
step:325/2160 train_time:10813ms step_avg:33.27ms
step:326/2160 train_time:10845ms step_avg:33.27ms
step:327/2160 train_time:10878ms step_avg:33.27ms
step:328/2160 train_time:10910ms step_avg:33.26ms
step:329/2160 train_time:10944ms step_avg:33.26ms
step:330/2160 train_time:10976ms step_avg:33.26ms
step:331/2160 train_time:11009ms step_avg:33.26ms
step:332/2160 train_time:11041ms step_avg:33.26ms
step:333/2160 train_time:11074ms step_avg:33.26ms
step:334/2160 train_time:11106ms step_avg:33.25ms
step:335/2160 train_time:11140ms step_avg:33.25ms
step:336/2160 train_time:11172ms step_avg:33.25ms
step:337/2160 train_time:11205ms step_avg:33.25ms
step:338/2160 train_time:11237ms step_avg:33.25ms
step:339/2160 train_time:11270ms step_avg:33.24ms
step:340/2160 train_time:11302ms step_avg:33.24ms
step:341/2160 train_time:11335ms step_avg:33.24ms
step:342/2160 train_time:11367ms step_avg:33.24ms
step:343/2160 train_time:11401ms step_avg:33.24ms
step:344/2160 train_time:11433ms step_avg:33.23ms
step:345/2160 train_time:11466ms step_avg:33.24ms
step:346/2160 train_time:11498ms step_avg:33.23ms
step:347/2160 train_time:11531ms step_avg:33.23ms
step:348/2160 train_time:11563ms step_avg:33.23ms
step:349/2160 train_time:11597ms step_avg:33.23ms
step:350/2160 train_time:11629ms step_avg:33.23ms
step:351/2160 train_time:11662ms step_avg:33.23ms
step:352/2160 train_time:11694ms step_avg:33.22ms
step:353/2160 train_time:11728ms step_avg:33.22ms
step:354/2160 train_time:11760ms step_avg:33.22ms
step:355/2160 train_time:11793ms step_avg:33.22ms
step:356/2160 train_time:11825ms step_avg:33.22ms
step:357/2160 train_time:11859ms step_avg:33.22ms
step:358/2160 train_time:11891ms step_avg:33.21ms
step:359/2160 train_time:11924ms step_avg:33.22ms
step:360/2160 train_time:11956ms step_avg:33.21ms
step:361/2160 train_time:11990ms step_avg:33.21ms
step:362/2160 train_time:12022ms step_avg:33.21ms
step:363/2160 train_time:12055ms step_avg:33.21ms
step:364/2160 train_time:12087ms step_avg:33.21ms
step:365/2160 train_time:12121ms step_avg:33.21ms
step:366/2160 train_time:12153ms step_avg:33.20ms
step:367/2160 train_time:12186ms step_avg:33.21ms
step:368/2160 train_time:12219ms step_avg:33.20ms
step:369/2160 train_time:12252ms step_avg:33.20ms
step:370/2160 train_time:12284ms step_avg:33.20ms
step:371/2160 train_time:12317ms step_avg:33.20ms
step:372/2160 train_time:12349ms step_avg:33.20ms
step:373/2160 train_time:12383ms step_avg:33.20ms
step:374/2160 train_time:12415ms step_avg:33.19ms
step:375/2160 train_time:12448ms step_avg:33.20ms
step:376/2160 train_time:12480ms step_avg:33.19ms
step:377/2160 train_time:12514ms step_avg:33.19ms
step:378/2160 train_time:12546ms step_avg:33.19ms
step:379/2160 train_time:12579ms step_avg:33.19ms
step:380/2160 train_time:12611ms step_avg:33.19ms
step:381/2160 train_time:12645ms step_avg:33.19ms
step:382/2160 train_time:12677ms step_avg:33.19ms
step:383/2160 train_time:12711ms step_avg:33.19ms
step:384/2160 train_time:12743ms step_avg:33.18ms
step:385/2160 train_time:12776ms step_avg:33.18ms
step:386/2160 train_time:12808ms step_avg:33.18ms
step:387/2160 train_time:12841ms step_avg:33.18ms
step:388/2160 train_time:12874ms step_avg:33.18ms
step:389/2160 train_time:12907ms step_avg:33.18ms
step:390/2160 train_time:12939ms step_avg:33.18ms
step:391/2160 train_time:12972ms step_avg:33.18ms
step:392/2160 train_time:13004ms step_avg:33.17ms
step:393/2160 train_time:13038ms step_avg:33.17ms
step:394/2160 train_time:13070ms step_avg:33.17ms
step:395/2160 train_time:13103ms step_avg:33.17ms
step:396/2160 train_time:13135ms step_avg:33.17ms
step:397/2160 train_time:13168ms step_avg:33.17ms
step:398/2160 train_time:13201ms step_avg:33.17ms
step:399/2160 train_time:13234ms step_avg:33.17ms
step:400/2160 train_time:13266ms step_avg:33.16ms
step:401/2160 train_time:13299ms step_avg:33.16ms
step:402/2160 train_time:13331ms step_avg:33.16ms
step:403/2160 train_time:13365ms step_avg:33.16ms
step:404/2160 train_time:13397ms step_avg:33.16ms
step:405/2160 train_time:13430ms step_avg:33.16ms
step:406/2160 train_time:13462ms step_avg:33.16ms
step:407/2160 train_time:13495ms step_avg:33.16ms
step:408/2160 train_time:13527ms step_avg:33.16ms
step:409/2160 train_time:13561ms step_avg:33.16ms
step:410/2160 train_time:13593ms step_avg:33.15ms
step:411/2160 train_time:13626ms step_avg:33.15ms
step:412/2160 train_time:13658ms step_avg:33.15ms
step:413/2160 train_time:13691ms step_avg:33.15ms
step:414/2160 train_time:13723ms step_avg:33.15ms
step:415/2160 train_time:13757ms step_avg:33.15ms
step:416/2160 train_time:13789ms step_avg:33.15ms
step:417/2160 train_time:13822ms step_avg:33.15ms
step:418/2160 train_time:13854ms step_avg:33.14ms
step:419/2160 train_time:13887ms step_avg:33.14ms
step:420/2160 train_time:13919ms step_avg:33.14ms
step:421/2160 train_time:13953ms step_avg:33.14ms
step:422/2160 train_time:13985ms step_avg:33.14ms
step:423/2160 train_time:14018ms step_avg:33.14ms
step:424/2160 train_time:14050ms step_avg:33.14ms
step:425/2160 train_time:14084ms step_avg:33.14ms
step:426/2160 train_time:14116ms step_avg:33.14ms
step:427/2160 train_time:14149ms step_avg:33.14ms
step:428/2160 train_time:14181ms step_avg:33.13ms
step:429/2160 train_time:14214ms step_avg:33.13ms
step:430/2160 train_time:14247ms step_avg:33.13ms
step:431/2160 train_time:14280ms step_avg:33.13ms
step:432/2160 train_time:14312ms step_avg:33.13ms
step:433/2160 train_time:14345ms step_avg:33.13ms
step:434/2160 train_time:14377ms step_avg:33.13ms
step:435/2160 train_time:14411ms step_avg:33.13ms
step:436/2160 train_time:14443ms step_avg:33.13ms
step:437/2160 train_time:14476ms step_avg:33.13ms
step:438/2160 train_time:14508ms step_avg:33.12ms
step:439/2160 train_time:14542ms step_avg:33.12ms
step:440/2160 train_time:14574ms step_avg:33.12ms
step:441/2160 train_time:14607ms step_avg:33.12ms
step:442/2160 train_time:14640ms step_avg:33.12ms
step:443/2160 train_time:14673ms step_avg:33.12ms
step:444/2160 train_time:14705ms step_avg:33.12ms
step:445/2160 train_time:14738ms step_avg:33.12ms
step:446/2160 train_time:14770ms step_avg:33.12ms
step:447/2160 train_time:14804ms step_avg:33.12ms
step:448/2160 train_time:14836ms step_avg:33.12ms
step:449/2160 train_time:14869ms step_avg:33.12ms
step:450/2160 train_time:14901ms step_avg:33.11ms
step:451/2160 train_time:14934ms step_avg:33.11ms
step:452/2160 train_time:14967ms step_avg:33.11ms
step:453/2160 train_time:15000ms step_avg:33.11ms
step:454/2160 train_time:15032ms step_avg:33.11ms
step:455/2160 train_time:15065ms step_avg:33.11ms
step:456/2160 train_time:15098ms step_avg:33.11ms
step:457/2160 train_time:15131ms step_avg:33.11ms
step:458/2160 train_time:15163ms step_avg:33.11ms
step:459/2160 train_time:15196ms step_avg:33.11ms
step:460/2160 train_time:15228ms step_avg:33.10ms
step:461/2160 train_time:15261ms step_avg:33.10ms
step:462/2160 train_time:15294ms step_avg:33.10ms
step:463/2160 train_time:15327ms step_avg:33.10ms
step:464/2160 train_time:15359ms step_avg:33.10ms
step:465/2160 train_time:15393ms step_avg:33.10ms
step:466/2160 train_time:15425ms step_avg:33.10ms
step:467/2160 train_time:15458ms step_avg:33.10ms
step:468/2160 train_time:15491ms step_avg:33.10ms
step:469/2160 train_time:15523ms step_avg:33.10ms
step:470/2160 train_time:15556ms step_avg:33.10ms
step:471/2160 train_time:15589ms step_avg:33.10ms
step:472/2160 train_time:15621ms step_avg:33.10ms
step:473/2160 train_time:15654ms step_avg:33.10ms
step:474/2160 train_time:15686ms step_avg:33.09ms
step:475/2160 train_time:15720ms step_avg:33.09ms
step:476/2160 train_time:15752ms step_avg:33.09ms
step:477/2160 train_time:15785ms step_avg:33.09ms
step:478/2160 train_time:15817ms step_avg:33.09ms
step:479/2160 train_time:15851ms step_avg:33.09ms
step:480/2160 train_time:15883ms step_avg:33.09ms
step:481/2160 train_time:15916ms step_avg:33.09ms
step:482/2160 train_time:15948ms step_avg:33.09ms
step:483/2160 train_time:15981ms step_avg:33.09ms
step:484/2160 train_time:16013ms step_avg:33.09ms
step:485/2160 train_time:16047ms step_avg:33.09ms
step:486/2160 train_time:16079ms step_avg:33.08ms
step:487/2160 train_time:16113ms step_avg:33.09ms
step:488/2160 train_time:16145ms step_avg:33.08ms
step:489/2160 train_time:16178ms step_avg:33.08ms
step:490/2160 train_time:16210ms step_avg:33.08ms
step:491/2160 train_time:16243ms step_avg:33.08ms
step:492/2160 train_time:16276ms step_avg:33.08ms
step:493/2160 train_time:16309ms step_avg:33.08ms
step:494/2160 train_time:16341ms step_avg:33.08ms
step:495/2160 train_time:16375ms step_avg:33.08ms
step:496/2160 train_time:16407ms step_avg:33.08ms
step:497/2160 train_time:16440ms step_avg:33.08ms
step:498/2160 train_time:16472ms step_avg:33.08ms
step:499/2160 train_time:16506ms step_avg:33.08ms
step:500/2160 train_time:16538ms step_avg:33.08ms
step:500/2160 val_loss:4.0152 train_time:16573ms step_avg:33.15ms
step:501/2160 train_time:16595ms step_avg:33.12ms
step:502/2160 train_time:16616ms step_avg:33.10ms
step:503/2160 train_time:16640ms step_avg:33.08ms
step:504/2160 train_time:16672ms step_avg:33.08ms
step:505/2160 train_time:16708ms step_avg:33.08ms
step:506/2160 train_time:16741ms step_avg:33.08ms
step:507/2160 train_time:16775ms step_avg:33.09ms
step:508/2160 train_time:16807ms step_avg:33.08ms
step:509/2160 train_time:16840ms step_avg:33.09ms
step:510/2160 train_time:16873ms step_avg:33.08ms
step:511/2160 train_time:16906ms step_avg:33.08ms
step:512/2160 train_time:16938ms step_avg:33.08ms
step:513/2160 train_time:16971ms step_avg:33.08ms
step:514/2160 train_time:17003ms step_avg:33.08ms
step:515/2160 train_time:17036ms step_avg:33.08ms
step:516/2160 train_time:17068ms step_avg:33.08ms
step:517/2160 train_time:17101ms step_avg:33.08ms
step:518/2160 train_time:17134ms step_avg:33.08ms
step:519/2160 train_time:17167ms step_avg:33.08ms
step:520/2160 train_time:17199ms step_avg:33.07ms
step:521/2160 train_time:17231ms step_avg:33.07ms
step:522/2160 train_time:17263ms step_avg:33.07ms
step:523/2160 train_time:17296ms step_avg:33.07ms
step:524/2160 train_time:17329ms step_avg:33.07ms
step:525/2160 train_time:17362ms step_avg:33.07ms
step:526/2160 train_time:17394ms step_avg:33.07ms
step:527/2160 train_time:17427ms step_avg:33.07ms
step:528/2160 train_time:17459ms step_avg:33.07ms
step:529/2160 train_time:17492ms step_avg:33.07ms
step:530/2160 train_time:17524ms step_avg:33.07ms
step:531/2160 train_time:17558ms step_avg:33.07ms
step:532/2160 train_time:17590ms step_avg:33.06ms
step:533/2160 train_time:17624ms step_avg:33.07ms
step:534/2160 train_time:17656ms step_avg:33.06ms
step:535/2160 train_time:17690ms step_avg:33.06ms
step:536/2160 train_time:17723ms step_avg:33.06ms
step:537/2160 train_time:17756ms step_avg:33.07ms
step:538/2160 train_time:17789ms step_avg:33.06ms
step:539/2160 train_time:17823ms step_avg:33.07ms
step:540/2160 train_time:17855ms step_avg:33.06ms
step:541/2160 train_time:17888ms step_avg:33.06ms
step:542/2160 train_time:17920ms step_avg:33.06ms
step:543/2160 train_time:17953ms step_avg:33.06ms
step:544/2160 train_time:17985ms step_avg:33.06ms
step:545/2160 train_time:18019ms step_avg:33.06ms
step:546/2160 train_time:18051ms step_avg:33.06ms
step:547/2160 train_time:18084ms step_avg:33.06ms
step:548/2160 train_time:18116ms step_avg:33.06ms
step:549/2160 train_time:18150ms step_avg:33.06ms
step:550/2160 train_time:18182ms step_avg:33.06ms
step:551/2160 train_time:18215ms step_avg:33.06ms
step:552/2160 train_time:18247ms step_avg:33.06ms
step:553/2160 train_time:18281ms step_avg:33.06ms
step:554/2160 train_time:18313ms step_avg:33.06ms
step:555/2160 train_time:18346ms step_avg:33.06ms
step:556/2160 train_time:18378ms step_avg:33.05ms
step:557/2160 train_time:18411ms step_avg:33.05ms
step:558/2160 train_time:18443ms step_avg:33.05ms
step:559/2160 train_time:18477ms step_avg:33.05ms
step:560/2160 train_time:18509ms step_avg:33.05ms
step:561/2160 train_time:18542ms step_avg:33.05ms
step:562/2160 train_time:18574ms step_avg:33.05ms
step:563/2160 train_time:18608ms step_avg:33.05ms
step:564/2160 train_time:18640ms step_avg:33.05ms
step:565/2160 train_time:18673ms step_avg:33.05ms
step:566/2160 train_time:18706ms step_avg:33.05ms
step:567/2160 train_time:18739ms step_avg:33.05ms
step:568/2160 train_time:18771ms step_avg:33.05ms
step:569/2160 train_time:18805ms step_avg:33.05ms
step:570/2160 train_time:18837ms step_avg:33.05ms
step:571/2160 train_time:18871ms step_avg:33.05ms
step:572/2160 train_time:18903ms step_avg:33.05ms
step:573/2160 train_time:18937ms step_avg:33.05ms
step:574/2160 train_time:18969ms step_avg:33.05ms
step:575/2160 train_time:19002ms step_avg:33.05ms
step:576/2160 train_time:19035ms step_avg:33.05ms
step:577/2160 train_time:19068ms step_avg:33.05ms
step:578/2160 train_time:19100ms step_avg:33.04ms
step:579/2160 train_time:19133ms step_avg:33.04ms
step:580/2160 train_time:19165ms step_avg:33.04ms
step:581/2160 train_time:19198ms step_avg:33.04ms
step:582/2160 train_time:19231ms step_avg:33.04ms
step:583/2160 train_time:19264ms step_avg:33.04ms
step:584/2160 train_time:19296ms step_avg:33.04ms
step:585/2160 train_time:19329ms step_avg:33.04ms
step:586/2160 train_time:19361ms step_avg:33.04ms
step:587/2160 train_time:19394ms step_avg:33.04ms
step:588/2160 train_time:19426ms step_avg:33.04ms
step:589/2160 train_time:19459ms step_avg:33.04ms
step:590/2160 train_time:19491ms step_avg:33.04ms
step:591/2160 train_time:19525ms step_avg:33.04ms
step:592/2160 train_time:19557ms step_avg:33.04ms
step:593/2160 train_time:19590ms step_avg:33.04ms
step:594/2160 train_time:19622ms step_avg:33.03ms
step:595/2160 train_time:19656ms step_avg:33.04ms
step:596/2160 train_time:19688ms step_avg:33.03ms
step:597/2160 train_time:19722ms step_avg:33.03ms
step:598/2160 train_time:19754ms step_avg:33.03ms
step:599/2160 train_time:19787ms step_avg:33.03ms
step:600/2160 train_time:19820ms step_avg:33.03ms
step:601/2160 train_time:19853ms step_avg:33.03ms
step:602/2160 train_time:19886ms step_avg:33.03ms
step:603/2160 train_time:19919ms step_avg:33.03ms
step:604/2160 train_time:19952ms step_avg:33.03ms
step:605/2160 train_time:19985ms step_avg:33.03ms
step:606/2160 train_time:20017ms step_avg:33.03ms
step:607/2160 train_time:20050ms step_avg:33.03ms
step:608/2160 train_time:20083ms step_avg:33.03ms
step:609/2160 train_time:20116ms step_avg:33.03ms
step:610/2160 train_time:20148ms step_avg:33.03ms
step:611/2160 train_time:20182ms step_avg:33.03ms
step:612/2160 train_time:20214ms step_avg:33.03ms
step:613/2160 train_time:20247ms step_avg:33.03ms
step:614/2160 train_time:20280ms step_avg:33.03ms
step:615/2160 train_time:20312ms step_avg:33.03ms
step:616/2160 train_time:20344ms step_avg:33.03ms
step:617/2160 train_time:20378ms step_avg:33.03ms
step:618/2160 train_time:20410ms step_avg:33.03ms
step:619/2160 train_time:20443ms step_avg:33.03ms
step:620/2160 train_time:20475ms step_avg:33.02ms
step:621/2160 train_time:20508ms step_avg:33.02ms
step:622/2160 train_time:20540ms step_avg:33.02ms
step:623/2160 train_time:20574ms step_avg:33.02ms
step:624/2160 train_time:20606ms step_avg:33.02ms
step:625/2160 train_time:20640ms step_avg:33.02ms
step:626/2160 train_time:20672ms step_avg:33.02ms
step:627/2160 train_time:20705ms step_avg:33.02ms
step:628/2160 train_time:20737ms step_avg:33.02ms
step:629/2160 train_time:20771ms step_avg:33.02ms
step:630/2160 train_time:20803ms step_avg:33.02ms
step:631/2160 train_time:20836ms step_avg:33.02ms
step:632/2160 train_time:20869ms step_avg:33.02ms
step:633/2160 train_time:20902ms step_avg:33.02ms
step:634/2160 train_time:20934ms step_avg:33.02ms
step:635/2160 train_time:20967ms step_avg:33.02ms
step:636/2160 train_time:20999ms step_avg:33.02ms
step:637/2160 train_time:21033ms step_avg:33.02ms
step:638/2160 train_time:21065ms step_avg:33.02ms
step:639/2160 train_time:21098ms step_avg:33.02ms
step:640/2160 train_time:21130ms step_avg:33.02ms
step:641/2160 train_time:21164ms step_avg:33.02ms
step:642/2160 train_time:21196ms step_avg:33.02ms
step:643/2160 train_time:21229ms step_avg:33.02ms
step:644/2160 train_time:21261ms step_avg:33.01ms
step:645/2160 train_time:21294ms step_avg:33.01ms
step:646/2160 train_time:21326ms step_avg:33.01ms
step:647/2160 train_time:21360ms step_avg:33.01ms
step:648/2160 train_time:21392ms step_avg:33.01ms
step:649/2160 train_time:21426ms step_avg:33.01ms
step:650/2160 train_time:21458ms step_avg:33.01ms
step:651/2160 train_time:21491ms step_avg:33.01ms
step:652/2160 train_time:21523ms step_avg:33.01ms
step:653/2160 train_time:21557ms step_avg:33.01ms
step:654/2160 train_time:21589ms step_avg:33.01ms
step:655/2160 train_time:21622ms step_avg:33.01ms
step:656/2160 train_time:21655ms step_avg:33.01ms
step:657/2160 train_time:21688ms step_avg:33.01ms
step:658/2160 train_time:21720ms step_avg:33.01ms
step:659/2160 train_time:21753ms step_avg:33.01ms
step:660/2160 train_time:21785ms step_avg:33.01ms
step:661/2160 train_time:21819ms step_avg:33.01ms
step:662/2160 train_time:21851ms step_avg:33.01ms
step:663/2160 train_time:21884ms step_avg:33.01ms
step:664/2160 train_time:21917ms step_avg:33.01ms
step:665/2160 train_time:21950ms step_avg:33.01ms
step:666/2160 train_time:21982ms step_avg:33.01ms
step:667/2160 train_time:22016ms step_avg:33.01ms
step:668/2160 train_time:22048ms step_avg:33.01ms
step:669/2160 train_time:22081ms step_avg:33.01ms
step:670/2160 train_time:22113ms step_avg:33.00ms
step:671/2160 train_time:22147ms step_avg:33.01ms
step:672/2160 train_time:22179ms step_avg:33.00ms
step:673/2160 train_time:22212ms step_avg:33.00ms
step:674/2160 train_time:22244ms step_avg:33.00ms
step:675/2160 train_time:22278ms step_avg:33.00ms
step:676/2160 train_time:22310ms step_avg:33.00ms
step:677/2160 train_time:22343ms step_avg:33.00ms
step:678/2160 train_time:22376ms step_avg:33.00ms
step:679/2160 train_time:22409ms step_avg:33.00ms
step:680/2160 train_time:22441ms step_avg:33.00ms
step:681/2160 train_time:22474ms step_avg:33.00ms
step:682/2160 train_time:22506ms step_avg:33.00ms
step:683/2160 train_time:22539ms step_avg:33.00ms
step:684/2160 train_time:22572ms step_avg:33.00ms
step:685/2160 train_time:22605ms step_avg:33.00ms
step:686/2160 train_time:22637ms step_avg:33.00ms
step:687/2160 train_time:22671ms step_avg:33.00ms
step:688/2160 train_time:22703ms step_avg:33.00ms
step:689/2160 train_time:22736ms step_avg:33.00ms
step:690/2160 train_time:22768ms step_avg:33.00ms
step:691/2160 train_time:22802ms step_avg:33.00ms
step:692/2160 train_time:22834ms step_avg:33.00ms
step:693/2160 train_time:22867ms step_avg:33.00ms
step:694/2160 train_time:22899ms step_avg:33.00ms
step:695/2160 train_time:22933ms step_avg:33.00ms
step:696/2160 train_time:22965ms step_avg:33.00ms
step:697/2160 train_time:22999ms step_avg:33.00ms
step:698/2160 train_time:23032ms step_avg:33.00ms
step:699/2160 train_time:23065ms step_avg:33.00ms
step:700/2160 train_time:23097ms step_avg:33.00ms
step:701/2160 train_time:23131ms step_avg:33.00ms
step:702/2160 train_time:23163ms step_avg:33.00ms
step:703/2160 train_time:23196ms step_avg:33.00ms
step:704/2160 train_time:23228ms step_avg:32.99ms
step:705/2160 train_time:23262ms step_avg:33.00ms
step:706/2160 train_time:23294ms step_avg:32.99ms
step:707/2160 train_time:23327ms step_avg:32.99ms
step:708/2160 train_time:23361ms step_avg:33.00ms
step:709/2160 train_time:23419ms step_avg:33.03ms
step:710/2160 train_time:23477ms step_avg:33.07ms
step:711/2160 train_time:23538ms step_avg:33.11ms
step:712/2160 train_time:23596ms step_avg:33.14ms
step:713/2160 train_time:23656ms step_avg:33.18ms
step:714/2160 train_time:23715ms step_avg:33.21ms
step:715/2160 train_time:23775ms step_avg:33.25ms
step:716/2160 train_time:23834ms step_avg:33.29ms
step:717/2160 train_time:23894ms step_avg:33.33ms
step:718/2160 train_time:23953ms step_avg:33.36ms
step:719/2160 train_time:24014ms step_avg:33.40ms
step:720/2160 train_time:24073ms step_avg:33.43ms
step:721/2160 train_time:24133ms step_avg:33.47ms
step:722/2160 train_time:24192ms step_avg:33.51ms
step:723/2160 train_time:24253ms step_avg:33.54ms
step:724/2160 train_time:24311ms step_avg:33.58ms
step:725/2160 train_time:24372ms step_avg:33.62ms
step:726/2160 train_time:24430ms step_avg:33.65ms
step:727/2160 train_time:24491ms step_avg:33.69ms
step:728/2160 train_time:24550ms step_avg:33.72ms
step:729/2160 train_time:24611ms step_avg:33.76ms
step:730/2160 train_time:24671ms step_avg:33.80ms
step:731/2160 train_time:24731ms step_avg:33.83ms
step:732/2160 train_time:24790ms step_avg:33.87ms
step:733/2160 train_time:24851ms step_avg:33.90ms
step:734/2160 train_time:24910ms step_avg:33.94ms
step:735/2160 train_time:24972ms step_avg:33.98ms
step:736/2160 train_time:25031ms step_avg:34.01ms
step:737/2160 train_time:25092ms step_avg:34.05ms
step:738/2160 train_time:25150ms step_avg:34.08ms
step:739/2160 train_time:25211ms step_avg:34.12ms
step:740/2160 train_time:25271ms step_avg:34.15ms
step:741/2160 train_time:25331ms step_avg:34.18ms
step:742/2160 train_time:25390ms step_avg:34.22ms
step:743/2160 train_time:25452ms step_avg:34.26ms
step:744/2160 train_time:25511ms step_avg:34.29ms
step:745/2160 train_time:25572ms step_avg:34.33ms
step:746/2160 train_time:25631ms step_avg:34.36ms
step:747/2160 train_time:25692ms step_avg:34.39ms
step:748/2160 train_time:25751ms step_avg:34.43ms
step:749/2160 train_time:25812ms step_avg:34.46ms
step:750/2160 train_time:25871ms step_avg:34.50ms
step:750/2160 val_loss:3.8675 train_time:25934ms step_avg:34.58ms
step:751/2160 train_time:25957ms step_avg:34.56ms
step:752/2160 train_time:25993ms step_avg:34.57ms
step:753/2160 train_time:26059ms step_avg:34.61ms
step:754/2160 train_time:26121ms step_avg:34.64ms
step:755/2160 train_time:26181ms step_avg:34.68ms
step:756/2160 train_time:26240ms step_avg:34.71ms
step:757/2160 train_time:26299ms step_avg:34.74ms
step:758/2160 train_time:26357ms step_avg:34.77ms
step:759/2160 train_time:26418ms step_avg:34.81ms
step:760/2160 train_time:26476ms step_avg:34.84ms
step:761/2160 train_time:26536ms step_avg:34.87ms
step:762/2160 train_time:26594ms step_avg:34.90ms
step:763/2160 train_time:26654ms step_avg:34.93ms
step:764/2160 train_time:26712ms step_avg:34.96ms
step:765/2160 train_time:26771ms step_avg:34.99ms
step:766/2160 train_time:26829ms step_avg:35.03ms
step:767/2160 train_time:26891ms step_avg:35.06ms
step:768/2160 train_time:26951ms step_avg:35.09ms
step:769/2160 train_time:27014ms step_avg:35.13ms
step:770/2160 train_time:27075ms step_avg:35.16ms
step:771/2160 train_time:27136ms step_avg:35.20ms
step:772/2160 train_time:27196ms step_avg:35.23ms
step:773/2160 train_time:27256ms step_avg:35.26ms
step:774/2160 train_time:27316ms step_avg:35.29ms
step:775/2160 train_time:27376ms step_avg:35.32ms
step:776/2160 train_time:27435ms step_avg:35.35ms
step:777/2160 train_time:27495ms step_avg:35.39ms
step:778/2160 train_time:27554ms step_avg:35.42ms
step:779/2160 train_time:27614ms step_avg:35.45ms
step:780/2160 train_time:27672ms step_avg:35.48ms
step:781/2160 train_time:27732ms step_avg:35.51ms
step:782/2160 train_time:27791ms step_avg:35.54ms
step:783/2160 train_time:27851ms step_avg:35.57ms
step:784/2160 train_time:27911ms step_avg:35.60ms
step:785/2160 train_time:27973ms step_avg:35.63ms
step:786/2160 train_time:28034ms step_avg:35.67ms
step:787/2160 train_time:28097ms step_avg:35.70ms
step:788/2160 train_time:28156ms step_avg:35.73ms
step:789/2160 train_time:28217ms step_avg:35.76ms
step:790/2160 train_time:28276ms step_avg:35.79ms
step:791/2160 train_time:28336ms step_avg:35.82ms
step:792/2160 train_time:28395ms step_avg:35.85ms
step:793/2160 train_time:28455ms step_avg:35.88ms
step:794/2160 train_time:28514ms step_avg:35.91ms
step:795/2160 train_time:28574ms step_avg:35.94ms
step:796/2160 train_time:28633ms step_avg:35.97ms
step:797/2160 train_time:28692ms step_avg:36.00ms
step:798/2160 train_time:28750ms step_avg:36.03ms
step:799/2160 train_time:28811ms step_avg:36.06ms
step:800/2160 train_time:28870ms step_avg:36.09ms
step:801/2160 train_time:28931ms step_avg:36.12ms
step:802/2160 train_time:28990ms step_avg:36.15ms
step:803/2160 train_time:29052ms step_avg:36.18ms
step:804/2160 train_time:29112ms step_avg:36.21ms
step:805/2160 train_time:29174ms step_avg:36.24ms
step:806/2160 train_time:29233ms step_avg:36.27ms
step:807/2160 train_time:29294ms step_avg:36.30ms
step:808/2160 train_time:29353ms step_avg:36.33ms
step:809/2160 train_time:29414ms step_avg:36.36ms
step:810/2160 train_time:29472ms step_avg:36.39ms
step:811/2160 train_time:29532ms step_avg:36.41ms
step:812/2160 train_time:29591ms step_avg:36.44ms
step:813/2160 train_time:29651ms step_avg:36.47ms
step:814/2160 train_time:29711ms step_avg:36.50ms
step:815/2160 train_time:29771ms step_avg:36.53ms
step:816/2160 train_time:29829ms step_avg:36.56ms
step:817/2160 train_time:29890ms step_avg:36.59ms
step:818/2160 train_time:29950ms step_avg:36.61ms
step:819/2160 train_time:30012ms step_avg:36.64ms
step:820/2160 train_time:30072ms step_avg:36.67ms
step:821/2160 train_time:30133ms step_avg:36.70ms
step:822/2160 train_time:30193ms step_avg:36.73ms
step:823/2160 train_time:30254ms step_avg:36.76ms
step:824/2160 train_time:30314ms step_avg:36.79ms
step:825/2160 train_time:30374ms step_avg:36.82ms
step:826/2160 train_time:30433ms step_avg:36.84ms
step:827/2160 train_time:30493ms step_avg:36.87ms
step:828/2160 train_time:30552ms step_avg:36.90ms
step:829/2160 train_time:30612ms step_avg:36.93ms
step:830/2160 train_time:30671ms step_avg:36.95ms
step:831/2160 train_time:30731ms step_avg:36.98ms
step:832/2160 train_time:30789ms step_avg:37.01ms
step:833/2160 train_time:30850ms step_avg:37.03ms
step:834/2160 train_time:30909ms step_avg:37.06ms
step:835/2160 train_time:30969ms step_avg:37.09ms
step:836/2160 train_time:31029ms step_avg:37.12ms
step:837/2160 train_time:31090ms step_avg:37.14ms
step:838/2160 train_time:31149ms step_avg:37.17ms
step:839/2160 train_time:31210ms step_avg:37.20ms
step:840/2160 train_time:31269ms step_avg:37.22ms
step:841/2160 train_time:31329ms step_avg:37.25ms
step:842/2160 train_time:31389ms step_avg:37.28ms
step:843/2160 train_time:31450ms step_avg:37.31ms
step:844/2160 train_time:31509ms step_avg:37.33ms
step:845/2160 train_time:31569ms step_avg:37.36ms
step:846/2160 train_time:31628ms step_avg:37.39ms
step:847/2160 train_time:31688ms step_avg:37.41ms
step:848/2160 train_time:31747ms step_avg:37.44ms
step:849/2160 train_time:31808ms step_avg:37.46ms
step:850/2160 train_time:31866ms step_avg:37.49ms
step:851/2160 train_time:31926ms step_avg:37.52ms
step:852/2160 train_time:31985ms step_avg:37.54ms
step:853/2160 train_time:32046ms step_avg:37.57ms
step:854/2160 train_time:32105ms step_avg:37.59ms
step:855/2160 train_time:32166ms step_avg:37.62ms
step:856/2160 train_time:32224ms step_avg:37.64ms
step:857/2160 train_time:32284ms step_avg:37.67ms
step:858/2160 train_time:32343ms step_avg:37.70ms
step:859/2160 train_time:32403ms step_avg:37.72ms
step:860/2160 train_time:32461ms step_avg:37.75ms
step:861/2160 train_time:32521ms step_avg:37.77ms
step:862/2160 train_time:32579ms step_avg:37.79ms
step:863/2160 train_time:32639ms step_avg:37.82ms
step:864/2160 train_time:32698ms step_avg:37.84ms
step:865/2160 train_time:32758ms step_avg:37.87ms
step:866/2160 train_time:32817ms step_avg:37.89ms
step:867/2160 train_time:32877ms step_avg:37.92ms
step:868/2160 train_time:32936ms step_avg:37.95ms
step:869/2160 train_time:32997ms step_avg:37.97ms
step:870/2160 train_time:33057ms step_avg:38.00ms
step:871/2160 train_time:33118ms step_avg:38.02ms
step:872/2160 train_time:33177ms step_avg:38.05ms
step:873/2160 train_time:33238ms step_avg:38.07ms
step:874/2160 train_time:33297ms step_avg:38.10ms
step:875/2160 train_time:33357ms step_avg:38.12ms
step:876/2160 train_time:33416ms step_avg:38.15ms
step:877/2160 train_time:33476ms step_avg:38.17ms
step:878/2160 train_time:33534ms step_avg:38.19ms
step:879/2160 train_time:33595ms step_avg:38.22ms
step:880/2160 train_time:33654ms step_avg:38.24ms
step:881/2160 train_time:33715ms step_avg:38.27ms
step:882/2160 train_time:33774ms step_avg:38.29ms
step:883/2160 train_time:33833ms step_avg:38.32ms
step:884/2160 train_time:33893ms step_avg:38.34ms
step:885/2160 train_time:33955ms step_avg:38.37ms
step:886/2160 train_time:34015ms step_avg:38.39ms
step:887/2160 train_time:34075ms step_avg:38.42ms
step:888/2160 train_time:34135ms step_avg:38.44ms
step:889/2160 train_time:34195ms step_avg:38.46ms
step:890/2160 train_time:34255ms step_avg:38.49ms
step:891/2160 train_time:34316ms step_avg:38.51ms
step:892/2160 train_time:34375ms step_avg:38.54ms
step:893/2160 train_time:34435ms step_avg:38.56ms
step:894/2160 train_time:34494ms step_avg:38.58ms
step:895/2160 train_time:34554ms step_avg:38.61ms
step:896/2160 train_time:34613ms step_avg:38.63ms
step:897/2160 train_time:34673ms step_avg:38.65ms
step:898/2160 train_time:34732ms step_avg:38.68ms
step:899/2160 train_time:34793ms step_avg:38.70ms
step:900/2160 train_time:34852ms step_avg:38.72ms
step:901/2160 train_time:34912ms step_avg:38.75ms
step:902/2160 train_time:34971ms step_avg:38.77ms
step:903/2160 train_time:35032ms step_avg:38.80ms
step:904/2160 train_time:35092ms step_avg:38.82ms
step:905/2160 train_time:35153ms step_avg:38.84ms
step:906/2160 train_time:35212ms step_avg:38.87ms
step:907/2160 train_time:35273ms step_avg:38.89ms
step:908/2160 train_time:35333ms step_avg:38.91ms
step:909/2160 train_time:35394ms step_avg:38.94ms
step:910/2160 train_time:35453ms step_avg:38.96ms
step:911/2160 train_time:35514ms step_avg:38.98ms
step:912/2160 train_time:35572ms step_avg:39.00ms
step:913/2160 train_time:35633ms step_avg:39.03ms
step:914/2160 train_time:35692ms step_avg:39.05ms
step:915/2160 train_time:35753ms step_avg:39.07ms
step:916/2160 train_time:35812ms step_avg:39.10ms
step:917/2160 train_time:35873ms step_avg:39.12ms
step:918/2160 train_time:35932ms step_avg:39.14ms
step:919/2160 train_time:35992ms step_avg:39.16ms
step:920/2160 train_time:36051ms step_avg:39.19ms
step:921/2160 train_time:36113ms step_avg:39.21ms
step:922/2160 train_time:36172ms step_avg:39.23ms
step:923/2160 train_time:36233ms step_avg:39.26ms
step:924/2160 train_time:36292ms step_avg:39.28ms
step:925/2160 train_time:36354ms step_avg:39.30ms
step:926/2160 train_time:36413ms step_avg:39.32ms
step:927/2160 train_time:36474ms step_avg:39.35ms
step:928/2160 train_time:36533ms step_avg:39.37ms
step:929/2160 train_time:36593ms step_avg:39.39ms
step:930/2160 train_time:36652ms step_avg:39.41ms
step:931/2160 train_time:36712ms step_avg:39.43ms
step:932/2160 train_time:36771ms step_avg:39.45ms
step:933/2160 train_time:36832ms step_avg:39.48ms
step:934/2160 train_time:36892ms step_avg:39.50ms
step:935/2160 train_time:36953ms step_avg:39.52ms
step:936/2160 train_time:37012ms step_avg:39.54ms
step:937/2160 train_time:37073ms step_avg:39.57ms
step:938/2160 train_time:37132ms step_avg:39.59ms
step:939/2160 train_time:37193ms step_avg:39.61ms
step:940/2160 train_time:37253ms step_avg:39.63ms
step:941/2160 train_time:37314ms step_avg:39.65ms
step:942/2160 train_time:37373ms step_avg:39.67ms
step:943/2160 train_time:37434ms step_avg:39.70ms
step:944/2160 train_time:37494ms step_avg:39.72ms
step:945/2160 train_time:37556ms step_avg:39.74ms
step:946/2160 train_time:37615ms step_avg:39.76ms
step:947/2160 train_time:37675ms step_avg:39.78ms
step:948/2160 train_time:37735ms step_avg:39.80ms
step:949/2160 train_time:37796ms step_avg:39.83ms
step:950/2160 train_time:37855ms step_avg:39.85ms
step:951/2160 train_time:37915ms step_avg:39.87ms
step:952/2160 train_time:37974ms step_avg:39.89ms
step:953/2160 train_time:38035ms step_avg:39.91ms
step:954/2160 train_time:38094ms step_avg:39.93ms
step:955/2160 train_time:38156ms step_avg:39.95ms
step:956/2160 train_time:38215ms step_avg:39.97ms
step:957/2160 train_time:38276ms step_avg:40.00ms
step:958/2160 train_time:38335ms step_avg:40.02ms
step:959/2160 train_time:38396ms step_avg:40.04ms
step:960/2160 train_time:38456ms step_avg:40.06ms
step:961/2160 train_time:38516ms step_avg:40.08ms
step:962/2160 train_time:38575ms step_avg:40.10ms
step:963/2160 train_time:38635ms step_avg:40.12ms
step:964/2160 train_time:38695ms step_avg:40.14ms
step:965/2160 train_time:38756ms step_avg:40.16ms
step:966/2160 train_time:38815ms step_avg:40.18ms
step:967/2160 train_time:38875ms step_avg:40.20ms
step:968/2160 train_time:38934ms step_avg:40.22ms
step:969/2160 train_time:38995ms step_avg:40.24ms
step:970/2160 train_time:39054ms step_avg:40.26ms
step:971/2160 train_time:39115ms step_avg:40.28ms
step:972/2160 train_time:39175ms step_avg:40.30ms
step:973/2160 train_time:39236ms step_avg:40.32ms
step:974/2160 train_time:39296ms step_avg:40.34ms
step:975/2160 train_time:39356ms step_avg:40.37ms
step:976/2160 train_time:39416ms step_avg:40.39ms
step:977/2160 train_time:39476ms step_avg:40.41ms
step:978/2160 train_time:39535ms step_avg:40.42ms
step:979/2160 train_time:39596ms step_avg:40.45ms
step:980/2160 train_time:39656ms step_avg:40.47ms
step:981/2160 train_time:39717ms step_avg:40.49ms
step:982/2160 train_time:39776ms step_avg:40.51ms
step:983/2160 train_time:39837ms step_avg:40.53ms
step:984/2160 train_time:39896ms step_avg:40.54ms
step:985/2160 train_time:39956ms step_avg:40.56ms
step:986/2160 train_time:40016ms step_avg:40.58ms
step:987/2160 train_time:40076ms step_avg:40.60ms
step:988/2160 train_time:40136ms step_avg:40.62ms
step:989/2160 train_time:40197ms step_avg:40.64ms
step:990/2160 train_time:40255ms step_avg:40.66ms
step:991/2160 train_time:40316ms step_avg:40.68ms
step:992/2160 train_time:40375ms step_avg:40.70ms
step:993/2160 train_time:40436ms step_avg:40.72ms
step:994/2160 train_time:40496ms step_avg:40.74ms
step:995/2160 train_time:40556ms step_avg:40.76ms
step:996/2160 train_time:40616ms step_avg:40.78ms
step:997/2160 train_time:40676ms step_avg:40.80ms
step:998/2160 train_time:40736ms step_avg:40.82ms
step:999/2160 train_time:40795ms step_avg:40.84ms
step:1000/2160 train_time:40855ms step_avg:40.85ms
step:1000/2160 val_loss:3.7127 train_time:40918ms step_avg:40.92ms
step:1001/2160 train_time:40939ms step_avg:40.90ms
step:1002/2160 train_time:40978ms step_avg:40.90ms
step:1003/2160 train_time:41041ms step_avg:40.92ms
step:1004/2160 train_time:41106ms step_avg:40.94ms
step:1005/2160 train_time:41166ms step_avg:40.96ms
step:1006/2160 train_time:41225ms step_avg:40.98ms
step:1007/2160 train_time:41284ms step_avg:41.00ms
step:1008/2160 train_time:41343ms step_avg:41.01ms
step:1009/2160 train_time:41402ms step_avg:41.03ms
step:1010/2160 train_time:41460ms step_avg:41.05ms
step:1011/2160 train_time:41519ms step_avg:41.07ms
step:1012/2160 train_time:41578ms step_avg:41.08ms
step:1013/2160 train_time:41638ms step_avg:41.10ms
step:1014/2160 train_time:41696ms step_avg:41.12ms
step:1015/2160 train_time:41756ms step_avg:41.14ms
step:1016/2160 train_time:41814ms step_avg:41.16ms
step:1017/2160 train_time:41874ms step_avg:41.17ms
step:1018/2160 train_time:41934ms step_avg:41.19ms
step:1019/2160 train_time:41997ms step_avg:41.21ms
step:1020/2160 train_time:42059ms step_avg:41.23ms
step:1021/2160 train_time:42121ms step_avg:41.26ms
step:1022/2160 train_time:42181ms step_avg:41.27ms
step:1023/2160 train_time:42242ms step_avg:41.29ms
step:1024/2160 train_time:42300ms step_avg:41.31ms
step:1025/2160 train_time:42361ms step_avg:41.33ms
step:1026/2160 train_time:42419ms step_avg:41.34ms
step:1027/2160 train_time:42479ms step_avg:41.36ms
step:1028/2160 train_time:42537ms step_avg:41.38ms
step:1029/2160 train_time:42597ms step_avg:41.40ms
step:1030/2160 train_time:42656ms step_avg:41.41ms
step:1031/2160 train_time:42716ms step_avg:41.43ms
step:1032/2160 train_time:42774ms step_avg:41.45ms
step:1033/2160 train_time:42835ms step_avg:41.47ms
step:1034/2160 train_time:42895ms step_avg:41.48ms
step:1035/2160 train_time:42957ms step_avg:41.50ms
step:1036/2160 train_time:43017ms step_avg:41.52ms
step:1037/2160 train_time:43080ms step_avg:41.54ms
step:1038/2160 train_time:43141ms step_avg:41.56ms
step:1039/2160 train_time:43201ms step_avg:41.58ms
step:1040/2160 train_time:43260ms step_avg:41.60ms
step:1041/2160 train_time:43320ms step_avg:41.61ms
step:1042/2160 train_time:43379ms step_avg:41.63ms
step:1043/2160 train_time:43440ms step_avg:41.65ms
step:1044/2160 train_time:43498ms step_avg:41.66ms
step:1045/2160 train_time:43558ms step_avg:41.68ms
step:1046/2160 train_time:43616ms step_avg:41.70ms
step:1047/2160 train_time:43676ms step_avg:41.72ms
step:1048/2160 train_time:43736ms step_avg:41.73ms
step:1049/2160 train_time:43796ms step_avg:41.75ms
step:1050/2160 train_time:43855ms step_avg:41.77ms
step:1051/2160 train_time:43916ms step_avg:41.79ms
step:1052/2160 train_time:43976ms step_avg:41.80ms
step:1053/2160 train_time:44038ms step_avg:41.82ms
step:1054/2160 train_time:44098ms step_avg:41.84ms
step:1055/2160 train_time:44160ms step_avg:41.86ms
step:1056/2160 train_time:44219ms step_avg:41.87ms
step:1057/2160 train_time:44280ms step_avg:41.89ms
step:1058/2160 train_time:44339ms step_avg:41.91ms
step:1059/2160 train_time:44399ms step_avg:41.93ms
step:1060/2160 train_time:44458ms step_avg:41.94ms
step:1061/2160 train_time:44518ms step_avg:41.96ms
step:1062/2160 train_time:44577ms step_avg:41.97ms
step:1063/2160 train_time:44637ms step_avg:41.99ms
step:1064/2160 train_time:44696ms step_avg:42.01ms
step:1065/2160 train_time:44756ms step_avg:42.02ms
step:1066/2160 train_time:44815ms step_avg:42.04ms
step:1067/2160 train_time:44877ms step_avg:42.06ms
step:1068/2160 train_time:44937ms step_avg:42.08ms
step:1069/2160 train_time:44999ms step_avg:42.09ms
step:1070/2160 train_time:45058ms step_avg:42.11ms
step:1071/2160 train_time:45119ms step_avg:42.13ms
step:1072/2160 train_time:45178ms step_avg:42.14ms
step:1073/2160 train_time:45240ms step_avg:42.16ms
step:1074/2160 train_time:45299ms step_avg:42.18ms
step:1075/2160 train_time:45360ms step_avg:42.19ms
step:1076/2160 train_time:45418ms step_avg:42.21ms
step:1077/2160 train_time:45478ms step_avg:42.23ms
step:1078/2160 train_time:45538ms step_avg:42.24ms
step:1079/2160 train_time:45598ms step_avg:42.26ms
step:1080/2160 train_time:45657ms step_avg:42.28ms
step:1081/2160 train_time:45718ms step_avg:42.29ms
step:1082/2160 train_time:45777ms step_avg:42.31ms
step:1083/2160 train_time:45839ms step_avg:42.33ms
step:1084/2160 train_time:45898ms step_avg:42.34ms
step:1085/2160 train_time:45959ms step_avg:42.36ms
step:1086/2160 train_time:46018ms step_avg:42.37ms
step:1087/2160 train_time:46079ms step_avg:42.39ms
step:1088/2160 train_time:46139ms step_avg:42.41ms
step:1089/2160 train_time:46200ms step_avg:42.42ms
step:1090/2160 train_time:46259ms step_avg:42.44ms
step:1091/2160 train_time:46320ms step_avg:42.46ms
step:1092/2160 train_time:46378ms step_avg:42.47ms
step:1093/2160 train_time:46439ms step_avg:42.49ms
step:1094/2160 train_time:46498ms step_avg:42.50ms
step:1095/2160 train_time:46559ms step_avg:42.52ms
step:1096/2160 train_time:46617ms step_avg:42.53ms
step:1097/2160 train_time:46677ms step_avg:42.55ms
step:1098/2160 train_time:46736ms step_avg:42.56ms
step:1099/2160 train_time:46798ms step_avg:42.58ms
step:1100/2160 train_time:46858ms step_avg:42.60ms
step:1101/2160 train_time:46919ms step_avg:42.61ms
step:1102/2160 train_time:46978ms step_avg:42.63ms
step:1103/2160 train_time:47039ms step_avg:42.65ms
step:1104/2160 train_time:47099ms step_avg:42.66ms
step:1105/2160 train_time:47160ms step_avg:42.68ms
step:1106/2160 train_time:47219ms step_avg:42.69ms
step:1107/2160 train_time:47280ms step_avg:42.71ms
step:1108/2160 train_time:47339ms step_avg:42.72ms
step:1109/2160 train_time:47399ms step_avg:42.74ms
step:1110/2160 train_time:47459ms step_avg:42.76ms
step:1111/2160 train_time:47519ms step_avg:42.77ms
step:1112/2160 train_time:47578ms step_avg:42.79ms
step:1113/2160 train_time:47638ms step_avg:42.80ms
step:1114/2160 train_time:47697ms step_avg:42.82ms
step:1115/2160 train_time:47758ms step_avg:42.83ms
step:1116/2160 train_time:47817ms step_avg:42.85ms
step:1117/2160 train_time:47878ms step_avg:42.86ms
step:1118/2160 train_time:47938ms step_avg:42.88ms
step:1119/2160 train_time:47999ms step_avg:42.89ms
step:1120/2160 train_time:48058ms step_avg:42.91ms
step:1121/2160 train_time:48119ms step_avg:42.93ms
step:1122/2160 train_time:48179ms step_avg:42.94ms
step:1123/2160 train_time:48240ms step_avg:42.96ms
step:1124/2160 train_time:48299ms step_avg:42.97ms
step:1125/2160 train_time:48360ms step_avg:42.99ms
step:1126/2160 train_time:48418ms step_avg:43.00ms
step:1127/2160 train_time:48479ms step_avg:43.02ms
step:1128/2160 train_time:48538ms step_avg:43.03ms
step:1129/2160 train_time:48599ms step_avg:43.05ms
step:1130/2160 train_time:48658ms step_avg:43.06ms
step:1131/2160 train_time:48718ms step_avg:43.07ms
step:1132/2160 train_time:48777ms step_avg:43.09ms
step:1133/2160 train_time:48839ms step_avg:43.11ms
step:1134/2160 train_time:48898ms step_avg:43.12ms
step:1135/2160 train_time:48959ms step_avg:43.14ms
step:1136/2160 train_time:49018ms step_avg:43.15ms
step:1137/2160 train_time:49079ms step_avg:43.17ms
step:1138/2160 train_time:49138ms step_avg:43.18ms
step:1139/2160 train_time:49199ms step_avg:43.20ms
step:1140/2160 train_time:49259ms step_avg:43.21ms
step:1141/2160 train_time:49319ms step_avg:43.22ms
step:1142/2160 train_time:49378ms step_avg:43.24ms
step:1143/2160 train_time:49439ms step_avg:43.25ms
step:1144/2160 train_time:49498ms step_avg:43.27ms
step:1145/2160 train_time:49558ms step_avg:43.28ms
step:1146/2160 train_time:49617ms step_avg:43.30ms
step:1147/2160 train_time:49678ms step_avg:43.31ms
step:1148/2160 train_time:49737ms step_avg:43.32ms
step:1149/2160 train_time:49797ms step_avg:43.34ms
step:1150/2160 train_time:49857ms step_avg:43.35ms
step:1151/2160 train_time:49917ms step_avg:43.37ms
step:1152/2160 train_time:49976ms step_avg:43.38ms
step:1153/2160 train_time:50037ms step_avg:43.40ms
step:1154/2160 train_time:50097ms step_avg:43.41ms
step:1155/2160 train_time:50158ms step_avg:43.43ms
step:1156/2160 train_time:50217ms step_avg:43.44ms
step:1157/2160 train_time:50278ms step_avg:43.46ms
step:1158/2160 train_time:50338ms step_avg:43.47ms
step:1159/2160 train_time:50399ms step_avg:43.48ms
step:1160/2160 train_time:50459ms step_avg:43.50ms
step:1161/2160 train_time:50519ms step_avg:43.51ms
step:1162/2160 train_time:50578ms step_avg:43.53ms
step:1163/2160 train_time:50639ms step_avg:43.54ms
step:1164/2160 train_time:50697ms step_avg:43.55ms
step:1165/2160 train_time:50757ms step_avg:43.57ms
step:1166/2160 train_time:50817ms step_avg:43.58ms
step:1167/2160 train_time:50878ms step_avg:43.60ms
step:1168/2160 train_time:50938ms step_avg:43.61ms
step:1169/2160 train_time:50999ms step_avg:43.63ms
step:1170/2160 train_time:51058ms step_avg:43.64ms
step:1171/2160 train_time:51119ms step_avg:43.65ms
step:1172/2160 train_time:51178ms step_avg:43.67ms
step:1173/2160 train_time:51239ms step_avg:43.68ms
step:1174/2160 train_time:51298ms step_avg:43.70ms
step:1175/2160 train_time:51359ms step_avg:43.71ms
step:1176/2160 train_time:51418ms step_avg:43.72ms
step:1177/2160 train_time:51480ms step_avg:43.74ms
step:1178/2160 train_time:51539ms step_avg:43.75ms
step:1179/2160 train_time:51600ms step_avg:43.77ms
step:1180/2160 train_time:51658ms step_avg:43.78ms
step:1181/2160 train_time:51718ms step_avg:43.79ms
step:1182/2160 train_time:51777ms step_avg:43.80ms
step:1183/2160 train_time:51838ms step_avg:43.82ms
step:1184/2160 train_time:51897ms step_avg:43.83ms
step:1185/2160 train_time:51958ms step_avg:43.85ms
step:1186/2160 train_time:52018ms step_avg:43.86ms
step:1187/2160 train_time:52079ms step_avg:43.87ms
step:1188/2160 train_time:52139ms step_avg:43.89ms
step:1189/2160 train_time:52199ms step_avg:43.90ms
step:1190/2160 train_time:52258ms step_avg:43.91ms
step:1191/2160 train_time:52320ms step_avg:43.93ms
step:1192/2160 train_time:52379ms step_avg:43.94ms
step:1193/2160 train_time:52439ms step_avg:43.96ms
step:1194/2160 train_time:52498ms step_avg:43.97ms
step:1195/2160 train_time:52559ms step_avg:43.98ms
step:1196/2160 train_time:52618ms step_avg:43.99ms
step:1197/2160 train_time:52679ms step_avg:44.01ms
step:1198/2160 train_time:52739ms step_avg:44.02ms
step:1199/2160 train_time:52799ms step_avg:44.04ms
step:1200/2160 train_time:52858ms step_avg:44.05ms
step:1201/2160 train_time:52918ms step_avg:44.06ms
step:1202/2160 train_time:52977ms step_avg:44.07ms
step:1203/2160 train_time:53039ms step_avg:44.09ms
step:1204/2160 train_time:53098ms step_avg:44.10ms
step:1205/2160 train_time:53158ms step_avg:44.11ms
step:1206/2160 train_time:53218ms step_avg:44.13ms
step:1207/2160 train_time:53279ms step_avg:44.14ms
step:1208/2160 train_time:53338ms step_avg:44.15ms
step:1209/2160 train_time:53399ms step_avg:44.17ms
step:1210/2160 train_time:53459ms step_avg:44.18ms
step:1211/2160 train_time:53519ms step_avg:44.19ms
step:1212/2160 train_time:53578ms step_avg:44.21ms
step:1213/2160 train_time:53639ms step_avg:44.22ms
step:1214/2160 train_time:53699ms step_avg:44.23ms
step:1215/2160 train_time:53759ms step_avg:44.25ms
step:1216/2160 train_time:53818ms step_avg:44.26ms
step:1217/2160 train_time:53879ms step_avg:44.27ms
step:1218/2160 train_time:53938ms step_avg:44.28ms
step:1219/2160 train_time:53999ms step_avg:44.30ms
step:1220/2160 train_time:54059ms step_avg:44.31ms
step:1221/2160 train_time:54119ms step_avg:44.32ms
step:1222/2160 train_time:54178ms step_avg:44.34ms
step:1223/2160 train_time:54239ms step_avg:44.35ms
step:1224/2160 train_time:54298ms step_avg:44.36ms
step:1225/2160 train_time:54359ms step_avg:44.37ms
step:1226/2160 train_time:54419ms step_avg:44.39ms
step:1227/2160 train_time:54479ms step_avg:44.40ms
step:1228/2160 train_time:54538ms step_avg:44.41ms
step:1229/2160 train_time:54599ms step_avg:44.43ms
step:1230/2160 train_time:54658ms step_avg:44.44ms
step:1231/2160 train_time:54718ms step_avg:44.45ms
step:1232/2160 train_time:54777ms step_avg:44.46ms
step:1233/2160 train_time:54838ms step_avg:44.48ms
step:1234/2160 train_time:54897ms step_avg:44.49ms
step:1235/2160 train_time:54958ms step_avg:44.50ms
step:1236/2160 train_time:55017ms step_avg:44.51ms
step:1237/2160 train_time:55078ms step_avg:44.53ms
step:1238/2160 train_time:55137ms step_avg:44.54ms
step:1239/2160 train_time:55198ms step_avg:44.55ms
step:1240/2160 train_time:55258ms step_avg:44.56ms
step:1241/2160 train_time:55318ms step_avg:44.58ms
step:1242/2160 train_time:55378ms step_avg:44.59ms
step:1243/2160 train_time:55439ms step_avg:44.60ms
step:1244/2160 train_time:55498ms step_avg:44.61ms
step:1245/2160 train_time:55558ms step_avg:44.63ms
step:1246/2160 train_time:55618ms step_avg:44.64ms
step:1247/2160 train_time:55679ms step_avg:44.65ms
step:1248/2160 train_time:55738ms step_avg:44.66ms
step:1249/2160 train_time:55799ms step_avg:44.68ms
step:1250/2160 train_time:55859ms step_avg:44.69ms
step:1250/2160 val_loss:3.5953 train_time:55920ms step_avg:44.74ms
step:1251/2160 train_time:55942ms step_avg:44.72ms
step:1252/2160 train_time:55980ms step_avg:44.71ms
step:1253/2160 train_time:56044ms step_avg:44.73ms
step:1254/2160 train_time:56105ms step_avg:44.74ms
step:1255/2160 train_time:56166ms step_avg:44.75ms
step:1256/2160 train_time:56225ms step_avg:44.76ms
step:1257/2160 train_time:56284ms step_avg:44.78ms
step:1258/2160 train_time:56342ms step_avg:44.79ms
step:1259/2160 train_time:56402ms step_avg:44.80ms
step:1260/2160 train_time:56460ms step_avg:44.81ms
step:1261/2160 train_time:56519ms step_avg:44.82ms
step:1262/2160 train_time:56577ms step_avg:44.83ms
step:1263/2160 train_time:56637ms step_avg:44.84ms
step:1264/2160 train_time:56695ms step_avg:44.85ms
step:1265/2160 train_time:56754ms step_avg:44.87ms
step:1266/2160 train_time:56813ms step_avg:44.88ms
step:1267/2160 train_time:56876ms step_avg:44.89ms
step:1268/2160 train_time:56936ms step_avg:44.90ms
step:1269/2160 train_time:56998ms step_avg:44.92ms
step:1270/2160 train_time:57057ms step_avg:44.93ms
step:1271/2160 train_time:57119ms step_avg:44.94ms
step:1272/2160 train_time:57178ms step_avg:44.95ms
step:1273/2160 train_time:57238ms step_avg:44.96ms
step:1274/2160 train_time:57297ms step_avg:44.97ms
step:1275/2160 train_time:57356ms step_avg:44.99ms
step:1276/2160 train_time:57415ms step_avg:45.00ms
step:1277/2160 train_time:57475ms step_avg:45.01ms
step:1278/2160 train_time:57533ms step_avg:45.02ms
step:1279/2160 train_time:57593ms step_avg:45.03ms
step:1280/2160 train_time:57651ms step_avg:45.04ms
step:1281/2160 train_time:57711ms step_avg:45.05ms
step:1282/2160 train_time:57770ms step_avg:45.06ms
step:1283/2160 train_time:57832ms step_avg:45.08ms
step:1284/2160 train_time:57893ms step_avg:45.09ms
step:1285/2160 train_time:57954ms step_avg:45.10ms
step:1286/2160 train_time:58014ms step_avg:45.11ms
step:1287/2160 train_time:58076ms step_avg:45.13ms
step:1288/2160 train_time:58135ms step_avg:45.14ms
step:1289/2160 train_time:58196ms step_avg:45.15ms
step:1290/2160 train_time:58254ms step_avg:45.16ms
step:1291/2160 train_time:58314ms step_avg:45.17ms
step:1292/2160 train_time:58373ms step_avg:45.18ms
step:1293/2160 train_time:58432ms step_avg:45.19ms
step:1294/2160 train_time:58492ms step_avg:45.20ms
step:1295/2160 train_time:58551ms step_avg:45.21ms
step:1296/2160 train_time:58610ms step_avg:45.22ms
step:1297/2160 train_time:58671ms step_avg:45.24ms
step:1298/2160 train_time:58730ms step_avg:45.25ms
step:1299/2160 train_time:58791ms step_avg:45.26ms
step:1300/2160 train_time:58851ms step_avg:45.27ms
step:1301/2160 train_time:58912ms step_avg:45.28ms
step:1302/2160 train_time:58972ms step_avg:45.29ms
step:1303/2160 train_time:59034ms step_avg:45.31ms
step:1304/2160 train_time:59093ms step_avg:45.32ms
step:1305/2160 train_time:59154ms step_avg:45.33ms
step:1306/2160 train_time:59214ms step_avg:45.34ms
step:1307/2160 train_time:59275ms step_avg:45.35ms
step:1308/2160 train_time:59333ms step_avg:45.36ms
step:1309/2160 train_time:59393ms step_avg:45.37ms
step:1310/2160 train_time:59452ms step_avg:45.38ms
step:1311/2160 train_time:59513ms step_avg:45.39ms
step:1312/2160 train_time:59572ms step_avg:45.41ms
step:1313/2160 train_time:59632ms step_avg:45.42ms
step:1314/2160 train_time:59691ms step_avg:45.43ms
step:1315/2160 train_time:59751ms step_avg:45.44ms
step:1316/2160 train_time:59812ms step_avg:45.45ms
step:1317/2160 train_time:59874ms step_avg:45.46ms
step:1318/2160 train_time:59933ms step_avg:45.47ms
step:1319/2160 train_time:59994ms step_avg:45.48ms
step:1320/2160 train_time:60053ms step_avg:45.49ms
step:1321/2160 train_time:60114ms step_avg:45.51ms
step:1322/2160 train_time:60173ms step_avg:45.52ms
step:1323/2160 train_time:60234ms step_avg:45.53ms
step:1324/2160 train_time:60292ms step_avg:45.54ms
step:1325/2160 train_time:60353ms step_avg:45.55ms
step:1326/2160 train_time:60412ms step_avg:45.56ms
step:1327/2160 train_time:60472ms step_avg:45.57ms
step:1328/2160 train_time:60530ms step_avg:45.58ms
step:1329/2160 train_time:60591ms step_avg:45.59ms
step:1330/2160 train_time:60650ms step_avg:45.60ms
step:1331/2160 train_time:60710ms step_avg:45.61ms
step:1332/2160 train_time:60770ms step_avg:45.62ms
step:1333/2160 train_time:60832ms step_avg:45.64ms
step:1334/2160 train_time:60892ms step_avg:45.65ms
step:1335/2160 train_time:60953ms step_avg:45.66ms
step:1336/2160 train_time:61013ms step_avg:45.67ms
step:1337/2160 train_time:61074ms step_avg:45.68ms
step:1338/2160 train_time:61133ms step_avg:45.69ms
step:1339/2160 train_time:61193ms step_avg:45.70ms
step:1340/2160 train_time:61253ms step_avg:45.71ms
step:1341/2160 train_time:61313ms step_avg:45.72ms
step:1342/2160 train_time:61372ms step_avg:45.73ms
step:1343/2160 train_time:61433ms step_avg:45.74ms
step:1344/2160 train_time:61492ms step_avg:45.75ms
step:1345/2160 train_time:61552ms step_avg:45.76ms
step:1346/2160 train_time:61611ms step_avg:45.77ms
step:1347/2160 train_time:61671ms step_avg:45.78ms
step:1348/2160 train_time:61730ms step_avg:45.79ms
step:1349/2160 train_time:61792ms step_avg:45.81ms
step:1350/2160 train_time:61852ms step_avg:45.82ms
step:1351/2160 train_time:61913ms step_avg:45.83ms
step:1352/2160 train_time:61973ms step_avg:45.84ms
step:1353/2160 train_time:62034ms step_avg:45.85ms
step:1354/2160 train_time:62093ms step_avg:45.86ms
step:1355/2160 train_time:62153ms step_avg:45.87ms
step:1356/2160 train_time:62213ms step_avg:45.88ms
step:1357/2160 train_time:62273ms step_avg:45.89ms
step:1358/2160 train_time:62332ms step_avg:45.90ms
step:1359/2160 train_time:62393ms step_avg:45.91ms
step:1360/2160 train_time:62452ms step_avg:45.92ms
step:1361/2160 train_time:62511ms step_avg:45.93ms
step:1362/2160 train_time:62570ms step_avg:45.94ms
step:1363/2160 train_time:62631ms step_avg:45.95ms
step:1364/2160 train_time:62691ms step_avg:45.96ms
step:1365/2160 train_time:62751ms step_avg:45.97ms
step:1366/2160 train_time:62811ms step_avg:45.98ms
step:1367/2160 train_time:62872ms step_avg:45.99ms
step:1368/2160 train_time:62931ms step_avg:46.00ms
step:1369/2160 train_time:62993ms step_avg:46.01ms
step:1370/2160 train_time:63052ms step_avg:46.02ms
step:1371/2160 train_time:63112ms step_avg:46.03ms
step:1372/2160 train_time:63171ms step_avg:46.04ms
step:1373/2160 train_time:63233ms step_avg:46.05ms
step:1374/2160 train_time:63292ms step_avg:46.06ms
step:1375/2160 train_time:63353ms step_avg:46.07ms
step:1376/2160 train_time:63412ms step_avg:46.08ms
step:1377/2160 train_time:63472ms step_avg:46.09ms
step:1378/2160 train_time:63531ms step_avg:46.10ms
step:1379/2160 train_time:63592ms step_avg:46.11ms
step:1380/2160 train_time:63651ms step_avg:46.12ms
step:1381/2160 train_time:63712ms step_avg:46.13ms
step:1382/2160 train_time:63771ms step_avg:46.14ms
step:1383/2160 train_time:63833ms step_avg:46.16ms
step:1384/2160 train_time:63892ms step_avg:46.16ms
step:1385/2160 train_time:63953ms step_avg:46.18ms
step:1386/2160 train_time:64012ms step_avg:46.18ms
step:1387/2160 train_time:64073ms step_avg:46.20ms
step:1388/2160 train_time:64133ms step_avg:46.21ms
step:1389/2160 train_time:64193ms step_avg:46.22ms
step:1390/2160 train_time:64252ms step_avg:46.22ms
step:1391/2160 train_time:64313ms step_avg:46.24ms
step:1392/2160 train_time:64372ms step_avg:46.24ms
step:1393/2160 train_time:64432ms step_avg:46.25ms
step:1394/2160 train_time:64493ms step_avg:46.26ms
step:1395/2160 train_time:64553ms step_avg:46.27ms
step:1396/2160 train_time:64612ms step_avg:46.28ms
step:1397/2160 train_time:64673ms step_avg:46.29ms
step:1398/2160 train_time:64733ms step_avg:46.30ms
step:1399/2160 train_time:64793ms step_avg:46.31ms
step:1400/2160 train_time:64852ms step_avg:46.32ms
step:1401/2160 train_time:64912ms step_avg:46.33ms
step:1402/2160 train_time:64971ms step_avg:46.34ms
step:1403/2160 train_time:65032ms step_avg:46.35ms
step:1404/2160 train_time:65091ms step_avg:46.36ms
step:1405/2160 train_time:65152ms step_avg:46.37ms
step:1406/2160 train_time:65211ms step_avg:46.38ms
step:1407/2160 train_time:65272ms step_avg:46.39ms
step:1408/2160 train_time:65331ms step_avg:46.40ms
step:1409/2160 train_time:65392ms step_avg:46.41ms
step:1410/2160 train_time:65451ms step_avg:46.42ms
step:1411/2160 train_time:65511ms step_avg:46.43ms
step:1412/2160 train_time:65571ms step_avg:46.44ms
step:1413/2160 train_time:65632ms step_avg:46.45ms
step:1414/2160 train_time:65691ms step_avg:46.46ms
step:1415/2160 train_time:65753ms step_avg:46.47ms
step:1416/2160 train_time:65841ms step_avg:46.50ms
step:1417/2160 train_time:65929ms step_avg:46.53ms
step:1418/2160 train_time:66016ms step_avg:46.56ms
step:1419/2160 train_time:66105ms step_avg:46.59ms
step:1420/2160 train_time:66191ms step_avg:46.61ms
step:1421/2160 train_time:66281ms step_avg:46.64ms
step:1422/2160 train_time:66367ms step_avg:46.67ms
step:1423/2160 train_time:66454ms step_avg:46.70ms
step:1424/2160 train_time:66541ms step_avg:46.73ms
step:1425/2160 train_time:66629ms step_avg:46.76ms
step:1426/2160 train_time:66716ms step_avg:46.79ms
step:1427/2160 train_time:66806ms step_avg:46.82ms
step:1428/2160 train_time:66892ms step_avg:46.84ms
step:1429/2160 train_time:66981ms step_avg:46.87ms
step:1430/2160 train_time:67067ms step_avg:46.90ms
step:1431/2160 train_time:67156ms step_avg:46.93ms
step:1432/2160 train_time:67244ms step_avg:46.96ms
step:1433/2160 train_time:67332ms step_avg:46.99ms
step:1434/2160 train_time:67419ms step_avg:47.01ms
step:1435/2160 train_time:67509ms step_avg:47.04ms
step:1436/2160 train_time:67595ms step_avg:47.07ms
step:1437/2160 train_time:67683ms step_avg:47.10ms
step:1438/2160 train_time:67769ms step_avg:47.13ms
step:1439/2160 train_time:67859ms step_avg:47.16ms
step:1440/2160 train_time:67946ms step_avg:47.18ms
step:1441/2160 train_time:68033ms step_avg:47.21ms
step:1442/2160 train_time:68120ms step_avg:47.24ms
step:1443/2160 train_time:68209ms step_avg:47.27ms
step:1444/2160 train_time:68295ms step_avg:47.30ms
step:1445/2160 train_time:68384ms step_avg:47.32ms
step:1446/2160 train_time:68469ms step_avg:47.35ms
step:1447/2160 train_time:68558ms step_avg:47.38ms
step:1448/2160 train_time:68645ms step_avg:47.41ms
step:1449/2160 train_time:68733ms step_avg:47.44ms
step:1450/2160 train_time:68821ms step_avg:47.46ms
step:1451/2160 train_time:68909ms step_avg:47.49ms
step:1452/2160 train_time:68996ms step_avg:47.52ms
step:1453/2160 train_time:69084ms step_avg:47.55ms
step:1454/2160 train_time:69170ms step_avg:47.57ms
step:1455/2160 train_time:69259ms step_avg:47.60ms
step:1456/2160 train_time:69346ms step_avg:47.63ms
step:1457/2160 train_time:69434ms step_avg:47.66ms
step:1458/2160 train_time:69520ms step_avg:47.68ms
step:1459/2160 train_time:69609ms step_avg:47.71ms
step:1460/2160 train_time:69696ms step_avg:47.74ms
step:1461/2160 train_time:69786ms step_avg:47.77ms
step:1462/2160 train_time:69872ms step_avg:47.79ms
step:1463/2160 train_time:69960ms step_avg:47.82ms
step:1464/2160 train_time:70047ms step_avg:47.85ms
step:1465/2160 train_time:70136ms step_avg:47.87ms
step:1466/2160 train_time:70223ms step_avg:47.90ms
step:1467/2160 train_time:70310ms step_avg:47.93ms
step:1468/2160 train_time:70397ms step_avg:47.95ms
step:1469/2160 train_time:70487ms step_avg:47.98ms
step:1470/2160 train_time:70574ms step_avg:48.01ms
step:1471/2160 train_time:70662ms step_avg:48.04ms
step:1472/2160 train_time:70749ms step_avg:48.06ms
step:1473/2160 train_time:70837ms step_avg:48.09ms
step:1474/2160 train_time:70924ms step_avg:48.12ms
step:1475/2160 train_time:71013ms step_avg:48.14ms
step:1476/2160 train_time:71099ms step_avg:48.17ms
step:1477/2160 train_time:71188ms step_avg:48.20ms
step:1478/2160 train_time:71274ms step_avg:48.22ms
step:1479/2160 train_time:71363ms step_avg:48.25ms
step:1480/2160 train_time:71449ms step_avg:48.28ms
step:1481/2160 train_time:71538ms step_avg:48.30ms
step:1482/2160 train_time:71626ms step_avg:48.33ms
step:1483/2160 train_time:71714ms step_avg:48.36ms
step:1484/2160 train_time:71801ms step_avg:48.38ms
step:1485/2160 train_time:71889ms step_avg:48.41ms
step:1486/2160 train_time:71976ms step_avg:48.44ms
step:1487/2160 train_time:72065ms step_avg:48.46ms
step:1488/2160 train_time:72152ms step_avg:48.49ms
step:1489/2160 train_time:72240ms step_avg:48.52ms
step:1490/2160 train_time:72327ms step_avg:48.54ms
step:1491/2160 train_time:72415ms step_avg:48.57ms
step:1492/2160 train_time:72501ms step_avg:48.59ms
step:1493/2160 train_time:72589ms step_avg:48.62ms
step:1494/2160 train_time:72677ms step_avg:48.65ms
step:1495/2160 train_time:72767ms step_avg:48.67ms
step:1496/2160 train_time:72854ms step_avg:48.70ms
step:1497/2160 train_time:72942ms step_avg:48.73ms
step:1498/2160 train_time:73027ms step_avg:48.75ms
step:1499/2160 train_time:73116ms step_avg:48.78ms
step:1500/2160 train_time:73203ms step_avg:48.80ms
step:1500/2160 val_loss:3.4934 train_time:73292ms step_avg:48.86ms
step:1501/2160 train_time:73314ms step_avg:48.84ms
step:1502/2160 train_time:73383ms step_avg:48.86ms
step:1503/2160 train_time:73479ms step_avg:48.89ms
step:1504/2160 train_time:73566ms step_avg:48.91ms
step:1505/2160 train_time:73654ms step_avg:48.94ms
step:1506/2160 train_time:73740ms step_avg:48.96ms
step:1507/2160 train_time:73826ms step_avg:48.99ms
step:1508/2160 train_time:73912ms step_avg:49.01ms
step:1509/2160 train_time:73999ms step_avg:49.04ms
step:1510/2160 train_time:74085ms step_avg:49.06ms
step:1511/2160 train_time:74173ms step_avg:49.09ms
step:1512/2160 train_time:74261ms step_avg:49.11ms
step:1513/2160 train_time:74354ms step_avg:49.14ms
step:1514/2160 train_time:74444ms step_avg:49.17ms
step:1515/2160 train_time:74534ms step_avg:49.20ms
step:1516/2160 train_time:74620ms step_avg:49.22ms
step:1517/2160 train_time:74707ms step_avg:49.25ms
step:1518/2160 train_time:74793ms step_avg:49.27ms
step:1519/2160 train_time:74879ms step_avg:49.30ms
step:1520/2160 train_time:74965ms step_avg:49.32ms
step:1521/2160 train_time:75053ms step_avg:49.34ms
step:1522/2160 train_time:75139ms step_avg:49.37ms
step:1523/2160 train_time:75228ms step_avg:49.39ms
step:1524/2160 train_time:75317ms step_avg:49.42ms
step:1525/2160 train_time:75408ms step_avg:49.45ms
step:1526/2160 train_time:75497ms step_avg:49.47ms
step:1527/2160 train_time:75586ms step_avg:49.50ms
step:1528/2160 train_time:75672ms step_avg:49.52ms
step:1529/2160 train_time:75760ms step_avg:49.55ms
step:1530/2160 train_time:75846ms step_avg:49.57ms
step:1531/2160 train_time:75935ms step_avg:49.60ms
step:1532/2160 train_time:76020ms step_avg:49.62ms
step:1533/2160 train_time:76108ms step_avg:49.65ms
step:1534/2160 train_time:76195ms step_avg:49.67ms
step:1535/2160 train_time:76283ms step_avg:49.70ms
step:1536/2160 train_time:76372ms step_avg:49.72ms
step:1537/2160 train_time:76462ms step_avg:49.75ms
step:1538/2160 train_time:76549ms step_avg:49.77ms
step:1539/2160 train_time:76638ms step_avg:49.80ms
step:1540/2160 train_time:76725ms step_avg:49.82ms
step:1541/2160 train_time:76813ms step_avg:49.85ms
step:1542/2160 train_time:76898ms step_avg:49.87ms
step:1543/2160 train_time:76985ms step_avg:49.89ms
step:1544/2160 train_time:77072ms step_avg:49.92ms
step:1545/2160 train_time:77160ms step_avg:49.94ms
step:1546/2160 train_time:77248ms step_avg:49.97ms
step:1547/2160 train_time:77339ms step_avg:49.99ms
step:1548/2160 train_time:77426ms step_avg:50.02ms
step:1549/2160 train_time:77515ms step_avg:50.04ms
step:1550/2160 train_time:77601ms step_avg:50.07ms
step:1551/2160 train_time:77690ms step_avg:50.09ms
step:1552/2160 train_time:77777ms step_avg:50.11ms
step:1553/2160 train_time:77866ms step_avg:50.14ms
step:1554/2160 train_time:77951ms step_avg:50.16ms
step:1555/2160 train_time:78040ms step_avg:50.19ms
step:1556/2160 train_time:78126ms step_avg:50.21ms
step:1557/2160 train_time:78215ms step_avg:50.23ms
step:1558/2160 train_time:78303ms step_avg:50.26ms
step:1559/2160 train_time:78393ms step_avg:50.28ms
step:1560/2160 train_time:78479ms step_avg:50.31ms
step:1561/2160 train_time:78568ms step_avg:50.33ms
step:1562/2160 train_time:78655ms step_avg:50.36ms
step:1563/2160 train_time:78742ms step_avg:50.38ms
step:1564/2160 train_time:78829ms step_avg:50.40ms
step:1565/2160 train_time:78917ms step_avg:50.43ms
step:1566/2160 train_time:79003ms step_avg:50.45ms
step:1567/2160 train_time:79091ms step_avg:50.47ms
step:1568/2160 train_time:79178ms step_avg:50.50ms
step:1569/2160 train_time:79266ms step_avg:50.52ms
step:1570/2160 train_time:79354ms step_avg:50.54ms
step:1571/2160 train_time:79443ms step_avg:50.57ms
step:1572/2160 train_time:79529ms step_avg:50.59ms
step:1573/2160 train_time:79618ms step_avg:50.62ms
step:1574/2160 train_time:79705ms step_avg:50.64ms
step:1575/2160 train_time:79793ms step_avg:50.66ms
step:1576/2160 train_time:79879ms step_avg:50.68ms
step:1577/2160 train_time:79967ms step_avg:50.71ms
step:1578/2160 train_time:80055ms step_avg:50.73ms
step:1579/2160 train_time:80143ms step_avg:50.76ms
step:1580/2160 train_time:80229ms step_avg:50.78ms
step:1581/2160 train_time:80318ms step_avg:50.80ms
step:1582/2160 train_time:80404ms step_avg:50.82ms
step:1583/2160 train_time:80493ms step_avg:50.85ms
step:1584/2160 train_time:80580ms step_avg:50.87ms
step:1585/2160 train_time:80669ms step_avg:50.90ms
step:1586/2160 train_time:80756ms step_avg:50.92ms
step:1587/2160 train_time:80844ms step_avg:50.94ms
step:1588/2160 train_time:80931ms step_avg:50.96ms
step:1589/2160 train_time:81020ms step_avg:50.99ms
step:1590/2160 train_time:81108ms step_avg:51.01ms
step:1591/2160 train_time:81197ms step_avg:51.04ms
step:1592/2160 train_time:81283ms step_avg:51.06ms
step:1593/2160 train_time:81373ms step_avg:51.08ms
step:1594/2160 train_time:81459ms step_avg:51.10ms
step:1595/2160 train_time:81549ms step_avg:51.13ms
step:1596/2160 train_time:81637ms step_avg:51.15ms
step:1597/2160 train_time:81726ms step_avg:51.17ms
step:1598/2160 train_time:81813ms step_avg:51.20ms
step:1599/2160 train_time:81901ms step_avg:51.22ms
step:1600/2160 train_time:81987ms step_avg:51.24ms
step:1601/2160 train_time:82075ms step_avg:51.26ms
step:1602/2160 train_time:82161ms step_avg:51.29ms
step:1603/2160 train_time:82250ms step_avg:51.31ms
step:1604/2160 train_time:82337ms step_avg:51.33ms
step:1605/2160 train_time:82426ms step_avg:51.36ms
step:1606/2160 train_time:82513ms step_avg:51.38ms
step:1607/2160 train_time:82602ms step_avg:51.40ms
step:1608/2160 train_time:82689ms step_avg:51.42ms
step:1609/2160 train_time:82778ms step_avg:51.45ms
step:1610/2160 train_time:82865ms step_avg:51.47ms
step:1611/2160 train_time:82953ms step_avg:51.49ms
step:1612/2160 train_time:83039ms step_avg:51.51ms
step:1613/2160 train_time:83128ms step_avg:51.54ms
step:1614/2160 train_time:83215ms step_avg:51.56ms
step:1615/2160 train_time:83303ms step_avg:51.58ms
step:1616/2160 train_time:83390ms step_avg:51.60ms
step:1617/2160 train_time:83478ms step_avg:51.63ms
step:1618/2160 train_time:83565ms step_avg:51.65ms
step:1619/2160 train_time:83654ms step_avg:51.67ms
step:1620/2160 train_time:83740ms step_avg:51.69ms
step:1621/2160 train_time:83828ms step_avg:51.71ms
step:1622/2160 train_time:83915ms step_avg:51.74ms
step:1623/2160 train_time:84003ms step_avg:51.76ms
step:1624/2160 train_time:84090ms step_avg:51.78ms
step:1625/2160 train_time:84179ms step_avg:51.80ms
step:1626/2160 train_time:84268ms step_avg:51.83ms
step:1627/2160 train_time:84356ms step_avg:51.85ms
step:1628/2160 train_time:84442ms step_avg:51.87ms
step:1629/2160 train_time:84533ms step_avg:51.89ms
step:1630/2160 train_time:84620ms step_avg:51.91ms
step:1631/2160 train_time:84708ms step_avg:51.94ms
step:1632/2160 train_time:84795ms step_avg:51.96ms
step:1633/2160 train_time:84882ms step_avg:51.98ms
step:1634/2160 train_time:84969ms step_avg:52.00ms
step:1635/2160 train_time:85059ms step_avg:52.02ms
step:1636/2160 train_time:85146ms step_avg:52.05ms
step:1637/2160 train_time:85236ms step_avg:52.07ms
step:1638/2160 train_time:85321ms step_avg:52.09ms
step:1639/2160 train_time:85410ms step_avg:52.11ms
step:1640/2160 train_time:85497ms step_avg:52.13ms
step:1641/2160 train_time:85586ms step_avg:52.15ms
step:1642/2160 train_time:85673ms step_avg:52.18ms
step:1643/2160 train_time:85760ms step_avg:52.20ms
step:1644/2160 train_time:85847ms step_avg:52.22ms
step:1645/2160 train_time:85936ms step_avg:52.24ms
step:1646/2160 train_time:86022ms step_avg:52.26ms
step:1647/2160 train_time:86111ms step_avg:52.28ms
step:1648/2160 train_time:86199ms step_avg:52.31ms
step:1649/2160 train_time:86287ms step_avg:52.33ms
step:1650/2160 train_time:86373ms step_avg:52.35ms
step:1651/2160 train_time:86462ms step_avg:52.37ms
step:1652/2160 train_time:86549ms step_avg:52.39ms
step:1653/2160 train_time:86639ms step_avg:52.41ms
step:1654/2160 train_time:86725ms step_avg:52.43ms
step:1655/2160 train_time:86813ms step_avg:52.46ms
step:1656/2160 train_time:86900ms step_avg:52.48ms
step:1657/2160 train_time:86988ms step_avg:52.50ms
step:1658/2160 train_time:87074ms step_avg:52.52ms
step:1659/2160 train_time:87163ms step_avg:52.54ms
step:1660/2160 train_time:87250ms step_avg:52.56ms
step:1661/2160 train_time:87339ms step_avg:52.58ms
step:1662/2160 train_time:87425ms step_avg:52.60ms
step:1663/2160 train_time:87514ms step_avg:52.62ms
step:1664/2160 train_time:87600ms step_avg:52.64ms
step:1665/2160 train_time:87689ms step_avg:52.67ms
step:1666/2160 train_time:87777ms step_avg:52.69ms
step:1667/2160 train_time:87864ms step_avg:52.71ms
step:1668/2160 train_time:87951ms step_avg:52.73ms
step:1669/2160 train_time:88040ms step_avg:52.75ms
step:1670/2160 train_time:88126ms step_avg:52.77ms
step:1671/2160 train_time:88214ms step_avg:52.79ms
step:1672/2160 train_time:88301ms step_avg:52.81ms
step:1673/2160 train_time:88387ms step_avg:52.83ms
step:1674/2160 train_time:88475ms step_avg:52.85ms
step:1675/2160 train_time:88563ms step_avg:52.87ms
step:1676/2160 train_time:88650ms step_avg:52.89ms
step:1677/2160 train_time:88739ms step_avg:52.92ms
step:1678/2160 train_time:88825ms step_avg:52.94ms
step:1679/2160 train_time:88914ms step_avg:52.96ms
step:1680/2160 train_time:89000ms step_avg:52.98ms
step:1681/2160 train_time:89088ms step_avg:53.00ms
step:1682/2160 train_time:89177ms step_avg:53.02ms
step:1683/2160 train_time:89266ms step_avg:53.04ms
step:1684/2160 train_time:89353ms step_avg:53.06ms
step:1685/2160 train_time:89441ms step_avg:53.08ms
step:1686/2160 train_time:89527ms step_avg:53.10ms
step:1687/2160 train_time:89616ms step_avg:53.12ms
step:1688/2160 train_time:89702ms step_avg:53.14ms
step:1689/2160 train_time:89790ms step_avg:53.16ms
step:1690/2160 train_time:89878ms step_avg:53.18ms
step:1691/2160 train_time:89967ms step_avg:53.20ms
step:1692/2160 train_time:90054ms step_avg:53.22ms
step:1693/2160 train_time:90141ms step_avg:53.24ms
step:1694/2160 train_time:90228ms step_avg:53.26ms
step:1695/2160 train_time:90317ms step_avg:53.28ms
step:1696/2160 train_time:90403ms step_avg:53.30ms
step:1697/2160 train_time:90492ms step_avg:53.32ms
step:1698/2160 train_time:90579ms step_avg:53.34ms
step:1699/2160 train_time:90668ms step_avg:53.37ms
step:1700/2160 train_time:90755ms step_avg:53.39ms
step:1701/2160 train_time:90844ms step_avg:53.41ms
step:1702/2160 train_time:90930ms step_avg:53.43ms
step:1703/2160 train_time:91019ms step_avg:53.45ms
step:1704/2160 train_time:91106ms step_avg:53.47ms
step:1705/2160 train_time:91194ms step_avg:53.49ms
step:1706/2160 train_time:91281ms step_avg:53.51ms
step:1707/2160 train_time:91368ms step_avg:53.53ms
step:1708/2160 train_time:91455ms step_avg:53.54ms
step:1709/2160 train_time:91543ms step_avg:53.57ms
step:1710/2160 train_time:91629ms step_avg:53.58ms
step:1711/2160 train_time:91718ms step_avg:53.61ms
step:1712/2160 train_time:91806ms step_avg:53.62ms
step:1713/2160 train_time:91894ms step_avg:53.65ms
step:1714/2160 train_time:91980ms step_avg:53.66ms
step:1715/2160 train_time:92068ms step_avg:53.68ms
step:1716/2160 train_time:92156ms step_avg:53.70ms
step:1717/2160 train_time:92244ms step_avg:53.72ms
step:1718/2160 train_time:92332ms step_avg:53.74ms
step:1719/2160 train_time:92420ms step_avg:53.76ms
step:1720/2160 train_time:92506ms step_avg:53.78ms
step:1721/2160 train_time:92595ms step_avg:53.80ms
step:1722/2160 train_time:92681ms step_avg:53.82ms
step:1723/2160 train_time:92769ms step_avg:53.84ms
step:1724/2160 train_time:92857ms step_avg:53.86ms
step:1725/2160 train_time:92945ms step_avg:53.88ms
step:1726/2160 train_time:93031ms step_avg:53.90ms
step:1727/2160 train_time:93120ms step_avg:53.92ms
step:1728/2160 train_time:93205ms step_avg:53.94ms
step:1729/2160 train_time:93294ms step_avg:53.96ms
step:1730/2160 train_time:93380ms step_avg:53.98ms
step:1731/2160 train_time:93469ms step_avg:54.00ms
step:1732/2160 train_time:93557ms step_avg:54.02ms
step:1733/2160 train_time:93645ms step_avg:54.04ms
step:1734/2160 train_time:93731ms step_avg:54.05ms
step:1735/2160 train_time:93820ms step_avg:54.07ms
step:1736/2160 train_time:93906ms step_avg:54.09ms
step:1737/2160 train_time:93996ms step_avg:54.11ms
step:1738/2160 train_time:94082ms step_avg:54.13ms
step:1739/2160 train_time:94170ms step_avg:54.15ms
step:1740/2160 train_time:94257ms step_avg:54.17ms
step:1741/2160 train_time:94345ms step_avg:54.19ms
step:1742/2160 train_time:94432ms step_avg:54.21ms
step:1743/2160 train_time:94520ms step_avg:54.23ms
step:1744/2160 train_time:94607ms step_avg:54.25ms
step:1745/2160 train_time:94695ms step_avg:54.27ms
step:1746/2160 train_time:94781ms step_avg:54.28ms
step:1747/2160 train_time:94870ms step_avg:54.30ms
step:1748/2160 train_time:94957ms step_avg:54.32ms
step:1749/2160 train_time:95046ms step_avg:54.34ms
step:1750/2160 train_time:95133ms step_avg:54.36ms
step:1750/2160 val_loss:3.3932 train_time:95222ms step_avg:54.41ms
step:1751/2160 train_time:95245ms step_avg:54.39ms
step:1752/2160 train_time:95312ms step_avg:54.40ms
step:1753/2160 train_time:95405ms step_avg:54.42ms
step:1754/2160 train_time:95495ms step_avg:54.44ms
step:1755/2160 train_time:95583ms step_avg:54.46ms
step:1756/2160 train_time:95669ms step_avg:54.48ms
step:1757/2160 train_time:95756ms step_avg:54.50ms
step:1758/2160 train_time:95842ms step_avg:54.52ms
step:1759/2160 train_time:95928ms step_avg:54.54ms
step:1760/2160 train_time:96014ms step_avg:54.55ms
step:1761/2160 train_time:96101ms step_avg:54.57ms
step:1762/2160 train_time:96189ms step_avg:54.59ms
step:1763/2160 train_time:96280ms step_avg:54.61ms
step:1764/2160 train_time:96369ms step_avg:54.63ms
step:1765/2160 train_time:96460ms step_avg:54.65ms
step:1766/2160 train_time:96548ms step_avg:54.67ms
step:1767/2160 train_time:96637ms step_avg:54.69ms
step:1768/2160 train_time:96723ms step_avg:54.71ms
step:1769/2160 train_time:96812ms step_avg:54.73ms
step:1770/2160 train_time:96897ms step_avg:54.74ms
step:1771/2160 train_time:96984ms step_avg:54.76ms
step:1772/2160 train_time:97070ms step_avg:54.78ms
step:1773/2160 train_time:97158ms step_avg:54.80ms
step:1774/2160 train_time:97245ms step_avg:54.82ms
step:1775/2160 train_time:97338ms step_avg:54.84ms
step:1776/2160 train_time:97426ms step_avg:54.86ms
step:1777/2160 train_time:97517ms step_avg:54.88ms
step:1778/2160 train_time:97603ms step_avg:54.89ms
step:1779/2160 train_time:97692ms step_avg:54.91ms
step:1780/2160 train_time:97777ms step_avg:54.93ms
step:1781/2160 train_time:97864ms step_avg:54.95ms
step:1782/2160 train_time:97950ms step_avg:54.97ms
step:1783/2160 train_time:98037ms step_avg:54.98ms
step:1784/2160 train_time:98123ms step_avg:55.00ms
step:1785/2160 train_time:98212ms step_avg:55.02ms
step:1786/2160 train_time:98299ms step_avg:55.04ms
step:1787/2160 train_time:98388ms step_avg:55.06ms
step:1788/2160 train_time:98476ms step_avg:55.08ms
step:1789/2160 train_time:98565ms step_avg:55.09ms
step:1790/2160 train_time:98651ms step_avg:55.11ms
step:1791/2160 train_time:98740ms step_avg:55.13ms
step:1792/2160 train_time:98826ms step_avg:55.15ms
step:1793/2160 train_time:98914ms step_avg:55.17ms
step:1794/2160 train_time:98999ms step_avg:55.18ms
step:1795/2160 train_time:99086ms step_avg:55.20ms
step:1796/2160 train_time:99172ms step_avg:55.22ms
step:1797/2160 train_time:99261ms step_avg:55.24ms
step:1798/2160 train_time:99348ms step_avg:55.25ms
step:1799/2160 train_time:99438ms step_avg:55.27ms
step:1800/2160 train_time:99525ms step_avg:55.29ms
step:1801/2160 train_time:99614ms step_avg:55.31ms
step:1802/2160 train_time:99700ms step_avg:55.33ms
step:1803/2160 train_time:99788ms step_avg:55.35ms
step:1804/2160 train_time:99875ms step_avg:55.36ms
step:1805/2160 train_time:99962ms step_avg:55.38ms
step:1806/2160 train_time:100048ms step_avg:55.40ms
step:1807/2160 train_time:100136ms step_avg:55.42ms
step:1808/2160 train_time:100223ms step_avg:55.43ms
step:1809/2160 train_time:100311ms step_avg:55.45ms
step:1810/2160 train_time:100398ms step_avg:55.47ms
step:1811/2160 train_time:100485ms step_avg:55.49ms
step:1812/2160 train_time:100573ms step_avg:55.50ms
step:1813/2160 train_time:100661ms step_avg:55.52ms
step:1814/2160 train_time:100749ms step_avg:55.54ms
step:1815/2160 train_time:100838ms step_avg:55.56ms
step:1816/2160 train_time:100924ms step_avg:55.57ms
step:1817/2160 train_time:101012ms step_avg:55.59ms
step:1818/2160 train_time:101098ms step_avg:55.61ms
step:1819/2160 train_time:101187ms step_avg:55.63ms
step:1820/2160 train_time:101275ms step_avg:55.65ms
step:1821/2160 train_time:101364ms step_avg:55.66ms
step:1822/2160 train_time:101451ms step_avg:55.68ms
step:1823/2160 train_time:101539ms step_avg:55.70ms
step:1824/2160 train_time:101625ms step_avg:55.72ms
step:1825/2160 train_time:101715ms step_avg:55.73ms
step:1826/2160 train_time:101800ms step_avg:55.75ms
step:1827/2160 train_time:101888ms step_avg:55.77ms
step:1828/2160 train_time:101974ms step_avg:55.78ms
step:1829/2160 train_time:102062ms step_avg:55.80ms
step:1830/2160 train_time:102149ms step_avg:55.82ms
step:1831/2160 train_time:102237ms step_avg:55.84ms
step:1832/2160 train_time:102323ms step_avg:55.85ms
step:1833/2160 train_time:102412ms step_avg:55.87ms
step:1834/2160 train_time:102498ms step_avg:55.89ms
step:1835/2160 train_time:102587ms step_avg:55.91ms
step:1836/2160 train_time:102674ms step_avg:55.92ms
step:1837/2160 train_time:102762ms step_avg:55.94ms
step:1838/2160 train_time:102849ms step_avg:55.96ms
step:1839/2160 train_time:102937ms step_avg:55.97ms
step:1840/2160 train_time:103022ms step_avg:55.99ms
step:1841/2160 train_time:103111ms step_avg:56.01ms
step:1842/2160 train_time:103197ms step_avg:56.02ms
step:1843/2160 train_time:103286ms step_avg:56.04ms
step:1844/2160 train_time:103374ms step_avg:56.06ms
step:1845/2160 train_time:103462ms step_avg:56.08ms
step:1846/2160 train_time:103549ms step_avg:56.09ms
step:1847/2160 train_time:103638ms step_avg:56.11ms
step:1848/2160 train_time:103724ms step_avg:56.13ms
step:1849/2160 train_time:103813ms step_avg:56.15ms
step:1850/2160 train_time:103898ms step_avg:56.16ms
step:1851/2160 train_time:103987ms step_avg:56.18ms
step:1852/2160 train_time:104074ms step_avg:56.20ms
step:1853/2160 train_time:104162ms step_avg:56.21ms
step:1854/2160 train_time:104250ms step_avg:56.23ms
step:1855/2160 train_time:104338ms step_avg:56.25ms
step:1856/2160 train_time:104425ms step_avg:56.26ms
step:1857/2160 train_time:104515ms step_avg:56.28ms
step:1858/2160 train_time:104602ms step_avg:56.30ms
step:1859/2160 train_time:104691ms step_avg:56.32ms
step:1860/2160 train_time:104777ms step_avg:56.33ms
step:1861/2160 train_time:104865ms step_avg:56.35ms
step:1862/2160 train_time:104952ms step_avg:56.36ms
step:1863/2160 train_time:105040ms step_avg:56.38ms
step:1864/2160 train_time:105126ms step_avg:56.40ms
step:1865/2160 train_time:105215ms step_avg:56.42ms
step:1866/2160 train_time:105302ms step_avg:56.43ms
step:1867/2160 train_time:105390ms step_avg:56.45ms
step:1868/2160 train_time:105477ms step_avg:56.47ms
step:1869/2160 train_time:105565ms step_avg:56.48ms
step:1870/2160 train_time:105652ms step_avg:56.50ms
step:1871/2160 train_time:105741ms step_avg:56.52ms
step:1872/2160 train_time:105827ms step_avg:56.53ms
step:1873/2160 train_time:105916ms step_avg:56.55ms
step:1874/2160 train_time:106002ms step_avg:56.56ms
step:1875/2160 train_time:106090ms step_avg:56.58ms
step:1876/2160 train_time:106177ms step_avg:56.60ms
step:1877/2160 train_time:106266ms step_avg:56.61ms
step:1878/2160 train_time:106352ms step_avg:56.63ms
step:1879/2160 train_time:106440ms step_avg:56.65ms
step:1880/2160 train_time:106528ms step_avg:56.66ms
step:1881/2160 train_time:106617ms step_avg:56.68ms
step:1882/2160 train_time:106704ms step_avg:56.70ms
step:1883/2160 train_time:106792ms step_avg:56.71ms
step:1884/2160 train_time:106877ms step_avg:56.73ms
step:1885/2160 train_time:106965ms step_avg:56.75ms
step:1886/2160 train_time:107053ms step_avg:56.76ms
step:1887/2160 train_time:107141ms step_avg:56.78ms
step:1888/2160 train_time:107228ms step_avg:56.79ms
step:1889/2160 train_time:107318ms step_avg:56.81ms
step:1890/2160 train_time:107404ms step_avg:56.83ms
step:1891/2160 train_time:107494ms step_avg:56.84ms
step:1892/2160 train_time:107579ms step_avg:56.86ms
step:1893/2160 train_time:107667ms step_avg:56.88ms
step:1894/2160 train_time:107754ms step_avg:56.89ms
step:1895/2160 train_time:107842ms step_avg:56.91ms
step:1896/2160 train_time:107929ms step_avg:56.92ms
step:1897/2160 train_time:108018ms step_avg:56.94ms
step:1898/2160 train_time:108104ms step_avg:56.96ms
step:1899/2160 train_time:108192ms step_avg:56.97ms
step:1900/2160 train_time:108278ms step_avg:56.99ms
step:1901/2160 train_time:108367ms step_avg:57.01ms
step:1902/2160 train_time:108453ms step_avg:57.02ms
step:1903/2160 train_time:108542ms step_avg:57.04ms
step:1904/2160 train_time:108628ms step_avg:57.05ms
step:1905/2160 train_time:108718ms step_avg:57.07ms
step:1906/2160 train_time:108804ms step_avg:57.09ms
step:1907/2160 train_time:108893ms step_avg:57.10ms
step:1908/2160 train_time:108979ms step_avg:57.12ms
step:1909/2160 train_time:109068ms step_avg:57.13ms
step:1910/2160 train_time:109154ms step_avg:57.15ms
step:1911/2160 train_time:109243ms step_avg:57.17ms
step:1912/2160 train_time:109330ms step_avg:57.18ms
step:1913/2160 train_time:109418ms step_avg:57.20ms
step:1914/2160 train_time:109505ms step_avg:57.21ms
step:1915/2160 train_time:109594ms step_avg:57.23ms
step:1916/2160 train_time:109681ms step_avg:57.24ms
step:1917/2160 train_time:109769ms step_avg:57.26ms
step:1918/2160 train_time:109855ms step_avg:57.28ms
step:1919/2160 train_time:109943ms step_avg:57.29ms
step:1920/2160 train_time:110030ms step_avg:57.31ms
step:1921/2160 train_time:110118ms step_avg:57.32ms
step:1922/2160 train_time:110204ms step_avg:57.34ms
step:1923/2160 train_time:110295ms step_avg:57.36ms
step:1924/2160 train_time:110382ms step_avg:57.37ms
step:1925/2160 train_time:110470ms step_avg:57.39ms
step:1926/2160 train_time:110556ms step_avg:57.40ms
step:1927/2160 train_time:110644ms step_avg:57.42ms
step:1928/2160 train_time:110731ms step_avg:57.43ms
step:1929/2160 train_time:110819ms step_avg:57.45ms
step:1930/2160 train_time:110906ms step_avg:57.46ms
step:1931/2160 train_time:110995ms step_avg:57.48ms
step:1932/2160 train_time:111082ms step_avg:57.50ms
step:1933/2160 train_time:111170ms step_avg:57.51ms
step:1934/2160 train_time:111256ms step_avg:57.53ms
step:1935/2160 train_time:111344ms step_avg:57.54ms
step:1936/2160 train_time:111432ms step_avg:57.56ms
step:1937/2160 train_time:111520ms step_avg:57.57ms
step:1938/2160 train_time:111607ms step_avg:57.59ms
step:1939/2160 train_time:111696ms step_avg:57.60ms
step:1940/2160 train_time:111782ms step_avg:57.62ms
step:1941/2160 train_time:111871ms step_avg:57.64ms
step:1942/2160 train_time:111957ms step_avg:57.65ms
step:1943/2160 train_time:112045ms step_avg:57.67ms
step:1944/2160 train_time:112132ms step_avg:57.68ms
step:1945/2160 train_time:112219ms step_avg:57.70ms
step:1946/2160 train_time:112305ms step_avg:57.71ms
step:1947/2160 train_time:112395ms step_avg:57.73ms
step:1948/2160 train_time:112481ms step_avg:57.74ms
step:1949/2160 train_time:112569ms step_avg:57.76ms
step:1950/2160 train_time:112655ms step_avg:57.77ms
step:1951/2160 train_time:112743ms step_avg:57.79ms
step:1952/2160 train_time:112830ms step_avg:57.80ms
step:1953/2160 train_time:112918ms step_avg:57.82ms
step:1954/2160 train_time:113005ms step_avg:57.83ms
step:1955/2160 train_time:113094ms step_avg:57.85ms
step:1956/2160 train_time:113180ms step_avg:57.86ms
step:1957/2160 train_time:113269ms step_avg:57.88ms
step:1958/2160 train_time:113356ms step_avg:57.89ms
step:1959/2160 train_time:113444ms step_avg:57.91ms
step:1960/2160 train_time:113532ms step_avg:57.92ms
step:1961/2160 train_time:113619ms step_avg:57.94ms
step:1962/2160 train_time:113705ms step_avg:57.95ms
step:1963/2160 train_time:113794ms step_avg:57.97ms
step:1964/2160 train_time:113880ms step_avg:57.98ms
step:1965/2160 train_time:113969ms step_avg:58.00ms
step:1966/2160 train_time:114055ms step_avg:58.01ms
step:1967/2160 train_time:114143ms step_avg:58.03ms
step:1968/2160 train_time:114230ms step_avg:58.04ms
step:1969/2160 train_time:114318ms step_avg:58.06ms
step:1970/2160 train_time:114404ms step_avg:58.07ms
step:1971/2160 train_time:114493ms step_avg:58.09ms
step:1972/2160 train_time:114579ms step_avg:58.10ms
step:1973/2160 train_time:114666ms step_avg:58.12ms
step:1974/2160 train_time:114754ms step_avg:58.13ms
step:1975/2160 train_time:114841ms step_avg:58.15ms
step:1976/2160 train_time:114928ms step_avg:58.16ms
step:1977/2160 train_time:115017ms step_avg:58.18ms
step:1978/2160 train_time:115103ms step_avg:58.19ms
step:1979/2160 train_time:115192ms step_avg:58.21ms
step:1980/2160 train_time:115277ms step_avg:58.22ms
step:1981/2160 train_time:115366ms step_avg:58.24ms
step:1982/2160 train_time:115453ms step_avg:58.25ms
step:1983/2160 train_time:115541ms step_avg:58.27ms
step:1984/2160 train_time:115627ms step_avg:58.28ms
step:1985/2160 train_time:115715ms step_avg:58.29ms
step:1986/2160 train_time:115801ms step_avg:58.31ms
step:1987/2160 train_time:115890ms step_avg:58.32ms
step:1988/2160 train_time:115977ms step_avg:58.34ms
step:1989/2160 train_time:116066ms step_avg:58.35ms
step:1990/2160 train_time:116154ms step_avg:58.37ms
step:1991/2160 train_time:116242ms step_avg:58.38ms
step:1992/2160 train_time:116328ms step_avg:58.40ms
step:1993/2160 train_time:116416ms step_avg:58.41ms
step:1994/2160 train_time:116502ms step_avg:58.43ms
step:1995/2160 train_time:116591ms step_avg:58.44ms
step:1996/2160 train_time:116677ms step_avg:58.46ms
step:1997/2160 train_time:116765ms step_avg:58.47ms
step:1998/2160 train_time:116852ms step_avg:58.48ms
step:1999/2160 train_time:116940ms step_avg:58.50ms
step:2000/2160 train_time:117027ms step_avg:58.51ms
step:2000/2160 val_loss:3.3152 train_time:117117ms step_avg:58.56ms
step:2001/2160 train_time:117139ms step_avg:58.54ms
step:2002/2160 train_time:117207ms step_avg:58.54ms
step:2003/2160 train_time:117298ms step_avg:58.56ms
step:2004/2160 train_time:117385ms step_avg:58.58ms
step:2005/2160 train_time:117472ms step_avg:58.59ms
step:2006/2160 train_time:117557ms step_avg:58.60ms
step:2007/2160 train_time:117644ms step_avg:58.62ms
step:2008/2160 train_time:117729ms step_avg:58.63ms
step:2009/2160 train_time:117817ms step_avg:58.64ms
step:2010/2160 train_time:117904ms step_avg:58.66ms
step:2011/2160 train_time:117992ms step_avg:58.67ms
step:2012/2160 train_time:118079ms step_avg:58.69ms
step:2013/2160 train_time:118169ms step_avg:58.70ms
step:2014/2160 train_time:118258ms step_avg:58.72ms
step:2015/2160 train_time:118348ms step_avg:58.73ms
step:2016/2160 train_time:118434ms step_avg:58.75ms
step:2017/2160 train_time:118521ms step_avg:58.76ms
step:2018/2160 train_time:118607ms step_avg:58.77ms
step:2019/2160 train_time:118693ms step_avg:58.79ms
step:2020/2160 train_time:118780ms step_avg:58.80ms
step:2021/2160 train_time:118868ms step_avg:58.82ms
step:2022/2160 train_time:118954ms step_avg:58.83ms
step:2023/2160 train_time:119044ms step_avg:58.85ms
step:2024/2160 train_time:119131ms step_avg:58.86ms
step:2025/2160 train_time:119220ms step_avg:58.87ms
step:2026/2160 train_time:119307ms step_avg:58.89ms
step:2027/2160 train_time:119397ms step_avg:58.90ms
step:2028/2160 train_time:119483ms step_avg:58.92ms
step:2029/2160 train_time:119571ms step_avg:58.93ms
step:2030/2160 train_time:119657ms step_avg:58.94ms
step:2031/2160 train_time:119746ms step_avg:58.96ms
step:2032/2160 train_time:119832ms step_avg:58.97ms
step:2033/2160 train_time:119919ms step_avg:58.99ms
step:2034/2160 train_time:120005ms step_avg:59.00ms
step:2035/2160 train_time:120094ms step_avg:59.01ms
step:2036/2160 train_time:120182ms step_avg:59.03ms
step:2037/2160 train_time:120272ms step_avg:59.04ms
step:2038/2160 train_time:120358ms step_avg:59.06ms
step:2039/2160 train_time:120446ms step_avg:59.07ms
step:2040/2160 train_time:120532ms step_avg:59.08ms
step:2041/2160 train_time:120620ms step_avg:59.10ms
step:2042/2160 train_time:120706ms step_avg:59.11ms
step:2043/2160 train_time:120794ms step_avg:59.13ms
step:2044/2160 train_time:120880ms step_avg:59.14ms
step:2045/2160 train_time:120968ms step_avg:59.15ms
step:2046/2160 train_time:121054ms step_avg:59.17ms
step:2047/2160 train_time:121144ms step_avg:59.18ms
step:2048/2160 train_time:121232ms step_avg:59.20ms
step:2049/2160 train_time:121321ms step_avg:59.21ms
step:2050/2160 train_time:121408ms step_avg:59.22ms
step:2051/2160 train_time:121496ms step_avg:59.24ms
step:2052/2160 train_time:121584ms step_avg:59.25ms
step:2053/2160 train_time:121672ms step_avg:59.27ms
step:2054/2160 train_time:121758ms step_avg:59.28ms
step:2055/2160 train_time:121846ms step_avg:59.29ms
step:2056/2160 train_time:121933ms step_avg:59.31ms
step:2057/2160 train_time:122021ms step_avg:59.32ms
step:2058/2160 train_time:122107ms step_avg:59.33ms
step:2059/2160 train_time:122196ms step_avg:59.35ms
step:2060/2160 train_time:122283ms step_avg:59.36ms
step:2061/2160 train_time:122372ms step_avg:59.38ms
step:2062/2160 train_time:122459ms step_avg:59.39ms
step:2063/2160 train_time:122547ms step_avg:59.40ms
step:2064/2160 train_time:122633ms step_avg:59.42ms
step:2065/2160 train_time:122721ms step_avg:59.43ms
step:2066/2160 train_time:122806ms step_avg:59.44ms
step:2067/2160 train_time:122895ms step_avg:59.46ms
step:2068/2160 train_time:122982ms step_avg:59.47ms
step:2069/2160 train_time:123069ms step_avg:59.48ms
step:2070/2160 train_time:123157ms step_avg:59.50ms
step:2071/2160 train_time:123245ms step_avg:59.51ms
step:2072/2160 train_time:123333ms step_avg:59.52ms
step:2073/2160 train_time:123423ms step_avg:59.54ms
step:2074/2160 train_time:123509ms step_avg:59.55ms
step:2075/2160 train_time:123596ms step_avg:59.56ms
step:2076/2160 train_time:123683ms step_avg:59.58ms
step:2077/2160 train_time:123771ms step_avg:59.59ms
step:2078/2160 train_time:123857ms step_avg:59.60ms
step:2079/2160 train_time:123945ms step_avg:59.62ms
step:2080/2160 train_time:124033ms step_avg:59.63ms
step:2081/2160 train_time:124121ms step_avg:59.65ms
step:2082/2160 train_time:124207ms step_avg:59.66ms
step:2083/2160 train_time:124297ms step_avg:59.67ms
step:2084/2160 train_time:124384ms step_avg:59.69ms
step:2085/2160 train_time:124472ms step_avg:59.70ms
step:2086/2160 train_time:124558ms step_avg:59.71ms
step:2087/2160 train_time:124647ms step_avg:59.73ms
step:2088/2160 train_time:124733ms step_avg:59.74ms
step:2089/2160 train_time:124821ms step_avg:59.75ms
step:2090/2160 train_time:124907ms step_avg:59.76ms
step:2091/2160 train_time:124995ms step_avg:59.78ms
step:2092/2160 train_time:125082ms step_avg:59.79ms
step:2093/2160 train_time:125171ms step_avg:59.80ms
step:2094/2160 train_time:125257ms step_avg:59.82ms
step:2095/2160 train_time:125347ms step_avg:59.83ms
step:2096/2160 train_time:125433ms step_avg:59.84ms
step:2097/2160 train_time:125521ms step_avg:59.86ms
step:2098/2160 train_time:125608ms step_avg:59.87ms
step:2099/2160 train_time:125695ms step_avg:59.88ms
step:2100/2160 train_time:125782ms step_avg:59.90ms
step:2101/2160 train_time:125870ms step_avg:59.91ms
step:2102/2160 train_time:125957ms step_avg:59.92ms
step:2103/2160 train_time:126045ms step_avg:59.94ms
step:2104/2160 train_time:126131ms step_avg:59.95ms
step:2105/2160 train_time:126219ms step_avg:59.96ms
step:2106/2160 train_time:126306ms step_avg:59.97ms
step:2107/2160 train_time:126394ms step_avg:59.99ms
step:2108/2160 train_time:126481ms step_avg:60.00ms
step:2109/2160 train_time:126569ms step_avg:60.01ms
step:2110/2160 train_time:126656ms step_avg:60.03ms
step:2111/2160 train_time:126745ms step_avg:60.04ms
step:2112/2160 train_time:126831ms step_avg:60.05ms
step:2113/2160 train_time:126919ms step_avg:60.07ms
step:2114/2160 train_time:127006ms step_avg:60.08ms
step:2115/2160 train_time:127094ms step_avg:60.09ms
step:2116/2160 train_time:127181ms step_avg:60.10ms
step:2117/2160 train_time:127269ms step_avg:60.12ms
step:2118/2160 train_time:127356ms step_avg:60.13ms
step:2119/2160 train_time:127445ms step_avg:60.14ms
step:2120/2160 train_time:127531ms step_avg:60.16ms
step:2121/2160 train_time:127619ms step_avg:60.17ms
step:2122/2160 train_time:127706ms step_avg:60.18ms
step:2123/2160 train_time:127795ms step_avg:60.20ms
step:2124/2160 train_time:127881ms step_avg:60.21ms
step:2125/2160 train_time:127970ms step_avg:60.22ms
step:2126/2160 train_time:128057ms step_avg:60.23ms
step:2127/2160 train_time:128146ms step_avg:60.25ms
step:2128/2160 train_time:128233ms step_avg:60.26ms
step:2129/2160 train_time:128321ms step_avg:60.27ms
step:2130/2160 train_time:128408ms step_avg:60.29ms
step:2131/2160 train_time:128497ms step_avg:60.30ms
step:2132/2160 train_time:128584ms step_avg:60.31ms
step:2133/2160 train_time:128672ms step_avg:60.32ms
step:2134/2160 train_time:128759ms step_avg:60.34ms
step:2135/2160 train_time:128847ms step_avg:60.35ms
step:2136/2160 train_time:128934ms step_avg:60.36ms
step:2137/2160 train_time:129023ms step_avg:60.38ms
step:2138/2160 train_time:129111ms step_avg:60.39ms
step:2139/2160 train_time:129200ms step_avg:60.40ms
step:2140/2160 train_time:129286ms step_avg:60.41ms
step:2141/2160 train_time:129375ms step_avg:60.43ms
step:2142/2160 train_time:129462ms step_avg:60.44ms
step:2143/2160 train_time:129550ms step_avg:60.45ms
step:2144/2160 train_time:129637ms step_avg:60.47ms
step:2145/2160 train_time:129727ms step_avg:60.48ms
step:2146/2160 train_time:129813ms step_avg:60.49ms
step:2147/2160 train_time:129901ms step_avg:60.50ms
step:2148/2160 train_time:129988ms step_avg:60.52ms
step:2149/2160 train_time:130076ms step_avg:60.53ms
step:2150/2160 train_time:130163ms step_avg:60.54ms
step:2151/2160 train_time:130252ms step_avg:60.55ms
step:2152/2160 train_time:130338ms step_avg:60.57ms
step:2153/2160 train_time:130427ms step_avg:60.58ms
step:2154/2160 train_time:130514ms step_avg:60.59ms
step:2155/2160 train_time:130604ms step_avg:60.60ms
step:2156/2160 train_time:130690ms step_avg:60.62ms
step:2157/2160 train_time:130778ms step_avg:60.63ms
step:2158/2160 train_time:130865ms step_avg:60.64ms
step:2159/2160 train_time:130952ms step_avg:60.65ms
step:2160/2160 train_time:131040ms step_avg:60.67ms
step:2160/2160 val_loss:3.2788 train_time:131130ms step_avg:60.71ms
peak memory allocated: 30706 MiB reserved: 44736 MiB

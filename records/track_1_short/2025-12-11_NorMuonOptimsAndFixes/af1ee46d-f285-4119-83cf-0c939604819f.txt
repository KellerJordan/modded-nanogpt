import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 08:50:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   40C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   40C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   42C    P0            128W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           28635      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           28636      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28637      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28638      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28639      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28640      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28641      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28642      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           28636      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           28637      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           28638      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           28639      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           28640      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           28641      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           28642      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2160 train_time:100ms step_avg:100.31ms
step:2/2160 train_time:126ms step_avg:63.23ms
step:3/2160 train_time:150ms step_avg:49.97ms
step:4/2160 train_time:175ms step_avg:43.64ms
step:5/2160 train_time:201ms step_avg:40.17ms
step:6/2160 train_time:345ms step_avg:57.43ms
step:7/2160 train_time:366ms step_avg:52.26ms
step:8/2160 train_time:398ms step_avg:49.73ms
step:9/2160 train_time:431ms step_avg:47.90ms
step:10/2160 train_time:464ms step_avg:46.41ms
step:11/2160 train_time:498ms step_avg:45.23ms
step:12/2160 train_time:531ms step_avg:44.23ms
step:13/2160 train_time:564ms step_avg:43.41ms
step:14/2160 train_time:598ms step_avg:42.70ms
step:15/2160 train_time:631ms step_avg:42.08ms
step:16/2160 train_time:664ms step_avg:41.52ms
step:17/2160 train_time:698ms step_avg:41.08ms
step:18/2160 train_time:731ms step_avg:40.63ms
step:19/2160 train_time:765ms step_avg:40.27ms
step:20/2160 train_time:798ms step_avg:39.91ms
step:21/2160 train_time:832ms step_avg:39.62ms
step:22/2160 train_time:865ms step_avg:39.33ms
step:23/2160 train_time:899ms step_avg:39.09ms
step:24/2160 train_time:932ms step_avg:38.85ms
step:25/2160 train_time:966ms step_avg:38.63ms
step:26/2160 train_time:999ms step_avg:38.43ms
step:27/2160 train_time:1033ms step_avg:38.24ms
step:28/2160 train_time:1066ms step_avg:38.06ms
step:29/2160 train_time:1099ms step_avg:37.91ms
step:30/2160 train_time:1133ms step_avg:37.76ms
step:31/2160 train_time:1166ms step_avg:37.62ms
step:32/2160 train_time:1200ms step_avg:37.48ms
step:33/2160 train_time:1233ms step_avg:37.37ms
step:34/2160 train_time:1266ms step_avg:37.25ms
step:35/2160 train_time:1302ms step_avg:37.20ms
step:36/2160 train_time:1335ms step_avg:37.09ms
step:37/2160 train_time:1370ms step_avg:37.02ms
step:38/2160 train_time:1403ms step_avg:36.92ms
step:39/2160 train_time:1437ms step_avg:36.84ms
step:40/2160 train_time:1470ms step_avg:36.75ms
step:41/2160 train_time:1504ms step_avg:36.68ms
step:42/2160 train_time:1537ms step_avg:36.61ms
step:43/2160 train_time:1571ms step_avg:36.54ms
step:44/2160 train_time:1604ms step_avg:36.47ms
step:45/2160 train_time:1638ms step_avg:36.40ms
step:46/2160 train_time:1672ms step_avg:36.34ms
step:47/2160 train_time:1705ms step_avg:36.28ms
step:48/2160 train_time:1739ms step_avg:36.23ms
step:49/2160 train_time:1772ms step_avg:36.17ms
step:50/2160 train_time:1805ms step_avg:36.11ms
step:51/2160 train_time:1839ms step_avg:36.06ms
step:52/2160 train_time:1872ms step_avg:36.01ms
step:53/2160 train_time:1906ms step_avg:35.96ms
step:54/2160 train_time:1939ms step_avg:35.91ms
step:55/2160 train_time:1973ms step_avg:35.87ms
step:56/2160 train_time:2006ms step_avg:35.82ms
step:57/2160 train_time:2040ms step_avg:35.78ms
step:58/2160 train_time:2073ms step_avg:35.74ms
step:59/2160 train_time:2107ms step_avg:35.71ms
step:60/2160 train_time:2140ms step_avg:35.67ms
step:61/2160 train_time:2173ms step_avg:35.63ms
step:62/2160 train_time:2207ms step_avg:35.59ms
step:63/2160 train_time:2241ms step_avg:35.58ms
step:64/2160 train_time:2275ms step_avg:35.55ms
step:65/2160 train_time:2309ms step_avg:35.52ms
step:66/2160 train_time:2342ms step_avg:35.49ms
step:67/2160 train_time:2376ms step_avg:35.47ms
step:68/2160 train_time:2410ms step_avg:35.43ms
step:69/2160 train_time:2443ms step_avg:35.41ms
step:70/2160 train_time:2477ms step_avg:35.38ms
step:71/2160 train_time:2511ms step_avg:35.36ms
step:72/2160 train_time:2544ms step_avg:35.34ms
step:73/2160 train_time:2578ms step_avg:35.32ms
step:74/2160 train_time:2612ms step_avg:35.29ms
step:75/2160 train_time:2645ms step_avg:35.27ms
step:76/2160 train_time:2679ms step_avg:35.25ms
step:77/2160 train_time:2713ms step_avg:35.23ms
step:78/2160 train_time:2746ms step_avg:35.20ms
step:79/2160 train_time:2780ms step_avg:35.18ms
step:80/2160 train_time:2813ms step_avg:35.16ms
step:81/2160 train_time:2846ms step_avg:35.14ms
step:82/2160 train_time:2880ms step_avg:35.12ms
step:83/2160 train_time:2913ms step_avg:35.10ms
step:84/2160 train_time:2946ms step_avg:35.08ms
step:85/2160 train_time:2980ms step_avg:35.06ms
step:86/2160 train_time:3013ms step_avg:35.04ms
step:87/2160 train_time:3047ms step_avg:35.02ms
step:88/2160 train_time:3080ms step_avg:35.00ms
step:89/2160 train_time:3114ms step_avg:34.99ms
step:90/2160 train_time:3147ms step_avg:34.97ms
step:91/2160 train_time:3181ms step_avg:34.96ms
step:92/2160 train_time:3214ms step_avg:34.94ms
step:93/2160 train_time:3248ms step_avg:34.92ms
step:94/2160 train_time:3281ms step_avg:34.90ms
step:95/2160 train_time:3315ms step_avg:34.89ms
step:96/2160 train_time:3348ms step_avg:34.87ms
step:97/2160 train_time:3382ms step_avg:34.86ms
step:98/2160 train_time:3415ms step_avg:34.85ms
step:99/2160 train_time:3449ms step_avg:34.84ms
step:100/2160 train_time:3482ms step_avg:34.82ms
step:101/2160 train_time:3516ms step_avg:34.81ms
step:102/2160 train_time:3550ms step_avg:34.80ms
step:103/2160 train_time:3583ms step_avg:34.79ms
step:104/2160 train_time:3616ms step_avg:34.77ms
step:105/2160 train_time:3650ms step_avg:34.76ms
step:106/2160 train_time:3683ms step_avg:34.75ms
step:107/2160 train_time:3717ms step_avg:34.74ms
step:108/2160 train_time:3751ms step_avg:34.73ms
step:109/2160 train_time:3785ms step_avg:34.72ms
step:110/2160 train_time:3818ms step_avg:34.71ms
step:111/2160 train_time:3851ms step_avg:34.70ms
step:112/2160 train_time:3885ms step_avg:34.68ms
step:113/2160 train_time:3918ms step_avg:34.68ms
step:114/2160 train_time:3952ms step_avg:34.67ms
step:115/2160 train_time:3985ms step_avg:34.65ms
step:116/2160 train_time:4019ms step_avg:34.64ms
step:117/2160 train_time:4052ms step_avg:34.63ms
step:118/2160 train_time:4086ms step_avg:34.62ms
step:119/2160 train_time:4119ms step_avg:34.62ms
step:120/2160 train_time:4152ms step_avg:34.60ms
step:121/2160 train_time:4186ms step_avg:34.60ms
step:122/2160 train_time:4220ms step_avg:34.59ms
step:123/2160 train_time:4253ms step_avg:34.58ms
step:124/2160 train_time:4286ms step_avg:34.57ms
step:125/2160 train_time:4321ms step_avg:34.56ms
step:126/2160 train_time:4354ms step_avg:34.55ms
step:127/2160 train_time:4387ms step_avg:34.55ms
step:128/2160 train_time:4421ms step_avg:34.54ms
step:129/2160 train_time:4454ms step_avg:34.53ms
step:130/2160 train_time:4487ms step_avg:34.52ms
step:131/2160 train_time:4522ms step_avg:34.52ms
step:132/2160 train_time:4555ms step_avg:34.51ms
step:133/2160 train_time:4589ms step_avg:34.50ms
step:134/2160 train_time:4622ms step_avg:34.49ms
step:135/2160 train_time:4655ms step_avg:34.48ms
step:136/2160 train_time:4689ms step_avg:34.48ms
step:137/2160 train_time:4722ms step_avg:34.47ms
step:138/2160 train_time:4756ms step_avg:34.46ms
step:139/2160 train_time:4789ms step_avg:34.46ms
step:140/2160 train_time:4822ms step_avg:34.45ms
step:141/2160 train_time:4856ms step_avg:34.44ms
step:142/2160 train_time:4890ms step_avg:34.43ms
step:143/2160 train_time:4923ms step_avg:34.43ms
step:144/2160 train_time:4957ms step_avg:34.42ms
step:145/2160 train_time:4991ms step_avg:34.42ms
step:146/2160 train_time:5024ms step_avg:34.41ms
step:147/2160 train_time:5058ms step_avg:34.41ms
step:148/2160 train_time:5091ms step_avg:34.40ms
step:149/2160 train_time:5125ms step_avg:34.40ms
step:150/2160 train_time:5158ms step_avg:34.39ms
step:151/2160 train_time:5192ms step_avg:34.38ms
step:152/2160 train_time:5225ms step_avg:34.37ms
step:153/2160 train_time:5259ms step_avg:34.37ms
step:154/2160 train_time:5292ms step_avg:34.36ms
step:155/2160 train_time:5326ms step_avg:34.36ms
step:156/2160 train_time:5359ms step_avg:34.35ms
step:157/2160 train_time:5392ms step_avg:34.35ms
step:158/2160 train_time:5425ms step_avg:34.34ms
step:159/2160 train_time:5460ms step_avg:34.34ms
step:160/2160 train_time:5494ms step_avg:34.34ms
step:161/2160 train_time:5527ms step_avg:34.33ms
step:162/2160 train_time:5560ms step_avg:34.32ms
step:163/2160 train_time:5594ms step_avg:34.32ms
step:164/2160 train_time:5627ms step_avg:34.31ms
step:165/2160 train_time:5661ms step_avg:34.31ms
step:166/2160 train_time:5694ms step_avg:34.30ms
step:167/2160 train_time:5728ms step_avg:34.30ms
step:168/2160 train_time:5761ms step_avg:34.29ms
step:169/2160 train_time:5795ms step_avg:34.29ms
step:170/2160 train_time:5828ms step_avg:34.28ms
step:171/2160 train_time:5862ms step_avg:34.28ms
step:172/2160 train_time:5895ms step_avg:34.27ms
step:173/2160 train_time:5929ms step_avg:34.27ms
step:174/2160 train_time:5962ms step_avg:34.27ms
step:175/2160 train_time:5996ms step_avg:34.26ms
step:176/2160 train_time:6029ms step_avg:34.26ms
step:177/2160 train_time:6063ms step_avg:34.25ms
step:178/2160 train_time:6096ms step_avg:34.25ms
step:179/2160 train_time:6130ms step_avg:34.24ms
step:180/2160 train_time:6163ms step_avg:34.24ms
step:181/2160 train_time:6197ms step_avg:34.24ms
step:182/2160 train_time:6230ms step_avg:34.23ms
step:183/2160 train_time:6263ms step_avg:34.23ms
step:184/2160 train_time:6297ms step_avg:34.22ms
step:185/2160 train_time:6330ms step_avg:34.22ms
step:186/2160 train_time:6363ms step_avg:34.21ms
step:187/2160 train_time:6397ms step_avg:34.21ms
step:188/2160 train_time:6430ms step_avg:34.20ms
step:189/2160 train_time:6464ms step_avg:34.20ms
step:190/2160 train_time:6497ms step_avg:34.20ms
step:191/2160 train_time:6531ms step_avg:34.19ms
step:192/2160 train_time:6564ms step_avg:34.19ms
step:193/2160 train_time:6598ms step_avg:34.19ms
step:194/2160 train_time:6631ms step_avg:34.18ms
step:195/2160 train_time:6665ms step_avg:34.18ms
step:196/2160 train_time:6698ms step_avg:34.18ms
step:197/2160 train_time:6732ms step_avg:34.17ms
step:198/2160 train_time:6765ms step_avg:34.17ms
step:199/2160 train_time:6799ms step_avg:34.17ms
step:200/2160 train_time:6832ms step_avg:34.16ms
step:201/2160 train_time:6866ms step_avg:34.16ms
step:202/2160 train_time:6899ms step_avg:34.15ms
step:203/2160 train_time:6933ms step_avg:34.15ms
step:204/2160 train_time:6966ms step_avg:34.15ms
step:205/2160 train_time:7000ms step_avg:34.15ms
step:206/2160 train_time:7033ms step_avg:34.14ms
step:207/2160 train_time:7066ms step_avg:34.14ms
step:208/2160 train_time:7100ms step_avg:34.13ms
step:209/2160 train_time:7133ms step_avg:34.13ms
step:210/2160 train_time:7167ms step_avg:34.13ms
step:211/2160 train_time:7201ms step_avg:34.13ms
step:212/2160 train_time:7234ms step_avg:34.12ms
step:213/2160 train_time:7267ms step_avg:34.12ms
step:214/2160 train_time:7301ms step_avg:34.12ms
step:215/2160 train_time:7334ms step_avg:34.11ms
step:216/2160 train_time:7368ms step_avg:34.11ms
step:217/2160 train_time:7401ms step_avg:34.11ms
step:218/2160 train_time:7435ms step_avg:34.10ms
step:219/2160 train_time:7468ms step_avg:34.10ms
step:220/2160 train_time:7502ms step_avg:34.10ms
step:221/2160 train_time:7535ms step_avg:34.10ms
step:222/2160 train_time:7569ms step_avg:34.09ms
step:223/2160 train_time:7602ms step_avg:34.09ms
step:224/2160 train_time:7636ms step_avg:34.09ms
step:225/2160 train_time:7669ms step_avg:34.09ms
step:226/2160 train_time:7702ms step_avg:34.08ms
step:227/2160 train_time:7736ms step_avg:34.08ms
step:228/2160 train_time:7769ms step_avg:34.08ms
step:229/2160 train_time:7803ms step_avg:34.07ms
step:230/2160 train_time:7836ms step_avg:34.07ms
step:231/2160 train_time:7870ms step_avg:34.07ms
step:232/2160 train_time:7903ms step_avg:34.06ms
step:233/2160 train_time:7937ms step_avg:34.06ms
step:234/2160 train_time:7970ms step_avg:34.06ms
step:235/2160 train_time:8003ms step_avg:34.06ms
step:236/2160 train_time:8037ms step_avg:34.05ms
step:237/2160 train_time:8070ms step_avg:34.05ms
step:238/2160 train_time:8103ms step_avg:34.05ms
step:239/2160 train_time:8138ms step_avg:34.05ms
step:240/2160 train_time:8171ms step_avg:34.05ms
step:241/2160 train_time:8205ms step_avg:34.04ms
step:242/2160 train_time:8238ms step_avg:34.04ms
step:243/2160 train_time:8271ms step_avg:34.04ms
step:244/2160 train_time:8304ms step_avg:34.03ms
step:245/2160 train_time:8339ms step_avg:34.04ms
step:246/2160 train_time:8372ms step_avg:34.03ms
step:247/2160 train_time:8405ms step_avg:34.03ms
step:248/2160 train_time:8439ms step_avg:34.03ms
step:249/2160 train_time:8472ms step_avg:34.02ms
step:250/2160 train_time:8505ms step_avg:34.02ms
step:250/2160 val_loss:4.3083 train_time:8540ms step_avg:34.16ms
step:251/2160 train_time:8563ms step_avg:34.11ms
step:252/2160 train_time:8585ms step_avg:34.07ms
step:253/2160 train_time:8609ms step_avg:34.03ms
step:254/2160 train_time:8643ms step_avg:34.03ms
step:255/2160 train_time:8679ms step_avg:34.04ms
step:256/2160 train_time:8714ms step_avg:34.04ms
step:257/2160 train_time:8749ms step_avg:34.04ms
step:258/2160 train_time:8782ms step_avg:34.04ms
step:259/2160 train_time:8816ms step_avg:34.04ms
step:260/2160 train_time:8849ms step_avg:34.04ms
step:261/2160 train_time:8883ms step_avg:34.03ms
step:262/2160 train_time:8916ms step_avg:34.03ms
step:263/2160 train_time:8950ms step_avg:34.03ms
step:264/2160 train_time:8983ms step_avg:34.02ms
step:265/2160 train_time:9017ms step_avg:34.03ms
step:266/2160 train_time:9050ms step_avg:34.02ms
step:267/2160 train_time:9083ms step_avg:34.02ms
step:268/2160 train_time:9117ms step_avg:34.02ms
step:269/2160 train_time:9150ms step_avg:34.01ms
step:270/2160 train_time:9183ms step_avg:34.01ms
step:271/2160 train_time:9217ms step_avg:34.01ms
step:272/2160 train_time:9249ms step_avg:34.00ms
step:273/2160 train_time:9283ms step_avg:34.00ms
step:274/2160 train_time:9316ms step_avg:34.00ms
step:275/2160 train_time:9350ms step_avg:34.00ms
step:276/2160 train_time:9382ms step_avg:33.99ms
step:277/2160 train_time:9416ms step_avg:33.99ms
step:278/2160 train_time:9449ms step_avg:33.99ms
step:279/2160 train_time:9482ms step_avg:33.99ms
step:280/2160 train_time:9516ms step_avg:33.98ms
step:281/2160 train_time:9549ms step_avg:33.98ms
step:282/2160 train_time:9582ms step_avg:33.98ms
step:283/2160 train_time:9616ms step_avg:33.98ms
step:284/2160 train_time:9650ms step_avg:33.98ms
step:285/2160 train_time:9684ms step_avg:33.98ms
step:286/2160 train_time:9718ms step_avg:33.98ms
step:287/2160 train_time:9752ms step_avg:33.98ms
step:288/2160 train_time:9785ms step_avg:33.98ms
step:289/2160 train_time:9819ms step_avg:33.98ms
step:290/2160 train_time:9852ms step_avg:33.97ms
step:291/2160 train_time:9886ms step_avg:33.97ms
step:292/2160 train_time:9919ms step_avg:33.97ms
step:293/2160 train_time:9953ms step_avg:33.97ms
step:294/2160 train_time:9986ms step_avg:33.96ms
step:295/2160 train_time:10020ms step_avg:33.97ms
step:296/2160 train_time:10053ms step_avg:33.96ms
step:297/2160 train_time:10086ms step_avg:33.96ms
step:298/2160 train_time:10120ms step_avg:33.96ms
step:299/2160 train_time:10153ms step_avg:33.96ms
step:300/2160 train_time:10186ms step_avg:33.95ms
step:301/2160 train_time:10220ms step_avg:33.95ms
step:302/2160 train_time:10253ms step_avg:33.95ms
step:303/2160 train_time:10287ms step_avg:33.95ms
step:304/2160 train_time:10320ms step_avg:33.95ms
step:305/2160 train_time:10353ms step_avg:33.95ms
step:306/2160 train_time:10386ms step_avg:33.94ms
step:307/2160 train_time:10420ms step_avg:33.94ms
step:308/2160 train_time:10453ms step_avg:33.94ms
step:309/2160 train_time:10487ms step_avg:33.94ms
step:310/2160 train_time:10520ms step_avg:33.94ms
step:311/2160 train_time:10553ms step_avg:33.93ms
step:312/2160 train_time:10586ms step_avg:33.93ms
step:313/2160 train_time:10621ms step_avg:33.93ms
step:314/2160 train_time:10654ms step_avg:33.93ms
step:315/2160 train_time:10688ms step_avg:33.93ms
step:316/2160 train_time:10721ms step_avg:33.93ms
step:317/2160 train_time:10755ms step_avg:33.93ms
step:318/2160 train_time:10788ms step_avg:33.92ms
step:319/2160 train_time:10822ms step_avg:33.92ms
step:320/2160 train_time:10855ms step_avg:33.92ms
step:321/2160 train_time:10889ms step_avg:33.92ms
step:322/2160 train_time:10922ms step_avg:33.92ms
step:323/2160 train_time:10955ms step_avg:33.92ms
step:324/2160 train_time:10989ms step_avg:33.92ms
step:325/2160 train_time:11023ms step_avg:33.92ms
step:326/2160 train_time:11057ms step_avg:33.92ms
step:327/2160 train_time:11090ms step_avg:33.91ms
step:328/2160 train_time:11123ms step_avg:33.91ms
step:329/2160 train_time:11157ms step_avg:33.91ms
step:330/2160 train_time:11190ms step_avg:33.91ms
step:331/2160 train_time:11224ms step_avg:33.91ms
step:332/2160 train_time:11258ms step_avg:33.91ms
step:333/2160 train_time:11291ms step_avg:33.91ms
step:334/2160 train_time:11324ms step_avg:33.91ms
step:335/2160 train_time:11358ms step_avg:33.90ms
step:336/2160 train_time:11391ms step_avg:33.90ms
step:337/2160 train_time:11425ms step_avg:33.90ms
step:338/2160 train_time:11458ms step_avg:33.90ms
step:339/2160 train_time:11491ms step_avg:33.90ms
step:340/2160 train_time:11525ms step_avg:33.90ms
step:341/2160 train_time:11558ms step_avg:33.89ms
step:342/2160 train_time:11591ms step_avg:33.89ms
step:343/2160 train_time:11625ms step_avg:33.89ms
step:344/2160 train_time:11658ms step_avg:33.89ms
step:345/2160 train_time:11692ms step_avg:33.89ms
step:346/2160 train_time:11725ms step_avg:33.89ms
step:347/2160 train_time:11759ms step_avg:33.89ms
step:348/2160 train_time:11792ms step_avg:33.88ms
step:349/2160 train_time:11827ms step_avg:33.89ms
step:350/2160 train_time:11860ms step_avg:33.88ms
step:351/2160 train_time:11893ms step_avg:33.88ms
step:352/2160 train_time:11926ms step_avg:33.88ms
step:353/2160 train_time:11960ms step_avg:33.88ms
step:354/2160 train_time:11993ms step_avg:33.88ms
step:355/2160 train_time:12028ms step_avg:33.88ms
step:356/2160 train_time:12061ms step_avg:33.88ms
step:357/2160 train_time:12095ms step_avg:33.88ms
step:358/2160 train_time:12128ms step_avg:33.88ms
step:359/2160 train_time:12162ms step_avg:33.88ms
step:360/2160 train_time:12195ms step_avg:33.88ms
step:361/2160 train_time:12229ms step_avg:33.88ms
step:362/2160 train_time:12262ms step_avg:33.87ms
step:363/2160 train_time:12296ms step_avg:33.87ms
step:364/2160 train_time:12329ms step_avg:33.87ms
step:365/2160 train_time:12362ms step_avg:33.87ms
step:366/2160 train_time:12396ms step_avg:33.87ms
step:367/2160 train_time:12429ms step_avg:33.87ms
step:368/2160 train_time:12463ms step_avg:33.87ms
step:369/2160 train_time:12496ms step_avg:33.87ms
step:370/2160 train_time:12529ms step_avg:33.86ms
step:371/2160 train_time:12563ms step_avg:33.86ms
step:372/2160 train_time:12596ms step_avg:33.86ms
step:373/2160 train_time:12630ms step_avg:33.86ms
step:374/2160 train_time:12663ms step_avg:33.86ms
step:375/2160 train_time:12696ms step_avg:33.86ms
step:376/2160 train_time:12729ms step_avg:33.85ms
step:377/2160 train_time:12764ms step_avg:33.86ms
step:378/2160 train_time:12797ms step_avg:33.85ms
step:379/2160 train_time:12830ms step_avg:33.85ms
step:380/2160 train_time:12864ms step_avg:33.85ms
step:381/2160 train_time:12897ms step_avg:33.85ms
step:382/2160 train_time:12930ms step_avg:33.85ms
step:383/2160 train_time:12965ms step_avg:33.85ms
step:384/2160 train_time:12998ms step_avg:33.85ms
step:385/2160 train_time:13031ms step_avg:33.85ms
step:386/2160 train_time:13065ms step_avg:33.85ms
step:387/2160 train_time:13099ms step_avg:33.85ms
step:388/2160 train_time:13132ms step_avg:33.84ms
step:389/2160 train_time:13166ms step_avg:33.85ms
step:390/2160 train_time:13199ms step_avg:33.84ms
step:391/2160 train_time:13233ms step_avg:33.84ms
step:392/2160 train_time:13266ms step_avg:33.84ms
step:393/2160 train_time:13299ms step_avg:33.84ms
step:394/2160 train_time:13332ms step_avg:33.84ms
step:395/2160 train_time:13366ms step_avg:33.84ms
step:396/2160 train_time:13400ms step_avg:33.84ms
step:397/2160 train_time:13433ms step_avg:33.84ms
step:398/2160 train_time:13466ms step_avg:33.83ms
step:399/2160 train_time:13500ms step_avg:33.83ms
step:400/2160 train_time:13533ms step_avg:33.83ms
step:401/2160 train_time:13567ms step_avg:33.83ms
step:402/2160 train_time:13600ms step_avg:33.83ms
step:403/2160 train_time:13633ms step_avg:33.83ms
step:404/2160 train_time:13667ms step_avg:33.83ms
step:405/2160 train_time:13700ms step_avg:33.83ms
step:406/2160 train_time:13734ms step_avg:33.83ms
step:407/2160 train_time:13768ms step_avg:33.83ms
step:408/2160 train_time:13801ms step_avg:33.83ms
step:409/2160 train_time:13835ms step_avg:33.83ms
step:410/2160 train_time:13869ms step_avg:33.83ms
step:411/2160 train_time:13902ms step_avg:33.82ms
step:412/2160 train_time:13935ms step_avg:33.82ms
step:413/2160 train_time:13969ms step_avg:33.82ms
step:414/2160 train_time:14002ms step_avg:33.82ms
step:415/2160 train_time:14035ms step_avg:33.82ms
step:416/2160 train_time:14069ms step_avg:33.82ms
step:417/2160 train_time:14102ms step_avg:33.82ms
step:418/2160 train_time:14136ms step_avg:33.82ms
step:419/2160 train_time:14169ms step_avg:33.82ms
step:420/2160 train_time:14202ms step_avg:33.82ms
step:421/2160 train_time:14236ms step_avg:33.81ms
step:422/2160 train_time:14269ms step_avg:33.81ms
step:423/2160 train_time:14303ms step_avg:33.81ms
step:424/2160 train_time:14336ms step_avg:33.81ms
step:425/2160 train_time:14369ms step_avg:33.81ms
step:426/2160 train_time:14403ms step_avg:33.81ms
step:427/2160 train_time:14436ms step_avg:33.81ms
step:428/2160 train_time:14469ms step_avg:33.81ms
step:429/2160 train_time:14503ms step_avg:33.81ms
step:430/2160 train_time:14537ms step_avg:33.81ms
step:431/2160 train_time:14570ms step_avg:33.81ms
step:432/2160 train_time:14603ms step_avg:33.80ms
step:433/2160 train_time:14637ms step_avg:33.80ms
step:434/2160 train_time:14670ms step_avg:33.80ms
step:435/2160 train_time:14704ms step_avg:33.80ms
step:436/2160 train_time:14737ms step_avg:33.80ms
step:437/2160 train_time:14771ms step_avg:33.80ms
step:438/2160 train_time:14804ms step_avg:33.80ms
step:439/2160 train_time:14838ms step_avg:33.80ms
step:440/2160 train_time:14871ms step_avg:33.80ms
step:441/2160 train_time:14905ms step_avg:33.80ms
step:442/2160 train_time:14938ms step_avg:33.80ms
step:443/2160 train_time:14971ms step_avg:33.80ms
step:444/2160 train_time:15004ms step_avg:33.79ms
step:445/2160 train_time:15038ms step_avg:33.79ms
step:446/2160 train_time:15071ms step_avg:33.79ms
step:447/2160 train_time:15105ms step_avg:33.79ms
step:448/2160 train_time:15138ms step_avg:33.79ms
step:449/2160 train_time:15172ms step_avg:33.79ms
step:450/2160 train_time:15205ms step_avg:33.79ms
step:451/2160 train_time:15239ms step_avg:33.79ms
step:452/2160 train_time:15272ms step_avg:33.79ms
step:453/2160 train_time:15307ms step_avg:33.79ms
step:454/2160 train_time:15340ms step_avg:33.79ms
step:455/2160 train_time:15374ms step_avg:33.79ms
step:456/2160 train_time:15407ms step_avg:33.79ms
step:457/2160 train_time:15441ms step_avg:33.79ms
step:458/2160 train_time:15474ms step_avg:33.78ms
step:459/2160 train_time:15507ms step_avg:33.79ms
step:460/2160 train_time:15540ms step_avg:33.78ms
step:461/2160 train_time:15574ms step_avg:33.78ms
step:462/2160 train_time:15607ms step_avg:33.78ms
step:463/2160 train_time:15641ms step_avg:33.78ms
step:464/2160 train_time:15675ms step_avg:33.78ms
step:465/2160 train_time:15708ms step_avg:33.78ms
step:466/2160 train_time:15741ms step_avg:33.78ms
step:467/2160 train_time:15775ms step_avg:33.78ms
step:468/2160 train_time:15808ms step_avg:33.78ms
step:469/2160 train_time:15842ms step_avg:33.78ms
step:470/2160 train_time:15875ms step_avg:33.78ms
step:471/2160 train_time:15909ms step_avg:33.78ms
step:472/2160 train_time:15942ms step_avg:33.77ms
step:473/2160 train_time:15975ms step_avg:33.77ms
step:474/2160 train_time:16008ms step_avg:33.77ms
step:475/2160 train_time:16042ms step_avg:33.77ms
step:476/2160 train_time:16075ms step_avg:33.77ms
step:477/2160 train_time:16109ms step_avg:33.77ms
step:478/2160 train_time:16142ms step_avg:33.77ms
step:479/2160 train_time:16176ms step_avg:33.77ms
step:480/2160 train_time:16209ms step_avg:33.77ms
step:481/2160 train_time:16242ms step_avg:33.77ms
step:482/2160 train_time:16275ms step_avg:33.77ms
step:483/2160 train_time:16309ms step_avg:33.77ms
step:484/2160 train_time:16342ms step_avg:33.77ms
step:485/2160 train_time:16376ms step_avg:33.76ms
step:486/2160 train_time:16409ms step_avg:33.76ms
step:487/2160 train_time:16443ms step_avg:33.76ms
step:488/2160 train_time:16477ms step_avg:33.76ms
step:489/2160 train_time:16511ms step_avg:33.76ms
step:490/2160 train_time:16544ms step_avg:33.76ms
step:491/2160 train_time:16577ms step_avg:33.76ms
step:492/2160 train_time:16610ms step_avg:33.76ms
step:493/2160 train_time:16644ms step_avg:33.76ms
step:494/2160 train_time:16677ms step_avg:33.76ms
step:495/2160 train_time:16711ms step_avg:33.76ms
step:496/2160 train_time:16744ms step_avg:33.76ms
step:497/2160 train_time:16778ms step_avg:33.76ms
step:498/2160 train_time:16811ms step_avg:33.76ms
step:499/2160 train_time:16844ms step_avg:33.76ms
step:500/2160 train_time:16877ms step_avg:33.75ms
step:500/2160 val_loss:4.0149 train_time:16912ms step_avg:33.82ms
step:501/2160 train_time:16935ms step_avg:33.80ms
step:502/2160 train_time:16957ms step_avg:33.78ms
step:503/2160 train_time:16981ms step_avg:33.76ms
step:504/2160 train_time:17015ms step_avg:33.76ms
step:505/2160 train_time:17052ms step_avg:33.77ms
step:506/2160 train_time:17087ms step_avg:33.77ms
step:507/2160 train_time:17122ms step_avg:33.77ms
step:508/2160 train_time:17155ms step_avg:33.77ms
step:509/2160 train_time:17189ms step_avg:33.77ms
step:510/2160 train_time:17222ms step_avg:33.77ms
step:511/2160 train_time:17255ms step_avg:33.77ms
step:512/2160 train_time:17288ms step_avg:33.77ms
step:513/2160 train_time:17322ms step_avg:33.77ms
step:514/2160 train_time:17355ms step_avg:33.76ms
step:515/2160 train_time:17388ms step_avg:33.76ms
step:516/2160 train_time:17421ms step_avg:33.76ms
step:517/2160 train_time:17455ms step_avg:33.76ms
step:518/2160 train_time:17488ms step_avg:33.76ms
step:519/2160 train_time:17521ms step_avg:33.76ms
step:520/2160 train_time:17554ms step_avg:33.76ms
step:521/2160 train_time:17587ms step_avg:33.76ms
step:522/2160 train_time:17620ms step_avg:33.76ms
step:523/2160 train_time:17654ms step_avg:33.75ms
step:524/2160 train_time:17687ms step_avg:33.75ms
step:525/2160 train_time:17720ms step_avg:33.75ms
step:526/2160 train_time:17753ms step_avg:33.75ms
step:527/2160 train_time:17786ms step_avg:33.75ms
step:528/2160 train_time:17819ms step_avg:33.75ms
step:529/2160 train_time:17853ms step_avg:33.75ms
step:530/2160 train_time:17886ms step_avg:33.75ms
step:531/2160 train_time:17920ms step_avg:33.75ms
step:532/2160 train_time:17953ms step_avg:33.75ms
step:533/2160 train_time:17987ms step_avg:33.75ms
step:534/2160 train_time:18020ms step_avg:33.75ms
step:535/2160 train_time:18055ms step_avg:33.75ms
step:536/2160 train_time:18088ms step_avg:33.75ms
step:537/2160 train_time:18123ms step_avg:33.75ms
step:538/2160 train_time:18156ms step_avg:33.75ms
step:539/2160 train_time:18190ms step_avg:33.75ms
step:540/2160 train_time:18224ms step_avg:33.75ms
step:541/2160 train_time:18257ms step_avg:33.75ms
step:542/2160 train_time:18290ms step_avg:33.75ms
step:543/2160 train_time:18324ms step_avg:33.75ms
step:544/2160 train_time:18357ms step_avg:33.75ms
step:545/2160 train_time:18391ms step_avg:33.74ms
step:546/2160 train_time:18424ms step_avg:33.74ms
step:547/2160 train_time:18457ms step_avg:33.74ms
step:548/2160 train_time:18490ms step_avg:33.74ms
step:549/2160 train_time:18524ms step_avg:33.74ms
step:550/2160 train_time:18557ms step_avg:33.74ms
step:551/2160 train_time:18591ms step_avg:33.74ms
step:552/2160 train_time:18624ms step_avg:33.74ms
step:553/2160 train_time:18658ms step_avg:33.74ms
step:554/2160 train_time:18691ms step_avg:33.74ms
step:555/2160 train_time:18724ms step_avg:33.74ms
step:556/2160 train_time:18758ms step_avg:33.74ms
step:557/2160 train_time:18791ms step_avg:33.74ms
step:558/2160 train_time:18824ms step_avg:33.73ms
step:559/2160 train_time:18858ms step_avg:33.73ms
step:560/2160 train_time:18890ms step_avg:33.73ms
step:561/2160 train_time:18925ms step_avg:33.73ms
step:562/2160 train_time:18958ms step_avg:33.73ms
step:563/2160 train_time:18992ms step_avg:33.73ms
step:564/2160 train_time:19025ms step_avg:33.73ms
step:565/2160 train_time:19059ms step_avg:33.73ms
step:566/2160 train_time:19092ms step_avg:33.73ms
step:567/2160 train_time:19126ms step_avg:33.73ms
step:568/2160 train_time:19159ms step_avg:33.73ms
step:569/2160 train_time:19193ms step_avg:33.73ms
step:570/2160 train_time:19226ms step_avg:33.73ms
step:571/2160 train_time:19260ms step_avg:33.73ms
step:572/2160 train_time:19293ms step_avg:33.73ms
step:573/2160 train_time:19327ms step_avg:33.73ms
step:574/2160 train_time:19361ms step_avg:33.73ms
step:575/2160 train_time:19394ms step_avg:33.73ms
step:576/2160 train_time:19427ms step_avg:33.73ms
step:577/2160 train_time:19461ms step_avg:33.73ms
step:578/2160 train_time:19494ms step_avg:33.73ms
step:579/2160 train_time:19527ms step_avg:33.73ms
step:580/2160 train_time:19561ms step_avg:33.73ms
step:581/2160 train_time:19594ms step_avg:33.72ms
step:582/2160 train_time:19627ms step_avg:33.72ms
step:583/2160 train_time:19661ms step_avg:33.72ms
step:584/2160 train_time:19694ms step_avg:33.72ms
step:585/2160 train_time:19727ms step_avg:33.72ms
step:586/2160 train_time:19760ms step_avg:33.72ms
step:587/2160 train_time:19794ms step_avg:33.72ms
step:588/2160 train_time:19827ms step_avg:33.72ms
step:589/2160 train_time:19861ms step_avg:33.72ms
step:590/2160 train_time:19894ms step_avg:33.72ms
step:591/2160 train_time:19928ms step_avg:33.72ms
step:592/2160 train_time:19961ms step_avg:33.72ms
step:593/2160 train_time:19995ms step_avg:33.72ms
step:594/2160 train_time:20028ms step_avg:33.72ms
step:595/2160 train_time:20062ms step_avg:33.72ms
step:596/2160 train_time:20096ms step_avg:33.72ms
step:597/2160 train_time:20130ms step_avg:33.72ms
step:598/2160 train_time:20163ms step_avg:33.72ms
step:599/2160 train_time:20196ms step_avg:33.72ms
step:600/2160 train_time:20230ms step_avg:33.72ms
step:601/2160 train_time:20263ms step_avg:33.72ms
step:602/2160 train_time:20296ms step_avg:33.71ms
step:603/2160 train_time:20330ms step_avg:33.72ms
step:604/2160 train_time:20363ms step_avg:33.71ms
step:605/2160 train_time:20397ms step_avg:33.71ms
step:606/2160 train_time:20430ms step_avg:33.71ms
step:607/2160 train_time:20464ms step_avg:33.71ms
step:608/2160 train_time:20497ms step_avg:33.71ms
step:609/2160 train_time:20531ms step_avg:33.71ms
step:610/2160 train_time:20564ms step_avg:33.71ms
step:611/2160 train_time:20597ms step_avg:33.71ms
step:612/2160 train_time:20630ms step_avg:33.71ms
step:613/2160 train_time:20665ms step_avg:33.71ms
step:614/2160 train_time:20698ms step_avg:33.71ms
step:615/2160 train_time:20731ms step_avg:33.71ms
step:616/2160 train_time:20764ms step_avg:33.71ms
step:617/2160 train_time:20798ms step_avg:33.71ms
step:618/2160 train_time:20831ms step_avg:33.71ms
step:619/2160 train_time:20865ms step_avg:33.71ms
step:620/2160 train_time:20898ms step_avg:33.71ms
step:621/2160 train_time:20932ms step_avg:33.71ms
step:622/2160 train_time:20965ms step_avg:33.71ms
step:623/2160 train_time:20999ms step_avg:33.71ms
step:624/2160 train_time:21033ms step_avg:33.71ms
step:625/2160 train_time:21067ms step_avg:33.71ms
step:626/2160 train_time:21100ms step_avg:33.71ms
step:627/2160 train_time:21133ms step_avg:33.71ms
step:628/2160 train_time:21167ms step_avg:33.70ms
step:629/2160 train_time:21200ms step_avg:33.71ms
step:630/2160 train_time:21234ms step_avg:33.70ms
step:631/2160 train_time:21268ms step_avg:33.70ms
step:632/2160 train_time:21301ms step_avg:33.70ms
step:633/2160 train_time:21334ms step_avg:33.70ms
step:634/2160 train_time:21367ms step_avg:33.70ms
step:635/2160 train_time:21401ms step_avg:33.70ms
step:636/2160 train_time:21434ms step_avg:33.70ms
step:637/2160 train_time:21468ms step_avg:33.70ms
step:638/2160 train_time:21501ms step_avg:33.70ms
step:639/2160 train_time:21535ms step_avg:33.70ms
step:640/2160 train_time:21568ms step_avg:33.70ms
step:641/2160 train_time:21602ms step_avg:33.70ms
step:642/2160 train_time:21635ms step_avg:33.70ms
step:643/2160 train_time:21668ms step_avg:33.70ms
step:644/2160 train_time:21702ms step_avg:33.70ms
step:645/2160 train_time:21735ms step_avg:33.70ms
step:646/2160 train_time:21768ms step_avg:33.70ms
step:647/2160 train_time:21803ms step_avg:33.70ms
step:648/2160 train_time:21835ms step_avg:33.70ms
step:649/2160 train_time:21869ms step_avg:33.70ms
step:650/2160 train_time:21902ms step_avg:33.70ms
step:651/2160 train_time:21936ms step_avg:33.70ms
step:652/2160 train_time:21968ms step_avg:33.69ms
step:653/2160 train_time:22003ms step_avg:33.69ms
step:654/2160 train_time:22036ms step_avg:33.69ms
step:655/2160 train_time:22070ms step_avg:33.69ms
step:656/2160 train_time:22103ms step_avg:33.69ms
step:657/2160 train_time:22137ms step_avg:33.69ms
step:658/2160 train_time:22170ms step_avg:33.69ms
step:659/2160 train_time:22204ms step_avg:33.69ms
step:660/2160 train_time:22237ms step_avg:33.69ms
step:661/2160 train_time:22271ms step_avg:33.69ms
step:662/2160 train_time:22304ms step_avg:33.69ms
step:663/2160 train_time:22338ms step_avg:33.69ms
step:664/2160 train_time:22371ms step_avg:33.69ms
step:665/2160 train_time:22405ms step_avg:33.69ms
step:666/2160 train_time:22438ms step_avg:33.69ms
step:667/2160 train_time:22472ms step_avg:33.69ms
step:668/2160 train_time:22505ms step_avg:33.69ms
step:669/2160 train_time:22539ms step_avg:33.69ms
step:670/2160 train_time:22572ms step_avg:33.69ms
step:671/2160 train_time:22606ms step_avg:33.69ms
step:672/2160 train_time:22640ms step_avg:33.69ms
step:673/2160 train_time:22673ms step_avg:33.69ms
step:674/2160 train_time:22706ms step_avg:33.69ms
step:675/2160 train_time:22740ms step_avg:33.69ms
step:676/2160 train_time:22773ms step_avg:33.69ms
step:677/2160 train_time:22807ms step_avg:33.69ms
step:678/2160 train_time:22840ms step_avg:33.69ms
step:679/2160 train_time:22874ms step_avg:33.69ms
step:680/2160 train_time:22907ms step_avg:33.69ms
step:681/2160 train_time:22941ms step_avg:33.69ms
step:682/2160 train_time:22974ms step_avg:33.69ms
step:683/2160 train_time:23008ms step_avg:33.69ms
step:684/2160 train_time:23041ms step_avg:33.69ms
step:685/2160 train_time:23075ms step_avg:33.69ms
step:686/2160 train_time:23108ms step_avg:33.68ms
step:687/2160 train_time:23142ms step_avg:33.69ms
step:688/2160 train_time:23175ms step_avg:33.68ms
step:689/2160 train_time:23209ms step_avg:33.68ms
step:690/2160 train_time:23242ms step_avg:33.68ms
step:691/2160 train_time:23276ms step_avg:33.68ms
step:692/2160 train_time:23309ms step_avg:33.68ms
step:693/2160 train_time:23343ms step_avg:33.68ms
step:694/2160 train_time:23376ms step_avg:33.68ms
step:695/2160 train_time:23410ms step_avg:33.68ms
step:696/2160 train_time:23444ms step_avg:33.68ms
step:697/2160 train_time:23477ms step_avg:33.68ms
step:698/2160 train_time:23510ms step_avg:33.68ms
step:699/2160 train_time:23544ms step_avg:33.68ms
step:700/2160 train_time:23577ms step_avg:33.68ms
step:701/2160 train_time:23611ms step_avg:33.68ms
step:702/2160 train_time:23644ms step_avg:33.68ms
step:703/2160 train_time:23678ms step_avg:33.68ms
step:704/2160 train_time:23711ms step_avg:33.68ms
step:705/2160 train_time:23745ms step_avg:33.68ms
step:706/2160 train_time:23778ms step_avg:33.68ms
step:707/2160 train_time:23811ms step_avg:33.68ms
step:708/2160 train_time:23845ms step_avg:33.68ms
step:709/2160 train_time:23905ms step_avg:33.72ms
step:710/2160 train_time:23964ms step_avg:33.75ms
step:711/2160 train_time:24025ms step_avg:33.79ms
step:712/2160 train_time:24084ms step_avg:33.83ms
step:713/2160 train_time:24145ms step_avg:33.86ms
step:714/2160 train_time:24204ms step_avg:33.90ms
step:715/2160 train_time:24265ms step_avg:33.94ms
step:716/2160 train_time:24324ms step_avg:33.97ms
step:717/2160 train_time:24386ms step_avg:34.01ms
step:718/2160 train_time:24445ms step_avg:34.05ms
step:719/2160 train_time:24506ms step_avg:34.08ms
step:720/2160 train_time:24565ms step_avg:34.12ms
step:721/2160 train_time:24626ms step_avg:34.16ms
step:722/2160 train_time:24685ms step_avg:34.19ms
step:723/2160 train_time:24746ms step_avg:34.23ms
step:724/2160 train_time:24805ms step_avg:34.26ms
step:725/2160 train_time:24866ms step_avg:34.30ms
step:726/2160 train_time:24925ms step_avg:34.33ms
step:727/2160 train_time:24987ms step_avg:34.37ms
step:728/2160 train_time:25046ms step_avg:34.40ms
step:729/2160 train_time:25107ms step_avg:34.44ms
step:730/2160 train_time:25166ms step_avg:34.47ms
step:731/2160 train_time:25228ms step_avg:34.51ms
step:732/2160 train_time:25287ms step_avg:34.55ms
step:733/2160 train_time:25349ms step_avg:34.58ms
step:734/2160 train_time:25408ms step_avg:34.62ms
step:735/2160 train_time:25469ms step_avg:34.65ms
step:736/2160 train_time:25528ms step_avg:34.69ms
step:737/2160 train_time:25590ms step_avg:34.72ms
step:738/2160 train_time:25650ms step_avg:34.76ms
step:739/2160 train_time:25711ms step_avg:34.79ms
step:740/2160 train_time:25771ms step_avg:34.83ms
step:741/2160 train_time:25832ms step_avg:34.86ms
step:742/2160 train_time:25891ms step_avg:34.89ms
step:743/2160 train_time:25953ms step_avg:34.93ms
step:744/2160 train_time:26012ms step_avg:34.96ms
step:745/2160 train_time:26074ms step_avg:35.00ms
step:746/2160 train_time:26133ms step_avg:35.03ms
step:747/2160 train_time:26195ms step_avg:35.07ms
step:748/2160 train_time:26254ms step_avg:35.10ms
step:749/2160 train_time:26316ms step_avg:35.13ms
step:750/2160 train_time:26375ms step_avg:35.17ms
step:750/2160 val_loss:3.8597 train_time:26437ms step_avg:35.25ms
step:751/2160 train_time:26460ms step_avg:35.23ms
step:752/2160 train_time:26499ms step_avg:35.24ms
step:753/2160 train_time:26564ms step_avg:35.28ms
step:754/2160 train_time:26626ms step_avg:35.31ms
step:755/2160 train_time:26687ms step_avg:35.35ms
step:756/2160 train_time:26746ms step_avg:35.38ms
step:757/2160 train_time:26807ms step_avg:35.41ms
step:758/2160 train_time:26866ms step_avg:35.44ms
step:759/2160 train_time:26927ms step_avg:35.48ms
step:760/2160 train_time:26985ms step_avg:35.51ms
step:761/2160 train_time:27046ms step_avg:35.54ms
step:762/2160 train_time:27105ms step_avg:35.57ms
step:763/2160 train_time:27166ms step_avg:35.60ms
step:764/2160 train_time:27225ms step_avg:35.63ms
step:765/2160 train_time:27285ms step_avg:35.67ms
step:766/2160 train_time:27344ms step_avg:35.70ms
step:767/2160 train_time:27407ms step_avg:35.73ms
step:768/2160 train_time:27468ms step_avg:35.77ms
step:769/2160 train_time:27531ms step_avg:35.80ms
step:770/2160 train_time:27591ms step_avg:35.83ms
step:771/2160 train_time:27653ms step_avg:35.87ms
step:772/2160 train_time:27713ms step_avg:35.90ms
step:773/2160 train_time:27774ms step_avg:35.93ms
step:774/2160 train_time:27833ms step_avg:35.96ms
step:775/2160 train_time:27893ms step_avg:35.99ms
step:776/2160 train_time:27953ms step_avg:36.02ms
step:777/2160 train_time:28014ms step_avg:36.05ms
step:778/2160 train_time:28073ms step_avg:36.08ms
step:779/2160 train_time:28134ms step_avg:36.12ms
step:780/2160 train_time:28194ms step_avg:36.15ms
step:781/2160 train_time:28254ms step_avg:36.18ms
step:782/2160 train_time:28313ms step_avg:36.21ms
step:783/2160 train_time:28374ms step_avg:36.24ms
step:784/2160 train_time:28434ms step_avg:36.27ms
step:785/2160 train_time:28496ms step_avg:36.30ms
step:786/2160 train_time:28555ms step_avg:36.33ms
step:787/2160 train_time:28617ms step_avg:36.36ms
step:788/2160 train_time:28677ms step_avg:36.39ms
step:789/2160 train_time:28738ms step_avg:36.42ms
step:790/2160 train_time:28797ms step_avg:36.45ms
step:791/2160 train_time:28858ms step_avg:36.48ms
step:792/2160 train_time:28917ms step_avg:36.51ms
step:793/2160 train_time:28979ms step_avg:36.54ms
step:794/2160 train_time:29038ms step_avg:36.57ms
step:795/2160 train_time:29099ms step_avg:36.60ms
step:796/2160 train_time:29158ms step_avg:36.63ms
step:797/2160 train_time:29219ms step_avg:36.66ms
step:798/2160 train_time:29277ms step_avg:36.69ms
step:799/2160 train_time:29339ms step_avg:36.72ms
step:800/2160 train_time:29398ms step_avg:36.75ms
step:801/2160 train_time:29460ms step_avg:36.78ms
step:802/2160 train_time:29520ms step_avg:36.81ms
step:803/2160 train_time:29582ms step_avg:36.84ms
step:804/2160 train_time:29643ms step_avg:36.87ms
step:805/2160 train_time:29705ms step_avg:36.90ms
step:806/2160 train_time:29765ms step_avg:36.93ms
step:807/2160 train_time:29826ms step_avg:36.96ms
step:808/2160 train_time:29885ms step_avg:36.99ms
step:809/2160 train_time:29946ms step_avg:37.02ms
step:810/2160 train_time:30006ms step_avg:37.04ms
step:811/2160 train_time:30067ms step_avg:37.07ms
step:812/2160 train_time:30127ms step_avg:37.10ms
step:813/2160 train_time:30188ms step_avg:37.13ms
step:814/2160 train_time:30247ms step_avg:37.16ms
step:815/2160 train_time:30308ms step_avg:37.19ms
step:816/2160 train_time:30367ms step_avg:37.22ms
step:817/2160 train_time:30429ms step_avg:37.24ms
step:818/2160 train_time:30488ms step_avg:37.27ms
step:819/2160 train_time:30550ms step_avg:37.30ms
step:820/2160 train_time:30610ms step_avg:37.33ms
step:821/2160 train_time:30671ms step_avg:37.36ms
step:822/2160 train_time:30731ms step_avg:37.39ms
step:823/2160 train_time:30792ms step_avg:37.41ms
step:824/2160 train_time:30851ms step_avg:37.44ms
step:825/2160 train_time:30912ms step_avg:37.47ms
step:826/2160 train_time:30971ms step_avg:37.49ms
step:827/2160 train_time:31032ms step_avg:37.52ms
step:828/2160 train_time:31091ms step_avg:37.55ms
step:829/2160 train_time:31152ms step_avg:37.58ms
step:830/2160 train_time:31212ms step_avg:37.60ms
step:831/2160 train_time:31272ms step_avg:37.63ms
step:832/2160 train_time:31332ms step_avg:37.66ms
step:833/2160 train_time:31392ms step_avg:37.69ms
step:834/2160 train_time:31452ms step_avg:37.71ms
step:835/2160 train_time:31513ms step_avg:37.74ms
step:836/2160 train_time:31573ms step_avg:37.77ms
step:837/2160 train_time:31634ms step_avg:37.79ms
step:838/2160 train_time:31694ms step_avg:37.82ms
step:839/2160 train_time:31754ms step_avg:37.85ms
step:840/2160 train_time:31814ms step_avg:37.87ms
step:841/2160 train_time:31875ms step_avg:37.90ms
step:842/2160 train_time:31935ms step_avg:37.93ms
step:843/2160 train_time:31996ms step_avg:37.96ms
step:844/2160 train_time:32056ms step_avg:37.98ms
step:845/2160 train_time:32116ms step_avg:38.01ms
step:846/2160 train_time:32175ms step_avg:38.03ms
step:847/2160 train_time:32237ms step_avg:38.06ms
step:848/2160 train_time:32295ms step_avg:38.08ms
step:849/2160 train_time:32356ms step_avg:38.11ms
step:850/2160 train_time:32415ms step_avg:38.14ms
step:851/2160 train_time:32476ms step_avg:38.16ms
step:852/2160 train_time:32536ms step_avg:38.19ms
step:853/2160 train_time:32597ms step_avg:38.21ms
step:854/2160 train_time:32657ms step_avg:38.24ms
step:855/2160 train_time:32718ms step_avg:38.27ms
step:856/2160 train_time:32778ms step_avg:38.29ms
step:857/2160 train_time:32839ms step_avg:38.32ms
step:858/2160 train_time:32898ms step_avg:38.34ms
step:859/2160 train_time:32959ms step_avg:38.37ms
step:860/2160 train_time:33019ms step_avg:38.39ms
step:861/2160 train_time:33081ms step_avg:38.42ms
step:862/2160 train_time:33140ms step_avg:38.45ms
step:863/2160 train_time:33201ms step_avg:38.47ms
step:864/2160 train_time:33261ms step_avg:38.50ms
step:865/2160 train_time:33323ms step_avg:38.52ms
step:866/2160 train_time:33383ms step_avg:38.55ms
step:867/2160 train_time:33444ms step_avg:38.57ms
step:868/2160 train_time:33504ms step_avg:38.60ms
step:869/2160 train_time:33566ms step_avg:38.63ms
step:870/2160 train_time:33626ms step_avg:38.65ms
step:871/2160 train_time:33688ms step_avg:38.68ms
step:872/2160 train_time:33747ms step_avg:38.70ms
step:873/2160 train_time:33809ms step_avg:38.73ms
step:874/2160 train_time:33868ms step_avg:38.75ms
step:875/2160 train_time:33929ms step_avg:38.78ms
step:876/2160 train_time:33988ms step_avg:38.80ms
step:877/2160 train_time:34050ms step_avg:38.83ms
step:878/2160 train_time:34110ms step_avg:38.85ms
step:879/2160 train_time:34171ms step_avg:38.87ms
step:880/2160 train_time:34231ms step_avg:38.90ms
step:881/2160 train_time:34292ms step_avg:38.92ms
step:882/2160 train_time:34352ms step_avg:38.95ms
step:883/2160 train_time:34412ms step_avg:38.97ms
step:884/2160 train_time:34472ms step_avg:39.00ms
step:885/2160 train_time:34533ms step_avg:39.02ms
step:886/2160 train_time:34593ms step_avg:39.04ms
step:887/2160 train_time:34654ms step_avg:39.07ms
step:888/2160 train_time:34713ms step_avg:39.09ms
step:889/2160 train_time:34775ms step_avg:39.12ms
step:890/2160 train_time:34834ms step_avg:39.14ms
step:891/2160 train_time:34894ms step_avg:39.16ms
step:892/2160 train_time:34953ms step_avg:39.19ms
step:893/2160 train_time:35014ms step_avg:39.21ms
step:894/2160 train_time:35074ms step_avg:39.23ms
step:895/2160 train_time:35135ms step_avg:39.26ms
step:896/2160 train_time:35195ms step_avg:39.28ms
step:897/2160 train_time:35257ms step_avg:39.31ms
step:898/2160 train_time:35317ms step_avg:39.33ms
step:899/2160 train_time:35378ms step_avg:39.35ms
step:900/2160 train_time:35437ms step_avg:39.37ms
step:901/2160 train_time:35498ms step_avg:39.40ms
step:902/2160 train_time:35558ms step_avg:39.42ms
step:903/2160 train_time:35619ms step_avg:39.44ms
step:904/2160 train_time:35678ms step_avg:39.47ms
step:905/2160 train_time:35740ms step_avg:39.49ms
step:906/2160 train_time:35800ms step_avg:39.51ms
step:907/2160 train_time:35862ms step_avg:39.54ms
step:908/2160 train_time:35922ms step_avg:39.56ms
step:909/2160 train_time:35984ms step_avg:39.59ms
step:910/2160 train_time:36044ms step_avg:39.61ms
step:911/2160 train_time:36106ms step_avg:39.63ms
step:912/2160 train_time:36167ms step_avg:39.66ms
step:913/2160 train_time:36228ms step_avg:39.68ms
step:914/2160 train_time:36288ms step_avg:39.70ms
step:915/2160 train_time:36349ms step_avg:39.73ms
step:916/2160 train_time:36408ms step_avg:39.75ms
step:917/2160 train_time:36470ms step_avg:39.77ms
step:918/2160 train_time:36529ms step_avg:39.79ms
step:919/2160 train_time:36590ms step_avg:39.82ms
step:920/2160 train_time:36650ms step_avg:39.84ms
step:921/2160 train_time:36711ms step_avg:39.86ms
step:922/2160 train_time:36770ms step_avg:39.88ms
step:923/2160 train_time:36832ms step_avg:39.91ms
step:924/2160 train_time:36893ms step_avg:39.93ms
step:925/2160 train_time:36954ms step_avg:39.95ms
step:926/2160 train_time:37014ms step_avg:39.97ms
step:927/2160 train_time:37075ms step_avg:40.00ms
step:928/2160 train_time:37134ms step_avg:40.02ms
step:929/2160 train_time:37195ms step_avg:40.04ms
step:930/2160 train_time:37254ms step_avg:40.06ms
step:931/2160 train_time:37315ms step_avg:40.08ms
step:932/2160 train_time:37374ms step_avg:40.10ms
step:933/2160 train_time:37435ms step_avg:40.12ms
step:934/2160 train_time:37495ms step_avg:40.14ms
step:935/2160 train_time:37556ms step_avg:40.17ms
step:936/2160 train_time:37615ms step_avg:40.19ms
step:937/2160 train_time:37677ms step_avg:40.21ms
step:938/2160 train_time:37736ms step_avg:40.23ms
step:939/2160 train_time:37797ms step_avg:40.25ms
step:940/2160 train_time:37857ms step_avg:40.27ms
step:941/2160 train_time:37918ms step_avg:40.30ms
step:942/2160 train_time:37978ms step_avg:40.32ms
step:943/2160 train_time:38040ms step_avg:40.34ms
step:944/2160 train_time:38099ms step_avg:40.36ms
step:945/2160 train_time:38161ms step_avg:40.38ms
step:946/2160 train_time:38220ms step_avg:40.40ms
step:947/2160 train_time:38282ms step_avg:40.42ms
step:948/2160 train_time:38341ms step_avg:40.44ms
step:949/2160 train_time:38403ms step_avg:40.47ms
step:950/2160 train_time:38463ms step_avg:40.49ms
step:951/2160 train_time:38525ms step_avg:40.51ms
step:952/2160 train_time:38585ms step_avg:40.53ms
step:953/2160 train_time:38646ms step_avg:40.55ms
step:954/2160 train_time:38706ms step_avg:40.57ms
step:955/2160 train_time:38768ms step_avg:40.59ms
step:956/2160 train_time:38827ms step_avg:40.61ms
step:957/2160 train_time:38889ms step_avg:40.64ms
step:958/2160 train_time:38948ms step_avg:40.66ms
step:959/2160 train_time:39010ms step_avg:40.68ms
step:960/2160 train_time:39069ms step_avg:40.70ms
step:961/2160 train_time:39130ms step_avg:40.72ms
step:962/2160 train_time:39190ms step_avg:40.74ms
step:963/2160 train_time:39251ms step_avg:40.76ms
step:964/2160 train_time:39310ms step_avg:40.78ms
step:965/2160 train_time:39371ms step_avg:40.80ms
step:966/2160 train_time:39431ms step_avg:40.82ms
step:967/2160 train_time:39492ms step_avg:40.84ms
step:968/2160 train_time:39551ms step_avg:40.86ms
step:969/2160 train_time:39612ms step_avg:40.88ms
step:970/2160 train_time:39672ms step_avg:40.90ms
step:971/2160 train_time:39732ms step_avg:40.92ms
step:972/2160 train_time:39792ms step_avg:40.94ms
step:973/2160 train_time:39853ms step_avg:40.96ms
step:974/2160 train_time:39913ms step_avg:40.98ms
step:975/2160 train_time:39974ms step_avg:41.00ms
step:976/2160 train_time:40033ms step_avg:41.02ms
step:977/2160 train_time:40094ms step_avg:41.04ms
step:978/2160 train_time:40153ms step_avg:41.06ms
step:979/2160 train_time:40214ms step_avg:41.08ms
step:980/2160 train_time:40274ms step_avg:41.10ms
step:981/2160 train_time:40335ms step_avg:41.12ms
step:982/2160 train_time:40395ms step_avg:41.14ms
step:983/2160 train_time:40455ms step_avg:41.16ms
step:984/2160 train_time:40515ms step_avg:41.17ms
step:985/2160 train_time:40576ms step_avg:41.19ms
step:986/2160 train_time:40635ms step_avg:41.21ms
step:987/2160 train_time:40697ms step_avg:41.23ms
step:988/2160 train_time:40756ms step_avg:41.25ms
step:989/2160 train_time:40817ms step_avg:41.27ms
step:990/2160 train_time:40876ms step_avg:41.29ms
step:991/2160 train_time:40938ms step_avg:41.31ms
step:992/2160 train_time:40998ms step_avg:41.33ms
step:993/2160 train_time:41059ms step_avg:41.35ms
step:994/2160 train_time:41119ms step_avg:41.37ms
step:995/2160 train_time:41180ms step_avg:41.39ms
step:996/2160 train_time:41240ms step_avg:41.41ms
step:997/2160 train_time:41302ms step_avg:41.43ms
step:998/2160 train_time:41362ms step_avg:41.44ms
step:999/2160 train_time:41424ms step_avg:41.47ms
step:1000/2160 train_time:41484ms step_avg:41.48ms
step:1000/2160 val_loss:3.6924 train_time:41546ms step_avg:41.55ms
step:1001/2160 train_time:41569ms step_avg:41.53ms
step:1002/2160 train_time:41608ms step_avg:41.52ms
step:1003/2160 train_time:41673ms step_avg:41.55ms
step:1004/2160 train_time:41737ms step_avg:41.57ms
step:1005/2160 train_time:41798ms step_avg:41.59ms
step:1006/2160 train_time:41857ms step_avg:41.61ms
step:1007/2160 train_time:41918ms step_avg:41.63ms
step:1008/2160 train_time:41977ms step_avg:41.64ms
step:1009/2160 train_time:42037ms step_avg:41.66ms
step:1010/2160 train_time:42096ms step_avg:41.68ms
step:1011/2160 train_time:42155ms step_avg:41.70ms
step:1012/2160 train_time:42214ms step_avg:41.71ms
step:1013/2160 train_time:42275ms step_avg:41.73ms
step:1014/2160 train_time:42334ms step_avg:41.75ms
step:1015/2160 train_time:42394ms step_avg:41.77ms
step:1016/2160 train_time:42453ms step_avg:41.78ms
step:1017/2160 train_time:42517ms step_avg:41.81ms
step:1018/2160 train_time:42578ms step_avg:41.83ms
step:1019/2160 train_time:42642ms step_avg:41.85ms
step:1020/2160 train_time:42702ms step_avg:41.86ms
step:1021/2160 train_time:42765ms step_avg:41.89ms
step:1022/2160 train_time:42826ms step_avg:41.90ms
step:1023/2160 train_time:42888ms step_avg:41.92ms
step:1024/2160 train_time:42948ms step_avg:41.94ms
step:1025/2160 train_time:43009ms step_avg:41.96ms
step:1026/2160 train_time:43069ms step_avg:41.98ms
step:1027/2160 train_time:43130ms step_avg:42.00ms
step:1028/2160 train_time:43189ms step_avg:42.01ms
step:1029/2160 train_time:43250ms step_avg:42.03ms
step:1030/2160 train_time:43309ms step_avg:42.05ms
step:1031/2160 train_time:43370ms step_avg:42.07ms
step:1032/2160 train_time:43430ms step_avg:42.08ms
step:1033/2160 train_time:43492ms step_avg:42.10ms
step:1034/2160 train_time:43552ms step_avg:42.12ms
step:1035/2160 train_time:43614ms step_avg:42.14ms
step:1036/2160 train_time:43674ms step_avg:42.16ms
step:1037/2160 train_time:43736ms step_avg:42.18ms
step:1038/2160 train_time:43796ms step_avg:42.19ms
step:1039/2160 train_time:43857ms step_avg:42.21ms
step:1040/2160 train_time:43917ms step_avg:42.23ms
step:1041/2160 train_time:43978ms step_avg:42.25ms
step:1042/2160 train_time:44037ms step_avg:42.26ms
step:1043/2160 train_time:44098ms step_avg:42.28ms
step:1044/2160 train_time:44158ms step_avg:42.30ms
step:1045/2160 train_time:44218ms step_avg:42.31ms
step:1046/2160 train_time:44278ms step_avg:42.33ms
step:1047/2160 train_time:44338ms step_avg:42.35ms
step:1048/2160 train_time:44398ms step_avg:42.36ms
step:1049/2160 train_time:44459ms step_avg:42.38ms
step:1050/2160 train_time:44518ms step_avg:42.40ms
step:1051/2160 train_time:44580ms step_avg:42.42ms
step:1052/2160 train_time:44641ms step_avg:42.43ms
step:1053/2160 train_time:44702ms step_avg:42.45ms
step:1054/2160 train_time:44762ms step_avg:42.47ms
step:1055/2160 train_time:44824ms step_avg:42.49ms
step:1056/2160 train_time:44883ms step_avg:42.50ms
step:1057/2160 train_time:44946ms step_avg:42.52ms
step:1058/2160 train_time:45006ms step_avg:42.54ms
step:1059/2160 train_time:45068ms step_avg:42.56ms
step:1060/2160 train_time:45128ms step_avg:42.57ms
step:1061/2160 train_time:45189ms step_avg:42.59ms
step:1062/2160 train_time:45249ms step_avg:42.61ms
step:1063/2160 train_time:45311ms step_avg:42.63ms
step:1064/2160 train_time:45370ms step_avg:42.64ms
step:1065/2160 train_time:45432ms step_avg:42.66ms
step:1066/2160 train_time:45491ms step_avg:42.67ms
step:1067/2160 train_time:45553ms step_avg:42.69ms
step:1068/2160 train_time:45612ms step_avg:42.71ms
step:1069/2160 train_time:45674ms step_avg:42.73ms
step:1070/2160 train_time:45733ms step_avg:42.74ms
step:1071/2160 train_time:45795ms step_avg:42.76ms
step:1072/2160 train_time:45856ms step_avg:42.78ms
step:1073/2160 train_time:45917ms step_avg:42.79ms
step:1074/2160 train_time:45978ms step_avg:42.81ms
step:1075/2160 train_time:46040ms step_avg:42.83ms
step:1076/2160 train_time:46099ms step_avg:42.84ms
step:1077/2160 train_time:46160ms step_avg:42.86ms
step:1078/2160 train_time:46219ms step_avg:42.88ms
step:1079/2160 train_time:46280ms step_avg:42.89ms
step:1080/2160 train_time:46339ms step_avg:42.91ms
step:1081/2160 train_time:46401ms step_avg:42.92ms
step:1082/2160 train_time:46460ms step_avg:42.94ms
step:1083/2160 train_time:46522ms step_avg:42.96ms
step:1084/2160 train_time:46581ms step_avg:42.97ms
step:1085/2160 train_time:46643ms step_avg:42.99ms
step:1086/2160 train_time:46703ms step_avg:43.00ms
step:1087/2160 train_time:46765ms step_avg:43.02ms
step:1088/2160 train_time:46825ms step_avg:43.04ms
step:1089/2160 train_time:46888ms step_avg:43.06ms
step:1090/2160 train_time:46949ms step_avg:43.07ms
step:1091/2160 train_time:47012ms step_avg:43.09ms
step:1092/2160 train_time:47072ms step_avg:43.11ms
step:1093/2160 train_time:47133ms step_avg:43.12ms
step:1094/2160 train_time:47192ms step_avg:43.14ms
step:1095/2160 train_time:47254ms step_avg:43.15ms
step:1096/2160 train_time:47313ms step_avg:43.17ms
step:1097/2160 train_time:47374ms step_avg:43.19ms
step:1098/2160 train_time:47433ms step_avg:43.20ms
step:1099/2160 train_time:47494ms step_avg:43.22ms
step:1100/2160 train_time:47554ms step_avg:43.23ms
step:1101/2160 train_time:47614ms step_avg:43.25ms
step:1102/2160 train_time:47674ms step_avg:43.26ms
step:1103/2160 train_time:47736ms step_avg:43.28ms
step:1104/2160 train_time:47796ms step_avg:43.29ms
step:1105/2160 train_time:47857ms step_avg:43.31ms
step:1106/2160 train_time:47916ms step_avg:43.32ms
step:1107/2160 train_time:47978ms step_avg:43.34ms
step:1108/2160 train_time:48038ms step_avg:43.36ms
step:1109/2160 train_time:48100ms step_avg:43.37ms
step:1110/2160 train_time:48159ms step_avg:43.39ms
step:1111/2160 train_time:48221ms step_avg:43.40ms
step:1112/2160 train_time:48280ms step_avg:43.42ms
step:1113/2160 train_time:48341ms step_avg:43.43ms
step:1114/2160 train_time:48401ms step_avg:43.45ms
step:1115/2160 train_time:48462ms step_avg:43.46ms
step:1116/2160 train_time:48522ms step_avg:43.48ms
step:1117/2160 train_time:48583ms step_avg:43.49ms
step:1118/2160 train_time:48642ms step_avg:43.51ms
step:1119/2160 train_time:48704ms step_avg:43.52ms
step:1120/2160 train_time:48764ms step_avg:43.54ms
step:1121/2160 train_time:48826ms step_avg:43.56ms
step:1122/2160 train_time:48886ms step_avg:43.57ms
step:1123/2160 train_time:48948ms step_avg:43.59ms
step:1124/2160 train_time:49008ms step_avg:43.60ms
step:1125/2160 train_time:49071ms step_avg:43.62ms
step:1126/2160 train_time:49131ms step_avg:43.63ms
step:1127/2160 train_time:49193ms step_avg:43.65ms
step:1128/2160 train_time:49252ms step_avg:43.66ms
step:1129/2160 train_time:49314ms step_avg:43.68ms
step:1130/2160 train_time:49373ms step_avg:43.69ms
step:1131/2160 train_time:49434ms step_avg:43.71ms
step:1132/2160 train_time:49493ms step_avg:43.72ms
step:1133/2160 train_time:49554ms step_avg:43.74ms
step:1134/2160 train_time:49613ms step_avg:43.75ms
step:1135/2160 train_time:49675ms step_avg:43.77ms
step:1136/2160 train_time:49734ms step_avg:43.78ms
step:1137/2160 train_time:49796ms step_avg:43.80ms
step:1138/2160 train_time:49857ms step_avg:43.81ms
step:1139/2160 train_time:49918ms step_avg:43.83ms
step:1140/2160 train_time:49977ms step_avg:43.84ms
step:1141/2160 train_time:50039ms step_avg:43.86ms
step:1142/2160 train_time:50099ms step_avg:43.87ms
step:1143/2160 train_time:50160ms step_avg:43.88ms
step:1144/2160 train_time:50220ms step_avg:43.90ms
step:1145/2160 train_time:50281ms step_avg:43.91ms
step:1146/2160 train_time:50340ms step_avg:43.93ms
step:1147/2160 train_time:50402ms step_avg:43.94ms
step:1148/2160 train_time:50462ms step_avg:43.96ms
step:1149/2160 train_time:50524ms step_avg:43.97ms
step:1150/2160 train_time:50583ms step_avg:43.99ms
step:1151/2160 train_time:50645ms step_avg:44.00ms
step:1152/2160 train_time:50705ms step_avg:44.02ms
step:1153/2160 train_time:50768ms step_avg:44.03ms
step:1154/2160 train_time:50828ms step_avg:44.05ms
step:1155/2160 train_time:50891ms step_avg:44.06ms
step:1156/2160 train_time:50951ms step_avg:44.08ms
step:1157/2160 train_time:51013ms step_avg:44.09ms
step:1158/2160 train_time:51072ms step_avg:44.10ms
step:1159/2160 train_time:51134ms step_avg:44.12ms
step:1160/2160 train_time:51193ms step_avg:44.13ms
step:1161/2160 train_time:51254ms step_avg:44.15ms
step:1162/2160 train_time:51314ms step_avg:44.16ms
step:1163/2160 train_time:51375ms step_avg:44.17ms
step:1164/2160 train_time:51434ms step_avg:44.19ms
step:1165/2160 train_time:51495ms step_avg:44.20ms
step:1166/2160 train_time:51555ms step_avg:44.22ms
step:1167/2160 train_time:51616ms step_avg:44.23ms
step:1168/2160 train_time:51675ms step_avg:44.24ms
step:1169/2160 train_time:51737ms step_avg:44.26ms
step:1170/2160 train_time:51796ms step_avg:44.27ms
step:1171/2160 train_time:51858ms step_avg:44.29ms
step:1172/2160 train_time:51918ms step_avg:44.30ms
step:1173/2160 train_time:51980ms step_avg:44.31ms
step:1174/2160 train_time:52040ms step_avg:44.33ms
step:1175/2160 train_time:52100ms step_avg:44.34ms
step:1176/2160 train_time:52160ms step_avg:44.35ms
step:1177/2160 train_time:52222ms step_avg:44.37ms
step:1178/2160 train_time:52282ms step_avg:44.38ms
step:1179/2160 train_time:52343ms step_avg:44.40ms
step:1180/2160 train_time:52403ms step_avg:44.41ms
step:1181/2160 train_time:52464ms step_avg:44.42ms
step:1182/2160 train_time:52524ms step_avg:44.44ms
step:1183/2160 train_time:52585ms step_avg:44.45ms
step:1184/2160 train_time:52645ms step_avg:44.46ms
step:1185/2160 train_time:52707ms step_avg:44.48ms
step:1186/2160 train_time:52767ms step_avg:44.49ms
step:1187/2160 train_time:52830ms step_avg:44.51ms
step:1188/2160 train_time:52890ms step_avg:44.52ms
step:1189/2160 train_time:52952ms step_avg:44.54ms
step:1190/2160 train_time:53012ms step_avg:44.55ms
step:1191/2160 train_time:53073ms step_avg:44.56ms
step:1192/2160 train_time:53133ms step_avg:44.57ms
step:1193/2160 train_time:53194ms step_avg:44.59ms
step:1194/2160 train_time:53254ms step_avg:44.60ms
step:1195/2160 train_time:53315ms step_avg:44.61ms
step:1196/2160 train_time:53374ms step_avg:44.63ms
step:1197/2160 train_time:53435ms step_avg:44.64ms
step:1198/2160 train_time:53495ms step_avg:44.65ms
step:1199/2160 train_time:53556ms step_avg:44.67ms
step:1200/2160 train_time:53617ms step_avg:44.68ms
step:1201/2160 train_time:53678ms step_avg:44.69ms
step:1202/2160 train_time:53737ms step_avg:44.71ms
step:1203/2160 train_time:53798ms step_avg:44.72ms
step:1204/2160 train_time:53859ms step_avg:44.73ms
step:1205/2160 train_time:53920ms step_avg:44.75ms
step:1206/2160 train_time:53979ms step_avg:44.76ms
step:1207/2160 train_time:54040ms step_avg:44.77ms
step:1208/2160 train_time:54100ms step_avg:44.78ms
step:1209/2160 train_time:54162ms step_avg:44.80ms
step:1210/2160 train_time:54222ms step_avg:44.81ms
step:1211/2160 train_time:54283ms step_avg:44.82ms
step:1212/2160 train_time:54342ms step_avg:44.84ms
step:1213/2160 train_time:54404ms step_avg:44.85ms
step:1214/2160 train_time:54464ms step_avg:44.86ms
step:1215/2160 train_time:54526ms step_avg:44.88ms
step:1216/2160 train_time:54586ms step_avg:44.89ms
step:1217/2160 train_time:54648ms step_avg:44.90ms
step:1218/2160 train_time:54709ms step_avg:44.92ms
step:1219/2160 train_time:54772ms step_avg:44.93ms
step:1220/2160 train_time:54831ms step_avg:44.94ms
step:1221/2160 train_time:54893ms step_avg:44.96ms
step:1222/2160 train_time:54952ms step_avg:44.97ms
step:1223/2160 train_time:55014ms step_avg:44.98ms
step:1224/2160 train_time:55073ms step_avg:44.99ms
step:1225/2160 train_time:55134ms step_avg:45.01ms
step:1226/2160 train_time:55193ms step_avg:45.02ms
step:1227/2160 train_time:55255ms step_avg:45.03ms
step:1228/2160 train_time:55314ms step_avg:45.04ms
step:1229/2160 train_time:55375ms step_avg:45.06ms
step:1230/2160 train_time:55435ms step_avg:45.07ms
step:1231/2160 train_time:55496ms step_avg:45.08ms
step:1232/2160 train_time:55556ms step_avg:45.09ms
step:1233/2160 train_time:55617ms step_avg:45.11ms
step:1234/2160 train_time:55677ms step_avg:45.12ms
step:1235/2160 train_time:55738ms step_avg:45.13ms
step:1236/2160 train_time:55798ms step_avg:45.14ms
step:1237/2160 train_time:55859ms step_avg:45.16ms
step:1238/2160 train_time:55918ms step_avg:45.17ms
step:1239/2160 train_time:55979ms step_avg:45.18ms
step:1240/2160 train_time:56038ms step_avg:45.19ms
step:1241/2160 train_time:56099ms step_avg:45.20ms
step:1242/2160 train_time:56159ms step_avg:45.22ms
step:1243/2160 train_time:56220ms step_avg:45.23ms
step:1244/2160 train_time:56280ms step_avg:45.24ms
step:1245/2160 train_time:56341ms step_avg:45.25ms
step:1246/2160 train_time:56401ms step_avg:45.27ms
step:1247/2160 train_time:56463ms step_avg:45.28ms
step:1248/2160 train_time:56523ms step_avg:45.29ms
step:1249/2160 train_time:56584ms step_avg:45.30ms
step:1250/2160 train_time:56644ms step_avg:45.32ms
step:1250/2160 val_loss:3.5744 train_time:56707ms step_avg:45.37ms
step:1251/2160 train_time:56730ms step_avg:45.35ms
step:1252/2160 train_time:56768ms step_avg:45.34ms
step:1253/2160 train_time:56834ms step_avg:45.36ms
step:1254/2160 train_time:56898ms step_avg:45.37ms
step:1255/2160 train_time:56960ms step_avg:45.39ms
step:1256/2160 train_time:57021ms step_avg:45.40ms
step:1257/2160 train_time:57083ms step_avg:45.41ms
step:1258/2160 train_time:57142ms step_avg:45.42ms
step:1259/2160 train_time:57203ms step_avg:45.44ms
step:1260/2160 train_time:57261ms step_avg:45.45ms
step:1261/2160 train_time:57323ms step_avg:45.46ms
step:1262/2160 train_time:57382ms step_avg:45.47ms
step:1263/2160 train_time:57442ms step_avg:45.48ms
step:1264/2160 train_time:57501ms step_avg:45.49ms
step:1265/2160 train_time:57562ms step_avg:45.50ms
step:1266/2160 train_time:57622ms step_avg:45.51ms
step:1267/2160 train_time:57683ms step_avg:45.53ms
step:1268/2160 train_time:57744ms step_avg:45.54ms
step:1269/2160 train_time:57807ms step_avg:45.55ms
step:1270/2160 train_time:57868ms step_avg:45.57ms
step:1271/2160 train_time:57930ms step_avg:45.58ms
step:1272/2160 train_time:57991ms step_avg:45.59ms
step:1273/2160 train_time:58052ms step_avg:45.60ms
step:1274/2160 train_time:58111ms step_avg:45.61ms
step:1275/2160 train_time:58173ms step_avg:45.63ms
step:1276/2160 train_time:58232ms step_avg:45.64ms
step:1277/2160 train_time:58293ms step_avg:45.65ms
step:1278/2160 train_time:58352ms step_avg:45.66ms
step:1279/2160 train_time:58413ms step_avg:45.67ms
step:1280/2160 train_time:58472ms step_avg:45.68ms
step:1281/2160 train_time:58533ms step_avg:45.69ms
step:1282/2160 train_time:58592ms step_avg:45.70ms
step:1283/2160 train_time:58654ms step_avg:45.72ms
step:1284/2160 train_time:58714ms step_avg:45.73ms
step:1285/2160 train_time:58777ms step_avg:45.74ms
step:1286/2160 train_time:58838ms step_avg:45.75ms
step:1287/2160 train_time:58902ms step_avg:45.77ms
step:1288/2160 train_time:58963ms step_avg:45.78ms
step:1289/2160 train_time:59025ms step_avg:45.79ms
step:1290/2160 train_time:59084ms step_avg:45.80ms
step:1291/2160 train_time:59145ms step_avg:45.81ms
step:1292/2160 train_time:59204ms step_avg:45.82ms
step:1293/2160 train_time:59265ms step_avg:45.84ms
step:1294/2160 train_time:59325ms step_avg:45.85ms
step:1295/2160 train_time:59385ms step_avg:45.86ms
step:1296/2160 train_time:59444ms step_avg:45.87ms
step:1297/2160 train_time:59505ms step_avg:45.88ms
step:1298/2160 train_time:59564ms step_avg:45.89ms
step:1299/2160 train_time:59626ms step_avg:45.90ms
step:1300/2160 train_time:59687ms step_avg:45.91ms
step:1301/2160 train_time:59748ms step_avg:45.92ms
step:1302/2160 train_time:59808ms step_avg:45.94ms
step:1303/2160 train_time:59870ms step_avg:45.95ms
step:1304/2160 train_time:59930ms step_avg:45.96ms
step:1305/2160 train_time:59992ms step_avg:45.97ms
step:1306/2160 train_time:60051ms step_avg:45.98ms
step:1307/2160 train_time:60112ms step_avg:45.99ms
step:1308/2160 train_time:60172ms step_avg:46.00ms
step:1309/2160 train_time:60234ms step_avg:46.02ms
step:1310/2160 train_time:60293ms step_avg:46.03ms
step:1311/2160 train_time:60355ms step_avg:46.04ms
step:1312/2160 train_time:60414ms step_avg:46.05ms
step:1313/2160 train_time:60476ms step_avg:46.06ms
step:1314/2160 train_time:60535ms step_avg:46.07ms
step:1315/2160 train_time:60597ms step_avg:46.08ms
step:1316/2160 train_time:60657ms step_avg:46.09ms
step:1317/2160 train_time:60719ms step_avg:46.10ms
step:1318/2160 train_time:60779ms step_avg:46.11ms
step:1319/2160 train_time:60842ms step_avg:46.13ms
step:1320/2160 train_time:60903ms step_avg:46.14ms
step:1321/2160 train_time:60965ms step_avg:46.15ms
step:1322/2160 train_time:61024ms step_avg:46.16ms
step:1323/2160 train_time:61086ms step_avg:46.17ms
step:1324/2160 train_time:61145ms step_avg:46.18ms
step:1325/2160 train_time:61207ms step_avg:46.19ms
step:1326/2160 train_time:61266ms step_avg:46.20ms
step:1327/2160 train_time:61327ms step_avg:46.21ms
step:1328/2160 train_time:61387ms step_avg:46.22ms
step:1329/2160 train_time:61448ms step_avg:46.24ms
step:1330/2160 train_time:61507ms step_avg:46.25ms
step:1331/2160 train_time:61568ms step_avg:46.26ms
step:1332/2160 train_time:61628ms step_avg:46.27ms
step:1333/2160 train_time:61689ms step_avg:46.28ms
step:1334/2160 train_time:61749ms step_avg:46.29ms
step:1335/2160 train_time:61811ms step_avg:46.30ms
step:1336/2160 train_time:61871ms step_avg:46.31ms
step:1337/2160 train_time:61932ms step_avg:46.32ms
step:1338/2160 train_time:61992ms step_avg:46.33ms
step:1339/2160 train_time:62054ms step_avg:46.34ms
step:1340/2160 train_time:62114ms step_avg:46.35ms
step:1341/2160 train_time:62176ms step_avg:46.37ms
step:1342/2160 train_time:62236ms step_avg:46.38ms
step:1343/2160 train_time:62298ms step_avg:46.39ms
step:1344/2160 train_time:62358ms step_avg:46.40ms
step:1345/2160 train_time:62420ms step_avg:46.41ms
step:1346/2160 train_time:62480ms step_avg:46.42ms
step:1347/2160 train_time:62542ms step_avg:46.43ms
step:1348/2160 train_time:62602ms step_avg:46.44ms
step:1349/2160 train_time:62664ms step_avg:46.45ms
step:1350/2160 train_time:62723ms step_avg:46.46ms
step:1351/2160 train_time:62785ms step_avg:46.47ms
step:1352/2160 train_time:62844ms step_avg:46.48ms
step:1353/2160 train_time:62905ms step_avg:46.49ms
step:1354/2160 train_time:62965ms step_avg:46.50ms
step:1355/2160 train_time:63026ms step_avg:46.51ms
step:1356/2160 train_time:63086ms step_avg:46.52ms
step:1357/2160 train_time:63147ms step_avg:46.53ms
step:1358/2160 train_time:63206ms step_avg:46.54ms
step:1359/2160 train_time:63267ms step_avg:46.55ms
step:1360/2160 train_time:63327ms step_avg:46.56ms
step:1361/2160 train_time:63388ms step_avg:46.57ms
step:1362/2160 train_time:63448ms step_avg:46.58ms
step:1363/2160 train_time:63508ms step_avg:46.59ms
step:1364/2160 train_time:63568ms step_avg:46.60ms
step:1365/2160 train_time:63629ms step_avg:46.61ms
step:1366/2160 train_time:63690ms step_avg:46.62ms
step:1367/2160 train_time:63751ms step_avg:46.64ms
step:1368/2160 train_time:63811ms step_avg:46.65ms
step:1369/2160 train_time:63872ms step_avg:46.66ms
step:1370/2160 train_time:63931ms step_avg:46.67ms
step:1371/2160 train_time:63993ms step_avg:46.68ms
step:1372/2160 train_time:64052ms step_avg:46.69ms
step:1373/2160 train_time:64113ms step_avg:46.70ms
step:1374/2160 train_time:64173ms step_avg:46.71ms
step:1375/2160 train_time:64235ms step_avg:46.72ms
step:1376/2160 train_time:64295ms step_avg:46.73ms
step:1377/2160 train_time:64357ms step_avg:46.74ms
step:1378/2160 train_time:64416ms step_avg:46.75ms
step:1379/2160 train_time:64479ms step_avg:46.76ms
step:1380/2160 train_time:64538ms step_avg:46.77ms
step:1381/2160 train_time:64601ms step_avg:46.78ms
step:1382/2160 train_time:64661ms step_avg:46.79ms
step:1383/2160 train_time:64723ms step_avg:46.80ms
step:1384/2160 train_time:64782ms step_avg:46.81ms
step:1385/2160 train_time:64844ms step_avg:46.82ms
step:1386/2160 train_time:64903ms step_avg:46.83ms
step:1387/2160 train_time:64965ms step_avg:46.84ms
step:1388/2160 train_time:65024ms step_avg:46.85ms
step:1389/2160 train_time:65086ms step_avg:46.86ms
step:1390/2160 train_time:65146ms step_avg:46.87ms
step:1391/2160 train_time:65208ms step_avg:46.88ms
step:1392/2160 train_time:65267ms step_avg:46.89ms
step:1393/2160 train_time:65329ms step_avg:46.90ms
step:1394/2160 train_time:65389ms step_avg:46.91ms
step:1395/2160 train_time:65450ms step_avg:46.92ms
step:1396/2160 train_time:65509ms step_avg:46.93ms
step:1397/2160 train_time:65571ms step_avg:46.94ms
step:1398/2160 train_time:65631ms step_avg:46.95ms
step:1399/2160 train_time:65692ms step_avg:46.96ms
step:1400/2160 train_time:65751ms step_avg:46.97ms
step:1401/2160 train_time:65813ms step_avg:46.98ms
step:1402/2160 train_time:65873ms step_avg:46.98ms
step:1403/2160 train_time:65935ms step_avg:47.00ms
step:1404/2160 train_time:65994ms step_avg:47.00ms
step:1405/2160 train_time:66056ms step_avg:47.02ms
step:1406/2160 train_time:66116ms step_avg:47.02ms
step:1407/2160 train_time:66178ms step_avg:47.03ms
step:1408/2160 train_time:66239ms step_avg:47.04ms
step:1409/2160 train_time:66301ms step_avg:47.06ms
step:1410/2160 train_time:66362ms step_avg:47.07ms
step:1411/2160 train_time:66424ms step_avg:47.08ms
step:1412/2160 train_time:66483ms step_avg:47.08ms
step:1413/2160 train_time:66545ms step_avg:47.09ms
step:1414/2160 train_time:66604ms step_avg:47.10ms
step:1415/2160 train_time:66665ms step_avg:47.11ms
step:1416/2160 train_time:66753ms step_avg:47.14ms
step:1417/2160 train_time:66842ms step_avg:47.17ms
step:1418/2160 train_time:66929ms step_avg:47.20ms
step:1419/2160 train_time:67018ms step_avg:47.23ms
step:1420/2160 train_time:67105ms step_avg:47.26ms
step:1421/2160 train_time:67195ms step_avg:47.29ms
step:1422/2160 train_time:67283ms step_avg:47.32ms
step:1423/2160 train_time:67373ms step_avg:47.35ms
step:1424/2160 train_time:67460ms step_avg:47.37ms
step:1425/2160 train_time:67550ms step_avg:47.40ms
step:1426/2160 train_time:67637ms step_avg:47.43ms
step:1427/2160 train_time:67726ms step_avg:47.46ms
step:1428/2160 train_time:67813ms step_avg:47.49ms
step:1429/2160 train_time:67902ms step_avg:47.52ms
step:1430/2160 train_time:67989ms step_avg:47.55ms
step:1431/2160 train_time:68079ms step_avg:47.57ms
step:1432/2160 train_time:68166ms step_avg:47.60ms
step:1433/2160 train_time:68256ms step_avg:47.63ms
step:1434/2160 train_time:68344ms step_avg:47.66ms
step:1435/2160 train_time:68434ms step_avg:47.69ms
step:1436/2160 train_time:68522ms step_avg:47.72ms
step:1437/2160 train_time:68611ms step_avg:47.75ms
step:1438/2160 train_time:68699ms step_avg:47.77ms
step:1439/2160 train_time:68789ms step_avg:47.80ms
step:1440/2160 train_time:68875ms step_avg:47.83ms
step:1441/2160 train_time:68964ms step_avg:47.86ms
step:1442/2160 train_time:69052ms step_avg:47.89ms
step:1443/2160 train_time:69141ms step_avg:47.91ms
step:1444/2160 train_time:69229ms step_avg:47.94ms
step:1445/2160 train_time:69319ms step_avg:47.97ms
step:1446/2160 train_time:69407ms step_avg:48.00ms
step:1447/2160 train_time:69496ms step_avg:48.03ms
step:1448/2160 train_time:69583ms step_avg:48.05ms
step:1449/2160 train_time:69673ms step_avg:48.08ms
step:1450/2160 train_time:69759ms step_avg:48.11ms
step:1451/2160 train_time:69849ms step_avg:48.14ms
step:1452/2160 train_time:69936ms step_avg:48.17ms
step:1453/2160 train_time:70026ms step_avg:48.19ms
step:1454/2160 train_time:70113ms step_avg:48.22ms
step:1455/2160 train_time:70203ms step_avg:48.25ms
step:1456/2160 train_time:70291ms step_avg:48.28ms
step:1457/2160 train_time:70380ms step_avg:48.30ms
step:1458/2160 train_time:70468ms step_avg:48.33ms
step:1459/2160 train_time:70557ms step_avg:48.36ms
step:1460/2160 train_time:70644ms step_avg:48.39ms
step:1461/2160 train_time:70734ms step_avg:48.41ms
step:1462/2160 train_time:70821ms step_avg:48.44ms
step:1463/2160 train_time:70910ms step_avg:48.47ms
step:1464/2160 train_time:70997ms step_avg:48.50ms
step:1465/2160 train_time:71087ms step_avg:48.52ms
step:1466/2160 train_time:71175ms step_avg:48.55ms
step:1467/2160 train_time:71265ms step_avg:48.58ms
step:1468/2160 train_time:71353ms step_avg:48.61ms
step:1469/2160 train_time:71442ms step_avg:48.63ms
step:1470/2160 train_time:71529ms step_avg:48.66ms
step:1471/2160 train_time:71617ms step_avg:48.69ms
step:1472/2160 train_time:71705ms step_avg:48.71ms
step:1473/2160 train_time:71794ms step_avg:48.74ms
step:1474/2160 train_time:71881ms step_avg:48.77ms
step:1475/2160 train_time:71972ms step_avg:48.79ms
step:1476/2160 train_time:72058ms step_avg:48.82ms
step:1477/2160 train_time:72148ms step_avg:48.85ms
step:1478/2160 train_time:72236ms step_avg:48.87ms
step:1479/2160 train_time:72325ms step_avg:48.90ms
step:1480/2160 train_time:72412ms step_avg:48.93ms
step:1481/2160 train_time:72501ms step_avg:48.95ms
step:1482/2160 train_time:72589ms step_avg:48.98ms
step:1483/2160 train_time:72678ms step_avg:49.01ms
step:1484/2160 train_time:72766ms step_avg:49.03ms
step:1485/2160 train_time:72855ms step_avg:49.06ms
step:1486/2160 train_time:72943ms step_avg:49.09ms
step:1487/2160 train_time:73034ms step_avg:49.11ms
step:1488/2160 train_time:73122ms step_avg:49.14ms
step:1489/2160 train_time:73210ms step_avg:49.17ms
step:1490/2160 train_time:73297ms step_avg:49.19ms
step:1491/2160 train_time:73386ms step_avg:49.22ms
step:1492/2160 train_time:73475ms step_avg:49.25ms
step:1493/2160 train_time:73565ms step_avg:49.27ms
step:1494/2160 train_time:73652ms step_avg:49.30ms
step:1495/2160 train_time:73740ms step_avg:49.32ms
step:1496/2160 train_time:73827ms step_avg:49.35ms
step:1497/2160 train_time:73916ms step_avg:49.38ms
step:1498/2160 train_time:74004ms step_avg:49.40ms
step:1499/2160 train_time:74094ms step_avg:49.43ms
step:1500/2160 train_time:74181ms step_avg:49.45ms
step:1500/2160 val_loss:3.4742 train_time:74270ms step_avg:49.51ms
step:1501/2160 train_time:74294ms step_avg:49.50ms
step:1502/2160 train_time:74360ms step_avg:49.51ms
step:1503/2160 train_time:74454ms step_avg:49.54ms
step:1504/2160 train_time:74543ms step_avg:49.56ms
step:1505/2160 train_time:74632ms step_avg:49.59ms
step:1506/2160 train_time:74718ms step_avg:49.61ms
step:1507/2160 train_time:74806ms step_avg:49.64ms
step:1508/2160 train_time:74892ms step_avg:49.66ms
step:1509/2160 train_time:74980ms step_avg:49.69ms
step:1510/2160 train_time:75069ms step_avg:49.71ms
step:1511/2160 train_time:75158ms step_avg:49.74ms
step:1512/2160 train_time:75247ms step_avg:49.77ms
step:1513/2160 train_time:75338ms step_avg:49.79ms
step:1514/2160 train_time:75428ms step_avg:49.82ms
step:1515/2160 train_time:75518ms step_avg:49.85ms
step:1516/2160 train_time:75605ms step_avg:49.87ms
step:1517/2160 train_time:75694ms step_avg:49.90ms
step:1518/2160 train_time:75781ms step_avg:49.92ms
step:1519/2160 train_time:75869ms step_avg:49.95ms
step:1520/2160 train_time:75955ms step_avg:49.97ms
step:1521/2160 train_time:76045ms step_avg:50.00ms
step:1522/2160 train_time:76132ms step_avg:50.02ms
step:1523/2160 train_time:76223ms step_avg:50.05ms
step:1524/2160 train_time:76311ms step_avg:50.07ms
step:1525/2160 train_time:76405ms step_avg:50.10ms
step:1526/2160 train_time:76493ms step_avg:50.13ms
step:1527/2160 train_time:76581ms step_avg:50.15ms
step:1528/2160 train_time:76669ms step_avg:50.18ms
step:1529/2160 train_time:76758ms step_avg:50.20ms
step:1530/2160 train_time:76844ms step_avg:50.22ms
step:1531/2160 train_time:76932ms step_avg:50.25ms
step:1532/2160 train_time:77019ms step_avg:50.27ms
step:1533/2160 train_time:77109ms step_avg:50.30ms
step:1534/2160 train_time:77197ms step_avg:50.32ms
step:1535/2160 train_time:77287ms step_avg:50.35ms
step:1536/2160 train_time:77374ms step_avg:50.37ms
step:1537/2160 train_time:77465ms step_avg:50.40ms
step:1538/2160 train_time:77553ms step_avg:50.42ms
step:1539/2160 train_time:77641ms step_avg:50.45ms
step:1540/2160 train_time:77729ms step_avg:50.47ms
step:1541/2160 train_time:77817ms step_avg:50.50ms
step:1542/2160 train_time:77905ms step_avg:50.52ms
step:1543/2160 train_time:77993ms step_avg:50.55ms
step:1544/2160 train_time:78080ms step_avg:50.57ms
step:1545/2160 train_time:78171ms step_avg:50.60ms
step:1546/2160 train_time:78260ms step_avg:50.62ms
step:1547/2160 train_time:78351ms step_avg:50.65ms
step:1548/2160 train_time:78440ms step_avg:50.67ms
step:1549/2160 train_time:78531ms step_avg:50.70ms
step:1550/2160 train_time:78618ms step_avg:50.72ms
step:1551/2160 train_time:78706ms step_avg:50.75ms
step:1552/2160 train_time:78794ms step_avg:50.77ms
step:1553/2160 train_time:78882ms step_avg:50.79ms
step:1554/2160 train_time:78968ms step_avg:50.82ms
step:1555/2160 train_time:79057ms step_avg:50.84ms
step:1556/2160 train_time:79146ms step_avg:50.86ms
step:1557/2160 train_time:79236ms step_avg:50.89ms
step:1558/2160 train_time:79324ms step_avg:50.91ms
step:1559/2160 train_time:79414ms step_avg:50.94ms
step:1560/2160 train_time:79502ms step_avg:50.96ms
step:1561/2160 train_time:79590ms step_avg:50.99ms
step:1562/2160 train_time:79677ms step_avg:51.01ms
step:1563/2160 train_time:79766ms step_avg:51.03ms
step:1564/2160 train_time:79853ms step_avg:51.06ms
step:1565/2160 train_time:79941ms step_avg:51.08ms
step:1566/2160 train_time:80028ms step_avg:51.10ms
step:1567/2160 train_time:80117ms step_avg:51.13ms
step:1568/2160 train_time:80206ms step_avg:51.15ms
step:1569/2160 train_time:80294ms step_avg:51.18ms
step:1570/2160 train_time:80382ms step_avg:51.20ms
step:1571/2160 train_time:80473ms step_avg:51.22ms
step:1572/2160 train_time:80561ms step_avg:51.25ms
step:1573/2160 train_time:80651ms step_avg:51.27ms
step:1574/2160 train_time:80738ms step_avg:51.29ms
step:1575/2160 train_time:80827ms step_avg:51.32ms
step:1576/2160 train_time:80914ms step_avg:51.34ms
step:1577/2160 train_time:81003ms step_avg:51.37ms
step:1578/2160 train_time:81090ms step_avg:51.39ms
step:1579/2160 train_time:81178ms step_avg:51.41ms
step:1580/2160 train_time:81265ms step_avg:51.43ms
step:1581/2160 train_time:81355ms step_avg:51.46ms
step:1582/2160 train_time:81442ms step_avg:51.48ms
step:1583/2160 train_time:81532ms step_avg:51.50ms
step:1584/2160 train_time:81620ms step_avg:51.53ms
step:1585/2160 train_time:81711ms step_avg:51.55ms
step:1586/2160 train_time:81799ms step_avg:51.58ms
step:1587/2160 train_time:81888ms step_avg:51.60ms
step:1588/2160 train_time:81975ms step_avg:51.62ms
step:1589/2160 train_time:82064ms step_avg:51.65ms
step:1590/2160 train_time:82151ms step_avg:51.67ms
step:1591/2160 train_time:82241ms step_avg:51.69ms
step:1592/2160 train_time:82328ms step_avg:51.71ms
step:1593/2160 train_time:82417ms step_avg:51.74ms
step:1594/2160 train_time:82505ms step_avg:51.76ms
step:1595/2160 train_time:82594ms step_avg:51.78ms
step:1596/2160 train_time:82680ms step_avg:51.80ms
step:1597/2160 train_time:82770ms step_avg:51.83ms
step:1598/2160 train_time:82857ms step_avg:51.85ms
step:1599/2160 train_time:82947ms step_avg:51.87ms
step:1600/2160 train_time:83034ms step_avg:51.90ms
step:1601/2160 train_time:83123ms step_avg:51.92ms
step:1602/2160 train_time:83210ms step_avg:51.94ms
step:1603/2160 train_time:83299ms step_avg:51.96ms
step:1604/2160 train_time:83387ms step_avg:51.99ms
step:1605/2160 train_time:83476ms step_avg:52.01ms
step:1606/2160 train_time:83563ms step_avg:52.03ms
step:1607/2160 train_time:83652ms step_avg:52.05ms
step:1608/2160 train_time:83739ms step_avg:52.08ms
step:1609/2160 train_time:83829ms step_avg:52.10ms
step:1610/2160 train_time:83916ms step_avg:52.12ms
step:1611/2160 train_time:84005ms step_avg:52.14ms
step:1612/2160 train_time:84092ms step_avg:52.17ms
step:1613/2160 train_time:84182ms step_avg:52.19ms
step:1614/2160 train_time:84270ms step_avg:52.21ms
step:1615/2160 train_time:84358ms step_avg:52.23ms
step:1616/2160 train_time:84445ms step_avg:52.26ms
step:1617/2160 train_time:84534ms step_avg:52.28ms
step:1618/2160 train_time:84622ms step_avg:52.30ms
step:1619/2160 train_time:84710ms step_avg:52.32ms
step:1620/2160 train_time:84798ms step_avg:52.34ms
step:1621/2160 train_time:84887ms step_avg:52.37ms
step:1622/2160 train_time:84974ms step_avg:52.39ms
step:1623/2160 train_time:85063ms step_avg:52.41ms
step:1624/2160 train_time:85150ms step_avg:52.43ms
step:1625/2160 train_time:85240ms step_avg:52.46ms
step:1626/2160 train_time:85328ms step_avg:52.48ms
step:1627/2160 train_time:85417ms step_avg:52.50ms
step:1628/2160 train_time:85504ms step_avg:52.52ms
step:1629/2160 train_time:85593ms step_avg:52.54ms
step:1630/2160 train_time:85682ms step_avg:52.57ms
step:1631/2160 train_time:85773ms step_avg:52.59ms
step:1632/2160 train_time:85860ms step_avg:52.61ms
step:1633/2160 train_time:85951ms step_avg:52.63ms
step:1634/2160 train_time:86038ms step_avg:52.66ms
step:1635/2160 train_time:86129ms step_avg:52.68ms
step:1636/2160 train_time:86216ms step_avg:52.70ms
step:1637/2160 train_time:86306ms step_avg:52.72ms
step:1638/2160 train_time:86393ms step_avg:52.74ms
step:1639/2160 train_time:86482ms step_avg:52.76ms
step:1640/2160 train_time:86569ms step_avg:52.79ms
step:1641/2160 train_time:86658ms step_avg:52.81ms
step:1642/2160 train_time:86745ms step_avg:52.83ms
step:1643/2160 train_time:86834ms step_avg:52.85ms
step:1644/2160 train_time:86922ms step_avg:52.87ms
step:1645/2160 train_time:87011ms step_avg:52.89ms
step:1646/2160 train_time:87098ms step_avg:52.91ms
step:1647/2160 train_time:87187ms step_avg:52.94ms
step:1648/2160 train_time:87275ms step_avg:52.96ms
step:1649/2160 train_time:87365ms step_avg:52.98ms
step:1650/2160 train_time:87451ms step_avg:53.00ms
step:1651/2160 train_time:87540ms step_avg:53.02ms
step:1652/2160 train_time:87629ms step_avg:53.04ms
step:1653/2160 train_time:87718ms step_avg:53.07ms
step:1654/2160 train_time:87805ms step_avg:53.09ms
step:1655/2160 train_time:87894ms step_avg:53.11ms
step:1656/2160 train_time:87981ms step_avg:53.13ms
step:1657/2160 train_time:88072ms step_avg:53.15ms
step:1658/2160 train_time:88159ms step_avg:53.17ms
step:1659/2160 train_time:88248ms step_avg:53.19ms
step:1660/2160 train_time:88335ms step_avg:53.21ms
step:1661/2160 train_time:88425ms step_avg:53.24ms
step:1662/2160 train_time:88511ms step_avg:53.26ms
step:1663/2160 train_time:88601ms step_avg:53.28ms
step:1664/2160 train_time:88688ms step_avg:53.30ms
step:1665/2160 train_time:88777ms step_avg:53.32ms
step:1666/2160 train_time:88866ms step_avg:53.34ms
step:1667/2160 train_time:88955ms step_avg:53.36ms
step:1668/2160 train_time:89043ms step_avg:53.38ms
step:1669/2160 train_time:89132ms step_avg:53.40ms
step:1670/2160 train_time:89220ms step_avg:53.43ms
step:1671/2160 train_time:89311ms step_avg:53.45ms
step:1672/2160 train_time:89397ms step_avg:53.47ms
step:1673/2160 train_time:89486ms step_avg:53.49ms
step:1674/2160 train_time:89572ms step_avg:53.51ms
step:1675/2160 train_time:89661ms step_avg:53.53ms
step:1676/2160 train_time:89748ms step_avg:53.55ms
step:1677/2160 train_time:89838ms step_avg:53.57ms
step:1678/2160 train_time:89925ms step_avg:53.59ms
step:1679/2160 train_time:90015ms step_avg:53.61ms
step:1680/2160 train_time:90102ms step_avg:53.63ms
step:1681/2160 train_time:90191ms step_avg:53.65ms
step:1682/2160 train_time:90278ms step_avg:53.67ms
step:1683/2160 train_time:90368ms step_avg:53.69ms
step:1684/2160 train_time:90455ms step_avg:53.71ms
step:1685/2160 train_time:90544ms step_avg:53.74ms
step:1686/2160 train_time:90631ms step_avg:53.75ms
step:1687/2160 train_time:90720ms step_avg:53.78ms
step:1688/2160 train_time:90808ms step_avg:53.80ms
step:1689/2160 train_time:90897ms step_avg:53.82ms
step:1690/2160 train_time:90984ms step_avg:53.84ms
step:1691/2160 train_time:91073ms step_avg:53.86ms
step:1692/2160 train_time:91160ms step_avg:53.88ms
step:1693/2160 train_time:91249ms step_avg:53.90ms
step:1694/2160 train_time:91337ms step_avg:53.92ms
step:1695/2160 train_time:91428ms step_avg:53.94ms
step:1696/2160 train_time:91514ms step_avg:53.96ms
step:1697/2160 train_time:91605ms step_avg:53.98ms
step:1698/2160 train_time:91692ms step_avg:54.00ms
step:1699/2160 train_time:91781ms step_avg:54.02ms
step:1700/2160 train_time:91868ms step_avg:54.04ms
step:1701/2160 train_time:91957ms step_avg:54.06ms
step:1702/2160 train_time:92046ms step_avg:54.08ms
step:1703/2160 train_time:92136ms step_avg:54.10ms
step:1704/2160 train_time:92223ms step_avg:54.12ms
step:1705/2160 train_time:92311ms step_avg:54.14ms
step:1706/2160 train_time:92398ms step_avg:54.16ms
step:1707/2160 train_time:92487ms step_avg:54.18ms
step:1708/2160 train_time:92574ms step_avg:54.20ms
step:1709/2160 train_time:92664ms step_avg:54.22ms
step:1710/2160 train_time:92750ms step_avg:54.24ms
step:1711/2160 train_time:92840ms step_avg:54.26ms
step:1712/2160 train_time:92927ms step_avg:54.28ms
step:1713/2160 train_time:93016ms step_avg:54.30ms
step:1714/2160 train_time:93105ms step_avg:54.32ms
step:1715/2160 train_time:93193ms step_avg:54.34ms
step:1716/2160 train_time:93280ms step_avg:54.36ms
step:1717/2160 train_time:93369ms step_avg:54.38ms
step:1718/2160 train_time:93456ms step_avg:54.40ms
step:1719/2160 train_time:93546ms step_avg:54.42ms
step:1720/2160 train_time:93632ms step_avg:54.44ms
step:1721/2160 train_time:93722ms step_avg:54.46ms
step:1722/2160 train_time:93809ms step_avg:54.48ms
step:1723/2160 train_time:93899ms step_avg:54.50ms
step:1724/2160 train_time:93986ms step_avg:54.52ms
step:1725/2160 train_time:94075ms step_avg:54.54ms
step:1726/2160 train_time:94162ms step_avg:54.55ms
step:1727/2160 train_time:94250ms step_avg:54.57ms
step:1728/2160 train_time:94338ms step_avg:54.59ms
step:1729/2160 train_time:94427ms step_avg:54.61ms
step:1730/2160 train_time:94514ms step_avg:54.63ms
step:1731/2160 train_time:94604ms step_avg:54.65ms
step:1732/2160 train_time:94691ms step_avg:54.67ms
step:1733/2160 train_time:94780ms step_avg:54.69ms
step:1734/2160 train_time:94867ms step_avg:54.71ms
step:1735/2160 train_time:94957ms step_avg:54.73ms
step:1736/2160 train_time:95045ms step_avg:54.75ms
step:1737/2160 train_time:95134ms step_avg:54.77ms
step:1738/2160 train_time:95222ms step_avg:54.79ms
step:1739/2160 train_time:95310ms step_avg:54.81ms
step:1740/2160 train_time:95398ms step_avg:54.83ms
step:1741/2160 train_time:95488ms step_avg:54.85ms
step:1742/2160 train_time:95575ms step_avg:54.87ms
step:1743/2160 train_time:95665ms step_avg:54.89ms
step:1744/2160 train_time:95752ms step_avg:54.90ms
step:1745/2160 train_time:95841ms step_avg:54.92ms
step:1746/2160 train_time:95929ms step_avg:54.94ms
step:1747/2160 train_time:96018ms step_avg:54.96ms
step:1748/2160 train_time:96106ms step_avg:54.98ms
step:1749/2160 train_time:96194ms step_avg:55.00ms
step:1750/2160 train_time:96281ms step_avg:55.02ms
step:1750/2160 val_loss:3.3805 train_time:96371ms step_avg:55.07ms
step:1751/2160 train_time:96395ms step_avg:55.05ms
step:1752/2160 train_time:96463ms step_avg:55.06ms
step:1753/2160 train_time:96556ms step_avg:55.08ms
step:1754/2160 train_time:96644ms step_avg:55.10ms
step:1755/2160 train_time:96733ms step_avg:55.12ms
step:1756/2160 train_time:96819ms step_avg:55.14ms
step:1757/2160 train_time:96907ms step_avg:55.15ms
step:1758/2160 train_time:96993ms step_avg:55.17ms
step:1759/2160 train_time:97081ms step_avg:55.19ms
step:1760/2160 train_time:97168ms step_avg:55.21ms
step:1761/2160 train_time:97256ms step_avg:55.23ms
step:1762/2160 train_time:97343ms step_avg:55.25ms
step:1763/2160 train_time:97435ms step_avg:55.27ms
step:1764/2160 train_time:97524ms step_avg:55.29ms
step:1765/2160 train_time:97613ms step_avg:55.30ms
step:1766/2160 train_time:97700ms step_avg:55.32ms
step:1767/2160 train_time:97788ms step_avg:55.34ms
step:1768/2160 train_time:97875ms step_avg:55.36ms
step:1769/2160 train_time:97964ms step_avg:55.38ms
step:1770/2160 train_time:98050ms step_avg:55.40ms
step:1771/2160 train_time:98140ms step_avg:55.41ms
step:1772/2160 train_time:98226ms step_avg:55.43ms
step:1773/2160 train_time:98315ms step_avg:55.45ms
step:1774/2160 train_time:98405ms step_avg:55.47ms
step:1775/2160 train_time:98496ms step_avg:55.49ms
step:1776/2160 train_time:98585ms step_avg:55.51ms
step:1777/2160 train_time:98675ms step_avg:55.53ms
step:1778/2160 train_time:98763ms step_avg:55.55ms
step:1779/2160 train_time:98850ms step_avg:55.57ms
step:1780/2160 train_time:98936ms step_avg:55.58ms
step:1781/2160 train_time:99025ms step_avg:55.60ms
step:1782/2160 train_time:99112ms step_avg:55.62ms
step:1783/2160 train_time:99202ms step_avg:55.64ms
step:1784/2160 train_time:99288ms step_avg:55.65ms
step:1785/2160 train_time:99377ms step_avg:55.67ms
step:1786/2160 train_time:99467ms step_avg:55.69ms
step:1787/2160 train_time:99557ms step_avg:55.71ms
step:1788/2160 train_time:99645ms step_avg:55.73ms
step:1789/2160 train_time:99733ms step_avg:55.75ms
step:1790/2160 train_time:99820ms step_avg:55.77ms
step:1791/2160 train_time:99908ms step_avg:55.78ms
step:1792/2160 train_time:99994ms step_avg:55.80ms
step:1793/2160 train_time:100083ms step_avg:55.82ms
step:1794/2160 train_time:100170ms step_avg:55.84ms
step:1795/2160 train_time:100258ms step_avg:55.85ms
step:1796/2160 train_time:100345ms step_avg:55.87ms
step:1797/2160 train_time:100435ms step_avg:55.89ms
step:1798/2160 train_time:100523ms step_avg:55.91ms
step:1799/2160 train_time:100611ms step_avg:55.93ms
step:1800/2160 train_time:100700ms step_avg:55.94ms
step:1801/2160 train_time:100789ms step_avg:55.96ms
step:1802/2160 train_time:100876ms step_avg:55.98ms
step:1803/2160 train_time:100964ms step_avg:56.00ms
step:1804/2160 train_time:101050ms step_avg:56.01ms
step:1805/2160 train_time:101140ms step_avg:56.03ms
step:1806/2160 train_time:101227ms step_avg:56.05ms
step:1807/2160 train_time:101316ms step_avg:56.07ms
step:1808/2160 train_time:101404ms step_avg:56.09ms
step:1809/2160 train_time:101495ms step_avg:56.11ms
step:1810/2160 train_time:101583ms step_avg:56.12ms
step:1811/2160 train_time:101672ms step_avg:56.14ms
step:1812/2160 train_time:101760ms step_avg:56.16ms
step:1813/2160 train_time:101848ms step_avg:56.18ms
step:1814/2160 train_time:101935ms step_avg:56.19ms
step:1815/2160 train_time:102024ms step_avg:56.21ms
step:1816/2160 train_time:102111ms step_avg:56.23ms
step:1817/2160 train_time:102199ms step_avg:56.25ms
step:1818/2160 train_time:102287ms step_avg:56.26ms
step:1819/2160 train_time:102376ms step_avg:56.28ms
step:1820/2160 train_time:102463ms step_avg:56.30ms
step:1821/2160 train_time:102553ms step_avg:56.32ms
step:1822/2160 train_time:102641ms step_avg:56.33ms
step:1823/2160 train_time:102730ms step_avg:56.35ms
step:1824/2160 train_time:102818ms step_avg:56.37ms
step:1825/2160 train_time:102908ms step_avg:56.39ms
step:1826/2160 train_time:102995ms step_avg:56.40ms
step:1827/2160 train_time:103084ms step_avg:56.42ms
step:1828/2160 train_time:103170ms step_avg:56.44ms
step:1829/2160 train_time:103260ms step_avg:56.46ms
step:1830/2160 train_time:103346ms step_avg:56.47ms
step:1831/2160 train_time:103435ms step_avg:56.49ms
step:1832/2160 train_time:103523ms step_avg:56.51ms
step:1833/2160 train_time:103612ms step_avg:56.53ms
step:1834/2160 train_time:103700ms step_avg:56.54ms
step:1835/2160 train_time:103789ms step_avg:56.56ms
step:1836/2160 train_time:103877ms step_avg:56.58ms
step:1837/2160 train_time:103967ms step_avg:56.60ms
step:1838/2160 train_time:104054ms step_avg:56.61ms
step:1839/2160 train_time:104143ms step_avg:56.63ms
step:1840/2160 train_time:104230ms step_avg:56.65ms
step:1841/2160 train_time:104318ms step_avg:56.66ms
step:1842/2160 train_time:104406ms step_avg:56.68ms
step:1843/2160 train_time:104496ms step_avg:56.70ms
step:1844/2160 train_time:104583ms step_avg:56.72ms
step:1845/2160 train_time:104672ms step_avg:56.73ms
step:1846/2160 train_time:104760ms step_avg:56.75ms
step:1847/2160 train_time:104849ms step_avg:56.77ms
step:1848/2160 train_time:104937ms step_avg:56.78ms
step:1849/2160 train_time:105027ms step_avg:56.80ms
step:1850/2160 train_time:105113ms step_avg:56.82ms
step:1851/2160 train_time:105203ms step_avg:56.84ms
step:1852/2160 train_time:105291ms step_avg:56.85ms
step:1853/2160 train_time:105380ms step_avg:56.87ms
step:1854/2160 train_time:105468ms step_avg:56.89ms
step:1855/2160 train_time:105556ms step_avg:56.90ms
step:1856/2160 train_time:105644ms step_avg:56.92ms
step:1857/2160 train_time:105733ms step_avg:56.94ms
step:1858/2160 train_time:105822ms step_avg:56.95ms
step:1859/2160 train_time:105910ms step_avg:56.97ms
step:1860/2160 train_time:105999ms step_avg:56.99ms
step:1861/2160 train_time:106087ms step_avg:57.01ms
step:1862/2160 train_time:106174ms step_avg:57.02ms
step:1863/2160 train_time:106264ms step_avg:57.04ms
step:1864/2160 train_time:106350ms step_avg:57.05ms
step:1865/2160 train_time:106440ms step_avg:57.07ms
step:1866/2160 train_time:106526ms step_avg:57.09ms
step:1867/2160 train_time:106615ms step_avg:57.10ms
step:1868/2160 train_time:106703ms step_avg:57.12ms
step:1869/2160 train_time:106792ms step_avg:57.14ms
step:1870/2160 train_time:106879ms step_avg:57.15ms
step:1871/2160 train_time:106969ms step_avg:57.17ms
step:1872/2160 train_time:107057ms step_avg:57.19ms
step:1873/2160 train_time:107146ms step_avg:57.21ms
step:1874/2160 train_time:107234ms step_avg:57.22ms
step:1875/2160 train_time:107323ms step_avg:57.24ms
step:1876/2160 train_time:107409ms step_avg:57.25ms
step:1877/2160 train_time:107500ms step_avg:57.27ms
step:1878/2160 train_time:107586ms step_avg:57.29ms
step:1879/2160 train_time:107676ms step_avg:57.30ms
step:1880/2160 train_time:107763ms step_avg:57.32ms
step:1881/2160 train_time:107852ms step_avg:57.34ms
step:1882/2160 train_time:107939ms step_avg:57.35ms
step:1883/2160 train_time:108028ms step_avg:57.37ms
step:1884/2160 train_time:108115ms step_avg:57.39ms
step:1885/2160 train_time:108205ms step_avg:57.40ms
step:1886/2160 train_time:108291ms step_avg:57.42ms
step:1887/2160 train_time:108380ms step_avg:57.44ms
step:1888/2160 train_time:108467ms step_avg:57.45ms
step:1889/2160 train_time:108558ms step_avg:57.47ms
step:1890/2160 train_time:108646ms step_avg:57.48ms
step:1891/2160 train_time:108736ms step_avg:57.50ms
step:1892/2160 train_time:108823ms step_avg:57.52ms
step:1893/2160 train_time:108911ms step_avg:57.53ms
step:1894/2160 train_time:108998ms step_avg:57.55ms
step:1895/2160 train_time:109088ms step_avg:57.57ms
step:1896/2160 train_time:109175ms step_avg:57.58ms
step:1897/2160 train_time:109265ms step_avg:57.60ms
step:1898/2160 train_time:109350ms step_avg:57.61ms
step:1899/2160 train_time:109439ms step_avg:57.63ms
step:1900/2160 train_time:109526ms step_avg:57.65ms
step:1901/2160 train_time:109615ms step_avg:57.66ms
step:1902/2160 train_time:109702ms step_avg:57.68ms
step:1903/2160 train_time:109791ms step_avg:57.69ms
step:1904/2160 train_time:109878ms step_avg:57.71ms
step:1905/2160 train_time:109968ms step_avg:57.73ms
step:1906/2160 train_time:110056ms step_avg:57.74ms
step:1907/2160 train_time:110145ms step_avg:57.76ms
step:1908/2160 train_time:110232ms step_avg:57.77ms
step:1909/2160 train_time:110320ms step_avg:57.79ms
step:1910/2160 train_time:110407ms step_avg:57.80ms
step:1911/2160 train_time:110497ms step_avg:57.82ms
step:1912/2160 train_time:110585ms step_avg:57.84ms
step:1913/2160 train_time:110674ms step_avg:57.85ms
step:1914/2160 train_time:110762ms step_avg:57.87ms
step:1915/2160 train_time:110850ms step_avg:57.89ms
step:1916/2160 train_time:110937ms step_avg:57.90ms
step:1917/2160 train_time:111028ms step_avg:57.92ms
step:1918/2160 train_time:111115ms step_avg:57.93ms
step:1919/2160 train_time:111206ms step_avg:57.95ms
step:1920/2160 train_time:111292ms step_avg:57.96ms
step:1921/2160 train_time:111382ms step_avg:57.98ms
step:1922/2160 train_time:111467ms step_avg:58.00ms
step:1923/2160 train_time:111557ms step_avg:58.01ms
step:1924/2160 train_time:111644ms step_avg:58.03ms
step:1925/2160 train_time:111733ms step_avg:58.04ms
step:1926/2160 train_time:111820ms step_avg:58.06ms
step:1927/2160 train_time:111910ms step_avg:58.07ms
step:1928/2160 train_time:111997ms step_avg:58.09ms
step:1929/2160 train_time:112087ms step_avg:58.11ms
step:1930/2160 train_time:112175ms step_avg:58.12ms
step:1931/2160 train_time:112264ms step_avg:58.14ms
step:1932/2160 train_time:112350ms step_avg:58.15ms
step:1933/2160 train_time:112440ms step_avg:58.17ms
step:1934/2160 train_time:112527ms step_avg:58.18ms
step:1935/2160 train_time:112615ms step_avg:58.20ms
step:1936/2160 train_time:112703ms step_avg:58.21ms
step:1937/2160 train_time:112791ms step_avg:58.23ms
step:1938/2160 train_time:112879ms step_avg:58.24ms
step:1939/2160 train_time:112968ms step_avg:58.26ms
step:1940/2160 train_time:113055ms step_avg:58.28ms
step:1941/2160 train_time:113145ms step_avg:58.29ms
step:1942/2160 train_time:113231ms step_avg:58.31ms
step:1943/2160 train_time:113320ms step_avg:58.32ms
step:1944/2160 train_time:113408ms step_avg:58.34ms
step:1945/2160 train_time:113497ms step_avg:58.35ms
step:1946/2160 train_time:113584ms step_avg:58.37ms
step:1947/2160 train_time:113673ms step_avg:58.38ms
step:1948/2160 train_time:113760ms step_avg:58.40ms
step:1949/2160 train_time:113849ms step_avg:58.41ms
step:1950/2160 train_time:113937ms step_avg:58.43ms
step:1951/2160 train_time:114026ms step_avg:58.44ms
step:1952/2160 train_time:114113ms step_avg:58.46ms
step:1953/2160 train_time:114202ms step_avg:58.47ms
step:1954/2160 train_time:114288ms step_avg:58.49ms
step:1955/2160 train_time:114377ms step_avg:58.51ms
step:1956/2160 train_time:114465ms step_avg:58.52ms
step:1957/2160 train_time:114555ms step_avg:58.54ms
step:1958/2160 train_time:114642ms step_avg:58.55ms
step:1959/2160 train_time:114730ms step_avg:58.57ms
step:1960/2160 train_time:114818ms step_avg:58.58ms
step:1961/2160 train_time:114907ms step_avg:58.60ms
step:1962/2160 train_time:114995ms step_avg:58.61ms
step:1963/2160 train_time:115083ms step_avg:58.63ms
step:1964/2160 train_time:115170ms step_avg:58.64ms
step:1965/2160 train_time:115260ms step_avg:58.66ms
step:1966/2160 train_time:115346ms step_avg:58.67ms
step:1967/2160 train_time:115435ms step_avg:58.69ms
step:1968/2160 train_time:115523ms step_avg:58.70ms
step:1969/2160 train_time:115611ms step_avg:58.72ms
step:1970/2160 train_time:115699ms step_avg:58.73ms
step:1971/2160 train_time:115788ms step_avg:58.75ms
step:1972/2160 train_time:115875ms step_avg:58.76ms
step:1973/2160 train_time:115966ms step_avg:58.78ms
step:1974/2160 train_time:116052ms step_avg:58.79ms
step:1975/2160 train_time:116143ms step_avg:58.81ms
step:1976/2160 train_time:116229ms step_avg:58.82ms
step:1977/2160 train_time:116319ms step_avg:58.84ms
step:1978/2160 train_time:116406ms step_avg:58.85ms
step:1979/2160 train_time:116495ms step_avg:58.87ms
step:1980/2160 train_time:116582ms step_avg:58.88ms
step:1981/2160 train_time:116671ms step_avg:58.89ms
step:1982/2160 train_time:116758ms step_avg:58.91ms
step:1983/2160 train_time:116848ms step_avg:58.92ms
step:1984/2160 train_time:116935ms step_avg:58.94ms
step:1985/2160 train_time:117024ms step_avg:58.95ms
step:1986/2160 train_time:117112ms step_avg:58.97ms
step:1987/2160 train_time:117201ms step_avg:58.98ms
step:1988/2160 train_time:117288ms step_avg:59.00ms
step:1989/2160 train_time:117376ms step_avg:59.01ms
step:1990/2160 train_time:117464ms step_avg:59.03ms
step:1991/2160 train_time:117552ms step_avg:59.04ms
step:1992/2160 train_time:117639ms step_avg:59.06ms
step:1993/2160 train_time:117728ms step_avg:59.07ms
step:1994/2160 train_time:117816ms step_avg:59.09ms
step:1995/2160 train_time:117905ms step_avg:59.10ms
step:1996/2160 train_time:117992ms step_avg:59.11ms
step:1997/2160 train_time:118081ms step_avg:59.13ms
step:1998/2160 train_time:118167ms step_avg:59.14ms
step:1999/2160 train_time:118256ms step_avg:59.16ms
step:2000/2160 train_time:118344ms step_avg:59.17ms
step:2000/2160 val_loss:3.3116 train_time:118433ms step_avg:59.22ms
step:2001/2160 train_time:118457ms step_avg:59.20ms
step:2002/2160 train_time:118526ms step_avg:59.20ms
step:2003/2160 train_time:118620ms step_avg:59.22ms
step:2004/2160 train_time:118707ms step_avg:59.24ms
step:2005/2160 train_time:118796ms step_avg:59.25ms
step:2006/2160 train_time:118882ms step_avg:59.26ms
step:2007/2160 train_time:118970ms step_avg:59.28ms
step:2008/2160 train_time:119056ms step_avg:59.29ms
step:2009/2160 train_time:119143ms step_avg:59.30ms
step:2010/2160 train_time:119229ms step_avg:59.32ms
step:2011/2160 train_time:119317ms step_avg:59.33ms
step:2012/2160 train_time:119405ms step_avg:59.35ms
step:2013/2160 train_time:119498ms step_avg:59.36ms
step:2014/2160 train_time:119586ms step_avg:59.38ms
step:2015/2160 train_time:119677ms step_avg:59.39ms
step:2016/2160 train_time:119763ms step_avg:59.41ms
step:2017/2160 train_time:119852ms step_avg:59.42ms
step:2018/2160 train_time:119938ms step_avg:59.43ms
step:2019/2160 train_time:120026ms step_avg:59.45ms
step:2020/2160 train_time:120112ms step_avg:59.46ms
step:2021/2160 train_time:120201ms step_avg:59.48ms
step:2022/2160 train_time:120288ms step_avg:59.49ms
step:2023/2160 train_time:120377ms step_avg:59.50ms
step:2024/2160 train_time:120465ms step_avg:59.52ms
step:2025/2160 train_time:120558ms step_avg:59.53ms
step:2026/2160 train_time:120645ms step_avg:59.55ms
step:2027/2160 train_time:120735ms step_avg:59.56ms
step:2028/2160 train_time:120821ms step_avg:59.58ms
step:2029/2160 train_time:120909ms step_avg:59.59ms
step:2030/2160 train_time:120996ms step_avg:59.60ms
step:2031/2160 train_time:121084ms step_avg:59.62ms
step:2032/2160 train_time:121170ms step_avg:59.63ms
step:2033/2160 train_time:121258ms step_avg:59.64ms
step:2034/2160 train_time:121345ms step_avg:59.66ms
step:2035/2160 train_time:121436ms step_avg:59.67ms
step:2036/2160 train_time:121524ms step_avg:59.69ms
step:2037/2160 train_time:121614ms step_avg:59.70ms
step:2038/2160 train_time:121702ms step_avg:59.72ms
step:2039/2160 train_time:121791ms step_avg:59.73ms
step:2040/2160 train_time:121878ms step_avg:59.74ms
step:2041/2160 train_time:121966ms step_avg:59.76ms
step:2042/2160 train_time:122052ms step_avg:59.77ms
step:2043/2160 train_time:122141ms step_avg:59.79ms
step:2044/2160 train_time:122227ms step_avg:59.80ms
step:2045/2160 train_time:122316ms step_avg:59.81ms
step:2046/2160 train_time:122404ms step_avg:59.83ms
step:2047/2160 train_time:122494ms step_avg:59.84ms
step:2048/2160 train_time:122582ms step_avg:59.85ms
step:2049/2160 train_time:122673ms step_avg:59.87ms
step:2050/2160 train_time:122762ms step_avg:59.88ms
step:2051/2160 train_time:122850ms step_avg:59.90ms
step:2052/2160 train_time:122937ms step_avg:59.91ms
step:2053/2160 train_time:123025ms step_avg:59.92ms
step:2054/2160 train_time:123112ms step_avg:59.94ms
step:2055/2160 train_time:123201ms step_avg:59.95ms
step:2056/2160 train_time:123287ms step_avg:59.96ms
step:2057/2160 train_time:123377ms step_avg:59.98ms
step:2058/2160 train_time:123463ms step_avg:59.99ms
step:2059/2160 train_time:123553ms step_avg:60.01ms
step:2060/2160 train_time:123642ms step_avg:60.02ms
step:2061/2160 train_time:123732ms step_avg:60.03ms
step:2062/2160 train_time:123820ms step_avg:60.05ms
step:2063/2160 train_time:123908ms step_avg:60.06ms
step:2064/2160 train_time:123995ms step_avg:60.07ms
step:2065/2160 train_time:124084ms step_avg:60.09ms
step:2066/2160 train_time:124171ms step_avg:60.10ms
step:2067/2160 train_time:124261ms step_avg:60.12ms
step:2068/2160 train_time:124347ms step_avg:60.13ms
step:2069/2160 train_time:124436ms step_avg:60.14ms
step:2070/2160 train_time:124523ms step_avg:60.16ms
step:2071/2160 train_time:124613ms step_avg:60.17ms
step:2072/2160 train_time:124702ms step_avg:60.18ms
step:2073/2160 train_time:124791ms step_avg:60.20ms
step:2074/2160 train_time:124878ms step_avg:60.21ms
step:2075/2160 train_time:124967ms step_avg:60.23ms
step:2076/2160 train_time:125054ms step_avg:60.24ms
step:2077/2160 train_time:125144ms step_avg:60.25ms
step:2078/2160 train_time:125232ms step_avg:60.27ms
step:2079/2160 train_time:125322ms step_avg:60.28ms
step:2080/2160 train_time:125408ms step_avg:60.29ms
step:2081/2160 train_time:125497ms step_avg:60.31ms
step:2082/2160 train_time:125584ms step_avg:60.32ms
step:2083/2160 train_time:125674ms step_avg:60.33ms
step:2084/2160 train_time:125761ms step_avg:60.35ms
step:2085/2160 train_time:125849ms step_avg:60.36ms
step:2086/2160 train_time:125936ms step_avg:60.37ms
step:2087/2160 train_time:126024ms step_avg:60.39ms
step:2088/2160 train_time:126111ms step_avg:60.40ms
step:2089/2160 train_time:126200ms step_avg:60.41ms
step:2090/2160 train_time:126287ms step_avg:60.42ms
step:2091/2160 train_time:126376ms step_avg:60.44ms
step:2092/2160 train_time:126463ms step_avg:60.45ms
step:2093/2160 train_time:126552ms step_avg:60.46ms
step:2094/2160 train_time:126641ms step_avg:60.48ms
step:2095/2160 train_time:126730ms step_avg:60.49ms
step:2096/2160 train_time:126817ms step_avg:60.50ms
step:2097/2160 train_time:126905ms step_avg:60.52ms
step:2098/2160 train_time:126992ms step_avg:60.53ms
step:2099/2160 train_time:127081ms step_avg:60.54ms
step:2100/2160 train_time:127168ms step_avg:60.56ms
step:2101/2160 train_time:127257ms step_avg:60.57ms
step:2102/2160 train_time:127343ms step_avg:60.58ms
step:2103/2160 train_time:127432ms step_avg:60.60ms
step:2104/2160 train_time:127519ms step_avg:60.61ms
step:2105/2160 train_time:127609ms step_avg:60.62ms
step:2106/2160 train_time:127696ms step_avg:60.63ms
step:2107/2160 train_time:127785ms step_avg:60.65ms
step:2108/2160 train_time:127873ms step_avg:60.66ms
step:2109/2160 train_time:127963ms step_avg:60.67ms
step:2110/2160 train_time:128050ms step_avg:60.69ms
step:2111/2160 train_time:128138ms step_avg:60.70ms
step:2112/2160 train_time:128226ms step_avg:60.71ms
step:2113/2160 train_time:128314ms step_avg:60.73ms
step:2114/2160 train_time:128401ms step_avg:60.74ms
step:2115/2160 train_time:128490ms step_avg:60.75ms
step:2116/2160 train_time:128578ms step_avg:60.76ms
step:2117/2160 train_time:128666ms step_avg:60.78ms
step:2118/2160 train_time:128753ms step_avg:60.79ms
step:2119/2160 train_time:128842ms step_avg:60.80ms
step:2120/2160 train_time:128929ms step_avg:60.82ms
step:2121/2160 train_time:129019ms step_avg:60.83ms
step:2122/2160 train_time:129106ms step_avg:60.84ms
step:2123/2160 train_time:129194ms step_avg:60.85ms
step:2124/2160 train_time:129280ms step_avg:60.87ms
step:2125/2160 train_time:129370ms step_avg:60.88ms
step:2126/2160 train_time:129457ms step_avg:60.89ms
step:2127/2160 train_time:129547ms step_avg:60.91ms
step:2128/2160 train_time:129635ms step_avg:60.92ms
step:2129/2160 train_time:129725ms step_avg:60.93ms
step:2130/2160 train_time:129812ms step_avg:60.94ms
step:2131/2160 train_time:129902ms step_avg:60.96ms
step:2132/2160 train_time:129989ms step_avg:60.97ms
step:2133/2160 train_time:130078ms step_avg:60.98ms
step:2134/2160 train_time:130165ms step_avg:61.00ms
step:2135/2160 train_time:130254ms step_avg:61.01ms
step:2136/2160 train_time:130341ms step_avg:61.02ms
step:2137/2160 train_time:130430ms step_avg:61.03ms
step:2138/2160 train_time:130518ms step_avg:61.05ms
step:2139/2160 train_time:130607ms step_avg:61.06ms
step:2140/2160 train_time:130694ms step_avg:61.07ms
step:2141/2160 train_time:130784ms step_avg:61.09ms
step:2142/2160 train_time:130871ms step_avg:61.10ms
step:2143/2160 train_time:130961ms step_avg:61.11ms
step:2144/2160 train_time:131048ms step_avg:61.12ms
step:2145/2160 train_time:131139ms step_avg:61.14ms
step:2146/2160 train_time:131225ms step_avg:61.15ms
step:2147/2160 train_time:131315ms step_avg:61.16ms
step:2148/2160 train_time:131403ms step_avg:61.17ms
step:2149/2160 train_time:131492ms step_avg:61.19ms
step:2150/2160 train_time:131579ms step_avg:61.20ms
step:2151/2160 train_time:131668ms step_avg:61.21ms
step:2152/2160 train_time:131755ms step_avg:61.22ms
step:2153/2160 train_time:131845ms step_avg:61.24ms
step:2154/2160 train_time:131933ms step_avg:61.25ms
step:2155/2160 train_time:132022ms step_avg:61.26ms
step:2156/2160 train_time:132108ms step_avg:61.27ms
step:2157/2160 train_time:132198ms step_avg:61.29ms
step:2158/2160 train_time:132285ms step_avg:61.30ms
step:2159/2160 train_time:132374ms step_avg:61.31ms
step:2160/2160 train_time:132461ms step_avg:61.32ms
step:2160/2160 val_loss:3.2796 train_time:132552ms step_avg:61.37ms
peak memory allocated: 30078 MiB reserved: 45076 MiB

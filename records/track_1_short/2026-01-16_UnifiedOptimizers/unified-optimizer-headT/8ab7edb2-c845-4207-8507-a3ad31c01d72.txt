import os
import sys

# Read the current file and the kernels file code ASAP, for logging
with open(sys.argv[0], 'r') as f: 
    code = f.read()
with open(os.path.join(os.path.dirname(sys.argv[0]), 'triton_kernels.py'), 'r') as f:
    code += f"\n\n{'-'*40}\n# triton_kernels.py\n{'-'*40}\n\n" 
    code += f.read()

import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch
import triton

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
from kernels import get_kernel
from torch import Tensor, nn

from triton_kernels import XXT, ba_plus_cAA, linear_relu_square

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng
# Transposed layout by @ChrisJMcCormick allows for faster gradient accumulation.

@torch.library.custom_op("nanogpt::mm_t", mutates_args=())
def mm_t_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    """Computes y = x @ w with F8 weights stored as (in_features, out_features)."""
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        assert x.shape[1] == w.shape[0]  # x: (batch, in), w: (in, out)

        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)

        # _scaled_mm requires column-major B. w_f8 is row-major (in, out).
        # .T.contiguous().T creates a column-major view without changing logical shape.
        w_f8_col_major = w_f8.T.contiguous().T

        out = torch._scaled_mm(
            x_f8,
            w_f8_col_major,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_t_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[0]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_t_backward", mutates_args=())
def mm_t_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        
        x_scale = grad.new_tensor(x_s, dtype=torch.float32)
        w_scale = grad.new_tensor(w_s, dtype=torch.float32)
        grad_scale = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        
        # grad_x = grad @ w.T
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T, 
            out_dtype=torch.bfloat16,
            scale_a=grad_scale,
            scale_b=w_scale,
            use_fast_accum=False,
        )
        
        # grad_w = x.T @ grad
        # Result is (in, out), naturally matching weight storage. No final .T needed.
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_scale,
            scale_b=grad_scale,
            use_fast_accum=False,
        )
        
        return grad_x, grad_w

    grad_x, grad_w = impl(g, x_f8, w_f8)

    return grad_x, grad_w

@mm_t_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward_t(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_t_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context_t(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_t_op.register_autograd(backward_t, setup_context=setup_context_t)

# -----------------------------------------------------------------------------
# Polar Express

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Combined NorMuon + Adam Optimizer

class NorMuonAndAdam:
    """
    Combined optimizer that handles both NorMuon (for weight matrices) and Adam (for embeddings/scalars).

    Unlike torch.optim.Optimizer, this class uses per-parameter settings from a param_table
    instead of parameter groups. Also, instead of using backward hooks for communication scheduling,
    this class uses explicit ordering.

    Two separate orderings are supported:
    - scatter_order: Order to launch reduce/reduce_scatter operations (for communication overlap)
    - work_order: Order to wait for reduce, compute update, and launch gather (for compute overlap)
    
    NorMuon (for attn/mlp banks):
    - SGD-momentum with Polar Express orthogonalization
    - Low-rank variance estimator (NorMuon)
    - Cautious weight decay
    - Mantissa tracking for precision
    - Parameter bank sharding
    
    Adam (for embeddings/scalars):
    - Standard Adam with bias correction
    - Cautious weight decay
    
    # Contributors include @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
    # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick

    Args:
        named_params: Iterator of (name, param) tuples from model.named_parameters()
        param_table: Dict mapping label -> settings dict with keys:
            optim: "adam" or "normuon"
            comms: "replicated" or "sharded" 
            adam_betas: [beta1, beta2] for adam params, None for normuon
        scatter_order: List of labels specifying order to launch reduces
        work_order: List of labels specifying order to process updates
        adam_defaults: Dict of Adam hyperparameters (lr, eps, weight_decay)
        normuon_defaults: Dict of NorMuon hyperparameters (lr, weight_decay, momentum, beta2)
    """

    # -----------------------------------
    # Initialization

    def __init__(self, named_params, param_table: dict, scatter_order: list, work_order: list,
                 adam_defaults: dict, normuon_defaults: dict):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        
        # Store defaults for each optimizer type
        self.adam_defaults = adam_defaults
        self.normuon_defaults = normuon_defaults
        self.param_table = param_table
        self.scatter_order = scatter_order
        self.work_order = work_order
        
        # Collect params by label and build settings
        self.settings: dict[nn.Parameter, dict] = {}
        self.state: dict[nn.Parameter, dict] = {}
        self._param_by_label: dict[str, nn.Parameter] = {}
        for name, param in named_params:
            label = getattr(param, "label", None)
            assert label is not None and label in param_table  # all params must have valid label
            assert label not in self._param_by_label  # exactly one param per label
            self._param_by_label[label] = param
            self._build_param_settings(param, label)
        
        # Assert scatter_order and work_order match present labels exactly
        present = set(self._param_by_label.keys())
        assert set(scatter_order) == present and set(work_order) == present
        
        # Handle world_size=1: overwrite comms to "none"
        if self.world_size == 1:
            for settings in self.settings.values():
                settings["comms"] = "none"
        
        # Initialize state for all params
        self._init_state()
        
        # 0-D CPU tensors to avoid recompilation
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_lr_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        
        # Track async operations
        self._reduce_futures: dict[nn.Parameter, tuple] = {}
        
        # Embed/lm_head tying state
        self.split_embed = False
        self._lm_head_param = self._param_by_label.get("lm_head")
        self._embed_param = self._param_by_label.get("embed")
    
    def _build_param_settings(self, param: nn.Parameter, label: str):
        """Build settings for a single parameter from param_table."""
        table_entry = self.param_table[label]
        optim = table_entry["optim"]
        comms = table_entry["comms"]
        adam_betas = table_entry.get("adam_betas")
        
        settings = {
            "label": label,
            "optim": optim,
            "comms": comms,
            "adam_betas": tuple(adam_betas) if adam_betas else None,
            "lr_mul": getattr(param, "lr_mul", 1.0),
            "wd_mul": getattr(param, "wd_mul", 1.0),
        }
        
        if optim == "adam":
            settings["lr"] = self.adam_defaults["lr"]
            settings["eps"] = self.adam_defaults["eps"]
            settings["weight_decay"] = self.adam_defaults["weight_decay"]
            settings["initial_lr"] = self.adam_defaults["lr"]
        elif optim == "normuon":
            reshape = getattr(param, "reshape", None)
            if reshape is None:
                raise ValueError(f"NorMuon param {label} must have .reshape attribute")
            if reshape[0] % self.world_size != 0:
                raise ValueError(f"reshape[0]={reshape[0]} must be divisible by world_size")
            
            chunk_size = reshape[0] // self.world_size
            settings["reshape"] = reshape
            settings["chunk_size"] = chunk_size
            settings["lr"] = self.normuon_defaults["lr"]
            settings["weight_decay"] = self.normuon_defaults["weight_decay"]
            settings["momentum"] = self.normuon_defaults["momentum"]
            settings["beta2"] = self.normuon_defaults["beta2"]
            settings["initial_lr"] = self.normuon_defaults["lr"]
            
            # Compute shape-based LR multiplier
            chunk_shape = (chunk_size, *reshape[1:])
            shape_mult = max(1.0, chunk_shape[-2] / chunk_shape[-1]) ** 0.5 if len(chunk_shape) >= 2 else 1.0
            settings["lr_mul"] = shape_mult * getattr(param, "lr_mul", 1.0)
            
            # Per-matrix LR multipliers for MLP c_proj (2x LR on odd indices)
            if label == "mlp":
                rank = dist.get_rank() if dist.is_initialized() else 0
                start_idx = rank * chunk_size
                per_matrix_lr_mul = []
                for i in range(chunk_size):
                    global_idx = start_idx + i
                    is_c_proj = (global_idx % 2 == 1)
                    per_matrix_lr_mul.append(2.0 if is_c_proj else 1.0)
                settings["per_matrix_lr_mul"] = per_matrix_lr_mul
        
        # Compute chunk_size for sharded Adam params
        if optim == "adam" and comms == "sharded":
            settings["chunk_size"] = param.shape[0] // self.world_size
        
        self.settings[param] = settings
    
    def _init_state(self):
        """Initialize optimizer state for all parameters."""
        for param, settings in self.settings.items():
            if settings["optim"] == "adam":
                # Sharded params use chunk state, replicated use full state
                if settings["comms"] == "sharded":
                    chunk_size = settings["chunk_size"]
                    chunk = param[:chunk_size]
                else:
                    chunk = param
                exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=param.device)
                self.state[param] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
            
            elif settings["optim"] == "normuon":
                reshape = settings["reshape"]
                chunk_size = settings["chunk_size"]
                chunk_shape = (chunk_size, *reshape[1:])
                
                # Momentum buffer (FP32 for precision)
                momentum_buffer = torch.zeros(
                    chunk_shape, dtype=torch.float32, device=param.device
                )
                
                # Second momentum buffer - reduced along one dimension
                if chunk_shape[-2] >= chunk_shape[-1]:
                    second_mom_shape = (*chunk_shape[:-1], 1)
                else:
                    second_mom_shape = (*chunk_shape[:-2], 1, chunk_shape[-1])
                second_momentum_buffer = torch.zeros(
                    second_mom_shape, dtype=torch.float32, device=param.device
                )
                
                # Mantissa buffer for precision tracking
                mantissa = torch.zeros(
                    chunk_shape, dtype=torch.uint16, device=param.device
                )
                
                self.state[param] = dict(
                    momentum_buffer=momentum_buffer,
                    second_momentum_buffer=second_momentum_buffer,
                    mantissa=mantissa,
                )

    # -----------------------------------
    # Reduce/Gather operations
    
    def _launch_reduce(self, param: nn.Parameter, grad: Tensor):
        """Launch async reduce for a parameter based on its comms policy."""
        settings = self.settings[param]
        comms = settings["comms"]
        
        if comms == "none":
            if settings["optim"] == "normuon":
                # NorMuon needs reshaped gradient even without communication
                reshape = settings["reshape"]
                grad = grad.view(reshape)
            self._reduce_futures[param] = (None, grad)
        elif comms == "replicated":
            future = dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future()
            self._reduce_futures[param] = (future, grad)
        elif comms == "sharded":
            if settings["optim"] == "normuon":
                # NorMuon: reshape before reduce_scatter
                reshape = settings["reshape"]
                chunk_size = settings["chunk_size"]
                grad_reshaped = grad.view(reshape)
                grad_chunk = torch.empty(
                    (chunk_size, *grad_reshaped.shape[1:]),
                    dtype=grad.dtype,
                    device=grad.device
                )
                future = dist.reduce_scatter_tensor(
                    grad_chunk, grad_reshaped.contiguous(), op=dist.ReduceOp.AVG, async_op=True
                ).get_future()
                self._reduce_futures[param] = (future, grad_chunk)
            else:
                # Adam: simple reduce_scatter
                chunk_size = settings["chunk_size"]
                grad_chunk = torch.empty_like(grad[:chunk_size])
                future = dist.reduce_scatter_tensor(
                    grad_chunk, grad, op=dist.ReduceOp.AVG, async_op=True
                ).get_future()
                self._reduce_futures[param] = (future, grad_chunk)

    def _launch_gather(self, param: nn.Parameter, p_slice: Tensor) -> "torch.futures.Future":
        """Launch async all_gather for a sharded parameter."""
        settings = self.settings[param]
        if settings["optim"] == "normuon":
            reshape = settings["reshape"]
            full_param = param.data.view(reshape)
            assert full_param.is_contiguous()
            return dist.all_gather_into_tensor(
                full_param, p_slice.contiguous(), async_op=True
            ).get_future()
        else:
            return dist.all_gather_into_tensor(
                param, p_slice.contiguous(), async_op=True
            ).get_future()

    # -----------------------------------
    # State management
    
    def reset(self):
        """Reset NorMuon momentum buffers and split_embed state (called on training reset)."""
        self.split_embed = False
        for param, settings in self.settings.items():
            if settings["optim"] == "normuon":
                state = self.state[param]
                state["momentum_buffer"].zero_()
                state["mantissa"].zero_()
                state["second_momentum_buffer"].zero_()
    
    def copy_lm_to_embed(self):
        """Copy lm_head weights to embed at split point (2/3 of training).
        
        Note: lm_head uses transposed storage (model_dim, vocab_size) while embed
        uses standard storage (vocab_size, model_dim), so we transpose when copying.
        
        Optimizer state is not copied because lm_head and embed have different sharding:
        - lm_head (768, 50304) is sharded to (96, 50304) per rank
        - embed (50304, 768) is sharded to (6288, 768) per rank
        The shapes are incompatible, so embed's optimizer state starts fresh.
        """
        lm_head = self._lm_head_param
        embed = self._embed_param
        if lm_head is None or embed is None:
            return
        
        # Copy step count so bias correction is consistent
        lm_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_state['step']
        
        # Transpose weight data (optimizer state stays at zeros - different sharding)
        embed.data.copy_(lm_head.data.T)
        
        # Mark as split
        self.split_embed = True
    
    def state_dict(self):
        """Return the optimizer state as a dict."""
        return {
            "state": {id(p): s for p, s in self.state.items()},
            "settings": {id(p): s for p, s in self.settings.items()},
        }
    
    def load_state_dict(self, state_dict):
        """Load optimizer state from a dict."""
        # Build id->param mapping
        id_to_param = {id(p): p for p in self.settings.keys()}
        
        # Load state, preserving dtypes
        for param_id, saved_state in state_dict["state"].items():
            if param_id in id_to_param:
                param = id_to_param[param_id]
                current_state = self.state[param]
                for k, v in saved_state.items():
                    if isinstance(v, torch.Tensor) and k in current_state:
                        target_dtype = current_state[k].dtype
                        current_state[k] = v.to(dtype=target_dtype, device=current_state[k].device)
                    else:
                        current_state[k] = v

    # -----------------------------------
    # Unified optimizer step with explicit ordering

    @torch.no_grad()
    def step(self, do_adam: bool = True):
        """
        Combined optimizer step with explicit ordering.
        
        Args:
            do_adam: If True, update Adam params. NorMuon params always updated.
        
        Flow:
        1. Scatter phase: Launch reduces in scatter_order
        2. Work phase: Process updates in work_order
           - Wait for reduce, compute update, launch gather
        3. Finalize phase: Wait for gathers
        
        When split_embed=False (tied):
        - embed.grad.T is aggregated into lm_head.grad before lm_head's reduce
        - embed is not updated (skipped entirely)
        - After lm_head gather, embed.data is copied from lm_head.data.T
        
        When split_embed=True (untied):
        - Both lm_head and embed are updated independently
        """
        rank = dist.get_rank() if dist.is_initialized() else 0
        lm_param, embed_param = self._lm_head_param, self._embed_param
        
        # ===== Phase 1: Launch reduces in scatter_order =====
        for label in self.scatter_order:
            param = self._param_by_label[label]
            settings = self.settings[param]
            
            if settings["optim"] == "adam" and not do_adam:
                continue
            if param.grad is None:
                continue
            
            # lm_head when tied: aggregate embed.grad.T (transposed shapes)
            if label == "lm_head" and do_adam and not self.split_embed:
                if embed_param is not None and embed_param.grad is not None:
                    param.grad.add_(embed_param.grad.T)
            
            # Skip embed when tied (copied from lm_head after gather)
            if label == "embed" and not self.split_embed:
                continue
            
            self._launch_reduce(param, param.grad)
        
        # ===== Phase 2: Process updates in work_order =====
        gather_futures = []
        lm_head_gather_future = None
        
        for label in self.work_order:
            param = self._param_by_label[label]
            if param not in self._reduce_futures:
                continue
            
            settings = self.settings[param]
            if settings["optim"] == "adam" and not do_adam:
                continue
            # Wait for reduce
            future, grad_chunk = self._reduce_futures[param]
            if future is not None:
                future.wait()
            # Apply update based on optim type
            if settings["optim"] == "adam":
                p_slice = self._adam_update(param, grad_chunk, settings, rank)
            else:
                p_slice = self._normuon_update(param, grad_chunk, settings, rank)
            # Launch gather for sharded params
            if settings["comms"] == "sharded" and self.world_size > 1:
                gather_fut = self._launch_gather(param, p_slice)
                if label == "lm_head":
                    lm_head_gather_future = gather_fut
                else:
                    gather_futures.append(gather_fut)
        
        # ===== Phase 3: Wait for gathers, sync embed if tied =====
        # Wait for lm_head gather first so we can copy to embed while other gathers complete
        if lm_head_gather_future is not None:
            lm_head_gather_future.wait()
        
        # When tied: copy lm_head.T to embed
        if do_adam and not self.split_embed and embed_param is not None and lm_param is not None:
            embed_param.data.copy_(lm_param.data.T)
        
        # Wait for remaining gathers
        for fut in gather_futures:
            fut.wait()
        
        self._reduce_futures.clear()
        
        # Clear grads for updated params
        for param, settings in self.settings.items():
            if settings["optim"] == "adam" and not do_adam:
                continue  # Don't clear Adam grads on even steps
            param.grad = None

    # -----------------------------------
    # Adam update

    def _adam_update(self, param: nn.Parameter, grad_chunk: Tensor, settings: dict, rank: int) -> Tensor:
        """Apply Adam update to a parameter. Returns the updated p_slice."""
        beta1, beta2 = settings["adam_betas"]
        eps = settings["eps"]
        wd = settings["weight_decay"]
        lr = settings["lr"] * settings["lr_mul"]
        
        # Get parameter slice
        if settings["comms"] == "sharded":
            chunk_size = settings["chunk_size"]
            p_slice = param[rank * chunk_size:(rank + 1) * chunk_size]
        else:
            p_slice = param
        
        state = self.state[param]
        state["step"] += 1
        t = state["step"]
        
        bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
        self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
        self._eff_wd_t.fill_(lr * lr * wd * settings["wd_mul"])
        
        NorMuonAndAdam._adam_update_step(
            p_slice, grad_chunk, state["exp_avg"], state["exp_avg_sq"],
            beta1, beta2, eps, self._step_size_t, self._eff_wd_t
        )
        
        return p_slice

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _adam_update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)
        # Cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)
        p_slice.add_(other=update, alpha=-1.0)

    # -----------------------------------
    # NorMuon update

    def _normuon_update(self, param: nn.Parameter, grad_chunk: Tensor, settings: dict, rank: int) -> Tensor:
        """Apply NorMuon update to a parameter. Returns the updated p_slice."""
        reshape = settings["reshape"]
        chunk_size = settings["chunk_size"]
        chunk_shape = grad_chunk.shape
        
        state = self.state[param]
        grad_chunk = grad_chunk.float()  # FP32 for momentum
        
        # Momentum update
        momentum_buffer = state["momentum_buffer"]
        momentum_buffer.lerp_(grad_chunk, 1 - settings["momentum"])
        updated_grads = grad_chunk.lerp_(momentum_buffer, settings["momentum"])
        
        self._eff_lr_t.fill_(settings["lr_mul"] * settings["lr"])
        self._eff_wd_t.fill_(settings["wd_mul"] * settings["weight_decay"] * settings["lr"])
        
        # Polar Express orthogonalization
        is_large_matrix = chunk_shape[-2] > 1024
        v_chunk = polar_express(updated_grads, split_baddbmm=is_large_matrix)
        
        # Variance reduction
        red_dim = -1 if chunk_shape[-2] >= chunk_shape[-1] else -2
        v_chunk = NorMuonAndAdam._apply_normuon_variance_reduction(
            v_chunk, state["second_momentum_buffer"], settings["beta2"], red_dim
        )
        
        # Update parameter, in place, with cautious weight decay
        param_view = param.data.view(reshape)
        p_slice = param_view[rank * chunk_size:(rank + 1) * chunk_size]
        
        # MLP has per-matrix LR multipliers (c_proj gets 2x LR)
        if "per_matrix_lr_mul" in settings:
            per_matrix_lr_mul = settings["per_matrix_lr_mul"]
            for mat_idx in range(chunk_size):
                self._eff_lr_t.fill_(settings["lr_mul"] * per_matrix_lr_mul[mat_idx] * settings["lr"])
                self._eff_wd_t.fill_(settings["wd_mul"] * settings["weight_decay"] * settings["lr"])
                NorMuonAndAdam._cautious_wd_and_update_inplace(
                    p_slice[mat_idx].view(torch.uint16), state["mantissa"][mat_idx], v_chunk[mat_idx],
                    self._eff_wd_t, self._eff_lr_t
                )
        else:
            NorMuonAndAdam._cautious_wd_and_update_inplace(
                p_slice.view(torch.uint16), state["mantissa"], v_chunk,
                self._eff_wd_t, self._eff_lr_t
            )
        
        return p_slice

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
        """
        Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
        Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
        bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
        float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
        """
        assert p.dtype == mantissa.dtype == torch.uint16
        grad = grad.float()
        wd_factor = wd_tensor.to(torch.float32)
        lr_factor = lr_tensor.to(torch.float32)
        p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
        p_precise = p_precise_raw.view(torch.float32)
        mask = (grad * p_precise) >= 0
        p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
        p.copy_((p_precise_raw >> 16).to(torch.uint16))
        mantissa.copy_(p_precise_raw.to(torch.uint16))

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
        """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
        v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
        red_dim_size = v_chunk.size(red_dim)
        v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
        v_norm = v_norm_sq.sqrt_()
        second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
        step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
        scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
        v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
        final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
        return v_chunk.mul_(final_scale.type_as(v_chunk))

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinearT(nn.Module):
    """
    Linear layer with transposed weight storage (in_features, out_features) which
    addresses the slow kernel that was used for gradient accumulation. @chrisjmccormick
    """
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s
        
        self.weight = nn.Parameter(torch.empty(in_features, out_features, dtype=torch.bfloat16))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        with torch.no_grad():
            nn.init.zeros_(self.weight) # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out = torch.ops.nanogpt.mm_t(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return x @ self.weight.type_as(x)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim
        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim
        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * qkvo_w[self.dim * 3:].type_as(y))
        return y

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, c_fc: Tensor, c_proj: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        # Fused triton kernel for relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, c_fc, c_proj)

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, has_attn: bool, has_mlp: bool, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if has_attn:
            if use_paired_head:
                self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads)
            else:
                self.attn = CausalSelfAttention(dim, head_dim, num_heads)
        else:
            self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP() if has_mlp else None

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor = None, c_fc: Tensor = None, c_proj: Tensor = None):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args, qkvo_w)
        if self.mlp is not None:
            x = x + self.mlp(norm(x), c_fc, c_proj)
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = nn.Linear(12, 1, bias=False)
        nn.init.zeros_(self.smear_gate.weight)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = nn.Linear(12, 1, bias=False)
        nn.init.zeros_(self.skip_gate.weight)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for i, ve in enumerate(self.value_embeds):
            ve.weight.label = f've{i}'  # ve0, ve1, ve2
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        # -----------------------------------
        # Parameter banks for sharded optimization, by @chrisjmccormick

        # Identify which layers have attention/MLP
        # Attention is skipped in layer 6 by @YouJiacheng
        self.attn_layer_indices = [i for i in range(num_layers) if i != 6]
        # All layers have MLP (At 11 layers--dropped first layer @EmelyanenkoK)
        self.mlp_layer_indices = list(range(num_layers))

        hdim = num_heads * head_dim
        mlp_hdim = 4 * model_dim

        # Create index mappings: layer_idx -> bank_idx
        self.layer_to_attn_idx = {layer_idx: bank_idx for bank_idx, layer_idx in enumerate(self.attn_layer_indices)}
        self.layer_to_mlp_idx = {layer_idx: bank_idx for bank_idx, layer_idx in enumerate(self.mlp_layer_indices)}

        # Attention bank: stores QKVO weights for all attention layers
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        # Shape: (num_attn_layers, 4*model_dim, hdim) = (10, 3072, 768)
        # Reshape for sharding: (40, 768, 768) for even distribution across 8 GPUs
        self.attn_bank = nn.Parameter(torch.empty(len(self.attn_layer_indices), 4 * model_dim, hdim))
        self.attn_bank.label = 'attn'
        self.attn_bank.reshape = (len(self.attn_layer_indices) * 4, hdim, hdim)  # (40, 768, 768)

        # MLP bank: stores c_fc and c_proj for all MLP layers
        # Shape: (num_mlp_layers + padding, 2, mlp_hdim, model_dim) = (12, 2, 3072, 768)
        # We add 1 padding layer (index 11) to get 12*2=24 matrices for even distribution across 8 GPUs
        # Reshape for sharding: (24, 3072, 768)
        num_mlp_with_padding = len(self.mlp_layer_indices) + 1  # 11 + 1 = 12
        self.mlp_bank = nn.Parameter(torch.empty(num_mlp_with_padding, 2, mlp_hdim, model_dim))
        self.mlp_bank.label = 'mlp'
        self.mlp_bank.reshape = (num_mlp_with_padding * 2, mlp_hdim, model_dim)  # (24, 3072, 768)

        # improved init scale by @YouJiacheng
        # Attention uses dim^-0.5, MLP uses 0.5 * dim^-0.5
        attn_std = model_dim ** -0.5
        attn_bound = (3 ** 0.5) * attn_std
        mlp_std = 0.5 * (model_dim ** -0.5)
        mlp_bound = (3 ** 0.5) * mlp_std
        with torch.no_grad():
            # Init attention bank (QKV uniform, O zero)
            self.attn_bank[:, :model_dim * 3, :].uniform_(-attn_bound, attn_bound)
            self.attn_bank[:, model_dim * 3:, :].zero_()
            # Init MLP bank (c_fc uniform, c_proj zero) 
            self.mlp_bank[:, 0, :, :].uniform_(-mlp_bound, mlp_bound)  # c_fc
            self.mlp_bank[:, 1, :, :].zero_()  # c_proj - zero init suggested by @Grad62304977

        # Create blocks with has_attn/has_mlp flags
        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([
            Block(model_dim, head_dim, num_heads, 
                  has_attn=(i in self.layer_to_attn_idx), 
                  has_mlp=(i in self.layer_to_mlp_idx),
                  use_paired_head=(i in self.paired_head_layers))
            for i in range(num_layers)
        ])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        # Transposed weight storage for faster gradient accumulation
        self.lm_head = CastedLinearT(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)

        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'
        with torch.no_grad():
            self.embed.weight.copy_(self.lm_head.weight.T)

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # Embedding lookup - embed is synced from lm_head during tied phase by optimizer
        x = self.embed(input_seq)
        
        # Value embeddings - always computed (not precomputed)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        # unbind weight banks to avoid select_backwards kernel
        attn_weights = self.attn_bank.unbind(0)  # tuple of [4*dim, hdim] tensors
        mlp_fcs = self.mlp_bank[:, 0, :, :].unbind(0)  # tuple of [mlp_hdim, dim] tensors
        mlp_projs = self.mlp_bank[:, 1, :, :].unbind(0)  # tuple of [mlp_hdim, dim] tensors

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            
            # Get weights for this layer from banks
            qkvo_w = attn_weights[self.layer_to_attn_idx[i]] if i in self.layer_to_attn_idx else None
            c_fc = mlp_fcs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            c_proj = mlp_projs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            
            x = self.blocks[i](x, attn_args, qkvo_w, c_fc, c_proj)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages the NorMuonAndAdam for all parameters with explicit ordering.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Adam optimizers are only stepped on odd steps @classiclarryd
        3. Explicit scatter_order and work_order for communication scheduling (no backward hooks)
        4. Muon has a linear momentum warmup and cooldown schedule
        5. Learning rates follow a linear decay schedule
        6. Embed is tied to lm_head until split step (2/3 of training), then untied @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm_head at 2/3 of training (weights and optimizer state copied)
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        
        # - Ordering dictates when to launch reduce/reduce_scatter operations
        # - "sharded" parameters use reduce_scatter/all_gather and "replicated" ones use all_reduce
        self.param_table = {
            "attn":           {"optim": "normuon", "comms": "sharded",     "adam_betas": None},
            "mlp":            {"optim": "normuon", "comms": "sharded",     "adam_betas": None},         
            "scalars":        {"optim": "adam",    "comms": "replicated",  "adam_betas": [0.9,  0.99]},
            "ve0":            {"optim": "adam",    "comms": "sharded",     "adam_betas": [0.75, 0.95]},
            "ve1":            {"optim": "adam",    "comms": "sharded",     "adam_betas": [0.75, 0.95]},
            "ve2":            {"optim": "adam",    "comms": "sharded",     "adam_betas": [0.75, 0.95]},
            "smear_gate":     {"optim": "adam",    "comms": "replicated",  "adam_betas": [0.9,  0.99]},
            "skip_gate":      {"optim": "adam",    "comms": "replicated",  "adam_betas": [0.9,  0.99]},
            "attn_gate_bank": {"optim": "adam",    "comms": "replicated",  "adam_betas": [0.9,  0.99]},
            "ve_gate_bank":   {"optim": "adam",    "comms": "replicated",  "adam_betas": [0.9,  0.99]},
            "x0_lambdas":     {"optim": "adam",    "comms": "replicated",  "adam_betas": [0.65, 0.95]},
            "lm_head":        {"optim": "adam",    "comms": "sharded",     "adam_betas": [0.5,  0.95]}, # Before embed for grad aggregation
            "embed":          {"optim": "adam",    "comms": "sharded",     "adam_betas": [0.5,  0.95]}, # Last (skipped when tied)
        }

        # - Process smaller/faster params first while large reduces complete
        # - lm_head must complete before embed sync (when tied)
        self.work_order = [
            "scalars", "smear_gate", "skip_gate", "attn_gate_bank", "ve_gate_bank", "x0_lambdas",  # Small, fast
            "ve0", "ve1", "ve2",  # Medium
            "lm_head", "embed",   # lm_head must complete before embed sync (when tied)
            "attn", "mlp",        # Large, polar express - process last to maximize overlap
        ]

        adam_defaults = dict(
            lr=0.008,
            eps=1e-10,
            weight_decay=0.005,
        )
        
        normuon_defaults = dict(
            lr=0.023,
            momentum=0.95,
            beta2=0.95,
            weight_decay=1.2,
        )
        
        self.optimizer = NorMuonAndAdam(
            model.named_parameters(),
            param_table=self.param_table,
            scatter_order=list(self.param_table.keys()),  # Dict order defines scatter priority
            work_order=self.work_order,
            adam_defaults=adam_defaults,
            normuon_defaults=normuon_defaults,
        )

        # Split embed from lm_head at 2/3 of training (on an odd step so Adam updates)
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations - 50) | 1

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_adam_step(self, step: int):
        """Adam params are only updated on odd steps."""
        return step % 2 == 1

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
            self.batch_size = new_batch_size # fix - train_loader_send_args was being set every step after the first batch size transition, instead of only on transitions.
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        do_adam = self._is_adam_step(step)
        
        # Update learning rates and momentum for all params
        for param, settings in self.optimizer.settings.items():
            settings["lr"] = settings["initial_lr"] * step_lr
            if settings["optim"] == "normuon":
                settings["momentum"] = muon_momentum
        
        # Step optimizer with do_adam flag
        self.optimizer.step(do_adam=do_adam)
        
        # At split step: copy lm_head to embed and mark as split
        if step == self.split_step:
            self.optimizer.copy_lm_to_embed()

    def reset(self, state=None):
        if state is not None:
            self.optimizer.load_state_dict(state)

        # Reset NorMuon momentum buffers and split_embed state
        self.optimizer.reset()

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return copy.deepcopy(self.optimizer.state_dict())

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
model.attn_bank.data = model.attn_bank.data.bfloat16()
model.mlp_bank.data = model.mlp_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizer=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizer"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizer=training_manager.get_state())
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        inputs, targets, cum_seqlens = train_loader.send(training_manager.train_loader_send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


----------------------------------------
# triton_kernels.py
----------------------------------------

import torch
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# -----------------------------------------------------------------------------
# Triton kernel for MLP: relu(x @ W1.T)^2, by @andrewbriand, @jrauvola

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Jan 17 01:22:21 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            136W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    821978      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    1   N/A  N/A    821979      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    821980      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    821981      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    821982      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    821983      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    821984      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    821985      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8317 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:76ms step_avg:75.86ms
step:2/1775 train_time:98ms step_avg:49.12ms
step:3/1775 train_time:117ms step_avg:38.86ms
step:4/1775 train_time:138ms step_avg:34.53ms
step:5/1775 train_time:168ms step_avg:33.62ms
step:6/1775 train_time:281ms step_avg:46.84ms
step:7/1775 train_time:297ms step_avg:42.44ms
step:8/1775 train_time:415ms step_avg:51.82ms
step:9/1775 train_time:445ms step_avg:49.44ms
step:10/1775 train_time:478ms step_avg:47.85ms
step:11/1775 train_time:509ms step_avg:46.28ms
step:12/1775 train_time:543ms step_avg:45.24ms
step:13/1775 train_time:573ms step_avg:44.11ms
step:14/1775 train_time:607ms step_avg:43.38ms
step:15/1775 train_time:638ms step_avg:42.54ms
step:16/1775 train_time:672ms step_avg:42.01ms
step:17/1775 train_time:703ms step_avg:41.35ms
step:18/1775 train_time:737ms step_avg:40.95ms
step:19/1775 train_time:768ms step_avg:40.42ms
step:20/1775 train_time:802ms step_avg:40.09ms
step:21/1775 train_time:832ms step_avg:39.64ms
step:22/1775 train_time:867ms step_avg:39.39ms
step:23/1775 train_time:897ms step_avg:39.00ms
step:24/1775 train_time:931ms step_avg:38.79ms
step:25/1775 train_time:962ms step_avg:38.48ms
step:26/1775 train_time:996ms step_avg:38.30ms
step:27/1775 train_time:1027ms step_avg:38.03ms
step:28/1775 train_time:1060ms step_avg:37.87ms
step:29/1775 train_time:1091ms step_avg:37.63ms
step:30/1775 train_time:1125ms step_avg:37.51ms
step:31/1775 train_time:1156ms step_avg:37.29ms
step:32/1775 train_time:1190ms step_avg:37.18ms
step:33/1775 train_time:1221ms step_avg:36.99ms
step:34/1775 train_time:1255ms step_avg:36.90ms
step:35/1775 train_time:1286ms step_avg:36.76ms
step:36/1775 train_time:1322ms step_avg:36.72ms
step:37/1775 train_time:1355ms step_avg:36.63ms
step:38/1775 train_time:1391ms step_avg:36.61ms
step:39/1775 train_time:1423ms step_avg:36.48ms
step:40/1775 train_time:1457ms step_avg:36.43ms
step:41/1775 train_time:1488ms step_avg:36.29ms
step:42/1775 train_time:1522ms step_avg:36.23ms
step:43/1775 train_time:1553ms step_avg:36.12ms
step:44/1775 train_time:1587ms step_avg:36.07ms
step:45/1775 train_time:1618ms step_avg:35.96ms
step:46/1775 train_time:1653ms step_avg:35.93ms
step:47/1775 train_time:1684ms step_avg:35.82ms
step:48/1775 train_time:1717ms step_avg:35.78ms
step:49/1775 train_time:1748ms step_avg:35.68ms
step:50/1775 train_time:1782ms step_avg:35.65ms
step:51/1775 train_time:1813ms step_avg:35.55ms
step:52/1775 train_time:1847ms step_avg:35.52ms
step:53/1775 train_time:1878ms step_avg:35.44ms
step:54/1775 train_time:1912ms step_avg:35.41ms
step:55/1775 train_time:1944ms step_avg:35.35ms
step:56/1775 train_time:1978ms step_avg:35.31ms
step:57/1775 train_time:2008ms step_avg:35.23ms
step:58/1775 train_time:2042ms step_avg:35.21ms
step:59/1775 train_time:2073ms step_avg:35.13ms
step:60/1775 train_time:2107ms step_avg:35.11ms
step:61/1775 train_time:2137ms step_avg:35.04ms
step:62/1775 train_time:2172ms step_avg:35.02ms
step:63/1775 train_time:2202ms step_avg:34.96ms
step:64/1775 train_time:2236ms step_avg:34.94ms
step:65/1775 train_time:2268ms step_avg:34.89ms
step:66/1775 train_time:2302ms step_avg:34.88ms
step:67/1775 train_time:2334ms step_avg:34.83ms
step:68/1775 train_time:2368ms step_avg:34.83ms
step:69/1775 train_time:2400ms step_avg:34.78ms
step:70/1775 train_time:2434ms step_avg:34.78ms
step:71/1775 train_time:2466ms step_avg:34.73ms
step:72/1775 train_time:2500ms step_avg:34.72ms
step:73/1775 train_time:2532ms step_avg:34.68ms
step:74/1775 train_time:2566ms step_avg:34.68ms
step:75/1775 train_time:2597ms step_avg:34.63ms
step:76/1775 train_time:2631ms step_avg:34.62ms
step:77/1775 train_time:2662ms step_avg:34.57ms
step:78/1775 train_time:2696ms step_avg:34.56ms
step:79/1775 train_time:2727ms step_avg:34.52ms
step:80/1775 train_time:2761ms step_avg:34.51ms
step:81/1775 train_time:2792ms step_avg:34.47ms
step:82/1775 train_time:2826ms step_avg:34.46ms
step:83/1775 train_time:2857ms step_avg:34.42ms
step:84/1775 train_time:2891ms step_avg:34.42ms
step:85/1775 train_time:2922ms step_avg:34.38ms
step:86/1775 train_time:2956ms step_avg:34.37ms
step:87/1775 train_time:2987ms step_avg:34.33ms
step:88/1775 train_time:3020ms step_avg:34.32ms
step:89/1775 train_time:3051ms step_avg:34.29ms
step:90/1775 train_time:3086ms step_avg:34.28ms
step:91/1775 train_time:3116ms step_avg:34.24ms
step:92/1775 train_time:3151ms step_avg:34.25ms
step:93/1775 train_time:3181ms step_avg:34.21ms
step:94/1775 train_time:3216ms step_avg:34.21ms
step:95/1775 train_time:3247ms step_avg:34.18ms
step:96/1775 train_time:3281ms step_avg:34.18ms
step:97/1775 train_time:3312ms step_avg:34.15ms
step:98/1775 train_time:3347ms step_avg:34.16ms
step:99/1775 train_time:3379ms step_avg:34.13ms
step:100/1775 train_time:3413ms step_avg:34.13ms
step:101/1775 train_time:3445ms step_avg:34.11ms
step:102/1775 train_time:3478ms step_avg:34.10ms
step:103/1775 train_time:3510ms step_avg:34.08ms
step:104/1775 train_time:3544ms step_avg:34.08ms
step:105/1775 train_time:3575ms step_avg:34.05ms
step:106/1775 train_time:3609ms step_avg:34.05ms
step:107/1775 train_time:3640ms step_avg:34.02ms
step:108/1775 train_time:3675ms step_avg:34.02ms
step:109/1775 train_time:3705ms step_avg:33.99ms
step:110/1775 train_time:3739ms step_avg:34.00ms
step:111/1775 train_time:3771ms step_avg:33.97ms
step:112/1775 train_time:3805ms step_avg:33.97ms
step:113/1775 train_time:3835ms step_avg:33.94ms
step:114/1775 train_time:3869ms step_avg:33.94ms
step:115/1775 train_time:3900ms step_avg:33.92ms
step:116/1775 train_time:3934ms step_avg:33.92ms
step:117/1775 train_time:3965ms step_avg:33.89ms
step:118/1775 train_time:3999ms step_avg:33.89ms
step:119/1775 train_time:4030ms step_avg:33.87ms
step:120/1775 train_time:4065ms step_avg:33.87ms
step:121/1775 train_time:4096ms step_avg:33.85ms
step:122/1775 train_time:4130ms step_avg:33.85ms
step:123/1775 train_time:4160ms step_avg:33.82ms
step:124/1775 train_time:4195ms step_avg:33.83ms
step:125/1775 train_time:4226ms step_avg:33.81ms
step:126/1775 train_time:4260ms step_avg:33.81ms
step:127/1775 train_time:4292ms step_avg:33.79ms
step:128/1775 train_time:4326ms step_avg:33.80ms
step:129/1775 train_time:4357ms step_avg:33.78ms
step:130/1775 train_time:4392ms step_avg:33.78ms
step:131/1775 train_time:4423ms step_avg:33.76ms
step:132/1775 train_time:4457ms step_avg:33.76ms
step:133/1775 train_time:4487ms step_avg:33.74ms
step:134/1775 train_time:4521ms step_avg:33.74ms
step:135/1775 train_time:4553ms step_avg:33.72ms
step:136/1775 train_time:4587ms step_avg:33.73ms
step:137/1775 train_time:4618ms step_avg:33.71ms
step:138/1775 train_time:4652ms step_avg:33.71ms
step:139/1775 train_time:4684ms step_avg:33.69ms
step:140/1775 train_time:4718ms step_avg:33.70ms
step:141/1775 train_time:4749ms step_avg:33.68ms
step:142/1775 train_time:4783ms step_avg:33.68ms
step:143/1775 train_time:4814ms step_avg:33.66ms
step:144/1775 train_time:4847ms step_avg:33.66ms
step:145/1775 train_time:4878ms step_avg:33.64ms
step:146/1775 train_time:4912ms step_avg:33.65ms
step:147/1775 train_time:4943ms step_avg:33.63ms
step:148/1775 train_time:4978ms step_avg:33.63ms
step:149/1775 train_time:5009ms step_avg:33.61ms
step:150/1775 train_time:5042ms step_avg:33.62ms
step:151/1775 train_time:5073ms step_avg:33.60ms
step:152/1775 train_time:5107ms step_avg:33.60ms
step:153/1775 train_time:5138ms step_avg:33.58ms
step:154/1775 train_time:5173ms step_avg:33.59ms
step:155/1775 train_time:5203ms step_avg:33.57ms
step:156/1775 train_time:5237ms step_avg:33.57ms
step:157/1775 train_time:5268ms step_avg:33.56ms
step:158/1775 train_time:5302ms step_avg:33.56ms
step:159/1775 train_time:5334ms step_avg:33.55ms
step:160/1775 train_time:5367ms step_avg:33.55ms
step:161/1775 train_time:5398ms step_avg:33.53ms
step:162/1775 train_time:5433ms step_avg:33.54ms
step:163/1775 train_time:5464ms step_avg:33.52ms
step:164/1775 train_time:5498ms step_avg:33.52ms
step:165/1775 train_time:5529ms step_avg:33.51ms
step:166/1775 train_time:5563ms step_avg:33.51ms
step:167/1775 train_time:5594ms step_avg:33.50ms
step:168/1775 train_time:5628ms step_avg:33.50ms
step:169/1775 train_time:5659ms step_avg:33.49ms
step:170/1775 train_time:5694ms step_avg:33.49ms
step:171/1775 train_time:5725ms step_avg:33.48ms
step:172/1775 train_time:5759ms step_avg:33.48ms
step:173/1775 train_time:5790ms step_avg:33.47ms
step:174/1775 train_time:5824ms step_avg:33.47ms
step:175/1775 train_time:5854ms step_avg:33.45ms
step:176/1775 train_time:5889ms step_avg:33.46ms
step:177/1775 train_time:5920ms step_avg:33.44ms
step:178/1775 train_time:5954ms step_avg:33.45ms
step:179/1775 train_time:5985ms step_avg:33.43ms
step:180/1775 train_time:6019ms step_avg:33.44ms
step:181/1775 train_time:6050ms step_avg:33.42ms
step:182/1775 train_time:6084ms step_avg:33.43ms
step:183/1775 train_time:6115ms step_avg:33.42ms
step:184/1775 train_time:6149ms step_avg:33.42ms
step:185/1775 train_time:6179ms step_avg:33.40ms
step:186/1775 train_time:6214ms step_avg:33.41ms
step:187/1775 train_time:6245ms step_avg:33.40ms
step:188/1775 train_time:6279ms step_avg:33.40ms
step:189/1775 train_time:6310ms step_avg:33.38ms
step:190/1775 train_time:6343ms step_avg:33.39ms
step:191/1775 train_time:6374ms step_avg:33.37ms
step:192/1775 train_time:6408ms step_avg:33.37ms
step:193/1775 train_time:6439ms step_avg:33.36ms
step:194/1775 train_time:6474ms step_avg:33.37ms
step:195/1775 train_time:6505ms step_avg:33.36ms
step:196/1775 train_time:6539ms step_avg:33.36ms
step:197/1775 train_time:6571ms step_avg:33.35ms
step:198/1775 train_time:6605ms step_avg:33.36ms
step:199/1775 train_time:6636ms step_avg:33.34ms
step:200/1775 train_time:6670ms step_avg:33.35ms
step:201/1775 train_time:6700ms step_avg:33.34ms
step:202/1775 train_time:6734ms step_avg:33.34ms
step:203/1775 train_time:6765ms step_avg:33.33ms
step:204/1775 train_time:6799ms step_avg:33.33ms
step:205/1775 train_time:6830ms step_avg:33.32ms
step:206/1775 train_time:6864ms step_avg:33.32ms
step:207/1775 train_time:6895ms step_avg:33.31ms
step:208/1775 train_time:6929ms step_avg:33.31ms
step:209/1775 train_time:6960ms step_avg:33.30ms
step:210/1775 train_time:6994ms step_avg:33.31ms
step:211/1775 train_time:7025ms step_avg:33.30ms
step:212/1775 train_time:7059ms step_avg:33.30ms
step:213/1775 train_time:7090ms step_avg:33.29ms
step:214/1775 train_time:7124ms step_avg:33.29ms
step:215/1775 train_time:7155ms step_avg:33.28ms
step:216/1775 train_time:7189ms step_avg:33.28ms
step:217/1775 train_time:7220ms step_avg:33.27ms
step:218/1775 train_time:7254ms step_avg:33.27ms
step:219/1775 train_time:7284ms step_avg:33.26ms
step:220/1775 train_time:7318ms step_avg:33.26ms
step:221/1775 train_time:7349ms step_avg:33.25ms
step:222/1775 train_time:7383ms step_avg:33.26ms
step:223/1775 train_time:7414ms step_avg:33.25ms
step:224/1775 train_time:7448ms step_avg:33.25ms
step:225/1775 train_time:7479ms step_avg:33.24ms
step:226/1775 train_time:7513ms step_avg:33.24ms
step:227/1775 train_time:7544ms step_avg:33.24ms
step:228/1775 train_time:7578ms step_avg:33.24ms
step:229/1775 train_time:7610ms step_avg:33.23ms
step:230/1775 train_time:7644ms step_avg:33.23ms
step:231/1775 train_time:7675ms step_avg:33.22ms
step:232/1775 train_time:7709ms step_avg:33.23ms
step:233/1775 train_time:7740ms step_avg:33.22ms
step:234/1775 train_time:7775ms step_avg:33.22ms
step:235/1775 train_time:7806ms step_avg:33.22ms
step:236/1775 train_time:7840ms step_avg:33.22ms
step:237/1775 train_time:7871ms step_avg:33.21ms
step:238/1775 train_time:7905ms step_avg:33.22ms
step:239/1775 train_time:7936ms step_avg:33.21ms
step:240/1775 train_time:7970ms step_avg:33.21ms
step:241/1775 train_time:8001ms step_avg:33.20ms
step:242/1775 train_time:8034ms step_avg:33.20ms
step:243/1775 train_time:8065ms step_avg:33.19ms
step:244/1775 train_time:8099ms step_avg:33.19ms
step:245/1775 train_time:8130ms step_avg:33.18ms
step:246/1775 train_time:8163ms step_avg:33.18ms
step:247/1775 train_time:8194ms step_avg:33.17ms
step:248/1775 train_time:8228ms step_avg:33.18ms
step:249/1775 train_time:8259ms step_avg:33.17ms
step:250/1775 train_time:8292ms step_avg:33.17ms
step:250/1775 val_loss:4.6163 train_time:8339ms step_avg:33.35ms
step:251/1775 train_time:8356ms step_avg:33.29ms
step:252/1775 train_time:8373ms step_avg:33.23ms
step:253/1775 train_time:8392ms step_avg:33.17ms
step:254/1775 train_time:8427ms step_avg:33.18ms
step:255/1775 train_time:8458ms step_avg:33.17ms
step:256/1775 train_time:8493ms step_avg:33.18ms
step:257/1775 train_time:8525ms step_avg:33.17ms
step:258/1775 train_time:8560ms step_avg:33.18ms
step:259/1775 train_time:8591ms step_avg:33.17ms
step:260/1775 train_time:8625ms step_avg:33.17ms
step:261/1775 train_time:8656ms step_avg:33.16ms
step:262/1775 train_time:8690ms step_avg:33.17ms
step:263/1775 train_time:8721ms step_avg:33.16ms
step:264/1775 train_time:8755ms step_avg:33.16ms
step:265/1775 train_time:8786ms step_avg:33.15ms
step:266/1775 train_time:8820ms step_avg:33.16ms
step:267/1775 train_time:8850ms step_avg:33.15ms
step:268/1775 train_time:8884ms step_avg:33.15ms
step:269/1775 train_time:8914ms step_avg:33.14ms
step:270/1775 train_time:8948ms step_avg:33.14ms
step:271/1775 train_time:8979ms step_avg:33.13ms
step:272/1775 train_time:9012ms step_avg:33.13ms
step:273/1775 train_time:9043ms step_avg:33.12ms
step:274/1775 train_time:9076ms step_avg:33.12ms
step:275/1775 train_time:9107ms step_avg:33.11ms
step:276/1775 train_time:9140ms step_avg:33.12ms
step:277/1775 train_time:9171ms step_avg:33.11ms
step:278/1775 train_time:9205ms step_avg:33.11ms
step:279/1775 train_time:9235ms step_avg:33.10ms
step:280/1775 train_time:9270ms step_avg:33.11ms
step:281/1775 train_time:9301ms step_avg:33.10ms
step:282/1775 train_time:9335ms step_avg:33.10ms
step:283/1775 train_time:9367ms step_avg:33.10ms
step:284/1775 train_time:9401ms step_avg:33.10ms
step:285/1775 train_time:9433ms step_avg:33.10ms
step:286/1775 train_time:9467ms step_avg:33.10ms
step:287/1775 train_time:9499ms step_avg:33.10ms
step:288/1775 train_time:9533ms step_avg:33.10ms
step:289/1775 train_time:9565ms step_avg:33.10ms
step:290/1775 train_time:9599ms step_avg:33.10ms
step:291/1775 train_time:9630ms step_avg:33.09ms
step:292/1775 train_time:9664ms step_avg:33.10ms
step:293/1775 train_time:9696ms step_avg:33.09ms
step:294/1775 train_time:9730ms step_avg:33.10ms
step:295/1775 train_time:9761ms step_avg:33.09ms
step:296/1775 train_time:9795ms step_avg:33.09ms
step:297/1775 train_time:9826ms step_avg:33.08ms
step:298/1775 train_time:9860ms step_avg:33.09ms
step:299/1775 train_time:9891ms step_avg:33.08ms
step:300/1775 train_time:9925ms step_avg:33.08ms
step:301/1775 train_time:9956ms step_avg:33.08ms
step:302/1775 train_time:9990ms step_avg:33.08ms
step:303/1775 train_time:10021ms step_avg:33.07ms
step:304/1775 train_time:10055ms step_avg:33.07ms
step:305/1775 train_time:10086ms step_avg:33.07ms
step:306/1775 train_time:10119ms step_avg:33.07ms
step:307/1775 train_time:10150ms step_avg:33.06ms
step:308/1775 train_time:10184ms step_avg:33.06ms
step:309/1775 train_time:10214ms step_avg:33.06ms
step:310/1775 train_time:10248ms step_avg:33.06ms
step:311/1775 train_time:10279ms step_avg:33.05ms
step:312/1775 train_time:10313ms step_avg:33.06ms
step:313/1775 train_time:10344ms step_avg:33.05ms
step:314/1775 train_time:10378ms step_avg:33.05ms
step:315/1775 train_time:10410ms step_avg:33.05ms
step:316/1775 train_time:10444ms step_avg:33.05ms
step:317/1775 train_time:10475ms step_avg:33.04ms
step:318/1775 train_time:10509ms step_avg:33.05ms
step:319/1775 train_time:10541ms step_avg:33.04ms
step:320/1775 train_time:10575ms step_avg:33.05ms
step:321/1775 train_time:10607ms step_avg:33.04ms
step:322/1775 train_time:10641ms step_avg:33.05ms
step:323/1775 train_time:10672ms step_avg:33.04ms
step:324/1775 train_time:10706ms step_avg:33.04ms
step:325/1775 train_time:10737ms step_avg:33.04ms
step:326/1775 train_time:10772ms step_avg:33.04ms
step:327/1775 train_time:10803ms step_avg:33.04ms
step:328/1775 train_time:10837ms step_avg:33.04ms
step:329/1775 train_time:10867ms step_avg:33.03ms
step:330/1775 train_time:10901ms step_avg:33.03ms
step:331/1775 train_time:10931ms step_avg:33.03ms
step:332/1775 train_time:10965ms step_avg:33.03ms
step:333/1775 train_time:10996ms step_avg:33.02ms
step:334/1775 train_time:11031ms step_avg:33.03ms
step:335/1775 train_time:11061ms step_avg:33.02ms
step:336/1775 train_time:11095ms step_avg:33.02ms
step:337/1775 train_time:11126ms step_avg:33.01ms
step:338/1775 train_time:11160ms step_avg:33.02ms
step:339/1775 train_time:11191ms step_avg:33.01ms
step:340/1775 train_time:11224ms step_avg:33.01ms
step:341/1775 train_time:11255ms step_avg:33.01ms
step:342/1775 train_time:11289ms step_avg:33.01ms
step:343/1775 train_time:11320ms step_avg:33.00ms
step:344/1775 train_time:11354ms step_avg:33.01ms
step:345/1775 train_time:11385ms step_avg:33.00ms
step:346/1775 train_time:11419ms step_avg:33.00ms
step:347/1775 train_time:11450ms step_avg:33.00ms
step:348/1775 train_time:11484ms step_avg:33.00ms
step:349/1775 train_time:11516ms step_avg:33.00ms
step:350/1775 train_time:11550ms step_avg:33.00ms
step:351/1775 train_time:11582ms step_avg:33.00ms
step:352/1775 train_time:11616ms step_avg:33.00ms
step:353/1775 train_time:11647ms step_avg:33.00ms
step:354/1775 train_time:11682ms step_avg:33.00ms
step:355/1775 train_time:11713ms step_avg:32.99ms
step:356/1775 train_time:11747ms step_avg:33.00ms
step:357/1775 train_time:11778ms step_avg:32.99ms
step:358/1775 train_time:11812ms step_avg:33.00ms
step:359/1775 train_time:11843ms step_avg:32.99ms
step:360/1775 train_time:11877ms step_avg:32.99ms
step:361/1775 train_time:11908ms step_avg:32.99ms
step:362/1775 train_time:11942ms step_avg:32.99ms
step:363/1775 train_time:11972ms step_avg:32.98ms
step:364/1775 train_time:12006ms step_avg:32.98ms
step:365/1775 train_time:12037ms step_avg:32.98ms
step:366/1775 train_time:12071ms step_avg:32.98ms
step:367/1775 train_time:12102ms step_avg:32.98ms
step:368/1775 train_time:12136ms step_avg:32.98ms
step:369/1775 train_time:12167ms step_avg:32.97ms
step:370/1775 train_time:12201ms step_avg:32.98ms
step:371/1775 train_time:12232ms step_avg:32.97ms
step:372/1775 train_time:12266ms step_avg:32.97ms
step:373/1775 train_time:12297ms step_avg:32.97ms
step:374/1775 train_time:12331ms step_avg:32.97ms
step:375/1775 train_time:12362ms step_avg:32.97ms
step:376/1775 train_time:12396ms step_avg:32.97ms
step:377/1775 train_time:12428ms step_avg:32.96ms
step:378/1775 train_time:12462ms step_avg:32.97ms
step:379/1775 train_time:12493ms step_avg:32.96ms
step:380/1775 train_time:12527ms step_avg:32.97ms
step:381/1775 train_time:12558ms step_avg:32.96ms
step:382/1775 train_time:12592ms step_avg:32.96ms
step:383/1775 train_time:12623ms step_avg:32.96ms
step:384/1775 train_time:12657ms step_avg:32.96ms
step:385/1775 train_time:12688ms step_avg:32.96ms
step:386/1775 train_time:12722ms step_avg:32.96ms
step:387/1775 train_time:12753ms step_avg:32.95ms
step:388/1775 train_time:12787ms step_avg:32.96ms
step:389/1775 train_time:12818ms step_avg:32.95ms
step:390/1775 train_time:12852ms step_avg:32.95ms
step:391/1775 train_time:12883ms step_avg:32.95ms
step:392/1775 train_time:12918ms step_avg:32.95ms
step:393/1775 train_time:12949ms step_avg:32.95ms
step:394/1775 train_time:12983ms step_avg:32.95ms
step:395/1775 train_time:13014ms step_avg:32.95ms
step:396/1775 train_time:13049ms step_avg:32.95ms
step:397/1775 train_time:13079ms step_avg:32.95ms
step:398/1775 train_time:13113ms step_avg:32.95ms
step:399/1775 train_time:13143ms step_avg:32.94ms
step:400/1775 train_time:13177ms step_avg:32.94ms
step:401/1775 train_time:13208ms step_avg:32.94ms
step:402/1775 train_time:13242ms step_avg:32.94ms
step:403/1775 train_time:13273ms step_avg:32.94ms
step:404/1775 train_time:13307ms step_avg:32.94ms
step:405/1775 train_time:13337ms step_avg:32.93ms
step:406/1775 train_time:13371ms step_avg:32.93ms
step:407/1775 train_time:13402ms step_avg:32.93ms
step:408/1775 train_time:13435ms step_avg:32.93ms
step:409/1775 train_time:13466ms step_avg:32.92ms
step:410/1775 train_time:13501ms step_avg:32.93ms
step:411/1775 train_time:13532ms step_avg:32.92ms
step:412/1775 train_time:13566ms step_avg:32.93ms
step:413/1775 train_time:13597ms step_avg:32.92ms
step:414/1775 train_time:13631ms step_avg:32.93ms
step:415/1775 train_time:13662ms step_avg:32.92ms
step:416/1775 train_time:13696ms step_avg:32.92ms
step:417/1775 train_time:13728ms step_avg:32.92ms
step:418/1775 train_time:13761ms step_avg:32.92ms
step:419/1775 train_time:13792ms step_avg:32.92ms
step:420/1775 train_time:13827ms step_avg:32.92ms
step:421/1775 train_time:13858ms step_avg:32.92ms
step:422/1775 train_time:13892ms step_avg:32.92ms
step:423/1775 train_time:13923ms step_avg:32.91ms
step:424/1775 train_time:13957ms step_avg:32.92ms
step:425/1775 train_time:13988ms step_avg:32.91ms
step:426/1775 train_time:14021ms step_avg:32.91ms
step:427/1775 train_time:14053ms step_avg:32.91ms
step:428/1775 train_time:14087ms step_avg:32.91ms
step:429/1775 train_time:14118ms step_avg:32.91ms
step:430/1775 train_time:14152ms step_avg:32.91ms
step:431/1775 train_time:14183ms step_avg:32.91ms
step:432/1775 train_time:14217ms step_avg:32.91ms
step:433/1775 train_time:14248ms step_avg:32.90ms
step:434/1775 train_time:14281ms step_avg:32.91ms
step:435/1775 train_time:14312ms step_avg:32.90ms
step:436/1775 train_time:14346ms step_avg:32.90ms
step:437/1775 train_time:14377ms step_avg:32.90ms
step:438/1775 train_time:14411ms step_avg:32.90ms
step:439/1775 train_time:14442ms step_avg:32.90ms
step:440/1775 train_time:14476ms step_avg:32.90ms
step:441/1775 train_time:14507ms step_avg:32.89ms
step:442/1775 train_time:14541ms step_avg:32.90ms
step:443/1775 train_time:14572ms step_avg:32.89ms
step:444/1775 train_time:14607ms step_avg:32.90ms
step:445/1775 train_time:14638ms step_avg:32.89ms
step:446/1775 train_time:14672ms step_avg:32.90ms
step:447/1775 train_time:14703ms step_avg:32.89ms
step:448/1775 train_time:14737ms step_avg:32.89ms
step:449/1775 train_time:14768ms step_avg:32.89ms
step:450/1775 train_time:14802ms step_avg:32.89ms
step:451/1775 train_time:14834ms step_avg:32.89ms
step:452/1775 train_time:14868ms step_avg:32.89ms
step:453/1775 train_time:14899ms step_avg:32.89ms
step:454/1775 train_time:14933ms step_avg:32.89ms
step:455/1775 train_time:14964ms step_avg:32.89ms
step:456/1775 train_time:14998ms step_avg:32.89ms
step:457/1775 train_time:15029ms step_avg:32.89ms
step:458/1775 train_time:15063ms step_avg:32.89ms
step:459/1775 train_time:15095ms step_avg:32.89ms
step:460/1775 train_time:15129ms step_avg:32.89ms
step:461/1775 train_time:15160ms step_avg:32.88ms
step:462/1775 train_time:15194ms step_avg:32.89ms
step:463/1775 train_time:15225ms step_avg:32.88ms
step:464/1775 train_time:15259ms step_avg:32.89ms
step:465/1775 train_time:15290ms step_avg:32.88ms
step:466/1775 train_time:15324ms step_avg:32.88ms
step:467/1775 train_time:15355ms step_avg:32.88ms
step:468/1775 train_time:15389ms step_avg:32.88ms
step:469/1775 train_time:15420ms step_avg:32.88ms
step:470/1775 train_time:15453ms step_avg:32.88ms
step:471/1775 train_time:15485ms step_avg:32.88ms
step:472/1775 train_time:15519ms step_avg:32.88ms
step:473/1775 train_time:15549ms step_avg:32.87ms
step:474/1775 train_time:15583ms step_avg:32.88ms
step:475/1775 train_time:15614ms step_avg:32.87ms
step:476/1775 train_time:15648ms step_avg:32.87ms
step:477/1775 train_time:15680ms step_avg:32.87ms
step:478/1775 train_time:15714ms step_avg:32.87ms
step:479/1775 train_time:15745ms step_avg:32.87ms
step:480/1775 train_time:15779ms step_avg:32.87ms
step:481/1775 train_time:15810ms step_avg:32.87ms
step:482/1775 train_time:15843ms step_avg:32.87ms
step:483/1775 train_time:15874ms step_avg:32.87ms
step:484/1775 train_time:15909ms step_avg:32.87ms
step:485/1775 train_time:15940ms step_avg:32.87ms
step:486/1775 train_time:15974ms step_avg:32.87ms
step:487/1775 train_time:16005ms step_avg:32.86ms
step:488/1775 train_time:16039ms step_avg:32.87ms
step:489/1775 train_time:16070ms step_avg:32.86ms
step:490/1775 train_time:16105ms step_avg:32.87ms
step:491/1775 train_time:16135ms step_avg:32.86ms
step:492/1775 train_time:16169ms step_avg:32.86ms
step:493/1775 train_time:16200ms step_avg:32.86ms
step:494/1775 train_time:16234ms step_avg:32.86ms
step:495/1775 train_time:16264ms step_avg:32.86ms
step:496/1775 train_time:16298ms step_avg:32.86ms
step:497/1775 train_time:16329ms step_avg:32.86ms
step:498/1775 train_time:16363ms step_avg:32.86ms
step:499/1775 train_time:16394ms step_avg:32.85ms
step:500/1775 train_time:16429ms step_avg:32.86ms
step:500/1775 val_loss:4.2699 train_time:16474ms step_avg:32.95ms
step:501/1775 train_time:16497ms step_avg:32.93ms
step:502/1775 train_time:16515ms step_avg:32.90ms
step:503/1775 train_time:16531ms step_avg:32.86ms
step:504/1775 train_time:16561ms step_avg:32.86ms
step:505/1775 train_time:16594ms step_avg:32.86ms
step:506/1775 train_time:16629ms step_avg:32.86ms
step:507/1775 train_time:16661ms step_avg:32.86ms
step:508/1775 train_time:16695ms step_avg:32.86ms
step:509/1775 train_time:16726ms step_avg:32.86ms
step:510/1775 train_time:16760ms step_avg:32.86ms
step:511/1775 train_time:16791ms step_avg:32.86ms
step:512/1775 train_time:16825ms step_avg:32.86ms
step:513/1775 train_time:16855ms step_avg:32.86ms
step:514/1775 train_time:16889ms step_avg:32.86ms
step:515/1775 train_time:16920ms step_avg:32.85ms
step:516/1775 train_time:16954ms step_avg:32.86ms
step:517/1775 train_time:16984ms step_avg:32.85ms
step:518/1775 train_time:17018ms step_avg:32.85ms
step:519/1775 train_time:17049ms step_avg:32.85ms
step:520/1775 train_time:17082ms step_avg:32.85ms
step:521/1775 train_time:17113ms step_avg:32.85ms
step:522/1775 train_time:17146ms step_avg:32.85ms
step:523/1775 train_time:17177ms step_avg:32.84ms
step:524/1775 train_time:17211ms step_avg:32.84ms
step:525/1775 train_time:17241ms step_avg:32.84ms
step:526/1775 train_time:17275ms step_avg:32.84ms
step:527/1775 train_time:17306ms step_avg:32.84ms
step:528/1775 train_time:17340ms step_avg:32.84ms
step:529/1775 train_time:17371ms step_avg:32.84ms
step:530/1775 train_time:17405ms step_avg:32.84ms
step:531/1775 train_time:17436ms step_avg:32.84ms
step:532/1775 train_time:17470ms step_avg:32.84ms
step:533/1775 train_time:17501ms step_avg:32.84ms
step:534/1775 train_time:17537ms step_avg:32.84ms
step:535/1775 train_time:17568ms step_avg:32.84ms
step:536/1775 train_time:17602ms step_avg:32.84ms
step:537/1775 train_time:17634ms step_avg:32.84ms
step:538/1775 train_time:17668ms step_avg:32.84ms
step:539/1775 train_time:17699ms step_avg:32.84ms
step:540/1775 train_time:17733ms step_avg:32.84ms
step:541/1775 train_time:17765ms step_avg:32.84ms
step:542/1775 train_time:17798ms step_avg:32.84ms
step:543/1775 train_time:17829ms step_avg:32.83ms
step:544/1775 train_time:17863ms step_avg:32.84ms
step:545/1775 train_time:17893ms step_avg:32.83ms
step:546/1775 train_time:17927ms step_avg:32.83ms
step:547/1775 train_time:17958ms step_avg:32.83ms
step:548/1775 train_time:17991ms step_avg:32.83ms
step:549/1775 train_time:18023ms step_avg:32.83ms
step:550/1775 train_time:18057ms step_avg:32.83ms
step:551/1775 train_time:18087ms step_avg:32.83ms
step:552/1775 train_time:18121ms step_avg:32.83ms
step:553/1775 train_time:18152ms step_avg:32.83ms
step:554/1775 train_time:18186ms step_avg:32.83ms
step:555/1775 train_time:18217ms step_avg:32.82ms
step:556/1775 train_time:18251ms step_avg:32.83ms
step:557/1775 train_time:18282ms step_avg:32.82ms
step:558/1775 train_time:18316ms step_avg:32.82ms
step:559/1775 train_time:18347ms step_avg:32.82ms
step:560/1775 train_time:18381ms step_avg:32.82ms
step:561/1775 train_time:18412ms step_avg:32.82ms
step:562/1775 train_time:18446ms step_avg:32.82ms
step:563/1775 train_time:18477ms step_avg:32.82ms
step:564/1775 train_time:18511ms step_avg:32.82ms
step:565/1775 train_time:18542ms step_avg:32.82ms
step:566/1775 train_time:18576ms step_avg:32.82ms
step:567/1775 train_time:18608ms step_avg:32.82ms
step:568/1775 train_time:18641ms step_avg:32.82ms
step:569/1775 train_time:18672ms step_avg:32.82ms
step:570/1775 train_time:18707ms step_avg:32.82ms
step:571/1775 train_time:18738ms step_avg:32.82ms
step:572/1775 train_time:18772ms step_avg:32.82ms
step:573/1775 train_time:18804ms step_avg:32.82ms
step:574/1775 train_time:18838ms step_avg:32.82ms
step:575/1775 train_time:18869ms step_avg:32.82ms
step:576/1775 train_time:18903ms step_avg:32.82ms
step:577/1775 train_time:18934ms step_avg:32.81ms
step:578/1775 train_time:18967ms step_avg:32.82ms
step:579/1775 train_time:18998ms step_avg:32.81ms
step:580/1775 train_time:19035ms step_avg:32.82ms
step:581/1775 train_time:19090ms step_avg:32.86ms
step:582/1775 train_time:19150ms step_avg:32.90ms
step:583/1775 train_time:19207ms step_avg:32.95ms
step:584/1775 train_time:19269ms step_avg:33.00ms
step:585/1775 train_time:19327ms step_avg:33.04ms
step:586/1775 train_time:19388ms step_avg:33.09ms
step:587/1775 train_time:19447ms step_avg:33.13ms
step:588/1775 train_time:19508ms step_avg:33.18ms
step:589/1775 train_time:19567ms step_avg:33.22ms
step:590/1775 train_time:19628ms step_avg:33.27ms
step:591/1775 train_time:19687ms step_avg:33.31ms
step:592/1775 train_time:19749ms step_avg:33.36ms
step:593/1775 train_time:19807ms step_avg:33.40ms
step:594/1775 train_time:19868ms step_avg:33.45ms
step:595/1775 train_time:19927ms step_avg:33.49ms
step:596/1775 train_time:19988ms step_avg:33.54ms
step:597/1775 train_time:20046ms step_avg:33.58ms
step:598/1775 train_time:20107ms step_avg:33.62ms
step:599/1775 train_time:20164ms step_avg:33.66ms
step:600/1775 train_time:20225ms step_avg:33.71ms
step:601/1775 train_time:20282ms step_avg:33.75ms
step:602/1775 train_time:20343ms step_avg:33.79ms
step:603/1775 train_time:20402ms step_avg:33.83ms
step:604/1775 train_time:20463ms step_avg:33.88ms
step:605/1775 train_time:20521ms step_avg:33.92ms
step:606/1775 train_time:20583ms step_avg:33.96ms
step:607/1775 train_time:20641ms step_avg:34.01ms
step:608/1775 train_time:20704ms step_avg:34.05ms
step:609/1775 train_time:20761ms step_avg:34.09ms
step:610/1775 train_time:20822ms step_avg:34.14ms
step:611/1775 train_time:20881ms step_avg:34.18ms
step:612/1775 train_time:20943ms step_avg:34.22ms
step:613/1775 train_time:21001ms step_avg:34.26ms
step:614/1775 train_time:21062ms step_avg:34.30ms
step:615/1775 train_time:21119ms step_avg:34.34ms
step:616/1775 train_time:21180ms step_avg:34.38ms
step:617/1775 train_time:21238ms step_avg:34.42ms
step:618/1775 train_time:21298ms step_avg:34.46ms
step:619/1775 train_time:21357ms step_avg:34.50ms
step:620/1775 train_time:21417ms step_avg:34.54ms
step:621/1775 train_time:21475ms step_avg:34.58ms
step:622/1775 train_time:21536ms step_avg:34.62ms
step:623/1775 train_time:21595ms step_avg:34.66ms
step:624/1775 train_time:21656ms step_avg:34.70ms
step:625/1775 train_time:21714ms step_avg:34.74ms
step:626/1775 train_time:21776ms step_avg:34.79ms
step:627/1775 train_time:21834ms step_avg:34.82ms
step:628/1775 train_time:21895ms step_avg:34.87ms
step:629/1775 train_time:21954ms step_avg:34.90ms
step:630/1775 train_time:22015ms step_avg:34.94ms
step:631/1775 train_time:22073ms step_avg:34.98ms
step:632/1775 train_time:22133ms step_avg:35.02ms
step:633/1775 train_time:22191ms step_avg:35.06ms
step:634/1775 train_time:22252ms step_avg:35.10ms
step:635/1775 train_time:22309ms step_avg:35.13ms
step:636/1775 train_time:22371ms step_avg:35.17ms
step:637/1775 train_time:22429ms step_avg:35.21ms
step:638/1775 train_time:22491ms step_avg:35.25ms
step:639/1775 train_time:22549ms step_avg:35.29ms
step:640/1775 train_time:22610ms step_avg:35.33ms
step:641/1775 train_time:22669ms step_avg:35.36ms
step:642/1775 train_time:22729ms step_avg:35.40ms
step:643/1775 train_time:22788ms step_avg:35.44ms
step:644/1775 train_time:22849ms step_avg:35.48ms
step:645/1775 train_time:22906ms step_avg:35.51ms
step:646/1775 train_time:22967ms step_avg:35.55ms
step:647/1775 train_time:23026ms step_avg:35.59ms
step:648/1775 train_time:23086ms step_avg:35.63ms
step:649/1775 train_time:23144ms step_avg:35.66ms
step:650/1775 train_time:23205ms step_avg:35.70ms
step:651/1775 train_time:23262ms step_avg:35.73ms
step:652/1775 train_time:23323ms step_avg:35.77ms
step:653/1775 train_time:23381ms step_avg:35.81ms
step:654/1775 train_time:23443ms step_avg:35.85ms
step:655/1775 train_time:23501ms step_avg:35.88ms
step:656/1775 train_time:23562ms step_avg:35.92ms
step:657/1775 train_time:23620ms step_avg:35.95ms
step:658/1775 train_time:23681ms step_avg:35.99ms
step:659/1775 train_time:23739ms step_avg:36.02ms
step:660/1775 train_time:23801ms step_avg:36.06ms
step:661/1775 train_time:23859ms step_avg:36.09ms
step:662/1775 train_time:23920ms step_avg:36.13ms
step:663/1775 train_time:23978ms step_avg:36.17ms
step:664/1775 train_time:24039ms step_avg:36.20ms
step:665/1775 train_time:24097ms step_avg:36.24ms
step:666/1775 train_time:24158ms step_avg:36.27ms
step:667/1775 train_time:24216ms step_avg:36.31ms
step:668/1775 train_time:24277ms step_avg:36.34ms
step:669/1775 train_time:24336ms step_avg:36.38ms
step:670/1775 train_time:24396ms step_avg:36.41ms
step:671/1775 train_time:24454ms step_avg:36.44ms
step:672/1775 train_time:24515ms step_avg:36.48ms
step:673/1775 train_time:24573ms step_avg:36.51ms
step:674/1775 train_time:24634ms step_avg:36.55ms
step:675/1775 train_time:24692ms step_avg:36.58ms
step:676/1775 train_time:24754ms step_avg:36.62ms
step:677/1775 train_time:24811ms step_avg:36.65ms
step:678/1775 train_time:24872ms step_avg:36.68ms
step:679/1775 train_time:24931ms step_avg:36.72ms
step:680/1775 train_time:24991ms step_avg:36.75ms
step:681/1775 train_time:25049ms step_avg:36.78ms
step:682/1775 train_time:25110ms step_avg:36.82ms
step:683/1775 train_time:25168ms step_avg:36.85ms
step:684/1775 train_time:25229ms step_avg:36.89ms
step:685/1775 train_time:25287ms step_avg:36.92ms
step:686/1775 train_time:25349ms step_avg:36.95ms
step:687/1775 train_time:25407ms step_avg:36.98ms
step:688/1775 train_time:25468ms step_avg:37.02ms
step:689/1775 train_time:25527ms step_avg:37.05ms
step:690/1775 train_time:25588ms step_avg:37.08ms
step:691/1775 train_time:25646ms step_avg:37.11ms
step:692/1775 train_time:25708ms step_avg:37.15ms
step:693/1775 train_time:25766ms step_avg:37.18ms
step:694/1775 train_time:25826ms step_avg:37.21ms
step:695/1775 train_time:25885ms step_avg:37.24ms
step:696/1775 train_time:25946ms step_avg:37.28ms
step:697/1775 train_time:26004ms step_avg:37.31ms
step:698/1775 train_time:26064ms step_avg:37.34ms
step:699/1775 train_time:26121ms step_avg:37.37ms
step:700/1775 train_time:26182ms step_avg:37.40ms
step:701/1775 train_time:26240ms step_avg:37.43ms
step:702/1775 train_time:26302ms step_avg:37.47ms
step:703/1775 train_time:26359ms step_avg:37.50ms
step:704/1775 train_time:26420ms step_avg:37.53ms
step:705/1775 train_time:26478ms step_avg:37.56ms
step:706/1775 train_time:26539ms step_avg:37.59ms
step:707/1775 train_time:26596ms step_avg:37.62ms
step:708/1775 train_time:26657ms step_avg:37.65ms
step:709/1775 train_time:26716ms step_avg:37.68ms
step:710/1775 train_time:26777ms step_avg:37.71ms
step:711/1775 train_time:26835ms step_avg:37.74ms
step:712/1775 train_time:26895ms step_avg:37.77ms
step:713/1775 train_time:26953ms step_avg:37.80ms
step:714/1775 train_time:27014ms step_avg:37.83ms
step:715/1775 train_time:27072ms step_avg:37.86ms
step:716/1775 train_time:27133ms step_avg:37.90ms
step:717/1775 train_time:27191ms step_avg:37.92ms
step:718/1775 train_time:27253ms step_avg:37.96ms
step:719/1775 train_time:27311ms step_avg:37.99ms
step:720/1775 train_time:27372ms step_avg:38.02ms
step:721/1775 train_time:27431ms step_avg:38.05ms
step:722/1775 train_time:27493ms step_avg:38.08ms
step:723/1775 train_time:27551ms step_avg:38.11ms
step:724/1775 train_time:27612ms step_avg:38.14ms
step:725/1775 train_time:27670ms step_avg:38.17ms
step:726/1775 train_time:27732ms step_avg:38.20ms
step:727/1775 train_time:27789ms step_avg:38.22ms
step:728/1775 train_time:27851ms step_avg:38.26ms
step:729/1775 train_time:27908ms step_avg:38.28ms
step:730/1775 train_time:27970ms step_avg:38.32ms
step:731/1775 train_time:28028ms step_avg:38.34ms
step:732/1775 train_time:28089ms step_avg:38.37ms
step:733/1775 train_time:28148ms step_avg:38.40ms
step:734/1775 train_time:28208ms step_avg:38.43ms
step:735/1775 train_time:28266ms step_avg:38.46ms
step:736/1775 train_time:28327ms step_avg:38.49ms
step:737/1775 train_time:28386ms step_avg:38.52ms
step:738/1775 train_time:28448ms step_avg:38.55ms
step:739/1775 train_time:28507ms step_avg:38.57ms
step:740/1775 train_time:28568ms step_avg:38.61ms
step:741/1775 train_time:28626ms step_avg:38.63ms
step:742/1775 train_time:28687ms step_avg:38.66ms
step:743/1775 train_time:28745ms step_avg:38.69ms
step:744/1775 train_time:28805ms step_avg:38.72ms
step:745/1775 train_time:28863ms step_avg:38.74ms
step:746/1775 train_time:28924ms step_avg:38.77ms
step:747/1775 train_time:28982ms step_avg:38.80ms
step:748/1775 train_time:29043ms step_avg:38.83ms
step:749/1775 train_time:29101ms step_avg:38.85ms
step:750/1775 train_time:29162ms step_avg:38.88ms
step:750/1775 val_loss:3.9944 train_time:29240ms step_avg:38.99ms
step:751/1775 train_time:29261ms step_avg:38.96ms
step:752/1775 train_time:29283ms step_avg:38.94ms
step:753/1775 train_time:29340ms step_avg:38.96ms
step:754/1775 train_time:29404ms step_avg:39.00ms
step:755/1775 train_time:29465ms step_avg:39.03ms
step:756/1775 train_time:29527ms step_avg:39.06ms
step:757/1775 train_time:29585ms step_avg:39.08ms
step:758/1775 train_time:29646ms step_avg:39.11ms
step:759/1775 train_time:29703ms step_avg:39.13ms
step:760/1775 train_time:29764ms step_avg:39.16ms
step:761/1775 train_time:29821ms step_avg:39.19ms
step:762/1775 train_time:29882ms step_avg:39.22ms
step:763/1775 train_time:29939ms step_avg:39.24ms
step:764/1775 train_time:30000ms step_avg:39.27ms
step:765/1775 train_time:30057ms step_avg:39.29ms
step:766/1775 train_time:30117ms step_avg:39.32ms
step:767/1775 train_time:30175ms step_avg:39.34ms
step:768/1775 train_time:30238ms step_avg:39.37ms
step:769/1775 train_time:30297ms step_avg:39.40ms
step:770/1775 train_time:30358ms step_avg:39.43ms
step:771/1775 train_time:30418ms step_avg:39.45ms
step:772/1775 train_time:30479ms step_avg:39.48ms
step:773/1775 train_time:30538ms step_avg:39.51ms
step:774/1775 train_time:30599ms step_avg:39.53ms
step:775/1775 train_time:30657ms step_avg:39.56ms
step:776/1775 train_time:30718ms step_avg:39.58ms
step:777/1775 train_time:30775ms step_avg:39.61ms
step:778/1775 train_time:30835ms step_avg:39.63ms
step:779/1775 train_time:30893ms step_avg:39.66ms
step:780/1775 train_time:30953ms step_avg:39.68ms
step:781/1775 train_time:31011ms step_avg:39.71ms
step:782/1775 train_time:31072ms step_avg:39.73ms
step:783/1775 train_time:31130ms step_avg:39.76ms
step:784/1775 train_time:31191ms step_avg:39.78ms
step:785/1775 train_time:31250ms step_avg:39.81ms
step:786/1775 train_time:31313ms step_avg:39.84ms
step:787/1775 train_time:31371ms step_avg:39.86ms
step:788/1775 train_time:31433ms step_avg:39.89ms
step:789/1775 train_time:31491ms step_avg:39.91ms
step:790/1775 train_time:31553ms step_avg:39.94ms
step:791/1775 train_time:31611ms step_avg:39.96ms
step:792/1775 train_time:31672ms step_avg:39.99ms
step:793/1775 train_time:31729ms step_avg:40.01ms
step:794/1775 train_time:31790ms step_avg:40.04ms
step:795/1775 train_time:31847ms step_avg:40.06ms
step:796/1775 train_time:31908ms step_avg:40.09ms
step:797/1775 train_time:31966ms step_avg:40.11ms
step:798/1775 train_time:32027ms step_avg:40.13ms
step:799/1775 train_time:32084ms step_avg:40.16ms
step:800/1775 train_time:32146ms step_avg:40.18ms
step:801/1775 train_time:32204ms step_avg:40.20ms
step:802/1775 train_time:32266ms step_avg:40.23ms
step:803/1775 train_time:32325ms step_avg:40.26ms
step:804/1775 train_time:32386ms step_avg:40.28ms
step:805/1775 train_time:32445ms step_avg:40.30ms
step:806/1775 train_time:32507ms step_avg:40.33ms
step:807/1775 train_time:32564ms step_avg:40.35ms
step:808/1775 train_time:32625ms step_avg:40.38ms
step:809/1775 train_time:32683ms step_avg:40.40ms
step:810/1775 train_time:32744ms step_avg:40.42ms
step:811/1775 train_time:32802ms step_avg:40.45ms
step:812/1775 train_time:32863ms step_avg:40.47ms
step:813/1775 train_time:32920ms step_avg:40.49ms
step:814/1775 train_time:32981ms step_avg:40.52ms
step:815/1775 train_time:33038ms step_avg:40.54ms
step:816/1775 train_time:33099ms step_avg:40.56ms
step:817/1775 train_time:33157ms step_avg:40.58ms
step:818/1775 train_time:33219ms step_avg:40.61ms
step:819/1775 train_time:33277ms step_avg:40.63ms
step:820/1775 train_time:33338ms step_avg:40.66ms
step:821/1775 train_time:33396ms step_avg:40.68ms
step:822/1775 train_time:33458ms step_avg:40.70ms
step:823/1775 train_time:33516ms step_avg:40.72ms
step:824/1775 train_time:33576ms step_avg:40.75ms
step:825/1775 train_time:33634ms step_avg:40.77ms
step:826/1775 train_time:33695ms step_avg:40.79ms
step:827/1775 train_time:33753ms step_avg:40.81ms
step:828/1775 train_time:33814ms step_avg:40.84ms
step:829/1775 train_time:33872ms step_avg:40.86ms
step:830/1775 train_time:33932ms step_avg:40.88ms
step:831/1775 train_time:33989ms step_avg:40.90ms
step:832/1775 train_time:34051ms step_avg:40.93ms
step:833/1775 train_time:34110ms step_avg:40.95ms
step:834/1775 train_time:34171ms step_avg:40.97ms
step:835/1775 train_time:34230ms step_avg:40.99ms
step:836/1775 train_time:34291ms step_avg:41.02ms
step:837/1775 train_time:34349ms step_avg:41.04ms
step:838/1775 train_time:34411ms step_avg:41.06ms
step:839/1775 train_time:34469ms step_avg:41.08ms
step:840/1775 train_time:34530ms step_avg:41.11ms
step:841/1775 train_time:34588ms step_avg:41.13ms
step:842/1775 train_time:34650ms step_avg:41.15ms
step:843/1775 train_time:34708ms step_avg:41.17ms
step:844/1775 train_time:34769ms step_avg:41.20ms
step:845/1775 train_time:34826ms step_avg:41.21ms
step:846/1775 train_time:34888ms step_avg:41.24ms
step:847/1775 train_time:34946ms step_avg:41.26ms
step:848/1775 train_time:35007ms step_avg:41.28ms
step:849/1775 train_time:35065ms step_avg:41.30ms
step:850/1775 train_time:35126ms step_avg:41.33ms
step:851/1775 train_time:35186ms step_avg:41.35ms
step:852/1775 train_time:35247ms step_avg:41.37ms
step:853/1775 train_time:35305ms step_avg:41.39ms
step:854/1775 train_time:35366ms step_avg:41.41ms
step:855/1775 train_time:35425ms step_avg:41.43ms
step:856/1775 train_time:35487ms step_avg:41.46ms
step:857/1775 train_time:35546ms step_avg:41.48ms
step:858/1775 train_time:35607ms step_avg:41.50ms
step:859/1775 train_time:35666ms step_avg:41.52ms
step:860/1775 train_time:35727ms step_avg:41.54ms
step:861/1775 train_time:35786ms step_avg:41.56ms
step:862/1775 train_time:35846ms step_avg:41.58ms
step:863/1775 train_time:35904ms step_avg:41.60ms
step:864/1775 train_time:35966ms step_avg:41.63ms
step:865/1775 train_time:36024ms step_avg:41.65ms
step:866/1775 train_time:36084ms step_avg:41.67ms
step:867/1775 train_time:36142ms step_avg:41.69ms
step:868/1775 train_time:36204ms step_avg:41.71ms
step:869/1775 train_time:36262ms step_avg:41.73ms
step:870/1775 train_time:36323ms step_avg:41.75ms
step:871/1775 train_time:36381ms step_avg:41.77ms
step:872/1775 train_time:36442ms step_avg:41.79ms
step:873/1775 train_time:36500ms step_avg:41.81ms
step:874/1775 train_time:36562ms step_avg:41.83ms
step:875/1775 train_time:36621ms step_avg:41.85ms
step:876/1775 train_time:36682ms step_avg:41.87ms
step:877/1775 train_time:36739ms step_avg:41.89ms
step:878/1775 train_time:36801ms step_avg:41.91ms
step:879/1775 train_time:36858ms step_avg:41.93ms
step:880/1775 train_time:36920ms step_avg:41.95ms
step:881/1775 train_time:36978ms step_avg:41.97ms
step:882/1775 train_time:37038ms step_avg:41.99ms
step:883/1775 train_time:37096ms step_avg:42.01ms
step:884/1775 train_time:37157ms step_avg:42.03ms
step:885/1775 train_time:37215ms step_avg:42.05ms
step:886/1775 train_time:37276ms step_avg:42.07ms
step:887/1775 train_time:37334ms step_avg:42.09ms
step:888/1775 train_time:37395ms step_avg:42.11ms
step:889/1775 train_time:37453ms step_avg:42.13ms
step:890/1775 train_time:37515ms step_avg:42.15ms
step:891/1775 train_time:37572ms step_avg:42.17ms
step:892/1775 train_time:37633ms step_avg:42.19ms
step:893/1775 train_time:37692ms step_avg:42.21ms
step:894/1775 train_time:37753ms step_avg:42.23ms
step:895/1775 train_time:37811ms step_avg:42.25ms
step:896/1775 train_time:37871ms step_avg:42.27ms
step:897/1775 train_time:37929ms step_avg:42.28ms
step:898/1775 train_time:37990ms step_avg:42.31ms
step:899/1775 train_time:38048ms step_avg:42.32ms
step:900/1775 train_time:38110ms step_avg:42.34ms
step:901/1775 train_time:38167ms step_avg:42.36ms
step:902/1775 train_time:38228ms step_avg:42.38ms
step:903/1775 train_time:38287ms step_avg:42.40ms
step:904/1775 train_time:38348ms step_avg:42.42ms
step:905/1775 train_time:38407ms step_avg:42.44ms
step:906/1775 train_time:38468ms step_avg:42.46ms
step:907/1775 train_time:38526ms step_avg:42.48ms
step:908/1775 train_time:38587ms step_avg:42.50ms
step:909/1775 train_time:38645ms step_avg:42.51ms
step:910/1775 train_time:38706ms step_avg:42.53ms
step:911/1775 train_time:38765ms step_avg:42.55ms
step:912/1775 train_time:38826ms step_avg:42.57ms
step:913/1775 train_time:38884ms step_avg:42.59ms
step:914/1775 train_time:38944ms step_avg:42.61ms
step:915/1775 train_time:39003ms step_avg:42.63ms
step:916/1775 train_time:39063ms step_avg:42.65ms
step:917/1775 train_time:39122ms step_avg:42.66ms
step:918/1775 train_time:39183ms step_avg:42.68ms
step:919/1775 train_time:39241ms step_avg:42.70ms
step:920/1775 train_time:39302ms step_avg:42.72ms
step:921/1775 train_time:39361ms step_avg:42.74ms
step:922/1775 train_time:39423ms step_avg:42.76ms
step:923/1775 train_time:39481ms step_avg:42.77ms
step:924/1775 train_time:39541ms step_avg:42.79ms
step:925/1775 train_time:39600ms step_avg:42.81ms
step:926/1775 train_time:39663ms step_avg:42.83ms
step:927/1775 train_time:39720ms step_avg:42.85ms
step:928/1775 train_time:39781ms step_avg:42.87ms
step:929/1775 train_time:39839ms step_avg:42.88ms
step:930/1775 train_time:39900ms step_avg:42.90ms
step:931/1775 train_time:39958ms step_avg:42.92ms
step:932/1775 train_time:40019ms step_avg:42.94ms
step:933/1775 train_time:40077ms step_avg:42.95ms
step:934/1775 train_time:40138ms step_avg:42.97ms
step:935/1775 train_time:40195ms step_avg:42.99ms
step:936/1775 train_time:40256ms step_avg:43.01ms
step:937/1775 train_time:40315ms step_avg:43.03ms
step:938/1775 train_time:40376ms step_avg:43.04ms
step:939/1775 train_time:40434ms step_avg:43.06ms
step:940/1775 train_time:40495ms step_avg:43.08ms
step:941/1775 train_time:40553ms step_avg:43.10ms
step:942/1775 train_time:40615ms step_avg:43.12ms
step:943/1775 train_time:40673ms step_avg:43.13ms
step:944/1775 train_time:40733ms step_avg:43.15ms
step:945/1775 train_time:40792ms step_avg:43.17ms
step:946/1775 train_time:40852ms step_avg:43.18ms
step:947/1775 train_time:40911ms step_avg:43.20ms
step:948/1775 train_time:40972ms step_avg:43.22ms
step:949/1775 train_time:41030ms step_avg:43.23ms
step:950/1775 train_time:41090ms step_avg:43.25ms
step:951/1775 train_time:41148ms step_avg:43.27ms
step:952/1775 train_time:41210ms step_avg:43.29ms
step:953/1775 train_time:41267ms step_avg:43.30ms
step:954/1775 train_time:41328ms step_avg:43.32ms
step:955/1775 train_time:41387ms step_avg:43.34ms
step:956/1775 train_time:41448ms step_avg:43.36ms
step:957/1775 train_time:41507ms step_avg:43.37ms
step:958/1775 train_time:41569ms step_avg:43.39ms
step:959/1775 train_time:41627ms step_avg:43.41ms
step:960/1775 train_time:41688ms step_avg:43.43ms
step:961/1775 train_time:41746ms step_avg:43.44ms
step:962/1775 train_time:41808ms step_avg:43.46ms
step:963/1775 train_time:41867ms step_avg:43.48ms
step:964/1775 train_time:41928ms step_avg:43.49ms
step:965/1775 train_time:41986ms step_avg:43.51ms
step:966/1775 train_time:42047ms step_avg:43.53ms
step:967/1775 train_time:42106ms step_avg:43.54ms
step:968/1775 train_time:42167ms step_avg:43.56ms
step:969/1775 train_time:42225ms step_avg:43.58ms
step:970/1775 train_time:42286ms step_avg:43.59ms
step:971/1775 train_time:42344ms step_avg:43.61ms
step:972/1775 train_time:42406ms step_avg:43.63ms
step:973/1775 train_time:42464ms step_avg:43.64ms
step:974/1775 train_time:42525ms step_avg:43.66ms
step:975/1775 train_time:42584ms step_avg:43.68ms
step:976/1775 train_time:42645ms step_avg:43.69ms
step:977/1775 train_time:42703ms step_avg:43.71ms
step:978/1775 train_time:42764ms step_avg:43.73ms
step:979/1775 train_time:42823ms step_avg:43.74ms
step:980/1775 train_time:42883ms step_avg:43.76ms
step:981/1775 train_time:42941ms step_avg:43.77ms
step:982/1775 train_time:43003ms step_avg:43.79ms
step:983/1775 train_time:43061ms step_avg:43.81ms
step:984/1775 train_time:43122ms step_avg:43.82ms
step:985/1775 train_time:43180ms step_avg:43.84ms
step:986/1775 train_time:43241ms step_avg:43.86ms
step:987/1775 train_time:43299ms step_avg:43.87ms
step:988/1775 train_time:43361ms step_avg:43.89ms
step:989/1775 train_time:43420ms step_avg:43.90ms
step:990/1775 train_time:43481ms step_avg:43.92ms
step:991/1775 train_time:43539ms step_avg:43.93ms
step:992/1775 train_time:43601ms step_avg:43.95ms
step:993/1775 train_time:43659ms step_avg:43.97ms
step:994/1775 train_time:43721ms step_avg:43.98ms
step:995/1775 train_time:43779ms step_avg:44.00ms
step:996/1775 train_time:43841ms step_avg:44.02ms
step:997/1775 train_time:43899ms step_avg:44.03ms
step:998/1775 train_time:43960ms step_avg:44.05ms
step:999/1775 train_time:44018ms step_avg:44.06ms
step:1000/1775 train_time:44079ms step_avg:44.08ms
step:1000/1775 val_loss:3.7232 train_time:44156ms step_avg:44.16ms
step:1001/1775 train_time:44175ms step_avg:44.13ms
step:1002/1775 train_time:44199ms step_avg:44.11ms
step:1003/1775 train_time:44259ms step_avg:44.13ms
step:1004/1775 train_time:44322ms step_avg:44.15ms
step:1005/1775 train_time:44380ms step_avg:44.16ms
step:1006/1775 train_time:44441ms step_avg:44.18ms
step:1007/1775 train_time:44498ms step_avg:44.19ms
step:1008/1775 train_time:44559ms step_avg:44.20ms
step:1009/1775 train_time:44616ms step_avg:44.22ms
step:1010/1775 train_time:44677ms step_avg:44.23ms
step:1011/1775 train_time:44734ms step_avg:44.25ms
step:1012/1775 train_time:44795ms step_avg:44.26ms
step:1013/1775 train_time:44851ms step_avg:44.28ms
step:1014/1775 train_time:44912ms step_avg:44.29ms
step:1015/1775 train_time:44970ms step_avg:44.31ms
step:1016/1775 train_time:45031ms step_avg:44.32ms
step:1017/1775 train_time:45089ms step_avg:44.34ms
step:1018/1775 train_time:45151ms step_avg:44.35ms
step:1019/1775 train_time:45212ms step_avg:44.37ms
step:1020/1775 train_time:45275ms step_avg:44.39ms
step:1021/1775 train_time:45335ms step_avg:44.40ms
step:1022/1775 train_time:45396ms step_avg:44.42ms
step:1023/1775 train_time:45455ms step_avg:44.43ms
step:1024/1775 train_time:45515ms step_avg:44.45ms
step:1025/1775 train_time:45573ms step_avg:44.46ms
step:1026/1775 train_time:45633ms step_avg:44.48ms
step:1027/1775 train_time:45690ms step_avg:44.49ms
step:1028/1775 train_time:45751ms step_avg:44.50ms
step:1029/1775 train_time:45808ms step_avg:44.52ms
step:1030/1775 train_time:45868ms step_avg:44.53ms
step:1031/1775 train_time:45926ms step_avg:44.54ms
step:1032/1775 train_time:45986ms step_avg:44.56ms
step:1033/1775 train_time:46044ms step_avg:44.57ms
step:1034/1775 train_time:46105ms step_avg:44.59ms
step:1035/1775 train_time:46163ms step_avg:44.60ms
step:1036/1775 train_time:46225ms step_avg:44.62ms
step:1037/1775 train_time:46284ms step_avg:44.63ms
step:1038/1775 train_time:46347ms step_avg:44.65ms
step:1039/1775 train_time:46404ms step_avg:44.66ms
step:1040/1775 train_time:46465ms step_avg:44.68ms
step:1041/1775 train_time:46523ms step_avg:44.69ms
step:1042/1775 train_time:46584ms step_avg:44.71ms
step:1043/1775 train_time:46642ms step_avg:44.72ms
step:1044/1775 train_time:46703ms step_avg:44.73ms
step:1045/1775 train_time:46760ms step_avg:44.75ms
step:1046/1775 train_time:46821ms step_avg:44.76ms
step:1047/1775 train_time:46878ms step_avg:44.77ms
step:1048/1775 train_time:46939ms step_avg:44.79ms
step:1049/1775 train_time:46997ms step_avg:44.80ms
step:1050/1775 train_time:47058ms step_avg:44.82ms
step:1051/1775 train_time:47116ms step_avg:44.83ms
step:1052/1775 train_time:47179ms step_avg:44.85ms
step:1053/1775 train_time:47237ms step_avg:44.86ms
step:1054/1775 train_time:47299ms step_avg:44.88ms
step:1055/1775 train_time:47357ms step_avg:44.89ms
step:1056/1775 train_time:47418ms step_avg:44.90ms
step:1057/1775 train_time:47476ms step_avg:44.92ms
step:1058/1775 train_time:47538ms step_avg:44.93ms
step:1059/1775 train_time:47596ms step_avg:44.94ms
step:1060/1775 train_time:47658ms step_avg:44.96ms
step:1061/1775 train_time:47716ms step_avg:44.97ms
step:1062/1775 train_time:47776ms step_avg:44.99ms
step:1063/1775 train_time:47834ms step_avg:45.00ms
step:1064/1775 train_time:47894ms step_avg:45.01ms
step:1065/1775 train_time:47952ms step_avg:45.03ms
step:1066/1775 train_time:48013ms step_avg:45.04ms
step:1067/1775 train_time:48071ms step_avg:45.05ms
step:1068/1775 train_time:48132ms step_avg:45.07ms
step:1069/1775 train_time:48191ms step_avg:45.08ms
step:1070/1775 train_time:48252ms step_avg:45.10ms
step:1071/1775 train_time:48311ms step_avg:45.11ms
step:1072/1775 train_time:48373ms step_avg:45.12ms
step:1073/1775 train_time:48432ms step_avg:45.14ms
step:1074/1775 train_time:48493ms step_avg:45.15ms
step:1075/1775 train_time:48552ms step_avg:45.16ms
step:1076/1775 train_time:48613ms step_avg:45.18ms
step:1077/1775 train_time:48670ms step_avg:45.19ms
step:1078/1775 train_time:48731ms step_avg:45.21ms
step:1079/1775 train_time:48789ms step_avg:45.22ms
step:1080/1775 train_time:48849ms step_avg:45.23ms
step:1081/1775 train_time:48907ms step_avg:45.24ms
step:1082/1775 train_time:48968ms step_avg:45.26ms
step:1083/1775 train_time:49026ms step_avg:45.27ms
step:1084/1775 train_time:49087ms step_avg:45.28ms
step:1085/1775 train_time:49146ms step_avg:45.30ms
step:1086/1775 train_time:49207ms step_avg:45.31ms
step:1087/1775 train_time:49266ms step_avg:45.32ms
step:1088/1775 train_time:49328ms step_avg:45.34ms
step:1089/1775 train_time:49386ms step_avg:45.35ms
step:1090/1775 train_time:49448ms step_avg:45.36ms
step:1091/1775 train_time:49505ms step_avg:45.38ms
step:1092/1775 train_time:49567ms step_avg:45.39ms
step:1093/1775 train_time:49625ms step_avg:45.40ms
step:1094/1775 train_time:49685ms step_avg:45.42ms
step:1095/1775 train_time:49743ms step_avg:45.43ms
step:1096/1775 train_time:49804ms step_avg:45.44ms
step:1097/1775 train_time:49862ms step_avg:45.45ms
step:1098/1775 train_time:49923ms step_avg:45.47ms
step:1099/1775 train_time:49981ms step_avg:45.48ms
step:1100/1775 train_time:50041ms step_avg:45.49ms
step:1101/1775 train_time:50099ms step_avg:45.50ms
step:1102/1775 train_time:50160ms step_avg:45.52ms
step:1103/1775 train_time:50218ms step_avg:45.53ms
step:1104/1775 train_time:50280ms step_avg:45.54ms
step:1105/1775 train_time:50338ms step_avg:45.55ms
step:1106/1775 train_time:50399ms step_avg:45.57ms
step:1107/1775 train_time:50457ms step_avg:45.58ms
step:1108/1775 train_time:50518ms step_avg:45.59ms
step:1109/1775 train_time:50576ms step_avg:45.61ms
step:1110/1775 train_time:50748ms step_avg:45.72ms
step:1111/1775 train_time:50764ms step_avg:45.69ms
step:1112/1775 train_time:50790ms step_avg:45.67ms
step:1113/1775 train_time:50845ms step_avg:45.68ms
step:1114/1775 train_time:50906ms step_avg:45.70ms
step:1115/1775 train_time:50962ms step_avg:45.71ms
step:1116/1775 train_time:51022ms step_avg:45.72ms
step:1117/1775 train_time:51079ms step_avg:45.73ms
step:1118/1775 train_time:51139ms step_avg:45.74ms
step:1119/1775 train_time:51196ms step_avg:45.75ms
step:1120/1775 train_time:51257ms step_avg:45.76ms
step:1121/1775 train_time:51314ms step_avg:45.78ms
step:1122/1775 train_time:51376ms step_avg:45.79ms
step:1123/1775 train_time:51433ms step_avg:45.80ms
step:1124/1775 train_time:51494ms step_avg:45.81ms
step:1125/1775 train_time:51552ms step_avg:45.82ms
step:1126/1775 train_time:51614ms step_avg:45.84ms
step:1127/1775 train_time:51676ms step_avg:45.85ms
step:1128/1775 train_time:51740ms step_avg:45.87ms
step:1129/1775 train_time:51799ms step_avg:45.88ms
step:1130/1775 train_time:51860ms step_avg:45.89ms
step:1131/1775 train_time:51918ms step_avg:45.90ms
step:1132/1775 train_time:51979ms step_avg:45.92ms
step:1133/1775 train_time:52036ms step_avg:45.93ms
step:1134/1775 train_time:52096ms step_avg:45.94ms
step:1135/1775 train_time:52153ms step_avg:45.95ms
step:1136/1775 train_time:52214ms step_avg:45.96ms
step:1137/1775 train_time:52271ms step_avg:45.97ms
step:1138/1775 train_time:52332ms step_avg:45.99ms
step:1139/1775 train_time:52390ms step_avg:46.00ms
step:1140/1775 train_time:52451ms step_avg:46.01ms
step:1141/1775 train_time:52508ms step_avg:46.02ms
step:1142/1775 train_time:52569ms step_avg:46.03ms
step:1143/1775 train_time:52629ms step_avg:46.04ms
step:1144/1775 train_time:52692ms step_avg:46.06ms
step:1145/1775 train_time:52752ms step_avg:46.07ms
step:1146/1775 train_time:52814ms step_avg:46.09ms
step:1147/1775 train_time:52873ms step_avg:46.10ms
step:1148/1775 train_time:52934ms step_avg:46.11ms
step:1149/1775 train_time:52992ms step_avg:46.12ms
step:1150/1775 train_time:53052ms step_avg:46.13ms
step:1151/1775 train_time:53110ms step_avg:46.14ms
step:1152/1775 train_time:53171ms step_avg:46.16ms
step:1153/1775 train_time:53228ms step_avg:46.16ms
step:1154/1775 train_time:53289ms step_avg:46.18ms
step:1155/1775 train_time:53346ms step_avg:46.19ms
step:1156/1775 train_time:53407ms step_avg:46.20ms
step:1157/1775 train_time:53465ms step_avg:46.21ms
step:1158/1775 train_time:53530ms step_avg:46.23ms
step:1159/1775 train_time:53610ms step_avg:46.26ms
step:1160/1775 train_time:53700ms step_avg:46.29ms
step:1161/1775 train_time:53786ms step_avg:46.33ms
step:1162/1775 train_time:53874ms step_avg:46.36ms
step:1163/1775 train_time:53957ms step_avg:46.40ms
step:1164/1775 train_time:54043ms step_avg:46.43ms
step:1165/1775 train_time:54126ms step_avg:46.46ms
step:1166/1775 train_time:54212ms step_avg:46.49ms
step:1167/1775 train_time:54297ms step_avg:46.53ms
step:1168/1775 train_time:54384ms step_avg:46.56ms
step:1169/1775 train_time:54468ms step_avg:46.59ms
step:1170/1775 train_time:54554ms step_avg:46.63ms
step:1171/1775 train_time:54639ms step_avg:46.66ms
step:1172/1775 train_time:54727ms step_avg:46.70ms
step:1173/1775 train_time:54813ms step_avg:46.73ms
step:1174/1775 train_time:54901ms step_avg:46.76ms
step:1175/1775 train_time:54985ms step_avg:46.80ms
step:1176/1775 train_time:55072ms step_avg:46.83ms
step:1177/1775 train_time:55155ms step_avg:46.86ms
step:1178/1775 train_time:55240ms step_avg:46.89ms
step:1179/1775 train_time:55324ms step_avg:46.92ms
step:1180/1775 train_time:55411ms step_avg:46.96ms
step:1181/1775 train_time:55495ms step_avg:46.99ms
step:1182/1775 train_time:55582ms step_avg:47.02ms
step:1183/1775 train_time:55667ms step_avg:47.06ms
step:1184/1775 train_time:55754ms step_avg:47.09ms
step:1185/1775 train_time:55840ms step_avg:47.12ms
step:1186/1775 train_time:55927ms step_avg:47.16ms
step:1187/1775 train_time:56012ms step_avg:47.19ms
step:1188/1775 train_time:56099ms step_avg:47.22ms
step:1189/1775 train_time:56183ms step_avg:47.25ms
step:1190/1775 train_time:56268ms step_avg:47.28ms
step:1191/1775 train_time:56352ms step_avg:47.31ms
step:1192/1775 train_time:56439ms step_avg:47.35ms
step:1193/1775 train_time:56524ms step_avg:47.38ms
step:1194/1775 train_time:56612ms step_avg:47.41ms
step:1195/1775 train_time:56696ms step_avg:47.44ms
step:1196/1775 train_time:56783ms step_avg:47.48ms
step:1197/1775 train_time:56868ms step_avg:47.51ms
step:1198/1775 train_time:56955ms step_avg:47.54ms
step:1199/1775 train_time:57039ms step_avg:47.57ms
step:1200/1775 train_time:57125ms step_avg:47.60ms
step:1201/1775 train_time:57208ms step_avg:47.63ms
step:1202/1775 train_time:57295ms step_avg:47.67ms
step:1203/1775 train_time:57379ms step_avg:47.70ms
step:1204/1775 train_time:57465ms step_avg:47.73ms
step:1205/1775 train_time:57550ms step_avg:47.76ms
step:1206/1775 train_time:57638ms step_avg:47.79ms
step:1207/1775 train_time:57722ms step_avg:47.82ms
step:1208/1775 train_time:57809ms step_avg:47.85ms
step:1209/1775 train_time:57893ms step_avg:47.89ms
step:1210/1775 train_time:57981ms step_avg:47.92ms
step:1211/1775 train_time:58064ms step_avg:47.95ms
step:1212/1775 train_time:58151ms step_avg:47.98ms
step:1213/1775 train_time:58235ms step_avg:48.01ms
step:1214/1775 train_time:58321ms step_avg:48.04ms
step:1215/1775 train_time:58405ms step_avg:48.07ms
step:1216/1775 train_time:58493ms step_avg:48.10ms
step:1217/1775 train_time:58577ms step_avg:48.13ms
step:1218/1775 train_time:58664ms step_avg:48.16ms
step:1219/1775 train_time:58748ms step_avg:48.19ms
step:1220/1775 train_time:58835ms step_avg:48.23ms
step:1221/1775 train_time:58920ms step_avg:48.26ms
step:1222/1775 train_time:59007ms step_avg:48.29ms
step:1223/1775 train_time:59091ms step_avg:48.32ms
step:1224/1775 train_time:59178ms step_avg:48.35ms
step:1225/1775 train_time:59262ms step_avg:48.38ms
step:1226/1775 train_time:59348ms step_avg:48.41ms
step:1227/1775 train_time:59432ms step_avg:48.44ms
step:1228/1775 train_time:59520ms step_avg:48.47ms
step:1229/1775 train_time:59604ms step_avg:48.50ms
step:1230/1775 train_time:59691ms step_avg:48.53ms
step:1231/1775 train_time:59775ms step_avg:48.56ms
step:1232/1775 train_time:59863ms step_avg:48.59ms
step:1233/1775 train_time:59947ms step_avg:48.62ms
step:1234/1775 train_time:60034ms step_avg:48.65ms
step:1235/1775 train_time:60119ms step_avg:48.68ms
step:1236/1775 train_time:60206ms step_avg:48.71ms
step:1237/1775 train_time:60289ms step_avg:48.74ms
step:1238/1775 train_time:60375ms step_avg:48.77ms
step:1239/1775 train_time:60460ms step_avg:48.80ms
step:1240/1775 train_time:60547ms step_avg:48.83ms
step:1241/1775 train_time:60633ms step_avg:48.86ms
step:1242/1775 train_time:60720ms step_avg:48.89ms
step:1243/1775 train_time:60805ms step_avg:48.92ms
step:1244/1775 train_time:60892ms step_avg:48.95ms
step:1245/1775 train_time:60976ms step_avg:48.98ms
step:1246/1775 train_time:61065ms step_avg:49.01ms
step:1247/1775 train_time:61148ms step_avg:49.04ms
step:1248/1775 train_time:61233ms step_avg:49.07ms
step:1249/1775 train_time:61317ms step_avg:49.09ms
step:1250/1775 train_time:61404ms step_avg:49.12ms
step:1250/1775 val_loss:3.4990 train_time:61515ms step_avg:49.21ms
step:1251/1775 train_time:61534ms step_avg:49.19ms
step:1252/1775 train_time:61579ms step_avg:49.18ms
step:1253/1775 train_time:61664ms step_avg:49.21ms
step:1254/1775 train_time:61751ms step_avg:49.24ms
step:1255/1775 train_time:61837ms step_avg:49.27ms
step:1256/1775 train_time:61923ms step_avg:49.30ms
step:1257/1775 train_time:62006ms step_avg:49.33ms
step:1258/1775 train_time:62093ms step_avg:49.36ms
step:1259/1775 train_time:62176ms step_avg:49.38ms
step:1260/1775 train_time:62261ms step_avg:49.41ms
step:1261/1775 train_time:62344ms step_avg:49.44ms
step:1262/1775 train_time:62432ms step_avg:49.47ms
step:1263/1775 train_time:62518ms step_avg:49.50ms
step:1264/1775 train_time:62608ms step_avg:49.53ms
step:1265/1775 train_time:62693ms step_avg:49.56ms
step:1266/1775 train_time:62780ms step_avg:49.59ms
step:1267/1775 train_time:62865ms step_avg:49.62ms
step:1268/1775 train_time:62950ms step_avg:49.65ms
step:1269/1775 train_time:63035ms step_avg:49.67ms
step:1270/1775 train_time:63120ms step_avg:49.70ms
step:1271/1775 train_time:63202ms step_avg:49.73ms
step:1272/1775 train_time:63289ms step_avg:49.76ms
step:1273/1775 train_time:63373ms step_avg:49.78ms
step:1274/1775 train_time:63460ms step_avg:49.81ms
step:1275/1775 train_time:63546ms step_avg:49.84ms
step:1276/1775 train_time:63635ms step_avg:49.87ms
step:1277/1775 train_time:63720ms step_avg:49.90ms
step:1278/1775 train_time:63808ms step_avg:49.93ms
step:1279/1775 train_time:63891ms step_avg:49.95ms
step:1280/1775 train_time:63978ms step_avg:49.98ms
step:1281/1775 train_time:64060ms step_avg:50.01ms
step:1282/1775 train_time:64146ms step_avg:50.04ms
step:1283/1775 train_time:64228ms step_avg:50.06ms
step:1284/1775 train_time:64315ms step_avg:50.09ms
step:1285/1775 train_time:64399ms step_avg:50.12ms
step:1286/1775 train_time:64485ms step_avg:50.14ms
step:1287/1775 train_time:64571ms step_avg:50.17ms
step:1288/1775 train_time:64659ms step_avg:50.20ms
step:1289/1775 train_time:64746ms step_avg:50.23ms
step:1290/1775 train_time:64834ms step_avg:50.26ms
step:1291/1775 train_time:64919ms step_avg:50.29ms
step:1292/1775 train_time:65004ms step_avg:50.31ms
step:1293/1775 train_time:65088ms step_avg:50.34ms
step:1294/1775 train_time:65175ms step_avg:50.37ms
step:1295/1775 train_time:65258ms step_avg:50.39ms
step:1296/1775 train_time:65344ms step_avg:50.42ms
step:1297/1775 train_time:65428ms step_avg:50.45ms
step:1298/1775 train_time:65516ms step_avg:50.47ms
step:1299/1775 train_time:65601ms step_avg:50.50ms
step:1300/1775 train_time:65689ms step_avg:50.53ms
step:1301/1775 train_time:65773ms step_avg:50.56ms
step:1302/1775 train_time:65860ms step_avg:50.58ms
step:1303/1775 train_time:65945ms step_avg:50.61ms
step:1304/1775 train_time:66031ms step_avg:50.64ms
step:1305/1775 train_time:66114ms step_avg:50.66ms
step:1306/1775 train_time:66200ms step_avg:50.69ms
step:1307/1775 train_time:66285ms step_avg:50.72ms
step:1308/1775 train_time:66371ms step_avg:50.74ms
step:1309/1775 train_time:66455ms step_avg:50.77ms
step:1310/1775 train_time:66542ms step_avg:50.80ms
step:1311/1775 train_time:66628ms step_avg:50.82ms
step:1312/1775 train_time:66716ms step_avg:50.85ms
step:1313/1775 train_time:66800ms step_avg:50.88ms
step:1314/1775 train_time:66887ms step_avg:50.90ms
step:1315/1775 train_time:66971ms step_avg:50.93ms
step:1316/1775 train_time:67058ms step_avg:50.96ms
step:1317/1775 train_time:67141ms step_avg:50.98ms
step:1318/1775 train_time:67228ms step_avg:51.01ms
step:1319/1775 train_time:67312ms step_avg:51.03ms
step:1320/1775 train_time:67398ms step_avg:51.06ms
step:1321/1775 train_time:67482ms step_avg:51.08ms
step:1322/1775 train_time:67571ms step_avg:51.11ms
step:1323/1775 train_time:67656ms step_avg:51.14ms
step:1324/1775 train_time:67743ms step_avg:51.17ms
step:1325/1775 train_time:67827ms step_avg:51.19ms
step:1326/1775 train_time:67914ms step_avg:51.22ms
step:1327/1775 train_time:67998ms step_avg:51.24ms
step:1328/1775 train_time:68083ms step_avg:51.27ms
step:1329/1775 train_time:68167ms step_avg:51.29ms
step:1330/1775 train_time:68254ms step_avg:51.32ms
step:1331/1775 train_time:68336ms step_avg:51.34ms
step:1332/1775 train_time:68424ms step_avg:51.37ms
step:1333/1775 train_time:68509ms step_avg:51.39ms
step:1334/1775 train_time:68597ms step_avg:51.42ms
step:1335/1775 train_time:68681ms step_avg:51.45ms
step:1336/1775 train_time:68768ms step_avg:51.47ms
step:1337/1775 train_time:68852ms step_avg:51.50ms
step:1338/1775 train_time:68940ms step_avg:51.52ms
step:1339/1775 train_time:69023ms step_avg:51.55ms
step:1340/1775 train_time:69111ms step_avg:51.58ms
step:1341/1775 train_time:69195ms step_avg:51.60ms
step:1342/1775 train_time:69281ms step_avg:51.63ms
step:1343/1775 train_time:69365ms step_avg:51.65ms
step:1344/1775 train_time:69451ms step_avg:51.67ms
step:1345/1775 train_time:69536ms step_avg:51.70ms
step:1346/1775 train_time:69625ms step_avg:51.73ms
step:1347/1775 train_time:69709ms step_avg:51.75ms
step:1348/1775 train_time:69796ms step_avg:51.78ms
step:1349/1775 train_time:69880ms step_avg:51.80ms
step:1350/1775 train_time:69966ms step_avg:51.83ms
step:1351/1775 train_time:70050ms step_avg:51.85ms
step:1352/1775 train_time:70137ms step_avg:51.88ms
step:1353/1775 train_time:70220ms step_avg:51.90ms
step:1354/1775 train_time:70307ms step_avg:51.93ms
step:1355/1775 train_time:70392ms step_avg:51.95ms
step:1356/1775 train_time:70479ms step_avg:51.98ms
step:1357/1775 train_time:70563ms step_avg:52.00ms
step:1358/1775 train_time:70650ms step_avg:52.02ms
step:1359/1775 train_time:70734ms step_avg:52.05ms
step:1360/1775 train_time:70820ms step_avg:52.07ms
step:1361/1775 train_time:70904ms step_avg:52.10ms
step:1362/1775 train_time:70991ms step_avg:52.12ms
step:1363/1775 train_time:71076ms step_avg:52.15ms
step:1364/1775 train_time:71163ms step_avg:52.17ms
step:1365/1775 train_time:71246ms step_avg:52.20ms
step:1366/1775 train_time:71333ms step_avg:52.22ms
step:1367/1775 train_time:71417ms step_avg:52.24ms
step:1368/1775 train_time:71505ms step_avg:52.27ms
step:1369/1775 train_time:71589ms step_avg:52.29ms
step:1370/1775 train_time:71676ms step_avg:52.32ms
step:1371/1775 train_time:71759ms step_avg:52.34ms
step:1372/1775 train_time:71846ms step_avg:52.37ms
step:1373/1775 train_time:71930ms step_avg:52.39ms
step:1374/1775 train_time:72017ms step_avg:52.41ms
step:1375/1775 train_time:72100ms step_avg:52.44ms
step:1376/1775 train_time:72186ms step_avg:52.46ms
step:1377/1775 train_time:72271ms step_avg:52.48ms
step:1378/1775 train_time:72357ms step_avg:52.51ms
step:1379/1775 train_time:72441ms step_avg:52.53ms
step:1380/1775 train_time:72528ms step_avg:52.56ms
step:1381/1775 train_time:72613ms step_avg:52.58ms
step:1382/1775 train_time:72700ms step_avg:52.60ms
step:1383/1775 train_time:72784ms step_avg:52.63ms
step:1384/1775 train_time:72872ms step_avg:52.65ms
step:1385/1775 train_time:72956ms step_avg:52.68ms
step:1386/1775 train_time:73043ms step_avg:52.70ms
step:1387/1775 train_time:73125ms step_avg:52.72ms
step:1388/1775 train_time:73214ms step_avg:52.75ms
step:1389/1775 train_time:73297ms step_avg:52.77ms
step:1390/1775 train_time:73384ms step_avg:52.79ms
step:1391/1775 train_time:73467ms step_avg:52.82ms
step:1392/1775 train_time:73554ms step_avg:52.84ms
step:1393/1775 train_time:73638ms step_avg:52.86ms
step:1394/1775 train_time:73725ms step_avg:52.89ms
step:1395/1775 train_time:73809ms step_avg:52.91ms
step:1396/1775 train_time:73896ms step_avg:52.93ms
step:1397/1775 train_time:73982ms step_avg:52.96ms
step:1398/1775 train_time:74067ms step_avg:52.98ms
step:1399/1775 train_time:74151ms step_avg:53.00ms
step:1400/1775 train_time:74238ms step_avg:53.03ms
step:1401/1775 train_time:74321ms step_avg:53.05ms
step:1402/1775 train_time:74409ms step_avg:53.07ms
step:1403/1775 train_time:74493ms step_avg:53.10ms
step:1404/1775 train_time:74581ms step_avg:53.12ms
step:1405/1775 train_time:74667ms step_avg:53.14ms
step:1406/1775 train_time:74753ms step_avg:53.17ms
step:1407/1775 train_time:74837ms step_avg:53.19ms
step:1408/1775 train_time:74925ms step_avg:53.21ms
step:1409/1775 train_time:75009ms step_avg:53.24ms
step:1410/1775 train_time:75096ms step_avg:53.26ms
step:1411/1775 train_time:75179ms step_avg:53.28ms
step:1412/1775 train_time:75265ms step_avg:53.30ms
step:1413/1775 train_time:75349ms step_avg:53.33ms
step:1414/1775 train_time:75437ms step_avg:53.35ms
step:1415/1775 train_time:75521ms step_avg:53.37ms
step:1416/1775 train_time:75608ms step_avg:53.40ms
step:1417/1775 train_time:75693ms step_avg:53.42ms
step:1418/1775 train_time:75780ms step_avg:53.44ms
step:1419/1775 train_time:75864ms step_avg:53.46ms
step:1420/1775 train_time:75952ms step_avg:53.49ms
step:1421/1775 train_time:76036ms step_avg:53.51ms
step:1422/1775 train_time:76123ms step_avg:53.53ms
step:1423/1775 train_time:76206ms step_avg:53.55ms
step:1424/1775 train_time:76292ms step_avg:53.58ms
step:1425/1775 train_time:76377ms step_avg:53.60ms
step:1426/1775 train_time:76463ms step_avg:53.62ms
step:1427/1775 train_time:76548ms step_avg:53.64ms
step:1428/1775 train_time:76635ms step_avg:53.67ms
step:1429/1775 train_time:76720ms step_avg:53.69ms
step:1430/1775 train_time:76807ms step_avg:53.71ms
step:1431/1775 train_time:76892ms step_avg:53.73ms
step:1432/1775 train_time:76980ms step_avg:53.76ms
step:1433/1775 train_time:77064ms step_avg:53.78ms
step:1434/1775 train_time:77151ms step_avg:53.80ms
step:1435/1775 train_time:77234ms step_avg:53.82ms
step:1436/1775 train_time:77321ms step_avg:53.84ms
step:1437/1775 train_time:77406ms step_avg:53.87ms
step:1438/1775 train_time:77494ms step_avg:53.89ms
step:1439/1775 train_time:77578ms step_avg:53.91ms
step:1440/1775 train_time:77664ms step_avg:53.93ms
step:1441/1775 train_time:77749ms step_avg:53.95ms
step:1442/1775 train_time:77835ms step_avg:53.98ms
step:1443/1775 train_time:77919ms step_avg:54.00ms
step:1444/1775 train_time:78007ms step_avg:54.02ms
step:1445/1775 train_time:78091ms step_avg:54.04ms
step:1446/1775 train_time:78178ms step_avg:54.06ms
step:1447/1775 train_time:78261ms step_avg:54.09ms
step:1448/1775 train_time:78349ms step_avg:54.11ms
step:1449/1775 train_time:78434ms step_avg:54.13ms
step:1450/1775 train_time:78520ms step_avg:54.15ms
step:1451/1775 train_time:78604ms step_avg:54.17ms
step:1452/1775 train_time:78691ms step_avg:54.20ms
step:1453/1775 train_time:78776ms step_avg:54.22ms
step:1454/1775 train_time:78863ms step_avg:54.24ms
step:1455/1775 train_time:78946ms step_avg:54.26ms
step:1456/1775 train_time:79033ms step_avg:54.28ms
step:1457/1775 train_time:79118ms step_avg:54.30ms
step:1458/1775 train_time:79203ms step_avg:54.32ms
step:1459/1775 train_time:79288ms step_avg:54.34ms
step:1460/1775 train_time:79377ms step_avg:54.37ms
step:1461/1775 train_time:79461ms step_avg:54.39ms
step:1462/1775 train_time:79548ms step_avg:54.41ms
step:1463/1775 train_time:79633ms step_avg:54.43ms
step:1464/1775 train_time:79719ms step_avg:54.45ms
step:1465/1775 train_time:79803ms step_avg:54.47ms
step:1466/1775 train_time:79891ms step_avg:54.50ms
step:1467/1775 train_time:79976ms step_avg:54.52ms
step:1468/1775 train_time:80063ms step_avg:54.54ms
step:1469/1775 train_time:80147ms step_avg:54.56ms
step:1470/1775 train_time:80234ms step_avg:54.58ms
step:1471/1775 train_time:80317ms step_avg:54.60ms
step:1472/1775 train_time:80405ms step_avg:54.62ms
step:1473/1775 train_time:80488ms step_avg:54.64ms
step:1474/1775 train_time:80575ms step_avg:54.66ms
step:1475/1775 train_time:80659ms step_avg:54.68ms
step:1476/1775 train_time:80745ms step_avg:54.71ms
step:1477/1775 train_time:80830ms step_avg:54.73ms
step:1478/1775 train_time:80918ms step_avg:54.75ms
step:1479/1775 train_time:81001ms step_avg:54.77ms
step:1480/1775 train_time:81089ms step_avg:54.79ms
step:1481/1775 train_time:81174ms step_avg:54.81ms
step:1482/1775 train_time:81260ms step_avg:54.83ms
step:1483/1775 train_time:81344ms step_avg:54.85ms
step:1484/1775 train_time:81431ms step_avg:54.87ms
step:1485/1775 train_time:81517ms step_avg:54.89ms
step:1486/1775 train_time:81603ms step_avg:54.91ms
step:1487/1775 train_time:81687ms step_avg:54.93ms
step:1488/1775 train_time:81775ms step_avg:54.96ms
step:1489/1775 train_time:81858ms step_avg:54.98ms
step:1490/1775 train_time:81946ms step_avg:55.00ms
step:1491/1775 train_time:82029ms step_avg:55.02ms
step:1492/1775 train_time:82118ms step_avg:55.04ms
step:1493/1775 train_time:82202ms step_avg:55.06ms
step:1494/1775 train_time:82289ms step_avg:55.08ms
step:1495/1775 train_time:82374ms step_avg:55.10ms
step:1496/1775 train_time:82460ms step_avg:55.12ms
step:1497/1775 train_time:82543ms step_avg:55.14ms
step:1498/1775 train_time:82629ms step_avg:55.16ms
step:1499/1775 train_time:82714ms step_avg:55.18ms
step:1500/1775 train_time:82800ms step_avg:55.20ms
step:1500/1775 val_loss:3.3721 train_time:82910ms step_avg:55.27ms
step:1501/1775 train_time:82929ms step_avg:55.25ms
step:1502/1775 train_time:82971ms step_avg:55.24ms
step:1503/1775 train_time:83057ms step_avg:55.26ms
step:1504/1775 train_time:83144ms step_avg:55.28ms
step:1505/1775 train_time:83229ms step_avg:55.30ms
step:1506/1775 train_time:83315ms step_avg:55.32ms
step:1507/1775 train_time:83400ms step_avg:55.34ms
step:1508/1775 train_time:83485ms step_avg:55.36ms
step:1509/1775 train_time:83568ms step_avg:55.38ms
step:1510/1775 train_time:83654ms step_avg:55.40ms
step:1511/1775 train_time:83738ms step_avg:55.42ms
step:1512/1775 train_time:83826ms step_avg:55.44ms
step:1513/1775 train_time:83912ms step_avg:55.46ms
step:1514/1775 train_time:84000ms step_avg:55.48ms
step:1515/1775 train_time:84085ms step_avg:55.50ms
step:1516/1775 train_time:84172ms step_avg:55.52ms
step:1517/1775 train_time:84255ms step_avg:55.54ms
step:1518/1775 train_time:84344ms step_avg:55.56ms
step:1519/1775 train_time:84426ms step_avg:55.58ms
step:1520/1775 train_time:84512ms step_avg:55.60ms
step:1521/1775 train_time:84595ms step_avg:55.62ms
step:1522/1775 train_time:84683ms step_avg:55.64ms
step:1523/1775 train_time:84766ms step_avg:55.66ms
step:1524/1775 train_time:84854ms step_avg:55.68ms
step:1525/1775 train_time:84939ms step_avg:55.70ms
step:1526/1775 train_time:85027ms step_avg:55.72ms
step:1527/1775 train_time:85112ms step_avg:55.74ms
step:1528/1775 train_time:85198ms step_avg:55.76ms
step:1529/1775 train_time:85282ms step_avg:55.78ms
step:1530/1775 train_time:85369ms step_avg:55.80ms
step:1531/1775 train_time:85452ms step_avg:55.81ms
step:1532/1775 train_time:85539ms step_avg:55.83ms
step:1533/1775 train_time:85622ms step_avg:55.85ms
step:1534/1775 train_time:85710ms step_avg:55.87ms
step:1535/1775 train_time:85795ms step_avg:55.89ms
step:1536/1775 train_time:85882ms step_avg:55.91ms
step:1537/1775 train_time:85967ms step_avg:55.93ms
step:1538/1775 train_time:86054ms step_avg:55.95ms
step:1539/1775 train_time:86139ms step_avg:55.97ms
step:1540/1775 train_time:86226ms step_avg:55.99ms
step:1541/1775 train_time:86309ms step_avg:56.01ms
step:1542/1775 train_time:86397ms step_avg:56.03ms
step:1543/1775 train_time:86480ms step_avg:56.05ms
step:1544/1775 train_time:86567ms step_avg:56.07ms
step:1545/1775 train_time:86650ms step_avg:56.08ms
step:1546/1775 train_time:86738ms step_avg:56.10ms
step:1547/1775 train_time:86823ms step_avg:56.12ms
step:1548/1775 train_time:86910ms step_avg:56.14ms
step:1549/1775 train_time:86995ms step_avg:56.16ms
step:1550/1775 train_time:87082ms step_avg:56.18ms
step:1551/1775 train_time:87167ms step_avg:56.20ms
step:1552/1775 train_time:87254ms step_avg:56.22ms
step:1553/1775 train_time:87338ms step_avg:56.24ms
step:1554/1775 train_time:87425ms step_avg:56.26ms
step:1555/1775 train_time:87508ms step_avg:56.28ms
step:1556/1775 train_time:87594ms step_avg:56.29ms
step:1557/1775 train_time:87678ms step_avg:56.31ms
step:1558/1775 train_time:87765ms step_avg:56.33ms
step:1559/1775 train_time:87849ms step_avg:56.35ms
step:1560/1775 train_time:87937ms step_avg:56.37ms
step:1561/1775 train_time:88021ms step_avg:56.39ms
step:1562/1775 train_time:88109ms step_avg:56.41ms
step:1563/1775 train_time:88194ms step_avg:56.43ms
step:1564/1775 train_time:88280ms step_avg:56.45ms
step:1565/1775 train_time:88365ms step_avg:56.46ms
step:1566/1775 train_time:88451ms step_avg:56.48ms
step:1567/1775 train_time:88535ms step_avg:56.50ms
step:1568/1775 train_time:88622ms step_avg:56.52ms
step:1569/1775 train_time:88706ms step_avg:56.54ms
step:1570/1775 train_time:88792ms step_avg:56.56ms
step:1571/1775 train_time:88877ms step_avg:56.57ms
step:1572/1775 train_time:88963ms step_avg:56.59ms
step:1573/1775 train_time:89047ms step_avg:56.61ms
step:1574/1775 train_time:89136ms step_avg:56.63ms
step:1575/1775 train_time:89219ms step_avg:56.65ms
step:1576/1775 train_time:89306ms step_avg:56.67ms
step:1577/1775 train_time:89390ms step_avg:56.68ms
step:1578/1775 train_time:89478ms step_avg:56.70ms
step:1579/1775 train_time:89561ms step_avg:56.72ms
step:1580/1775 train_time:89648ms step_avg:56.74ms
step:1581/1775 train_time:89731ms step_avg:56.76ms
step:1582/1775 train_time:89818ms step_avg:56.77ms
step:1583/1775 train_time:89903ms step_avg:56.79ms
step:1584/1775 train_time:89990ms step_avg:56.81ms
step:1585/1775 train_time:90075ms step_avg:56.83ms
step:1586/1775 train_time:90162ms step_avg:56.85ms
step:1587/1775 train_time:90247ms step_avg:56.87ms
step:1588/1775 train_time:90333ms step_avg:56.88ms
step:1589/1775 train_time:90417ms step_avg:56.90ms
step:1590/1775 train_time:90505ms step_avg:56.92ms
step:1591/1775 train_time:90588ms step_avg:56.94ms
step:1592/1775 train_time:90675ms step_avg:56.96ms
step:1593/1775 train_time:90758ms step_avg:56.97ms
step:1594/1775 train_time:90845ms step_avg:56.99ms
step:1595/1775 train_time:90929ms step_avg:57.01ms
step:1596/1775 train_time:91018ms step_avg:57.03ms
step:1597/1775 train_time:91102ms step_avg:57.05ms
step:1598/1775 train_time:91190ms step_avg:57.07ms
step:1599/1775 train_time:91275ms step_avg:57.08ms
step:1600/1775 train_time:91361ms step_avg:57.10ms
step:1601/1775 train_time:91446ms step_avg:57.12ms
step:1602/1775 train_time:91532ms step_avg:57.14ms
step:1603/1775 train_time:91615ms step_avg:57.15ms
step:1604/1775 train_time:91702ms step_avg:57.17ms
step:1605/1775 train_time:91786ms step_avg:57.19ms
step:1606/1775 train_time:91872ms step_avg:57.21ms
step:1607/1775 train_time:91957ms step_avg:57.22ms
step:1608/1775 train_time:92045ms step_avg:57.24ms
step:1609/1775 train_time:92129ms step_avg:57.26ms
step:1610/1775 train_time:92215ms step_avg:57.28ms
step:1611/1775 train_time:92301ms step_avg:57.29ms
step:1612/1775 train_time:92389ms step_avg:57.31ms
step:1613/1775 train_time:92473ms step_avg:57.33ms
step:1614/1775 train_time:92560ms step_avg:57.35ms
step:1615/1775 train_time:92645ms step_avg:57.37ms
step:1616/1775 train_time:92731ms step_avg:57.38ms
step:1617/1775 train_time:92815ms step_avg:57.40ms
step:1618/1775 train_time:92902ms step_avg:57.42ms
step:1619/1775 train_time:92986ms step_avg:57.43ms
step:1620/1775 train_time:93072ms step_avg:57.45ms
step:1621/1775 train_time:93157ms step_avg:57.47ms
step:1622/1775 train_time:93243ms step_avg:57.49ms
step:1623/1775 train_time:93327ms step_avg:57.50ms
step:1624/1775 train_time:93414ms step_avg:57.52ms
step:1625/1775 train_time:93498ms step_avg:57.54ms
step:1626/1775 train_time:93585ms step_avg:57.56ms
step:1627/1775 train_time:93668ms step_avg:57.57ms
step:1628/1775 train_time:93756ms step_avg:57.59ms
step:1629/1775 train_time:93839ms step_avg:57.61ms
step:1630/1775 train_time:93928ms step_avg:57.62ms
step:1631/1775 train_time:94010ms step_avg:57.64ms
step:1632/1775 train_time:94098ms step_avg:57.66ms
step:1633/1775 train_time:94182ms step_avg:57.67ms
step:1634/1775 train_time:94268ms step_avg:57.69ms
step:1635/1775 train_time:94352ms step_avg:57.71ms
step:1636/1775 train_time:94439ms step_avg:57.73ms
step:1637/1775 train_time:94524ms step_avg:57.74ms
step:1638/1775 train_time:94610ms step_avg:57.76ms
step:1639/1775 train_time:94694ms step_avg:57.78ms
step:1640/1775 train_time:94783ms step_avg:57.79ms
step:1641/1775 train_time:94866ms step_avg:57.81ms
step:1642/1775 train_time:94952ms step_avg:57.83ms
step:1643/1775 train_time:95036ms step_avg:57.84ms
step:1644/1775 train_time:95123ms step_avg:57.86ms
step:1645/1775 train_time:95207ms step_avg:57.88ms
step:1646/1775 train_time:95294ms step_avg:57.89ms
step:1647/1775 train_time:95378ms step_avg:57.91ms
step:1648/1775 train_time:95465ms step_avg:57.93ms
step:1649/1775 train_time:95548ms step_avg:57.94ms
step:1650/1775 train_time:95635ms step_avg:57.96ms
step:1651/1775 train_time:95719ms step_avg:57.98ms
step:1652/1775 train_time:95806ms step_avg:57.99ms
step:1653/1775 train_time:95890ms step_avg:58.01ms
step:1654/1775 train_time:95976ms step_avg:58.03ms
step:1655/1775 train_time:96060ms step_avg:58.04ms
step:1656/1775 train_time:96147ms step_avg:58.06ms
step:1657/1775 train_time:96231ms step_avg:58.08ms
step:1658/1775 train_time:96317ms step_avg:58.09ms
step:1659/1775 train_time:96401ms step_avg:58.11ms
step:1660/1775 train_time:96488ms step_avg:58.13ms
step:1661/1775 train_time:96572ms step_avg:58.14ms
step:1662/1775 train_time:96659ms step_avg:58.16ms
step:1663/1775 train_time:96743ms step_avg:58.17ms
step:1664/1775 train_time:96831ms step_avg:58.19ms
step:1665/1775 train_time:96915ms step_avg:58.21ms
step:1666/1775 train_time:97002ms step_avg:58.22ms
step:1667/1775 train_time:97087ms step_avg:58.24ms
step:1668/1775 train_time:97174ms step_avg:58.26ms
step:1669/1775 train_time:97257ms step_avg:58.27ms
step:1670/1775 train_time:97344ms step_avg:58.29ms
step:1671/1775 train_time:97429ms step_avg:58.31ms
step:1672/1775 train_time:97515ms step_avg:58.32ms
step:1673/1775 train_time:97599ms step_avg:58.34ms
step:1674/1775 train_time:97687ms step_avg:58.36ms
step:1675/1775 train_time:97769ms step_avg:58.37ms
step:1676/1775 train_time:97856ms step_avg:58.39ms
step:1677/1775 train_time:97941ms step_avg:58.40ms
step:1678/1775 train_time:98028ms step_avg:58.42ms
step:1679/1775 train_time:98111ms step_avg:58.43ms
step:1680/1775 train_time:98198ms step_avg:58.45ms
step:1681/1775 train_time:98283ms step_avg:58.47ms
step:1682/1775 train_time:98370ms step_avg:58.48ms
step:1683/1775 train_time:98455ms step_avg:58.50ms
step:1684/1775 train_time:98543ms step_avg:58.52ms
step:1685/1775 train_time:98627ms step_avg:58.53ms
step:1686/1775 train_time:98713ms step_avg:58.55ms
step:1687/1775 train_time:98797ms step_avg:58.56ms
step:1688/1775 train_time:98885ms step_avg:58.58ms
step:1689/1775 train_time:98968ms step_avg:58.60ms
step:1690/1775 train_time:99056ms step_avg:58.61ms
step:1691/1775 train_time:99139ms step_avg:58.63ms
step:1692/1775 train_time:99226ms step_avg:58.64ms
step:1693/1775 train_time:99310ms step_avg:58.66ms
step:1694/1775 train_time:99397ms step_avg:58.68ms
step:1695/1775 train_time:99482ms step_avg:58.69ms
step:1696/1775 train_time:99569ms step_avg:58.71ms
step:1697/1775 train_time:99654ms step_avg:58.72ms
step:1698/1775 train_time:99741ms step_avg:58.74ms
step:1699/1775 train_time:99824ms step_avg:58.75ms
step:1700/1775 train_time:99911ms step_avg:58.77ms
step:1701/1775 train_time:99996ms step_avg:58.79ms
step:1702/1775 train_time:100084ms step_avg:58.80ms
step:1703/1775 train_time:100168ms step_avg:58.82ms
step:1704/1775 train_time:100255ms step_avg:58.83ms
step:1705/1775 train_time:100338ms step_avg:58.85ms
step:1706/1775 train_time:100427ms step_avg:58.87ms
step:1707/1775 train_time:100510ms step_avg:58.88ms
step:1708/1775 train_time:100597ms step_avg:58.90ms
step:1709/1775 train_time:100681ms step_avg:58.91ms
step:1710/1775 train_time:100768ms step_avg:58.93ms
step:1711/1775 train_time:100851ms step_avg:58.94ms
step:1712/1775 train_time:100938ms step_avg:58.96ms
step:1713/1775 train_time:101022ms step_avg:58.97ms
step:1714/1775 train_time:101109ms step_avg:58.99ms
step:1715/1775 train_time:101194ms step_avg:59.01ms
step:1716/1775 train_time:101280ms step_avg:59.02ms
step:1717/1775 train_time:101364ms step_avg:59.04ms
step:1718/1775 train_time:101452ms step_avg:59.05ms
step:1719/1775 train_time:101537ms step_avg:59.07ms
step:1720/1775 train_time:101623ms step_avg:59.08ms
step:1721/1775 train_time:101707ms step_avg:59.10ms
step:1722/1775 train_time:101795ms step_avg:59.11ms
step:1723/1775 train_time:101879ms step_avg:59.13ms
step:1724/1775 train_time:101965ms step_avg:59.14ms
step:1725/1775 train_time:102048ms step_avg:59.16ms
step:1726/1775 train_time:102134ms step_avg:59.17ms
step:1727/1775 train_time:102218ms step_avg:59.19ms
step:1728/1775 train_time:102305ms step_avg:59.20ms
step:1729/1775 train_time:102389ms step_avg:59.22ms
step:1730/1775 train_time:102476ms step_avg:59.23ms
step:1731/1775 train_time:102561ms step_avg:59.25ms
step:1732/1775 train_time:102648ms step_avg:59.27ms
step:1733/1775 train_time:102733ms step_avg:59.28ms
step:1734/1775 train_time:102821ms step_avg:59.30ms
step:1735/1775 train_time:102905ms step_avg:59.31ms
step:1736/1775 train_time:102995ms step_avg:59.33ms
step:1737/1775 train_time:103080ms step_avg:59.34ms
step:1738/1775 train_time:103166ms step_avg:59.36ms
step:1739/1775 train_time:103251ms step_avg:59.37ms
step:1740/1775 train_time:103338ms step_avg:59.39ms
step:1741/1775 train_time:103422ms step_avg:59.40ms
step:1742/1775 train_time:103509ms step_avg:59.42ms
step:1743/1775 train_time:103594ms step_avg:59.43ms
step:1744/1775 train_time:103680ms step_avg:59.45ms
step:1745/1775 train_time:103766ms step_avg:59.46ms
step:1746/1775 train_time:103853ms step_avg:59.48ms
step:1747/1775 train_time:103938ms step_avg:59.50ms
step:1748/1775 train_time:104026ms step_avg:59.51ms
step:1749/1775 train_time:104109ms step_avg:59.52ms
step:1750/1775 train_time:104196ms step_avg:59.54ms
step:1750/1775 val_loss:3.2848 train_time:104307ms step_avg:59.60ms
step:1751/1775 train_time:104326ms step_avg:59.58ms
step:1752/1775 train_time:104368ms step_avg:59.57ms
step:1753/1775 train_time:104457ms step_avg:59.59ms
step:1754/1775 train_time:104545ms step_avg:59.60ms
step:1755/1775 train_time:104630ms step_avg:59.62ms
step:1756/1775 train_time:104717ms step_avg:59.63ms
step:1757/1775 train_time:104799ms step_avg:59.65ms
step:1758/1775 train_time:104886ms step_avg:59.66ms
step:1759/1775 train_time:104969ms step_avg:59.68ms
step:1760/1775 train_time:105056ms step_avg:59.69ms
step:1761/1775 train_time:105140ms step_avg:59.70ms
step:1762/1775 train_time:105228ms step_avg:59.72ms
step:1763/1775 train_time:105314ms step_avg:59.74ms
step:1764/1775 train_time:105406ms step_avg:59.75ms
step:1765/1775 train_time:105493ms step_avg:59.77ms
step:1766/1775 train_time:105581ms step_avg:59.79ms
step:1767/1775 train_time:105666ms step_avg:59.80ms
step:1768/1775 train_time:105751ms step_avg:59.81ms
step:1769/1775 train_time:105835ms step_avg:59.83ms
step:1770/1775 train_time:105921ms step_avg:59.84ms
step:1771/1775 train_time:106004ms step_avg:59.86ms
step:1772/1775 train_time:106092ms step_avg:59.87ms
step:1773/1775 train_time:106176ms step_avg:59.89ms
step:1774/1775 train_time:106264ms step_avg:59.90ms
step:1775/1775 train_time:106351ms step_avg:59.92ms
step:1775/1775 val_loss:3.2782 train_time:106463ms step_avg:59.98ms
peak memory allocated: 28944 MiB reserved: 44738 MiB

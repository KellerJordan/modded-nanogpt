import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Jan 16 22:19:16 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     44294      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    1   N/A  N/A     44295      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A     44296      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A     44297      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A     44298      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A     44299      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A     44300      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A     44301      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8310 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:93ms step_avg:93.04ms
step:2/1775 train_time:115ms step_avg:57.56ms
step:3/1775 train_time:135ms step_avg:44.90ms
step:4/1775 train_time:166ms step_avg:41.40ms
step:5/1775 train_time:197ms step_avg:39.33ms
step:6/1775 train_time:292ms step_avg:48.62ms
step:7/1775 train_time:309ms step_avg:44.15ms
step:8/1775 train_time:330ms step_avg:41.27ms
step:9/1775 train_time:361ms step_avg:40.16ms
step:10/1775 train_time:395ms step_avg:39.47ms
step:11/1775 train_time:426ms step_avg:38.71ms
step:12/1775 train_time:459ms step_avg:38.26ms
step:13/1775 train_time:490ms step_avg:37.73ms
step:14/1775 train_time:524ms step_avg:37.45ms
step:15/1775 train_time:555ms step_avg:37.03ms
step:16/1775 train_time:589ms step_avg:36.83ms
step:17/1775 train_time:621ms step_avg:36.50ms
step:18/1775 train_time:654ms step_avg:36.34ms
step:19/1775 train_time:685ms step_avg:36.08ms
step:20/1775 train_time:719ms step_avg:35.94ms
step:21/1775 train_time:750ms step_avg:35.72ms
step:22/1775 train_time:784ms step_avg:35.63ms
step:23/1775 train_time:815ms step_avg:35.44ms
step:24/1775 train_time:849ms step_avg:35.37ms
step:25/1775 train_time:880ms step_avg:35.21ms
step:26/1775 train_time:914ms step_avg:35.14ms
step:27/1775 train_time:945ms step_avg:35.00ms
step:28/1775 train_time:978ms step_avg:34.94ms
step:29/1775 train_time:1010ms step_avg:34.82ms
step:30/1775 train_time:1043ms step_avg:34.77ms
step:31/1775 train_time:1075ms step_avg:34.67ms
step:32/1775 train_time:1108ms step_avg:34.64ms
step:33/1775 train_time:1140ms step_avg:34.55ms
step:34/1775 train_time:1174ms step_avg:34.54ms
step:35/1775 train_time:1207ms step_avg:34.47ms
step:36/1775 train_time:1241ms step_avg:34.47ms
step:37/1775 train_time:1274ms step_avg:34.44ms
step:38/1775 train_time:1309ms step_avg:34.45ms
step:39/1775 train_time:1341ms step_avg:34.39ms
step:40/1775 train_time:1375ms step_avg:34.38ms
step:41/1775 train_time:1407ms step_avg:34.31ms
step:42/1775 train_time:1441ms step_avg:34.31ms
step:43/1775 train_time:1472ms step_avg:34.24ms
step:44/1775 train_time:1506ms step_avg:34.23ms
step:45/1775 train_time:1538ms step_avg:34.18ms
step:46/1775 train_time:1573ms step_avg:34.19ms
step:47/1775 train_time:1605ms step_avg:34.14ms
step:48/1775 train_time:1638ms step_avg:34.13ms
step:49/1775 train_time:1670ms step_avg:34.07ms
step:50/1775 train_time:1703ms step_avg:34.06ms
step:51/1775 train_time:1734ms step_avg:34.01ms
step:52/1775 train_time:1769ms step_avg:34.01ms
step:53/1775 train_time:1800ms step_avg:33.97ms
step:54/1775 train_time:1834ms step_avg:33.96ms
step:55/1775 train_time:1865ms step_avg:33.91ms
step:56/1775 train_time:1898ms step_avg:33.90ms
step:57/1775 train_time:1930ms step_avg:33.86ms
step:58/1775 train_time:1963ms step_avg:33.85ms
step:59/1775 train_time:1995ms step_avg:33.81ms
step:60/1775 train_time:2029ms step_avg:33.81ms
step:61/1775 train_time:2060ms step_avg:33.77ms
step:62/1775 train_time:2094ms step_avg:33.77ms
step:63/1775 train_time:2126ms step_avg:33.74ms
step:64/1775 train_time:2160ms step_avg:33.75ms
step:65/1775 train_time:2191ms step_avg:33.71ms
step:66/1775 train_time:2225ms step_avg:33.71ms
step:67/1775 train_time:2257ms step_avg:33.68ms
step:68/1775 train_time:2291ms step_avg:33.70ms
step:69/1775 train_time:2323ms step_avg:33.67ms
step:70/1775 train_time:2357ms step_avg:33.68ms
step:71/1775 train_time:2389ms step_avg:33.65ms
step:72/1775 train_time:2423ms step_avg:33.66ms
step:73/1775 train_time:2455ms step_avg:33.63ms
step:74/1775 train_time:2489ms step_avg:33.64ms
step:75/1775 train_time:2521ms step_avg:33.61ms
step:76/1775 train_time:2555ms step_avg:33.61ms
step:77/1775 train_time:2586ms step_avg:33.59ms
step:78/1775 train_time:2620ms step_avg:33.58ms
step:79/1775 train_time:2653ms step_avg:33.58ms
step:80/1775 train_time:2685ms step_avg:33.57ms
step:81/1775 train_time:2717ms step_avg:33.54ms
step:82/1775 train_time:2751ms step_avg:33.55ms
step:83/1775 train_time:2783ms step_avg:33.52ms
step:84/1775 train_time:2816ms step_avg:33.53ms
step:85/1775 train_time:2848ms step_avg:33.51ms
step:86/1775 train_time:2882ms step_avg:33.51ms
step:87/1775 train_time:2913ms step_avg:33.48ms
step:88/1775 train_time:2947ms step_avg:33.49ms
step:89/1775 train_time:2978ms step_avg:33.46ms
step:90/1775 train_time:3012ms step_avg:33.47ms
step:91/1775 train_time:3044ms step_avg:33.45ms
step:92/1775 train_time:3077ms step_avg:33.45ms
step:93/1775 train_time:3108ms step_avg:33.42ms
step:94/1775 train_time:3142ms step_avg:33.42ms
step:95/1775 train_time:3173ms step_avg:33.40ms
step:96/1775 train_time:3207ms step_avg:33.41ms
step:97/1775 train_time:3239ms step_avg:33.39ms
step:98/1775 train_time:3273ms step_avg:33.40ms
step:99/1775 train_time:3305ms step_avg:33.38ms
step:100/1775 train_time:3338ms step_avg:33.38ms
step:101/1775 train_time:3370ms step_avg:33.37ms
step:102/1775 train_time:3404ms step_avg:33.38ms
step:103/1775 train_time:3436ms step_avg:33.36ms
step:104/1775 train_time:3471ms step_avg:33.37ms
step:105/1775 train_time:3502ms step_avg:33.35ms
step:106/1775 train_time:3536ms step_avg:33.35ms
step:107/1775 train_time:3567ms step_avg:33.33ms
step:108/1775 train_time:3601ms step_avg:33.34ms
step:109/1775 train_time:3632ms step_avg:33.32ms
step:110/1775 train_time:3666ms step_avg:33.33ms
step:111/1775 train_time:3698ms step_avg:33.31ms
step:112/1775 train_time:3731ms step_avg:33.31ms
step:113/1775 train_time:3763ms step_avg:33.30ms
step:114/1775 train_time:3796ms step_avg:33.30ms
step:115/1775 train_time:3828ms step_avg:33.29ms
step:116/1775 train_time:3861ms step_avg:33.29ms
step:117/1775 train_time:3893ms step_avg:33.27ms
step:118/1775 train_time:3927ms step_avg:33.28ms
step:119/1775 train_time:3958ms step_avg:33.26ms
step:120/1775 train_time:3992ms step_avg:33.27ms
step:121/1775 train_time:4024ms step_avg:33.25ms
step:122/1775 train_time:4057ms step_avg:33.26ms
step:123/1775 train_time:4089ms step_avg:33.24ms
step:124/1775 train_time:4123ms step_avg:33.25ms
step:125/1775 train_time:4154ms step_avg:33.23ms
step:126/1775 train_time:4188ms step_avg:33.24ms
step:127/1775 train_time:4220ms step_avg:33.23ms
step:128/1775 train_time:4254ms step_avg:33.23ms
step:129/1775 train_time:4285ms step_avg:33.22ms
step:130/1775 train_time:4319ms step_avg:33.22ms
step:131/1775 train_time:4350ms step_avg:33.21ms
step:132/1775 train_time:4384ms step_avg:33.21ms
step:133/1775 train_time:4416ms step_avg:33.20ms
step:134/1775 train_time:4450ms step_avg:33.21ms
step:135/1775 train_time:4482ms step_avg:33.20ms
step:136/1775 train_time:4515ms step_avg:33.20ms
step:137/1775 train_time:4547ms step_avg:33.19ms
step:138/1775 train_time:4580ms step_avg:33.19ms
step:139/1775 train_time:4612ms step_avg:33.18ms
step:140/1775 train_time:4646ms step_avg:33.19ms
step:141/1775 train_time:4677ms step_avg:33.17ms
step:142/1775 train_time:4711ms step_avg:33.18ms
step:143/1775 train_time:4743ms step_avg:33.17ms
step:144/1775 train_time:4776ms step_avg:33.17ms
step:145/1775 train_time:4808ms step_avg:33.16ms
step:146/1775 train_time:4841ms step_avg:33.16ms
step:147/1775 train_time:4873ms step_avg:33.15ms
step:148/1775 train_time:4907ms step_avg:33.16ms
step:149/1775 train_time:4939ms step_avg:33.15ms
step:150/1775 train_time:4972ms step_avg:33.15ms
step:151/1775 train_time:5003ms step_avg:33.14ms
step:152/1775 train_time:5037ms step_avg:33.14ms
step:153/1775 train_time:5069ms step_avg:33.13ms
step:154/1775 train_time:5102ms step_avg:33.13ms
step:155/1775 train_time:5134ms step_avg:33.12ms
step:156/1775 train_time:5168ms step_avg:33.13ms
step:157/1775 train_time:5199ms step_avg:33.12ms
step:158/1775 train_time:5233ms step_avg:33.12ms
step:159/1775 train_time:5265ms step_avg:33.11ms
step:160/1775 train_time:5298ms step_avg:33.12ms
step:161/1775 train_time:5330ms step_avg:33.11ms
step:162/1775 train_time:5364ms step_avg:33.11ms
step:163/1775 train_time:5395ms step_avg:33.10ms
step:164/1775 train_time:5430ms step_avg:33.11ms
step:165/1775 train_time:5461ms step_avg:33.10ms
step:166/1775 train_time:5495ms step_avg:33.10ms
step:167/1775 train_time:5526ms step_avg:33.09ms
step:168/1775 train_time:5560ms step_avg:33.09ms
step:169/1775 train_time:5591ms step_avg:33.09ms
step:170/1775 train_time:5625ms step_avg:33.09ms
step:171/1775 train_time:5657ms step_avg:33.08ms
step:172/1775 train_time:5691ms step_avg:33.09ms
step:173/1775 train_time:5722ms step_avg:33.08ms
step:174/1775 train_time:5756ms step_avg:33.08ms
step:175/1775 train_time:5788ms step_avg:33.07ms
step:176/1775 train_time:5821ms step_avg:33.07ms
step:177/1775 train_time:5853ms step_avg:33.07ms
step:178/1775 train_time:5887ms step_avg:33.07ms
step:179/1775 train_time:5918ms step_avg:33.06ms
step:180/1775 train_time:5952ms step_avg:33.07ms
step:181/1775 train_time:5984ms step_avg:33.06ms
step:182/1775 train_time:6017ms step_avg:33.06ms
step:183/1775 train_time:6049ms step_avg:33.05ms
step:184/1775 train_time:6082ms step_avg:33.06ms
step:185/1775 train_time:6113ms step_avg:33.05ms
step:186/1775 train_time:6147ms step_avg:33.05ms
step:187/1775 train_time:6179ms step_avg:33.04ms
step:188/1775 train_time:6212ms step_avg:33.05ms
step:189/1775 train_time:6244ms step_avg:33.04ms
step:190/1775 train_time:6278ms step_avg:33.04ms
step:191/1775 train_time:6309ms step_avg:33.03ms
step:192/1775 train_time:6344ms step_avg:33.04ms
step:193/1775 train_time:6375ms step_avg:33.03ms
step:194/1775 train_time:6409ms step_avg:33.04ms
step:195/1775 train_time:6440ms step_avg:33.03ms
step:196/1775 train_time:6474ms step_avg:33.03ms
step:197/1775 train_time:6505ms step_avg:33.02ms
step:198/1775 train_time:6539ms step_avg:33.03ms
step:199/1775 train_time:6571ms step_avg:33.02ms
step:200/1775 train_time:6604ms step_avg:33.02ms
step:201/1775 train_time:6636ms step_avg:33.01ms
step:202/1775 train_time:6670ms step_avg:33.02ms
step:203/1775 train_time:6702ms step_avg:33.02ms
step:204/1775 train_time:6736ms step_avg:33.02ms
step:205/1775 train_time:6767ms step_avg:33.01ms
step:206/1775 train_time:6801ms step_avg:33.01ms
step:207/1775 train_time:6832ms step_avg:33.01ms
step:208/1775 train_time:6866ms step_avg:33.01ms
step:209/1775 train_time:6897ms step_avg:33.00ms
step:210/1775 train_time:6931ms step_avg:33.01ms
step:211/1775 train_time:6963ms step_avg:33.00ms
step:212/1775 train_time:6996ms step_avg:33.00ms
step:213/1775 train_time:7027ms step_avg:32.99ms
step:214/1775 train_time:7061ms step_avg:32.99ms
step:215/1775 train_time:7093ms step_avg:32.99ms
step:216/1775 train_time:7126ms step_avg:32.99ms
step:217/1775 train_time:7158ms step_avg:32.99ms
step:218/1775 train_time:7192ms step_avg:32.99ms
step:219/1775 train_time:7223ms step_avg:32.98ms
step:220/1775 train_time:7257ms step_avg:32.99ms
step:221/1775 train_time:7289ms step_avg:32.98ms
step:222/1775 train_time:7322ms step_avg:32.98ms
step:223/1775 train_time:7354ms step_avg:32.98ms
step:224/1775 train_time:7388ms step_avg:32.98ms
step:225/1775 train_time:7419ms step_avg:32.97ms
step:226/1775 train_time:7453ms step_avg:32.98ms
step:227/1775 train_time:7485ms step_avg:32.97ms
step:228/1775 train_time:7518ms step_avg:32.97ms
step:229/1775 train_time:7550ms step_avg:32.97ms
step:230/1775 train_time:7584ms step_avg:32.97ms
step:231/1775 train_time:7615ms step_avg:32.97ms
step:232/1775 train_time:7649ms step_avg:32.97ms
step:233/1775 train_time:7681ms step_avg:32.96ms
step:234/1775 train_time:7714ms step_avg:32.97ms
step:235/1775 train_time:7746ms step_avg:32.96ms
step:236/1775 train_time:7779ms step_avg:32.96ms
step:237/1775 train_time:7810ms step_avg:32.96ms
step:238/1775 train_time:7844ms step_avg:32.96ms
step:239/1775 train_time:7876ms step_avg:32.95ms
step:240/1775 train_time:7909ms step_avg:32.96ms
step:241/1775 train_time:7941ms step_avg:32.95ms
step:242/1775 train_time:7975ms step_avg:32.95ms
step:243/1775 train_time:8006ms step_avg:32.95ms
step:244/1775 train_time:8040ms step_avg:32.95ms
step:245/1775 train_time:8072ms step_avg:32.95ms
step:246/1775 train_time:8105ms step_avg:32.95ms
step:247/1775 train_time:8137ms step_avg:32.94ms
step:248/1775 train_time:8171ms step_avg:32.95ms
step:249/1775 train_time:8202ms step_avg:32.94ms
step:250/1775 train_time:8236ms step_avg:32.94ms
step:250/1775 val_loss:4.6082 train_time:8277ms step_avg:33.11ms
step:251/1775 train_time:8297ms step_avg:33.06ms
step:252/1775 train_time:8316ms step_avg:33.00ms
step:253/1775 train_time:8334ms step_avg:32.94ms
step:254/1775 train_time:8368ms step_avg:32.95ms
step:255/1775 train_time:8401ms step_avg:32.95ms
step:256/1775 train_time:8436ms step_avg:32.95ms
step:257/1775 train_time:8467ms step_avg:32.95ms
step:258/1775 train_time:8501ms step_avg:32.95ms
step:259/1775 train_time:8532ms step_avg:32.94ms
step:260/1775 train_time:8566ms step_avg:32.95ms
step:261/1775 train_time:8597ms step_avg:32.94ms
step:262/1775 train_time:8630ms step_avg:32.94ms
step:263/1775 train_time:8661ms step_avg:32.93ms
step:264/1775 train_time:8695ms step_avg:32.93ms
step:265/1775 train_time:8726ms step_avg:32.93ms
step:266/1775 train_time:8760ms step_avg:32.93ms
step:267/1775 train_time:8792ms step_avg:32.93ms
step:268/1775 train_time:8825ms step_avg:32.93ms
step:269/1775 train_time:8856ms step_avg:32.92ms
step:270/1775 train_time:8890ms step_avg:32.92ms
step:271/1775 train_time:8921ms step_avg:32.92ms
step:272/1775 train_time:8954ms step_avg:32.92ms
step:273/1775 train_time:8986ms step_avg:32.91ms
step:274/1775 train_time:9019ms step_avg:32.92ms
step:275/1775 train_time:9050ms step_avg:32.91ms
step:276/1775 train_time:9084ms step_avg:32.91ms
step:277/1775 train_time:9115ms step_avg:32.91ms
step:278/1775 train_time:9148ms step_avg:32.91ms
step:279/1775 train_time:9180ms step_avg:32.90ms
step:280/1775 train_time:9214ms step_avg:32.91ms
step:281/1775 train_time:9246ms step_avg:32.90ms
step:282/1775 train_time:9280ms step_avg:32.91ms
step:283/1775 train_time:9312ms step_avg:32.90ms
step:284/1775 train_time:9346ms step_avg:32.91ms
step:285/1775 train_time:9378ms step_avg:32.90ms
step:286/1775 train_time:9411ms step_avg:32.91ms
step:287/1775 train_time:9443ms step_avg:32.90ms
step:288/1775 train_time:9477ms step_avg:32.91ms
step:289/1775 train_time:9508ms step_avg:32.90ms
step:290/1775 train_time:9542ms step_avg:32.90ms
step:291/1775 train_time:9574ms step_avg:32.90ms
step:292/1775 train_time:9607ms step_avg:32.90ms
step:293/1775 train_time:9638ms step_avg:32.90ms
step:294/1775 train_time:9672ms step_avg:32.90ms
step:295/1775 train_time:9703ms step_avg:32.89ms
step:296/1775 train_time:9737ms step_avg:32.90ms
step:297/1775 train_time:9768ms step_avg:32.89ms
step:298/1775 train_time:9802ms step_avg:32.89ms
step:299/1775 train_time:9834ms step_avg:32.89ms
step:300/1775 train_time:9868ms step_avg:32.89ms
step:301/1775 train_time:9899ms step_avg:32.89ms
step:302/1775 train_time:9933ms step_avg:32.89ms
step:303/1775 train_time:9965ms step_avg:32.89ms
step:304/1775 train_time:9999ms step_avg:32.89ms
step:305/1775 train_time:10030ms step_avg:32.89ms
step:306/1775 train_time:10063ms step_avg:32.89ms
step:307/1775 train_time:10095ms step_avg:32.88ms
step:308/1775 train_time:10128ms step_avg:32.88ms
step:309/1775 train_time:10160ms step_avg:32.88ms
step:310/1775 train_time:10194ms step_avg:32.88ms
step:311/1775 train_time:10225ms step_avg:32.88ms
step:312/1775 train_time:10259ms step_avg:32.88ms
step:313/1775 train_time:10291ms step_avg:32.88ms
step:314/1775 train_time:10324ms step_avg:32.88ms
step:315/1775 train_time:10356ms step_avg:32.88ms
step:316/1775 train_time:10390ms step_avg:32.88ms
step:317/1775 train_time:10421ms step_avg:32.87ms
step:318/1775 train_time:10455ms step_avg:32.88ms
step:319/1775 train_time:10487ms step_avg:32.87ms
step:320/1775 train_time:10521ms step_avg:32.88ms
step:321/1775 train_time:10553ms step_avg:32.87ms
step:322/1775 train_time:10586ms step_avg:32.88ms
step:323/1775 train_time:10618ms step_avg:32.87ms
step:324/1775 train_time:10652ms step_avg:32.88ms
step:325/1775 train_time:10683ms step_avg:32.87ms
step:326/1775 train_time:10717ms step_avg:32.87ms
step:327/1775 train_time:10748ms step_avg:32.87ms
step:328/1775 train_time:10782ms step_avg:32.87ms
step:329/1775 train_time:10813ms step_avg:32.87ms
step:330/1775 train_time:10846ms step_avg:32.87ms
step:331/1775 train_time:10878ms step_avg:32.86ms
step:332/1775 train_time:10911ms step_avg:32.86ms
step:333/1775 train_time:10942ms step_avg:32.86ms
step:334/1775 train_time:10976ms step_avg:32.86ms
step:335/1775 train_time:11008ms step_avg:32.86ms
step:336/1775 train_time:11041ms step_avg:32.86ms
step:337/1775 train_time:11072ms step_avg:32.86ms
step:338/1775 train_time:11106ms step_avg:32.86ms
step:339/1775 train_time:11138ms step_avg:32.86ms
step:340/1775 train_time:11172ms step_avg:32.86ms
step:341/1775 train_time:11204ms step_avg:32.86ms
step:342/1775 train_time:11239ms step_avg:32.86ms
step:343/1775 train_time:11270ms step_avg:32.86ms
step:344/1775 train_time:11304ms step_avg:32.86ms
step:345/1775 train_time:11335ms step_avg:32.86ms
step:346/1775 train_time:11369ms step_avg:32.86ms
step:347/1775 train_time:11400ms step_avg:32.85ms
step:348/1775 train_time:11434ms step_avg:32.86ms
step:349/1775 train_time:11466ms step_avg:32.85ms
step:350/1775 train_time:11500ms step_avg:32.86ms
step:351/1775 train_time:11532ms step_avg:32.85ms
step:352/1775 train_time:11565ms step_avg:32.86ms
step:353/1775 train_time:11597ms step_avg:32.85ms
step:354/1775 train_time:11630ms step_avg:32.85ms
step:355/1775 train_time:11662ms step_avg:32.85ms
step:356/1775 train_time:11696ms step_avg:32.85ms
step:357/1775 train_time:11728ms step_avg:32.85ms
step:358/1775 train_time:11761ms step_avg:32.85ms
step:359/1775 train_time:11793ms step_avg:32.85ms
step:360/1775 train_time:11826ms step_avg:32.85ms
step:361/1775 train_time:11858ms step_avg:32.85ms
step:362/1775 train_time:11891ms step_avg:32.85ms
step:363/1775 train_time:11922ms step_avg:32.84ms
step:364/1775 train_time:11956ms step_avg:32.84ms
step:365/1775 train_time:11987ms step_avg:32.84ms
step:366/1775 train_time:12021ms step_avg:32.84ms
step:367/1775 train_time:12052ms step_avg:32.84ms
step:368/1775 train_time:12086ms step_avg:32.84ms
step:369/1775 train_time:12117ms step_avg:32.84ms
step:370/1775 train_time:12151ms step_avg:32.84ms
step:371/1775 train_time:12183ms step_avg:32.84ms
step:372/1775 train_time:12217ms step_avg:32.84ms
step:373/1775 train_time:12249ms step_avg:32.84ms
step:374/1775 train_time:12283ms step_avg:32.84ms
step:375/1775 train_time:12314ms step_avg:32.84ms
step:376/1775 train_time:12348ms step_avg:32.84ms
step:377/1775 train_time:12379ms step_avg:32.84ms
step:378/1775 train_time:12413ms step_avg:32.84ms
step:379/1775 train_time:12445ms step_avg:32.84ms
step:380/1775 train_time:12479ms step_avg:32.84ms
step:381/1775 train_time:12510ms step_avg:32.83ms
step:382/1775 train_time:12544ms step_avg:32.84ms
step:383/1775 train_time:12576ms step_avg:32.84ms
step:384/1775 train_time:12609ms step_avg:32.84ms
step:385/1775 train_time:12641ms step_avg:32.83ms
step:386/1775 train_time:12674ms step_avg:32.84ms
step:387/1775 train_time:12706ms step_avg:32.83ms
step:388/1775 train_time:12740ms step_avg:32.84ms
step:389/1775 train_time:12771ms step_avg:32.83ms
step:390/1775 train_time:12805ms step_avg:32.83ms
step:391/1775 train_time:12836ms step_avg:32.83ms
step:392/1775 train_time:12870ms step_avg:32.83ms
step:393/1775 train_time:12902ms step_avg:32.83ms
step:394/1775 train_time:12936ms step_avg:32.83ms
step:395/1775 train_time:12967ms step_avg:32.83ms
step:396/1775 train_time:13001ms step_avg:32.83ms
step:397/1775 train_time:13033ms step_avg:32.83ms
step:398/1775 train_time:13066ms step_avg:32.83ms
step:399/1775 train_time:13098ms step_avg:32.83ms
step:400/1775 train_time:13131ms step_avg:32.83ms
step:401/1775 train_time:13163ms step_avg:32.83ms
step:402/1775 train_time:13197ms step_avg:32.83ms
step:403/1775 train_time:13229ms step_avg:32.83ms
step:404/1775 train_time:13262ms step_avg:32.83ms
step:405/1775 train_time:13294ms step_avg:32.82ms
step:406/1775 train_time:13328ms step_avg:32.83ms
step:407/1775 train_time:13359ms step_avg:32.82ms
step:408/1775 train_time:13393ms step_avg:32.83ms
step:409/1775 train_time:13425ms step_avg:32.82ms
step:410/1775 train_time:13458ms step_avg:32.82ms
step:411/1775 train_time:13490ms step_avg:32.82ms
step:412/1775 train_time:13524ms step_avg:32.82ms
step:413/1775 train_time:13555ms step_avg:32.82ms
step:414/1775 train_time:13589ms step_avg:32.82ms
step:415/1775 train_time:13620ms step_avg:32.82ms
step:416/1775 train_time:13654ms step_avg:32.82ms
step:417/1775 train_time:13686ms step_avg:32.82ms
step:418/1775 train_time:13720ms step_avg:32.82ms
step:419/1775 train_time:13751ms step_avg:32.82ms
step:420/1775 train_time:13785ms step_avg:32.82ms
step:421/1775 train_time:13817ms step_avg:32.82ms
step:422/1775 train_time:13850ms step_avg:32.82ms
step:423/1775 train_time:13882ms step_avg:32.82ms
step:424/1775 train_time:13916ms step_avg:32.82ms
step:425/1775 train_time:13947ms step_avg:32.82ms
step:426/1775 train_time:13981ms step_avg:32.82ms
step:427/1775 train_time:14013ms step_avg:32.82ms
step:428/1775 train_time:14046ms step_avg:32.82ms
step:429/1775 train_time:14078ms step_avg:32.82ms
step:430/1775 train_time:14111ms step_avg:32.82ms
step:431/1775 train_time:14143ms step_avg:32.81ms
step:432/1775 train_time:14176ms step_avg:32.82ms
step:433/1775 train_time:14208ms step_avg:32.81ms
step:434/1775 train_time:14242ms step_avg:32.82ms
step:435/1775 train_time:14274ms step_avg:32.81ms
step:436/1775 train_time:14308ms step_avg:32.82ms
step:437/1775 train_time:14339ms step_avg:32.81ms
step:438/1775 train_time:14373ms step_avg:32.82ms
step:439/1775 train_time:14404ms step_avg:32.81ms
step:440/1775 train_time:14439ms step_avg:32.82ms
step:441/1775 train_time:14471ms step_avg:32.81ms
step:442/1775 train_time:14505ms step_avg:32.82ms
step:443/1775 train_time:14536ms step_avg:32.81ms
step:444/1775 train_time:14570ms step_avg:32.81ms
step:445/1775 train_time:14602ms step_avg:32.81ms
step:446/1775 train_time:14636ms step_avg:32.82ms
step:447/1775 train_time:14668ms step_avg:32.81ms
step:448/1775 train_time:14701ms step_avg:32.81ms
step:449/1775 train_time:14732ms step_avg:32.81ms
step:450/1775 train_time:14766ms step_avg:32.81ms
step:451/1775 train_time:14798ms step_avg:32.81ms
step:452/1775 train_time:14831ms step_avg:32.81ms
step:453/1775 train_time:14863ms step_avg:32.81ms
step:454/1775 train_time:14897ms step_avg:32.81ms
step:455/1775 train_time:14928ms step_avg:32.81ms
step:456/1775 train_time:14962ms step_avg:32.81ms
step:457/1775 train_time:14994ms step_avg:32.81ms
step:458/1775 train_time:15027ms step_avg:32.81ms
step:459/1775 train_time:15059ms step_avg:32.81ms
step:460/1775 train_time:15093ms step_avg:32.81ms
step:461/1775 train_time:15124ms step_avg:32.81ms
step:462/1775 train_time:15158ms step_avg:32.81ms
step:463/1775 train_time:15189ms step_avg:32.81ms
step:464/1775 train_time:15223ms step_avg:32.81ms
step:465/1775 train_time:15254ms step_avg:32.81ms
step:466/1775 train_time:15288ms step_avg:32.81ms
step:467/1775 train_time:15320ms step_avg:32.80ms
step:468/1775 train_time:15354ms step_avg:32.81ms
step:469/1775 train_time:15385ms step_avg:32.80ms
step:470/1775 train_time:15420ms step_avg:32.81ms
step:471/1775 train_time:15452ms step_avg:32.81ms
step:472/1775 train_time:15485ms step_avg:32.81ms
step:473/1775 train_time:15516ms step_avg:32.80ms
step:474/1775 train_time:15550ms step_avg:32.81ms
step:475/1775 train_time:15582ms step_avg:32.81ms
step:476/1775 train_time:15616ms step_avg:32.81ms
step:477/1775 train_time:15647ms step_avg:32.80ms
step:478/1775 train_time:15682ms step_avg:32.81ms
step:479/1775 train_time:15713ms step_avg:32.80ms
step:480/1775 train_time:15747ms step_avg:32.81ms
step:481/1775 train_time:15778ms step_avg:32.80ms
step:482/1775 train_time:15812ms step_avg:32.80ms
step:483/1775 train_time:15843ms step_avg:32.80ms
step:484/1775 train_time:15877ms step_avg:32.80ms
step:485/1775 train_time:15908ms step_avg:32.80ms
step:486/1775 train_time:15942ms step_avg:32.80ms
step:487/1775 train_time:15974ms step_avg:32.80ms
step:488/1775 train_time:16007ms step_avg:32.80ms
step:489/1775 train_time:16039ms step_avg:32.80ms
step:490/1775 train_time:16073ms step_avg:32.80ms
step:491/1775 train_time:16104ms step_avg:32.80ms
step:492/1775 train_time:16139ms step_avg:32.80ms
step:493/1775 train_time:16170ms step_avg:32.80ms
step:494/1775 train_time:16204ms step_avg:32.80ms
step:495/1775 train_time:16235ms step_avg:32.80ms
step:496/1775 train_time:16268ms step_avg:32.80ms
step:497/1775 train_time:16300ms step_avg:32.80ms
step:498/1775 train_time:16334ms step_avg:32.80ms
step:499/1775 train_time:16366ms step_avg:32.80ms
step:500/1775 train_time:16400ms step_avg:32.80ms
step:500/1775 val_loss:4.2747 train_time:16441ms step_avg:32.88ms
step:501/1775 train_time:16463ms step_avg:32.86ms
step:502/1775 train_time:16482ms step_avg:32.83ms
step:503/1775 train_time:16499ms step_avg:32.80ms
step:504/1775 train_time:16532ms step_avg:32.80ms
step:505/1775 train_time:16565ms step_avg:32.80ms
step:506/1775 train_time:16599ms step_avg:32.80ms
step:507/1775 train_time:16631ms step_avg:32.80ms
step:508/1775 train_time:16664ms step_avg:32.80ms
step:509/1775 train_time:16696ms step_avg:32.80ms
step:510/1775 train_time:16730ms step_avg:32.80ms
step:511/1775 train_time:16761ms step_avg:32.80ms
step:512/1775 train_time:16795ms step_avg:32.80ms
step:513/1775 train_time:16827ms step_avg:32.80ms
step:514/1775 train_time:16860ms step_avg:32.80ms
step:515/1775 train_time:16892ms step_avg:32.80ms
step:516/1775 train_time:16925ms step_avg:32.80ms
step:517/1775 train_time:16957ms step_avg:32.80ms
step:518/1775 train_time:16990ms step_avg:32.80ms
step:519/1775 train_time:17021ms step_avg:32.80ms
step:520/1775 train_time:17055ms step_avg:32.80ms
step:521/1775 train_time:17086ms step_avg:32.79ms
step:522/1775 train_time:17119ms step_avg:32.79ms
step:523/1775 train_time:17150ms step_avg:32.79ms
step:524/1775 train_time:17184ms step_avg:32.79ms
step:525/1775 train_time:17215ms step_avg:32.79ms
step:526/1775 train_time:17248ms step_avg:32.79ms
step:527/1775 train_time:17280ms step_avg:32.79ms
step:528/1775 train_time:17313ms step_avg:32.79ms
step:529/1775 train_time:17345ms step_avg:32.79ms
step:530/1775 train_time:17379ms step_avg:32.79ms
step:531/1775 train_time:17412ms step_avg:32.79ms
step:532/1775 train_time:17446ms step_avg:32.79ms
step:533/1775 train_time:17478ms step_avg:32.79ms
step:534/1775 train_time:17513ms step_avg:32.79ms
step:535/1775 train_time:17545ms step_avg:32.79ms
step:536/1775 train_time:17578ms step_avg:32.80ms
step:537/1775 train_time:17610ms step_avg:32.79ms
step:538/1775 train_time:17644ms step_avg:32.80ms
step:539/1775 train_time:17676ms step_avg:32.79ms
step:540/1775 train_time:17710ms step_avg:32.80ms
step:541/1775 train_time:17741ms step_avg:32.79ms
step:542/1775 train_time:17775ms step_avg:32.80ms
step:543/1775 train_time:17807ms step_avg:32.79ms
step:544/1775 train_time:17840ms step_avg:32.79ms
step:545/1775 train_time:17871ms step_avg:32.79ms
step:546/1775 train_time:17905ms step_avg:32.79ms
step:547/1775 train_time:17936ms step_avg:32.79ms
step:548/1775 train_time:17970ms step_avg:32.79ms
step:549/1775 train_time:18001ms step_avg:32.79ms
step:550/1775 train_time:18035ms step_avg:32.79ms
step:551/1775 train_time:18067ms step_avg:32.79ms
step:552/1775 train_time:18100ms step_avg:32.79ms
step:553/1775 train_time:18132ms step_avg:32.79ms
step:554/1775 train_time:18165ms step_avg:32.79ms
step:555/1775 train_time:18196ms step_avg:32.79ms
step:556/1775 train_time:18230ms step_avg:32.79ms
step:557/1775 train_time:18261ms step_avg:32.78ms
step:558/1775 train_time:18295ms step_avg:32.79ms
step:559/1775 train_time:18327ms step_avg:32.78ms
step:560/1775 train_time:18360ms step_avg:32.79ms
step:561/1775 train_time:18391ms step_avg:32.78ms
step:562/1775 train_time:18425ms step_avg:32.79ms
step:563/1775 train_time:18457ms step_avg:32.78ms
step:564/1775 train_time:18491ms step_avg:32.79ms
step:565/1775 train_time:18523ms step_avg:32.78ms
step:566/1775 train_time:18557ms step_avg:32.79ms
step:567/1775 train_time:18588ms step_avg:32.78ms
step:568/1775 train_time:18622ms step_avg:32.79ms
step:569/1775 train_time:18654ms step_avg:32.78ms
step:570/1775 train_time:18687ms step_avg:32.78ms
step:571/1775 train_time:18718ms step_avg:32.78ms
step:572/1775 train_time:18752ms step_avg:32.78ms
step:573/1775 train_time:18785ms step_avg:32.78ms
step:574/1775 train_time:18818ms step_avg:32.78ms
step:575/1775 train_time:18850ms step_avg:32.78ms
step:576/1775 train_time:18883ms step_avg:32.78ms
step:577/1775 train_time:18914ms step_avg:32.78ms
step:578/1775 train_time:18948ms step_avg:32.78ms
step:579/1775 train_time:18980ms step_avg:32.78ms
step:580/1775 train_time:19016ms step_avg:32.79ms
step:581/1775 train_time:19075ms step_avg:32.83ms
step:582/1775 train_time:19135ms step_avg:32.88ms
step:583/1775 train_time:19193ms step_avg:32.92ms
step:584/1775 train_time:19254ms step_avg:32.97ms
step:585/1775 train_time:19314ms step_avg:33.01ms
step:586/1775 train_time:19374ms step_avg:33.06ms
step:587/1775 train_time:19433ms step_avg:33.10ms
step:588/1775 train_time:19494ms step_avg:33.15ms
step:589/1775 train_time:19553ms step_avg:33.20ms
step:590/1775 train_time:19614ms step_avg:33.24ms
step:591/1775 train_time:19673ms step_avg:33.29ms
step:592/1775 train_time:19734ms step_avg:33.33ms
step:593/1775 train_time:19793ms step_avg:33.38ms
step:594/1775 train_time:19854ms step_avg:33.42ms
step:595/1775 train_time:19913ms step_avg:33.47ms
step:596/1775 train_time:19973ms step_avg:33.51ms
step:597/1775 train_time:20031ms step_avg:33.55ms
step:598/1775 train_time:20092ms step_avg:33.60ms
step:599/1775 train_time:20151ms step_avg:33.64ms
step:600/1775 train_time:20212ms step_avg:33.69ms
step:601/1775 train_time:20270ms step_avg:33.73ms
step:602/1775 train_time:20331ms step_avg:33.77ms
step:603/1775 train_time:20390ms step_avg:33.81ms
step:604/1775 train_time:20451ms step_avg:33.86ms
step:605/1775 train_time:20509ms step_avg:33.90ms
step:606/1775 train_time:20571ms step_avg:33.95ms
step:607/1775 train_time:20630ms step_avg:33.99ms
step:608/1775 train_time:20691ms step_avg:34.03ms
step:609/1775 train_time:20750ms step_avg:34.07ms
step:610/1775 train_time:20812ms step_avg:34.12ms
step:611/1775 train_time:20870ms step_avg:34.16ms
step:612/1775 train_time:20931ms step_avg:34.20ms
step:613/1775 train_time:20991ms step_avg:34.24ms
step:614/1775 train_time:21051ms step_avg:34.29ms
step:615/1775 train_time:21109ms step_avg:34.32ms
step:616/1775 train_time:21170ms step_avg:34.37ms
step:617/1775 train_time:21228ms step_avg:34.41ms
step:618/1775 train_time:21288ms step_avg:34.45ms
step:619/1775 train_time:21348ms step_avg:34.49ms
step:620/1775 train_time:21409ms step_avg:34.53ms
step:621/1775 train_time:21467ms step_avg:34.57ms
step:622/1775 train_time:21529ms step_avg:34.61ms
step:623/1775 train_time:21587ms step_avg:34.65ms
step:624/1775 train_time:21649ms step_avg:34.69ms
step:625/1775 train_time:21708ms step_avg:34.73ms
step:626/1775 train_time:21769ms step_avg:34.78ms
step:627/1775 train_time:21828ms step_avg:34.81ms
step:628/1775 train_time:21890ms step_avg:34.86ms
step:629/1775 train_time:21948ms step_avg:34.89ms
step:630/1775 train_time:22009ms step_avg:34.94ms
step:631/1775 train_time:22068ms step_avg:34.97ms
step:632/1775 train_time:22129ms step_avg:35.01ms
step:633/1775 train_time:22188ms step_avg:35.05ms
step:634/1775 train_time:22248ms step_avg:35.09ms
step:635/1775 train_time:22306ms step_avg:35.13ms
step:636/1775 train_time:22367ms step_avg:35.17ms
step:637/1775 train_time:22427ms step_avg:35.21ms
step:638/1775 train_time:22488ms step_avg:35.25ms
step:639/1775 train_time:22547ms step_avg:35.28ms
step:640/1775 train_time:22609ms step_avg:35.33ms
step:641/1775 train_time:22668ms step_avg:35.36ms
step:642/1775 train_time:22729ms step_avg:35.40ms
step:643/1775 train_time:22789ms step_avg:35.44ms
step:644/1775 train_time:22850ms step_avg:35.48ms
step:645/1775 train_time:22908ms step_avg:35.52ms
step:646/1775 train_time:22969ms step_avg:35.56ms
step:647/1775 train_time:23028ms step_avg:35.59ms
step:648/1775 train_time:23089ms step_avg:35.63ms
step:649/1775 train_time:23147ms step_avg:35.67ms
step:650/1775 train_time:23208ms step_avg:35.70ms
step:651/1775 train_time:23267ms step_avg:35.74ms
step:652/1775 train_time:23328ms step_avg:35.78ms
step:653/1775 train_time:23386ms step_avg:35.81ms
step:654/1775 train_time:23447ms step_avg:35.85ms
step:655/1775 train_time:23506ms step_avg:35.89ms
step:656/1775 train_time:23567ms step_avg:35.93ms
step:657/1775 train_time:23626ms step_avg:35.96ms
step:658/1775 train_time:23688ms step_avg:36.00ms
step:659/1775 train_time:23747ms step_avg:36.04ms
step:660/1775 train_time:23809ms step_avg:36.07ms
step:661/1775 train_time:23867ms step_avg:36.11ms
step:662/1775 train_time:23928ms step_avg:36.15ms
step:663/1775 train_time:23987ms step_avg:36.18ms
step:664/1775 train_time:24048ms step_avg:36.22ms
step:665/1775 train_time:24106ms step_avg:36.25ms
step:666/1775 train_time:24167ms step_avg:36.29ms
step:667/1775 train_time:24226ms step_avg:36.32ms
step:668/1775 train_time:24286ms step_avg:36.36ms
step:669/1775 train_time:24345ms step_avg:36.39ms
step:670/1775 train_time:24406ms step_avg:36.43ms
step:671/1775 train_time:24465ms step_avg:36.46ms
step:672/1775 train_time:24525ms step_avg:36.50ms
step:673/1775 train_time:24584ms step_avg:36.53ms
step:674/1775 train_time:24645ms step_avg:36.57ms
step:675/1775 train_time:24703ms step_avg:36.60ms
step:676/1775 train_time:24764ms step_avg:36.63ms
step:677/1775 train_time:24823ms step_avg:36.67ms
step:678/1775 train_time:24884ms step_avg:36.70ms
step:679/1775 train_time:24943ms step_avg:36.73ms
step:680/1775 train_time:25004ms step_avg:36.77ms
step:681/1775 train_time:25063ms step_avg:36.80ms
step:682/1775 train_time:25124ms step_avg:36.84ms
step:683/1775 train_time:25182ms step_avg:36.87ms
step:684/1775 train_time:25244ms step_avg:36.91ms
step:685/1775 train_time:25302ms step_avg:36.94ms
step:686/1775 train_time:25363ms step_avg:36.97ms
step:687/1775 train_time:25421ms step_avg:37.00ms
step:688/1775 train_time:25483ms step_avg:37.04ms
step:689/1775 train_time:25541ms step_avg:37.07ms
step:690/1775 train_time:25602ms step_avg:37.10ms
step:691/1775 train_time:25661ms step_avg:37.14ms
step:692/1775 train_time:25723ms step_avg:37.17ms
step:693/1775 train_time:25781ms step_avg:37.20ms
step:694/1775 train_time:25843ms step_avg:37.24ms
step:695/1775 train_time:25901ms step_avg:37.27ms
step:696/1775 train_time:25963ms step_avg:37.30ms
step:697/1775 train_time:26022ms step_avg:37.33ms
step:698/1775 train_time:26082ms step_avg:37.37ms
step:699/1775 train_time:26140ms step_avg:37.40ms
step:700/1775 train_time:26202ms step_avg:37.43ms
step:701/1775 train_time:26260ms step_avg:37.46ms
step:702/1775 train_time:26322ms step_avg:37.50ms
step:703/1775 train_time:26379ms step_avg:37.52ms
step:704/1775 train_time:26441ms step_avg:37.56ms
step:705/1775 train_time:26500ms step_avg:37.59ms
step:706/1775 train_time:26562ms step_avg:37.62ms
step:707/1775 train_time:26621ms step_avg:37.65ms
step:708/1775 train_time:26683ms step_avg:37.69ms
step:709/1775 train_time:26741ms step_avg:37.72ms
step:710/1775 train_time:26802ms step_avg:37.75ms
step:711/1775 train_time:26861ms step_avg:37.78ms
step:712/1775 train_time:26923ms step_avg:37.81ms
step:713/1775 train_time:26982ms step_avg:37.84ms
step:714/1775 train_time:27043ms step_avg:37.88ms
step:715/1775 train_time:27101ms step_avg:37.90ms
step:716/1775 train_time:27162ms step_avg:37.94ms
step:717/1775 train_time:27221ms step_avg:37.96ms
step:718/1775 train_time:27282ms step_avg:38.00ms
step:719/1775 train_time:27341ms step_avg:38.03ms
step:720/1775 train_time:27402ms step_avg:38.06ms
step:721/1775 train_time:27461ms step_avg:38.09ms
step:722/1775 train_time:27522ms step_avg:38.12ms
step:723/1775 train_time:27580ms step_avg:38.15ms
step:724/1775 train_time:27642ms step_avg:38.18ms
step:725/1775 train_time:27701ms step_avg:38.21ms
step:726/1775 train_time:27762ms step_avg:38.24ms
step:727/1775 train_time:27821ms step_avg:38.27ms
step:728/1775 train_time:27882ms step_avg:38.30ms
step:729/1775 train_time:27941ms step_avg:38.33ms
step:730/1775 train_time:28002ms step_avg:38.36ms
step:731/1775 train_time:28060ms step_avg:38.39ms
step:732/1775 train_time:28122ms step_avg:38.42ms
step:733/1775 train_time:28180ms step_avg:38.44ms
step:734/1775 train_time:28241ms step_avg:38.48ms
step:735/1775 train_time:28299ms step_avg:38.50ms
step:736/1775 train_time:28361ms step_avg:38.53ms
step:737/1775 train_time:28420ms step_avg:38.56ms
step:738/1775 train_time:28481ms step_avg:38.59ms
step:739/1775 train_time:28540ms step_avg:38.62ms
step:740/1775 train_time:28602ms step_avg:38.65ms
step:741/1775 train_time:28661ms step_avg:38.68ms
step:742/1775 train_time:28722ms step_avg:38.71ms
step:743/1775 train_time:28781ms step_avg:38.74ms
step:744/1775 train_time:28842ms step_avg:38.77ms
step:745/1775 train_time:28901ms step_avg:38.79ms
step:746/1775 train_time:28962ms step_avg:38.82ms
step:747/1775 train_time:29021ms step_avg:38.85ms
step:748/1775 train_time:29082ms step_avg:38.88ms
step:749/1775 train_time:29140ms step_avg:38.91ms
step:750/1775 train_time:29201ms step_avg:38.94ms
step:750/1775 val_loss:3.9884 train_time:29272ms step_avg:39.03ms
step:751/1775 train_time:29293ms step_avg:39.01ms
step:752/1775 train_time:29323ms step_avg:38.99ms
step:753/1775 train_time:29385ms step_avg:39.02ms
step:754/1775 train_time:29446ms step_avg:39.05ms
step:755/1775 train_time:29507ms step_avg:39.08ms
step:756/1775 train_time:29568ms step_avg:39.11ms
step:757/1775 train_time:29625ms step_avg:39.14ms
step:758/1775 train_time:29686ms step_avg:39.16ms
step:759/1775 train_time:29744ms step_avg:39.19ms
step:760/1775 train_time:29804ms step_avg:39.22ms
step:761/1775 train_time:29863ms step_avg:39.24ms
step:762/1775 train_time:29924ms step_avg:39.27ms
step:763/1775 train_time:29982ms step_avg:39.30ms
step:764/1775 train_time:30043ms step_avg:39.32ms
step:765/1775 train_time:30102ms step_avg:39.35ms
step:766/1775 train_time:30162ms step_avg:39.38ms
step:767/1775 train_time:30222ms step_avg:39.40ms
step:768/1775 train_time:30286ms step_avg:39.43ms
step:769/1775 train_time:30345ms step_avg:39.46ms
step:770/1775 train_time:30407ms step_avg:39.49ms
step:771/1775 train_time:30467ms step_avg:39.52ms
step:772/1775 train_time:30528ms step_avg:39.54ms
step:773/1775 train_time:30586ms step_avg:39.57ms
step:774/1775 train_time:30647ms step_avg:39.60ms
step:775/1775 train_time:30705ms step_avg:39.62ms
step:776/1775 train_time:30766ms step_avg:39.65ms
step:777/1775 train_time:30824ms step_avg:39.67ms
step:778/1775 train_time:30884ms step_avg:39.70ms
step:779/1775 train_time:30942ms step_avg:39.72ms
step:780/1775 train_time:31003ms step_avg:39.75ms
step:781/1775 train_time:31061ms step_avg:39.77ms
step:782/1775 train_time:31123ms step_avg:39.80ms
step:783/1775 train_time:31181ms step_avg:39.82ms
step:784/1775 train_time:31244ms step_avg:39.85ms
step:785/1775 train_time:31303ms step_avg:39.88ms
step:786/1775 train_time:31365ms step_avg:39.90ms
step:787/1775 train_time:31424ms step_avg:39.93ms
step:788/1775 train_time:31486ms step_avg:39.96ms
step:789/1775 train_time:31546ms step_avg:39.98ms
step:790/1775 train_time:31607ms step_avg:40.01ms
step:791/1775 train_time:31665ms step_avg:40.03ms
step:792/1775 train_time:31727ms step_avg:40.06ms
step:793/1775 train_time:31785ms step_avg:40.08ms
step:794/1775 train_time:31845ms step_avg:40.11ms
step:795/1775 train_time:31903ms step_avg:40.13ms
step:796/1775 train_time:31963ms step_avg:40.16ms
step:797/1775 train_time:32022ms step_avg:40.18ms
step:798/1775 train_time:32083ms step_avg:40.20ms
step:799/1775 train_time:32141ms step_avg:40.23ms
step:800/1775 train_time:32202ms step_avg:40.25ms
step:801/1775 train_time:32262ms step_avg:40.28ms
step:802/1775 train_time:32324ms step_avg:40.30ms
step:803/1775 train_time:32384ms step_avg:40.33ms
step:804/1775 train_time:32445ms step_avg:40.35ms
step:805/1775 train_time:32504ms step_avg:40.38ms
step:806/1775 train_time:32566ms step_avg:40.40ms
step:807/1775 train_time:32624ms step_avg:40.43ms
step:808/1775 train_time:32685ms step_avg:40.45ms
step:809/1775 train_time:32743ms step_avg:40.47ms
step:810/1775 train_time:32804ms step_avg:40.50ms
step:811/1775 train_time:32862ms step_avg:40.52ms
step:812/1775 train_time:32923ms step_avg:40.55ms
step:813/1775 train_time:32981ms step_avg:40.57ms
step:814/1775 train_time:33041ms step_avg:40.59ms
step:815/1775 train_time:33099ms step_avg:40.61ms
step:816/1775 train_time:33161ms step_avg:40.64ms
step:817/1775 train_time:33220ms step_avg:40.66ms
step:818/1775 train_time:33281ms step_avg:40.69ms
step:819/1775 train_time:33340ms step_avg:40.71ms
step:820/1775 train_time:33402ms step_avg:40.73ms
step:821/1775 train_time:33462ms step_avg:40.76ms
step:822/1775 train_time:33524ms step_avg:40.78ms
step:823/1775 train_time:33583ms step_avg:40.81ms
step:824/1775 train_time:33644ms step_avg:40.83ms
step:825/1775 train_time:33703ms step_avg:40.85ms
step:826/1775 train_time:33764ms step_avg:40.88ms
step:827/1775 train_time:33822ms step_avg:40.90ms
step:828/1775 train_time:33882ms step_avg:40.92ms
step:829/1775 train_time:33941ms step_avg:40.94ms
step:830/1775 train_time:34001ms step_avg:40.97ms
step:831/1775 train_time:34060ms step_avg:40.99ms
step:832/1775 train_time:34121ms step_avg:41.01ms
step:833/1775 train_time:34180ms step_avg:41.03ms
step:834/1775 train_time:34241ms step_avg:41.06ms
step:835/1775 train_time:34300ms step_avg:41.08ms
step:836/1775 train_time:34362ms step_avg:41.10ms
step:837/1775 train_time:34421ms step_avg:41.12ms
step:838/1775 train_time:34482ms step_avg:41.15ms
step:839/1775 train_time:34541ms step_avg:41.17ms
step:840/1775 train_time:34604ms step_avg:41.20ms
step:841/1775 train_time:34663ms step_avg:41.22ms
step:842/1775 train_time:34724ms step_avg:41.24ms
step:843/1775 train_time:34783ms step_avg:41.26ms
step:844/1775 train_time:34844ms step_avg:41.28ms
step:845/1775 train_time:34902ms step_avg:41.30ms
step:846/1775 train_time:34963ms step_avg:41.33ms
step:847/1775 train_time:35022ms step_avg:41.35ms
step:848/1775 train_time:35083ms step_avg:41.37ms
step:849/1775 train_time:35141ms step_avg:41.39ms
step:850/1775 train_time:35203ms step_avg:41.42ms
step:851/1775 train_time:35263ms step_avg:41.44ms
step:852/1775 train_time:35324ms step_avg:41.46ms
step:853/1775 train_time:35383ms step_avg:41.48ms
step:854/1775 train_time:35444ms step_avg:41.50ms
step:855/1775 train_time:35503ms step_avg:41.52ms
step:856/1775 train_time:35565ms step_avg:41.55ms
step:857/1775 train_time:35624ms step_avg:41.57ms
step:858/1775 train_time:35685ms step_avg:41.59ms
step:859/1775 train_time:35743ms step_avg:41.61ms
step:860/1775 train_time:35804ms step_avg:41.63ms
step:861/1775 train_time:35862ms step_avg:41.65ms
step:862/1775 train_time:35924ms step_avg:41.67ms
step:863/1775 train_time:35982ms step_avg:41.69ms
step:864/1775 train_time:36043ms step_avg:41.72ms
step:865/1775 train_time:36101ms step_avg:41.74ms
step:866/1775 train_time:36162ms step_avg:41.76ms
step:867/1775 train_time:36222ms step_avg:41.78ms
step:868/1775 train_time:36283ms step_avg:41.80ms
step:869/1775 train_time:36341ms step_avg:41.82ms
step:870/1775 train_time:36402ms step_avg:41.84ms
step:871/1775 train_time:36461ms step_avg:41.86ms
step:872/1775 train_time:36523ms step_avg:41.88ms
step:873/1775 train_time:36581ms step_avg:41.90ms
step:874/1775 train_time:36642ms step_avg:41.92ms
step:875/1775 train_time:36701ms step_avg:41.94ms
step:876/1775 train_time:36763ms step_avg:41.97ms
step:877/1775 train_time:36822ms step_avg:41.99ms
step:878/1775 train_time:36882ms step_avg:42.01ms
step:879/1775 train_time:36941ms step_avg:42.03ms
step:880/1775 train_time:37001ms step_avg:42.05ms
step:881/1775 train_time:37060ms step_avg:42.07ms
step:882/1775 train_time:37121ms step_avg:42.09ms
step:883/1775 train_time:37180ms step_avg:42.11ms
step:884/1775 train_time:37241ms step_avg:42.13ms
step:885/1775 train_time:37300ms step_avg:42.15ms
step:886/1775 train_time:37361ms step_avg:42.17ms
step:887/1775 train_time:37420ms step_avg:42.19ms
step:888/1775 train_time:37481ms step_avg:42.21ms
step:889/1775 train_time:37540ms step_avg:42.23ms
step:890/1775 train_time:37602ms step_avg:42.25ms
step:891/1775 train_time:37661ms step_avg:42.27ms
step:892/1775 train_time:37722ms step_avg:42.29ms
step:893/1775 train_time:37781ms step_avg:42.31ms
step:894/1775 train_time:37842ms step_avg:42.33ms
step:895/1775 train_time:37901ms step_avg:42.35ms
step:896/1775 train_time:37962ms step_avg:42.37ms
step:897/1775 train_time:38020ms step_avg:42.39ms
step:898/1775 train_time:38081ms step_avg:42.41ms
step:899/1775 train_time:38139ms step_avg:42.42ms
step:900/1775 train_time:38200ms step_avg:42.44ms
step:901/1775 train_time:38259ms step_avg:42.46ms
step:902/1775 train_time:38321ms step_avg:42.48ms
step:903/1775 train_time:38380ms step_avg:42.50ms
step:904/1775 train_time:38441ms step_avg:42.52ms
step:905/1775 train_time:38500ms step_avg:42.54ms
step:906/1775 train_time:38562ms step_avg:42.56ms
step:907/1775 train_time:38621ms step_avg:42.58ms
step:908/1775 train_time:38682ms step_avg:42.60ms
step:909/1775 train_time:38741ms step_avg:42.62ms
step:910/1775 train_time:38802ms step_avg:42.64ms
step:911/1775 train_time:38861ms step_avg:42.66ms
step:912/1775 train_time:38922ms step_avg:42.68ms
step:913/1775 train_time:38980ms step_avg:42.69ms
step:914/1775 train_time:39041ms step_avg:42.71ms
step:915/1775 train_time:39100ms step_avg:42.73ms
step:916/1775 train_time:39161ms step_avg:42.75ms
step:917/1775 train_time:39220ms step_avg:42.77ms
step:918/1775 train_time:39280ms step_avg:42.79ms
step:919/1775 train_time:39340ms step_avg:42.81ms
step:920/1775 train_time:39402ms step_avg:42.83ms
step:921/1775 train_time:39461ms step_avg:42.85ms
step:922/1775 train_time:39523ms step_avg:42.87ms
step:923/1775 train_time:39581ms step_avg:42.88ms
step:924/1775 train_time:39642ms step_avg:42.90ms
step:925/1775 train_time:39701ms step_avg:42.92ms
step:926/1775 train_time:39762ms step_avg:42.94ms
step:927/1775 train_time:39821ms step_avg:42.96ms
step:928/1775 train_time:39882ms step_avg:42.98ms
step:929/1775 train_time:39941ms step_avg:42.99ms
step:930/1775 train_time:40003ms step_avg:43.01ms
step:931/1775 train_time:40061ms step_avg:43.03ms
step:932/1775 train_time:40123ms step_avg:43.05ms
step:933/1775 train_time:40181ms step_avg:43.07ms
step:934/1775 train_time:40242ms step_avg:43.09ms
step:935/1775 train_time:40301ms step_avg:43.10ms
step:936/1775 train_time:40362ms step_avg:43.12ms
step:937/1775 train_time:40421ms step_avg:43.14ms
step:938/1775 train_time:40483ms step_avg:43.16ms
step:939/1775 train_time:40542ms step_avg:43.18ms
step:940/1775 train_time:40603ms step_avg:43.19ms
step:941/1775 train_time:40661ms step_avg:43.21ms
step:942/1775 train_time:40723ms step_avg:43.23ms
step:943/1775 train_time:40781ms step_avg:43.25ms
step:944/1775 train_time:40842ms step_avg:43.26ms
step:945/1775 train_time:40901ms step_avg:43.28ms
step:946/1775 train_time:40962ms step_avg:43.30ms
step:947/1775 train_time:41021ms step_avg:43.32ms
step:948/1775 train_time:41082ms step_avg:43.34ms
step:949/1775 train_time:41141ms step_avg:43.35ms
step:950/1775 train_time:41201ms step_avg:43.37ms
step:951/1775 train_time:41261ms step_avg:43.39ms
step:952/1775 train_time:41322ms step_avg:43.41ms
step:953/1775 train_time:41381ms step_avg:43.42ms
step:954/1775 train_time:41442ms step_avg:43.44ms
step:955/1775 train_time:41500ms step_avg:43.46ms
step:956/1775 train_time:41562ms step_avg:43.47ms
step:957/1775 train_time:41620ms step_avg:43.49ms
step:958/1775 train_time:41681ms step_avg:43.51ms
step:959/1775 train_time:41740ms step_avg:43.52ms
step:960/1775 train_time:41801ms step_avg:43.54ms
step:961/1775 train_time:41860ms step_avg:43.56ms
step:962/1775 train_time:41921ms step_avg:43.58ms
step:963/1775 train_time:41980ms step_avg:43.59ms
step:964/1775 train_time:42041ms step_avg:43.61ms
step:965/1775 train_time:42099ms step_avg:43.63ms
step:966/1775 train_time:42161ms step_avg:43.64ms
step:967/1775 train_time:42219ms step_avg:43.66ms
step:968/1775 train_time:42280ms step_avg:43.68ms
step:969/1775 train_time:42339ms step_avg:43.69ms
step:970/1775 train_time:42400ms step_avg:43.71ms
step:971/1775 train_time:42459ms step_avg:43.73ms
step:972/1775 train_time:42520ms step_avg:43.75ms
step:973/1775 train_time:42579ms step_avg:43.76ms
step:974/1775 train_time:42640ms step_avg:43.78ms
step:975/1775 train_time:42698ms step_avg:43.79ms
step:976/1775 train_time:42759ms step_avg:43.81ms
step:977/1775 train_time:42818ms step_avg:43.83ms
step:978/1775 train_time:42879ms step_avg:43.84ms
step:979/1775 train_time:42937ms step_avg:43.86ms
step:980/1775 train_time:42999ms step_avg:43.88ms
step:981/1775 train_time:43058ms step_avg:43.89ms
step:982/1775 train_time:43119ms step_avg:43.91ms
step:983/1775 train_time:43177ms step_avg:43.92ms
step:984/1775 train_time:43239ms step_avg:43.94ms
step:985/1775 train_time:43297ms step_avg:43.96ms
step:986/1775 train_time:43359ms step_avg:43.98ms
step:987/1775 train_time:43418ms step_avg:43.99ms
step:988/1775 train_time:43480ms step_avg:44.01ms
step:989/1775 train_time:43539ms step_avg:44.02ms
step:990/1775 train_time:43601ms step_avg:44.04ms
step:991/1775 train_time:43659ms step_avg:44.06ms
step:992/1775 train_time:43721ms step_avg:44.07ms
step:993/1775 train_time:43780ms step_avg:44.09ms
step:994/1775 train_time:43842ms step_avg:44.11ms
step:995/1775 train_time:43900ms step_avg:44.12ms
step:996/1775 train_time:43961ms step_avg:44.14ms
step:997/1775 train_time:44020ms step_avg:44.15ms
step:998/1775 train_time:44081ms step_avg:44.17ms
step:999/1775 train_time:44140ms step_avg:44.18ms
step:1000/1775 train_time:44200ms step_avg:44.20ms
step:1000/1775 val_loss:3.7342 train_time:44271ms step_avg:44.27ms
step:1001/1775 train_time:44290ms step_avg:44.25ms
step:1002/1775 train_time:44322ms step_avg:44.23ms
step:1003/1775 train_time:44382ms step_avg:44.25ms
step:1004/1775 train_time:44444ms step_avg:44.27ms
step:1005/1775 train_time:44503ms step_avg:44.28ms
step:1006/1775 train_time:44563ms step_avg:44.30ms
step:1007/1775 train_time:44622ms step_avg:44.31ms
step:1008/1775 train_time:44683ms step_avg:44.33ms
step:1009/1775 train_time:44741ms step_avg:44.34ms
step:1010/1775 train_time:44801ms step_avg:44.36ms
step:1011/1775 train_time:44860ms step_avg:44.37ms
step:1012/1775 train_time:44920ms step_avg:44.39ms
step:1013/1775 train_time:44978ms step_avg:44.40ms
step:1014/1775 train_time:45039ms step_avg:44.42ms
step:1015/1775 train_time:45097ms step_avg:44.43ms
step:1016/1775 train_time:45158ms step_avg:44.45ms
step:1017/1775 train_time:45216ms step_avg:44.46ms
step:1018/1775 train_time:45278ms step_avg:44.48ms
step:1019/1775 train_time:45339ms step_avg:44.49ms
step:1020/1775 train_time:45400ms step_avg:44.51ms
step:1021/1775 train_time:45459ms step_avg:44.52ms
step:1022/1775 train_time:45521ms step_avg:44.54ms
step:1023/1775 train_time:45580ms step_avg:44.56ms
step:1024/1775 train_time:45640ms step_avg:44.57ms
step:1025/1775 train_time:45698ms step_avg:44.58ms
step:1026/1775 train_time:45759ms step_avg:44.60ms
step:1027/1775 train_time:45817ms step_avg:44.61ms
step:1028/1775 train_time:45878ms step_avg:44.63ms
step:1029/1775 train_time:45936ms step_avg:44.64ms
step:1030/1775 train_time:45997ms step_avg:44.66ms
step:1031/1775 train_time:46055ms step_avg:44.67ms
step:1032/1775 train_time:46116ms step_avg:44.69ms
step:1033/1775 train_time:46174ms step_avg:44.70ms
step:1034/1775 train_time:46236ms step_avg:44.72ms
step:1035/1775 train_time:46295ms step_avg:44.73ms
step:1036/1775 train_time:46357ms step_avg:44.75ms
step:1037/1775 train_time:46417ms step_avg:44.76ms
step:1038/1775 train_time:46479ms step_avg:44.78ms
step:1039/1775 train_time:46537ms step_avg:44.79ms
step:1040/1775 train_time:46598ms step_avg:44.81ms
step:1041/1775 train_time:46656ms step_avg:44.82ms
step:1042/1775 train_time:46717ms step_avg:44.83ms
step:1043/1775 train_time:46776ms step_avg:44.85ms
step:1044/1775 train_time:46837ms step_avg:44.86ms
step:1045/1775 train_time:46895ms step_avg:44.88ms
step:1046/1775 train_time:46955ms step_avg:44.89ms
step:1047/1775 train_time:47014ms step_avg:44.90ms
step:1048/1775 train_time:47075ms step_avg:44.92ms
step:1049/1775 train_time:47133ms step_avg:44.93ms
step:1050/1775 train_time:47194ms step_avg:44.95ms
step:1051/1775 train_time:47253ms step_avg:44.96ms
step:1052/1775 train_time:47315ms step_avg:44.98ms
step:1053/1775 train_time:47374ms step_avg:44.99ms
step:1054/1775 train_time:47436ms step_avg:45.01ms
step:1055/1775 train_time:47496ms step_avg:45.02ms
step:1056/1775 train_time:47557ms step_avg:45.03ms
step:1057/1775 train_time:47615ms step_avg:45.05ms
step:1058/1775 train_time:47677ms step_avg:45.06ms
step:1059/1775 train_time:47735ms step_avg:45.08ms
step:1060/1775 train_time:47796ms step_avg:45.09ms
step:1061/1775 train_time:47854ms step_avg:45.10ms
step:1062/1775 train_time:47914ms step_avg:45.12ms
step:1063/1775 train_time:47973ms step_avg:45.13ms
step:1064/1775 train_time:48034ms step_avg:45.14ms
step:1065/1775 train_time:48092ms step_avg:45.16ms
step:1066/1775 train_time:48153ms step_avg:45.17ms
step:1067/1775 train_time:48211ms step_avg:45.18ms
step:1068/1775 train_time:48273ms step_avg:45.20ms
step:1069/1775 train_time:48333ms step_avg:45.21ms
step:1070/1775 train_time:48394ms step_avg:45.23ms
step:1071/1775 train_time:48453ms step_avg:45.24ms
step:1072/1775 train_time:48515ms step_avg:45.26ms
step:1073/1775 train_time:48574ms step_avg:45.27ms
step:1074/1775 train_time:48635ms step_avg:45.28ms
step:1075/1775 train_time:48694ms step_avg:45.30ms
step:1076/1775 train_time:48755ms step_avg:45.31ms
step:1077/1775 train_time:48813ms step_avg:45.32ms
step:1078/1775 train_time:48874ms step_avg:45.34ms
step:1079/1775 train_time:48933ms step_avg:45.35ms
step:1080/1775 train_time:48994ms step_avg:45.37ms
step:1081/1775 train_time:49052ms step_avg:45.38ms
step:1082/1775 train_time:49113ms step_avg:45.39ms
step:1083/1775 train_time:49171ms step_avg:45.40ms
step:1084/1775 train_time:49232ms step_avg:45.42ms
step:1085/1775 train_time:49291ms step_avg:45.43ms
step:1086/1775 train_time:49352ms step_avg:45.44ms
step:1087/1775 train_time:49411ms step_avg:45.46ms
step:1088/1775 train_time:49471ms step_avg:45.47ms
step:1089/1775 train_time:49531ms step_avg:45.48ms
step:1090/1775 train_time:49593ms step_avg:45.50ms
step:1091/1775 train_time:49652ms step_avg:45.51ms
step:1092/1775 train_time:49713ms step_avg:45.52ms
step:1093/1775 train_time:49771ms step_avg:45.54ms
step:1094/1775 train_time:49833ms step_avg:45.55ms
step:1095/1775 train_time:49892ms step_avg:45.56ms
step:1096/1775 train_time:49953ms step_avg:45.58ms
step:1097/1775 train_time:50011ms step_avg:45.59ms
step:1098/1775 train_time:50073ms step_avg:45.60ms
step:1099/1775 train_time:50132ms step_avg:45.62ms
step:1100/1775 train_time:50193ms step_avg:45.63ms
step:1101/1775 train_time:50251ms step_avg:45.64ms
step:1102/1775 train_time:50312ms step_avg:45.65ms
step:1103/1775 train_time:50370ms step_avg:45.67ms
step:1104/1775 train_time:50432ms step_avg:45.68ms
step:1105/1775 train_time:50492ms step_avg:45.69ms
step:1106/1775 train_time:50553ms step_avg:45.71ms
step:1107/1775 train_time:50613ms step_avg:45.72ms
step:1108/1775 train_time:50673ms step_avg:45.73ms
step:1109/1775 train_time:50733ms step_avg:45.75ms
step:1110/1775 train_time:50794ms step_avg:45.76ms
step:1111/1775 train_time:50852ms step_avg:45.77ms
step:1112/1775 train_time:50913ms step_avg:45.79ms
step:1113/1775 train_time:50971ms step_avg:45.80ms
step:1114/1775 train_time:51032ms step_avg:45.81ms
step:1115/1775 train_time:51091ms step_avg:45.82ms
step:1116/1775 train_time:51153ms step_avg:45.84ms
step:1117/1775 train_time:51212ms step_avg:45.85ms
step:1118/1775 train_time:51273ms step_avg:45.86ms
step:1119/1775 train_time:51332ms step_avg:45.87ms
step:1120/1775 train_time:51393ms step_avg:45.89ms
step:1121/1775 train_time:51452ms step_avg:45.90ms
step:1122/1775 train_time:51514ms step_avg:45.91ms
step:1123/1775 train_time:51573ms step_avg:45.92ms
step:1124/1775 train_time:51635ms step_avg:45.94ms
step:1125/1775 train_time:51694ms step_avg:45.95ms
step:1126/1775 train_time:51755ms step_avg:45.96ms
step:1127/1775 train_time:51813ms step_avg:45.97ms
step:1128/1775 train_time:51874ms step_avg:45.99ms
step:1129/1775 train_time:51933ms step_avg:46.00ms
step:1130/1775 train_time:51994ms step_avg:46.01ms
step:1131/1775 train_time:52054ms step_avg:46.02ms
step:1132/1775 train_time:52115ms step_avg:46.04ms
step:1133/1775 train_time:52173ms step_avg:46.05ms
step:1134/1775 train_time:52235ms step_avg:46.06ms
step:1135/1775 train_time:52294ms step_avg:46.07ms
step:1136/1775 train_time:52355ms step_avg:46.09ms
step:1137/1775 train_time:52413ms step_avg:46.10ms
step:1138/1775 train_time:52474ms step_avg:46.11ms
step:1139/1775 train_time:52533ms step_avg:46.12ms
step:1140/1775 train_time:52594ms step_avg:46.14ms
step:1141/1775 train_time:52653ms step_avg:46.15ms
step:1142/1775 train_time:52714ms step_avg:46.16ms
step:1143/1775 train_time:52773ms step_avg:46.17ms
step:1144/1775 train_time:52835ms step_avg:46.18ms
step:1145/1775 train_time:52894ms step_avg:46.20ms
step:1146/1775 train_time:52955ms step_avg:46.21ms
step:1147/1775 train_time:53013ms step_avg:46.22ms
step:1148/1775 train_time:53074ms step_avg:46.23ms
step:1149/1775 train_time:53133ms step_avg:46.24ms
step:1150/1775 train_time:53195ms step_avg:46.26ms
step:1151/1775 train_time:53254ms step_avg:46.27ms
step:1152/1775 train_time:53315ms step_avg:46.28ms
step:1153/1775 train_time:53374ms step_avg:46.29ms
step:1154/1775 train_time:53436ms step_avg:46.30ms
step:1155/1775 train_time:53494ms step_avg:46.32ms
step:1156/1775 train_time:53555ms step_avg:46.33ms
step:1157/1775 train_time:53615ms step_avg:46.34ms
step:1158/1775 train_time:53678ms step_avg:46.35ms
step:1159/1775 train_time:53762ms step_avg:46.39ms
step:1160/1775 train_time:53851ms step_avg:46.42ms
step:1161/1775 train_time:53934ms step_avg:46.45ms
step:1162/1775 train_time:54021ms step_avg:46.49ms
step:1163/1775 train_time:54106ms step_avg:46.52ms
step:1164/1775 train_time:54193ms step_avg:46.56ms
step:1165/1775 train_time:54276ms step_avg:46.59ms
step:1166/1775 train_time:54364ms step_avg:46.62ms
step:1167/1775 train_time:54450ms step_avg:46.66ms
step:1168/1775 train_time:54536ms step_avg:46.69ms
step:1169/1775 train_time:54621ms step_avg:46.72ms
step:1170/1775 train_time:54709ms step_avg:46.76ms
step:1171/1775 train_time:54793ms step_avg:46.79ms
step:1172/1775 train_time:54880ms step_avg:46.83ms
step:1173/1775 train_time:54965ms step_avg:46.86ms
step:1174/1775 train_time:55052ms step_avg:46.89ms
step:1175/1775 train_time:55136ms step_avg:46.92ms
step:1176/1775 train_time:55225ms step_avg:46.96ms
step:1177/1775 train_time:55308ms step_avg:46.99ms
step:1178/1775 train_time:55394ms step_avg:47.02ms
step:1179/1775 train_time:55479ms step_avg:47.06ms
step:1180/1775 train_time:55566ms step_avg:47.09ms
step:1181/1775 train_time:55651ms step_avg:47.12ms
step:1182/1775 train_time:55737ms step_avg:47.15ms
step:1183/1775 train_time:55823ms step_avg:47.19ms
step:1184/1775 train_time:55911ms step_avg:47.22ms
step:1185/1775 train_time:55994ms step_avg:47.25ms
step:1186/1775 train_time:56081ms step_avg:47.29ms
step:1187/1775 train_time:56166ms step_avg:47.32ms
step:1188/1775 train_time:56253ms step_avg:47.35ms
step:1189/1775 train_time:56337ms step_avg:47.38ms
step:1190/1775 train_time:56425ms step_avg:47.42ms
step:1191/1775 train_time:56510ms step_avg:47.45ms
step:1192/1775 train_time:56596ms step_avg:47.48ms
step:1193/1775 train_time:56682ms step_avg:47.51ms
step:1194/1775 train_time:56770ms step_avg:47.55ms
step:1195/1775 train_time:56854ms step_avg:47.58ms
step:1196/1775 train_time:56941ms step_avg:47.61ms
step:1197/1775 train_time:57025ms step_avg:47.64ms
step:1198/1775 train_time:57113ms step_avg:47.67ms
step:1199/1775 train_time:57197ms step_avg:47.70ms
step:1200/1775 train_time:57284ms step_avg:47.74ms
step:1201/1775 train_time:57369ms step_avg:47.77ms
step:1202/1775 train_time:57455ms step_avg:47.80ms
step:1203/1775 train_time:57542ms step_avg:47.83ms
step:1204/1775 train_time:57629ms step_avg:47.86ms
step:1205/1775 train_time:57712ms step_avg:47.89ms
step:1206/1775 train_time:57800ms step_avg:47.93ms
step:1207/1775 train_time:57885ms step_avg:47.96ms
step:1208/1775 train_time:57973ms step_avg:47.99ms
step:1209/1775 train_time:58057ms step_avg:48.02ms
step:1210/1775 train_time:58144ms step_avg:48.05ms
step:1211/1775 train_time:58229ms step_avg:48.08ms
step:1212/1775 train_time:58315ms step_avg:48.11ms
step:1213/1775 train_time:58398ms step_avg:48.14ms
step:1214/1775 train_time:58486ms step_avg:48.18ms
step:1215/1775 train_time:58571ms step_avg:48.21ms
step:1216/1775 train_time:58656ms step_avg:48.24ms
step:1217/1775 train_time:58741ms step_avg:48.27ms
step:1218/1775 train_time:58829ms step_avg:48.30ms
step:1219/1775 train_time:58914ms step_avg:48.33ms
step:1220/1775 train_time:59001ms step_avg:48.36ms
step:1221/1775 train_time:59084ms step_avg:48.39ms
step:1222/1775 train_time:59172ms step_avg:48.42ms
step:1223/1775 train_time:59257ms step_avg:48.45ms
step:1224/1775 train_time:59345ms step_avg:48.48ms
step:1225/1775 train_time:59430ms step_avg:48.51ms
step:1226/1775 train_time:59516ms step_avg:48.54ms
step:1227/1775 train_time:59601ms step_avg:48.57ms
step:1228/1775 train_time:59688ms step_avg:48.61ms
step:1229/1775 train_time:59772ms step_avg:48.63ms
step:1230/1775 train_time:59859ms step_avg:48.67ms
step:1231/1775 train_time:59945ms step_avg:48.70ms
step:1232/1775 train_time:60033ms step_avg:48.73ms
step:1233/1775 train_time:60118ms step_avg:48.76ms
step:1234/1775 train_time:60205ms step_avg:48.79ms
step:1235/1775 train_time:60289ms step_avg:48.82ms
step:1236/1775 train_time:60376ms step_avg:48.85ms
step:1237/1775 train_time:60461ms step_avg:48.88ms
step:1238/1775 train_time:60549ms step_avg:48.91ms
step:1239/1775 train_time:60633ms step_avg:48.94ms
step:1240/1775 train_time:60721ms step_avg:48.97ms
step:1241/1775 train_time:60806ms step_avg:49.00ms
step:1242/1775 train_time:60892ms step_avg:49.03ms
step:1243/1775 train_time:60977ms step_avg:49.06ms
step:1244/1775 train_time:61066ms step_avg:49.09ms
step:1245/1775 train_time:61150ms step_avg:49.12ms
step:1246/1775 train_time:61236ms step_avg:49.15ms
step:1247/1775 train_time:61321ms step_avg:49.18ms
step:1248/1775 train_time:61410ms step_avg:49.21ms
step:1249/1775 train_time:61494ms step_avg:49.23ms
step:1250/1775 train_time:61580ms step_avg:49.26ms
step:1250/1775 val_loss:3.5041 train_time:61681ms step_avg:49.34ms
step:1251/1775 train_time:61701ms step_avg:49.32ms
step:1252/1775 train_time:61754ms step_avg:49.32ms
step:1253/1775 train_time:61842ms step_avg:49.36ms
step:1254/1775 train_time:61929ms step_avg:49.39ms
step:1255/1775 train_time:62012ms step_avg:49.41ms
step:1256/1775 train_time:62097ms step_avg:49.44ms
step:1257/1775 train_time:62182ms step_avg:49.47ms
step:1258/1775 train_time:62268ms step_avg:49.50ms
step:1259/1775 train_time:62353ms step_avg:49.53ms
step:1260/1775 train_time:62440ms step_avg:49.56ms
step:1261/1775 train_time:62525ms step_avg:49.58ms
step:1262/1775 train_time:62612ms step_avg:49.61ms
step:1263/1775 train_time:62701ms step_avg:49.64ms
step:1264/1775 train_time:62788ms step_avg:49.67ms
step:1265/1775 train_time:62873ms step_avg:49.70ms
step:1266/1775 train_time:62962ms step_avg:49.73ms
step:1267/1775 train_time:63046ms step_avg:49.76ms
step:1268/1775 train_time:63132ms step_avg:49.79ms
step:1269/1775 train_time:63216ms step_avg:49.82ms
step:1270/1775 train_time:63303ms step_avg:49.84ms
step:1271/1775 train_time:63387ms step_avg:49.87ms
step:1272/1775 train_time:63474ms step_avg:49.90ms
step:1273/1775 train_time:63560ms step_avg:49.93ms
step:1274/1775 train_time:63648ms step_avg:49.96ms
step:1275/1775 train_time:63733ms step_avg:49.99ms
step:1276/1775 train_time:63821ms step_avg:50.02ms
step:1277/1775 train_time:63906ms step_avg:50.04ms
step:1278/1775 train_time:63992ms step_avg:50.07ms
step:1279/1775 train_time:64076ms step_avg:50.10ms
step:1280/1775 train_time:64163ms step_avg:50.13ms
step:1281/1775 train_time:64247ms step_avg:50.15ms
step:1282/1775 train_time:64334ms step_avg:50.18ms
step:1283/1775 train_time:64418ms step_avg:50.21ms
step:1284/1775 train_time:64505ms step_avg:50.24ms
step:1285/1775 train_time:64589ms step_avg:50.26ms
step:1286/1775 train_time:64677ms step_avg:50.29ms
step:1287/1775 train_time:64764ms step_avg:50.32ms
step:1288/1775 train_time:64851ms step_avg:50.35ms
step:1289/1775 train_time:64936ms step_avg:50.38ms
step:1290/1775 train_time:65023ms step_avg:50.41ms
step:1291/1775 train_time:65107ms step_avg:50.43ms
step:1292/1775 train_time:65193ms step_avg:50.46ms
step:1293/1775 train_time:65278ms step_avg:50.49ms
step:1294/1775 train_time:65366ms step_avg:50.51ms
step:1295/1775 train_time:65450ms step_avg:50.54ms
step:1296/1775 train_time:65537ms step_avg:50.57ms
step:1297/1775 train_time:65623ms step_avg:50.60ms
step:1298/1775 train_time:65710ms step_avg:50.62ms
step:1299/1775 train_time:65796ms step_avg:50.65ms
step:1300/1775 train_time:65885ms step_avg:50.68ms
step:1301/1775 train_time:65970ms step_avg:50.71ms
step:1302/1775 train_time:66056ms step_avg:50.73ms
step:1303/1775 train_time:66141ms step_avg:50.76ms
step:1304/1775 train_time:66227ms step_avg:50.79ms
step:1305/1775 train_time:66310ms step_avg:50.81ms
step:1306/1775 train_time:66398ms step_avg:50.84ms
step:1307/1775 train_time:66483ms step_avg:50.87ms
step:1308/1775 train_time:66570ms step_avg:50.89ms
step:1309/1775 train_time:66655ms step_avg:50.92ms
step:1310/1775 train_time:66744ms step_avg:50.95ms
step:1311/1775 train_time:66828ms step_avg:50.97ms
step:1312/1775 train_time:66916ms step_avg:51.00ms
step:1313/1775 train_time:67001ms step_avg:51.03ms
step:1314/1775 train_time:67088ms step_avg:51.06ms
step:1315/1775 train_time:67171ms step_avg:51.08ms
step:1316/1775 train_time:67260ms step_avg:51.11ms
step:1317/1775 train_time:67344ms step_avg:51.13ms
step:1318/1775 train_time:67431ms step_avg:51.16ms
step:1319/1775 train_time:67516ms step_avg:51.19ms
step:1320/1775 train_time:67603ms step_avg:51.21ms
step:1321/1775 train_time:67688ms step_avg:51.24ms
step:1322/1775 train_time:67775ms step_avg:51.27ms
step:1323/1775 train_time:67860ms step_avg:51.29ms
step:1324/1775 train_time:67948ms step_avg:51.32ms
step:1325/1775 train_time:68032ms step_avg:51.35ms
step:1326/1775 train_time:68120ms step_avg:51.37ms
step:1327/1775 train_time:68205ms step_avg:51.40ms
step:1328/1775 train_time:68291ms step_avg:51.42ms
step:1329/1775 train_time:68377ms step_avg:51.45ms
step:1330/1775 train_time:68465ms step_avg:51.48ms
step:1331/1775 train_time:68548ms step_avg:51.50ms
step:1332/1775 train_time:68637ms step_avg:51.53ms
step:1333/1775 train_time:68721ms step_avg:51.55ms
step:1334/1775 train_time:68808ms step_avg:51.58ms
step:1335/1775 train_time:68892ms step_avg:51.60ms
step:1336/1775 train_time:68979ms step_avg:51.63ms
step:1337/1775 train_time:69065ms step_avg:51.66ms
step:1338/1775 train_time:69151ms step_avg:51.68ms
step:1339/1775 train_time:69236ms step_avg:51.71ms
step:1340/1775 train_time:69325ms step_avg:51.74ms
step:1341/1775 train_time:69409ms step_avg:51.76ms
step:1342/1775 train_time:69496ms step_avg:51.79ms
step:1343/1775 train_time:69581ms step_avg:51.81ms
step:1344/1775 train_time:69668ms step_avg:51.84ms
step:1345/1775 train_time:69753ms step_avg:51.86ms
step:1346/1775 train_time:69841ms step_avg:51.89ms
step:1347/1775 train_time:69927ms step_avg:51.91ms
step:1348/1775 train_time:70013ms step_avg:51.94ms
step:1349/1775 train_time:70097ms step_avg:51.96ms
step:1350/1775 train_time:70184ms step_avg:51.99ms
step:1351/1775 train_time:70268ms step_avg:52.01ms
step:1352/1775 train_time:70356ms step_avg:52.04ms
step:1353/1775 train_time:70440ms step_avg:52.06ms
step:1354/1775 train_time:70527ms step_avg:52.09ms
step:1355/1775 train_time:70611ms step_avg:52.11ms
step:1356/1775 train_time:70700ms step_avg:52.14ms
step:1357/1775 train_time:70784ms step_avg:52.16ms
step:1358/1775 train_time:70870ms step_avg:52.19ms
step:1359/1775 train_time:70955ms step_avg:52.21ms
step:1360/1775 train_time:71042ms step_avg:52.24ms
step:1361/1775 train_time:71126ms step_avg:52.26ms
step:1362/1775 train_time:71212ms step_avg:52.28ms
step:1363/1775 train_time:71298ms step_avg:52.31ms
step:1364/1775 train_time:71385ms step_avg:52.33ms
step:1365/1775 train_time:71469ms step_avg:52.36ms
step:1366/1775 train_time:71555ms step_avg:52.38ms
step:1367/1775 train_time:71640ms step_avg:52.41ms
step:1368/1775 train_time:71728ms step_avg:52.43ms
step:1369/1775 train_time:71812ms step_avg:52.46ms
step:1370/1775 train_time:71899ms step_avg:52.48ms
step:1371/1775 train_time:71984ms step_avg:52.50ms
step:1372/1775 train_time:72070ms step_avg:52.53ms
step:1373/1775 train_time:72156ms step_avg:52.55ms
step:1374/1775 train_time:72244ms step_avg:52.58ms
step:1375/1775 train_time:72328ms step_avg:52.60ms
step:1376/1775 train_time:72415ms step_avg:52.63ms
step:1377/1775 train_time:72499ms step_avg:52.65ms
step:1378/1775 train_time:72586ms step_avg:52.68ms
step:1379/1775 train_time:72671ms step_avg:52.70ms
step:1380/1775 train_time:72759ms step_avg:52.72ms
step:1381/1775 train_time:72845ms step_avg:52.75ms
step:1382/1775 train_time:72931ms step_avg:52.77ms
step:1383/1775 train_time:73016ms step_avg:52.80ms
step:1384/1775 train_time:73103ms step_avg:52.82ms
step:1385/1775 train_time:73187ms step_avg:52.84ms
step:1386/1775 train_time:73274ms step_avg:52.87ms
step:1387/1775 train_time:73359ms step_avg:52.89ms
step:1388/1775 train_time:73448ms step_avg:52.92ms
step:1389/1775 train_time:73532ms step_avg:52.94ms
step:1390/1775 train_time:73619ms step_avg:52.96ms
step:1391/1775 train_time:73704ms step_avg:52.99ms
step:1392/1775 train_time:73789ms step_avg:53.01ms
step:1393/1775 train_time:73874ms step_avg:53.03ms
step:1394/1775 train_time:73963ms step_avg:53.06ms
step:1395/1775 train_time:74047ms step_avg:53.08ms
step:1396/1775 train_time:74133ms step_avg:53.10ms
step:1397/1775 train_time:74217ms step_avg:53.13ms
step:1398/1775 train_time:74304ms step_avg:53.15ms
step:1399/1775 train_time:74389ms step_avg:53.17ms
step:1400/1775 train_time:74476ms step_avg:53.20ms
step:1401/1775 train_time:74561ms step_avg:53.22ms
step:1402/1775 train_time:74648ms step_avg:53.24ms
step:1403/1775 train_time:74732ms step_avg:53.27ms
step:1404/1775 train_time:74819ms step_avg:53.29ms
step:1405/1775 train_time:74904ms step_avg:53.31ms
step:1406/1775 train_time:74990ms step_avg:53.34ms
step:1407/1775 train_time:75075ms step_avg:53.36ms
step:1408/1775 train_time:75164ms step_avg:53.38ms
step:1409/1775 train_time:75248ms step_avg:53.41ms
step:1410/1775 train_time:75335ms step_avg:53.43ms
step:1411/1775 train_time:75419ms step_avg:53.45ms
step:1412/1775 train_time:75506ms step_avg:53.47ms
step:1413/1775 train_time:75590ms step_avg:53.50ms
step:1414/1775 train_time:75678ms step_avg:53.52ms
step:1415/1775 train_time:75764ms step_avg:53.54ms
step:1416/1775 train_time:75849ms step_avg:53.57ms
step:1417/1775 train_time:75933ms step_avg:53.59ms
step:1418/1775 train_time:76021ms step_avg:53.61ms
step:1419/1775 train_time:76106ms step_avg:53.63ms
step:1420/1775 train_time:76192ms step_avg:53.66ms
step:1421/1775 train_time:76276ms step_avg:53.68ms
step:1422/1775 train_time:76365ms step_avg:53.70ms
step:1423/1775 train_time:76448ms step_avg:53.72ms
step:1424/1775 train_time:76534ms step_avg:53.75ms
step:1425/1775 train_time:76620ms step_avg:53.77ms
step:1426/1775 train_time:76706ms step_avg:53.79ms
step:1427/1775 train_time:76791ms step_avg:53.81ms
step:1428/1775 train_time:76877ms step_avg:53.84ms
step:1429/1775 train_time:76962ms step_avg:53.86ms
step:1430/1775 train_time:77049ms step_avg:53.88ms
step:1431/1775 train_time:77132ms step_avg:53.90ms
step:1432/1775 train_time:77220ms step_avg:53.92ms
step:1433/1775 train_time:77306ms step_avg:53.95ms
step:1434/1775 train_time:77392ms step_avg:53.97ms
step:1435/1775 train_time:77477ms step_avg:53.99ms
step:1436/1775 train_time:77565ms step_avg:54.01ms
step:1437/1775 train_time:77649ms step_avg:54.04ms
step:1438/1775 train_time:77736ms step_avg:54.06ms
step:1439/1775 train_time:77822ms step_avg:54.08ms
step:1440/1775 train_time:77908ms step_avg:54.10ms
step:1441/1775 train_time:77993ms step_avg:54.12ms
step:1442/1775 train_time:78079ms step_avg:54.15ms
step:1443/1775 train_time:78165ms step_avg:54.17ms
step:1444/1775 train_time:78251ms step_avg:54.19ms
step:1445/1775 train_time:78336ms step_avg:54.21ms
step:1446/1775 train_time:78424ms step_avg:54.24ms
step:1447/1775 train_time:78508ms step_avg:54.26ms
step:1448/1775 train_time:78595ms step_avg:54.28ms
step:1449/1775 train_time:78680ms step_avg:54.30ms
step:1450/1775 train_time:78767ms step_avg:54.32ms
step:1451/1775 train_time:78851ms step_avg:54.34ms
step:1452/1775 train_time:78939ms step_avg:54.37ms
step:1453/1775 train_time:79024ms step_avg:54.39ms
step:1454/1775 train_time:79110ms step_avg:54.41ms
step:1455/1775 train_time:79197ms step_avg:54.43ms
step:1456/1775 train_time:79285ms step_avg:54.45ms
step:1457/1775 train_time:79368ms step_avg:54.47ms
step:1458/1775 train_time:79455ms step_avg:54.50ms
step:1459/1775 train_time:79540ms step_avg:54.52ms
step:1460/1775 train_time:79628ms step_avg:54.54ms
step:1461/1775 train_time:79713ms step_avg:54.56ms
step:1462/1775 train_time:79801ms step_avg:54.58ms
step:1463/1775 train_time:79886ms step_avg:54.60ms
step:1464/1775 train_time:79971ms step_avg:54.62ms
step:1465/1775 train_time:80056ms step_avg:54.65ms
step:1466/1775 train_time:80144ms step_avg:54.67ms
step:1467/1775 train_time:80228ms step_avg:54.69ms
step:1468/1775 train_time:80314ms step_avg:54.71ms
step:1469/1775 train_time:80399ms step_avg:54.73ms
step:1470/1775 train_time:80486ms step_avg:54.75ms
step:1471/1775 train_time:80570ms step_avg:54.77ms
step:1472/1775 train_time:80657ms step_avg:54.79ms
step:1473/1775 train_time:80743ms step_avg:54.82ms
step:1474/1775 train_time:80830ms step_avg:54.84ms
step:1475/1775 train_time:80913ms step_avg:54.86ms
step:1476/1775 train_time:81001ms step_avg:54.88ms
step:1477/1775 train_time:81086ms step_avg:54.90ms
step:1478/1775 train_time:81174ms step_avg:54.92ms
step:1479/1775 train_time:81256ms step_avg:54.94ms
step:1480/1775 train_time:81346ms step_avg:54.96ms
step:1481/1775 train_time:81430ms step_avg:54.98ms
step:1482/1775 train_time:81516ms step_avg:55.00ms
step:1483/1775 train_time:81600ms step_avg:55.02ms
step:1484/1775 train_time:81687ms step_avg:55.04ms
step:1485/1775 train_time:81772ms step_avg:55.07ms
step:1486/1775 train_time:81860ms step_avg:55.09ms
step:1487/1775 train_time:81944ms step_avg:55.11ms
step:1488/1775 train_time:82031ms step_avg:55.13ms
step:1489/1775 train_time:82115ms step_avg:55.15ms
step:1490/1775 train_time:82203ms step_avg:55.17ms
step:1491/1775 train_time:82287ms step_avg:55.19ms
step:1492/1775 train_time:82374ms step_avg:55.21ms
step:1493/1775 train_time:82459ms step_avg:55.23ms
step:1494/1775 train_time:82546ms step_avg:55.25ms
step:1495/1775 train_time:82630ms step_avg:55.27ms
step:1496/1775 train_time:82718ms step_avg:55.29ms
step:1497/1775 train_time:82804ms step_avg:55.31ms
step:1498/1775 train_time:82890ms step_avg:55.33ms
step:1499/1775 train_time:82975ms step_avg:55.35ms
step:1500/1775 train_time:83062ms step_avg:55.37ms
step:1500/1775 val_loss:3.3767 train_time:83160ms step_avg:55.44ms
step:1501/1775 train_time:83180ms step_avg:55.42ms
step:1502/1775 train_time:83235ms step_avg:55.42ms
step:1503/1775 train_time:83324ms step_avg:55.44ms
step:1504/1775 train_time:83410ms step_avg:55.46ms
step:1505/1775 train_time:83494ms step_avg:55.48ms
step:1506/1775 train_time:83579ms step_avg:55.50ms
step:1507/1775 train_time:83664ms step_avg:55.52ms
step:1508/1775 train_time:83750ms step_avg:55.54ms
step:1509/1775 train_time:83832ms step_avg:55.55ms
step:1510/1775 train_time:83919ms step_avg:55.58ms
step:1511/1775 train_time:84005ms step_avg:55.60ms
step:1512/1775 train_time:84092ms step_avg:55.62ms
step:1513/1775 train_time:84177ms step_avg:55.64ms
step:1514/1775 train_time:84267ms step_avg:55.66ms
step:1515/1775 train_time:84353ms step_avg:55.68ms
step:1516/1775 train_time:84440ms step_avg:55.70ms
step:1517/1775 train_time:84524ms step_avg:55.72ms
step:1518/1775 train_time:84610ms step_avg:55.74ms
step:1519/1775 train_time:84694ms step_avg:55.76ms
step:1520/1775 train_time:84780ms step_avg:55.78ms
step:1521/1775 train_time:84864ms step_avg:55.80ms
step:1522/1775 train_time:84953ms step_avg:55.82ms
step:1523/1775 train_time:85036ms step_avg:55.83ms
step:1524/1775 train_time:85124ms step_avg:55.86ms
step:1525/1775 train_time:85210ms step_avg:55.88ms
step:1526/1775 train_time:85297ms step_avg:55.90ms
step:1527/1775 train_time:85384ms step_avg:55.92ms
step:1528/1775 train_time:85471ms step_avg:55.94ms
step:1529/1775 train_time:85555ms step_avg:55.96ms
step:1530/1775 train_time:85641ms step_avg:55.97ms
step:1531/1775 train_time:85726ms step_avg:55.99ms
step:1532/1775 train_time:85812ms step_avg:56.01ms
step:1533/1775 train_time:85896ms step_avg:56.03ms
step:1534/1775 train_time:85984ms step_avg:56.05ms
step:1535/1775 train_time:86069ms step_avg:56.07ms
step:1536/1775 train_time:86157ms step_avg:56.09ms
step:1537/1775 train_time:86243ms step_avg:56.11ms
step:1538/1775 train_time:86330ms step_avg:56.13ms
step:1539/1775 train_time:86415ms step_avg:56.15ms
step:1540/1775 train_time:86502ms step_avg:56.17ms
step:1541/1775 train_time:86586ms step_avg:56.19ms
step:1542/1775 train_time:86674ms step_avg:56.21ms
step:1543/1775 train_time:86757ms step_avg:56.23ms
step:1544/1775 train_time:86844ms step_avg:56.25ms
step:1545/1775 train_time:86929ms step_avg:56.26ms
step:1546/1775 train_time:87015ms step_avg:56.28ms
step:1547/1775 train_time:87099ms step_avg:56.30ms
step:1548/1775 train_time:87188ms step_avg:56.32ms
step:1549/1775 train_time:87273ms step_avg:56.34ms
step:1550/1775 train_time:87360ms step_avg:56.36ms
step:1551/1775 train_time:87444ms step_avg:56.38ms
step:1552/1775 train_time:87532ms step_avg:56.40ms
step:1553/1775 train_time:87616ms step_avg:56.42ms
step:1554/1775 train_time:87703ms step_avg:56.44ms
step:1555/1775 train_time:87787ms step_avg:56.45ms
step:1556/1775 train_time:87875ms step_avg:56.47ms
step:1557/1775 train_time:87959ms step_avg:56.49ms
step:1558/1775 train_time:88046ms step_avg:56.51ms
step:1559/1775 train_time:88131ms step_avg:56.53ms
step:1560/1775 train_time:88218ms step_avg:56.55ms
step:1561/1775 train_time:88304ms step_avg:56.57ms
step:1562/1775 train_time:88392ms step_avg:56.59ms
step:1563/1775 train_time:88476ms step_avg:56.61ms
step:1564/1775 train_time:88562ms step_avg:56.63ms
step:1565/1775 train_time:88646ms step_avg:56.64ms
step:1566/1775 train_time:88734ms step_avg:56.66ms
step:1567/1775 train_time:88818ms step_avg:56.68ms
step:1568/1775 train_time:88904ms step_avg:56.70ms
step:1569/1775 train_time:88989ms step_avg:56.72ms
step:1570/1775 train_time:89076ms step_avg:56.74ms
step:1571/1775 train_time:89162ms step_avg:56.75ms
step:1572/1775 train_time:89250ms step_avg:56.77ms
step:1573/1775 train_time:89333ms step_avg:56.79ms
step:1574/1775 train_time:89421ms step_avg:56.81ms
step:1575/1775 train_time:89506ms step_avg:56.83ms
step:1576/1775 train_time:89592ms step_avg:56.85ms
step:1577/1775 train_time:89675ms step_avg:56.86ms
step:1578/1775 train_time:89764ms step_avg:56.88ms
step:1579/1775 train_time:89850ms step_avg:56.90ms
step:1580/1775 train_time:89936ms step_avg:56.92ms
step:1581/1775 train_time:90019ms step_avg:56.94ms
step:1582/1775 train_time:90107ms step_avg:56.96ms
step:1583/1775 train_time:90193ms step_avg:56.98ms
step:1584/1775 train_time:90279ms step_avg:56.99ms
step:1585/1775 train_time:90364ms step_avg:57.01ms
step:1586/1775 train_time:90452ms step_avg:57.03ms
step:1587/1775 train_time:90536ms step_avg:57.05ms
step:1588/1775 train_time:90624ms step_avg:57.07ms
step:1589/1775 train_time:90708ms step_avg:57.09ms
step:1590/1775 train_time:90795ms step_avg:57.10ms
step:1591/1775 train_time:90879ms step_avg:57.12ms
step:1592/1775 train_time:90966ms step_avg:57.14ms
step:1593/1775 train_time:91051ms step_avg:57.16ms
step:1594/1775 train_time:91137ms step_avg:57.18ms
step:1595/1775 train_time:91221ms step_avg:57.19ms
step:1596/1775 train_time:91309ms step_avg:57.21ms
step:1597/1775 train_time:91393ms step_avg:57.23ms
step:1598/1775 train_time:91480ms step_avg:57.25ms
step:1599/1775 train_time:91565ms step_avg:57.26ms
step:1600/1775 train_time:91653ms step_avg:57.28ms
step:1601/1775 train_time:91736ms step_avg:57.30ms
step:1602/1775 train_time:91824ms step_avg:57.32ms
step:1603/1775 train_time:91909ms step_avg:57.34ms
step:1604/1775 train_time:91995ms step_avg:57.35ms
step:1605/1775 train_time:92080ms step_avg:57.37ms
step:1606/1775 train_time:92167ms step_avg:57.39ms
step:1607/1775 train_time:92252ms step_avg:57.41ms
step:1608/1775 train_time:92339ms step_avg:57.42ms
step:1609/1775 train_time:92422ms step_avg:57.44ms
step:1610/1775 train_time:92510ms step_avg:57.46ms
step:1611/1775 train_time:92594ms step_avg:57.48ms
step:1612/1775 train_time:92682ms step_avg:57.50ms
step:1613/1775 train_time:92767ms step_avg:57.51ms
step:1614/1775 train_time:92853ms step_avg:57.53ms
step:1615/1775 train_time:92938ms step_avg:57.55ms
step:1616/1775 train_time:93025ms step_avg:57.56ms
step:1617/1775 train_time:93110ms step_avg:57.58ms
step:1618/1775 train_time:93196ms step_avg:57.60ms
step:1619/1775 train_time:93280ms step_avg:57.62ms
step:1620/1775 train_time:93368ms step_avg:57.63ms
step:1621/1775 train_time:93453ms step_avg:57.65ms
step:1622/1775 train_time:93539ms step_avg:57.67ms
step:1623/1775 train_time:93624ms step_avg:57.69ms
step:1624/1775 train_time:93711ms step_avg:57.70ms
step:1625/1775 train_time:93795ms step_avg:57.72ms
step:1626/1775 train_time:93882ms step_avg:57.74ms
step:1627/1775 train_time:93965ms step_avg:57.75ms
step:1628/1775 train_time:94054ms step_avg:57.77ms
step:1629/1775 train_time:94137ms step_avg:57.79ms
step:1630/1775 train_time:94225ms step_avg:57.81ms
step:1631/1775 train_time:94309ms step_avg:57.82ms
step:1632/1775 train_time:94395ms step_avg:57.84ms
step:1633/1775 train_time:94481ms step_avg:57.86ms
step:1634/1775 train_time:94569ms step_avg:57.88ms
step:1635/1775 train_time:94653ms step_avg:57.89ms
step:1636/1775 train_time:94738ms step_avg:57.91ms
step:1637/1775 train_time:94824ms step_avg:57.93ms
step:1638/1775 train_time:94911ms step_avg:57.94ms
step:1639/1775 train_time:94995ms step_avg:57.96ms
step:1640/1775 train_time:95082ms step_avg:57.98ms
step:1641/1775 train_time:95166ms step_avg:57.99ms
step:1642/1775 train_time:95254ms step_avg:58.01ms
step:1643/1775 train_time:95337ms step_avg:58.03ms
step:1644/1775 train_time:95425ms step_avg:58.04ms
step:1645/1775 train_time:95510ms step_avg:58.06ms
step:1646/1775 train_time:95596ms step_avg:58.08ms
step:1647/1775 train_time:95680ms step_avg:58.09ms
step:1648/1775 train_time:95769ms step_avg:58.11ms
step:1649/1775 train_time:95853ms step_avg:58.13ms
step:1650/1775 train_time:95939ms step_avg:58.14ms
step:1651/1775 train_time:96025ms step_avg:58.16ms
step:1652/1775 train_time:96112ms step_avg:58.18ms
step:1653/1775 train_time:96196ms step_avg:58.19ms
step:1654/1775 train_time:96283ms step_avg:58.21ms
step:1655/1775 train_time:96368ms step_avg:58.23ms
step:1656/1775 train_time:96455ms step_avg:58.25ms
step:1657/1775 train_time:96539ms step_avg:58.26ms
step:1658/1775 train_time:96626ms step_avg:58.28ms
step:1659/1775 train_time:96711ms step_avg:58.29ms
step:1660/1775 train_time:96797ms step_avg:58.31ms
step:1661/1775 train_time:96881ms step_avg:58.33ms
step:1662/1775 train_time:96969ms step_avg:58.34ms
step:1663/1775 train_time:97052ms step_avg:58.36ms
step:1664/1775 train_time:97139ms step_avg:58.38ms
step:1665/1775 train_time:97224ms step_avg:58.39ms
step:1666/1775 train_time:97312ms step_avg:58.41ms
step:1667/1775 train_time:97396ms step_avg:58.43ms
step:1668/1775 train_time:97484ms step_avg:58.44ms
step:1669/1775 train_time:97569ms step_avg:58.46ms
step:1670/1775 train_time:97656ms step_avg:58.48ms
step:1671/1775 train_time:97741ms step_avg:58.49ms
step:1672/1775 train_time:97829ms step_avg:58.51ms
step:1673/1775 train_time:97913ms step_avg:58.53ms
step:1674/1775 train_time:98001ms step_avg:58.54ms
step:1675/1775 train_time:98085ms step_avg:58.56ms
step:1676/1775 train_time:98172ms step_avg:58.58ms
step:1677/1775 train_time:98256ms step_avg:58.59ms
step:1678/1775 train_time:98345ms step_avg:58.61ms
step:1679/1775 train_time:98429ms step_avg:58.62ms
step:1680/1775 train_time:98515ms step_avg:58.64ms
step:1681/1775 train_time:98599ms step_avg:58.65ms
step:1682/1775 train_time:98687ms step_avg:58.67ms
step:1683/1775 train_time:98771ms step_avg:58.69ms
step:1684/1775 train_time:98859ms step_avg:58.70ms
step:1685/1775 train_time:98944ms step_avg:58.72ms
step:1686/1775 train_time:99031ms step_avg:58.74ms
step:1687/1775 train_time:99115ms step_avg:58.75ms
step:1688/1775 train_time:99202ms step_avg:58.77ms
step:1689/1775 train_time:99287ms step_avg:58.78ms
step:1690/1775 train_time:99374ms step_avg:58.80ms
step:1691/1775 train_time:99458ms step_avg:58.82ms
step:1692/1775 train_time:99545ms step_avg:58.83ms
step:1693/1775 train_time:99631ms step_avg:58.85ms
step:1694/1775 train_time:99717ms step_avg:58.86ms
step:1695/1775 train_time:99801ms step_avg:58.88ms
step:1696/1775 train_time:99888ms step_avg:58.90ms
step:1697/1775 train_time:99972ms step_avg:58.91ms
step:1698/1775 train_time:100060ms step_avg:58.93ms
step:1699/1775 train_time:100144ms step_avg:58.94ms
step:1700/1775 train_time:100232ms step_avg:58.96ms
step:1701/1775 train_time:100317ms step_avg:58.98ms
step:1702/1775 train_time:100405ms step_avg:58.99ms
step:1703/1775 train_time:100489ms step_avg:59.01ms
step:1704/1775 train_time:100576ms step_avg:59.02ms
step:1705/1775 train_time:100661ms step_avg:59.04ms
step:1706/1775 train_time:100749ms step_avg:59.06ms
step:1707/1775 train_time:100832ms step_avg:59.07ms
step:1708/1775 train_time:100919ms step_avg:59.09ms
step:1709/1775 train_time:101003ms step_avg:59.10ms
step:1710/1775 train_time:101092ms step_avg:59.12ms
step:1711/1775 train_time:101176ms step_avg:59.13ms
step:1712/1775 train_time:101264ms step_avg:59.15ms
step:1713/1775 train_time:101350ms step_avg:59.16ms
step:1714/1775 train_time:101435ms step_avg:59.18ms
step:1715/1775 train_time:101520ms step_avg:59.20ms
step:1716/1775 train_time:101608ms step_avg:59.21ms
step:1717/1775 train_time:101692ms step_avg:59.23ms
step:1718/1775 train_time:101778ms step_avg:59.24ms
step:1719/1775 train_time:101864ms step_avg:59.26ms
step:1720/1775 train_time:101952ms step_avg:59.27ms
step:1721/1775 train_time:102035ms step_avg:59.29ms
step:1722/1775 train_time:102123ms step_avg:59.30ms
step:1723/1775 train_time:102207ms step_avg:59.32ms
step:1724/1775 train_time:102294ms step_avg:59.34ms
step:1725/1775 train_time:102378ms step_avg:59.35ms
step:1726/1775 train_time:102466ms step_avg:59.37ms
step:1727/1775 train_time:102549ms step_avg:59.38ms
step:1728/1775 train_time:102636ms step_avg:59.40ms
step:1729/1775 train_time:102721ms step_avg:59.41ms
step:1730/1775 train_time:102808ms step_avg:59.43ms
step:1731/1775 train_time:102894ms step_avg:59.44ms
step:1732/1775 train_time:102981ms step_avg:59.46ms
step:1733/1775 train_time:103065ms step_avg:59.47ms
step:1734/1775 train_time:103152ms step_avg:59.49ms
step:1735/1775 train_time:103236ms step_avg:59.50ms
step:1736/1775 train_time:103328ms step_avg:59.52ms
step:1737/1775 train_time:103414ms step_avg:59.54ms
step:1738/1775 train_time:103503ms step_avg:59.55ms
step:1739/1775 train_time:103588ms step_avg:59.57ms
step:1740/1775 train_time:103675ms step_avg:59.58ms
step:1741/1775 train_time:103761ms step_avg:59.60ms
step:1742/1775 train_time:103849ms step_avg:59.61ms
step:1743/1775 train_time:103932ms step_avg:59.63ms
step:1744/1775 train_time:104020ms step_avg:59.64ms
step:1745/1775 train_time:104105ms step_avg:59.66ms
step:1746/1775 train_time:104193ms step_avg:59.68ms
step:1747/1775 train_time:104278ms step_avg:59.69ms
step:1748/1775 train_time:104366ms step_avg:59.71ms
step:1749/1775 train_time:104451ms step_avg:59.72ms
step:1750/1775 train_time:104538ms step_avg:59.74ms
step:1750/1775 val_loss:3.2854 train_time:104638ms step_avg:59.79ms
step:1751/1775 train_time:104658ms step_avg:59.77ms
step:1752/1775 train_time:104712ms step_avg:59.77ms
step:1753/1775 train_time:104799ms step_avg:59.78ms
step:1754/1775 train_time:104888ms step_avg:59.80ms
step:1755/1775 train_time:104972ms step_avg:59.81ms
step:1756/1775 train_time:105059ms step_avg:59.83ms
step:1757/1775 train_time:105143ms step_avg:59.84ms
step:1758/1775 train_time:105228ms step_avg:59.86ms
step:1759/1775 train_time:105311ms step_avg:59.87ms
step:1760/1775 train_time:105399ms step_avg:59.89ms
step:1761/1775 train_time:105484ms step_avg:59.90ms
step:1762/1775 train_time:105574ms step_avg:59.92ms
step:1763/1775 train_time:105661ms step_avg:59.93ms
step:1764/1775 train_time:105750ms step_avg:59.95ms
step:1765/1775 train_time:105837ms step_avg:59.96ms
step:1766/1775 train_time:105927ms step_avg:59.98ms
step:1767/1775 train_time:106011ms step_avg:59.99ms
step:1768/1775 train_time:106099ms step_avg:60.01ms
step:1769/1775 train_time:106183ms step_avg:60.02ms
step:1770/1775 train_time:106270ms step_avg:60.04ms
step:1771/1775 train_time:106352ms step_avg:60.05ms
step:1772/1775 train_time:106440ms step_avg:60.07ms
step:1773/1775 train_time:106526ms step_avg:60.08ms
step:1774/1775 train_time:106614ms step_avg:60.10ms
step:1775/1775 train_time:106703ms step_avg:60.11ms
step:1775/1775 val_loss:3.2790 train_time:106804ms step_avg:60.17ms
peak memory allocated: 29148 MiB reserved: 45018 MiB

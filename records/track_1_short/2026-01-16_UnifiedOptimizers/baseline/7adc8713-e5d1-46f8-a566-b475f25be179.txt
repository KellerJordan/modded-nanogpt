import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Jan 16 22:37:24 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            137W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    136594      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    1   N/A  N/A    136595      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A    136596      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A    136597      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A    136598      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A    136599      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A    136600      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A    136601      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8314 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:71ms step_avg:71.28ms
step:2/1775 train_time:94ms step_avg:47.03ms
step:3/1775 train_time:113ms step_avg:37.69ms
step:4/1775 train_time:141ms step_avg:35.32ms
step:5/1775 train_time:173ms step_avg:34.51ms
step:6/1775 train_time:264ms step_avg:44.00ms
step:7/1775 train_time:281ms step_avg:40.20ms
step:8/1775 train_time:307ms step_avg:38.36ms
step:9/1775 train_time:338ms step_avg:37.56ms
step:10/1775 train_time:371ms step_avg:37.14ms
step:11/1775 train_time:402ms step_avg:36.58ms
step:12/1775 train_time:436ms step_avg:36.37ms
step:13/1775 train_time:468ms step_avg:35.98ms
step:14/1775 train_time:501ms step_avg:35.80ms
step:15/1775 train_time:532ms step_avg:35.49ms
step:16/1775 train_time:566ms step_avg:35.36ms
step:17/1775 train_time:597ms step_avg:35.14ms
step:18/1775 train_time:631ms step_avg:35.05ms
step:19/1775 train_time:662ms step_avg:34.85ms
step:20/1775 train_time:696ms step_avg:34.80ms
step:21/1775 train_time:727ms step_avg:34.64ms
step:22/1775 train_time:761ms step_avg:34.59ms
step:23/1775 train_time:792ms step_avg:34.45ms
step:24/1775 train_time:826ms step_avg:34.41ms
step:25/1775 train_time:857ms step_avg:34.28ms
step:26/1775 train_time:890ms step_avg:34.25ms
step:27/1775 train_time:922ms step_avg:34.14ms
step:28/1775 train_time:955ms step_avg:34.12ms
step:29/1775 train_time:987ms step_avg:34.02ms
step:30/1775 train_time:1020ms step_avg:34.01ms
step:31/1775 train_time:1052ms step_avg:33.94ms
step:32/1775 train_time:1086ms step_avg:33.94ms
step:33/1775 train_time:1118ms step_avg:33.89ms
step:34/1775 train_time:1153ms step_avg:33.90ms
step:35/1775 train_time:1185ms step_avg:33.86ms
step:36/1775 train_time:1220ms step_avg:33.89ms
step:37/1775 train_time:1252ms step_avg:33.83ms
step:38/1775 train_time:1286ms step_avg:33.84ms
step:39/1775 train_time:1318ms step_avg:33.80ms
step:40/1775 train_time:1352ms step_avg:33.81ms
step:41/1775 train_time:1384ms step_avg:33.75ms
step:42/1775 train_time:1418ms step_avg:33.77ms
step:43/1775 train_time:1450ms step_avg:33.71ms
step:44/1775 train_time:1483ms step_avg:33.71ms
step:45/1775 train_time:1516ms step_avg:33.69ms
step:46/1775 train_time:1549ms step_avg:33.68ms
step:47/1775 train_time:1581ms step_avg:33.63ms
step:48/1775 train_time:1614ms step_avg:33.63ms
step:49/1775 train_time:1646ms step_avg:33.58ms
step:50/1775 train_time:1680ms step_avg:33.59ms
step:51/1775 train_time:1711ms step_avg:33.56ms
step:52/1775 train_time:1745ms step_avg:33.55ms
step:53/1775 train_time:1777ms step_avg:33.53ms
step:54/1775 train_time:1810ms step_avg:33.53ms
step:55/1775 train_time:1842ms step_avg:33.48ms
step:56/1775 train_time:1875ms step_avg:33.49ms
step:57/1775 train_time:1907ms step_avg:33.45ms
step:58/1775 train_time:1940ms step_avg:33.45ms
step:59/1775 train_time:1972ms step_avg:33.42ms
step:60/1775 train_time:2005ms step_avg:33.42ms
step:61/1775 train_time:2037ms step_avg:33.39ms
step:62/1775 train_time:2071ms step_avg:33.40ms
step:63/1775 train_time:2102ms step_avg:33.37ms
step:64/1775 train_time:2137ms step_avg:33.39ms
step:65/1775 train_time:2169ms step_avg:33.36ms
step:66/1775 train_time:2202ms step_avg:33.37ms
step:67/1775 train_time:2234ms step_avg:33.35ms
step:68/1775 train_time:2268ms step_avg:33.35ms
step:69/1775 train_time:2300ms step_avg:33.33ms
step:70/1775 train_time:2335ms step_avg:33.36ms
step:71/1775 train_time:2366ms step_avg:33.33ms
step:72/1775 train_time:2400ms step_avg:33.34ms
step:73/1775 train_time:2432ms step_avg:33.31ms
step:74/1775 train_time:2465ms step_avg:33.32ms
step:75/1775 train_time:2497ms step_avg:33.29ms
step:76/1775 train_time:2531ms step_avg:33.30ms
step:77/1775 train_time:2562ms step_avg:33.28ms
step:78/1775 train_time:2597ms step_avg:33.29ms
step:79/1775 train_time:2628ms step_avg:33.27ms
step:80/1775 train_time:2662ms step_avg:33.27ms
step:81/1775 train_time:2694ms step_avg:33.25ms
step:82/1775 train_time:2727ms step_avg:33.26ms
step:83/1775 train_time:2759ms step_avg:33.24ms
step:84/1775 train_time:2792ms step_avg:33.24ms
step:85/1775 train_time:2824ms step_avg:33.22ms
step:86/1775 train_time:2857ms step_avg:33.22ms
step:87/1775 train_time:2889ms step_avg:33.20ms
step:88/1775 train_time:2922ms step_avg:33.21ms
step:89/1775 train_time:2954ms step_avg:33.19ms
step:90/1775 train_time:2987ms step_avg:33.19ms
step:91/1775 train_time:3019ms step_avg:33.18ms
step:92/1775 train_time:3053ms step_avg:33.18ms
step:93/1775 train_time:3084ms step_avg:33.16ms
step:94/1775 train_time:3118ms step_avg:33.17ms
step:95/1775 train_time:3150ms step_avg:33.16ms
step:96/1775 train_time:3184ms step_avg:33.16ms
step:97/1775 train_time:3216ms step_avg:33.15ms
step:98/1775 train_time:3250ms step_avg:33.16ms
step:99/1775 train_time:3281ms step_avg:33.14ms
step:100/1775 train_time:3315ms step_avg:33.15ms
step:101/1775 train_time:3347ms step_avg:33.14ms
step:102/1775 train_time:3381ms step_avg:33.15ms
step:103/1775 train_time:3412ms step_avg:33.13ms
step:104/1775 train_time:3446ms step_avg:33.13ms
step:105/1775 train_time:3478ms step_avg:33.12ms
step:106/1775 train_time:3512ms step_avg:33.13ms
step:107/1775 train_time:3543ms step_avg:33.11ms
step:108/1775 train_time:3577ms step_avg:33.12ms
step:109/1775 train_time:3609ms step_avg:33.11ms
step:110/1775 train_time:3642ms step_avg:33.11ms
step:111/1775 train_time:3674ms step_avg:33.10ms
step:112/1775 train_time:3707ms step_avg:33.10ms
step:113/1775 train_time:3739ms step_avg:33.08ms
step:114/1775 train_time:3773ms step_avg:33.09ms
step:115/1775 train_time:3804ms step_avg:33.08ms
step:116/1775 train_time:3838ms step_avg:33.09ms
step:117/1775 train_time:3869ms step_avg:33.07ms
step:118/1775 train_time:3903ms step_avg:33.08ms
step:119/1775 train_time:3934ms step_avg:33.06ms
step:120/1775 train_time:3967ms step_avg:33.06ms
step:121/1775 train_time:3999ms step_avg:33.05ms
step:122/1775 train_time:4033ms step_avg:33.06ms
step:123/1775 train_time:4064ms step_avg:33.04ms
step:124/1775 train_time:4098ms step_avg:33.05ms
step:125/1775 train_time:4130ms step_avg:33.04ms
step:126/1775 train_time:4164ms step_avg:33.05ms
step:127/1775 train_time:4196ms step_avg:33.04ms
step:128/1775 train_time:4229ms step_avg:33.04ms
step:129/1775 train_time:4261ms step_avg:33.03ms
step:130/1775 train_time:4295ms step_avg:33.04ms
step:131/1775 train_time:4326ms step_avg:33.03ms
step:132/1775 train_time:4360ms step_avg:33.03ms
step:133/1775 train_time:4392ms step_avg:33.02ms
step:134/1775 train_time:4426ms step_avg:33.03ms
step:135/1775 train_time:4458ms step_avg:33.02ms
step:136/1775 train_time:4491ms step_avg:33.02ms
step:137/1775 train_time:4523ms step_avg:33.01ms
step:138/1775 train_time:4557ms step_avg:33.02ms
step:139/1775 train_time:4588ms step_avg:33.01ms
step:140/1775 train_time:4622ms step_avg:33.01ms
step:141/1775 train_time:4653ms step_avg:33.00ms
step:142/1775 train_time:4686ms step_avg:33.00ms
step:143/1775 train_time:4718ms step_avg:32.99ms
step:144/1775 train_time:4752ms step_avg:33.00ms
step:145/1775 train_time:4783ms step_avg:32.99ms
step:146/1775 train_time:4817ms step_avg:32.99ms
step:147/1775 train_time:4848ms step_avg:32.98ms
step:148/1775 train_time:4882ms step_avg:32.99ms
step:149/1775 train_time:4913ms step_avg:32.98ms
step:150/1775 train_time:4947ms step_avg:32.98ms
step:151/1775 train_time:4979ms step_avg:32.97ms
step:152/1775 train_time:5012ms step_avg:32.97ms
step:153/1775 train_time:5043ms step_avg:32.96ms
step:154/1775 train_time:5078ms step_avg:32.97ms
step:155/1775 train_time:5110ms step_avg:32.97ms
step:156/1775 train_time:5144ms step_avg:32.97ms
step:157/1775 train_time:5176ms step_avg:32.97ms
step:158/1775 train_time:5210ms step_avg:32.97ms
step:159/1775 train_time:5241ms step_avg:32.96ms
step:160/1775 train_time:5275ms step_avg:32.97ms
step:161/1775 train_time:5306ms step_avg:32.96ms
step:162/1775 train_time:5340ms step_avg:32.96ms
step:163/1775 train_time:5372ms step_avg:32.95ms
step:164/1775 train_time:5405ms step_avg:32.96ms
step:165/1775 train_time:5437ms step_avg:32.95ms
step:166/1775 train_time:5470ms step_avg:32.95ms
step:167/1775 train_time:5502ms step_avg:32.95ms
step:168/1775 train_time:5536ms step_avg:32.95ms
step:169/1775 train_time:5568ms step_avg:32.95ms
step:170/1775 train_time:5602ms step_avg:32.95ms
step:171/1775 train_time:5633ms step_avg:32.94ms
step:172/1775 train_time:5666ms step_avg:32.94ms
step:173/1775 train_time:5698ms step_avg:32.94ms
step:174/1775 train_time:5732ms step_avg:32.94ms
step:175/1775 train_time:5764ms step_avg:32.93ms
step:176/1775 train_time:5798ms step_avg:32.94ms
step:177/1775 train_time:5829ms step_avg:32.93ms
step:178/1775 train_time:5863ms step_avg:32.94ms
step:179/1775 train_time:5894ms step_avg:32.93ms
step:180/1775 train_time:5928ms step_avg:32.93ms
step:181/1775 train_time:5959ms step_avg:32.92ms
step:182/1775 train_time:5993ms step_avg:32.93ms
step:183/1775 train_time:6025ms step_avg:32.92ms
step:184/1775 train_time:6059ms step_avg:32.93ms
step:185/1775 train_time:6091ms step_avg:32.92ms
step:186/1775 train_time:6124ms step_avg:32.93ms
step:187/1775 train_time:6156ms step_avg:32.92ms
step:188/1775 train_time:6190ms step_avg:32.93ms
step:189/1775 train_time:6221ms step_avg:32.92ms
step:190/1775 train_time:6255ms step_avg:32.92ms
step:191/1775 train_time:6287ms step_avg:32.91ms
step:192/1775 train_time:6321ms step_avg:32.92ms
step:193/1775 train_time:6352ms step_avg:32.91ms
step:194/1775 train_time:6385ms step_avg:32.91ms
step:195/1775 train_time:6417ms step_avg:32.91ms
step:196/1775 train_time:6451ms step_avg:32.91ms
step:197/1775 train_time:6482ms step_avg:32.91ms
step:198/1775 train_time:6517ms step_avg:32.91ms
step:199/1775 train_time:6548ms step_avg:32.91ms
step:200/1775 train_time:6582ms step_avg:32.91ms
step:201/1775 train_time:6614ms step_avg:32.90ms
step:202/1775 train_time:6647ms step_avg:32.91ms
step:203/1775 train_time:6679ms step_avg:32.90ms
step:204/1775 train_time:6713ms step_avg:32.91ms
step:205/1775 train_time:6744ms step_avg:32.90ms
step:206/1775 train_time:6778ms step_avg:32.90ms
step:207/1775 train_time:6810ms step_avg:32.90ms
step:208/1775 train_time:6843ms step_avg:32.90ms
step:209/1775 train_time:6875ms step_avg:32.89ms
step:210/1775 train_time:6909ms step_avg:32.90ms
step:211/1775 train_time:6940ms step_avg:32.89ms
step:212/1775 train_time:6974ms step_avg:32.90ms
step:213/1775 train_time:7006ms step_avg:32.89ms
step:214/1775 train_time:7039ms step_avg:32.89ms
step:215/1775 train_time:7071ms step_avg:32.89ms
step:216/1775 train_time:7104ms step_avg:32.89ms
step:217/1775 train_time:7136ms step_avg:32.88ms
step:218/1775 train_time:7169ms step_avg:32.89ms
step:219/1775 train_time:7201ms step_avg:32.88ms
step:220/1775 train_time:7235ms step_avg:32.89ms
step:221/1775 train_time:7266ms step_avg:32.88ms
step:222/1775 train_time:7300ms step_avg:32.88ms
step:223/1775 train_time:7331ms step_avg:32.88ms
step:224/1775 train_time:7365ms step_avg:32.88ms
step:225/1775 train_time:7396ms step_avg:32.87ms
step:226/1775 train_time:7430ms step_avg:32.88ms
step:227/1775 train_time:7462ms step_avg:32.87ms
step:228/1775 train_time:7496ms step_avg:32.88ms
step:229/1775 train_time:7527ms step_avg:32.87ms
step:230/1775 train_time:7561ms step_avg:32.87ms
step:231/1775 train_time:7592ms step_avg:32.87ms
step:232/1775 train_time:7626ms step_avg:32.87ms
step:233/1775 train_time:7657ms step_avg:32.86ms
step:234/1775 train_time:7691ms step_avg:32.87ms
step:235/1775 train_time:7723ms step_avg:32.86ms
step:236/1775 train_time:7757ms step_avg:32.87ms
step:237/1775 train_time:7788ms step_avg:32.86ms
step:238/1775 train_time:7822ms step_avg:32.86ms
step:239/1775 train_time:7853ms step_avg:32.86ms
step:240/1775 train_time:7886ms step_avg:32.86ms
step:241/1775 train_time:7918ms step_avg:32.85ms
step:242/1775 train_time:7952ms step_avg:32.86ms
step:243/1775 train_time:7983ms step_avg:32.85ms
step:244/1775 train_time:8018ms step_avg:32.86ms
step:245/1775 train_time:8049ms step_avg:32.85ms
step:246/1775 train_time:8083ms step_avg:32.86ms
step:247/1775 train_time:8114ms step_avg:32.85ms
step:248/1775 train_time:8148ms step_avg:32.85ms
step:249/1775 train_time:8179ms step_avg:32.85ms
step:250/1775 train_time:8213ms step_avg:32.85ms
step:250/1775 val_loss:4.6053 train_time:8255ms step_avg:33.02ms
step:251/1775 train_time:8273ms step_avg:32.96ms
step:252/1775 train_time:8291ms step_avg:32.90ms
step:253/1775 train_time:8314ms step_avg:32.86ms
step:254/1775 train_time:8347ms step_avg:32.86ms
step:255/1775 train_time:8380ms step_avg:32.86ms
step:256/1775 train_time:8414ms step_avg:32.87ms
step:257/1775 train_time:8446ms step_avg:32.86ms
step:258/1775 train_time:8479ms step_avg:32.87ms
step:259/1775 train_time:8511ms step_avg:32.86ms
step:260/1775 train_time:8544ms step_avg:32.86ms
step:261/1775 train_time:8575ms step_avg:32.86ms
step:262/1775 train_time:8609ms step_avg:32.86ms
step:263/1775 train_time:8640ms step_avg:32.85ms
step:264/1775 train_time:8674ms step_avg:32.85ms
step:265/1775 train_time:8705ms step_avg:32.85ms
step:266/1775 train_time:8738ms step_avg:32.85ms
step:267/1775 train_time:8769ms step_avg:32.84ms
step:268/1775 train_time:8802ms step_avg:32.85ms
step:269/1775 train_time:8834ms step_avg:32.84ms
step:270/1775 train_time:8867ms step_avg:32.84ms
step:271/1775 train_time:8898ms step_avg:32.83ms
step:272/1775 train_time:8932ms step_avg:32.84ms
step:273/1775 train_time:8963ms step_avg:32.83ms
step:274/1775 train_time:8997ms step_avg:32.83ms
step:275/1775 train_time:9028ms step_avg:32.83ms
step:276/1775 train_time:9061ms step_avg:32.83ms
step:277/1775 train_time:9093ms step_avg:32.83ms
step:278/1775 train_time:9126ms step_avg:32.83ms
step:279/1775 train_time:9157ms step_avg:32.82ms
step:280/1775 train_time:9192ms step_avg:32.83ms
step:281/1775 train_time:9223ms step_avg:32.82ms
step:282/1775 train_time:9257ms step_avg:32.83ms
step:283/1775 train_time:9289ms step_avg:32.82ms
step:284/1775 train_time:9323ms step_avg:32.83ms
step:285/1775 train_time:9355ms step_avg:32.83ms
step:286/1775 train_time:9389ms step_avg:32.83ms
step:287/1775 train_time:9421ms step_avg:32.83ms
step:288/1775 train_time:9456ms step_avg:32.83ms
step:289/1775 train_time:9487ms step_avg:32.83ms
step:290/1775 train_time:9521ms step_avg:32.83ms
step:291/1775 train_time:9553ms step_avg:32.83ms
step:292/1775 train_time:9586ms step_avg:32.83ms
step:293/1775 train_time:9618ms step_avg:32.82ms
step:294/1775 train_time:9651ms step_avg:32.83ms
step:295/1775 train_time:9682ms step_avg:32.82ms
step:296/1775 train_time:9716ms step_avg:32.82ms
step:297/1775 train_time:9747ms step_avg:32.82ms
step:298/1775 train_time:9780ms step_avg:32.82ms
step:299/1775 train_time:9812ms step_avg:32.82ms
step:300/1775 train_time:9845ms step_avg:32.82ms
step:301/1775 train_time:9876ms step_avg:32.81ms
step:302/1775 train_time:9910ms step_avg:32.81ms
step:303/1775 train_time:9941ms step_avg:32.81ms
step:304/1775 train_time:9975ms step_avg:32.81ms
step:305/1775 train_time:10006ms step_avg:32.81ms
step:306/1775 train_time:10039ms step_avg:32.81ms
step:307/1775 train_time:10071ms step_avg:32.80ms
step:308/1775 train_time:10104ms step_avg:32.80ms
step:309/1775 train_time:10135ms step_avg:32.80ms
step:310/1775 train_time:10169ms step_avg:32.80ms
step:311/1775 train_time:10201ms step_avg:32.80ms
step:312/1775 train_time:10234ms step_avg:32.80ms
step:313/1775 train_time:10266ms step_avg:32.80ms
step:314/1775 train_time:10300ms step_avg:32.80ms
step:315/1775 train_time:10332ms step_avg:32.80ms
step:316/1775 train_time:10366ms step_avg:32.80ms
step:317/1775 train_time:10397ms step_avg:32.80ms
step:318/1775 train_time:10431ms step_avg:32.80ms
step:319/1775 train_time:10463ms step_avg:32.80ms
step:320/1775 train_time:10497ms step_avg:32.80ms
step:321/1775 train_time:10528ms step_avg:32.80ms
step:322/1775 train_time:10562ms step_avg:32.80ms
step:323/1775 train_time:10594ms step_avg:32.80ms
step:324/1775 train_time:10628ms step_avg:32.80ms
step:325/1775 train_time:10659ms step_avg:32.80ms
step:326/1775 train_time:10693ms step_avg:32.80ms
step:327/1775 train_time:10725ms step_avg:32.80ms
step:328/1775 train_time:10758ms step_avg:32.80ms
step:329/1775 train_time:10789ms step_avg:32.79ms
step:330/1775 train_time:10823ms step_avg:32.80ms
step:331/1775 train_time:10854ms step_avg:32.79ms
step:332/1775 train_time:10888ms step_avg:32.80ms
step:333/1775 train_time:10919ms step_avg:32.79ms
step:334/1775 train_time:10953ms step_avg:32.79ms
step:335/1775 train_time:10984ms step_avg:32.79ms
step:336/1775 train_time:11018ms step_avg:32.79ms
step:337/1775 train_time:11049ms step_avg:32.79ms
step:338/1775 train_time:11082ms step_avg:32.79ms
step:339/1775 train_time:11114ms step_avg:32.78ms
step:340/1775 train_time:11147ms step_avg:32.79ms
step:341/1775 train_time:11179ms step_avg:32.78ms
step:342/1775 train_time:11212ms step_avg:32.78ms
step:343/1775 train_time:11244ms step_avg:32.78ms
step:344/1775 train_time:11278ms step_avg:32.78ms
step:345/1775 train_time:11309ms step_avg:32.78ms
step:346/1775 train_time:11343ms step_avg:32.78ms
step:347/1775 train_time:11374ms step_avg:32.78ms
step:348/1775 train_time:11408ms step_avg:32.78ms
step:349/1775 train_time:11440ms step_avg:32.78ms
step:350/1775 train_time:11474ms step_avg:32.78ms
step:351/1775 train_time:11506ms step_avg:32.78ms
step:352/1775 train_time:11539ms step_avg:32.78ms
step:353/1775 train_time:11570ms step_avg:32.78ms
step:354/1775 train_time:11604ms step_avg:32.78ms
step:355/1775 train_time:11636ms step_avg:32.78ms
step:356/1775 train_time:11670ms step_avg:32.78ms
step:357/1775 train_time:11701ms step_avg:32.78ms
step:358/1775 train_time:11735ms step_avg:32.78ms
step:359/1775 train_time:11767ms step_avg:32.78ms
step:360/1775 train_time:11800ms step_avg:32.78ms
step:361/1775 train_time:11832ms step_avg:32.77ms
step:362/1775 train_time:11865ms step_avg:32.78ms
step:363/1775 train_time:11896ms step_avg:32.77ms
step:364/1775 train_time:11931ms step_avg:32.78ms
step:365/1775 train_time:11962ms step_avg:32.77ms
step:366/1775 train_time:11996ms step_avg:32.78ms
step:367/1775 train_time:12027ms step_avg:32.77ms
step:368/1775 train_time:12060ms step_avg:32.77ms
step:369/1775 train_time:12092ms step_avg:32.77ms
step:370/1775 train_time:12125ms step_avg:32.77ms
step:371/1775 train_time:12157ms step_avg:32.77ms
step:372/1775 train_time:12191ms step_avg:32.77ms
step:373/1775 train_time:12222ms step_avg:32.77ms
step:374/1775 train_time:12256ms step_avg:32.77ms
step:375/1775 train_time:12287ms step_avg:32.77ms
step:376/1775 train_time:12321ms step_avg:32.77ms
step:377/1775 train_time:12352ms step_avg:32.76ms
step:378/1775 train_time:12386ms step_avg:32.77ms
step:379/1775 train_time:12418ms step_avg:32.76ms
step:380/1775 train_time:12451ms step_avg:32.77ms
step:381/1775 train_time:12483ms step_avg:32.76ms
step:382/1775 train_time:12516ms step_avg:32.77ms
step:383/1775 train_time:12548ms step_avg:32.76ms
step:384/1775 train_time:12582ms step_avg:32.76ms
step:385/1775 train_time:12613ms step_avg:32.76ms
step:386/1775 train_time:12647ms step_avg:32.77ms
step:387/1775 train_time:12679ms step_avg:32.76ms
step:388/1775 train_time:12713ms step_avg:32.76ms
step:389/1775 train_time:12744ms step_avg:32.76ms
step:390/1775 train_time:12778ms step_avg:32.76ms
step:391/1775 train_time:12809ms step_avg:32.76ms
step:392/1775 train_time:12843ms step_avg:32.76ms
step:393/1775 train_time:12875ms step_avg:32.76ms
step:394/1775 train_time:12908ms step_avg:32.76ms
step:395/1775 train_time:12939ms step_avg:32.76ms
step:396/1775 train_time:12973ms step_avg:32.76ms
step:397/1775 train_time:13005ms step_avg:32.76ms
step:398/1775 train_time:13039ms step_avg:32.76ms
step:399/1775 train_time:13070ms step_avg:32.76ms
step:400/1775 train_time:13104ms step_avg:32.76ms
step:401/1775 train_time:13135ms step_avg:32.76ms
step:402/1775 train_time:13169ms step_avg:32.76ms
step:403/1775 train_time:13201ms step_avg:32.76ms
step:404/1775 train_time:13234ms step_avg:32.76ms
step:405/1775 train_time:13266ms step_avg:32.75ms
step:406/1775 train_time:13299ms step_avg:32.76ms
step:407/1775 train_time:13331ms step_avg:32.75ms
step:408/1775 train_time:13365ms step_avg:32.76ms
step:409/1775 train_time:13397ms step_avg:32.75ms
step:410/1775 train_time:13430ms step_avg:32.76ms
step:411/1775 train_time:13462ms step_avg:32.75ms
step:412/1775 train_time:13496ms step_avg:32.76ms
step:413/1775 train_time:13527ms step_avg:32.75ms
step:414/1775 train_time:13561ms step_avg:32.75ms
step:415/1775 train_time:13592ms step_avg:32.75ms
step:416/1775 train_time:13625ms step_avg:32.75ms
step:417/1775 train_time:13657ms step_avg:32.75ms
step:418/1775 train_time:13692ms step_avg:32.76ms
step:419/1775 train_time:13723ms step_avg:32.75ms
step:420/1775 train_time:13757ms step_avg:32.75ms
step:421/1775 train_time:13788ms step_avg:32.75ms
step:422/1775 train_time:13821ms step_avg:32.75ms
step:423/1775 train_time:13853ms step_avg:32.75ms
step:424/1775 train_time:13887ms step_avg:32.75ms
step:425/1775 train_time:13918ms step_avg:32.75ms
step:426/1775 train_time:13951ms step_avg:32.75ms
step:427/1775 train_time:13983ms step_avg:32.75ms
step:428/1775 train_time:14017ms step_avg:32.75ms
step:429/1775 train_time:14049ms step_avg:32.75ms
step:430/1775 train_time:14082ms step_avg:32.75ms
step:431/1775 train_time:14114ms step_avg:32.75ms
step:432/1775 train_time:14147ms step_avg:32.75ms
step:433/1775 train_time:14179ms step_avg:32.74ms
step:434/1775 train_time:14212ms step_avg:32.75ms
step:435/1775 train_time:14244ms step_avg:32.74ms
step:436/1775 train_time:14277ms step_avg:32.75ms
step:437/1775 train_time:14309ms step_avg:32.74ms
step:438/1775 train_time:14343ms step_avg:32.75ms
step:439/1775 train_time:14375ms step_avg:32.74ms
step:440/1775 train_time:14408ms step_avg:32.75ms
step:441/1775 train_time:14440ms step_avg:32.74ms
step:442/1775 train_time:14473ms step_avg:32.75ms
step:443/1775 train_time:14505ms step_avg:32.74ms
step:444/1775 train_time:14538ms step_avg:32.74ms
step:445/1775 train_time:14570ms step_avg:32.74ms
step:446/1775 train_time:14603ms step_avg:32.74ms
step:447/1775 train_time:14635ms step_avg:32.74ms
step:448/1775 train_time:14668ms step_avg:32.74ms
step:449/1775 train_time:14700ms step_avg:32.74ms
step:450/1775 train_time:14734ms step_avg:32.74ms
step:451/1775 train_time:14765ms step_avg:32.74ms
step:452/1775 train_time:14799ms step_avg:32.74ms
step:453/1775 train_time:14831ms step_avg:32.74ms
step:454/1775 train_time:14864ms step_avg:32.74ms
step:455/1775 train_time:14896ms step_avg:32.74ms
step:456/1775 train_time:14930ms step_avg:32.74ms
step:457/1775 train_time:14961ms step_avg:32.74ms
step:458/1775 train_time:14995ms step_avg:32.74ms
step:459/1775 train_time:15026ms step_avg:32.74ms
step:460/1775 train_time:15060ms step_avg:32.74ms
step:461/1775 train_time:15091ms step_avg:32.74ms
step:462/1775 train_time:15125ms step_avg:32.74ms
step:463/1775 train_time:15156ms step_avg:32.73ms
step:464/1775 train_time:15190ms step_avg:32.74ms
step:465/1775 train_time:15221ms step_avg:32.73ms
step:466/1775 train_time:15255ms step_avg:32.74ms
step:467/1775 train_time:15287ms step_avg:32.73ms
step:468/1775 train_time:15321ms step_avg:32.74ms
step:469/1775 train_time:15353ms step_avg:32.74ms
step:470/1775 train_time:15386ms step_avg:32.74ms
step:471/1775 train_time:15418ms step_avg:32.73ms
step:472/1775 train_time:15451ms step_avg:32.74ms
step:473/1775 train_time:15483ms step_avg:32.73ms
step:474/1775 train_time:15517ms step_avg:32.74ms
step:475/1775 train_time:15549ms step_avg:32.73ms
step:476/1775 train_time:15582ms step_avg:32.74ms
step:477/1775 train_time:15614ms step_avg:32.73ms
step:478/1775 train_time:15648ms step_avg:32.74ms
step:479/1775 train_time:15679ms step_avg:32.73ms
step:480/1775 train_time:15713ms step_avg:32.74ms
step:481/1775 train_time:15745ms step_avg:32.73ms
step:482/1775 train_time:15778ms step_avg:32.74ms
step:483/1775 train_time:15810ms step_avg:32.73ms
step:484/1775 train_time:15843ms step_avg:32.73ms
step:485/1775 train_time:15875ms step_avg:32.73ms
step:486/1775 train_time:15909ms step_avg:32.73ms
step:487/1775 train_time:15940ms step_avg:32.73ms
step:488/1775 train_time:15974ms step_avg:32.73ms
step:489/1775 train_time:16006ms step_avg:32.73ms
step:490/1775 train_time:16040ms step_avg:32.73ms
step:491/1775 train_time:16071ms step_avg:32.73ms
step:492/1775 train_time:16105ms step_avg:32.73ms
step:493/1775 train_time:16136ms step_avg:32.73ms
step:494/1775 train_time:16170ms step_avg:32.73ms
step:495/1775 train_time:16201ms step_avg:32.73ms
step:496/1775 train_time:16235ms step_avg:32.73ms
step:497/1775 train_time:16267ms step_avg:32.73ms
step:498/1775 train_time:16300ms step_avg:32.73ms
step:499/1775 train_time:16332ms step_avg:32.73ms
step:500/1775 train_time:16366ms step_avg:32.73ms
step:500/1775 val_loss:4.2747 train_time:16407ms step_avg:32.81ms
step:501/1775 train_time:16426ms step_avg:32.79ms
step:502/1775 train_time:16445ms step_avg:32.76ms
step:503/1775 train_time:16464ms step_avg:32.73ms
step:504/1775 train_time:16498ms step_avg:32.73ms
step:505/1775 train_time:16531ms step_avg:32.73ms
step:506/1775 train_time:16566ms step_avg:32.74ms
step:507/1775 train_time:16597ms step_avg:32.74ms
step:508/1775 train_time:16632ms step_avg:32.74ms
step:509/1775 train_time:16663ms step_avg:32.74ms
step:510/1775 train_time:16697ms step_avg:32.74ms
step:511/1775 train_time:16728ms step_avg:32.74ms
step:512/1775 train_time:16761ms step_avg:32.74ms
step:513/1775 train_time:16792ms step_avg:32.73ms
step:514/1775 train_time:16826ms step_avg:32.74ms
step:515/1775 train_time:16858ms step_avg:32.73ms
step:516/1775 train_time:16891ms step_avg:32.73ms
step:517/1775 train_time:16922ms step_avg:32.73ms
step:518/1775 train_time:16955ms step_avg:32.73ms
step:519/1775 train_time:16986ms step_avg:32.73ms
step:520/1775 train_time:17020ms step_avg:32.73ms
step:521/1775 train_time:17051ms step_avg:32.73ms
step:522/1775 train_time:17085ms step_avg:32.73ms
step:523/1775 train_time:17116ms step_avg:32.73ms
step:524/1775 train_time:17150ms step_avg:32.73ms
step:525/1775 train_time:17181ms step_avg:32.73ms
step:526/1775 train_time:17215ms step_avg:32.73ms
step:527/1775 train_time:17246ms step_avg:32.73ms
step:528/1775 train_time:17279ms step_avg:32.73ms
step:529/1775 train_time:17311ms step_avg:32.72ms
step:530/1775 train_time:17346ms step_avg:32.73ms
step:531/1775 train_time:17377ms step_avg:32.73ms
step:532/1775 train_time:17411ms step_avg:32.73ms
step:533/1775 train_time:17443ms step_avg:32.73ms
step:534/1775 train_time:17477ms step_avg:32.73ms
step:535/1775 train_time:17509ms step_avg:32.73ms
step:536/1775 train_time:17544ms step_avg:32.73ms
step:537/1775 train_time:17575ms step_avg:32.73ms
step:538/1775 train_time:17609ms step_avg:32.73ms
step:539/1775 train_time:17641ms step_avg:32.73ms
step:540/1775 train_time:17674ms step_avg:32.73ms
step:541/1775 train_time:17706ms step_avg:32.73ms
step:542/1775 train_time:17739ms step_avg:32.73ms
step:543/1775 train_time:17772ms step_avg:32.73ms
step:544/1775 train_time:17806ms step_avg:32.73ms
step:545/1775 train_time:17837ms step_avg:32.73ms
step:546/1775 train_time:17870ms step_avg:32.73ms
step:547/1775 train_time:17902ms step_avg:32.73ms
step:548/1775 train_time:17935ms step_avg:32.73ms
step:549/1775 train_time:17967ms step_avg:32.73ms
step:550/1775 train_time:18000ms step_avg:32.73ms
step:551/1775 train_time:18032ms step_avg:32.73ms
step:552/1775 train_time:18065ms step_avg:32.73ms
step:553/1775 train_time:18096ms step_avg:32.72ms
step:554/1775 train_time:18130ms step_avg:32.73ms
step:555/1775 train_time:18162ms step_avg:32.72ms
step:556/1775 train_time:18195ms step_avg:32.73ms
step:557/1775 train_time:18226ms step_avg:32.72ms
step:558/1775 train_time:18260ms step_avg:32.72ms
step:559/1775 train_time:18291ms step_avg:32.72ms
step:560/1775 train_time:18325ms step_avg:32.72ms
step:561/1775 train_time:18357ms step_avg:32.72ms
step:562/1775 train_time:18391ms step_avg:32.72ms
step:563/1775 train_time:18423ms step_avg:32.72ms
step:564/1775 train_time:18456ms step_avg:32.72ms
step:565/1775 train_time:18488ms step_avg:32.72ms
step:566/1775 train_time:18522ms step_avg:32.72ms
step:567/1775 train_time:18554ms step_avg:32.72ms
step:568/1775 train_time:18587ms step_avg:32.72ms
step:569/1775 train_time:18619ms step_avg:32.72ms
step:570/1775 train_time:18652ms step_avg:32.72ms
step:571/1775 train_time:18685ms step_avg:32.72ms
step:572/1775 train_time:18718ms step_avg:32.72ms
step:573/1775 train_time:18750ms step_avg:32.72ms
step:574/1775 train_time:18784ms step_avg:32.72ms
step:575/1775 train_time:18815ms step_avg:32.72ms
step:576/1775 train_time:18850ms step_avg:32.73ms
step:577/1775 train_time:18881ms step_avg:32.72ms
step:578/1775 train_time:18915ms step_avg:32.72ms
step:579/1775 train_time:18946ms step_avg:32.72ms
step:580/1775 train_time:18982ms step_avg:32.73ms
step:581/1775 train_time:19041ms step_avg:32.77ms
step:582/1775 train_time:19102ms step_avg:32.82ms
step:583/1775 train_time:19160ms step_avg:32.86ms
step:584/1775 train_time:19221ms step_avg:32.91ms
step:585/1775 train_time:19279ms step_avg:32.96ms
step:586/1775 train_time:19341ms step_avg:33.00ms
step:587/1775 train_time:19399ms step_avg:33.05ms
step:588/1775 train_time:19461ms step_avg:33.10ms
step:589/1775 train_time:19520ms step_avg:33.14ms
step:590/1775 train_time:19582ms step_avg:33.19ms
step:591/1775 train_time:19641ms step_avg:33.23ms
step:592/1775 train_time:19703ms step_avg:33.28ms
step:593/1775 train_time:19762ms step_avg:33.32ms
step:594/1775 train_time:19824ms step_avg:33.37ms
step:595/1775 train_time:19882ms step_avg:33.42ms
step:596/1775 train_time:19943ms step_avg:33.46ms
step:597/1775 train_time:20001ms step_avg:33.50ms
step:598/1775 train_time:20062ms step_avg:33.55ms
step:599/1775 train_time:20120ms step_avg:33.59ms
step:600/1775 train_time:20182ms step_avg:33.64ms
step:601/1775 train_time:20240ms step_avg:33.68ms
step:602/1775 train_time:20302ms step_avg:33.72ms
step:603/1775 train_time:20360ms step_avg:33.76ms
step:604/1775 train_time:20421ms step_avg:33.81ms
step:605/1775 train_time:20480ms step_avg:33.85ms
step:606/1775 train_time:20541ms step_avg:33.90ms
step:607/1775 train_time:20600ms step_avg:33.94ms
step:608/1775 train_time:20663ms step_avg:33.99ms
step:609/1775 train_time:20721ms step_avg:34.03ms
step:610/1775 train_time:20783ms step_avg:34.07ms
step:611/1775 train_time:20841ms step_avg:34.11ms
step:612/1775 train_time:20903ms step_avg:34.16ms
step:613/1775 train_time:20962ms step_avg:34.20ms
step:614/1775 train_time:21023ms step_avg:34.24ms
step:615/1775 train_time:21082ms step_avg:34.28ms
step:616/1775 train_time:21143ms step_avg:34.32ms
step:617/1775 train_time:21201ms step_avg:34.36ms
step:618/1775 train_time:21262ms step_avg:34.40ms
step:619/1775 train_time:21321ms step_avg:34.44ms
step:620/1775 train_time:21382ms step_avg:34.49ms
step:621/1775 train_time:21441ms step_avg:34.53ms
step:622/1775 train_time:21502ms step_avg:34.57ms
step:623/1775 train_time:21560ms step_avg:34.61ms
step:624/1775 train_time:21622ms step_avg:34.65ms
step:625/1775 train_time:21682ms step_avg:34.69ms
step:626/1775 train_time:21742ms step_avg:34.73ms
step:627/1775 train_time:21801ms step_avg:34.77ms
step:628/1775 train_time:21862ms step_avg:34.81ms
step:629/1775 train_time:21921ms step_avg:34.85ms
step:630/1775 train_time:21983ms step_avg:34.89ms
step:631/1775 train_time:22042ms step_avg:34.93ms
step:632/1775 train_time:22103ms step_avg:34.97ms
step:633/1775 train_time:22161ms step_avg:35.01ms
step:634/1775 train_time:22222ms step_avg:35.05ms
step:635/1775 train_time:22280ms step_avg:35.09ms
step:636/1775 train_time:22341ms step_avg:35.13ms
step:637/1775 train_time:22400ms step_avg:35.17ms
step:638/1775 train_time:22461ms step_avg:35.21ms
step:639/1775 train_time:22520ms step_avg:35.24ms
step:640/1775 train_time:22582ms step_avg:35.28ms
step:641/1775 train_time:22640ms step_avg:35.32ms
step:642/1775 train_time:22702ms step_avg:35.36ms
step:643/1775 train_time:22761ms step_avg:35.40ms
step:644/1775 train_time:22822ms step_avg:35.44ms
step:645/1775 train_time:22881ms step_avg:35.47ms
step:646/1775 train_time:22943ms step_avg:35.52ms
step:647/1775 train_time:23002ms step_avg:35.55ms
step:648/1775 train_time:23063ms step_avg:35.59ms
step:649/1775 train_time:23121ms step_avg:35.63ms
step:650/1775 train_time:23183ms step_avg:35.67ms
step:651/1775 train_time:23241ms step_avg:35.70ms
step:652/1775 train_time:23302ms step_avg:35.74ms
step:653/1775 train_time:23360ms step_avg:35.77ms
step:654/1775 train_time:23422ms step_avg:35.81ms
step:655/1775 train_time:23482ms step_avg:35.85ms
step:656/1775 train_time:23543ms step_avg:35.89ms
step:657/1775 train_time:23602ms step_avg:35.92ms
step:658/1775 train_time:23663ms step_avg:35.96ms
step:659/1775 train_time:23722ms step_avg:36.00ms
step:660/1775 train_time:23784ms step_avg:36.04ms
step:661/1775 train_time:23843ms step_avg:36.07ms
step:662/1775 train_time:23905ms step_avg:36.11ms
step:663/1775 train_time:23963ms step_avg:36.14ms
step:664/1775 train_time:24024ms step_avg:36.18ms
step:665/1775 train_time:24082ms step_avg:36.21ms
step:666/1775 train_time:24143ms step_avg:36.25ms
step:667/1775 train_time:24202ms step_avg:36.28ms
step:668/1775 train_time:24263ms step_avg:36.32ms
step:669/1775 train_time:24322ms step_avg:36.36ms
step:670/1775 train_time:24384ms step_avg:36.39ms
step:671/1775 train_time:24442ms step_avg:36.43ms
step:672/1775 train_time:24503ms step_avg:36.46ms
step:673/1775 train_time:24562ms step_avg:36.50ms
step:674/1775 train_time:24622ms step_avg:36.53ms
step:675/1775 train_time:24681ms step_avg:36.57ms
step:676/1775 train_time:24743ms step_avg:36.60ms
step:677/1775 train_time:24802ms step_avg:36.63ms
step:678/1775 train_time:24864ms step_avg:36.67ms
step:679/1775 train_time:24922ms step_avg:36.70ms
step:680/1775 train_time:24984ms step_avg:36.74ms
step:681/1775 train_time:25042ms step_avg:36.77ms
step:682/1775 train_time:25103ms step_avg:36.81ms
step:683/1775 train_time:25162ms step_avg:36.84ms
step:684/1775 train_time:25224ms step_avg:36.88ms
step:685/1775 train_time:25282ms step_avg:36.91ms
step:686/1775 train_time:25344ms step_avg:36.94ms
step:687/1775 train_time:25403ms step_avg:36.98ms
step:688/1775 train_time:25464ms step_avg:37.01ms
step:689/1775 train_time:25523ms step_avg:37.04ms
step:690/1775 train_time:25584ms step_avg:37.08ms
step:691/1775 train_time:25642ms step_avg:37.11ms
step:692/1775 train_time:25704ms step_avg:37.14ms
step:693/1775 train_time:25763ms step_avg:37.18ms
step:694/1775 train_time:25824ms step_avg:37.21ms
step:695/1775 train_time:25884ms step_avg:37.24ms
step:696/1775 train_time:25945ms step_avg:37.28ms
step:697/1775 train_time:26004ms step_avg:37.31ms
step:698/1775 train_time:26065ms step_avg:37.34ms
step:699/1775 train_time:26124ms step_avg:37.37ms
step:700/1775 train_time:26186ms step_avg:37.41ms
step:701/1775 train_time:26244ms step_avg:37.44ms
step:702/1775 train_time:26306ms step_avg:37.47ms
step:703/1775 train_time:26364ms step_avg:37.50ms
step:704/1775 train_time:26426ms step_avg:37.54ms
step:705/1775 train_time:26485ms step_avg:37.57ms
step:706/1775 train_time:26546ms step_avg:37.60ms
step:707/1775 train_time:26604ms step_avg:37.63ms
step:708/1775 train_time:26665ms step_avg:37.66ms
step:709/1775 train_time:26723ms step_avg:37.69ms
step:710/1775 train_time:26786ms step_avg:37.73ms
step:711/1775 train_time:26844ms step_avg:37.75ms
step:712/1775 train_time:26905ms step_avg:37.79ms
step:713/1775 train_time:26963ms step_avg:37.82ms
step:714/1775 train_time:27024ms step_avg:37.85ms
step:715/1775 train_time:27083ms step_avg:37.88ms
step:716/1775 train_time:27144ms step_avg:37.91ms
step:717/1775 train_time:27203ms step_avg:37.94ms
step:718/1775 train_time:27264ms step_avg:37.97ms
step:719/1775 train_time:27324ms step_avg:38.00ms
step:720/1775 train_time:27385ms step_avg:38.03ms
step:721/1775 train_time:27444ms step_avg:38.06ms
step:722/1775 train_time:27505ms step_avg:38.10ms
step:723/1775 train_time:27564ms step_avg:38.12ms
step:724/1775 train_time:27626ms step_avg:38.16ms
step:725/1775 train_time:27684ms step_avg:38.19ms
step:726/1775 train_time:27746ms step_avg:38.22ms
step:727/1775 train_time:27804ms step_avg:38.24ms
step:728/1775 train_time:27865ms step_avg:38.28ms
step:729/1775 train_time:27924ms step_avg:38.30ms
step:730/1775 train_time:27986ms step_avg:38.34ms
step:731/1775 train_time:28044ms step_avg:38.36ms
step:732/1775 train_time:28105ms step_avg:38.40ms
step:733/1775 train_time:28164ms step_avg:38.42ms
step:734/1775 train_time:28225ms step_avg:38.45ms
step:735/1775 train_time:28284ms step_avg:38.48ms
step:736/1775 train_time:28345ms step_avg:38.51ms
step:737/1775 train_time:28404ms step_avg:38.54ms
step:738/1775 train_time:28465ms step_avg:38.57ms
step:739/1775 train_time:28523ms step_avg:38.60ms
step:740/1775 train_time:28585ms step_avg:38.63ms
step:741/1775 train_time:28643ms step_avg:38.65ms
step:742/1775 train_time:28704ms step_avg:38.68ms
step:743/1775 train_time:28763ms step_avg:38.71ms
step:744/1775 train_time:28824ms step_avg:38.74ms
step:745/1775 train_time:28882ms step_avg:38.77ms
step:746/1775 train_time:28943ms step_avg:38.80ms
step:747/1775 train_time:29002ms step_avg:38.82ms
step:748/1775 train_time:29063ms step_avg:38.85ms
step:749/1775 train_time:29122ms step_avg:38.88ms
step:750/1775 train_time:29184ms step_avg:38.91ms
step:750/1775 val_loss:3.9948 train_time:29254ms step_avg:39.01ms
step:751/1775 train_time:29273ms step_avg:38.98ms
step:752/1775 train_time:29304ms step_avg:38.97ms
step:753/1775 train_time:29364ms step_avg:39.00ms
step:754/1775 train_time:29426ms step_avg:39.03ms
step:755/1775 train_time:29484ms step_avg:39.05ms
step:756/1775 train_time:29545ms step_avg:39.08ms
step:757/1775 train_time:29603ms step_avg:39.11ms
step:758/1775 train_time:29663ms step_avg:39.13ms
step:759/1775 train_time:29721ms step_avg:39.16ms
step:760/1775 train_time:29781ms step_avg:39.19ms
step:761/1775 train_time:29840ms step_avg:39.21ms
step:762/1775 train_time:29901ms step_avg:39.24ms
step:763/1775 train_time:29959ms step_avg:39.26ms
step:764/1775 train_time:30020ms step_avg:39.29ms
step:765/1775 train_time:30079ms step_avg:39.32ms
step:766/1775 train_time:30141ms step_avg:39.35ms
step:767/1775 train_time:30200ms step_avg:39.37ms
step:768/1775 train_time:30263ms step_avg:39.41ms
step:769/1775 train_time:30323ms step_avg:39.43ms
step:770/1775 train_time:30384ms step_avg:39.46ms
step:771/1775 train_time:30443ms step_avg:39.48ms
step:772/1775 train_time:30504ms step_avg:39.51ms
step:773/1775 train_time:30562ms step_avg:39.54ms
step:774/1775 train_time:30623ms step_avg:39.56ms
step:775/1775 train_time:30681ms step_avg:39.59ms
step:776/1775 train_time:30742ms step_avg:39.62ms
step:777/1775 train_time:30800ms step_avg:39.64ms
step:778/1775 train_time:30860ms step_avg:39.67ms
step:779/1775 train_time:30919ms step_avg:39.69ms
step:780/1775 train_time:30979ms step_avg:39.72ms
step:781/1775 train_time:31037ms step_avg:39.74ms
step:782/1775 train_time:31097ms step_avg:39.77ms
step:783/1775 train_time:31156ms step_avg:39.79ms
step:784/1775 train_time:31219ms step_avg:39.82ms
step:785/1775 train_time:31279ms step_avg:39.85ms
step:786/1775 train_time:31341ms step_avg:39.87ms
step:787/1775 train_time:31400ms step_avg:39.90ms
step:788/1775 train_time:31462ms step_avg:39.93ms
step:789/1775 train_time:31520ms step_avg:39.95ms
step:790/1775 train_time:31582ms step_avg:39.98ms
step:791/1775 train_time:31639ms step_avg:40.00ms
step:792/1775 train_time:31700ms step_avg:40.02ms
step:793/1775 train_time:31758ms step_avg:40.05ms
step:794/1775 train_time:31819ms step_avg:40.07ms
step:795/1775 train_time:31877ms step_avg:40.10ms
step:796/1775 train_time:31937ms step_avg:40.12ms
step:797/1775 train_time:31995ms step_avg:40.14ms
step:798/1775 train_time:32055ms step_avg:40.17ms
step:799/1775 train_time:32113ms step_avg:40.19ms
step:800/1775 train_time:32175ms step_avg:40.22ms
step:801/1775 train_time:32234ms step_avg:40.24ms
step:802/1775 train_time:32296ms step_avg:40.27ms
step:803/1775 train_time:32356ms step_avg:40.29ms
step:804/1775 train_time:32418ms step_avg:40.32ms
step:805/1775 train_time:32477ms step_avg:40.34ms
step:806/1775 train_time:32538ms step_avg:40.37ms
step:807/1775 train_time:32596ms step_avg:40.39ms
step:808/1775 train_time:32658ms step_avg:40.42ms
step:809/1775 train_time:32717ms step_avg:40.44ms
step:810/1775 train_time:32777ms step_avg:40.47ms
step:811/1775 train_time:32835ms step_avg:40.49ms
step:812/1775 train_time:32895ms step_avg:40.51ms
step:813/1775 train_time:32954ms step_avg:40.53ms
step:814/1775 train_time:33016ms step_avg:40.56ms
step:815/1775 train_time:33074ms step_avg:40.58ms
step:816/1775 train_time:33135ms step_avg:40.61ms
step:817/1775 train_time:33194ms step_avg:40.63ms
step:818/1775 train_time:33256ms step_avg:40.65ms
step:819/1775 train_time:33315ms step_avg:40.68ms
step:820/1775 train_time:33377ms step_avg:40.70ms
step:821/1775 train_time:33437ms step_avg:40.73ms
step:822/1775 train_time:33498ms step_avg:40.75ms
step:823/1775 train_time:33557ms step_avg:40.77ms
step:824/1775 train_time:33618ms step_avg:40.80ms
step:825/1775 train_time:33677ms step_avg:40.82ms
step:826/1775 train_time:33738ms step_avg:40.85ms
step:827/1775 train_time:33797ms step_avg:40.87ms
step:828/1775 train_time:33858ms step_avg:40.89ms
step:829/1775 train_time:33917ms step_avg:40.91ms
step:830/1775 train_time:33977ms step_avg:40.94ms
step:831/1775 train_time:34036ms step_avg:40.96ms
step:832/1775 train_time:34097ms step_avg:40.98ms
step:833/1775 train_time:34156ms step_avg:41.00ms
step:834/1775 train_time:34218ms step_avg:41.03ms
step:835/1775 train_time:34277ms step_avg:41.05ms
step:836/1775 train_time:34339ms step_avg:41.07ms
step:837/1775 train_time:34397ms step_avg:41.10ms
step:838/1775 train_time:34459ms step_avg:41.12ms
step:839/1775 train_time:34518ms step_avg:41.14ms
step:840/1775 train_time:34579ms step_avg:41.17ms
step:841/1775 train_time:34639ms step_avg:41.19ms
step:842/1775 train_time:34699ms step_avg:41.21ms
step:843/1775 train_time:34758ms step_avg:41.23ms
step:844/1775 train_time:34820ms step_avg:41.26ms
step:845/1775 train_time:34879ms step_avg:41.28ms
step:846/1775 train_time:34940ms step_avg:41.30ms
step:847/1775 train_time:34998ms step_avg:41.32ms
step:848/1775 train_time:35060ms step_avg:41.34ms
step:849/1775 train_time:35119ms step_avg:41.37ms
step:850/1775 train_time:35180ms step_avg:41.39ms
step:851/1775 train_time:35240ms step_avg:41.41ms
step:852/1775 train_time:35301ms step_avg:41.43ms
step:853/1775 train_time:35360ms step_avg:41.45ms
step:854/1775 train_time:35421ms step_avg:41.48ms
step:855/1775 train_time:35479ms step_avg:41.50ms
step:856/1775 train_time:35540ms step_avg:41.52ms
step:857/1775 train_time:35599ms step_avg:41.54ms
step:858/1775 train_time:35660ms step_avg:41.56ms
step:859/1775 train_time:35719ms step_avg:41.58ms
step:860/1775 train_time:35779ms step_avg:41.60ms
step:861/1775 train_time:35838ms step_avg:41.62ms
step:862/1775 train_time:35899ms step_avg:41.65ms
step:863/1775 train_time:35957ms step_avg:41.67ms
step:864/1775 train_time:36019ms step_avg:41.69ms
step:865/1775 train_time:36078ms step_avg:41.71ms
step:866/1775 train_time:36139ms step_avg:41.73ms
step:867/1775 train_time:36198ms step_avg:41.75ms
step:868/1775 train_time:36260ms step_avg:41.77ms
step:869/1775 train_time:36318ms step_avg:41.79ms
step:870/1775 train_time:36380ms step_avg:41.82ms
step:871/1775 train_time:36438ms step_avg:41.84ms
step:872/1775 train_time:36500ms step_avg:41.86ms
step:873/1775 train_time:36558ms step_avg:41.88ms
step:874/1775 train_time:36619ms step_avg:41.90ms
step:875/1775 train_time:36678ms step_avg:41.92ms
step:876/1775 train_time:36739ms step_avg:41.94ms
step:877/1775 train_time:36797ms step_avg:41.96ms
step:878/1775 train_time:36858ms step_avg:41.98ms
step:879/1775 train_time:36917ms step_avg:42.00ms
step:880/1775 train_time:36978ms step_avg:42.02ms
step:881/1775 train_time:37037ms step_avg:42.04ms
step:882/1775 train_time:37098ms step_avg:42.06ms
step:883/1775 train_time:37156ms step_avg:42.08ms
step:884/1775 train_time:37219ms step_avg:42.10ms
step:885/1775 train_time:37277ms step_avg:42.12ms
step:886/1775 train_time:37339ms step_avg:42.14ms
step:887/1775 train_time:37398ms step_avg:42.16ms
step:888/1775 train_time:37459ms step_avg:42.18ms
step:889/1775 train_time:37517ms step_avg:42.20ms
step:890/1775 train_time:37578ms step_avg:42.22ms
step:891/1775 train_time:37637ms step_avg:42.24ms
step:892/1775 train_time:37698ms step_avg:42.26ms
step:893/1775 train_time:37757ms step_avg:42.28ms
step:894/1775 train_time:37818ms step_avg:42.30ms
step:895/1775 train_time:37877ms step_avg:42.32ms
step:896/1775 train_time:37938ms step_avg:42.34ms
step:897/1775 train_time:37996ms step_avg:42.36ms
step:898/1775 train_time:38057ms step_avg:42.38ms
step:899/1775 train_time:38116ms step_avg:42.40ms
step:900/1775 train_time:38178ms step_avg:42.42ms
step:901/1775 train_time:38237ms step_avg:42.44ms
step:902/1775 train_time:38299ms step_avg:42.46ms
step:903/1775 train_time:38357ms step_avg:42.48ms
step:904/1775 train_time:38419ms step_avg:42.50ms
step:905/1775 train_time:38478ms step_avg:42.52ms
step:906/1775 train_time:38540ms step_avg:42.54ms
step:907/1775 train_time:38599ms step_avg:42.56ms
step:908/1775 train_time:38660ms step_avg:42.58ms
step:909/1775 train_time:38718ms step_avg:42.59ms
step:910/1775 train_time:38779ms step_avg:42.61ms
step:911/1775 train_time:38838ms step_avg:42.63ms
step:912/1775 train_time:38898ms step_avg:42.65ms
step:913/1775 train_time:38957ms step_avg:42.67ms
step:914/1775 train_time:39019ms step_avg:42.69ms
step:915/1775 train_time:39078ms step_avg:42.71ms
step:916/1775 train_time:39140ms step_avg:42.73ms
step:917/1775 train_time:39198ms step_avg:42.75ms
step:918/1775 train_time:39260ms step_avg:42.77ms
step:919/1775 train_time:39319ms step_avg:42.78ms
step:920/1775 train_time:39380ms step_avg:42.80ms
step:921/1775 train_time:39439ms step_avg:42.82ms
step:922/1775 train_time:39501ms step_avg:42.84ms
step:923/1775 train_time:39560ms step_avg:42.86ms
step:924/1775 train_time:39621ms step_avg:42.88ms
step:925/1775 train_time:39680ms step_avg:42.90ms
step:926/1775 train_time:39741ms step_avg:42.92ms
step:927/1775 train_time:39799ms step_avg:42.93ms
step:928/1775 train_time:39860ms step_avg:42.95ms
step:929/1775 train_time:39919ms step_avg:42.97ms
step:930/1775 train_time:39980ms step_avg:42.99ms
step:931/1775 train_time:40039ms step_avg:43.01ms
step:932/1775 train_time:40100ms step_avg:43.03ms
step:933/1775 train_time:40158ms step_avg:43.04ms
step:934/1775 train_time:40220ms step_avg:43.06ms
step:935/1775 train_time:40278ms step_avg:43.08ms
step:936/1775 train_time:40340ms step_avg:43.10ms
step:937/1775 train_time:40398ms step_avg:43.11ms
step:938/1775 train_time:40460ms step_avg:43.13ms
step:939/1775 train_time:40519ms step_avg:43.15ms
step:940/1775 train_time:40579ms step_avg:43.17ms
step:941/1775 train_time:40638ms step_avg:43.19ms
step:942/1775 train_time:40699ms step_avg:43.21ms
step:943/1775 train_time:40758ms step_avg:43.22ms
step:944/1775 train_time:40820ms step_avg:43.24ms
step:945/1775 train_time:40878ms step_avg:43.26ms
step:946/1775 train_time:40939ms step_avg:43.28ms
step:947/1775 train_time:40998ms step_avg:43.29ms
step:948/1775 train_time:41059ms step_avg:43.31ms
step:949/1775 train_time:41118ms step_avg:43.33ms
step:950/1775 train_time:41178ms step_avg:43.35ms
step:951/1775 train_time:41237ms step_avg:43.36ms
step:952/1775 train_time:41298ms step_avg:43.38ms
step:953/1775 train_time:41356ms step_avg:43.40ms
step:954/1775 train_time:41417ms step_avg:43.41ms
step:955/1775 train_time:41476ms step_avg:43.43ms
step:956/1775 train_time:41538ms step_avg:43.45ms
step:957/1775 train_time:41596ms step_avg:43.47ms
step:958/1775 train_time:41658ms step_avg:43.48ms
step:959/1775 train_time:41716ms step_avg:43.50ms
step:960/1775 train_time:41777ms step_avg:43.52ms
step:961/1775 train_time:41835ms step_avg:43.53ms
step:962/1775 train_time:41896ms step_avg:43.55ms
step:963/1775 train_time:41955ms step_avg:43.57ms
step:964/1775 train_time:42017ms step_avg:43.59ms
step:965/1775 train_time:42076ms step_avg:43.60ms
step:966/1775 train_time:42138ms step_avg:43.62ms
step:967/1775 train_time:42196ms step_avg:43.64ms
step:968/1775 train_time:42258ms step_avg:43.66ms
step:969/1775 train_time:42318ms step_avg:43.67ms
step:970/1775 train_time:42379ms step_avg:43.69ms
step:971/1775 train_time:42437ms step_avg:43.70ms
step:972/1775 train_time:42499ms step_avg:43.72ms
step:973/1775 train_time:42557ms step_avg:43.74ms
step:974/1775 train_time:42618ms step_avg:43.76ms
step:975/1775 train_time:42678ms step_avg:43.77ms
step:976/1775 train_time:42738ms step_avg:43.79ms
step:977/1775 train_time:42797ms step_avg:43.80ms
step:978/1775 train_time:42858ms step_avg:43.82ms
step:979/1775 train_time:42917ms step_avg:43.84ms
step:980/1775 train_time:42978ms step_avg:43.86ms
step:981/1775 train_time:43037ms step_avg:43.87ms
step:982/1775 train_time:43098ms step_avg:43.89ms
step:983/1775 train_time:43157ms step_avg:43.90ms
step:984/1775 train_time:43219ms step_avg:43.92ms
step:985/1775 train_time:43278ms step_avg:43.94ms
step:986/1775 train_time:43338ms step_avg:43.95ms
step:987/1775 train_time:43397ms step_avg:43.97ms
step:988/1775 train_time:43459ms step_avg:43.99ms
step:989/1775 train_time:43517ms step_avg:44.00ms
step:990/1775 train_time:43578ms step_avg:44.02ms
step:991/1775 train_time:43636ms step_avg:44.03ms
step:992/1775 train_time:43697ms step_avg:44.05ms
step:993/1775 train_time:43757ms step_avg:44.07ms
step:994/1775 train_time:43818ms step_avg:44.08ms
step:995/1775 train_time:43876ms step_avg:44.10ms
step:996/1775 train_time:43938ms step_avg:44.11ms
step:997/1775 train_time:43996ms step_avg:44.13ms
step:998/1775 train_time:44057ms step_avg:44.15ms
step:999/1775 train_time:44116ms step_avg:44.16ms
step:1000/1775 train_time:44178ms step_avg:44.18ms
step:1000/1775 val_loss:3.7415 train_time:44248ms step_avg:44.25ms
step:1001/1775 train_time:44267ms step_avg:44.22ms
step:1002/1775 train_time:44298ms step_avg:44.21ms
step:1003/1775 train_time:44359ms step_avg:44.23ms
step:1004/1775 train_time:44421ms step_avg:44.24ms
step:1005/1775 train_time:44479ms step_avg:44.26ms
step:1006/1775 train_time:44540ms step_avg:44.27ms
step:1007/1775 train_time:44598ms step_avg:44.29ms
step:1008/1775 train_time:44659ms step_avg:44.30ms
step:1009/1775 train_time:44718ms step_avg:44.32ms
step:1010/1775 train_time:44778ms step_avg:44.33ms
step:1011/1775 train_time:44837ms step_avg:44.35ms
step:1012/1775 train_time:44897ms step_avg:44.36ms
step:1013/1775 train_time:44955ms step_avg:44.38ms
step:1014/1775 train_time:45016ms step_avg:44.39ms
step:1015/1775 train_time:45074ms step_avg:44.41ms
step:1016/1775 train_time:45135ms step_avg:44.42ms
step:1017/1775 train_time:45195ms step_avg:44.44ms
step:1018/1775 train_time:45256ms step_avg:44.46ms
step:1019/1775 train_time:45316ms step_avg:44.47ms
step:1020/1775 train_time:45378ms step_avg:44.49ms
step:1021/1775 train_time:45436ms step_avg:44.50ms
step:1022/1775 train_time:45497ms step_avg:44.52ms
step:1023/1775 train_time:45555ms step_avg:44.53ms
step:1024/1775 train_time:45616ms step_avg:44.55ms
step:1025/1775 train_time:45674ms step_avg:44.56ms
step:1026/1775 train_time:45735ms step_avg:44.58ms
step:1027/1775 train_time:45793ms step_avg:44.59ms
step:1028/1775 train_time:45854ms step_avg:44.61ms
step:1029/1775 train_time:45913ms step_avg:44.62ms
step:1030/1775 train_time:45973ms step_avg:44.63ms
step:1031/1775 train_time:46032ms step_avg:44.65ms
step:1032/1775 train_time:46092ms step_avg:44.66ms
step:1033/1775 train_time:46152ms step_avg:44.68ms
step:1034/1775 train_time:46213ms step_avg:44.69ms
step:1035/1775 train_time:46273ms step_avg:44.71ms
step:1036/1775 train_time:46334ms step_avg:44.72ms
step:1037/1775 train_time:46393ms step_avg:44.74ms
step:1038/1775 train_time:46454ms step_avg:44.75ms
step:1039/1775 train_time:46512ms step_avg:44.77ms
step:1040/1775 train_time:46573ms step_avg:44.78ms
step:1041/1775 train_time:46632ms step_avg:44.80ms
step:1042/1775 train_time:46693ms step_avg:44.81ms
step:1043/1775 train_time:46751ms step_avg:44.82ms
step:1044/1775 train_time:46812ms step_avg:44.84ms
step:1045/1775 train_time:46870ms step_avg:44.85ms
step:1046/1775 train_time:46931ms step_avg:44.87ms
step:1047/1775 train_time:46989ms step_avg:44.88ms
step:1048/1775 train_time:47050ms step_avg:44.90ms
step:1049/1775 train_time:47108ms step_avg:44.91ms
step:1050/1775 train_time:47170ms step_avg:44.92ms
step:1051/1775 train_time:47229ms step_avg:44.94ms
step:1052/1775 train_time:47291ms step_avg:44.95ms
step:1053/1775 train_time:47350ms step_avg:44.97ms
step:1054/1775 train_time:47412ms step_avg:44.98ms
step:1055/1775 train_time:47472ms step_avg:45.00ms
step:1056/1775 train_time:47533ms step_avg:45.01ms
step:1057/1775 train_time:47591ms step_avg:45.02ms
step:1058/1775 train_time:47653ms step_avg:45.04ms
step:1059/1775 train_time:47711ms step_avg:45.05ms
step:1060/1775 train_time:47772ms step_avg:45.07ms
step:1061/1775 train_time:47830ms step_avg:45.08ms
step:1062/1775 train_time:47891ms step_avg:45.10ms
step:1063/1775 train_time:47949ms step_avg:45.11ms
step:1064/1775 train_time:48010ms step_avg:45.12ms
step:1065/1775 train_time:48068ms step_avg:45.13ms
step:1066/1775 train_time:48129ms step_avg:45.15ms
step:1067/1775 train_time:48189ms step_avg:45.16ms
step:1068/1775 train_time:48251ms step_avg:45.18ms
step:1069/1775 train_time:48310ms step_avg:45.19ms
step:1070/1775 train_time:48371ms step_avg:45.21ms
step:1071/1775 train_time:48431ms step_avg:45.22ms
step:1072/1775 train_time:48492ms step_avg:45.23ms
step:1073/1775 train_time:48551ms step_avg:45.25ms
step:1074/1775 train_time:48611ms step_avg:45.26ms
step:1075/1775 train_time:48670ms step_avg:45.27ms
step:1076/1775 train_time:48731ms step_avg:45.29ms
step:1077/1775 train_time:48789ms step_avg:45.30ms
step:1078/1775 train_time:48850ms step_avg:45.32ms
step:1079/1775 train_time:48908ms step_avg:45.33ms
step:1080/1775 train_time:48969ms step_avg:45.34ms
step:1081/1775 train_time:49027ms step_avg:45.35ms
step:1082/1775 train_time:49088ms step_avg:45.37ms
step:1083/1775 train_time:49147ms step_avg:45.38ms
step:1084/1775 train_time:49208ms step_avg:45.39ms
step:1085/1775 train_time:49267ms step_avg:45.41ms
step:1086/1775 train_time:49329ms step_avg:45.42ms
step:1087/1775 train_time:49388ms step_avg:45.43ms
step:1088/1775 train_time:49450ms step_avg:45.45ms
step:1089/1775 train_time:49509ms step_avg:45.46ms
step:1090/1775 train_time:49571ms step_avg:45.48ms
step:1091/1775 train_time:49629ms step_avg:45.49ms
step:1092/1775 train_time:49690ms step_avg:45.50ms
step:1093/1775 train_time:49748ms step_avg:45.52ms
step:1094/1775 train_time:49809ms step_avg:45.53ms
step:1095/1775 train_time:49868ms step_avg:45.54ms
step:1096/1775 train_time:49928ms step_avg:45.55ms
step:1097/1775 train_time:49986ms step_avg:45.57ms
step:1098/1775 train_time:50048ms step_avg:45.58ms
step:1099/1775 train_time:50107ms step_avg:45.59ms
step:1100/1775 train_time:50168ms step_avg:45.61ms
step:1101/1775 train_time:50227ms step_avg:45.62ms
step:1102/1775 train_time:50289ms step_avg:45.63ms
step:1103/1775 train_time:50348ms step_avg:45.65ms
step:1104/1775 train_time:50409ms step_avg:45.66ms
step:1105/1775 train_time:50468ms step_avg:45.67ms
step:1106/1775 train_time:50529ms step_avg:45.69ms
step:1107/1775 train_time:50588ms step_avg:45.70ms
step:1108/1775 train_time:50650ms step_avg:45.71ms
step:1109/1775 train_time:50709ms step_avg:45.72ms
step:1110/1775 train_time:50769ms step_avg:45.74ms
step:1111/1775 train_time:50828ms step_avg:45.75ms
step:1112/1775 train_time:50888ms step_avg:45.76ms
step:1113/1775 train_time:50948ms step_avg:45.78ms
step:1114/1775 train_time:51009ms step_avg:45.79ms
step:1115/1775 train_time:51068ms step_avg:45.80ms
step:1116/1775 train_time:51128ms step_avg:45.81ms
step:1117/1775 train_time:51188ms step_avg:45.83ms
step:1118/1775 train_time:51250ms step_avg:45.84ms
step:1119/1775 train_time:51308ms step_avg:45.85ms
step:1120/1775 train_time:51370ms step_avg:45.87ms
step:1121/1775 train_time:51429ms step_avg:45.88ms
step:1122/1775 train_time:51491ms step_avg:45.89ms
step:1123/1775 train_time:51550ms step_avg:45.90ms
step:1124/1775 train_time:51611ms step_avg:45.92ms
step:1125/1775 train_time:51670ms step_avg:45.93ms
step:1126/1775 train_time:51731ms step_avg:45.94ms
step:1127/1775 train_time:51790ms step_avg:45.95ms
step:1128/1775 train_time:51851ms step_avg:45.97ms
step:1129/1775 train_time:51909ms step_avg:45.98ms
step:1130/1775 train_time:51970ms step_avg:45.99ms
step:1131/1775 train_time:52029ms step_avg:46.00ms
step:1132/1775 train_time:52089ms step_avg:46.02ms
step:1133/1775 train_time:52149ms step_avg:46.03ms
step:1134/1775 train_time:52211ms step_avg:46.04ms
step:1135/1775 train_time:52269ms step_avg:46.05ms
step:1136/1775 train_time:52331ms step_avg:46.07ms
step:1137/1775 train_time:52390ms step_avg:46.08ms
step:1138/1775 train_time:52451ms step_avg:46.09ms
step:1139/1775 train_time:52510ms step_avg:46.10ms
step:1140/1775 train_time:52571ms step_avg:46.11ms
step:1141/1775 train_time:52629ms step_avg:46.13ms
step:1142/1775 train_time:52690ms step_avg:46.14ms
step:1143/1775 train_time:52748ms step_avg:46.15ms
step:1144/1775 train_time:52810ms step_avg:46.16ms
step:1145/1775 train_time:52868ms step_avg:46.17ms
step:1146/1775 train_time:52929ms step_avg:46.19ms
step:1147/1775 train_time:52988ms step_avg:46.20ms
step:1148/1775 train_time:53050ms step_avg:46.21ms
step:1149/1775 train_time:53109ms step_avg:46.22ms
step:1150/1775 train_time:53169ms step_avg:46.23ms
step:1151/1775 train_time:53227ms step_avg:46.24ms
step:1152/1775 train_time:53288ms step_avg:46.26ms
step:1153/1775 train_time:53347ms step_avg:46.27ms
step:1154/1775 train_time:53408ms step_avg:46.28ms
step:1155/1775 train_time:53466ms step_avg:46.29ms
step:1156/1775 train_time:53529ms step_avg:46.31ms
step:1157/1775 train_time:53587ms step_avg:46.32ms
step:1158/1775 train_time:53652ms step_avg:46.33ms
step:1159/1775 train_time:53735ms step_avg:46.36ms
step:1160/1775 train_time:53824ms step_avg:46.40ms
step:1161/1775 train_time:53908ms step_avg:46.43ms
step:1162/1775 train_time:53994ms step_avg:46.47ms
step:1163/1775 train_time:54079ms step_avg:46.50ms
step:1164/1775 train_time:54168ms step_avg:46.54ms
step:1165/1775 train_time:54252ms step_avg:46.57ms
step:1166/1775 train_time:54339ms step_avg:46.60ms
step:1167/1775 train_time:54425ms step_avg:46.64ms
step:1168/1775 train_time:54511ms step_avg:46.67ms
step:1169/1775 train_time:54596ms step_avg:46.70ms
step:1170/1775 train_time:54684ms step_avg:46.74ms
step:1171/1775 train_time:54768ms step_avg:46.77ms
step:1172/1775 train_time:54855ms step_avg:46.80ms
step:1173/1775 train_time:54940ms step_avg:46.84ms
step:1174/1775 train_time:55028ms step_avg:46.87ms
step:1175/1775 train_time:55112ms step_avg:46.90ms
step:1176/1775 train_time:55199ms step_avg:46.94ms
step:1177/1775 train_time:55283ms step_avg:46.97ms
step:1178/1775 train_time:55371ms step_avg:47.00ms
step:1179/1775 train_time:55455ms step_avg:47.04ms
step:1180/1775 train_time:55542ms step_avg:47.07ms
step:1181/1775 train_time:55627ms step_avg:47.10ms
step:1182/1775 train_time:55712ms step_avg:47.13ms
step:1183/1775 train_time:55796ms step_avg:47.17ms
step:1184/1775 train_time:55884ms step_avg:47.20ms
step:1185/1775 train_time:55968ms step_avg:47.23ms
step:1186/1775 train_time:56054ms step_avg:47.26ms
step:1187/1775 train_time:56139ms step_avg:47.29ms
step:1188/1775 train_time:56227ms step_avg:47.33ms
step:1189/1775 train_time:56311ms step_avg:47.36ms
step:1190/1775 train_time:56398ms step_avg:47.39ms
step:1191/1775 train_time:56482ms step_avg:47.42ms
step:1192/1775 train_time:56570ms step_avg:47.46ms
step:1193/1775 train_time:56653ms step_avg:47.49ms
step:1194/1775 train_time:56741ms step_avg:47.52ms
step:1195/1775 train_time:56825ms step_avg:47.55ms
step:1196/1775 train_time:56911ms step_avg:47.58ms
step:1197/1775 train_time:56996ms step_avg:47.62ms
step:1198/1775 train_time:57084ms step_avg:47.65ms
step:1199/1775 train_time:57168ms step_avg:47.68ms
step:1200/1775 train_time:57255ms step_avg:47.71ms
step:1201/1775 train_time:57340ms step_avg:47.74ms
step:1202/1775 train_time:57428ms step_avg:47.78ms
step:1203/1775 train_time:57512ms step_avg:47.81ms
step:1204/1775 train_time:57599ms step_avg:47.84ms
step:1205/1775 train_time:57683ms step_avg:47.87ms
step:1206/1775 train_time:57770ms step_avg:47.90ms
step:1207/1775 train_time:57855ms step_avg:47.93ms
step:1208/1775 train_time:57942ms step_avg:47.97ms
step:1209/1775 train_time:58026ms step_avg:48.00ms
step:1210/1775 train_time:58112ms step_avg:48.03ms
step:1211/1775 train_time:58198ms step_avg:48.06ms
step:1212/1775 train_time:58286ms step_avg:48.09ms
step:1213/1775 train_time:58370ms step_avg:48.12ms
step:1214/1775 train_time:58457ms step_avg:48.15ms
step:1215/1775 train_time:58542ms step_avg:48.18ms
step:1216/1775 train_time:58630ms step_avg:48.22ms
step:1217/1775 train_time:58715ms step_avg:48.25ms
step:1218/1775 train_time:58801ms step_avg:48.28ms
step:1219/1775 train_time:58886ms step_avg:48.31ms
step:1220/1775 train_time:58973ms step_avg:48.34ms
step:1221/1775 train_time:59057ms step_avg:48.37ms
step:1222/1775 train_time:59145ms step_avg:48.40ms
step:1223/1775 train_time:59229ms step_avg:48.43ms
step:1224/1775 train_time:59316ms step_avg:48.46ms
step:1225/1775 train_time:59402ms step_avg:48.49ms
step:1226/1775 train_time:59489ms step_avg:48.52ms
step:1227/1775 train_time:59573ms step_avg:48.55ms
step:1228/1775 train_time:59660ms step_avg:48.58ms
step:1229/1775 train_time:59745ms step_avg:48.61ms
step:1230/1775 train_time:59831ms step_avg:48.64ms
step:1231/1775 train_time:59915ms step_avg:48.67ms
step:1232/1775 train_time:60004ms step_avg:48.70ms
step:1233/1775 train_time:60089ms step_avg:48.73ms
step:1234/1775 train_time:60176ms step_avg:48.76ms
step:1235/1775 train_time:60260ms step_avg:48.79ms
step:1236/1775 train_time:60347ms step_avg:48.82ms
step:1237/1775 train_time:60431ms step_avg:48.85ms
step:1238/1775 train_time:60519ms step_avg:48.88ms
step:1239/1775 train_time:60604ms step_avg:48.91ms
step:1240/1775 train_time:60690ms step_avg:48.94ms
step:1241/1775 train_time:60775ms step_avg:48.97ms
step:1242/1775 train_time:60862ms step_avg:49.00ms
step:1243/1775 train_time:60946ms step_avg:49.03ms
step:1244/1775 train_time:61032ms step_avg:49.06ms
step:1245/1775 train_time:61117ms step_avg:49.09ms
step:1246/1775 train_time:61207ms step_avg:49.12ms
step:1247/1775 train_time:61290ms step_avg:49.15ms
step:1248/1775 train_time:61378ms step_avg:49.18ms
step:1249/1775 train_time:61463ms step_avg:49.21ms
step:1250/1775 train_time:61549ms step_avg:49.24ms
step:1250/1775 val_loss:3.5108 train_time:61648ms step_avg:49.32ms
step:1251/1775 train_time:61669ms step_avg:49.30ms
step:1252/1775 train_time:61724ms step_avg:49.30ms
step:1253/1775 train_time:61809ms step_avg:49.33ms
step:1254/1775 train_time:61898ms step_avg:49.36ms
step:1255/1775 train_time:61983ms step_avg:49.39ms
step:1256/1775 train_time:62069ms step_avg:49.42ms
step:1257/1775 train_time:62152ms step_avg:49.44ms
step:1258/1775 train_time:62239ms step_avg:49.47ms
step:1259/1775 train_time:62323ms step_avg:49.50ms
step:1260/1775 train_time:62409ms step_avg:49.53ms
step:1261/1775 train_time:62492ms step_avg:49.56ms
step:1262/1775 train_time:62580ms step_avg:49.59ms
step:1263/1775 train_time:62668ms step_avg:49.62ms
step:1264/1775 train_time:62756ms step_avg:49.65ms
step:1265/1775 train_time:62841ms step_avg:49.68ms
step:1266/1775 train_time:62930ms step_avg:49.71ms
step:1267/1775 train_time:63015ms step_avg:49.74ms
step:1268/1775 train_time:63101ms step_avg:49.76ms
step:1269/1775 train_time:63185ms step_avg:49.79ms
step:1270/1775 train_time:63271ms step_avg:49.82ms
step:1271/1775 train_time:63357ms step_avg:49.85ms
step:1272/1775 train_time:63443ms step_avg:49.88ms
step:1273/1775 train_time:63528ms step_avg:49.90ms
step:1274/1775 train_time:63616ms step_avg:49.93ms
step:1275/1775 train_time:63701ms step_avg:49.96ms
step:1276/1775 train_time:63790ms step_avg:49.99ms
step:1277/1775 train_time:63874ms step_avg:50.02ms
step:1278/1775 train_time:63962ms step_avg:50.05ms
step:1279/1775 train_time:64047ms step_avg:50.08ms
step:1280/1775 train_time:64132ms step_avg:50.10ms
step:1281/1775 train_time:64216ms step_avg:50.13ms
step:1282/1775 train_time:64303ms step_avg:50.16ms
step:1283/1775 train_time:64387ms step_avg:50.18ms
step:1284/1775 train_time:64473ms step_avg:50.21ms
step:1285/1775 train_time:64557ms step_avg:50.24ms
step:1286/1775 train_time:64646ms step_avg:50.27ms
step:1287/1775 train_time:64731ms step_avg:50.30ms
step:1288/1775 train_time:64819ms step_avg:50.33ms
step:1289/1775 train_time:64904ms step_avg:50.35ms
step:1290/1775 train_time:64991ms step_avg:50.38ms
step:1291/1775 train_time:65075ms step_avg:50.41ms
step:1292/1775 train_time:65162ms step_avg:50.43ms
step:1293/1775 train_time:65247ms step_avg:50.46ms
step:1294/1775 train_time:65333ms step_avg:50.49ms
step:1295/1775 train_time:65418ms step_avg:50.52ms
step:1296/1775 train_time:65504ms step_avg:50.54ms
step:1297/1775 train_time:65590ms step_avg:50.57ms
step:1298/1775 train_time:65678ms step_avg:50.60ms
step:1299/1775 train_time:65764ms step_avg:50.63ms
step:1300/1775 train_time:65851ms step_avg:50.65ms
step:1301/1775 train_time:65934ms step_avg:50.68ms
step:1302/1775 train_time:66021ms step_avg:50.71ms
step:1303/1775 train_time:66106ms step_avg:50.73ms
step:1304/1775 train_time:66192ms step_avg:50.76ms
step:1305/1775 train_time:66276ms step_avg:50.79ms
step:1306/1775 train_time:66364ms step_avg:50.81ms
step:1307/1775 train_time:66449ms step_avg:50.84ms
step:1308/1775 train_time:66536ms step_avg:50.87ms
step:1309/1775 train_time:66621ms step_avg:50.89ms
step:1310/1775 train_time:66710ms step_avg:50.92ms
step:1311/1775 train_time:66795ms step_avg:50.95ms
step:1312/1775 train_time:66884ms step_avg:50.98ms
step:1313/1775 train_time:66969ms step_avg:51.00ms
step:1314/1775 train_time:67055ms step_avg:51.03ms
step:1315/1775 train_time:67139ms step_avg:51.06ms
step:1316/1775 train_time:67226ms step_avg:51.08ms
step:1317/1775 train_time:67311ms step_avg:51.11ms
step:1318/1775 train_time:67398ms step_avg:51.14ms
step:1319/1775 train_time:67483ms step_avg:51.16ms
step:1320/1775 train_time:67570ms step_avg:51.19ms
step:1321/1775 train_time:67655ms step_avg:51.22ms
step:1322/1775 train_time:67744ms step_avg:51.24ms
step:1323/1775 train_time:67829ms step_avg:51.27ms
step:1324/1775 train_time:67916ms step_avg:51.30ms
step:1325/1775 train_time:68000ms step_avg:51.32ms
step:1326/1775 train_time:68089ms step_avg:51.35ms
step:1327/1775 train_time:68172ms step_avg:51.37ms
step:1328/1775 train_time:68259ms step_avg:51.40ms
step:1329/1775 train_time:68344ms step_avg:51.42ms
step:1330/1775 train_time:68431ms step_avg:51.45ms
step:1331/1775 train_time:68515ms step_avg:51.48ms
step:1332/1775 train_time:68604ms step_avg:51.50ms
step:1333/1775 train_time:68689ms step_avg:51.53ms
step:1334/1775 train_time:68775ms step_avg:51.56ms
step:1335/1775 train_time:68859ms step_avg:51.58ms
step:1336/1775 train_time:68946ms step_avg:51.61ms
step:1337/1775 train_time:69030ms step_avg:51.63ms
step:1338/1775 train_time:69116ms step_avg:51.66ms
step:1339/1775 train_time:69200ms step_avg:51.68ms
step:1340/1775 train_time:69289ms step_avg:51.71ms
step:1341/1775 train_time:69372ms step_avg:51.73ms
step:1342/1775 train_time:69459ms step_avg:51.76ms
step:1343/1775 train_time:69543ms step_avg:51.78ms
step:1344/1775 train_time:69630ms step_avg:51.81ms
step:1345/1775 train_time:69715ms step_avg:51.83ms
step:1346/1775 train_time:69804ms step_avg:51.86ms
step:1347/1775 train_time:69889ms step_avg:51.88ms
step:1348/1775 train_time:69975ms step_avg:51.91ms
step:1349/1775 train_time:70060ms step_avg:51.93ms
step:1350/1775 train_time:70148ms step_avg:51.96ms
step:1351/1775 train_time:70231ms step_avg:51.98ms
step:1352/1775 train_time:70317ms step_avg:52.01ms
step:1353/1775 train_time:70402ms step_avg:52.03ms
step:1354/1775 train_time:70490ms step_avg:52.06ms
step:1355/1775 train_time:70574ms step_avg:52.08ms
step:1356/1775 train_time:70661ms step_avg:52.11ms
step:1357/1775 train_time:70745ms step_avg:52.13ms
step:1358/1775 train_time:70833ms step_avg:52.16ms
step:1359/1775 train_time:70918ms step_avg:52.18ms
step:1360/1775 train_time:71005ms step_avg:52.21ms
step:1361/1775 train_time:71090ms step_avg:52.23ms
step:1362/1775 train_time:71177ms step_avg:52.26ms
step:1363/1775 train_time:71261ms step_avg:52.28ms
step:1364/1775 train_time:71348ms step_avg:52.31ms
step:1365/1775 train_time:71433ms step_avg:52.33ms
step:1366/1775 train_time:71520ms step_avg:52.36ms
step:1367/1775 train_time:71605ms step_avg:52.38ms
step:1368/1775 train_time:71692ms step_avg:52.41ms
step:1369/1775 train_time:71776ms step_avg:52.43ms
step:1370/1775 train_time:71864ms step_avg:52.46ms
step:1371/1775 train_time:71949ms step_avg:52.48ms
step:1372/1775 train_time:72035ms step_avg:52.50ms
step:1373/1775 train_time:72120ms step_avg:52.53ms
step:1374/1775 train_time:72208ms step_avg:52.55ms
step:1375/1775 train_time:72292ms step_avg:52.58ms
step:1376/1775 train_time:72380ms step_avg:52.60ms
step:1377/1775 train_time:72465ms step_avg:52.63ms
step:1378/1775 train_time:72553ms step_avg:52.65ms
step:1379/1775 train_time:72638ms step_avg:52.67ms
step:1380/1775 train_time:72726ms step_avg:52.70ms
step:1381/1775 train_time:72810ms step_avg:52.72ms
step:1382/1775 train_time:72896ms step_avg:52.75ms
step:1383/1775 train_time:72981ms step_avg:52.77ms
step:1384/1775 train_time:73068ms step_avg:52.79ms
step:1385/1775 train_time:73153ms step_avg:52.82ms
step:1386/1775 train_time:73240ms step_avg:52.84ms
step:1387/1775 train_time:73324ms step_avg:52.87ms
step:1388/1775 train_time:73411ms step_avg:52.89ms
step:1389/1775 train_time:73496ms step_avg:52.91ms
step:1390/1775 train_time:73584ms step_avg:52.94ms
step:1391/1775 train_time:73668ms step_avg:52.96ms
step:1392/1775 train_time:73754ms step_avg:52.98ms
step:1393/1775 train_time:73838ms step_avg:53.01ms
step:1394/1775 train_time:73926ms step_avg:53.03ms
step:1395/1775 train_time:74010ms step_avg:53.05ms
step:1396/1775 train_time:74096ms step_avg:53.08ms
step:1397/1775 train_time:74180ms step_avg:53.10ms
step:1398/1775 train_time:74268ms step_avg:53.12ms
step:1399/1775 train_time:74352ms step_avg:53.15ms
step:1400/1775 train_time:74440ms step_avg:53.17ms
step:1401/1775 train_time:74526ms step_avg:53.19ms
step:1402/1775 train_time:74612ms step_avg:53.22ms
step:1403/1775 train_time:74697ms step_avg:53.24ms
step:1404/1775 train_time:74787ms step_avg:53.27ms
step:1405/1775 train_time:74869ms step_avg:53.29ms
step:1406/1775 train_time:74956ms step_avg:53.31ms
step:1407/1775 train_time:75041ms step_avg:53.33ms
step:1408/1775 train_time:75130ms step_avg:53.36ms
step:1409/1775 train_time:75214ms step_avg:53.38ms
step:1410/1775 train_time:75301ms step_avg:53.40ms
step:1411/1775 train_time:75386ms step_avg:53.43ms
step:1412/1775 train_time:75473ms step_avg:53.45ms
step:1413/1775 train_time:75556ms step_avg:53.47ms
step:1414/1775 train_time:75644ms step_avg:53.50ms
step:1415/1775 train_time:75729ms step_avg:53.52ms
step:1416/1775 train_time:75814ms step_avg:53.54ms
step:1417/1775 train_time:75898ms step_avg:53.56ms
step:1418/1775 train_time:75988ms step_avg:53.59ms
step:1419/1775 train_time:76071ms step_avg:53.61ms
step:1420/1775 train_time:76158ms step_avg:53.63ms
step:1421/1775 train_time:76242ms step_avg:53.65ms
step:1422/1775 train_time:76330ms step_avg:53.68ms
step:1423/1775 train_time:76414ms step_avg:53.70ms
step:1424/1775 train_time:76502ms step_avg:53.72ms
step:1425/1775 train_time:76587ms step_avg:53.75ms
step:1426/1775 train_time:76674ms step_avg:53.77ms
step:1427/1775 train_time:76758ms step_avg:53.79ms
step:1428/1775 train_time:76844ms step_avg:53.81ms
step:1429/1775 train_time:76928ms step_avg:53.83ms
step:1430/1775 train_time:77015ms step_avg:53.86ms
step:1431/1775 train_time:77100ms step_avg:53.88ms
step:1432/1775 train_time:77188ms step_avg:53.90ms
step:1433/1775 train_time:77272ms step_avg:53.92ms
step:1434/1775 train_time:77360ms step_avg:53.95ms
step:1435/1775 train_time:77445ms step_avg:53.97ms
step:1436/1775 train_time:77532ms step_avg:53.99ms
step:1437/1775 train_time:77616ms step_avg:54.01ms
step:1438/1775 train_time:77704ms step_avg:54.04ms
step:1439/1775 train_time:77789ms step_avg:54.06ms
step:1440/1775 train_time:77875ms step_avg:54.08ms
step:1441/1775 train_time:77960ms step_avg:54.10ms
step:1442/1775 train_time:78049ms step_avg:54.13ms
step:1443/1775 train_time:78133ms step_avg:54.15ms
step:1444/1775 train_time:78220ms step_avg:54.17ms
step:1445/1775 train_time:78305ms step_avg:54.19ms
step:1446/1775 train_time:78393ms step_avg:54.21ms
step:1447/1775 train_time:78478ms step_avg:54.23ms
step:1448/1775 train_time:78566ms step_avg:54.26ms
step:1449/1775 train_time:78650ms step_avg:54.28ms
step:1450/1775 train_time:78736ms step_avg:54.30ms
step:1451/1775 train_time:78821ms step_avg:54.32ms
step:1452/1775 train_time:78909ms step_avg:54.34ms
step:1453/1775 train_time:78993ms step_avg:54.37ms
step:1454/1775 train_time:79080ms step_avg:54.39ms
step:1455/1775 train_time:79164ms step_avg:54.41ms
step:1456/1775 train_time:79252ms step_avg:54.43ms
step:1457/1775 train_time:79336ms step_avg:54.45ms
step:1458/1775 train_time:79423ms step_avg:54.47ms
step:1459/1775 train_time:79508ms step_avg:54.49ms
step:1460/1775 train_time:79594ms step_avg:54.52ms
step:1461/1775 train_time:79679ms step_avg:54.54ms
step:1462/1775 train_time:79766ms step_avg:54.56ms
step:1463/1775 train_time:79851ms step_avg:54.58ms
step:1464/1775 train_time:79937ms step_avg:54.60ms
step:1465/1775 train_time:80023ms step_avg:54.62ms
step:1466/1775 train_time:80111ms step_avg:54.65ms
step:1467/1775 train_time:80195ms step_avg:54.67ms
step:1468/1775 train_time:80282ms step_avg:54.69ms
step:1469/1775 train_time:80366ms step_avg:54.71ms
step:1470/1775 train_time:80452ms step_avg:54.73ms
step:1471/1775 train_time:80536ms step_avg:54.75ms
step:1472/1775 train_time:80624ms step_avg:54.77ms
step:1473/1775 train_time:80709ms step_avg:54.79ms
step:1474/1775 train_time:80795ms step_avg:54.81ms
step:1475/1775 train_time:80880ms step_avg:54.83ms
step:1476/1775 train_time:80967ms step_avg:54.86ms
step:1477/1775 train_time:81051ms step_avg:54.88ms
step:1478/1775 train_time:81137ms step_avg:54.90ms
step:1479/1775 train_time:81221ms step_avg:54.92ms
step:1480/1775 train_time:81310ms step_avg:54.94ms
step:1481/1775 train_time:81394ms step_avg:54.96ms
step:1482/1775 train_time:81482ms step_avg:54.98ms
step:1483/1775 train_time:81566ms step_avg:55.00ms
step:1484/1775 train_time:81652ms step_avg:55.02ms
step:1485/1775 train_time:81736ms step_avg:55.04ms
step:1486/1775 train_time:81824ms step_avg:55.06ms
step:1487/1775 train_time:81909ms step_avg:55.08ms
step:1488/1775 train_time:81995ms step_avg:55.10ms
step:1489/1775 train_time:82080ms step_avg:55.12ms
step:1490/1775 train_time:82167ms step_avg:55.15ms
step:1491/1775 train_time:82251ms step_avg:55.16ms
step:1492/1775 train_time:82338ms step_avg:55.19ms
step:1493/1775 train_time:82422ms step_avg:55.21ms
step:1494/1775 train_time:82510ms step_avg:55.23ms
step:1495/1775 train_time:82595ms step_avg:55.25ms
step:1496/1775 train_time:82683ms step_avg:55.27ms
step:1497/1775 train_time:82767ms step_avg:55.29ms
step:1498/1775 train_time:82853ms step_avg:55.31ms
step:1499/1775 train_time:82939ms step_avg:55.33ms
step:1500/1775 train_time:83027ms step_avg:55.35ms
step:1500/1775 val_loss:3.3784 train_time:83127ms step_avg:55.42ms
step:1501/1775 train_time:83145ms step_avg:55.39ms
step:1502/1775 train_time:83199ms step_avg:55.39ms
step:1503/1775 train_time:83286ms step_avg:55.41ms
step:1504/1775 train_time:83373ms step_avg:55.43ms
step:1505/1775 train_time:83457ms step_avg:55.45ms
step:1506/1775 train_time:83544ms step_avg:55.47ms
step:1507/1775 train_time:83628ms step_avg:55.49ms
step:1508/1775 train_time:83714ms step_avg:55.51ms
step:1509/1775 train_time:83798ms step_avg:55.53ms
step:1510/1775 train_time:83886ms step_avg:55.55ms
step:1511/1775 train_time:83970ms step_avg:55.57ms
step:1512/1775 train_time:84059ms step_avg:55.59ms
step:1513/1775 train_time:84147ms step_avg:55.62ms
step:1514/1775 train_time:84233ms step_avg:55.64ms
step:1515/1775 train_time:84320ms step_avg:55.66ms
step:1516/1775 train_time:84408ms step_avg:55.68ms
step:1517/1775 train_time:84491ms step_avg:55.70ms
step:1518/1775 train_time:84577ms step_avg:55.72ms
step:1519/1775 train_time:84660ms step_avg:55.73ms
step:1520/1775 train_time:84748ms step_avg:55.76ms
step:1521/1775 train_time:84830ms step_avg:55.77ms
step:1522/1775 train_time:84920ms step_avg:55.79ms
step:1523/1775 train_time:85005ms step_avg:55.81ms
step:1524/1775 train_time:85091ms step_avg:55.83ms
step:1525/1775 train_time:85177ms step_avg:55.85ms
step:1526/1775 train_time:85266ms step_avg:55.88ms
step:1527/1775 train_time:85351ms step_avg:55.89ms
step:1528/1775 train_time:85437ms step_avg:55.91ms
step:1529/1775 train_time:85522ms step_avg:55.93ms
step:1530/1775 train_time:85609ms step_avg:55.95ms
step:1531/1775 train_time:85692ms step_avg:55.97ms
step:1532/1775 train_time:85778ms step_avg:55.99ms
step:1533/1775 train_time:85861ms step_avg:56.01ms
step:1534/1775 train_time:85949ms step_avg:56.03ms
step:1535/1775 train_time:86034ms step_avg:56.05ms
step:1536/1775 train_time:86122ms step_avg:56.07ms
step:1537/1775 train_time:86207ms step_avg:56.09ms
step:1538/1775 train_time:86294ms step_avg:56.11ms
step:1539/1775 train_time:86379ms step_avg:56.13ms
step:1540/1775 train_time:86466ms step_avg:56.15ms
step:1541/1775 train_time:86550ms step_avg:56.16ms
step:1542/1775 train_time:86637ms step_avg:56.19ms
step:1543/1775 train_time:86722ms step_avg:56.20ms
step:1544/1775 train_time:86809ms step_avg:56.22ms
step:1545/1775 train_time:86892ms step_avg:56.24ms
step:1546/1775 train_time:86980ms step_avg:56.26ms
step:1547/1775 train_time:87064ms step_avg:56.28ms
step:1548/1775 train_time:87152ms step_avg:56.30ms
step:1549/1775 train_time:87237ms step_avg:56.32ms
step:1550/1775 train_time:87324ms step_avg:56.34ms
step:1551/1775 train_time:87409ms step_avg:56.36ms
step:1552/1775 train_time:87495ms step_avg:56.38ms
step:1553/1775 train_time:87580ms step_avg:56.39ms
step:1554/1775 train_time:87666ms step_avg:56.41ms
step:1555/1775 train_time:87751ms step_avg:56.43ms
step:1556/1775 train_time:87838ms step_avg:56.45ms
step:1557/1775 train_time:87922ms step_avg:56.47ms
step:1558/1775 train_time:88009ms step_avg:56.49ms
step:1559/1775 train_time:88092ms step_avg:56.51ms
step:1560/1775 train_time:88180ms step_avg:56.53ms
step:1561/1775 train_time:88264ms step_avg:56.54ms
step:1562/1775 train_time:88351ms step_avg:56.56ms
step:1563/1775 train_time:88436ms step_avg:56.58ms
step:1564/1775 train_time:88523ms step_avg:56.60ms
step:1565/1775 train_time:88607ms step_avg:56.62ms
step:1566/1775 train_time:88693ms step_avg:56.64ms
step:1567/1775 train_time:88777ms step_avg:56.65ms
step:1568/1775 train_time:88866ms step_avg:56.67ms
step:1569/1775 train_time:88950ms step_avg:56.69ms
step:1570/1775 train_time:89037ms step_avg:56.71ms
step:1571/1775 train_time:89122ms step_avg:56.73ms
step:1572/1775 train_time:89210ms step_avg:56.75ms
step:1573/1775 train_time:89293ms step_avg:56.77ms
step:1574/1775 train_time:89381ms step_avg:56.79ms
step:1575/1775 train_time:89464ms step_avg:56.80ms
step:1576/1775 train_time:89551ms step_avg:56.82ms
step:1577/1775 train_time:89636ms step_avg:56.84ms
step:1578/1775 train_time:89724ms step_avg:56.86ms
step:1579/1775 train_time:89808ms step_avg:56.88ms
step:1580/1775 train_time:89894ms step_avg:56.90ms
step:1581/1775 train_time:89980ms step_avg:56.91ms
step:1582/1775 train_time:90067ms step_avg:56.93ms
step:1583/1775 train_time:90151ms step_avg:56.95ms
step:1584/1775 train_time:90239ms step_avg:56.97ms
step:1585/1775 train_time:90324ms step_avg:56.99ms
step:1586/1775 train_time:90411ms step_avg:57.01ms
step:1587/1775 train_time:90496ms step_avg:57.02ms
step:1588/1775 train_time:90584ms step_avg:57.04ms
step:1589/1775 train_time:90669ms step_avg:57.06ms
step:1590/1775 train_time:90755ms step_avg:57.08ms
step:1591/1775 train_time:90839ms step_avg:57.10ms
step:1592/1775 train_time:90927ms step_avg:57.12ms
step:1593/1775 train_time:91012ms step_avg:57.13ms
step:1594/1775 train_time:91099ms step_avg:57.15ms
step:1595/1775 train_time:91184ms step_avg:57.17ms
step:1596/1775 train_time:91271ms step_avg:57.19ms
step:1597/1775 train_time:91357ms step_avg:57.21ms
step:1598/1775 train_time:91443ms step_avg:57.22ms
step:1599/1775 train_time:91527ms step_avg:57.24ms
step:1600/1775 train_time:91614ms step_avg:57.26ms
step:1601/1775 train_time:91698ms step_avg:57.28ms
step:1602/1775 train_time:91786ms step_avg:57.29ms
step:1603/1775 train_time:91870ms step_avg:57.31ms
step:1604/1775 train_time:91958ms step_avg:57.33ms
step:1605/1775 train_time:92043ms step_avg:57.35ms
step:1606/1775 train_time:92129ms step_avg:57.37ms
step:1607/1775 train_time:92213ms step_avg:57.38ms
step:1608/1775 train_time:92300ms step_avg:57.40ms
step:1609/1775 train_time:92384ms step_avg:57.42ms
step:1610/1775 train_time:92472ms step_avg:57.44ms
step:1611/1775 train_time:92557ms step_avg:57.45ms
step:1612/1775 train_time:92646ms step_avg:57.47ms
step:1613/1775 train_time:92730ms step_avg:57.49ms
step:1614/1775 train_time:92816ms step_avg:57.51ms
step:1615/1775 train_time:92901ms step_avg:57.52ms
step:1616/1775 train_time:92989ms step_avg:57.54ms
step:1617/1775 train_time:93073ms step_avg:57.56ms
step:1618/1775 train_time:93160ms step_avg:57.58ms
step:1619/1775 train_time:93245ms step_avg:57.59ms
step:1620/1775 train_time:93331ms step_avg:57.61ms
step:1621/1775 train_time:93416ms step_avg:57.63ms
step:1622/1775 train_time:93503ms step_avg:57.65ms
step:1623/1775 train_time:93587ms step_avg:57.66ms
step:1624/1775 train_time:93673ms step_avg:57.68ms
step:1625/1775 train_time:93758ms step_avg:57.70ms
step:1626/1775 train_time:93847ms step_avg:57.72ms
step:1627/1775 train_time:93930ms step_avg:57.73ms
step:1628/1775 train_time:94020ms step_avg:57.75ms
step:1629/1775 train_time:94105ms step_avg:57.77ms
step:1630/1775 train_time:94191ms step_avg:57.79ms
step:1631/1775 train_time:94276ms step_avg:57.80ms
step:1632/1775 train_time:94364ms step_avg:57.82ms
step:1633/1775 train_time:94448ms step_avg:57.84ms
step:1634/1775 train_time:94535ms step_avg:57.85ms
step:1635/1775 train_time:94619ms step_avg:57.87ms
step:1636/1775 train_time:94708ms step_avg:57.89ms
step:1637/1775 train_time:94792ms step_avg:57.91ms
step:1638/1775 train_time:94878ms step_avg:57.92ms
step:1639/1775 train_time:94963ms step_avg:57.94ms
step:1640/1775 train_time:95050ms step_avg:57.96ms
step:1641/1775 train_time:95135ms step_avg:57.97ms
step:1642/1775 train_time:95224ms step_avg:57.99ms
step:1643/1775 train_time:95309ms step_avg:58.01ms
step:1644/1775 train_time:95395ms step_avg:58.03ms
step:1645/1775 train_time:95479ms step_avg:58.04ms
step:1646/1775 train_time:95567ms step_avg:58.06ms
step:1647/1775 train_time:95650ms step_avg:58.08ms
step:1648/1775 train_time:95738ms step_avg:58.09ms
step:1649/1775 train_time:95823ms step_avg:58.11ms
step:1650/1775 train_time:95910ms step_avg:58.13ms
step:1651/1775 train_time:95996ms step_avg:58.14ms
step:1652/1775 train_time:96085ms step_avg:58.16ms
step:1653/1775 train_time:96168ms step_avg:58.18ms
step:1654/1775 train_time:96256ms step_avg:58.20ms
step:1655/1775 train_time:96342ms step_avg:58.21ms
step:1656/1775 train_time:96428ms step_avg:58.23ms
step:1657/1775 train_time:96512ms step_avg:58.25ms
step:1658/1775 train_time:96599ms step_avg:58.26ms
step:1659/1775 train_time:96683ms step_avg:58.28ms
step:1660/1775 train_time:96771ms step_avg:58.30ms
step:1661/1775 train_time:96855ms step_avg:58.31ms
step:1662/1775 train_time:96943ms step_avg:58.33ms
step:1663/1775 train_time:97027ms step_avg:58.34ms
step:1664/1775 train_time:97114ms step_avg:58.36ms
step:1665/1775 train_time:97198ms step_avg:58.38ms
step:1666/1775 train_time:97287ms step_avg:58.40ms
step:1667/1775 train_time:97371ms step_avg:58.41ms
step:1668/1775 train_time:97459ms step_avg:58.43ms
step:1669/1775 train_time:97544ms step_avg:58.44ms
step:1670/1775 train_time:97631ms step_avg:58.46ms
step:1671/1775 train_time:97715ms step_avg:58.48ms
step:1672/1775 train_time:97802ms step_avg:58.49ms
step:1673/1775 train_time:97886ms step_avg:58.51ms
step:1674/1775 train_time:97972ms step_avg:58.53ms
step:1675/1775 train_time:98057ms step_avg:58.54ms
step:1676/1775 train_time:98145ms step_avg:58.56ms
step:1677/1775 train_time:98228ms step_avg:58.57ms
step:1678/1775 train_time:98315ms step_avg:58.59ms
step:1679/1775 train_time:98399ms step_avg:58.61ms
step:1680/1775 train_time:98487ms step_avg:58.62ms
step:1681/1775 train_time:98572ms step_avg:58.64ms
step:1682/1775 train_time:98659ms step_avg:58.66ms
step:1683/1775 train_time:98743ms step_avg:58.67ms
step:1684/1775 train_time:98830ms step_avg:58.69ms
step:1685/1775 train_time:98914ms step_avg:58.70ms
step:1686/1775 train_time:99003ms step_avg:58.72ms
step:1687/1775 train_time:99087ms step_avg:58.74ms
step:1688/1775 train_time:99174ms step_avg:58.75ms
step:1689/1775 train_time:99258ms step_avg:58.77ms
step:1690/1775 train_time:99346ms step_avg:58.78ms
step:1691/1775 train_time:99430ms step_avg:58.80ms
step:1692/1775 train_time:99517ms step_avg:58.82ms
step:1693/1775 train_time:99601ms step_avg:58.83ms
step:1694/1775 train_time:99688ms step_avg:58.85ms
step:1695/1775 train_time:99773ms step_avg:58.86ms
step:1696/1775 train_time:99860ms step_avg:58.88ms
step:1697/1775 train_time:99946ms step_avg:58.90ms
step:1698/1775 train_time:100031ms step_avg:58.91ms
step:1699/1775 train_time:100116ms step_avg:58.93ms
step:1700/1775 train_time:100204ms step_avg:58.94ms
step:1701/1775 train_time:100287ms step_avg:58.96ms
step:1702/1775 train_time:100374ms step_avg:58.97ms
step:1703/1775 train_time:100460ms step_avg:58.99ms
step:1704/1775 train_time:100548ms step_avg:59.01ms
step:1705/1775 train_time:100633ms step_avg:59.02ms
step:1706/1775 train_time:100721ms step_avg:59.04ms
step:1707/1775 train_time:100805ms step_avg:59.05ms
step:1708/1775 train_time:100891ms step_avg:59.07ms
step:1709/1775 train_time:100976ms step_avg:59.08ms
step:1710/1775 train_time:101065ms step_avg:59.10ms
step:1711/1775 train_time:101149ms step_avg:59.12ms
step:1712/1775 train_time:101235ms step_avg:59.13ms
step:1713/1775 train_time:101320ms step_avg:59.15ms
step:1714/1775 train_time:101407ms step_avg:59.16ms
step:1715/1775 train_time:101490ms step_avg:59.18ms
step:1716/1775 train_time:101579ms step_avg:59.20ms
step:1717/1775 train_time:101664ms step_avg:59.21ms
step:1718/1775 train_time:101751ms step_avg:59.23ms
step:1719/1775 train_time:101835ms step_avg:59.24ms
step:1720/1775 train_time:101922ms step_avg:59.26ms
step:1721/1775 train_time:102007ms step_avg:59.27ms
step:1722/1775 train_time:102092ms step_avg:59.29ms
step:1723/1775 train_time:102179ms step_avg:59.30ms
step:1724/1775 train_time:102267ms step_avg:59.32ms
step:1725/1775 train_time:102351ms step_avg:59.33ms
step:1726/1775 train_time:102438ms step_avg:59.35ms
step:1727/1775 train_time:102522ms step_avg:59.36ms
step:1728/1775 train_time:102609ms step_avg:59.38ms
step:1729/1775 train_time:102693ms step_avg:59.39ms
step:1730/1775 train_time:102781ms step_avg:59.41ms
step:1731/1775 train_time:102867ms step_avg:59.43ms
step:1732/1775 train_time:102952ms step_avg:59.44ms
step:1733/1775 train_time:103038ms step_avg:59.46ms
step:1734/1775 train_time:103126ms step_avg:59.47ms
step:1735/1775 train_time:103210ms step_avg:59.49ms
step:1736/1775 train_time:103301ms step_avg:59.51ms
step:1737/1775 train_time:103387ms step_avg:59.52ms
step:1738/1775 train_time:103474ms step_avg:59.54ms
step:1739/1775 train_time:103559ms step_avg:59.55ms
step:1740/1775 train_time:103646ms step_avg:59.57ms
step:1741/1775 train_time:103730ms step_avg:59.58ms
step:1742/1775 train_time:103819ms step_avg:59.60ms
step:1743/1775 train_time:103904ms step_avg:59.61ms
step:1744/1775 train_time:103991ms step_avg:59.63ms
step:1745/1775 train_time:104076ms step_avg:59.64ms
step:1746/1775 train_time:104164ms step_avg:59.66ms
step:1747/1775 train_time:104249ms step_avg:59.67ms
step:1748/1775 train_time:104336ms step_avg:59.69ms
step:1749/1775 train_time:104420ms step_avg:59.70ms
step:1750/1775 train_time:104509ms step_avg:59.72ms
step:1750/1775 val_loss:3.2869 train_time:104607ms step_avg:59.78ms
step:1751/1775 train_time:104626ms step_avg:59.75ms
step:1752/1775 train_time:104681ms step_avg:59.75ms
step:1753/1775 train_time:104766ms step_avg:59.76ms
step:1754/1775 train_time:104856ms step_avg:59.78ms
step:1755/1775 train_time:104941ms step_avg:59.80ms
step:1756/1775 train_time:105026ms step_avg:59.81ms
step:1757/1775 train_time:105110ms step_avg:59.82ms
step:1758/1775 train_time:105197ms step_avg:59.84ms
step:1759/1775 train_time:105282ms step_avg:59.85ms
step:1760/1775 train_time:105369ms step_avg:59.87ms
step:1761/1775 train_time:105453ms step_avg:59.88ms
step:1762/1775 train_time:105542ms step_avg:59.90ms
step:1763/1775 train_time:105629ms step_avg:59.91ms
step:1764/1775 train_time:105717ms step_avg:59.93ms
step:1765/1775 train_time:105802ms step_avg:59.94ms
step:1766/1775 train_time:105890ms step_avg:59.96ms
step:1767/1775 train_time:105975ms step_avg:59.97ms
step:1768/1775 train_time:106062ms step_avg:59.99ms
step:1769/1775 train_time:106146ms step_avg:60.00ms
step:1770/1775 train_time:106234ms step_avg:60.02ms
step:1771/1775 train_time:106320ms step_avg:60.03ms
step:1772/1775 train_time:106406ms step_avg:60.05ms
step:1773/1775 train_time:106491ms step_avg:60.06ms
step:1774/1775 train_time:106581ms step_avg:60.08ms
step:1775/1775 train_time:106665ms step_avg:60.09ms
step:1775/1775 val_loss:3.2802 train_time:106765ms step_avg:60.15ms
peak memory allocated: 29148 MiB reserved: 44818 MiB

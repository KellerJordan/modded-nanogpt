import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Jan 16 22:22:53 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            137W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     62551      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    1   N/A  N/A     62552      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    2   N/A  N/A     62553      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    3   N/A  N/A     62554      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    4   N/A  N/A     62555      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    5   N/A  N/A     62556      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    6   N/A  N/A     62557      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
|    7   N/A  N/A     62558      C   ...omamba/envs/speedrun/bin/python3.12       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8314 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:99ms step_avg:99.03ms
step:2/1775 train_time:121ms step_avg:60.54ms
step:3/1775 train_time:140ms step_avg:46.77ms
step:4/1775 train_time:165ms step_avg:41.28ms
step:5/1775 train_time:196ms step_avg:39.23ms
step:6/1775 train_time:288ms step_avg:48.04ms
step:7/1775 train_time:306ms step_avg:43.75ms
step:8/1775 train_time:326ms step_avg:40.81ms
step:9/1775 train_time:358ms step_avg:39.73ms
step:10/1775 train_time:391ms step_avg:39.07ms
step:11/1775 train_time:422ms step_avg:38.35ms
step:12/1775 train_time:455ms step_avg:37.94ms
step:13/1775 train_time:487ms step_avg:37.43ms
step:14/1775 train_time:520ms step_avg:37.15ms
step:15/1775 train_time:552ms step_avg:36.79ms
step:16/1775 train_time:586ms step_avg:36.60ms
step:17/1775 train_time:617ms step_avg:36.28ms
step:18/1775 train_time:650ms step_avg:36.14ms
step:19/1775 train_time:682ms step_avg:35.88ms
step:20/1775 train_time:715ms step_avg:35.77ms
step:21/1775 train_time:747ms step_avg:35.56ms
step:22/1775 train_time:780ms step_avg:35.47ms
step:23/1775 train_time:812ms step_avg:35.30ms
step:24/1775 train_time:845ms step_avg:35.23ms
step:25/1775 train_time:877ms step_avg:35.08ms
step:26/1775 train_time:911ms step_avg:35.02ms
step:27/1775 train_time:942ms step_avg:34.89ms
step:28/1775 train_time:975ms step_avg:34.84ms
step:29/1775 train_time:1007ms step_avg:34.72ms
step:30/1775 train_time:1040ms step_avg:34.68ms
step:31/1775 train_time:1072ms step_avg:34.57ms
step:32/1775 train_time:1105ms step_avg:34.55ms
step:33/1775 train_time:1137ms step_avg:34.45ms
step:34/1775 train_time:1171ms step_avg:34.45ms
step:35/1775 train_time:1204ms step_avg:34.39ms
step:36/1775 train_time:1239ms step_avg:34.42ms
step:37/1775 train_time:1272ms step_avg:34.37ms
step:38/1775 train_time:1306ms step_avg:34.38ms
step:39/1775 train_time:1339ms step_avg:34.32ms
step:40/1775 train_time:1372ms step_avg:34.31ms
step:41/1775 train_time:1404ms step_avg:34.25ms
step:42/1775 train_time:1439ms step_avg:34.27ms
step:43/1775 train_time:1470ms step_avg:34.18ms
step:44/1775 train_time:1504ms step_avg:34.18ms
step:45/1775 train_time:1535ms step_avg:34.12ms
step:46/1775 train_time:1569ms step_avg:34.11ms
step:47/1775 train_time:1601ms step_avg:34.06ms
step:48/1775 train_time:1634ms step_avg:34.05ms
step:49/1775 train_time:1666ms step_avg:34.00ms
step:50/1775 train_time:1699ms step_avg:33.99ms
step:51/1775 train_time:1731ms step_avg:33.94ms
step:52/1775 train_time:1764ms step_avg:33.93ms
step:53/1775 train_time:1796ms step_avg:33.88ms
step:54/1775 train_time:1830ms step_avg:33.88ms
step:55/1775 train_time:1861ms step_avg:33.84ms
step:56/1775 train_time:1895ms step_avg:33.84ms
step:57/1775 train_time:1926ms step_avg:33.80ms
step:58/1775 train_time:1960ms step_avg:33.79ms
step:59/1775 train_time:1991ms step_avg:33.75ms
step:60/1775 train_time:2025ms step_avg:33.75ms
step:61/1775 train_time:2057ms step_avg:33.72ms
step:62/1775 train_time:2091ms step_avg:33.73ms
step:63/1775 train_time:2123ms step_avg:33.69ms
step:64/1775 train_time:2157ms step_avg:33.70ms
step:65/1775 train_time:2188ms step_avg:33.66ms
step:66/1775 train_time:2222ms step_avg:33.67ms
step:67/1775 train_time:2254ms step_avg:33.64ms
step:68/1775 train_time:2288ms step_avg:33.65ms
step:69/1775 train_time:2320ms step_avg:33.62ms
step:70/1775 train_time:2354ms step_avg:33.63ms
step:71/1775 train_time:2386ms step_avg:33.61ms
step:72/1775 train_time:2420ms step_avg:33.61ms
step:73/1775 train_time:2452ms step_avg:33.59ms
step:74/1775 train_time:2486ms step_avg:33.59ms
step:75/1775 train_time:2517ms step_avg:33.56ms
step:76/1775 train_time:2551ms step_avg:33.57ms
step:77/1775 train_time:2583ms step_avg:33.54ms
step:78/1775 train_time:2617ms step_avg:33.55ms
step:79/1775 train_time:2648ms step_avg:33.52ms
step:80/1775 train_time:2682ms step_avg:33.52ms
step:81/1775 train_time:2713ms step_avg:33.50ms
step:82/1775 train_time:2747ms step_avg:33.50ms
step:83/1775 train_time:2778ms step_avg:33.47ms
step:84/1775 train_time:2812ms step_avg:33.47ms
step:85/1775 train_time:2844ms step_avg:33.45ms
step:86/1775 train_time:2877ms step_avg:33.46ms
step:87/1775 train_time:2909ms step_avg:33.43ms
step:88/1775 train_time:2942ms step_avg:33.44ms
step:89/1775 train_time:2974ms step_avg:33.42ms
step:90/1775 train_time:3008ms step_avg:33.42ms
step:91/1775 train_time:3039ms step_avg:33.40ms
step:92/1775 train_time:3073ms step_avg:33.40ms
step:93/1775 train_time:3105ms step_avg:33.38ms
step:94/1775 train_time:3139ms step_avg:33.39ms
step:95/1775 train_time:3170ms step_avg:33.37ms
step:96/1775 train_time:3204ms step_avg:33.38ms
step:97/1775 train_time:3236ms step_avg:33.36ms
step:98/1775 train_time:3270ms step_avg:33.37ms
step:99/1775 train_time:3302ms step_avg:33.36ms
step:100/1775 train_time:3336ms step_avg:33.36ms
step:101/1775 train_time:3368ms step_avg:33.35ms
step:102/1775 train_time:3402ms step_avg:33.35ms
step:103/1775 train_time:3434ms step_avg:33.34ms
step:104/1775 train_time:3469ms step_avg:33.35ms
step:105/1775 train_time:3500ms step_avg:33.33ms
step:106/1775 train_time:3534ms step_avg:33.34ms
step:107/1775 train_time:3566ms step_avg:33.33ms
step:108/1775 train_time:3600ms step_avg:33.33ms
step:109/1775 train_time:3631ms step_avg:33.31ms
step:110/1775 train_time:3665ms step_avg:33.32ms
step:111/1775 train_time:3697ms step_avg:33.30ms
step:112/1775 train_time:3730ms step_avg:33.31ms
step:113/1775 train_time:3762ms step_avg:33.29ms
step:114/1775 train_time:3795ms step_avg:33.29ms
step:115/1775 train_time:3827ms step_avg:33.28ms
step:116/1775 train_time:3860ms step_avg:33.28ms
step:117/1775 train_time:3892ms step_avg:33.26ms
step:118/1775 train_time:3925ms step_avg:33.27ms
step:119/1775 train_time:3957ms step_avg:33.25ms
step:120/1775 train_time:3991ms step_avg:33.26ms
step:121/1775 train_time:4022ms step_avg:33.24ms
step:122/1775 train_time:4056ms step_avg:33.25ms
step:123/1775 train_time:4087ms step_avg:33.23ms
step:124/1775 train_time:4121ms step_avg:33.23ms
step:125/1775 train_time:4153ms step_avg:33.22ms
step:126/1775 train_time:4186ms step_avg:33.23ms
step:127/1775 train_time:4218ms step_avg:33.21ms
step:128/1775 train_time:4252ms step_avg:33.22ms
step:129/1775 train_time:4284ms step_avg:33.21ms
step:130/1775 train_time:4318ms step_avg:33.22ms
step:131/1775 train_time:4350ms step_avg:33.20ms
step:132/1775 train_time:4383ms step_avg:33.21ms
step:133/1775 train_time:4415ms step_avg:33.20ms
step:134/1775 train_time:4449ms step_avg:33.20ms
step:135/1775 train_time:4481ms step_avg:33.19ms
step:136/1775 train_time:4515ms step_avg:33.20ms
step:137/1775 train_time:4547ms step_avg:33.19ms
step:138/1775 train_time:4580ms step_avg:33.19ms
step:139/1775 train_time:4611ms step_avg:33.18ms
step:140/1775 train_time:4645ms step_avg:33.18ms
step:141/1775 train_time:4677ms step_avg:33.17ms
step:142/1775 train_time:4711ms step_avg:33.18ms
step:143/1775 train_time:4743ms step_avg:33.17ms
step:144/1775 train_time:4776ms step_avg:33.17ms
step:145/1775 train_time:4808ms step_avg:33.16ms
step:146/1775 train_time:4841ms step_avg:33.16ms
step:147/1775 train_time:4873ms step_avg:33.15ms
step:148/1775 train_time:4907ms step_avg:33.15ms
step:149/1775 train_time:4939ms step_avg:33.15ms
step:150/1775 train_time:4972ms step_avg:33.15ms
step:151/1775 train_time:5004ms step_avg:33.14ms
step:152/1775 train_time:5037ms step_avg:33.14ms
step:153/1775 train_time:5069ms step_avg:33.13ms
step:154/1775 train_time:5102ms step_avg:33.13ms
step:155/1775 train_time:5134ms step_avg:33.13ms
step:156/1775 train_time:5168ms step_avg:33.13ms
step:157/1775 train_time:5200ms step_avg:33.12ms
step:158/1775 train_time:5233ms step_avg:33.12ms
step:159/1775 train_time:5265ms step_avg:33.11ms
step:160/1775 train_time:5298ms step_avg:33.12ms
step:161/1775 train_time:5330ms step_avg:33.11ms
step:162/1775 train_time:5364ms step_avg:33.11ms
step:163/1775 train_time:5396ms step_avg:33.10ms
step:164/1775 train_time:5429ms step_avg:33.11ms
step:165/1775 train_time:5461ms step_avg:33.09ms
step:166/1775 train_time:5494ms step_avg:33.10ms
step:167/1775 train_time:5526ms step_avg:33.09ms
step:168/1775 train_time:5560ms step_avg:33.10ms
step:169/1775 train_time:5592ms step_avg:33.09ms
step:170/1775 train_time:5625ms step_avg:33.09ms
step:171/1775 train_time:5657ms step_avg:33.08ms
step:172/1775 train_time:5691ms step_avg:33.08ms
step:173/1775 train_time:5722ms step_avg:33.08ms
step:174/1775 train_time:5756ms step_avg:33.08ms
step:175/1775 train_time:5787ms step_avg:33.07ms
step:176/1775 train_time:5821ms step_avg:33.07ms
step:177/1775 train_time:5852ms step_avg:33.06ms
step:178/1775 train_time:5886ms step_avg:33.07ms
step:179/1775 train_time:5917ms step_avg:33.06ms
step:180/1775 train_time:5952ms step_avg:33.07ms
step:181/1775 train_time:5984ms step_avg:33.06ms
step:182/1775 train_time:6017ms step_avg:33.06ms
step:183/1775 train_time:6049ms step_avg:33.05ms
step:184/1775 train_time:6082ms step_avg:33.06ms
step:185/1775 train_time:6114ms step_avg:33.05ms
step:186/1775 train_time:6148ms step_avg:33.05ms
step:187/1775 train_time:6179ms step_avg:33.04ms
step:188/1775 train_time:6213ms step_avg:33.05ms
step:189/1775 train_time:6245ms step_avg:33.04ms
step:190/1775 train_time:6278ms step_avg:33.04ms
step:191/1775 train_time:6310ms step_avg:33.04ms
step:192/1775 train_time:6344ms step_avg:33.04ms
step:193/1775 train_time:6375ms step_avg:33.03ms
step:194/1775 train_time:6409ms step_avg:33.04ms
step:195/1775 train_time:6441ms step_avg:33.03ms
step:196/1775 train_time:6474ms step_avg:33.03ms
step:197/1775 train_time:6505ms step_avg:33.02ms
step:198/1775 train_time:6540ms step_avg:33.03ms
step:199/1775 train_time:6571ms step_avg:33.02ms
step:200/1775 train_time:6605ms step_avg:33.03ms
step:201/1775 train_time:6637ms step_avg:33.02ms
step:202/1775 train_time:6671ms step_avg:33.03ms
step:203/1775 train_time:6703ms step_avg:33.02ms
step:204/1775 train_time:6736ms step_avg:33.02ms
step:205/1775 train_time:6768ms step_avg:33.01ms
step:206/1775 train_time:6801ms step_avg:33.02ms
step:207/1775 train_time:6832ms step_avg:33.01ms
step:208/1775 train_time:6866ms step_avg:33.01ms
step:209/1775 train_time:6898ms step_avg:33.00ms
step:210/1775 train_time:6932ms step_avg:33.01ms
step:211/1775 train_time:6963ms step_avg:33.00ms
step:212/1775 train_time:6997ms step_avg:33.00ms
step:213/1775 train_time:7028ms step_avg:33.00ms
step:214/1775 train_time:7061ms step_avg:33.00ms
step:215/1775 train_time:7093ms step_avg:32.99ms
step:216/1775 train_time:7126ms step_avg:32.99ms
step:217/1775 train_time:7158ms step_avg:32.99ms
step:218/1775 train_time:7192ms step_avg:32.99ms
step:219/1775 train_time:7223ms step_avg:32.98ms
step:220/1775 train_time:7258ms step_avg:32.99ms
step:221/1775 train_time:7288ms step_avg:32.98ms
step:222/1775 train_time:7322ms step_avg:32.98ms
step:223/1775 train_time:7353ms step_avg:32.97ms
step:224/1775 train_time:7387ms step_avg:32.98ms
step:225/1775 train_time:7418ms step_avg:32.97ms
step:226/1775 train_time:7453ms step_avg:32.98ms
step:227/1775 train_time:7485ms step_avg:32.97ms
step:228/1775 train_time:7519ms step_avg:32.98ms
step:229/1775 train_time:7551ms step_avg:32.97ms
step:230/1775 train_time:7585ms step_avg:32.98ms
step:231/1775 train_time:7616ms step_avg:32.97ms
step:232/1775 train_time:7650ms step_avg:32.98ms
step:233/1775 train_time:7682ms step_avg:32.97ms
step:234/1775 train_time:7715ms step_avg:32.97ms
step:235/1775 train_time:7747ms step_avg:32.97ms
step:236/1775 train_time:7781ms step_avg:32.97ms
step:237/1775 train_time:7813ms step_avg:32.96ms
step:238/1775 train_time:7846ms step_avg:32.97ms
step:239/1775 train_time:7878ms step_avg:32.96ms
step:240/1775 train_time:7912ms step_avg:32.97ms
step:241/1775 train_time:7944ms step_avg:32.96ms
step:242/1775 train_time:7977ms step_avg:32.96ms
step:243/1775 train_time:8009ms step_avg:32.96ms
step:244/1775 train_time:8042ms step_avg:32.96ms
step:245/1775 train_time:8074ms step_avg:32.95ms
step:246/1775 train_time:8107ms step_avg:32.96ms
step:247/1775 train_time:8139ms step_avg:32.95ms
step:248/1775 train_time:8173ms step_avg:32.95ms
step:249/1775 train_time:8204ms step_avg:32.95ms
step:250/1775 train_time:8237ms step_avg:32.95ms
step:250/1775 val_loss:4.6127 train_time:8280ms step_avg:33.12ms
step:251/1775 train_time:8304ms step_avg:33.08ms
step:252/1775 train_time:8323ms step_avg:33.03ms
step:253/1775 train_time:8340ms step_avg:32.97ms
step:254/1775 train_time:8373ms step_avg:32.96ms
step:255/1775 train_time:8405ms step_avg:32.96ms
step:256/1775 train_time:8439ms step_avg:32.96ms
step:257/1775 train_time:8471ms step_avg:32.96ms
step:258/1775 train_time:8505ms step_avg:32.96ms
step:259/1775 train_time:8536ms step_avg:32.96ms
step:260/1775 train_time:8570ms step_avg:32.96ms
step:261/1775 train_time:8601ms step_avg:32.96ms
step:262/1775 train_time:8635ms step_avg:32.96ms
step:263/1775 train_time:8666ms step_avg:32.95ms
step:264/1775 train_time:8699ms step_avg:32.95ms
step:265/1775 train_time:8731ms step_avg:32.95ms
step:266/1775 train_time:8764ms step_avg:32.95ms
step:267/1775 train_time:8795ms step_avg:32.94ms
step:268/1775 train_time:8829ms step_avg:32.94ms
step:269/1775 train_time:8860ms step_avg:32.94ms
step:270/1775 train_time:8894ms step_avg:32.94ms
step:271/1775 train_time:8926ms step_avg:32.94ms
step:272/1775 train_time:8959ms step_avg:32.94ms
step:273/1775 train_time:8990ms step_avg:32.93ms
step:274/1775 train_time:9023ms step_avg:32.93ms
step:275/1775 train_time:9055ms step_avg:32.93ms
step:276/1775 train_time:9088ms step_avg:32.93ms
step:277/1775 train_time:9120ms step_avg:32.92ms
step:278/1775 train_time:9153ms step_avg:32.92ms
step:279/1775 train_time:9184ms step_avg:32.92ms
step:280/1775 train_time:9219ms step_avg:32.92ms
step:281/1775 train_time:9250ms step_avg:32.92ms
step:282/1775 train_time:9284ms step_avg:32.92ms
step:283/1775 train_time:9316ms step_avg:32.92ms
step:284/1775 train_time:9350ms step_avg:32.92ms
step:285/1775 train_time:9382ms step_avg:32.92ms
step:286/1775 train_time:9416ms step_avg:32.92ms
step:287/1775 train_time:9448ms step_avg:32.92ms
step:288/1775 train_time:9482ms step_avg:32.92ms
step:289/1775 train_time:9514ms step_avg:32.92ms
step:290/1775 train_time:9548ms step_avg:32.92ms
step:291/1775 train_time:9579ms step_avg:32.92ms
step:292/1775 train_time:9612ms step_avg:32.92ms
step:293/1775 train_time:9644ms step_avg:32.91ms
step:294/1775 train_time:9678ms step_avg:32.92ms
step:295/1775 train_time:9709ms step_avg:32.91ms
step:296/1775 train_time:9742ms step_avg:32.91ms
step:297/1775 train_time:9774ms step_avg:32.91ms
step:298/1775 train_time:9808ms step_avg:32.91ms
step:299/1775 train_time:9839ms step_avg:32.91ms
step:300/1775 train_time:9872ms step_avg:32.91ms
step:301/1775 train_time:9904ms step_avg:32.91ms
step:302/1775 train_time:9938ms step_avg:32.91ms
step:303/1775 train_time:9969ms step_avg:32.90ms
step:304/1775 train_time:10003ms step_avg:32.90ms
step:305/1775 train_time:10034ms step_avg:32.90ms
step:306/1775 train_time:10067ms step_avg:32.90ms
step:307/1775 train_time:10099ms step_avg:32.90ms
step:308/1775 train_time:10133ms step_avg:32.90ms
step:309/1775 train_time:10164ms step_avg:32.89ms
step:310/1775 train_time:10197ms step_avg:32.89ms
step:311/1775 train_time:10229ms step_avg:32.89ms
step:312/1775 train_time:10263ms step_avg:32.89ms
step:313/1775 train_time:10294ms step_avg:32.89ms
step:314/1775 train_time:10328ms step_avg:32.89ms
step:315/1775 train_time:10359ms step_avg:32.89ms
step:316/1775 train_time:10393ms step_avg:32.89ms
step:317/1775 train_time:10425ms step_avg:32.89ms
step:318/1775 train_time:10459ms step_avg:32.89ms
step:319/1775 train_time:10491ms step_avg:32.89ms
step:320/1775 train_time:10525ms step_avg:32.89ms
step:321/1775 train_time:10557ms step_avg:32.89ms
step:322/1775 train_time:10590ms step_avg:32.89ms
step:323/1775 train_time:10622ms step_avg:32.88ms
step:324/1775 train_time:10655ms step_avg:32.89ms
step:325/1775 train_time:10687ms step_avg:32.88ms
step:326/1775 train_time:10721ms step_avg:32.89ms
step:327/1775 train_time:10752ms step_avg:32.88ms
step:328/1775 train_time:10786ms step_avg:32.88ms
step:329/1775 train_time:10817ms step_avg:32.88ms
step:330/1775 train_time:10850ms step_avg:32.88ms
step:331/1775 train_time:10882ms step_avg:32.88ms
step:332/1775 train_time:10915ms step_avg:32.88ms
step:333/1775 train_time:10947ms step_avg:32.87ms
step:334/1775 train_time:10981ms step_avg:32.88ms
step:335/1775 train_time:11012ms step_avg:32.87ms
step:336/1775 train_time:11046ms step_avg:32.87ms
step:337/1775 train_time:11078ms step_avg:32.87ms
step:338/1775 train_time:11111ms step_avg:32.87ms
step:339/1775 train_time:11142ms step_avg:32.87ms
step:340/1775 train_time:11176ms step_avg:32.87ms
step:341/1775 train_time:11208ms step_avg:32.87ms
step:342/1775 train_time:11242ms step_avg:32.87ms
step:343/1775 train_time:11273ms step_avg:32.87ms
step:344/1775 train_time:11307ms step_avg:32.87ms
step:345/1775 train_time:11339ms step_avg:32.87ms
step:346/1775 train_time:11372ms step_avg:32.87ms
step:347/1775 train_time:11404ms step_avg:32.86ms
step:348/1775 train_time:11438ms step_avg:32.87ms
step:349/1775 train_time:11470ms step_avg:32.86ms
step:350/1775 train_time:11504ms step_avg:32.87ms
step:351/1775 train_time:11536ms step_avg:32.87ms
step:352/1775 train_time:11569ms step_avg:32.87ms
step:353/1775 train_time:11601ms step_avg:32.86ms
step:354/1775 train_time:11634ms step_avg:32.86ms
step:355/1775 train_time:11666ms step_avg:32.86ms
step:356/1775 train_time:11700ms step_avg:32.86ms
step:357/1775 train_time:11731ms step_avg:32.86ms
step:358/1775 train_time:11765ms step_avg:32.86ms
step:359/1775 train_time:11797ms step_avg:32.86ms
step:360/1775 train_time:11830ms step_avg:32.86ms
step:361/1775 train_time:11861ms step_avg:32.86ms
step:362/1775 train_time:11895ms step_avg:32.86ms
step:363/1775 train_time:11927ms step_avg:32.86ms
step:364/1775 train_time:11960ms step_avg:32.86ms
step:365/1775 train_time:11991ms step_avg:32.85ms
step:366/1775 train_time:12025ms step_avg:32.86ms
step:367/1775 train_time:12056ms step_avg:32.85ms
step:368/1775 train_time:12090ms step_avg:32.85ms
step:369/1775 train_time:12122ms step_avg:32.85ms
step:370/1775 train_time:12155ms step_avg:32.85ms
step:371/1775 train_time:12187ms step_avg:32.85ms
step:372/1775 train_time:12220ms step_avg:32.85ms
step:373/1775 train_time:12252ms step_avg:32.85ms
step:374/1775 train_time:12286ms step_avg:32.85ms
step:375/1775 train_time:12317ms step_avg:32.85ms
step:376/1775 train_time:12352ms step_avg:32.85ms
step:377/1775 train_time:12383ms step_avg:32.85ms
step:378/1775 train_time:12417ms step_avg:32.85ms
step:379/1775 train_time:12448ms step_avg:32.84ms
step:380/1775 train_time:12482ms step_avg:32.85ms
step:381/1775 train_time:12514ms step_avg:32.85ms
step:382/1775 train_time:12548ms step_avg:32.85ms
step:383/1775 train_time:12579ms step_avg:32.84ms
step:384/1775 train_time:12613ms step_avg:32.85ms
step:385/1775 train_time:12644ms step_avg:32.84ms
step:386/1775 train_time:12678ms step_avg:32.84ms
step:387/1775 train_time:12709ms step_avg:32.84ms
step:388/1775 train_time:12743ms step_avg:32.84ms
step:389/1775 train_time:12775ms step_avg:32.84ms
step:390/1775 train_time:12808ms step_avg:32.84ms
step:391/1775 train_time:12840ms step_avg:32.84ms
step:392/1775 train_time:12873ms step_avg:32.84ms
step:393/1775 train_time:12904ms step_avg:32.84ms
step:394/1775 train_time:12938ms step_avg:32.84ms
step:395/1775 train_time:12970ms step_avg:32.83ms
step:396/1775 train_time:13003ms step_avg:32.84ms
step:397/1775 train_time:13034ms step_avg:32.83ms
step:398/1775 train_time:13068ms step_avg:32.83ms
step:399/1775 train_time:13099ms step_avg:32.83ms
step:400/1775 train_time:13133ms step_avg:32.83ms
step:401/1775 train_time:13164ms step_avg:32.83ms
step:402/1775 train_time:13198ms step_avg:32.83ms
step:403/1775 train_time:13230ms step_avg:32.83ms
step:404/1775 train_time:13263ms step_avg:32.83ms
step:405/1775 train_time:13295ms step_avg:32.83ms
step:406/1775 train_time:13329ms step_avg:32.83ms
step:407/1775 train_time:13360ms step_avg:32.83ms
step:408/1775 train_time:13393ms step_avg:32.83ms
step:409/1775 train_time:13425ms step_avg:32.82ms
step:410/1775 train_time:13459ms step_avg:32.83ms
step:411/1775 train_time:13490ms step_avg:32.82ms
step:412/1775 train_time:13524ms step_avg:32.83ms
step:413/1775 train_time:13556ms step_avg:32.82ms
step:414/1775 train_time:13590ms step_avg:32.82ms
step:415/1775 train_time:13621ms step_avg:32.82ms
step:416/1775 train_time:13655ms step_avg:32.82ms
step:417/1775 train_time:13686ms step_avg:32.82ms
step:418/1775 train_time:13720ms step_avg:32.82ms
step:419/1775 train_time:13751ms step_avg:32.82ms
step:420/1775 train_time:13785ms step_avg:32.82ms
step:421/1775 train_time:13817ms step_avg:32.82ms
step:422/1775 train_time:13850ms step_avg:32.82ms
step:423/1775 train_time:13882ms step_avg:32.82ms
step:424/1775 train_time:13915ms step_avg:32.82ms
step:425/1775 train_time:13947ms step_avg:32.82ms
step:426/1775 train_time:13981ms step_avg:32.82ms
step:427/1775 train_time:14012ms step_avg:32.81ms
step:428/1775 train_time:14046ms step_avg:32.82ms
step:429/1775 train_time:14077ms step_avg:32.81ms
step:430/1775 train_time:14110ms step_avg:32.81ms
step:431/1775 train_time:14142ms step_avg:32.81ms
step:432/1775 train_time:14175ms step_avg:32.81ms
step:433/1775 train_time:14207ms step_avg:32.81ms
step:434/1775 train_time:14241ms step_avg:32.81ms
step:435/1775 train_time:14272ms step_avg:32.81ms
step:436/1775 train_time:14306ms step_avg:32.81ms
step:437/1775 train_time:14338ms step_avg:32.81ms
step:438/1775 train_time:14371ms step_avg:32.81ms
step:439/1775 train_time:14403ms step_avg:32.81ms
step:440/1775 train_time:14436ms step_avg:32.81ms
step:441/1775 train_time:14468ms step_avg:32.81ms
step:442/1775 train_time:14502ms step_avg:32.81ms
step:443/1775 train_time:14533ms step_avg:32.81ms
step:444/1775 train_time:14567ms step_avg:32.81ms
step:445/1775 train_time:14598ms step_avg:32.81ms
step:446/1775 train_time:14632ms step_avg:32.81ms
step:447/1775 train_time:14664ms step_avg:32.81ms
step:448/1775 train_time:14697ms step_avg:32.81ms
step:449/1775 train_time:14729ms step_avg:32.80ms
step:450/1775 train_time:14763ms step_avg:32.81ms
step:451/1775 train_time:14795ms step_avg:32.80ms
step:452/1775 train_time:14828ms step_avg:32.81ms
step:453/1775 train_time:14860ms step_avg:32.80ms
step:454/1775 train_time:14893ms step_avg:32.80ms
step:455/1775 train_time:14925ms step_avg:32.80ms
step:456/1775 train_time:14958ms step_avg:32.80ms
step:457/1775 train_time:14990ms step_avg:32.80ms
step:458/1775 train_time:15023ms step_avg:32.80ms
step:459/1775 train_time:15055ms step_avg:32.80ms
step:460/1775 train_time:15088ms step_avg:32.80ms
step:461/1775 train_time:15120ms step_avg:32.80ms
step:462/1775 train_time:15154ms step_avg:32.80ms
step:463/1775 train_time:15185ms step_avg:32.80ms
step:464/1775 train_time:15219ms step_avg:32.80ms
step:465/1775 train_time:15251ms step_avg:32.80ms
step:466/1775 train_time:15284ms step_avg:32.80ms
step:467/1775 train_time:15316ms step_avg:32.80ms
step:468/1775 train_time:15349ms step_avg:32.80ms
step:469/1775 train_time:15381ms step_avg:32.79ms
step:470/1775 train_time:15415ms step_avg:32.80ms
step:471/1775 train_time:15446ms step_avg:32.79ms
step:472/1775 train_time:15480ms step_avg:32.80ms
step:473/1775 train_time:15511ms step_avg:32.79ms
step:474/1775 train_time:15545ms step_avg:32.80ms
step:475/1775 train_time:15577ms step_avg:32.79ms
step:476/1775 train_time:15611ms step_avg:32.80ms
step:477/1775 train_time:15642ms step_avg:32.79ms
step:478/1775 train_time:15676ms step_avg:32.79ms
step:479/1775 train_time:15707ms step_avg:32.79ms
step:480/1775 train_time:15741ms step_avg:32.79ms
step:481/1775 train_time:15773ms step_avg:32.79ms
step:482/1775 train_time:15806ms step_avg:32.79ms
step:483/1775 train_time:15838ms step_avg:32.79ms
step:484/1775 train_time:15872ms step_avg:32.79ms
step:485/1775 train_time:15903ms step_avg:32.79ms
step:486/1775 train_time:15937ms step_avg:32.79ms
step:487/1775 train_time:15968ms step_avg:32.79ms
step:488/1775 train_time:16002ms step_avg:32.79ms
step:489/1775 train_time:16034ms step_avg:32.79ms
step:490/1775 train_time:16067ms step_avg:32.79ms
step:491/1775 train_time:16099ms step_avg:32.79ms
step:492/1775 train_time:16132ms step_avg:32.79ms
step:493/1775 train_time:16164ms step_avg:32.79ms
step:494/1775 train_time:16198ms step_avg:32.79ms
step:495/1775 train_time:16229ms step_avg:32.79ms
step:496/1775 train_time:16263ms step_avg:32.79ms
step:497/1775 train_time:16294ms step_avg:32.78ms
step:498/1775 train_time:16328ms step_avg:32.79ms
step:499/1775 train_time:16360ms step_avg:32.79ms
step:500/1775 train_time:16393ms step_avg:32.79ms
step:500/1775 val_loss:4.2947 train_time:16435ms step_avg:32.87ms
step:501/1775 train_time:16456ms step_avg:32.85ms
step:502/1775 train_time:16476ms step_avg:32.82ms
step:503/1775 train_time:16493ms step_avg:32.79ms
step:504/1775 train_time:16526ms step_avg:32.79ms
step:505/1775 train_time:16558ms step_avg:32.79ms
step:506/1775 train_time:16592ms step_avg:32.79ms
step:507/1775 train_time:16624ms step_avg:32.79ms
step:508/1775 train_time:16657ms step_avg:32.79ms
step:509/1775 train_time:16689ms step_avg:32.79ms
step:510/1775 train_time:16723ms step_avg:32.79ms
step:511/1775 train_time:16754ms step_avg:32.79ms
step:512/1775 train_time:16788ms step_avg:32.79ms
step:513/1775 train_time:16819ms step_avg:32.79ms
step:514/1775 train_time:16852ms step_avg:32.79ms
step:515/1775 train_time:16884ms step_avg:32.78ms
step:516/1775 train_time:16917ms step_avg:32.79ms
step:517/1775 train_time:16948ms step_avg:32.78ms
step:518/1775 train_time:16982ms step_avg:32.78ms
step:519/1775 train_time:17013ms step_avg:32.78ms
step:520/1775 train_time:17046ms step_avg:32.78ms
step:521/1775 train_time:17078ms step_avg:32.78ms
step:522/1775 train_time:17112ms step_avg:32.78ms
step:523/1775 train_time:17143ms step_avg:32.78ms
step:524/1775 train_time:17176ms step_avg:32.78ms
step:525/1775 train_time:17207ms step_avg:32.78ms
step:526/1775 train_time:17241ms step_avg:32.78ms
step:527/1775 train_time:17272ms step_avg:32.77ms
step:528/1775 train_time:17305ms step_avg:32.78ms
step:529/1775 train_time:17337ms step_avg:32.77ms
step:530/1775 train_time:17371ms step_avg:32.78ms
step:531/1775 train_time:17403ms step_avg:32.77ms
step:532/1775 train_time:17437ms step_avg:32.78ms
step:533/1775 train_time:17469ms step_avg:32.77ms
step:534/1775 train_time:17503ms step_avg:32.78ms
step:535/1775 train_time:17535ms step_avg:32.78ms
step:536/1775 train_time:17569ms step_avg:32.78ms
step:537/1775 train_time:17601ms step_avg:32.78ms
step:538/1775 train_time:17635ms step_avg:32.78ms
step:539/1775 train_time:17667ms step_avg:32.78ms
step:540/1775 train_time:17700ms step_avg:32.78ms
step:541/1775 train_time:17732ms step_avg:32.78ms
step:542/1775 train_time:17765ms step_avg:32.78ms
step:543/1775 train_time:17797ms step_avg:32.77ms
step:544/1775 train_time:17830ms step_avg:32.78ms
step:545/1775 train_time:17861ms step_avg:32.77ms
step:546/1775 train_time:17895ms step_avg:32.78ms
step:547/1775 train_time:17927ms step_avg:32.77ms
step:548/1775 train_time:17960ms step_avg:32.77ms
step:549/1775 train_time:17991ms step_avg:32.77ms
step:550/1775 train_time:18024ms step_avg:32.77ms
step:551/1775 train_time:18056ms step_avg:32.77ms
step:552/1775 train_time:18089ms step_avg:32.77ms
step:553/1775 train_time:18121ms step_avg:32.77ms
step:554/1775 train_time:18155ms step_avg:32.77ms
step:555/1775 train_time:18186ms step_avg:32.77ms
step:556/1775 train_time:18220ms step_avg:32.77ms
step:557/1775 train_time:18251ms step_avg:32.77ms
step:558/1775 train_time:18285ms step_avg:32.77ms
step:559/1775 train_time:18316ms step_avg:32.77ms
step:560/1775 train_time:18350ms step_avg:32.77ms
step:561/1775 train_time:18381ms step_avg:32.77ms
step:562/1775 train_time:18416ms step_avg:32.77ms
step:563/1775 train_time:18448ms step_avg:32.77ms
step:564/1775 train_time:18482ms step_avg:32.77ms
step:565/1775 train_time:18514ms step_avg:32.77ms
step:566/1775 train_time:18548ms step_avg:32.77ms
step:567/1775 train_time:18579ms step_avg:32.77ms
step:568/1775 train_time:18613ms step_avg:32.77ms
step:569/1775 train_time:18645ms step_avg:32.77ms
step:570/1775 train_time:18679ms step_avg:32.77ms
step:571/1775 train_time:18711ms step_avg:32.77ms
step:572/1775 train_time:18744ms step_avg:32.77ms
step:573/1775 train_time:18776ms step_avg:32.77ms
step:574/1775 train_time:18810ms step_avg:32.77ms
step:575/1775 train_time:18841ms step_avg:32.77ms
step:576/1775 train_time:18875ms step_avg:32.77ms
step:577/1775 train_time:18906ms step_avg:32.77ms
step:578/1775 train_time:18939ms step_avg:32.77ms
step:579/1775 train_time:18971ms step_avg:32.76ms
step:580/1775 train_time:19007ms step_avg:32.77ms
step:581/1775 train_time:19065ms step_avg:32.81ms
step:582/1775 train_time:19126ms step_avg:32.86ms
step:583/1775 train_time:19184ms step_avg:32.90ms
step:584/1775 train_time:19244ms step_avg:32.95ms
step:585/1775 train_time:19303ms step_avg:33.00ms
step:586/1775 train_time:19365ms step_avg:33.05ms
step:587/1775 train_time:19423ms step_avg:33.09ms
step:588/1775 train_time:19484ms step_avg:33.14ms
step:589/1775 train_time:19543ms step_avg:33.18ms
step:590/1775 train_time:19605ms step_avg:33.23ms
step:591/1775 train_time:19663ms step_avg:33.27ms
step:592/1775 train_time:19724ms step_avg:33.32ms
step:593/1775 train_time:19783ms step_avg:33.36ms
step:594/1775 train_time:19844ms step_avg:33.41ms
step:595/1775 train_time:19903ms step_avg:33.45ms
step:596/1775 train_time:19964ms step_avg:33.50ms
step:597/1775 train_time:20022ms step_avg:33.54ms
step:598/1775 train_time:20082ms step_avg:33.58ms
step:599/1775 train_time:20140ms step_avg:33.62ms
step:600/1775 train_time:20201ms step_avg:33.67ms
step:601/1775 train_time:20259ms step_avg:33.71ms
step:602/1775 train_time:20321ms step_avg:33.76ms
step:603/1775 train_time:20379ms step_avg:33.80ms
step:604/1775 train_time:20440ms step_avg:33.84ms
step:605/1775 train_time:20498ms step_avg:33.88ms
step:606/1775 train_time:20560ms step_avg:33.93ms
step:607/1775 train_time:20619ms step_avg:33.97ms
step:608/1775 train_time:20680ms step_avg:34.01ms
step:609/1775 train_time:20739ms step_avg:34.05ms
step:610/1775 train_time:20800ms step_avg:34.10ms
step:611/1775 train_time:20859ms step_avg:34.14ms
step:612/1775 train_time:20921ms step_avg:34.18ms
step:613/1775 train_time:20979ms step_avg:34.22ms
step:614/1775 train_time:21041ms step_avg:34.27ms
step:615/1775 train_time:21099ms step_avg:34.31ms
step:616/1775 train_time:21160ms step_avg:34.35ms
step:617/1775 train_time:21218ms step_avg:34.39ms
step:618/1775 train_time:21279ms step_avg:34.43ms
step:619/1775 train_time:21338ms step_avg:34.47ms
step:620/1775 train_time:21399ms step_avg:34.51ms
step:621/1775 train_time:21458ms step_avg:34.55ms
step:622/1775 train_time:21518ms step_avg:34.60ms
step:623/1775 train_time:21577ms step_avg:34.63ms
step:624/1775 train_time:21639ms step_avg:34.68ms
step:625/1775 train_time:21697ms step_avg:34.72ms
step:626/1775 train_time:21758ms step_avg:34.76ms
step:627/1775 train_time:21816ms step_avg:34.79ms
step:628/1775 train_time:21878ms step_avg:34.84ms
step:629/1775 train_time:21937ms step_avg:34.88ms
step:630/1775 train_time:21997ms step_avg:34.92ms
step:631/1775 train_time:22056ms step_avg:34.95ms
step:632/1775 train_time:22117ms step_avg:34.99ms
step:633/1775 train_time:22175ms step_avg:35.03ms
step:634/1775 train_time:22236ms step_avg:35.07ms
step:635/1775 train_time:22294ms step_avg:35.11ms
step:636/1775 train_time:22356ms step_avg:35.15ms
step:637/1775 train_time:22415ms step_avg:35.19ms
step:638/1775 train_time:22476ms step_avg:35.23ms
step:639/1775 train_time:22536ms step_avg:35.27ms
step:640/1775 train_time:22597ms step_avg:35.31ms
step:641/1775 train_time:22656ms step_avg:35.34ms
step:642/1775 train_time:22717ms step_avg:35.38ms
step:643/1775 train_time:22776ms step_avg:35.42ms
step:644/1775 train_time:22836ms step_avg:35.46ms
step:645/1775 train_time:22895ms step_avg:35.50ms
step:646/1775 train_time:22956ms step_avg:35.54ms
step:647/1775 train_time:23014ms step_avg:35.57ms
step:648/1775 train_time:23076ms step_avg:35.61ms
step:649/1775 train_time:23134ms step_avg:35.65ms
step:650/1775 train_time:23195ms step_avg:35.68ms
step:651/1775 train_time:23253ms step_avg:35.72ms
step:652/1775 train_time:23314ms step_avg:35.76ms
step:653/1775 train_time:23373ms step_avg:35.79ms
step:654/1775 train_time:23435ms step_avg:35.83ms
step:655/1775 train_time:23494ms step_avg:35.87ms
step:656/1775 train_time:23555ms step_avg:35.91ms
step:657/1775 train_time:23615ms step_avg:35.94ms
step:658/1775 train_time:23676ms step_avg:35.98ms
step:659/1775 train_time:23735ms step_avg:36.02ms
step:660/1775 train_time:23795ms step_avg:36.05ms
step:661/1775 train_time:23854ms step_avg:36.09ms
step:662/1775 train_time:23915ms step_avg:36.13ms
step:663/1775 train_time:23974ms step_avg:36.16ms
step:664/1775 train_time:24035ms step_avg:36.20ms
step:665/1775 train_time:24093ms step_avg:36.23ms
step:666/1775 train_time:24155ms step_avg:36.27ms
step:667/1775 train_time:24213ms step_avg:36.30ms
step:668/1775 train_time:24275ms step_avg:36.34ms
step:669/1775 train_time:24333ms step_avg:36.37ms
step:670/1775 train_time:24394ms step_avg:36.41ms
step:671/1775 train_time:24453ms step_avg:36.44ms
step:672/1775 train_time:24514ms step_avg:36.48ms
step:673/1775 train_time:24573ms step_avg:36.51ms
step:674/1775 train_time:24634ms step_avg:36.55ms
step:675/1775 train_time:24693ms step_avg:36.58ms
step:676/1775 train_time:24754ms step_avg:36.62ms
step:677/1775 train_time:24813ms step_avg:36.65ms
step:678/1775 train_time:24874ms step_avg:36.69ms
step:679/1775 train_time:24933ms step_avg:36.72ms
step:680/1775 train_time:24994ms step_avg:36.76ms
step:681/1775 train_time:25053ms step_avg:36.79ms
step:682/1775 train_time:25114ms step_avg:36.82ms
step:683/1775 train_time:25173ms step_avg:36.86ms
step:684/1775 train_time:25234ms step_avg:36.89ms
step:685/1775 train_time:25293ms step_avg:36.92ms
step:686/1775 train_time:25354ms step_avg:36.96ms
step:687/1775 train_time:25414ms step_avg:36.99ms
step:688/1775 train_time:25475ms step_avg:37.03ms
step:689/1775 train_time:25534ms step_avg:37.06ms
step:690/1775 train_time:25595ms step_avg:37.09ms
step:691/1775 train_time:25654ms step_avg:37.13ms
step:692/1775 train_time:25715ms step_avg:37.16ms
step:693/1775 train_time:25773ms step_avg:37.19ms
step:694/1775 train_time:25835ms step_avg:37.23ms
step:695/1775 train_time:25893ms step_avg:37.26ms
step:696/1775 train_time:25954ms step_avg:37.29ms
step:697/1775 train_time:26012ms step_avg:37.32ms
step:698/1775 train_time:26073ms step_avg:37.35ms
step:699/1775 train_time:26133ms step_avg:37.39ms
step:700/1775 train_time:26194ms step_avg:37.42ms
step:701/1775 train_time:26252ms step_avg:37.45ms
step:702/1775 train_time:26314ms step_avg:37.48ms
step:703/1775 train_time:26373ms step_avg:37.51ms
step:704/1775 train_time:26435ms step_avg:37.55ms
step:705/1775 train_time:26493ms step_avg:37.58ms
step:706/1775 train_time:26554ms step_avg:37.61ms
step:707/1775 train_time:26613ms step_avg:37.64ms
step:708/1775 train_time:26674ms step_avg:37.68ms
step:709/1775 train_time:26732ms step_avg:37.70ms
step:710/1775 train_time:26794ms step_avg:37.74ms
step:711/1775 train_time:26853ms step_avg:37.77ms
step:712/1775 train_time:26914ms step_avg:37.80ms
step:713/1775 train_time:26973ms step_avg:37.83ms
step:714/1775 train_time:27034ms step_avg:37.86ms
step:715/1775 train_time:27092ms step_avg:37.89ms
step:716/1775 train_time:27153ms step_avg:37.92ms
step:717/1775 train_time:27212ms step_avg:37.95ms
step:718/1775 train_time:27274ms step_avg:37.99ms
step:719/1775 train_time:27333ms step_avg:38.02ms
step:720/1775 train_time:27394ms step_avg:38.05ms
step:721/1775 train_time:27453ms step_avg:38.08ms
step:722/1775 train_time:27514ms step_avg:38.11ms
step:723/1775 train_time:27572ms step_avg:38.14ms
step:724/1775 train_time:27634ms step_avg:38.17ms
step:725/1775 train_time:27692ms step_avg:38.20ms
step:726/1775 train_time:27753ms step_avg:38.23ms
step:727/1775 train_time:27811ms step_avg:38.25ms
step:728/1775 train_time:27873ms step_avg:38.29ms
step:729/1775 train_time:27932ms step_avg:38.32ms
step:730/1775 train_time:27993ms step_avg:38.35ms
step:731/1775 train_time:28052ms step_avg:38.37ms
step:732/1775 train_time:28112ms step_avg:38.40ms
step:733/1775 train_time:28170ms step_avg:38.43ms
step:734/1775 train_time:28233ms step_avg:38.46ms
step:735/1775 train_time:28291ms step_avg:38.49ms
step:736/1775 train_time:28353ms step_avg:38.52ms
step:737/1775 train_time:28413ms step_avg:38.55ms
step:738/1775 train_time:28475ms step_avg:38.58ms
step:739/1775 train_time:28534ms step_avg:38.61ms
step:740/1775 train_time:28595ms step_avg:38.64ms
step:741/1775 train_time:28654ms step_avg:38.67ms
step:742/1775 train_time:28715ms step_avg:38.70ms
step:743/1775 train_time:28773ms step_avg:38.73ms
step:744/1775 train_time:28834ms step_avg:38.76ms
step:745/1775 train_time:28893ms step_avg:38.78ms
step:746/1775 train_time:28954ms step_avg:38.81ms
step:747/1775 train_time:29013ms step_avg:38.84ms
step:748/1775 train_time:29074ms step_avg:38.87ms
step:749/1775 train_time:29133ms step_avg:38.90ms
step:750/1775 train_time:29194ms step_avg:38.92ms
step:750/1775 val_loss:3.9973 train_time:29264ms step_avg:39.02ms
step:751/1775 train_time:29286ms step_avg:39.00ms
step:752/1775 train_time:29316ms step_avg:38.98ms
step:753/1775 train_time:29375ms step_avg:39.01ms
step:754/1775 train_time:29438ms step_avg:39.04ms
step:755/1775 train_time:29497ms step_avg:39.07ms
step:756/1775 train_time:29558ms step_avg:39.10ms
step:757/1775 train_time:29616ms step_avg:39.12ms
step:758/1775 train_time:29676ms step_avg:39.15ms
step:759/1775 train_time:29734ms step_avg:39.18ms
step:760/1775 train_time:29794ms step_avg:39.20ms
step:761/1775 train_time:29852ms step_avg:39.23ms
step:762/1775 train_time:29912ms step_avg:39.26ms
step:763/1775 train_time:29970ms step_avg:39.28ms
step:764/1775 train_time:30031ms step_avg:39.31ms
step:765/1775 train_time:30089ms step_avg:39.33ms
step:766/1775 train_time:30150ms step_avg:39.36ms
step:767/1775 train_time:30208ms step_avg:39.39ms
step:768/1775 train_time:30271ms step_avg:39.42ms
step:769/1775 train_time:30331ms step_avg:39.44ms
step:770/1775 train_time:30393ms step_avg:39.47ms
step:771/1775 train_time:30454ms step_avg:39.50ms
step:772/1775 train_time:30515ms step_avg:39.53ms
step:773/1775 train_time:30574ms step_avg:39.55ms
step:774/1775 train_time:30634ms step_avg:39.58ms
step:775/1775 train_time:30692ms step_avg:39.60ms
step:776/1775 train_time:30753ms step_avg:39.63ms
step:777/1775 train_time:30811ms step_avg:39.65ms
step:778/1775 train_time:30872ms step_avg:39.68ms
step:779/1775 train_time:30930ms step_avg:39.70ms
step:780/1775 train_time:30990ms step_avg:39.73ms
step:781/1775 train_time:31048ms step_avg:39.75ms
step:782/1775 train_time:31108ms step_avg:39.78ms
step:783/1775 train_time:31167ms step_avg:39.80ms
step:784/1775 train_time:31228ms step_avg:39.83ms
step:785/1775 train_time:31287ms step_avg:39.86ms
step:786/1775 train_time:31349ms step_avg:39.88ms
step:787/1775 train_time:31410ms step_avg:39.91ms
step:788/1775 train_time:31472ms step_avg:39.94ms
step:789/1775 train_time:31531ms step_avg:39.96ms
step:790/1775 train_time:31592ms step_avg:39.99ms
step:791/1775 train_time:31650ms step_avg:40.01ms
step:792/1775 train_time:31711ms step_avg:40.04ms
step:793/1775 train_time:31770ms step_avg:40.06ms
step:794/1775 train_time:31831ms step_avg:40.09ms
step:795/1775 train_time:31889ms step_avg:40.11ms
step:796/1775 train_time:31949ms step_avg:40.14ms
step:797/1775 train_time:32008ms step_avg:40.16ms
step:798/1775 train_time:32069ms step_avg:40.19ms
step:799/1775 train_time:32126ms step_avg:40.21ms
step:800/1775 train_time:32187ms step_avg:40.23ms
step:801/1775 train_time:32246ms step_avg:40.26ms
step:802/1775 train_time:32308ms step_avg:40.28ms
step:803/1775 train_time:32367ms step_avg:40.31ms
step:804/1775 train_time:32429ms step_avg:40.33ms
step:805/1775 train_time:32489ms step_avg:40.36ms
step:806/1775 train_time:32551ms step_avg:40.39ms
step:807/1775 train_time:32609ms step_avg:40.41ms
step:808/1775 train_time:32671ms step_avg:40.43ms
step:809/1775 train_time:32730ms step_avg:40.46ms
step:810/1775 train_time:32791ms step_avg:40.48ms
step:811/1775 train_time:32849ms step_avg:40.50ms
step:812/1775 train_time:32910ms step_avg:40.53ms
step:813/1775 train_time:32969ms step_avg:40.55ms
step:814/1775 train_time:33030ms step_avg:40.58ms
step:815/1775 train_time:33088ms step_avg:40.60ms
step:816/1775 train_time:33148ms step_avg:40.62ms
step:817/1775 train_time:33207ms step_avg:40.64ms
step:818/1775 train_time:33268ms step_avg:40.67ms
step:819/1775 train_time:33326ms step_avg:40.69ms
step:820/1775 train_time:33387ms step_avg:40.72ms
step:821/1775 train_time:33447ms step_avg:40.74ms
step:822/1775 train_time:33509ms step_avg:40.77ms
step:823/1775 train_time:33569ms step_avg:40.79ms
step:824/1775 train_time:33631ms step_avg:40.81ms
step:825/1775 train_time:33690ms step_avg:40.84ms
step:826/1775 train_time:33751ms step_avg:40.86ms
step:827/1775 train_time:33810ms step_avg:40.88ms
step:828/1775 train_time:33870ms step_avg:40.91ms
step:829/1775 train_time:33929ms step_avg:40.93ms
step:830/1775 train_time:33989ms step_avg:40.95ms
step:831/1775 train_time:34048ms step_avg:40.97ms
step:832/1775 train_time:34109ms step_avg:41.00ms
step:833/1775 train_time:34167ms step_avg:41.02ms
step:834/1775 train_time:34228ms step_avg:41.04ms
step:835/1775 train_time:34287ms step_avg:41.06ms
step:836/1775 train_time:34349ms step_avg:41.09ms
step:837/1775 train_time:34407ms step_avg:41.11ms
step:838/1775 train_time:34470ms step_avg:41.13ms
step:839/1775 train_time:34530ms step_avg:41.16ms
step:840/1775 train_time:34592ms step_avg:41.18ms
step:841/1775 train_time:34650ms step_avg:41.20ms
step:842/1775 train_time:34711ms step_avg:41.23ms
step:843/1775 train_time:34771ms step_avg:41.25ms
step:844/1775 train_time:34832ms step_avg:41.27ms
step:845/1775 train_time:34890ms step_avg:41.29ms
step:846/1775 train_time:34952ms step_avg:41.31ms
step:847/1775 train_time:35010ms step_avg:41.33ms
step:848/1775 train_time:35071ms step_avg:41.36ms
step:849/1775 train_time:35131ms step_avg:41.38ms
step:850/1775 train_time:35192ms step_avg:41.40ms
step:851/1775 train_time:35251ms step_avg:41.42ms
step:852/1775 train_time:35313ms step_avg:41.45ms
step:853/1775 train_time:35372ms step_avg:41.47ms
step:854/1775 train_time:35433ms step_avg:41.49ms
step:855/1775 train_time:35491ms step_avg:41.51ms
step:856/1775 train_time:35553ms step_avg:41.53ms
step:857/1775 train_time:35612ms step_avg:41.55ms
step:858/1775 train_time:35672ms step_avg:41.58ms
step:859/1775 train_time:35731ms step_avg:41.60ms
step:860/1775 train_time:35792ms step_avg:41.62ms
step:861/1775 train_time:35851ms step_avg:41.64ms
step:862/1775 train_time:35912ms step_avg:41.66ms
step:863/1775 train_time:35970ms step_avg:41.68ms
step:864/1775 train_time:36031ms step_avg:41.70ms
step:865/1775 train_time:36090ms step_avg:41.72ms
step:866/1775 train_time:36152ms step_avg:41.75ms
step:867/1775 train_time:36211ms step_avg:41.77ms
step:868/1775 train_time:36272ms step_avg:41.79ms
step:869/1775 train_time:36331ms step_avg:41.81ms
step:870/1775 train_time:36392ms step_avg:41.83ms
step:871/1775 train_time:36451ms step_avg:41.85ms
step:872/1775 train_time:36512ms step_avg:41.87ms
step:873/1775 train_time:36572ms step_avg:41.89ms
step:874/1775 train_time:36632ms step_avg:41.91ms
step:875/1775 train_time:36691ms step_avg:41.93ms
step:876/1775 train_time:36752ms step_avg:41.95ms
step:877/1775 train_time:36811ms step_avg:41.97ms
step:878/1775 train_time:36871ms step_avg:41.99ms
step:879/1775 train_time:36930ms step_avg:42.01ms
step:880/1775 train_time:36991ms step_avg:42.03ms
step:881/1775 train_time:37050ms step_avg:42.05ms
step:882/1775 train_time:37110ms step_avg:42.07ms
step:883/1775 train_time:37169ms step_avg:42.09ms
step:884/1775 train_time:37231ms step_avg:42.12ms
step:885/1775 train_time:37289ms step_avg:42.14ms
step:886/1775 train_time:37350ms step_avg:42.16ms
step:887/1775 train_time:37409ms step_avg:42.17ms
step:888/1775 train_time:37471ms step_avg:42.20ms
step:889/1775 train_time:37529ms step_avg:42.21ms
step:890/1775 train_time:37591ms step_avg:42.24ms
step:891/1775 train_time:37650ms step_avg:42.26ms
step:892/1775 train_time:37712ms step_avg:42.28ms
step:893/1775 train_time:37770ms step_avg:42.30ms
step:894/1775 train_time:37831ms step_avg:42.32ms
step:895/1775 train_time:37890ms step_avg:42.34ms
step:896/1775 train_time:37951ms step_avg:42.36ms
step:897/1775 train_time:38010ms step_avg:42.37ms
step:898/1775 train_time:38071ms step_avg:42.40ms
step:899/1775 train_time:38130ms step_avg:42.41ms
step:900/1775 train_time:38191ms step_avg:42.43ms
step:901/1775 train_time:38250ms step_avg:42.45ms
step:902/1775 train_time:38311ms step_avg:42.47ms
step:903/1775 train_time:38370ms step_avg:42.49ms
step:904/1775 train_time:38431ms step_avg:42.51ms
step:905/1775 train_time:38490ms step_avg:42.53ms
step:906/1775 train_time:38551ms step_avg:42.55ms
step:907/1775 train_time:38610ms step_avg:42.57ms
step:908/1775 train_time:38671ms step_avg:42.59ms
step:909/1775 train_time:38729ms step_avg:42.61ms
step:910/1775 train_time:38790ms step_avg:42.63ms
step:911/1775 train_time:38848ms step_avg:42.64ms
step:912/1775 train_time:38909ms step_avg:42.66ms
step:913/1775 train_time:38968ms step_avg:42.68ms
step:914/1775 train_time:39029ms step_avg:42.70ms
step:915/1775 train_time:39087ms step_avg:42.72ms
step:916/1775 train_time:39149ms step_avg:42.74ms
step:917/1775 train_time:39208ms step_avg:42.76ms
step:918/1775 train_time:39270ms step_avg:42.78ms
step:919/1775 train_time:39328ms step_avg:42.79ms
step:920/1775 train_time:39389ms step_avg:42.81ms
step:921/1775 train_time:39448ms step_avg:42.83ms
step:922/1775 train_time:39510ms step_avg:42.85ms
step:923/1775 train_time:39570ms step_avg:42.87ms
step:924/1775 train_time:39630ms step_avg:42.89ms
step:925/1775 train_time:39689ms step_avg:42.91ms
step:926/1775 train_time:39750ms step_avg:42.93ms
step:927/1775 train_time:39809ms step_avg:42.94ms
step:928/1775 train_time:39871ms step_avg:42.96ms
step:929/1775 train_time:39930ms step_avg:42.98ms
step:930/1775 train_time:39991ms step_avg:43.00ms
step:931/1775 train_time:40049ms step_avg:43.02ms
step:932/1775 train_time:40111ms step_avg:43.04ms
step:933/1775 train_time:40170ms step_avg:43.05ms
step:934/1775 train_time:40232ms step_avg:43.07ms
step:935/1775 train_time:40290ms step_avg:43.09ms
step:936/1775 train_time:40351ms step_avg:43.11ms
step:937/1775 train_time:40410ms step_avg:43.13ms
step:938/1775 train_time:40471ms step_avg:43.15ms
step:939/1775 train_time:40530ms step_avg:43.16ms
step:940/1775 train_time:40591ms step_avg:43.18ms
step:941/1775 train_time:40650ms step_avg:43.20ms
step:942/1775 train_time:40711ms step_avg:43.22ms
step:943/1775 train_time:40770ms step_avg:43.23ms
step:944/1775 train_time:40831ms step_avg:43.25ms
step:945/1775 train_time:40889ms step_avg:43.27ms
step:946/1775 train_time:40951ms step_avg:43.29ms
step:947/1775 train_time:41009ms step_avg:43.30ms
step:948/1775 train_time:41071ms step_avg:43.32ms
step:949/1775 train_time:41130ms step_avg:43.34ms
step:950/1775 train_time:41191ms step_avg:43.36ms
step:951/1775 train_time:41250ms step_avg:43.37ms
step:952/1775 train_time:41311ms step_avg:43.39ms
step:953/1775 train_time:41370ms step_avg:43.41ms
step:954/1775 train_time:41430ms step_avg:43.43ms
step:955/1775 train_time:41489ms step_avg:43.44ms
step:956/1775 train_time:41550ms step_avg:43.46ms
step:957/1775 train_time:41609ms step_avg:43.48ms
step:958/1775 train_time:41671ms step_avg:43.50ms
step:959/1775 train_time:41728ms step_avg:43.51ms
step:960/1775 train_time:41790ms step_avg:43.53ms
step:961/1775 train_time:41849ms step_avg:43.55ms
step:962/1775 train_time:41910ms step_avg:43.57ms
step:963/1775 train_time:41968ms step_avg:43.58ms
step:964/1775 train_time:42029ms step_avg:43.60ms
step:965/1775 train_time:42088ms step_avg:43.61ms
step:966/1775 train_time:42150ms step_avg:43.63ms
step:967/1775 train_time:42208ms step_avg:43.65ms
step:968/1775 train_time:42270ms step_avg:43.67ms
step:969/1775 train_time:42329ms step_avg:43.68ms
step:970/1775 train_time:42391ms step_avg:43.70ms
step:971/1775 train_time:42450ms step_avg:43.72ms
step:972/1775 train_time:42512ms step_avg:43.74ms
step:973/1775 train_time:42570ms step_avg:43.75ms
step:974/1775 train_time:42632ms step_avg:43.77ms
step:975/1775 train_time:42690ms step_avg:43.78ms
step:976/1775 train_time:42752ms step_avg:43.80ms
step:977/1775 train_time:42811ms step_avg:43.82ms
step:978/1775 train_time:42872ms step_avg:43.84ms
step:979/1775 train_time:42931ms step_avg:43.85ms
step:980/1775 train_time:42992ms step_avg:43.87ms
step:981/1775 train_time:43050ms step_avg:43.88ms
step:982/1775 train_time:43112ms step_avg:43.90ms
step:983/1775 train_time:43170ms step_avg:43.92ms
step:984/1775 train_time:43232ms step_avg:43.93ms
step:985/1775 train_time:43291ms step_avg:43.95ms
step:986/1775 train_time:43352ms step_avg:43.97ms
step:987/1775 train_time:43410ms step_avg:43.98ms
step:988/1775 train_time:43472ms step_avg:44.00ms
step:989/1775 train_time:43531ms step_avg:44.01ms
step:990/1775 train_time:43592ms step_avg:44.03ms
step:991/1775 train_time:43651ms step_avg:44.05ms
step:992/1775 train_time:43712ms step_avg:44.06ms
step:993/1775 train_time:43771ms step_avg:44.08ms
step:994/1775 train_time:43832ms step_avg:44.10ms
step:995/1775 train_time:43891ms step_avg:44.11ms
step:996/1775 train_time:43952ms step_avg:44.13ms
step:997/1775 train_time:44010ms step_avg:44.14ms
step:998/1775 train_time:44071ms step_avg:44.16ms
step:999/1775 train_time:44131ms step_avg:44.17ms
step:1000/1775 train_time:44192ms step_avg:44.19ms
step:1000/1775 val_loss:3.7324 train_time:44262ms step_avg:44.26ms
step:1001/1775 train_time:44284ms step_avg:44.24ms
step:1002/1775 train_time:44315ms step_avg:44.23ms
step:1003/1775 train_time:44373ms step_avg:44.24ms
step:1004/1775 train_time:44437ms step_avg:44.26ms
step:1005/1775 train_time:44495ms step_avg:44.27ms
step:1006/1775 train_time:44556ms step_avg:44.29ms
step:1007/1775 train_time:44614ms step_avg:44.30ms
step:1008/1775 train_time:44675ms step_avg:44.32ms
step:1009/1775 train_time:44733ms step_avg:44.33ms
step:1010/1775 train_time:44793ms step_avg:44.35ms
step:1011/1775 train_time:44851ms step_avg:44.36ms
step:1012/1775 train_time:44911ms step_avg:44.38ms
step:1013/1775 train_time:44969ms step_avg:44.39ms
step:1014/1775 train_time:45030ms step_avg:44.41ms
step:1015/1775 train_time:45088ms step_avg:44.42ms
step:1016/1775 train_time:45149ms step_avg:44.44ms
step:1017/1775 train_time:45210ms step_avg:44.45ms
step:1018/1775 train_time:45272ms step_avg:44.47ms
step:1019/1775 train_time:45332ms step_avg:44.49ms
step:1020/1775 train_time:45394ms step_avg:44.50ms
step:1021/1775 train_time:45453ms step_avg:44.52ms
step:1022/1775 train_time:45514ms step_avg:44.53ms
step:1023/1775 train_time:45573ms step_avg:44.55ms
step:1024/1775 train_time:45634ms step_avg:44.56ms
step:1025/1775 train_time:45692ms step_avg:44.58ms
step:1026/1775 train_time:45752ms step_avg:44.59ms
step:1027/1775 train_time:45811ms step_avg:44.61ms
step:1028/1775 train_time:45872ms step_avg:44.62ms
step:1029/1775 train_time:45929ms step_avg:44.63ms
step:1030/1775 train_time:45990ms step_avg:44.65ms
step:1031/1775 train_time:46048ms step_avg:44.66ms
step:1032/1775 train_time:46109ms step_avg:44.68ms
step:1033/1775 train_time:46168ms step_avg:44.69ms
step:1034/1775 train_time:46230ms step_avg:44.71ms
step:1035/1775 train_time:46290ms step_avg:44.73ms
step:1036/1775 train_time:46352ms step_avg:44.74ms
step:1037/1775 train_time:46412ms step_avg:44.76ms
step:1038/1775 train_time:46473ms step_avg:44.77ms
step:1039/1775 train_time:46532ms step_avg:44.79ms
step:1040/1775 train_time:46593ms step_avg:44.80ms
step:1041/1775 train_time:46652ms step_avg:44.81ms
step:1042/1775 train_time:46713ms step_avg:44.83ms
step:1043/1775 train_time:46771ms step_avg:44.84ms
step:1044/1775 train_time:46832ms step_avg:44.86ms
step:1045/1775 train_time:46889ms step_avg:44.87ms
step:1046/1775 train_time:46950ms step_avg:44.89ms
step:1047/1775 train_time:47008ms step_avg:44.90ms
step:1048/1775 train_time:47069ms step_avg:44.91ms
step:1049/1775 train_time:47128ms step_avg:44.93ms
step:1050/1775 train_time:47190ms step_avg:44.94ms
step:1051/1775 train_time:47249ms step_avg:44.96ms
step:1052/1775 train_time:47312ms step_avg:44.97ms
step:1053/1775 train_time:47372ms step_avg:44.99ms
step:1054/1775 train_time:47433ms step_avg:45.00ms
step:1055/1775 train_time:47492ms step_avg:45.02ms
step:1056/1775 train_time:47553ms step_avg:45.03ms
step:1057/1775 train_time:47611ms step_avg:45.04ms
step:1058/1775 train_time:47673ms step_avg:45.06ms
step:1059/1775 train_time:47731ms step_avg:45.07ms
step:1060/1775 train_time:47792ms step_avg:45.09ms
step:1061/1775 train_time:47850ms step_avg:45.10ms
step:1062/1775 train_time:47911ms step_avg:45.11ms
step:1063/1775 train_time:47969ms step_avg:45.13ms
step:1064/1775 train_time:48030ms step_avg:45.14ms
step:1065/1775 train_time:48089ms step_avg:45.15ms
step:1066/1775 train_time:48149ms step_avg:45.17ms
step:1067/1775 train_time:48209ms step_avg:45.18ms
step:1068/1775 train_time:48270ms step_avg:45.20ms
step:1069/1775 train_time:48329ms step_avg:45.21ms
step:1070/1775 train_time:48391ms step_avg:45.23ms
step:1071/1775 train_time:48450ms step_avg:45.24ms
step:1072/1775 train_time:48512ms step_avg:45.25ms
step:1073/1775 train_time:48570ms step_avg:45.27ms
step:1074/1775 train_time:48631ms step_avg:45.28ms
step:1075/1775 train_time:48689ms step_avg:45.29ms
step:1076/1775 train_time:48750ms step_avg:45.31ms
step:1077/1775 train_time:48809ms step_avg:45.32ms
step:1078/1775 train_time:48869ms step_avg:45.33ms
step:1079/1775 train_time:48928ms step_avg:45.35ms
step:1080/1775 train_time:48988ms step_avg:45.36ms
step:1081/1775 train_time:49047ms step_avg:45.37ms
step:1082/1775 train_time:49108ms step_avg:45.39ms
step:1083/1775 train_time:49166ms step_avg:45.40ms
step:1084/1775 train_time:49228ms step_avg:45.41ms
step:1085/1775 train_time:49288ms step_avg:45.43ms
step:1086/1775 train_time:49349ms step_avg:45.44ms
step:1087/1775 train_time:49408ms step_avg:45.45ms
step:1088/1775 train_time:49470ms step_avg:45.47ms
step:1089/1775 train_time:49529ms step_avg:45.48ms
step:1090/1775 train_time:49590ms step_avg:45.50ms
step:1091/1775 train_time:49649ms step_avg:45.51ms
step:1092/1775 train_time:49710ms step_avg:45.52ms
step:1093/1775 train_time:49769ms step_avg:45.53ms
step:1094/1775 train_time:49830ms step_avg:45.55ms
step:1095/1775 train_time:49889ms step_avg:45.56ms
step:1096/1775 train_time:49949ms step_avg:45.57ms
step:1097/1775 train_time:50008ms step_avg:45.59ms
step:1098/1775 train_time:50069ms step_avg:45.60ms
step:1099/1775 train_time:50129ms step_avg:45.61ms
step:1100/1775 train_time:50189ms step_avg:45.63ms
step:1101/1775 train_time:50248ms step_avg:45.64ms
step:1102/1775 train_time:50310ms step_avg:45.65ms
step:1103/1775 train_time:50369ms step_avg:45.67ms
step:1104/1775 train_time:50430ms step_avg:45.68ms
step:1105/1775 train_time:50489ms step_avg:45.69ms
step:1106/1775 train_time:50550ms step_avg:45.71ms
step:1107/1775 train_time:50609ms step_avg:45.72ms
step:1108/1775 train_time:50670ms step_avg:45.73ms
step:1109/1775 train_time:50730ms step_avg:45.74ms
step:1110/1775 train_time:50790ms step_avg:45.76ms
step:1111/1775 train_time:50848ms step_avg:45.77ms
step:1112/1775 train_time:50910ms step_avg:45.78ms
step:1113/1775 train_time:50968ms step_avg:45.79ms
step:1114/1775 train_time:51029ms step_avg:45.81ms
step:1115/1775 train_time:51088ms step_avg:45.82ms
step:1116/1775 train_time:51149ms step_avg:45.83ms
step:1117/1775 train_time:51207ms step_avg:45.84ms
step:1118/1775 train_time:51269ms step_avg:45.86ms
step:1119/1775 train_time:51328ms step_avg:45.87ms
step:1120/1775 train_time:51390ms step_avg:45.88ms
step:1121/1775 train_time:51449ms step_avg:45.90ms
step:1122/1775 train_time:51510ms step_avg:45.91ms
step:1123/1775 train_time:51568ms step_avg:45.92ms
step:1124/1775 train_time:51630ms step_avg:45.93ms
step:1125/1775 train_time:51689ms step_avg:45.95ms
step:1126/1775 train_time:51750ms step_avg:45.96ms
step:1127/1775 train_time:51808ms step_avg:45.97ms
step:1128/1775 train_time:51869ms step_avg:45.98ms
step:1129/1775 train_time:51928ms step_avg:45.99ms
step:1130/1775 train_time:51989ms step_avg:46.01ms
step:1131/1775 train_time:52047ms step_avg:46.02ms
step:1132/1775 train_time:52109ms step_avg:46.03ms
step:1133/1775 train_time:52167ms step_avg:46.04ms
step:1134/1775 train_time:52228ms step_avg:46.06ms
step:1135/1775 train_time:52287ms step_avg:46.07ms
step:1136/1775 train_time:52348ms step_avg:46.08ms
step:1137/1775 train_time:52407ms step_avg:46.09ms
step:1138/1775 train_time:52468ms step_avg:46.11ms
step:1139/1775 train_time:52528ms step_avg:46.12ms
step:1140/1775 train_time:52589ms step_avg:46.13ms
step:1141/1775 train_time:52648ms step_avg:46.14ms
step:1142/1775 train_time:52709ms step_avg:46.16ms
step:1143/1775 train_time:52768ms step_avg:46.17ms
step:1144/1775 train_time:52829ms step_avg:46.18ms
step:1145/1775 train_time:52888ms step_avg:46.19ms
step:1146/1775 train_time:52949ms step_avg:46.20ms
step:1147/1775 train_time:53008ms step_avg:46.21ms
step:1148/1775 train_time:53068ms step_avg:46.23ms
step:1149/1775 train_time:53127ms step_avg:46.24ms
step:1150/1775 train_time:53189ms step_avg:46.25ms
step:1151/1775 train_time:53248ms step_avg:46.26ms
step:1152/1775 train_time:53309ms step_avg:46.28ms
step:1153/1775 train_time:53368ms step_avg:46.29ms
step:1154/1775 train_time:53430ms step_avg:46.30ms
step:1155/1775 train_time:53488ms step_avg:46.31ms
step:1156/1775 train_time:53550ms step_avg:46.32ms
step:1157/1775 train_time:53609ms step_avg:46.33ms
step:1158/1775 train_time:53673ms step_avg:46.35ms
step:1159/1775 train_time:53757ms step_avg:46.38ms
step:1160/1775 train_time:53845ms step_avg:46.42ms
step:1161/1775 train_time:53929ms step_avg:46.45ms
step:1162/1775 train_time:54015ms step_avg:46.48ms
step:1163/1775 train_time:54100ms step_avg:46.52ms
step:1164/1775 train_time:54187ms step_avg:46.55ms
step:1165/1775 train_time:54271ms step_avg:46.58ms
step:1166/1775 train_time:54359ms step_avg:46.62ms
step:1167/1775 train_time:54443ms step_avg:46.65ms
step:1168/1775 train_time:54529ms step_avg:46.69ms
step:1169/1775 train_time:54614ms step_avg:46.72ms
step:1170/1775 train_time:54702ms step_avg:46.75ms
step:1171/1775 train_time:54786ms step_avg:46.79ms
step:1172/1775 train_time:54873ms step_avg:46.82ms
step:1173/1775 train_time:54958ms step_avg:46.85ms
step:1174/1775 train_time:55045ms step_avg:46.89ms
step:1175/1775 train_time:55129ms step_avg:46.92ms
step:1176/1775 train_time:55217ms step_avg:46.95ms
step:1177/1775 train_time:55302ms step_avg:46.99ms
step:1178/1775 train_time:55389ms step_avg:47.02ms
step:1179/1775 train_time:55474ms step_avg:47.05ms
step:1180/1775 train_time:55562ms step_avg:47.09ms
step:1181/1775 train_time:55647ms step_avg:47.12ms
step:1182/1775 train_time:55733ms step_avg:47.15ms
step:1183/1775 train_time:55818ms step_avg:47.18ms
step:1184/1775 train_time:55905ms step_avg:47.22ms
step:1185/1775 train_time:55989ms step_avg:47.25ms
step:1186/1775 train_time:56077ms step_avg:47.28ms
step:1187/1775 train_time:56162ms step_avg:47.31ms
step:1188/1775 train_time:56248ms step_avg:47.35ms
step:1189/1775 train_time:56334ms step_avg:47.38ms
step:1190/1775 train_time:56423ms step_avg:47.41ms
step:1191/1775 train_time:56506ms step_avg:47.44ms
step:1192/1775 train_time:56593ms step_avg:47.48ms
step:1193/1775 train_time:56678ms step_avg:47.51ms
step:1194/1775 train_time:56765ms step_avg:47.54ms
step:1195/1775 train_time:56849ms step_avg:47.57ms
step:1196/1775 train_time:56936ms step_avg:47.61ms
step:1197/1775 train_time:57022ms step_avg:47.64ms
step:1198/1775 train_time:57108ms step_avg:47.67ms
step:1199/1775 train_time:57194ms step_avg:47.70ms
step:1200/1775 train_time:57282ms step_avg:47.73ms
step:1201/1775 train_time:57367ms step_avg:47.77ms
step:1202/1775 train_time:57454ms step_avg:47.80ms
step:1203/1775 train_time:57538ms step_avg:47.83ms
step:1204/1775 train_time:57626ms step_avg:47.86ms
step:1205/1775 train_time:57709ms step_avg:47.89ms
step:1206/1775 train_time:57798ms step_avg:47.93ms
step:1207/1775 train_time:57883ms step_avg:47.96ms
step:1208/1775 train_time:57969ms step_avg:47.99ms
step:1209/1775 train_time:58052ms step_avg:48.02ms
step:1210/1775 train_time:58140ms step_avg:48.05ms
step:1211/1775 train_time:58226ms step_avg:48.08ms
step:1212/1775 train_time:58312ms step_avg:48.11ms
step:1213/1775 train_time:58395ms step_avg:48.14ms
step:1214/1775 train_time:58484ms step_avg:48.17ms
step:1215/1775 train_time:58567ms step_avg:48.20ms
step:1216/1775 train_time:58654ms step_avg:48.23ms
step:1217/1775 train_time:58739ms step_avg:48.27ms
step:1218/1775 train_time:58826ms step_avg:48.30ms
step:1219/1775 train_time:58909ms step_avg:48.33ms
step:1220/1775 train_time:58997ms step_avg:48.36ms
step:1221/1775 train_time:59081ms step_avg:48.39ms
step:1222/1775 train_time:59168ms step_avg:48.42ms
step:1223/1775 train_time:59253ms step_avg:48.45ms
step:1224/1775 train_time:59340ms step_avg:48.48ms
step:1225/1775 train_time:59425ms step_avg:48.51ms
step:1226/1775 train_time:59511ms step_avg:48.54ms
step:1227/1775 train_time:59596ms step_avg:48.57ms
step:1228/1775 train_time:59684ms step_avg:48.60ms
step:1229/1775 train_time:59768ms step_avg:48.63ms
step:1230/1775 train_time:59854ms step_avg:48.66ms
step:1231/1775 train_time:59939ms step_avg:48.69ms
step:1232/1775 train_time:60026ms step_avg:48.72ms
step:1233/1775 train_time:60110ms step_avg:48.75ms
step:1234/1775 train_time:60198ms step_avg:48.78ms
step:1235/1775 train_time:60283ms step_avg:48.81ms
step:1236/1775 train_time:60369ms step_avg:48.84ms
step:1237/1775 train_time:60453ms step_avg:48.87ms
step:1238/1775 train_time:60541ms step_avg:48.90ms
step:1239/1775 train_time:60626ms step_avg:48.93ms
step:1240/1775 train_time:60713ms step_avg:48.96ms
step:1241/1775 train_time:60798ms step_avg:48.99ms
step:1242/1775 train_time:60885ms step_avg:49.02ms
step:1243/1775 train_time:60970ms step_avg:49.05ms
step:1244/1775 train_time:61058ms step_avg:49.08ms
step:1245/1775 train_time:61143ms step_avg:49.11ms
step:1246/1775 train_time:61230ms step_avg:49.14ms
step:1247/1775 train_time:61314ms step_avg:49.17ms
step:1248/1775 train_time:61401ms step_avg:49.20ms
step:1249/1775 train_time:61485ms step_avg:49.23ms
step:1250/1775 train_time:61572ms step_avg:49.26ms
step:1250/1775 val_loss:3.5044 train_time:61673ms step_avg:49.34ms
step:1251/1775 train_time:61694ms step_avg:49.32ms
step:1252/1775 train_time:61748ms step_avg:49.32ms
step:1253/1775 train_time:61833ms step_avg:49.35ms
step:1254/1775 train_time:61922ms step_avg:49.38ms
step:1255/1775 train_time:62007ms step_avg:49.41ms
step:1256/1775 train_time:62093ms step_avg:49.44ms
step:1257/1775 train_time:62177ms step_avg:49.46ms
step:1258/1775 train_time:62263ms step_avg:49.49ms
step:1259/1775 train_time:62347ms step_avg:49.52ms
step:1260/1775 train_time:62434ms step_avg:49.55ms
step:1261/1775 train_time:62519ms step_avg:49.58ms
step:1262/1775 train_time:62608ms step_avg:49.61ms
step:1263/1775 train_time:62694ms step_avg:49.64ms
step:1264/1775 train_time:62782ms step_avg:49.67ms
step:1265/1775 train_time:62868ms step_avg:49.70ms
step:1266/1775 train_time:62955ms step_avg:49.73ms
step:1267/1775 train_time:63039ms step_avg:49.75ms
step:1268/1775 train_time:63126ms step_avg:49.78ms
step:1269/1775 train_time:63211ms step_avg:49.81ms
step:1270/1775 train_time:63298ms step_avg:49.84ms
step:1271/1775 train_time:63382ms step_avg:49.87ms
step:1272/1775 train_time:63468ms step_avg:49.90ms
step:1273/1775 train_time:63552ms step_avg:49.92ms
step:1274/1775 train_time:63641ms step_avg:49.95ms
step:1275/1775 train_time:63727ms step_avg:49.98ms
step:1276/1775 train_time:63814ms step_avg:50.01ms
step:1277/1775 train_time:63898ms step_avg:50.04ms
step:1278/1775 train_time:63985ms step_avg:50.07ms
step:1279/1775 train_time:64069ms step_avg:50.09ms
step:1280/1775 train_time:64156ms step_avg:50.12ms
step:1281/1775 train_time:64240ms step_avg:50.15ms
step:1282/1775 train_time:64327ms step_avg:50.18ms
step:1283/1775 train_time:64411ms step_avg:50.20ms
step:1284/1775 train_time:64499ms step_avg:50.23ms
step:1285/1775 train_time:64585ms step_avg:50.26ms
step:1286/1775 train_time:64672ms step_avg:50.29ms
step:1287/1775 train_time:64758ms step_avg:50.32ms
step:1288/1775 train_time:64847ms step_avg:50.35ms
step:1289/1775 train_time:64929ms step_avg:50.37ms
step:1290/1775 train_time:65017ms step_avg:50.40ms
step:1291/1775 train_time:65102ms step_avg:50.43ms
step:1292/1775 train_time:65189ms step_avg:50.46ms
step:1293/1775 train_time:65274ms step_avg:50.48ms
step:1294/1775 train_time:65360ms step_avg:50.51ms
step:1295/1775 train_time:65445ms step_avg:50.54ms
step:1296/1775 train_time:65530ms step_avg:50.56ms
step:1297/1775 train_time:65615ms step_avg:50.59ms
step:1298/1775 train_time:65704ms step_avg:50.62ms
step:1299/1775 train_time:65789ms step_avg:50.65ms
step:1300/1775 train_time:65877ms step_avg:50.67ms
step:1301/1775 train_time:65961ms step_avg:50.70ms
step:1302/1775 train_time:66049ms step_avg:50.73ms
step:1303/1775 train_time:66132ms step_avg:50.75ms
step:1304/1775 train_time:66219ms step_avg:50.78ms
step:1305/1775 train_time:66304ms step_avg:50.81ms
step:1306/1775 train_time:66391ms step_avg:50.84ms
step:1307/1775 train_time:66474ms step_avg:50.86ms
step:1308/1775 train_time:66562ms step_avg:50.89ms
step:1309/1775 train_time:66648ms step_avg:50.92ms
step:1310/1775 train_time:66733ms step_avg:50.94ms
step:1311/1775 train_time:66819ms step_avg:50.97ms
step:1312/1775 train_time:66907ms step_avg:51.00ms
step:1313/1775 train_time:66991ms step_avg:51.02ms
step:1314/1775 train_time:67080ms step_avg:51.05ms
step:1315/1775 train_time:67165ms step_avg:51.08ms
step:1316/1775 train_time:67251ms step_avg:51.10ms
step:1317/1775 train_time:67335ms step_avg:51.13ms
step:1318/1775 train_time:67423ms step_avg:51.16ms
step:1319/1775 train_time:67508ms step_avg:51.18ms
step:1320/1775 train_time:67594ms step_avg:51.21ms
step:1321/1775 train_time:67679ms step_avg:51.23ms
step:1322/1775 train_time:67767ms step_avg:51.26ms
step:1323/1775 train_time:67852ms step_avg:51.29ms
step:1324/1775 train_time:67940ms step_avg:51.31ms
step:1325/1775 train_time:68025ms step_avg:51.34ms
step:1326/1775 train_time:68112ms step_avg:51.37ms
step:1327/1775 train_time:68197ms step_avg:51.39ms
step:1328/1775 train_time:68284ms step_avg:51.42ms
step:1329/1775 train_time:68368ms step_avg:51.44ms
step:1330/1775 train_time:68455ms step_avg:51.47ms
step:1331/1775 train_time:68539ms step_avg:51.49ms
step:1332/1775 train_time:68627ms step_avg:51.52ms
step:1333/1775 train_time:68710ms step_avg:51.55ms
step:1334/1775 train_time:68797ms step_avg:51.57ms
step:1335/1775 train_time:68882ms step_avg:51.60ms
step:1336/1775 train_time:68970ms step_avg:51.62ms
step:1337/1775 train_time:69055ms step_avg:51.65ms
step:1338/1775 train_time:69142ms step_avg:51.68ms
step:1339/1775 train_time:69226ms step_avg:51.70ms
step:1340/1775 train_time:69312ms step_avg:51.73ms
step:1341/1775 train_time:69396ms step_avg:51.75ms
step:1342/1775 train_time:69484ms step_avg:51.78ms
step:1343/1775 train_time:69568ms step_avg:51.80ms
step:1344/1775 train_time:69654ms step_avg:51.83ms
step:1345/1775 train_time:69739ms step_avg:51.85ms
step:1346/1775 train_time:69827ms step_avg:51.88ms
step:1347/1775 train_time:69911ms step_avg:51.90ms
step:1348/1775 train_time:69998ms step_avg:51.93ms
step:1349/1775 train_time:70083ms step_avg:51.95ms
step:1350/1775 train_time:70170ms step_avg:51.98ms
step:1351/1775 train_time:70254ms step_avg:52.00ms
step:1352/1775 train_time:70342ms step_avg:52.03ms
step:1353/1775 train_time:70426ms step_avg:52.05ms
step:1354/1775 train_time:70511ms step_avg:52.08ms
step:1355/1775 train_time:70597ms step_avg:52.10ms
step:1356/1775 train_time:70685ms step_avg:52.13ms
step:1357/1775 train_time:70769ms step_avg:52.15ms
step:1358/1775 train_time:70856ms step_avg:52.18ms
step:1359/1775 train_time:70942ms step_avg:52.20ms
step:1360/1775 train_time:71028ms step_avg:52.23ms
step:1361/1775 train_time:71112ms step_avg:52.25ms
step:1362/1775 train_time:71200ms step_avg:52.28ms
step:1363/1775 train_time:71285ms step_avg:52.30ms
step:1364/1775 train_time:71371ms step_avg:52.32ms
step:1365/1775 train_time:71456ms step_avg:52.35ms
step:1366/1775 train_time:71543ms step_avg:52.37ms
step:1367/1775 train_time:71627ms step_avg:52.40ms
step:1368/1775 train_time:71713ms step_avg:52.42ms
step:1369/1775 train_time:71799ms step_avg:52.45ms
step:1370/1775 train_time:71887ms step_avg:52.47ms
step:1371/1775 train_time:71970ms step_avg:52.49ms
step:1372/1775 train_time:72059ms step_avg:52.52ms
step:1373/1775 train_time:72142ms step_avg:52.54ms
step:1374/1775 train_time:72228ms step_avg:52.57ms
step:1375/1775 train_time:72313ms step_avg:52.59ms
step:1376/1775 train_time:72400ms step_avg:52.62ms
step:1377/1775 train_time:72485ms step_avg:52.64ms
step:1378/1775 train_time:72571ms step_avg:52.66ms
step:1379/1775 train_time:72656ms step_avg:52.69ms
step:1380/1775 train_time:72745ms step_avg:52.71ms
step:1381/1775 train_time:72828ms step_avg:52.74ms
step:1382/1775 train_time:72915ms step_avg:52.76ms
step:1383/1775 train_time:73000ms step_avg:52.78ms
step:1384/1775 train_time:73088ms step_avg:52.81ms
step:1385/1775 train_time:73172ms step_avg:52.83ms
step:1386/1775 train_time:73260ms step_avg:52.86ms
step:1387/1775 train_time:73343ms step_avg:52.88ms
step:1388/1775 train_time:73431ms step_avg:52.90ms
step:1389/1775 train_time:73516ms step_avg:52.93ms
step:1390/1775 train_time:73604ms step_avg:52.95ms
step:1391/1775 train_time:73689ms step_avg:52.98ms
step:1392/1775 train_time:73774ms step_avg:53.00ms
step:1393/1775 train_time:73859ms step_avg:53.02ms
step:1394/1775 train_time:73948ms step_avg:53.05ms
step:1395/1775 train_time:74031ms step_avg:53.07ms
step:1396/1775 train_time:74118ms step_avg:53.09ms
step:1397/1775 train_time:74202ms step_avg:53.12ms
step:1398/1775 train_time:74290ms step_avg:53.14ms
step:1399/1775 train_time:74374ms step_avg:53.16ms
step:1400/1775 train_time:74462ms step_avg:53.19ms
step:1401/1775 train_time:74547ms step_avg:53.21ms
step:1402/1775 train_time:74633ms step_avg:53.23ms
step:1403/1775 train_time:74718ms step_avg:53.26ms
step:1404/1775 train_time:74805ms step_avg:53.28ms
step:1405/1775 train_time:74889ms step_avg:53.30ms
step:1406/1775 train_time:74977ms step_avg:53.33ms
step:1407/1775 train_time:75060ms step_avg:53.35ms
step:1408/1775 train_time:75149ms step_avg:53.37ms
step:1409/1775 train_time:75233ms step_avg:53.39ms
step:1410/1775 train_time:75322ms step_avg:53.42ms
step:1411/1775 train_time:75406ms step_avg:53.44ms
step:1412/1775 train_time:75492ms step_avg:53.46ms
step:1413/1775 train_time:75576ms step_avg:53.49ms
step:1414/1775 train_time:75666ms step_avg:53.51ms
step:1415/1775 train_time:75750ms step_avg:53.53ms
step:1416/1775 train_time:75836ms step_avg:53.56ms
step:1417/1775 train_time:75921ms step_avg:53.58ms
step:1418/1775 train_time:76009ms step_avg:53.60ms
step:1419/1775 train_time:76093ms step_avg:53.62ms
step:1420/1775 train_time:76181ms step_avg:53.65ms
step:1421/1775 train_time:76266ms step_avg:53.67ms
step:1422/1775 train_time:76352ms step_avg:53.69ms
step:1423/1775 train_time:76437ms step_avg:53.72ms
step:1424/1775 train_time:76524ms step_avg:53.74ms
step:1425/1775 train_time:76608ms step_avg:53.76ms
step:1426/1775 train_time:76696ms step_avg:53.78ms
step:1427/1775 train_time:76781ms step_avg:53.81ms
step:1428/1775 train_time:76868ms step_avg:53.83ms
step:1429/1775 train_time:76951ms step_avg:53.85ms
step:1430/1775 train_time:77039ms step_avg:53.87ms
step:1431/1775 train_time:77123ms step_avg:53.89ms
step:1432/1775 train_time:77210ms step_avg:53.92ms
step:1433/1775 train_time:77296ms step_avg:53.94ms
step:1434/1775 train_time:77383ms step_avg:53.96ms
step:1435/1775 train_time:77467ms step_avg:53.98ms
step:1436/1775 train_time:77554ms step_avg:54.01ms
step:1437/1775 train_time:77639ms step_avg:54.03ms
step:1438/1775 train_time:77726ms step_avg:54.05ms
step:1439/1775 train_time:77810ms step_avg:54.07ms
step:1440/1775 train_time:77896ms step_avg:54.09ms
step:1441/1775 train_time:77982ms step_avg:54.12ms
step:1442/1775 train_time:78070ms step_avg:54.14ms
step:1443/1775 train_time:78153ms step_avg:54.16ms
step:1444/1775 train_time:78241ms step_avg:54.18ms
step:1445/1775 train_time:78326ms step_avg:54.20ms
step:1446/1775 train_time:78413ms step_avg:54.23ms
step:1447/1775 train_time:78497ms step_avg:54.25ms
step:1448/1775 train_time:78587ms step_avg:54.27ms
step:1449/1775 train_time:78671ms step_avg:54.29ms
step:1450/1775 train_time:78758ms step_avg:54.32ms
step:1451/1775 train_time:78842ms step_avg:54.34ms
step:1452/1775 train_time:78929ms step_avg:54.36ms
step:1453/1775 train_time:79014ms step_avg:54.38ms
step:1454/1775 train_time:79102ms step_avg:54.40ms
step:1455/1775 train_time:79186ms step_avg:54.42ms
step:1456/1775 train_time:79272ms step_avg:54.45ms
step:1457/1775 train_time:79357ms step_avg:54.47ms
step:1458/1775 train_time:79444ms step_avg:54.49ms
step:1459/1775 train_time:79528ms step_avg:54.51ms
step:1460/1775 train_time:79615ms step_avg:54.53ms
step:1461/1775 train_time:79702ms step_avg:54.55ms
step:1462/1775 train_time:79789ms step_avg:54.58ms
step:1463/1775 train_time:79874ms step_avg:54.60ms
step:1464/1775 train_time:79962ms step_avg:54.62ms
step:1465/1775 train_time:80048ms step_avg:54.64ms
step:1466/1775 train_time:80134ms step_avg:54.66ms
step:1467/1775 train_time:80218ms step_avg:54.68ms
step:1468/1775 train_time:80306ms step_avg:54.70ms
step:1469/1775 train_time:80390ms step_avg:54.72ms
step:1470/1775 train_time:80477ms step_avg:54.75ms
step:1471/1775 train_time:80561ms step_avg:54.77ms
step:1472/1775 train_time:80649ms step_avg:54.79ms
step:1473/1775 train_time:80733ms step_avg:54.81ms
step:1474/1775 train_time:80820ms step_avg:54.83ms
step:1475/1775 train_time:80905ms step_avg:54.85ms
step:1476/1775 train_time:80992ms step_avg:54.87ms
step:1477/1775 train_time:81078ms step_avg:54.89ms
step:1478/1775 train_time:81166ms step_avg:54.92ms
step:1479/1775 train_time:81249ms step_avg:54.94ms
step:1480/1775 train_time:81336ms step_avg:54.96ms
step:1481/1775 train_time:81421ms step_avg:54.98ms
step:1482/1775 train_time:81509ms step_avg:55.00ms
step:1483/1775 train_time:81593ms step_avg:55.02ms
step:1484/1775 train_time:81679ms step_avg:55.04ms
step:1485/1775 train_time:81765ms step_avg:55.06ms
step:1486/1775 train_time:81852ms step_avg:55.08ms
step:1487/1775 train_time:81937ms step_avg:55.10ms
step:1488/1775 train_time:82027ms step_avg:55.13ms
step:1489/1775 train_time:82110ms step_avg:55.14ms
step:1490/1775 train_time:82198ms step_avg:55.17ms
step:1491/1775 train_time:82283ms step_avg:55.19ms
step:1492/1775 train_time:82369ms step_avg:55.21ms
step:1493/1775 train_time:82454ms step_avg:55.23ms
step:1494/1775 train_time:82541ms step_avg:55.25ms
step:1495/1775 train_time:82626ms step_avg:55.27ms
step:1496/1775 train_time:82712ms step_avg:55.29ms
step:1497/1775 train_time:82797ms step_avg:55.31ms
step:1498/1775 train_time:82886ms step_avg:55.33ms
step:1499/1775 train_time:82970ms step_avg:55.35ms
step:1500/1775 train_time:83058ms step_avg:55.37ms
step:1500/1775 val_loss:3.3762 train_time:83157ms step_avg:55.44ms
step:1501/1775 train_time:83178ms step_avg:55.42ms
step:1502/1775 train_time:83231ms step_avg:55.41ms
step:1503/1775 train_time:83317ms step_avg:55.43ms
step:1504/1775 train_time:83406ms step_avg:55.46ms
step:1505/1775 train_time:83490ms step_avg:55.48ms
step:1506/1775 train_time:83576ms step_avg:55.50ms
step:1507/1775 train_time:83660ms step_avg:55.51ms
step:1508/1775 train_time:83746ms step_avg:55.53ms
step:1509/1775 train_time:83831ms step_avg:55.55ms
step:1510/1775 train_time:83916ms step_avg:55.57ms
step:1511/1775 train_time:84000ms step_avg:55.59ms
step:1512/1775 train_time:84090ms step_avg:55.62ms
step:1513/1775 train_time:84176ms step_avg:55.64ms
step:1514/1775 train_time:84265ms step_avg:55.66ms
step:1515/1775 train_time:84349ms step_avg:55.68ms
step:1516/1775 train_time:84436ms step_avg:55.70ms
step:1517/1775 train_time:84522ms step_avg:55.72ms
step:1518/1775 train_time:84609ms step_avg:55.74ms
step:1519/1775 train_time:84692ms step_avg:55.75ms
step:1520/1775 train_time:84779ms step_avg:55.78ms
step:1521/1775 train_time:84863ms step_avg:55.79ms
step:1522/1775 train_time:84949ms step_avg:55.81ms
step:1523/1775 train_time:85034ms step_avg:55.83ms
step:1524/1775 train_time:85121ms step_avg:55.85ms
step:1525/1775 train_time:85207ms step_avg:55.87ms
step:1526/1775 train_time:85294ms step_avg:55.89ms
step:1527/1775 train_time:85379ms step_avg:55.91ms
step:1528/1775 train_time:85467ms step_avg:55.93ms
step:1529/1775 train_time:85552ms step_avg:55.95ms
step:1530/1775 train_time:85638ms step_avg:55.97ms
step:1531/1775 train_time:85722ms step_avg:55.99ms
step:1532/1775 train_time:85810ms step_avg:56.01ms
step:1533/1775 train_time:85893ms step_avg:56.03ms
step:1534/1775 train_time:85980ms step_avg:56.05ms
step:1535/1775 train_time:86065ms step_avg:56.07ms
step:1536/1775 train_time:86154ms step_avg:56.09ms
step:1537/1775 train_time:86240ms step_avg:56.11ms
step:1538/1775 train_time:86328ms step_avg:56.13ms
step:1539/1775 train_time:86412ms step_avg:56.15ms
step:1540/1775 train_time:86500ms step_avg:56.17ms
step:1541/1775 train_time:86585ms step_avg:56.19ms
step:1542/1775 train_time:86673ms step_avg:56.21ms
step:1543/1775 train_time:86755ms step_avg:56.22ms
step:1544/1775 train_time:86842ms step_avg:56.24ms
step:1545/1775 train_time:86927ms step_avg:56.26ms
step:1546/1775 train_time:87014ms step_avg:56.28ms
step:1547/1775 train_time:87098ms step_avg:56.30ms
step:1548/1775 train_time:87186ms step_avg:56.32ms
step:1549/1775 train_time:87272ms step_avg:56.34ms
step:1550/1775 train_time:87359ms step_avg:56.36ms
step:1551/1775 train_time:87443ms step_avg:56.38ms
step:1552/1775 train_time:87532ms step_avg:56.40ms
step:1553/1775 train_time:87615ms step_avg:56.42ms
step:1554/1775 train_time:87702ms step_avg:56.44ms
step:1555/1775 train_time:87786ms step_avg:56.45ms
step:1556/1775 train_time:87873ms step_avg:56.47ms
step:1557/1775 train_time:87957ms step_avg:56.49ms
step:1558/1775 train_time:88044ms step_avg:56.51ms
step:1559/1775 train_time:88129ms step_avg:56.53ms
step:1560/1775 train_time:88216ms step_avg:56.55ms
step:1561/1775 train_time:88302ms step_avg:56.57ms
step:1562/1775 train_time:88389ms step_avg:56.59ms
step:1563/1775 train_time:88474ms step_avg:56.61ms
step:1564/1775 train_time:88560ms step_avg:56.62ms
step:1565/1775 train_time:88644ms step_avg:56.64ms
step:1566/1775 train_time:88732ms step_avg:56.66ms
step:1567/1775 train_time:88814ms step_avg:56.68ms
step:1568/1775 train_time:88902ms step_avg:56.70ms
step:1569/1775 train_time:88987ms step_avg:56.72ms
step:1570/1775 train_time:89075ms step_avg:56.74ms
step:1571/1775 train_time:89158ms step_avg:56.75ms
step:1572/1775 train_time:89247ms step_avg:56.77ms
step:1573/1775 train_time:89333ms step_avg:56.79ms
step:1574/1775 train_time:89419ms step_avg:56.81ms
step:1575/1775 train_time:89503ms step_avg:56.83ms
step:1576/1775 train_time:89591ms step_avg:56.85ms
step:1577/1775 train_time:89675ms step_avg:56.86ms
step:1578/1775 train_time:89762ms step_avg:56.88ms
step:1579/1775 train_time:89845ms step_avg:56.90ms
step:1580/1775 train_time:89933ms step_avg:56.92ms
step:1581/1775 train_time:90017ms step_avg:56.94ms
step:1582/1775 train_time:90104ms step_avg:56.96ms
step:1583/1775 train_time:90190ms step_avg:56.97ms
step:1584/1775 train_time:90277ms step_avg:56.99ms
step:1585/1775 train_time:90361ms step_avg:57.01ms
step:1586/1775 train_time:90450ms step_avg:57.03ms
step:1587/1775 train_time:90533ms step_avg:57.05ms
step:1588/1775 train_time:90621ms step_avg:57.07ms
step:1589/1775 train_time:90705ms step_avg:57.08ms
step:1590/1775 train_time:90793ms step_avg:57.10ms
step:1591/1775 train_time:90877ms step_avg:57.12ms
step:1592/1775 train_time:90965ms step_avg:57.14ms
step:1593/1775 train_time:91049ms step_avg:57.16ms
step:1594/1775 train_time:91135ms step_avg:57.17ms
step:1595/1775 train_time:91220ms step_avg:57.19ms
step:1596/1775 train_time:91309ms step_avg:57.21ms
step:1597/1775 train_time:91393ms step_avg:57.23ms
step:1598/1775 train_time:91479ms step_avg:57.25ms
step:1599/1775 train_time:91563ms step_avg:57.26ms
step:1600/1775 train_time:91652ms step_avg:57.28ms
step:1601/1775 train_time:91735ms step_avg:57.30ms
step:1602/1775 train_time:91822ms step_avg:57.32ms
step:1603/1775 train_time:91906ms step_avg:57.33ms
step:1604/1775 train_time:91993ms step_avg:57.35ms
step:1605/1775 train_time:92078ms step_avg:57.37ms
step:1606/1775 train_time:92165ms step_avg:57.39ms
step:1607/1775 train_time:92250ms step_avg:57.41ms
step:1608/1775 train_time:92337ms step_avg:57.42ms
step:1609/1775 train_time:92423ms step_avg:57.44ms
step:1610/1775 train_time:92513ms step_avg:57.46ms
step:1611/1775 train_time:92597ms step_avg:57.48ms
step:1612/1775 train_time:92683ms step_avg:57.50ms
step:1613/1775 train_time:92768ms step_avg:57.51ms
step:1614/1775 train_time:92854ms step_avg:57.53ms
step:1615/1775 train_time:92939ms step_avg:57.55ms
step:1616/1775 train_time:93026ms step_avg:57.57ms
step:1617/1775 train_time:93111ms step_avg:57.58ms
step:1618/1775 train_time:93197ms step_avg:57.60ms
step:1619/1775 train_time:93284ms step_avg:57.62ms
step:1620/1775 train_time:93373ms step_avg:57.64ms
step:1621/1775 train_time:93456ms step_avg:57.65ms
step:1622/1775 train_time:93542ms step_avg:57.67ms
step:1623/1775 train_time:93626ms step_avg:57.69ms
step:1624/1775 train_time:93714ms step_avg:57.71ms
step:1625/1775 train_time:93798ms step_avg:57.72ms
step:1626/1775 train_time:93885ms step_avg:57.74ms
step:1627/1775 train_time:93970ms step_avg:57.76ms
step:1628/1775 train_time:94057ms step_avg:57.77ms
step:1629/1775 train_time:94142ms step_avg:57.79ms
step:1630/1775 train_time:94230ms step_avg:57.81ms
step:1631/1775 train_time:94314ms step_avg:57.83ms
step:1632/1775 train_time:94402ms step_avg:57.84ms
step:1633/1775 train_time:94486ms step_avg:57.86ms
step:1634/1775 train_time:94574ms step_avg:57.88ms
step:1635/1775 train_time:94657ms step_avg:57.89ms
step:1636/1775 train_time:94744ms step_avg:57.91ms
step:1637/1775 train_time:94829ms step_avg:57.93ms
step:1638/1775 train_time:94915ms step_avg:57.95ms
step:1639/1775 train_time:95000ms step_avg:57.96ms
step:1640/1775 train_time:95087ms step_avg:57.98ms
step:1641/1775 train_time:95172ms step_avg:58.00ms
step:1642/1775 train_time:95259ms step_avg:58.01ms
step:1643/1775 train_time:95343ms step_avg:58.03ms
step:1644/1775 train_time:95431ms step_avg:58.05ms
step:1645/1775 train_time:95515ms step_avg:58.06ms
step:1646/1775 train_time:95602ms step_avg:58.08ms
step:1647/1775 train_time:95687ms step_avg:58.10ms
step:1648/1775 train_time:95774ms step_avg:58.12ms
step:1649/1775 train_time:95858ms step_avg:58.13ms
step:1650/1775 train_time:95945ms step_avg:58.15ms
step:1651/1775 train_time:96030ms step_avg:58.16ms
step:1652/1775 train_time:96117ms step_avg:58.18ms
step:1653/1775 train_time:96203ms step_avg:58.20ms
step:1654/1775 train_time:96291ms step_avg:58.22ms
step:1655/1775 train_time:96375ms step_avg:58.23ms
step:1656/1775 train_time:96462ms step_avg:58.25ms
step:1657/1775 train_time:96546ms step_avg:58.27ms
step:1658/1775 train_time:96633ms step_avg:58.28ms
step:1659/1775 train_time:96717ms step_avg:58.30ms
step:1660/1775 train_time:96804ms step_avg:58.32ms
step:1661/1775 train_time:96889ms step_avg:58.33ms
step:1662/1775 train_time:96976ms step_avg:58.35ms
step:1663/1775 train_time:97059ms step_avg:58.36ms
step:1664/1775 train_time:97148ms step_avg:58.38ms
step:1665/1775 train_time:97232ms step_avg:58.40ms
step:1666/1775 train_time:97320ms step_avg:58.42ms
step:1667/1775 train_time:97406ms step_avg:58.43ms
step:1668/1775 train_time:97494ms step_avg:58.45ms
step:1669/1775 train_time:97578ms step_avg:58.46ms
step:1670/1775 train_time:97665ms step_avg:58.48ms
step:1671/1775 train_time:97749ms step_avg:58.50ms
step:1672/1775 train_time:97836ms step_avg:58.51ms
step:1673/1775 train_time:97922ms step_avg:58.53ms
step:1674/1775 train_time:98008ms step_avg:58.55ms
step:1675/1775 train_time:98093ms step_avg:58.56ms
step:1676/1775 train_time:98180ms step_avg:58.58ms
step:1677/1775 train_time:98265ms step_avg:58.60ms
step:1678/1775 train_time:98353ms step_avg:58.61ms
step:1679/1775 train_time:98437ms step_avg:58.63ms
step:1680/1775 train_time:98524ms step_avg:58.65ms
step:1681/1775 train_time:98609ms step_avg:58.66ms
step:1682/1775 train_time:98696ms step_avg:58.68ms
step:1683/1775 train_time:98780ms step_avg:58.69ms
step:1684/1775 train_time:98868ms step_avg:58.71ms
step:1685/1775 train_time:98952ms step_avg:58.72ms
step:1686/1775 train_time:99038ms step_avg:58.74ms
step:1687/1775 train_time:99124ms step_avg:58.76ms
step:1688/1775 train_time:99212ms step_avg:58.78ms
step:1689/1775 train_time:99295ms step_avg:58.79ms
step:1690/1775 train_time:99383ms step_avg:58.81ms
step:1691/1775 train_time:99468ms step_avg:58.82ms
step:1692/1775 train_time:99555ms step_avg:58.84ms
step:1693/1775 train_time:99639ms step_avg:58.85ms
step:1694/1775 train_time:99728ms step_avg:58.87ms
step:1695/1775 train_time:99813ms step_avg:58.89ms
step:1696/1775 train_time:99899ms step_avg:58.90ms
step:1697/1775 train_time:99984ms step_avg:58.92ms
step:1698/1775 train_time:100072ms step_avg:58.94ms
step:1699/1775 train_time:100156ms step_avg:58.95ms
step:1700/1775 train_time:100243ms step_avg:58.97ms
step:1701/1775 train_time:100328ms step_avg:58.98ms
step:1702/1775 train_time:100415ms step_avg:59.00ms
step:1703/1775 train_time:100500ms step_avg:59.01ms
step:1704/1775 train_time:100589ms step_avg:59.03ms
step:1705/1775 train_time:100674ms step_avg:59.05ms
step:1706/1775 train_time:100760ms step_avg:59.06ms
step:1707/1775 train_time:100844ms step_avg:59.08ms
step:1708/1775 train_time:100932ms step_avg:59.09ms
step:1709/1775 train_time:101016ms step_avg:59.11ms
step:1710/1775 train_time:101103ms step_avg:59.12ms
step:1711/1775 train_time:101188ms step_avg:59.14ms
step:1712/1775 train_time:101276ms step_avg:59.16ms
step:1713/1775 train_time:101360ms step_avg:59.17ms
step:1714/1775 train_time:101447ms step_avg:59.19ms
step:1715/1775 train_time:101532ms step_avg:59.20ms
step:1716/1775 train_time:101618ms step_avg:59.22ms
step:1717/1775 train_time:101702ms step_avg:59.23ms
step:1718/1775 train_time:101790ms step_avg:59.25ms
step:1719/1775 train_time:101874ms step_avg:59.26ms
step:1720/1775 train_time:101961ms step_avg:59.28ms
step:1721/1775 train_time:102045ms step_avg:59.29ms
step:1722/1775 train_time:102133ms step_avg:59.31ms
step:1723/1775 train_time:102218ms step_avg:59.33ms
step:1724/1775 train_time:102305ms step_avg:59.34ms
step:1725/1775 train_time:102390ms step_avg:59.36ms
step:1726/1775 train_time:102476ms step_avg:59.37ms
step:1727/1775 train_time:102560ms step_avg:59.39ms
step:1728/1775 train_time:102648ms step_avg:59.40ms
step:1729/1775 train_time:102732ms step_avg:59.42ms
step:1730/1775 train_time:102819ms step_avg:59.43ms
step:1731/1775 train_time:102905ms step_avg:59.45ms
step:1732/1775 train_time:102992ms step_avg:59.46ms
step:1733/1775 train_time:103076ms step_avg:59.48ms
step:1734/1775 train_time:103163ms step_avg:59.49ms
step:1735/1775 train_time:103248ms step_avg:59.51ms
step:1736/1775 train_time:103339ms step_avg:59.53ms
step:1737/1775 train_time:103425ms step_avg:59.54ms
step:1738/1775 train_time:103513ms step_avg:59.56ms
step:1739/1775 train_time:103597ms step_avg:59.57ms
step:1740/1775 train_time:103685ms step_avg:59.59ms
step:1741/1775 train_time:103770ms step_avg:59.60ms
step:1742/1775 train_time:103856ms step_avg:59.62ms
step:1743/1775 train_time:103941ms step_avg:59.63ms
step:1744/1775 train_time:104030ms step_avg:59.65ms
step:1745/1775 train_time:104114ms step_avg:59.66ms
step:1746/1775 train_time:104203ms step_avg:59.68ms
step:1747/1775 train_time:104288ms step_avg:59.70ms
step:1748/1775 train_time:104375ms step_avg:59.71ms
step:1749/1775 train_time:104459ms step_avg:59.72ms
step:1750/1775 train_time:104547ms step_avg:59.74ms
step:1750/1775 val_loss:3.2854 train_time:104645ms step_avg:59.80ms
step:1751/1775 train_time:104666ms step_avg:59.77ms
step:1752/1775 train_time:104719ms step_avg:59.77ms
step:1753/1775 train_time:104809ms step_avg:59.79ms
step:1754/1775 train_time:104898ms step_avg:59.80ms
step:1755/1775 train_time:104983ms step_avg:59.82ms
step:1756/1775 train_time:105069ms step_avg:59.83ms
step:1757/1775 train_time:105152ms step_avg:59.85ms
step:1758/1775 train_time:105237ms step_avg:59.86ms
step:1759/1775 train_time:105322ms step_avg:59.88ms
step:1760/1775 train_time:105409ms step_avg:59.89ms
step:1761/1775 train_time:105494ms step_avg:59.91ms
step:1762/1775 train_time:105583ms step_avg:59.92ms
step:1763/1775 train_time:105670ms step_avg:59.94ms
step:1764/1775 train_time:105762ms step_avg:59.96ms
step:1765/1775 train_time:105849ms step_avg:59.97ms
step:1766/1775 train_time:105938ms step_avg:59.99ms
step:1767/1775 train_time:106023ms step_avg:60.00ms
step:1768/1775 train_time:106109ms step_avg:60.02ms
step:1769/1775 train_time:106191ms step_avg:60.03ms
step:1770/1775 train_time:106278ms step_avg:60.04ms
step:1771/1775 train_time:106363ms step_avg:60.06ms
step:1772/1775 train_time:106450ms step_avg:60.07ms
step:1773/1775 train_time:106535ms step_avg:60.09ms
step:1774/1775 train_time:106624ms step_avg:60.10ms
step:1775/1775 train_time:106711ms step_avg:60.12ms
step:1775/1775 val_loss:3.2791 train_time:106812ms step_avg:60.18ms
peak memory allocated: 29148 MiB reserved: 44678 MiB

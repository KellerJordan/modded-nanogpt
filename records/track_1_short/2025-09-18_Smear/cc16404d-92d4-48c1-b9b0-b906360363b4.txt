import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:13:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   24C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   26C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:134ms step_avg:133.92ms
step:2/1645 train_time:153ms step_avg:76.49ms
step:3/1645 train_time:224ms step_avg:74.77ms
step:4/1645 train_time:314ms step_avg:78.52ms
step:5/1645 train_time:405ms step_avg:80.98ms
step:6/1645 train_time:496ms step_avg:82.62ms
step:7/1645 train_time:586ms step_avg:83.76ms
step:8/1645 train_time:677ms step_avg:84.66ms
step:9/1645 train_time:769ms step_avg:85.43ms
step:10/1645 train_time:860ms step_avg:85.97ms
step:11/1645 train_time:951ms step_avg:86.42ms
step:12/1645 train_time:1045ms step_avg:87.04ms
step:13/1645 train_time:1141ms step_avg:87.76ms
step:14/1645 train_time:1235ms step_avg:88.19ms
step:15/1645 train_time:1326ms step_avg:88.41ms
step:16/1645 train_time:1417ms step_avg:88.59ms
step:17/1645 train_time:1510ms step_avg:88.83ms
step:18/1645 train_time:1601ms step_avg:88.93ms
step:19/1645 train_time:1692ms step_avg:89.05ms
step:20/1645 train_time:1783ms step_avg:89.17ms
step:21/1645 train_time:1875ms step_avg:89.27ms
step:22/1645 train_time:1967ms step_avg:89.39ms
step:23/1645 train_time:2059ms step_avg:89.51ms
step:24/1645 train_time:2154ms step_avg:89.75ms
step:25/1645 train_time:2247ms step_avg:89.90ms
step:26/1645 train_time:2340ms step_avg:89.98ms
step:27/1645 train_time:2432ms step_avg:90.09ms
step:28/1645 train_time:2523ms step_avg:90.12ms
step:29/1645 train_time:2615ms step_avg:90.17ms
step:30/1645 train_time:2707ms step_avg:90.22ms
step:31/1645 train_time:2798ms step_avg:90.25ms
step:32/1645 train_time:2890ms step_avg:90.32ms
step:33/1645 train_time:2981ms step_avg:90.34ms
step:34/1645 train_time:3074ms step_avg:90.40ms
step:35/1645 train_time:3167ms step_avg:90.49ms
step:36/1645 train_time:3259ms step_avg:90.54ms
step:37/1645 train_time:3352ms step_avg:90.60ms
step:38/1645 train_time:3444ms step_avg:90.63ms
step:39/1645 train_time:3535ms step_avg:90.65ms
step:40/1645 train_time:3627ms step_avg:90.67ms
step:41/1645 train_time:3718ms step_avg:90.69ms
step:42/1645 train_time:3811ms step_avg:90.73ms
step:43/1645 train_time:3902ms step_avg:90.75ms
step:44/1645 train_time:3994ms step_avg:90.78ms
step:45/1645 train_time:4087ms step_avg:90.82ms
step:46/1645 train_time:4179ms step_avg:90.84ms
step:47/1645 train_time:4272ms step_avg:90.89ms
step:48/1645 train_time:4364ms step_avg:90.92ms
step:49/1645 train_time:4456ms step_avg:90.94ms
step:50/1645 train_time:4547ms step_avg:90.95ms
step:51/1645 train_time:4639ms step_avg:90.95ms
step:52/1645 train_time:4730ms step_avg:90.96ms
step:53/1645 train_time:4821ms step_avg:90.96ms
step:54/1645 train_time:4913ms step_avg:90.98ms
step:55/1645 train_time:5006ms step_avg:91.01ms
step:56/1645 train_time:5098ms step_avg:91.04ms
step:57/1645 train_time:5191ms step_avg:91.06ms
step:58/1645 train_time:5283ms step_avg:91.09ms
step:59/1645 train_time:5375ms step_avg:91.10ms
step:60/1645 train_time:5467ms step_avg:91.12ms
step:61/1645 train_time:5558ms step_avg:91.12ms
step:62/1645 train_time:5650ms step_avg:91.13ms
step:63/1645 train_time:5741ms step_avg:91.13ms
step:64/1645 train_time:5833ms step_avg:91.14ms
step:65/1645 train_time:5924ms step_avg:91.14ms
step:66/1645 train_time:6016ms step_avg:91.15ms
step:67/1645 train_time:6109ms step_avg:91.18ms
step:68/1645 train_time:6201ms step_avg:91.19ms
step:69/1645 train_time:6294ms step_avg:91.22ms
step:70/1645 train_time:6387ms step_avg:91.24ms
step:71/1645 train_time:6478ms step_avg:91.24ms
step:72/1645 train_time:6571ms step_avg:91.26ms
step:73/1645 train_time:6662ms step_avg:91.27ms
step:74/1645 train_time:6754ms step_avg:91.27ms
step:75/1645 train_time:6845ms step_avg:91.27ms
step:76/1645 train_time:6937ms step_avg:91.27ms
step:77/1645 train_time:7029ms step_avg:91.28ms
step:78/1645 train_time:7121ms step_avg:91.29ms
step:79/1645 train_time:7212ms step_avg:91.30ms
step:80/1645 train_time:7305ms step_avg:91.31ms
step:81/1645 train_time:7396ms step_avg:91.31ms
step:82/1645 train_time:7489ms step_avg:91.32ms
step:83/1645 train_time:7580ms step_avg:91.33ms
step:84/1645 train_time:7672ms step_avg:91.33ms
step:85/1645 train_time:7764ms step_avg:91.34ms
step:86/1645 train_time:7855ms step_avg:91.34ms
step:87/1645 train_time:7946ms step_avg:91.34ms
step:88/1645 train_time:8038ms step_avg:91.34ms
step:89/1645 train_time:8131ms step_avg:91.36ms
step:90/1645 train_time:8224ms step_avg:91.37ms
step:91/1645 train_time:8316ms step_avg:91.39ms
step:92/1645 train_time:8408ms step_avg:91.40ms
step:93/1645 train_time:8500ms step_avg:91.40ms
step:94/1645 train_time:8592ms step_avg:91.41ms
step:95/1645 train_time:8685ms step_avg:91.42ms
step:96/1645 train_time:8776ms step_avg:91.42ms
step:97/1645 train_time:8868ms step_avg:91.43ms
step:98/1645 train_time:8960ms step_avg:91.43ms
step:99/1645 train_time:9052ms step_avg:91.43ms
step:100/1645 train_time:9143ms step_avg:91.43ms
step:101/1645 train_time:9235ms step_avg:91.44ms
step:102/1645 train_time:9328ms step_avg:91.45ms
step:103/1645 train_time:9420ms step_avg:91.46ms
step:104/1645 train_time:9512ms step_avg:91.46ms
step:105/1645 train_time:9604ms step_avg:91.47ms
step:106/1645 train_time:9695ms step_avg:91.47ms
step:107/1645 train_time:9788ms step_avg:91.48ms
step:108/1645 train_time:9879ms step_avg:91.47ms
step:109/1645 train_time:9971ms step_avg:91.47ms
step:110/1645 train_time:10062ms step_avg:91.47ms
step:111/1645 train_time:10154ms step_avg:91.47ms
step:112/1645 train_time:10246ms step_avg:91.48ms
step:113/1645 train_time:10337ms step_avg:91.48ms
step:114/1645 train_time:10431ms step_avg:91.50ms
step:115/1645 train_time:10523ms step_avg:91.50ms
step:116/1645 train_time:10614ms step_avg:91.50ms
step:117/1645 train_time:10705ms step_avg:91.50ms
step:118/1645 train_time:10797ms step_avg:91.50ms
step:119/1645 train_time:10889ms step_avg:91.50ms
step:120/1645 train_time:10980ms step_avg:91.50ms
step:121/1645 train_time:11072ms step_avg:91.50ms
step:122/1645 train_time:11163ms step_avg:91.50ms
step:123/1645 train_time:11255ms step_avg:91.50ms
step:124/1645 train_time:11347ms step_avg:91.51ms
step:125/1645 train_time:11438ms step_avg:91.51ms
step:125/1645 val_loss:4.3312 train_time:11530ms step_avg:92.24ms
step:126/1645 train_time:11546ms step_avg:91.64ms
step:127/1645 train_time:11629ms step_avg:91.56ms
step:128/1645 train_time:11732ms step_avg:91.66ms
step:129/1645 train_time:11827ms step_avg:91.68ms
step:130/1645 train_time:11919ms step_avg:91.69ms
step:131/1645 train_time:12011ms step_avg:91.69ms
step:132/1645 train_time:12102ms step_avg:91.68ms
step:133/1645 train_time:12192ms step_avg:91.67ms
step:134/1645 train_time:12283ms step_avg:91.66ms
step:135/1645 train_time:12373ms step_avg:91.65ms
step:136/1645 train_time:12464ms step_avg:91.65ms
step:137/1645 train_time:12555ms step_avg:91.64ms
step:138/1645 train_time:12648ms step_avg:91.65ms
step:139/1645 train_time:12744ms step_avg:91.68ms
step:140/1645 train_time:12838ms step_avg:91.70ms
step:141/1645 train_time:12929ms step_avg:91.70ms
step:142/1645 train_time:13020ms step_avg:91.69ms
step:143/1645 train_time:13111ms step_avg:91.69ms
step:144/1645 train_time:13202ms step_avg:91.68ms
step:145/1645 train_time:13293ms step_avg:91.67ms
step:146/1645 train_time:13383ms step_avg:91.67ms
step:147/1645 train_time:13474ms step_avg:91.66ms
step:148/1645 train_time:13565ms step_avg:91.66ms
step:149/1645 train_time:13658ms step_avg:91.66ms
step:150/1645 train_time:13750ms step_avg:91.67ms
step:151/1645 train_time:13844ms step_avg:91.68ms
step:152/1645 train_time:13936ms step_avg:91.69ms
step:153/1645 train_time:14028ms step_avg:91.69ms
step:154/1645 train_time:14120ms step_avg:91.69ms
step:155/1645 train_time:14211ms step_avg:91.68ms
step:156/1645 train_time:14302ms step_avg:91.68ms
step:157/1645 train_time:14393ms step_avg:91.67ms
step:158/1645 train_time:14484ms step_avg:91.67ms
step:159/1645 train_time:14575ms step_avg:91.67ms
step:160/1645 train_time:14666ms step_avg:91.66ms
step:161/1645 train_time:14759ms step_avg:91.67ms
step:162/1645 train_time:14851ms step_avg:91.68ms
step:163/1645 train_time:14944ms step_avg:91.68ms
step:164/1645 train_time:15037ms step_avg:91.69ms
step:165/1645 train_time:15129ms step_avg:91.69ms
step:166/1645 train_time:15221ms step_avg:91.69ms
step:167/1645 train_time:15313ms step_avg:91.69ms
step:168/1645 train_time:15404ms step_avg:91.69ms
step:169/1645 train_time:15495ms step_avg:91.68ms
step:170/1645 train_time:15586ms step_avg:91.68ms
step:171/1645 train_time:15678ms step_avg:91.68ms
step:172/1645 train_time:15769ms step_avg:91.68ms
step:173/1645 train_time:15862ms step_avg:91.69ms
step:174/1645 train_time:15953ms step_avg:91.69ms
step:175/1645 train_time:16045ms step_avg:91.69ms
step:176/1645 train_time:16137ms step_avg:91.69ms
step:177/1645 train_time:16229ms step_avg:91.69ms
step:178/1645 train_time:16321ms step_avg:91.69ms
step:179/1645 train_time:16413ms step_avg:91.69ms
step:180/1645 train_time:16505ms step_avg:91.69ms
step:181/1645 train_time:16596ms step_avg:91.69ms
step:182/1645 train_time:16687ms step_avg:91.69ms
step:183/1645 train_time:16779ms step_avg:91.69ms
step:184/1645 train_time:16870ms step_avg:91.69ms
step:185/1645 train_time:16963ms step_avg:91.69ms
step:186/1645 train_time:17055ms step_avg:91.69ms
step:187/1645 train_time:17146ms step_avg:91.69ms
step:188/1645 train_time:17238ms step_avg:91.69ms
step:189/1645 train_time:17330ms step_avg:91.69ms
step:190/1645 train_time:17422ms step_avg:91.69ms
step:191/1645 train_time:17514ms step_avg:91.70ms
step:192/1645 train_time:17606ms step_avg:91.70ms
step:193/1645 train_time:17698ms step_avg:91.70ms
step:194/1645 train_time:17789ms step_avg:91.69ms
step:195/1645 train_time:17880ms step_avg:91.69ms
step:196/1645 train_time:17971ms step_avg:91.69ms
step:197/1645 train_time:18063ms step_avg:91.69ms
step:198/1645 train_time:18154ms step_avg:91.69ms
step:199/1645 train_time:18246ms step_avg:91.69ms
step:200/1645 train_time:18338ms step_avg:91.69ms
step:201/1645 train_time:18429ms step_avg:91.69ms
step:202/1645 train_time:18521ms step_avg:91.69ms
step:203/1645 train_time:18612ms step_avg:91.68ms
step:204/1645 train_time:18703ms step_avg:91.68ms
step:205/1645 train_time:18796ms step_avg:91.69ms
step:206/1645 train_time:18886ms step_avg:91.68ms
step:207/1645 train_time:18978ms step_avg:91.68ms
step:208/1645 train_time:19069ms step_avg:91.68ms
step:209/1645 train_time:19160ms step_avg:91.67ms
step:210/1645 train_time:19252ms step_avg:91.67ms
step:211/1645 train_time:19344ms step_avg:91.68ms
step:212/1645 train_time:19435ms step_avg:91.67ms
step:213/1645 train_time:19527ms step_avg:91.68ms
step:214/1645 train_time:19619ms step_avg:91.68ms
step:215/1645 train_time:19711ms step_avg:91.68ms
step:216/1645 train_time:19803ms step_avg:91.68ms
step:217/1645 train_time:19896ms step_avg:91.68ms
step:218/1645 train_time:19986ms step_avg:91.68ms
step:219/1645 train_time:20078ms step_avg:91.68ms
step:220/1645 train_time:20169ms step_avg:91.68ms
step:221/1645 train_time:20261ms step_avg:91.68ms
step:222/1645 train_time:20353ms step_avg:91.68ms
step:223/1645 train_time:20444ms step_avg:91.68ms
step:224/1645 train_time:20537ms step_avg:91.68ms
step:225/1645 train_time:20628ms step_avg:91.68ms
step:226/1645 train_time:20720ms step_avg:91.68ms
step:227/1645 train_time:20812ms step_avg:91.68ms
step:228/1645 train_time:20904ms step_avg:91.68ms
step:229/1645 train_time:20995ms step_avg:91.68ms
step:230/1645 train_time:21086ms step_avg:91.68ms
step:231/1645 train_time:21178ms step_avg:91.68ms
step:232/1645 train_time:21270ms step_avg:91.68ms
step:233/1645 train_time:21362ms step_avg:91.68ms
step:234/1645 train_time:21454ms step_avg:91.69ms
step:235/1645 train_time:21546ms step_avg:91.68ms
step:236/1645 train_time:21638ms step_avg:91.69ms
step:237/1645 train_time:21730ms step_avg:91.69ms
step:238/1645 train_time:21821ms step_avg:91.69ms
step:239/1645 train_time:21913ms step_avg:91.69ms
step:240/1645 train_time:22004ms step_avg:91.69ms
step:241/1645 train_time:22096ms step_avg:91.69ms
step:242/1645 train_time:22187ms step_avg:91.68ms
step:243/1645 train_time:22279ms step_avg:91.68ms
step:244/1645 train_time:22369ms step_avg:91.68ms
step:245/1645 train_time:22461ms step_avg:91.68ms
step:246/1645 train_time:22553ms step_avg:91.68ms
step:247/1645 train_time:22644ms step_avg:91.68ms
step:248/1645 train_time:22737ms step_avg:91.68ms
step:249/1645 train_time:22828ms step_avg:91.68ms
step:250/1645 train_time:22921ms step_avg:91.68ms
step:250/1645 val_loss:3.9747 train_time:23014ms step_avg:92.06ms
step:251/1645 train_time:23029ms step_avg:91.75ms
step:252/1645 train_time:23108ms step_avg:91.70ms
step:253/1645 train_time:23204ms step_avg:91.72ms
step:254/1645 train_time:23296ms step_avg:91.72ms
step:255/1645 train_time:23387ms step_avg:91.71ms
step:256/1645 train_time:23478ms step_avg:91.71ms
step:257/1645 train_time:23569ms step_avg:91.71ms
step:258/1645 train_time:23660ms step_avg:91.70ms
step:259/1645 train_time:23750ms step_avg:91.70ms
step:260/1645 train_time:23842ms step_avg:91.70ms
step:261/1645 train_time:23933ms step_avg:91.70ms
step:262/1645 train_time:24025ms step_avg:91.70ms
step:263/1645 train_time:24118ms step_avg:91.70ms
step:264/1645 train_time:24210ms step_avg:91.70ms
step:265/1645 train_time:24303ms step_avg:91.71ms
step:266/1645 train_time:24394ms step_avg:91.71ms
step:267/1645 train_time:24485ms step_avg:91.71ms
step:268/1645 train_time:24577ms step_avg:91.70ms
step:269/1645 train_time:24667ms step_avg:91.70ms
step:270/1645 train_time:24758ms step_avg:91.70ms
step:271/1645 train_time:24849ms step_avg:91.69ms
step:272/1645 train_time:24940ms step_avg:91.69ms
step:273/1645 train_time:25032ms step_avg:91.69ms
step:274/1645 train_time:25123ms step_avg:91.69ms
step:275/1645 train_time:25215ms step_avg:91.69ms
step:276/1645 train_time:25306ms step_avg:91.69ms
step:277/1645 train_time:25399ms step_avg:91.69ms
step:278/1645 train_time:25490ms step_avg:91.69ms
step:279/1645 train_time:25582ms step_avg:91.69ms
step:280/1645 train_time:25674ms step_avg:91.69ms
step:281/1645 train_time:25765ms step_avg:91.69ms
step:282/1645 train_time:25856ms step_avg:91.69ms
step:283/1645 train_time:25947ms step_avg:91.69ms
step:284/1645 train_time:26039ms step_avg:91.69ms
step:285/1645 train_time:26130ms step_avg:91.69ms
step:286/1645 train_time:26222ms step_avg:91.69ms
step:287/1645 train_time:26314ms step_avg:91.69ms
step:288/1645 train_time:26406ms step_avg:91.69ms
step:289/1645 train_time:26499ms step_avg:91.69ms
step:290/1645 train_time:26591ms step_avg:91.69ms
step:291/1645 train_time:26683ms step_avg:91.69ms
step:292/1645 train_time:26774ms step_avg:91.69ms
step:293/1645 train_time:26865ms step_avg:91.69ms
step:294/1645 train_time:26957ms step_avg:91.69ms
step:295/1645 train_time:27048ms step_avg:91.69ms
step:296/1645 train_time:27139ms step_avg:91.69ms
step:297/1645 train_time:27232ms step_avg:91.69ms
step:298/1645 train_time:27324ms step_avg:91.69ms
step:299/1645 train_time:27416ms step_avg:91.69ms
step:300/1645 train_time:27508ms step_avg:91.69ms
step:301/1645 train_time:27602ms step_avg:91.70ms
step:302/1645 train_time:27693ms step_avg:91.70ms
step:303/1645 train_time:27784ms step_avg:91.70ms
step:304/1645 train_time:27875ms step_avg:91.70ms
step:305/1645 train_time:27967ms step_avg:91.69ms
step:306/1645 train_time:28059ms step_avg:91.69ms
step:307/1645 train_time:28149ms step_avg:91.69ms
step:308/1645 train_time:28241ms step_avg:91.69ms
step:309/1645 train_time:28333ms step_avg:91.69ms
step:310/1645 train_time:28424ms step_avg:91.69ms
step:311/1645 train_time:28515ms step_avg:91.69ms
step:312/1645 train_time:28607ms step_avg:91.69ms
step:313/1645 train_time:28700ms step_avg:91.69ms
step:314/1645 train_time:28792ms step_avg:91.69ms
step:315/1645 train_time:28882ms step_avg:91.69ms
step:316/1645 train_time:28974ms step_avg:91.69ms
step:317/1645 train_time:29066ms step_avg:91.69ms
step:318/1645 train_time:29157ms step_avg:91.69ms
step:319/1645 train_time:29249ms step_avg:91.69ms
step:320/1645 train_time:29340ms step_avg:91.69ms
step:321/1645 train_time:29431ms step_avg:91.69ms
step:322/1645 train_time:29523ms step_avg:91.69ms
step:323/1645 train_time:29614ms step_avg:91.69ms
step:324/1645 train_time:29706ms step_avg:91.68ms
step:325/1645 train_time:29799ms step_avg:91.69ms
step:326/1645 train_time:29889ms step_avg:91.68ms
step:327/1645 train_time:29982ms step_avg:91.69ms
step:328/1645 train_time:30074ms step_avg:91.69ms
step:329/1645 train_time:30165ms step_avg:91.69ms
step:330/1645 train_time:30256ms step_avg:91.68ms
step:331/1645 train_time:30348ms step_avg:91.69ms
step:332/1645 train_time:30439ms step_avg:91.68ms
step:333/1645 train_time:30531ms step_avg:91.68ms
step:334/1645 train_time:30623ms step_avg:91.68ms
step:335/1645 train_time:30714ms step_avg:91.68ms
step:336/1645 train_time:30806ms step_avg:91.68ms
step:337/1645 train_time:30898ms step_avg:91.69ms
step:338/1645 train_time:30989ms step_avg:91.68ms
step:339/1645 train_time:31081ms step_avg:91.68ms
step:340/1645 train_time:31173ms step_avg:91.68ms
step:341/1645 train_time:31264ms step_avg:91.68ms
step:342/1645 train_time:31356ms step_avg:91.68ms
step:343/1645 train_time:31447ms step_avg:91.68ms
step:344/1645 train_time:31538ms step_avg:91.68ms
step:345/1645 train_time:31629ms step_avg:91.68ms
step:346/1645 train_time:31721ms step_avg:91.68ms
step:347/1645 train_time:31814ms step_avg:91.68ms
step:348/1645 train_time:31905ms step_avg:91.68ms
step:349/1645 train_time:31997ms step_avg:91.68ms
step:350/1645 train_time:32088ms step_avg:91.68ms
step:351/1645 train_time:32181ms step_avg:91.68ms
step:352/1645 train_time:32272ms step_avg:91.68ms
step:353/1645 train_time:32364ms step_avg:91.68ms
step:354/1645 train_time:32456ms step_avg:91.68ms
step:355/1645 train_time:32547ms step_avg:91.68ms
step:356/1645 train_time:32638ms step_avg:91.68ms
step:357/1645 train_time:32729ms step_avg:91.68ms
step:358/1645 train_time:32821ms step_avg:91.68ms
step:359/1645 train_time:32913ms step_avg:91.68ms
step:360/1645 train_time:33005ms step_avg:91.68ms
step:361/1645 train_time:33097ms step_avg:91.68ms
step:362/1645 train_time:33189ms step_avg:91.68ms
step:363/1645 train_time:33282ms step_avg:91.69ms
step:364/1645 train_time:33373ms step_avg:91.68ms
step:365/1645 train_time:33464ms step_avg:91.68ms
step:366/1645 train_time:33556ms step_avg:91.68ms
step:367/1645 train_time:33647ms step_avg:91.68ms
step:368/1645 train_time:33738ms step_avg:91.68ms
step:369/1645 train_time:33829ms step_avg:91.68ms
step:370/1645 train_time:33921ms step_avg:91.68ms
step:371/1645 train_time:34013ms step_avg:91.68ms
step:372/1645 train_time:34105ms step_avg:91.68ms
step:373/1645 train_time:34196ms step_avg:91.68ms
step:374/1645 train_time:34288ms step_avg:91.68ms
step:375/1645 train_time:34381ms step_avg:91.68ms
step:375/1645 val_loss:3.8144 train_time:34473ms step_avg:91.93ms
step:376/1645 train_time:34489ms step_avg:91.72ms
step:377/1645 train_time:34569ms step_avg:91.69ms
step:378/1645 train_time:34664ms step_avg:91.70ms
step:379/1645 train_time:34756ms step_avg:91.71ms
step:380/1645 train_time:34848ms step_avg:91.70ms
step:381/1645 train_time:34938ms step_avg:91.70ms
step:382/1645 train_time:35029ms step_avg:91.70ms
step:383/1645 train_time:35120ms step_avg:91.70ms
step:384/1645 train_time:35211ms step_avg:91.70ms
step:385/1645 train_time:35302ms step_avg:91.69ms
step:386/1645 train_time:35392ms step_avg:91.69ms
step:387/1645 train_time:35485ms step_avg:91.69ms
step:388/1645 train_time:35578ms step_avg:91.70ms
step:389/1645 train_time:35670ms step_avg:91.70ms
step:390/1645 train_time:35762ms step_avg:91.70ms
step:391/1645 train_time:35854ms step_avg:91.70ms
step:392/1645 train_time:35945ms step_avg:91.70ms
step:393/1645 train_time:36036ms step_avg:91.70ms
step:394/1645 train_time:36128ms step_avg:91.69ms
step:395/1645 train_time:36219ms step_avg:91.69ms
step:396/1645 train_time:36310ms step_avg:91.69ms
step:397/1645 train_time:36402ms step_avg:91.69ms
step:398/1645 train_time:36494ms step_avg:91.69ms
step:399/1645 train_time:36586ms step_avg:91.70ms
step:400/1645 train_time:36678ms step_avg:91.70ms
step:401/1645 train_time:36770ms step_avg:91.69ms
step:402/1645 train_time:36863ms step_avg:91.70ms
step:403/1645 train_time:36955ms step_avg:91.70ms
step:404/1645 train_time:37047ms step_avg:91.70ms
step:405/1645 train_time:37138ms step_avg:91.70ms
step:406/1645 train_time:37229ms step_avg:91.70ms
step:407/1645 train_time:37321ms step_avg:91.70ms
step:408/1645 train_time:37412ms step_avg:91.70ms
step:409/1645 train_time:37503ms step_avg:91.69ms
step:410/1645 train_time:37595ms step_avg:91.69ms
step:411/1645 train_time:37687ms step_avg:91.69ms
step:412/1645 train_time:37778ms step_avg:91.69ms
step:413/1645 train_time:37869ms step_avg:91.69ms
step:414/1645 train_time:37961ms step_avg:91.69ms
step:415/1645 train_time:38053ms step_avg:91.69ms
step:416/1645 train_time:38144ms step_avg:91.69ms
step:417/1645 train_time:38236ms step_avg:91.69ms
step:418/1645 train_time:38327ms step_avg:91.69ms
step:419/1645 train_time:38419ms step_avg:91.69ms
step:420/1645 train_time:38510ms step_avg:91.69ms
step:421/1645 train_time:38601ms step_avg:91.69ms
step:422/1645 train_time:38693ms step_avg:91.69ms
step:423/1645 train_time:38785ms step_avg:91.69ms
step:424/1645 train_time:38876ms step_avg:91.69ms
step:425/1645 train_time:38969ms step_avg:91.69ms
step:426/1645 train_time:39060ms step_avg:91.69ms
step:427/1645 train_time:39152ms step_avg:91.69ms
step:428/1645 train_time:39243ms step_avg:91.69ms
step:429/1645 train_time:39334ms step_avg:91.69ms
step:430/1645 train_time:39425ms step_avg:91.69ms
step:431/1645 train_time:39517ms step_avg:91.69ms
step:432/1645 train_time:39608ms step_avg:91.69ms
step:433/1645 train_time:39701ms step_avg:91.69ms
step:434/1645 train_time:39792ms step_avg:91.69ms
step:435/1645 train_time:39884ms step_avg:91.69ms
step:436/1645 train_time:39975ms step_avg:91.69ms
step:437/1645 train_time:40067ms step_avg:91.69ms
step:438/1645 train_time:40158ms step_avg:91.68ms
step:439/1645 train_time:40249ms step_avg:91.68ms
step:440/1645 train_time:40341ms step_avg:91.68ms
step:441/1645 train_time:40433ms step_avg:91.68ms
step:442/1645 train_time:40524ms step_avg:91.68ms
step:443/1645 train_time:40615ms step_avg:91.68ms
step:444/1645 train_time:40707ms step_avg:91.68ms
step:445/1645 train_time:40798ms step_avg:91.68ms
step:446/1645 train_time:40889ms step_avg:91.68ms
step:447/1645 train_time:40982ms step_avg:91.68ms
step:448/1645 train_time:41073ms step_avg:91.68ms
step:449/1645 train_time:41165ms step_avg:91.68ms
step:450/1645 train_time:41256ms step_avg:91.68ms
step:451/1645 train_time:41348ms step_avg:91.68ms
step:452/1645 train_time:41440ms step_avg:91.68ms
step:453/1645 train_time:41532ms step_avg:91.68ms
step:454/1645 train_time:41623ms step_avg:91.68ms
step:455/1645 train_time:41715ms step_avg:91.68ms
step:456/1645 train_time:41807ms step_avg:91.68ms
step:457/1645 train_time:41898ms step_avg:91.68ms
step:458/1645 train_time:41991ms step_avg:91.68ms
step:459/1645 train_time:42082ms step_avg:91.68ms
step:460/1645 train_time:42173ms step_avg:91.68ms
step:461/1645 train_time:42265ms step_avg:91.68ms
step:462/1645 train_time:42356ms step_avg:91.68ms
step:463/1645 train_time:42448ms step_avg:91.68ms
step:464/1645 train_time:42540ms step_avg:91.68ms
step:465/1645 train_time:42631ms step_avg:91.68ms
step:466/1645 train_time:42723ms step_avg:91.68ms
step:467/1645 train_time:42816ms step_avg:91.68ms
step:468/1645 train_time:42906ms step_avg:91.68ms
step:469/1645 train_time:42997ms step_avg:91.68ms
step:470/1645 train_time:43088ms step_avg:91.68ms
step:471/1645 train_time:43180ms step_avg:91.68ms
step:472/1645 train_time:43271ms step_avg:91.68ms
step:473/1645 train_time:43363ms step_avg:91.68ms
step:474/1645 train_time:43456ms step_avg:91.68ms
step:475/1645 train_time:43548ms step_avg:91.68ms
step:476/1645 train_time:43641ms step_avg:91.68ms
step:477/1645 train_time:43732ms step_avg:91.68ms
step:478/1645 train_time:43824ms step_avg:91.68ms
step:479/1645 train_time:43915ms step_avg:91.68ms
step:480/1645 train_time:44006ms step_avg:91.68ms
step:481/1645 train_time:44098ms step_avg:91.68ms
step:482/1645 train_time:44189ms step_avg:91.68ms
step:483/1645 train_time:44281ms step_avg:91.68ms
step:484/1645 train_time:44373ms step_avg:91.68ms
step:485/1645 train_time:44464ms step_avg:91.68ms
step:486/1645 train_time:44558ms step_avg:91.68ms
step:487/1645 train_time:44649ms step_avg:91.68ms
step:488/1645 train_time:44741ms step_avg:91.68ms
step:489/1645 train_time:44832ms step_avg:91.68ms
step:490/1645 train_time:44924ms step_avg:91.68ms
step:491/1645 train_time:45015ms step_avg:91.68ms
step:492/1645 train_time:45106ms step_avg:91.68ms
step:493/1645 train_time:45199ms step_avg:91.68ms
step:494/1645 train_time:45290ms step_avg:91.68ms
step:495/1645 train_time:45382ms step_avg:91.68ms
step:496/1645 train_time:45473ms step_avg:91.68ms
step:497/1645 train_time:45565ms step_avg:91.68ms
step:498/1645 train_time:45656ms step_avg:91.68ms
step:499/1645 train_time:45748ms step_avg:91.68ms
step:500/1645 train_time:45840ms step_avg:91.68ms
step:500/1645 val_loss:3.7121 train_time:45932ms step_avg:91.86ms
step:501/1645 train_time:45947ms step_avg:91.71ms
step:502/1645 train_time:46028ms step_avg:91.69ms
step:503/1645 train_time:46121ms step_avg:91.69ms
step:504/1645 train_time:46214ms step_avg:91.69ms
step:505/1645 train_time:46304ms step_avg:91.69ms
step:506/1645 train_time:46395ms step_avg:91.69ms
step:507/1645 train_time:46486ms step_avg:91.69ms
step:508/1645 train_time:46577ms step_avg:91.69ms
step:509/1645 train_time:46667ms step_avg:91.68ms
step:510/1645 train_time:46758ms step_avg:91.68ms
step:511/1645 train_time:46849ms step_avg:91.68ms
step:512/1645 train_time:46942ms step_avg:91.68ms
step:513/1645 train_time:47035ms step_avg:91.69ms
step:514/1645 train_time:47127ms step_avg:91.69ms
step:515/1645 train_time:47219ms step_avg:91.69ms
step:516/1645 train_time:47310ms step_avg:91.69ms
step:517/1645 train_time:47402ms step_avg:91.69ms
step:518/1645 train_time:47493ms step_avg:91.69ms
step:519/1645 train_time:47585ms step_avg:91.69ms
step:520/1645 train_time:47675ms step_avg:91.68ms
step:521/1645 train_time:47766ms step_avg:91.68ms
step:522/1645 train_time:47857ms step_avg:91.68ms
step:523/1645 train_time:47949ms step_avg:91.68ms
step:524/1645 train_time:48041ms step_avg:91.68ms
step:525/1645 train_time:48134ms step_avg:91.68ms
step:526/1645 train_time:48226ms step_avg:91.68ms
step:527/1645 train_time:48317ms step_avg:91.68ms
step:528/1645 train_time:48409ms step_avg:91.68ms
step:529/1645 train_time:48501ms step_avg:91.68ms
step:530/1645 train_time:48592ms step_avg:91.68ms
step:531/1645 train_time:48683ms step_avg:91.68ms
step:532/1645 train_time:48774ms step_avg:91.68ms
step:533/1645 train_time:48865ms step_avg:91.68ms
step:534/1645 train_time:48957ms step_avg:91.68ms
step:535/1645 train_time:49048ms step_avg:91.68ms
step:536/1645 train_time:49140ms step_avg:91.68ms
step:537/1645 train_time:49233ms step_avg:91.68ms
step:538/1645 train_time:49324ms step_avg:91.68ms
step:539/1645 train_time:49416ms step_avg:91.68ms
step:540/1645 train_time:49508ms step_avg:91.68ms
step:541/1645 train_time:49599ms step_avg:91.68ms
step:542/1645 train_time:49691ms step_avg:91.68ms
step:543/1645 train_time:49782ms step_avg:91.68ms
step:544/1645 train_time:49874ms step_avg:91.68ms
step:545/1645 train_time:49965ms step_avg:91.68ms
step:546/1645 train_time:50057ms step_avg:91.68ms
step:547/1645 train_time:50148ms step_avg:91.68ms
step:548/1645 train_time:50239ms step_avg:91.68ms
step:549/1645 train_time:50331ms step_avg:91.68ms
step:550/1645 train_time:50423ms step_avg:91.68ms
step:551/1645 train_time:50517ms step_avg:91.68ms
step:552/1645 train_time:50610ms step_avg:91.69ms
step:553/1645 train_time:50704ms step_avg:91.69ms
step:554/1645 train_time:50796ms step_avg:91.69ms
step:555/1645 train_time:50889ms step_avg:91.69ms
step:556/1645 train_time:50982ms step_avg:91.69ms
step:557/1645 train_time:51076ms step_avg:91.70ms
step:558/1645 train_time:51170ms step_avg:91.70ms
step:559/1645 train_time:51262ms step_avg:91.70ms
step:560/1645 train_time:51355ms step_avg:91.71ms
step:561/1645 train_time:51448ms step_avg:91.71ms
step:562/1645 train_time:51540ms step_avg:91.71ms
step:563/1645 train_time:51634ms step_avg:91.71ms
step:564/1645 train_time:51726ms step_avg:91.71ms
step:565/1645 train_time:51819ms step_avg:91.71ms
step:566/1645 train_time:51912ms step_avg:91.72ms
step:567/1645 train_time:52007ms step_avg:91.72ms
step:568/1645 train_time:52099ms step_avg:91.72ms
step:569/1645 train_time:52191ms step_avg:91.72ms
step:570/1645 train_time:52284ms step_avg:91.73ms
step:571/1645 train_time:52376ms step_avg:91.73ms
step:572/1645 train_time:52470ms step_avg:91.73ms
step:573/1645 train_time:52563ms step_avg:91.73ms
step:574/1645 train_time:52656ms step_avg:91.74ms
step:575/1645 train_time:52749ms step_avg:91.74ms
step:576/1645 train_time:52841ms step_avg:91.74ms
step:577/1645 train_time:52935ms step_avg:91.74ms
step:578/1645 train_time:53028ms step_avg:91.74ms
step:579/1645 train_time:53120ms step_avg:91.74ms
step:580/1645 train_time:53215ms step_avg:91.75ms
step:581/1645 train_time:53308ms step_avg:91.75ms
step:582/1645 train_time:53400ms step_avg:91.75ms
step:583/1645 train_time:53493ms step_avg:91.75ms
step:584/1645 train_time:53586ms step_avg:91.76ms
step:585/1645 train_time:53679ms step_avg:91.76ms
step:586/1645 train_time:53772ms step_avg:91.76ms
step:587/1645 train_time:53865ms step_avg:91.76ms
step:588/1645 train_time:53958ms step_avg:91.77ms
step:589/1645 train_time:54051ms step_avg:91.77ms
step:590/1645 train_time:54144ms step_avg:91.77ms
step:591/1645 train_time:54237ms step_avg:91.77ms
step:592/1645 train_time:54330ms step_avg:91.77ms
step:593/1645 train_time:54423ms step_avg:91.78ms
step:594/1645 train_time:54518ms step_avg:91.78ms
step:595/1645 train_time:54611ms step_avg:91.78ms
step:596/1645 train_time:54703ms step_avg:91.78ms
step:597/1645 train_time:54796ms step_avg:91.79ms
step:598/1645 train_time:54889ms step_avg:91.79ms
step:599/1645 train_time:54982ms step_avg:91.79ms
step:600/1645 train_time:55075ms step_avg:91.79ms
step:601/1645 train_time:55168ms step_avg:91.79ms
step:602/1645 train_time:55260ms step_avg:91.79ms
step:603/1645 train_time:55353ms step_avg:91.80ms
step:604/1645 train_time:55445ms step_avg:91.80ms
step:605/1645 train_time:55538ms step_avg:91.80ms
step:606/1645 train_time:55631ms step_avg:91.80ms
step:607/1645 train_time:55724ms step_avg:91.80ms
step:608/1645 train_time:55818ms step_avg:91.81ms
step:609/1645 train_time:55911ms step_avg:91.81ms
step:610/1645 train_time:56004ms step_avg:91.81ms
step:611/1645 train_time:56096ms step_avg:91.81ms
step:612/1645 train_time:56188ms step_avg:91.81ms
step:613/1645 train_time:56281ms step_avg:91.81ms
step:614/1645 train_time:56374ms step_avg:91.81ms
step:615/1645 train_time:56468ms step_avg:91.82ms
step:616/1645 train_time:56560ms step_avg:91.82ms
step:617/1645 train_time:56653ms step_avg:91.82ms
step:618/1645 train_time:56745ms step_avg:91.82ms
step:619/1645 train_time:56838ms step_avg:91.82ms
step:620/1645 train_time:56932ms step_avg:91.83ms
step:621/1645 train_time:57024ms step_avg:91.83ms
step:622/1645 train_time:57117ms step_avg:91.83ms
step:623/1645 train_time:57210ms step_avg:91.83ms
step:624/1645 train_time:57303ms step_avg:91.83ms
step:625/1645 train_time:57396ms step_avg:91.83ms
step:625/1645 val_loss:3.6105 train_time:57489ms step_avg:91.98ms
step:626/1645 train_time:57512ms step_avg:91.87ms
step:627/1645 train_time:57586ms step_avg:91.84ms
step:628/1645 train_time:57685ms step_avg:91.86ms
step:629/1645 train_time:57778ms step_avg:91.86ms
step:630/1645 train_time:57870ms step_avg:91.86ms
step:631/1645 train_time:57961ms step_avg:91.86ms
step:632/1645 train_time:58052ms step_avg:91.85ms
step:633/1645 train_time:58144ms step_avg:91.85ms
step:634/1645 train_time:58235ms step_avg:91.85ms
step:635/1645 train_time:58327ms step_avg:91.85ms
step:636/1645 train_time:58424ms step_avg:91.86ms
step:637/1645 train_time:58522ms step_avg:91.87ms
step:638/1645 train_time:58617ms step_avg:91.88ms
step:639/1645 train_time:58710ms step_avg:91.88ms
step:640/1645 train_time:58803ms step_avg:91.88ms
step:641/1645 train_time:58895ms step_avg:91.88ms
step:642/1645 train_time:58988ms step_avg:91.88ms
step:643/1645 train_time:59080ms step_avg:91.88ms
step:644/1645 train_time:59172ms step_avg:91.88ms
step:645/1645 train_time:59263ms step_avg:91.88ms
step:646/1645 train_time:59356ms step_avg:91.88ms
step:647/1645 train_time:59450ms step_avg:91.89ms
step:648/1645 train_time:59544ms step_avg:91.89ms
step:649/1645 train_time:59637ms step_avg:91.89ms
step:650/1645 train_time:59730ms step_avg:91.89ms
step:651/1645 train_time:59823ms step_avg:91.89ms
step:652/1645 train_time:59917ms step_avg:91.90ms
step:653/1645 train_time:60009ms step_avg:91.90ms
step:654/1645 train_time:60102ms step_avg:91.90ms
step:655/1645 train_time:60194ms step_avg:91.90ms
step:656/1645 train_time:60285ms step_avg:91.90ms
step:657/1645 train_time:60379ms step_avg:91.90ms
step:658/1645 train_time:60472ms step_avg:91.90ms
step:659/1645 train_time:60565ms step_avg:91.90ms
step:660/1645 train_time:60659ms step_avg:91.91ms
step:661/1645 train_time:60752ms step_avg:91.91ms
step:662/1645 train_time:60847ms step_avg:91.91ms
step:663/1645 train_time:60941ms step_avg:91.92ms
step:664/1645 train_time:61034ms step_avg:91.92ms
step:665/1645 train_time:61126ms step_avg:91.92ms
step:666/1645 train_time:61218ms step_avg:91.92ms
step:667/1645 train_time:61311ms step_avg:91.92ms
step:668/1645 train_time:61404ms step_avg:91.92ms
step:669/1645 train_time:61497ms step_avg:91.92ms
step:670/1645 train_time:61590ms step_avg:91.93ms
step:671/1645 train_time:61684ms step_avg:91.93ms
step:672/1645 train_time:61777ms step_avg:91.93ms
step:673/1645 train_time:61870ms step_avg:91.93ms
step:674/1645 train_time:61963ms step_avg:91.93ms
step:675/1645 train_time:62056ms step_avg:91.93ms
step:676/1645 train_time:62149ms step_avg:91.94ms
step:677/1645 train_time:62241ms step_avg:91.94ms
step:678/1645 train_time:62334ms step_avg:91.94ms
step:679/1645 train_time:62427ms step_avg:91.94ms
step:680/1645 train_time:62519ms step_avg:91.94ms
step:681/1645 train_time:62612ms step_avg:91.94ms
step:682/1645 train_time:62705ms step_avg:91.94ms
step:683/1645 train_time:62797ms step_avg:91.94ms
step:684/1645 train_time:62891ms step_avg:91.95ms
step:685/1645 train_time:62984ms step_avg:91.95ms
step:686/1645 train_time:63077ms step_avg:91.95ms
step:687/1645 train_time:63169ms step_avg:91.95ms
step:688/1645 train_time:63261ms step_avg:91.95ms
step:689/1645 train_time:63354ms step_avg:91.95ms
step:690/1645 train_time:63447ms step_avg:91.95ms
step:691/1645 train_time:63540ms step_avg:91.95ms
step:692/1645 train_time:63633ms step_avg:91.96ms
step:693/1645 train_time:63726ms step_avg:91.96ms
step:694/1645 train_time:63819ms step_avg:91.96ms
step:695/1645 train_time:63912ms step_avg:91.96ms
step:696/1645 train_time:64006ms step_avg:91.96ms
step:697/1645 train_time:64098ms step_avg:91.96ms
step:698/1645 train_time:64190ms step_avg:91.96ms
step:699/1645 train_time:64283ms step_avg:91.96ms
step:700/1645 train_time:64376ms step_avg:91.97ms
step:701/1645 train_time:64468ms step_avg:91.97ms
step:702/1645 train_time:64562ms step_avg:91.97ms
step:703/1645 train_time:64655ms step_avg:91.97ms
step:704/1645 train_time:64748ms step_avg:91.97ms
step:705/1645 train_time:64842ms step_avg:91.97ms
step:706/1645 train_time:64935ms step_avg:91.98ms
step:707/1645 train_time:65028ms step_avg:91.98ms
step:708/1645 train_time:65121ms step_avg:91.98ms
step:709/1645 train_time:65214ms step_avg:91.98ms
step:710/1645 train_time:65306ms step_avg:91.98ms
step:711/1645 train_time:65399ms step_avg:91.98ms
step:712/1645 train_time:65492ms step_avg:91.98ms
step:713/1645 train_time:65585ms step_avg:91.98ms
step:714/1645 train_time:65679ms step_avg:91.99ms
step:715/1645 train_time:65771ms step_avg:91.99ms
step:716/1645 train_time:65864ms step_avg:91.99ms
step:717/1645 train_time:65958ms step_avg:91.99ms
step:718/1645 train_time:66050ms step_avg:91.99ms
step:719/1645 train_time:66143ms step_avg:91.99ms
step:720/1645 train_time:66236ms step_avg:91.99ms
step:721/1645 train_time:66328ms step_avg:92.00ms
step:722/1645 train_time:66422ms step_avg:92.00ms
step:723/1645 train_time:66516ms step_avg:92.00ms
step:724/1645 train_time:66608ms step_avg:92.00ms
step:725/1645 train_time:66700ms step_avg:92.00ms
step:726/1645 train_time:66792ms step_avg:92.00ms
step:727/1645 train_time:66886ms step_avg:92.00ms
step:728/1645 train_time:66979ms step_avg:92.00ms
step:729/1645 train_time:67072ms step_avg:92.01ms
step:730/1645 train_time:67164ms step_avg:92.01ms
step:731/1645 train_time:67258ms step_avg:92.01ms
step:732/1645 train_time:67351ms step_avg:92.01ms
step:733/1645 train_time:67444ms step_avg:92.01ms
step:734/1645 train_time:67538ms step_avg:92.01ms
step:735/1645 train_time:67630ms step_avg:92.01ms
step:736/1645 train_time:67724ms step_avg:92.02ms
step:737/1645 train_time:67816ms step_avg:92.02ms
step:738/1645 train_time:67909ms step_avg:92.02ms
step:739/1645 train_time:68002ms step_avg:92.02ms
step:740/1645 train_time:68095ms step_avg:92.02ms
step:741/1645 train_time:68188ms step_avg:92.02ms
step:742/1645 train_time:68281ms step_avg:92.02ms
step:743/1645 train_time:68375ms step_avg:92.03ms
step:744/1645 train_time:68468ms step_avg:92.03ms
step:745/1645 train_time:68561ms step_avg:92.03ms
step:746/1645 train_time:68654ms step_avg:92.03ms
step:747/1645 train_time:68747ms step_avg:92.03ms
step:748/1645 train_time:68841ms step_avg:92.03ms
step:749/1645 train_time:68935ms step_avg:92.04ms
step:750/1645 train_time:69029ms step_avg:92.04ms
step:750/1645 val_loss:3.5580 train_time:69120ms step_avg:92.16ms
step:751/1645 train_time:69143ms step_avg:92.07ms
step:752/1645 train_time:69216ms step_avg:92.04ms
step:753/1645 train_time:69309ms step_avg:92.04ms
step:754/1645 train_time:69401ms step_avg:92.04ms
step:755/1645 train_time:69494ms step_avg:92.05ms
step:756/1645 train_time:69587ms step_avg:92.05ms
step:757/1645 train_time:69679ms step_avg:92.05ms
step:758/1645 train_time:69772ms step_avg:92.05ms
step:759/1645 train_time:69864ms step_avg:92.05ms
step:760/1645 train_time:69957ms step_avg:92.05ms
step:761/1645 train_time:70051ms step_avg:92.05ms
step:762/1645 train_time:70145ms step_avg:92.05ms
step:763/1645 train_time:70239ms step_avg:92.06ms
step:764/1645 train_time:70332ms step_avg:92.06ms
step:765/1645 train_time:70425ms step_avg:92.06ms
step:766/1645 train_time:70518ms step_avg:92.06ms
step:767/1645 train_time:70610ms step_avg:92.06ms
step:768/1645 train_time:70702ms step_avg:92.06ms
step:769/1645 train_time:70795ms step_avg:92.06ms
step:770/1645 train_time:70887ms step_avg:92.06ms
step:771/1645 train_time:70980ms step_avg:92.06ms
step:772/1645 train_time:71075ms step_avg:92.07ms
step:773/1645 train_time:71168ms step_avg:92.07ms
step:774/1645 train_time:71260ms step_avg:92.07ms
step:775/1645 train_time:71354ms step_avg:92.07ms
step:776/1645 train_time:71446ms step_avg:92.07ms
step:777/1645 train_time:71539ms step_avg:92.07ms
step:778/1645 train_time:71630ms step_avg:92.07ms
step:779/1645 train_time:71723ms step_avg:92.07ms
step:780/1645 train_time:71815ms step_avg:92.07ms
step:781/1645 train_time:71907ms step_avg:92.07ms
step:782/1645 train_time:72001ms step_avg:92.07ms
step:783/1645 train_time:72094ms step_avg:92.07ms
step:784/1645 train_time:72187ms step_avg:92.08ms
step:785/1645 train_time:72281ms step_avg:92.08ms
step:786/1645 train_time:72374ms step_avg:92.08ms
step:787/1645 train_time:72466ms step_avg:92.08ms
step:788/1645 train_time:72558ms step_avg:92.08ms
step:789/1645 train_time:72651ms step_avg:92.08ms
step:790/1645 train_time:72744ms step_avg:92.08ms
step:791/1645 train_time:72837ms step_avg:92.08ms
step:792/1645 train_time:72929ms step_avg:92.08ms
step:793/1645 train_time:73022ms step_avg:92.08ms
step:794/1645 train_time:73115ms step_avg:92.08ms
step:795/1645 train_time:73208ms step_avg:92.09ms
step:796/1645 train_time:73301ms step_avg:92.09ms
step:797/1645 train_time:73394ms step_avg:92.09ms
step:798/1645 train_time:73488ms step_avg:92.09ms
step:799/1645 train_time:73582ms step_avg:92.09ms
step:800/1645 train_time:73675ms step_avg:92.09ms
step:801/1645 train_time:73768ms step_avg:92.09ms
step:802/1645 train_time:73860ms step_avg:92.09ms
step:803/1645 train_time:73955ms step_avg:92.10ms
step:804/1645 train_time:74047ms step_avg:92.10ms
step:805/1645 train_time:74139ms step_avg:92.10ms
step:806/1645 train_time:74232ms step_avg:92.10ms
step:807/1645 train_time:74325ms step_avg:92.10ms
step:808/1645 train_time:74418ms step_avg:92.10ms
step:809/1645 train_time:74512ms step_avg:92.10ms
step:810/1645 train_time:74604ms step_avg:92.10ms
step:811/1645 train_time:74698ms step_avg:92.11ms
step:812/1645 train_time:74792ms step_avg:92.11ms
step:813/1645 train_time:74884ms step_avg:92.11ms
step:814/1645 train_time:74978ms step_avg:92.11ms
step:815/1645 train_time:75070ms step_avg:92.11ms
step:816/1645 train_time:75163ms step_avg:92.11ms
step:817/1645 train_time:75256ms step_avg:92.11ms
step:818/1645 train_time:75349ms step_avg:92.11ms
step:819/1645 train_time:75442ms step_avg:92.12ms
step:820/1645 train_time:75535ms step_avg:92.12ms
step:821/1645 train_time:75628ms step_avg:92.12ms
step:822/1645 train_time:75721ms step_avg:92.12ms
step:823/1645 train_time:75814ms step_avg:92.12ms
step:824/1645 train_time:75907ms step_avg:92.12ms
step:825/1645 train_time:75999ms step_avg:92.12ms
step:826/1645 train_time:76092ms step_avg:92.12ms
step:827/1645 train_time:76185ms step_avg:92.12ms
step:828/1645 train_time:76278ms step_avg:92.12ms
step:829/1645 train_time:76371ms step_avg:92.12ms
step:830/1645 train_time:76464ms step_avg:92.13ms
step:831/1645 train_time:76557ms step_avg:92.13ms
step:832/1645 train_time:76650ms step_avg:92.13ms
step:833/1645 train_time:76743ms step_avg:92.13ms
step:834/1645 train_time:76836ms step_avg:92.13ms
step:835/1645 train_time:76929ms step_avg:92.13ms
step:836/1645 train_time:77021ms step_avg:92.13ms
step:837/1645 train_time:77115ms step_avg:92.13ms
step:838/1645 train_time:77207ms step_avg:92.13ms
step:839/1645 train_time:77300ms step_avg:92.13ms
step:840/1645 train_time:77394ms step_avg:92.14ms
step:841/1645 train_time:77487ms step_avg:92.14ms
step:842/1645 train_time:77580ms step_avg:92.14ms
step:843/1645 train_time:77673ms step_avg:92.14ms
step:844/1645 train_time:77766ms step_avg:92.14ms
step:845/1645 train_time:77858ms step_avg:92.14ms
step:846/1645 train_time:77952ms step_avg:92.14ms
step:847/1645 train_time:78044ms step_avg:92.14ms
step:848/1645 train_time:78137ms step_avg:92.14ms
step:849/1645 train_time:78230ms step_avg:92.14ms
step:850/1645 train_time:78322ms step_avg:92.14ms
step:851/1645 train_time:78417ms step_avg:92.15ms
step:852/1645 train_time:78511ms step_avg:92.15ms
step:853/1645 train_time:78604ms step_avg:92.15ms
step:854/1645 train_time:78697ms step_avg:92.15ms
step:855/1645 train_time:78790ms step_avg:92.15ms
step:856/1645 train_time:78883ms step_avg:92.15ms
step:857/1645 train_time:78977ms step_avg:92.15ms
step:858/1645 train_time:79069ms step_avg:92.15ms
step:859/1645 train_time:79161ms step_avg:92.16ms
step:860/1645 train_time:79255ms step_avg:92.16ms
step:861/1645 train_time:79348ms step_avg:92.16ms
step:862/1645 train_time:79441ms step_avg:92.16ms
step:863/1645 train_time:79534ms step_avg:92.16ms
step:864/1645 train_time:79626ms step_avg:92.16ms
step:865/1645 train_time:79719ms step_avg:92.16ms
step:866/1645 train_time:79812ms step_avg:92.16ms
step:867/1645 train_time:79904ms step_avg:92.16ms
step:868/1645 train_time:79997ms step_avg:92.16ms
step:869/1645 train_time:80091ms step_avg:92.16ms
step:870/1645 train_time:80184ms step_avg:92.17ms
step:871/1645 train_time:80277ms step_avg:92.17ms
step:872/1645 train_time:80369ms step_avg:92.17ms
step:873/1645 train_time:80462ms step_avg:92.17ms
step:874/1645 train_time:80555ms step_avg:92.17ms
step:875/1645 train_time:80649ms step_avg:92.17ms
step:875/1645 val_loss:3.5132 train_time:80742ms step_avg:92.28ms
step:876/1645 train_time:80763ms step_avg:92.20ms
step:877/1645 train_time:80839ms step_avg:92.18ms
step:878/1645 train_time:80937ms step_avg:92.18ms
step:879/1645 train_time:81030ms step_avg:92.18ms
step:880/1645 train_time:81122ms step_avg:92.18ms
step:881/1645 train_time:81214ms step_avg:92.18ms
step:882/1645 train_time:81305ms step_avg:92.18ms
step:883/1645 train_time:81399ms step_avg:92.18ms
step:884/1645 train_time:81492ms step_avg:92.18ms
step:885/1645 train_time:81584ms step_avg:92.18ms
step:886/1645 train_time:81676ms step_avg:92.19ms
step:887/1645 train_time:81770ms step_avg:92.19ms
step:888/1645 train_time:81865ms step_avg:92.19ms
step:889/1645 train_time:81960ms step_avg:92.19ms
step:890/1645 train_time:82053ms step_avg:92.19ms
step:891/1645 train_time:82145ms step_avg:92.19ms
step:892/1645 train_time:82238ms step_avg:92.20ms
step:893/1645 train_time:82331ms step_avg:92.20ms
step:894/1645 train_time:82423ms step_avg:92.20ms
step:895/1645 train_time:82516ms step_avg:92.20ms
step:896/1645 train_time:82608ms step_avg:92.20ms
step:897/1645 train_time:82702ms step_avg:92.20ms
step:898/1645 train_time:82795ms step_avg:92.20ms
step:899/1645 train_time:82889ms step_avg:92.20ms
step:900/1645 train_time:82983ms step_avg:92.20ms
step:901/1645 train_time:83077ms step_avg:92.21ms
step:902/1645 train_time:83170ms step_avg:92.21ms
step:903/1645 train_time:83263ms step_avg:92.21ms
step:904/1645 train_time:83355ms step_avg:92.21ms
step:905/1645 train_time:83448ms step_avg:92.21ms
step:906/1645 train_time:83540ms step_avg:92.21ms
step:907/1645 train_time:83633ms step_avg:92.21ms
step:908/1645 train_time:83725ms step_avg:92.21ms
step:909/1645 train_time:83819ms step_avg:92.21ms
step:910/1645 train_time:83912ms step_avg:92.21ms
step:911/1645 train_time:84005ms step_avg:92.21ms
step:912/1645 train_time:84098ms step_avg:92.21ms
step:913/1645 train_time:84191ms step_avg:92.21ms
step:914/1645 train_time:84283ms step_avg:92.21ms
step:915/1645 train_time:84376ms step_avg:92.21ms
step:916/1645 train_time:84468ms step_avg:92.21ms
step:917/1645 train_time:84561ms step_avg:92.21ms
step:918/1645 train_time:84654ms step_avg:92.22ms
step:919/1645 train_time:84747ms step_avg:92.22ms
step:920/1645 train_time:84840ms step_avg:92.22ms
step:921/1645 train_time:84934ms step_avg:92.22ms
step:922/1645 train_time:85027ms step_avg:92.22ms
step:923/1645 train_time:85121ms step_avg:92.22ms
step:924/1645 train_time:85214ms step_avg:92.22ms
step:925/1645 train_time:85307ms step_avg:92.22ms
step:926/1645 train_time:85400ms step_avg:92.22ms
step:927/1645 train_time:85492ms step_avg:92.22ms
step:928/1645 train_time:85585ms step_avg:92.23ms
step:929/1645 train_time:85677ms step_avg:92.23ms
step:930/1645 train_time:85770ms step_avg:92.23ms
step:931/1645 train_time:85863ms step_avg:92.23ms
step:932/1645 train_time:85956ms step_avg:92.23ms
step:933/1645 train_time:86049ms step_avg:92.23ms
step:934/1645 train_time:86142ms step_avg:92.23ms
step:935/1645 train_time:86234ms step_avg:92.23ms
step:936/1645 train_time:86328ms step_avg:92.23ms
step:937/1645 train_time:86421ms step_avg:92.23ms
step:938/1645 train_time:86513ms step_avg:92.23ms
step:939/1645 train_time:86606ms step_avg:92.23ms
step:940/1645 train_time:86700ms step_avg:92.23ms
step:941/1645 train_time:86792ms step_avg:92.23ms
step:942/1645 train_time:86886ms step_avg:92.24ms
step:943/1645 train_time:86978ms step_avg:92.24ms
step:944/1645 train_time:87070ms step_avg:92.24ms
step:945/1645 train_time:87163ms step_avg:92.24ms
step:946/1645 train_time:87256ms step_avg:92.24ms
step:947/1645 train_time:87349ms step_avg:92.24ms
step:948/1645 train_time:87442ms step_avg:92.24ms
step:949/1645 train_time:87535ms step_avg:92.24ms
step:950/1645 train_time:87629ms step_avg:92.24ms
step:951/1645 train_time:87722ms step_avg:92.24ms
step:952/1645 train_time:87816ms step_avg:92.24ms
step:953/1645 train_time:87908ms step_avg:92.24ms
step:954/1645 train_time:88002ms step_avg:92.25ms
step:955/1645 train_time:88095ms step_avg:92.25ms
step:956/1645 train_time:88188ms step_avg:92.25ms
step:957/1645 train_time:88281ms step_avg:92.25ms
step:958/1645 train_time:88373ms step_avg:92.25ms
step:959/1645 train_time:88466ms step_avg:92.25ms
step:960/1645 train_time:88559ms step_avg:92.25ms
step:961/1645 train_time:88652ms step_avg:92.25ms
step:962/1645 train_time:88746ms step_avg:92.25ms
step:963/1645 train_time:88838ms step_avg:92.25ms
step:964/1645 train_time:88931ms step_avg:92.25ms
step:965/1645 train_time:89024ms step_avg:92.25ms
step:966/1645 train_time:89117ms step_avg:92.25ms
step:967/1645 train_time:89210ms step_avg:92.25ms
step:968/1645 train_time:89303ms step_avg:92.26ms
step:969/1645 train_time:89396ms step_avg:92.26ms
step:970/1645 train_time:89488ms step_avg:92.26ms
step:971/1645 train_time:89581ms step_avg:92.26ms
step:972/1645 train_time:89674ms step_avg:92.26ms
step:973/1645 train_time:89767ms step_avg:92.26ms
step:974/1645 train_time:89860ms step_avg:92.26ms
step:975/1645 train_time:89953ms step_avg:92.26ms
step:976/1645 train_time:90046ms step_avg:92.26ms
step:977/1645 train_time:90141ms step_avg:92.26ms
step:978/1645 train_time:90234ms step_avg:92.26ms
step:979/1645 train_time:90327ms step_avg:92.26ms
step:980/1645 train_time:90421ms step_avg:92.27ms
step:981/1645 train_time:90514ms step_avg:92.27ms
step:982/1645 train_time:90607ms step_avg:92.27ms
step:983/1645 train_time:90702ms step_avg:92.27ms
step:984/1645 train_time:90794ms step_avg:92.27ms
step:985/1645 train_time:90886ms step_avg:92.27ms
step:986/1645 train_time:90979ms step_avg:92.27ms
step:987/1645 train_time:91072ms step_avg:92.27ms
step:988/1645 train_time:91164ms step_avg:92.27ms
step:989/1645 train_time:91258ms step_avg:92.27ms
step:990/1645 train_time:91350ms step_avg:92.27ms
step:991/1645 train_time:91443ms step_avg:92.27ms
step:992/1645 train_time:91536ms step_avg:92.27ms
step:993/1645 train_time:91630ms step_avg:92.28ms
step:994/1645 train_time:91722ms step_avg:92.28ms
step:995/1645 train_time:91816ms step_avg:92.28ms
step:996/1645 train_time:91909ms step_avg:92.28ms
step:997/1645 train_time:92003ms step_avg:92.28ms
step:998/1645 train_time:92095ms step_avg:92.28ms
step:999/1645 train_time:92188ms step_avg:92.28ms
step:1000/1645 train_time:92281ms step_avg:92.28ms
step:1000/1645 val_loss:3.4628 train_time:92373ms step_avg:92.37ms
step:1001/1645 train_time:92398ms step_avg:92.31ms
step:1002/1645 train_time:92470ms step_avg:92.29ms
step:1003/1645 train_time:92565ms step_avg:92.29ms
step:1004/1645 train_time:92657ms step_avg:92.29ms
step:1005/1645 train_time:92748ms step_avg:92.29ms
step:1006/1645 train_time:92840ms step_avg:92.29ms
step:1007/1645 train_time:92932ms step_avg:92.29ms
step:1008/1645 train_time:93025ms step_avg:92.29ms
step:1009/1645 train_time:93117ms step_avg:92.29ms
step:1010/1645 train_time:93211ms step_avg:92.29ms
step:1011/1645 train_time:93304ms step_avg:92.29ms
step:1012/1645 train_time:93398ms step_avg:92.29ms
step:1013/1645 train_time:93493ms step_avg:92.29ms
step:1014/1645 train_time:93586ms step_avg:92.29ms
step:1015/1645 train_time:93678ms step_avg:92.29ms
step:1016/1645 train_time:93771ms step_avg:92.29ms
step:1017/1645 train_time:93863ms step_avg:92.29ms
step:1018/1645 train_time:93956ms step_avg:92.29ms
step:1019/1645 train_time:94049ms step_avg:92.30ms
step:1020/1645 train_time:94141ms step_avg:92.30ms
step:1021/1645 train_time:94234ms step_avg:92.30ms
step:1022/1645 train_time:94328ms step_avg:92.30ms
step:1023/1645 train_time:94421ms step_avg:92.30ms
step:1024/1645 train_time:94515ms step_avg:92.30ms
step:1025/1645 train_time:94608ms step_avg:92.30ms
step:1026/1645 train_time:94700ms step_avg:92.30ms
step:1027/1645 train_time:94793ms step_avg:92.30ms
step:1028/1645 train_time:94886ms step_avg:92.30ms
step:1029/1645 train_time:94978ms step_avg:92.30ms
step:1030/1645 train_time:95071ms step_avg:92.30ms
step:1031/1645 train_time:95164ms step_avg:92.30ms
step:1032/1645 train_time:95257ms step_avg:92.30ms
step:1033/1645 train_time:95351ms step_avg:92.30ms
step:1034/1645 train_time:95444ms step_avg:92.31ms
step:1035/1645 train_time:95538ms step_avg:92.31ms
step:1036/1645 train_time:95630ms step_avg:92.31ms
step:1037/1645 train_time:95723ms step_avg:92.31ms
step:1038/1645 train_time:95816ms step_avg:92.31ms
step:1039/1645 train_time:95909ms step_avg:92.31ms
step:1040/1645 train_time:96001ms step_avg:92.31ms
step:1041/1645 train_time:96094ms step_avg:92.31ms
step:1042/1645 train_time:96186ms step_avg:92.31ms
step:1043/1645 train_time:96280ms step_avg:92.31ms
step:1044/1645 train_time:96374ms step_avg:92.31ms
step:1045/1645 train_time:96466ms step_avg:92.31ms
step:1046/1645 train_time:96560ms step_avg:92.31ms
step:1047/1645 train_time:96653ms step_avg:92.31ms
step:1048/1645 train_time:96746ms step_avg:92.32ms
step:1049/1645 train_time:96839ms step_avg:92.32ms
step:1050/1645 train_time:96931ms step_avg:92.32ms
step:1051/1645 train_time:97024ms step_avg:92.32ms
step:1052/1645 train_time:97117ms step_avg:92.32ms
step:1053/1645 train_time:97210ms step_avg:92.32ms
step:1054/1645 train_time:97302ms step_avg:92.32ms
step:1055/1645 train_time:97396ms step_avg:92.32ms
step:1056/1645 train_time:97490ms step_avg:92.32ms
step:1057/1645 train_time:97583ms step_avg:92.32ms
step:1058/1645 train_time:97678ms step_avg:92.32ms
step:1059/1645 train_time:97771ms step_avg:92.32ms
step:1060/1645 train_time:97865ms step_avg:92.33ms
step:1061/1645 train_time:97958ms step_avg:92.33ms
step:1062/1645 train_time:98051ms step_avg:92.33ms
step:1063/1645 train_time:98143ms step_avg:92.33ms
step:1064/1645 train_time:98236ms step_avg:92.33ms
step:1065/1645 train_time:98328ms step_avg:92.33ms
step:1066/1645 train_time:98421ms step_avg:92.33ms
step:1067/1645 train_time:98514ms step_avg:92.33ms
step:1068/1645 train_time:98607ms step_avg:92.33ms
step:1069/1645 train_time:98700ms step_avg:92.33ms
step:1070/1645 train_time:98795ms step_avg:92.33ms
step:1071/1645 train_time:98888ms step_avg:92.33ms
step:1072/1645 train_time:98980ms step_avg:92.33ms
step:1073/1645 train_time:99073ms step_avg:92.33ms
step:1074/1645 train_time:99166ms step_avg:92.33ms
step:1075/1645 train_time:99258ms step_avg:92.33ms
step:1076/1645 train_time:99351ms step_avg:92.33ms
step:1077/1645 train_time:99443ms step_avg:92.33ms
step:1078/1645 train_time:99537ms step_avg:92.33ms
step:1079/1645 train_time:99630ms step_avg:92.34ms
step:1080/1645 train_time:99723ms step_avg:92.34ms
step:1081/1645 train_time:99817ms step_avg:92.34ms
step:1082/1645 train_time:99910ms step_avg:92.34ms
step:1083/1645 train_time:100002ms step_avg:92.34ms
step:1084/1645 train_time:100096ms step_avg:92.34ms
step:1085/1645 train_time:100189ms step_avg:92.34ms
step:1086/1645 train_time:100281ms step_avg:92.34ms
step:1087/1645 train_time:100374ms step_avg:92.34ms
step:1088/1645 train_time:100467ms step_avg:92.34ms
step:1089/1645 train_time:100560ms step_avg:92.34ms
step:1090/1645 train_time:100653ms step_avg:92.34ms
step:1091/1645 train_time:100746ms step_avg:92.34ms
step:1092/1645 train_time:100839ms step_avg:92.34ms
step:1093/1645 train_time:100933ms step_avg:92.34ms
step:1094/1645 train_time:101026ms step_avg:92.35ms
step:1095/1645 train_time:101119ms step_avg:92.35ms
step:1096/1645 train_time:101212ms step_avg:92.35ms
step:1097/1645 train_time:101304ms step_avg:92.35ms
step:1098/1645 train_time:101396ms step_avg:92.35ms
step:1099/1645 train_time:101490ms step_avg:92.35ms
step:1100/1645 train_time:101585ms step_avg:92.35ms
step:1101/1645 train_time:101679ms step_avg:92.35ms
step:1102/1645 train_time:101773ms step_avg:92.35ms
step:1103/1645 train_time:101866ms step_avg:92.35ms
step:1104/1645 train_time:101960ms step_avg:92.36ms
step:1105/1645 train_time:102055ms step_avg:92.36ms
step:1106/1645 train_time:102148ms step_avg:92.36ms
step:1107/1645 train_time:102241ms step_avg:92.36ms
step:1108/1645 train_time:102334ms step_avg:92.36ms
step:1109/1645 train_time:102427ms step_avg:92.36ms
step:1110/1645 train_time:102520ms step_avg:92.36ms
step:1111/1645 train_time:102614ms step_avg:92.36ms
step:1112/1645 train_time:102708ms step_avg:92.36ms
step:1113/1645 train_time:102800ms step_avg:92.36ms
step:1114/1645 train_time:102895ms step_avg:92.37ms
step:1115/1645 train_time:102989ms step_avg:92.37ms
step:1116/1645 train_time:103083ms step_avg:92.37ms
step:1117/1645 train_time:103177ms step_avg:92.37ms
step:1118/1645 train_time:103270ms step_avg:92.37ms
step:1119/1645 train_time:103364ms step_avg:92.37ms
step:1120/1645 train_time:103457ms step_avg:92.37ms
step:1121/1645 train_time:103551ms step_avg:92.37ms
step:1122/1645 train_time:103645ms step_avg:92.38ms
step:1123/1645 train_time:103739ms step_avg:92.38ms
step:1124/1645 train_time:103833ms step_avg:92.38ms
step:1125/1645 train_time:103926ms step_avg:92.38ms
step:1125/1645 val_loss:3.4103 train_time:104020ms step_avg:92.46ms
step:1126/1645 train_time:104041ms step_avg:92.40ms
step:1127/1645 train_time:104122ms step_avg:92.39ms
step:1128/1645 train_time:104223ms step_avg:92.40ms
step:1129/1645 train_time:104319ms step_avg:92.40ms
step:1130/1645 train_time:104411ms step_avg:92.40ms
step:1131/1645 train_time:104503ms step_avg:92.40ms
step:1132/1645 train_time:104596ms step_avg:92.40ms
step:1133/1645 train_time:104688ms step_avg:92.40ms
step:1134/1645 train_time:104781ms step_avg:92.40ms
step:1135/1645 train_time:104873ms step_avg:92.40ms
step:1136/1645 train_time:104966ms step_avg:92.40ms
step:1137/1645 train_time:105061ms step_avg:92.40ms
step:1138/1645 train_time:105158ms step_avg:92.41ms
step:1139/1645 train_time:105253ms step_avg:92.41ms
step:1140/1645 train_time:105348ms step_avg:92.41ms
step:1141/1645 train_time:105441ms step_avg:92.41ms
step:1142/1645 train_time:105535ms step_avg:92.41ms
step:1143/1645 train_time:105627ms step_avg:92.41ms
step:1144/1645 train_time:105720ms step_avg:92.41ms
step:1145/1645 train_time:105812ms step_avg:92.41ms
step:1146/1645 train_time:105905ms step_avg:92.41ms
step:1147/1645 train_time:105999ms step_avg:92.41ms
step:1148/1645 train_time:106093ms step_avg:92.42ms
step:1149/1645 train_time:106188ms step_avg:92.42ms
step:1150/1645 train_time:106282ms step_avg:92.42ms
step:1151/1645 train_time:106376ms step_avg:92.42ms
step:1152/1645 train_time:106469ms step_avg:92.42ms
step:1153/1645 train_time:106563ms step_avg:92.42ms
step:1154/1645 train_time:106656ms step_avg:92.42ms
step:1155/1645 train_time:106749ms step_avg:92.42ms
step:1156/1645 train_time:106842ms step_avg:92.42ms
step:1157/1645 train_time:106935ms step_avg:92.42ms
step:1158/1645 train_time:107029ms step_avg:92.43ms
step:1159/1645 train_time:107123ms step_avg:92.43ms
step:1160/1645 train_time:107217ms step_avg:92.43ms
step:1161/1645 train_time:107312ms step_avg:92.43ms
step:1162/1645 train_time:107405ms step_avg:92.43ms
step:1163/1645 train_time:107498ms step_avg:92.43ms
step:1164/1645 train_time:107592ms step_avg:92.43ms
step:1165/1645 train_time:107685ms step_avg:92.43ms
step:1166/1645 train_time:107779ms step_avg:92.43ms
step:1167/1645 train_time:107872ms step_avg:92.43ms
step:1168/1645 train_time:107965ms step_avg:92.44ms
step:1169/1645 train_time:108058ms step_avg:92.44ms
step:1170/1645 train_time:108152ms step_avg:92.44ms
step:1171/1645 train_time:108247ms step_avg:92.44ms
step:1172/1645 train_time:108341ms step_avg:92.44ms
step:1173/1645 train_time:108434ms step_avg:92.44ms
step:1174/1645 train_time:108528ms step_avg:92.44ms
step:1175/1645 train_time:108621ms step_avg:92.44ms
step:1176/1645 train_time:108714ms step_avg:92.44ms
step:1177/1645 train_time:108807ms step_avg:92.44ms
step:1178/1645 train_time:108900ms step_avg:92.45ms
step:1179/1645 train_time:108994ms step_avg:92.45ms
step:1180/1645 train_time:109087ms step_avg:92.45ms
step:1181/1645 train_time:109181ms step_avg:92.45ms
step:1182/1645 train_time:109274ms step_avg:92.45ms
step:1183/1645 train_time:109368ms step_avg:92.45ms
step:1184/1645 train_time:109462ms step_avg:92.45ms
step:1185/1645 train_time:109555ms step_avg:92.45ms
step:1186/1645 train_time:109648ms step_avg:92.45ms
step:1187/1645 train_time:109742ms step_avg:92.45ms
step:1188/1645 train_time:109835ms step_avg:92.45ms
step:1189/1645 train_time:109928ms step_avg:92.45ms
step:1190/1645 train_time:110022ms step_avg:92.46ms
step:1191/1645 train_time:110115ms step_avg:92.46ms
step:1192/1645 train_time:110209ms step_avg:92.46ms
step:1193/1645 train_time:110303ms step_avg:92.46ms
step:1194/1645 train_time:110397ms step_avg:92.46ms
step:1195/1645 train_time:110490ms step_avg:92.46ms
step:1196/1645 train_time:110584ms step_avg:92.46ms
step:1197/1645 train_time:110677ms step_avg:92.46ms
step:1198/1645 train_time:110771ms step_avg:92.46ms
step:1199/1645 train_time:110865ms step_avg:92.46ms
step:1200/1645 train_time:110958ms step_avg:92.46ms
step:1201/1645 train_time:111051ms step_avg:92.47ms
step:1202/1645 train_time:111145ms step_avg:92.47ms
step:1203/1645 train_time:111239ms step_avg:92.47ms
step:1204/1645 train_time:111333ms step_avg:92.47ms
step:1205/1645 train_time:111427ms step_avg:92.47ms
step:1206/1645 train_time:111520ms step_avg:92.47ms
step:1207/1645 train_time:111613ms step_avg:92.47ms
step:1208/1645 train_time:111707ms step_avg:92.47ms
step:1209/1645 train_time:111800ms step_avg:92.47ms
step:1210/1645 train_time:111893ms step_avg:92.47ms
step:1211/1645 train_time:111987ms step_avg:92.47ms
step:1212/1645 train_time:112080ms step_avg:92.48ms
step:1213/1645 train_time:112173ms step_avg:92.48ms
step:1214/1645 train_time:112267ms step_avg:92.48ms
step:1215/1645 train_time:112360ms step_avg:92.48ms
step:1216/1645 train_time:112454ms step_avg:92.48ms
step:1217/1645 train_time:112547ms step_avg:92.48ms
step:1218/1645 train_time:112641ms step_avg:92.48ms
step:1219/1645 train_time:112734ms step_avg:92.48ms
step:1220/1645 train_time:112827ms step_avg:92.48ms
step:1221/1645 train_time:112922ms step_avg:92.48ms
step:1222/1645 train_time:113015ms step_avg:92.48ms
step:1223/1645 train_time:113108ms step_avg:92.48ms
step:1224/1645 train_time:113204ms step_avg:92.49ms
step:1225/1645 train_time:113297ms step_avg:92.49ms
step:1226/1645 train_time:113390ms step_avg:92.49ms
step:1227/1645 train_time:113484ms step_avg:92.49ms
step:1228/1645 train_time:113578ms step_avg:92.49ms
step:1229/1645 train_time:113670ms step_avg:92.49ms
step:1230/1645 train_time:113765ms step_avg:92.49ms
step:1231/1645 train_time:113858ms step_avg:92.49ms
step:1232/1645 train_time:113951ms step_avg:92.49ms
step:1233/1645 train_time:114045ms step_avg:92.49ms
step:1234/1645 train_time:114138ms step_avg:92.49ms
step:1235/1645 train_time:114231ms step_avg:92.50ms
step:1236/1645 train_time:114325ms step_avg:92.50ms
step:1237/1645 train_time:114419ms step_avg:92.50ms
step:1238/1645 train_time:114513ms step_avg:92.50ms
step:1239/1645 train_time:114606ms step_avg:92.50ms
step:1240/1645 train_time:114700ms step_avg:92.50ms
step:1241/1645 train_time:114793ms step_avg:92.50ms
step:1242/1645 train_time:114886ms step_avg:92.50ms
step:1243/1645 train_time:114980ms step_avg:92.50ms
step:1244/1645 train_time:115073ms step_avg:92.50ms
step:1245/1645 train_time:115167ms step_avg:92.50ms
step:1246/1645 train_time:115261ms step_avg:92.50ms
step:1247/1645 train_time:115354ms step_avg:92.51ms
step:1248/1645 train_time:115448ms step_avg:92.51ms
step:1249/1645 train_time:115541ms step_avg:92.51ms
step:1250/1645 train_time:115634ms step_avg:92.51ms
step:1250/1645 val_loss:3.3722 train_time:115728ms step_avg:92.58ms
step:1251/1645 train_time:115753ms step_avg:92.53ms
step:1252/1645 train_time:115828ms step_avg:92.51ms
step:1253/1645 train_time:115924ms step_avg:92.52ms
step:1254/1645 train_time:116018ms step_avg:92.52ms
step:1255/1645 train_time:116110ms step_avg:92.52ms
step:1256/1645 train_time:116202ms step_avg:92.52ms
step:1257/1645 train_time:116294ms step_avg:92.52ms
step:1258/1645 train_time:116387ms step_avg:92.52ms
step:1259/1645 train_time:116479ms step_avg:92.52ms
step:1260/1645 train_time:116573ms step_avg:92.52ms
step:1261/1645 train_time:116667ms step_avg:92.52ms
step:1262/1645 train_time:116764ms step_avg:92.52ms
step:1263/1645 train_time:116859ms step_avg:92.53ms
step:1264/1645 train_time:116954ms step_avg:92.53ms
step:1265/1645 train_time:117048ms step_avg:92.53ms
step:1266/1645 train_time:117141ms step_avg:92.53ms
step:1267/1645 train_time:117234ms step_avg:92.53ms
step:1268/1645 train_time:117328ms step_avg:92.53ms
step:1269/1645 train_time:117422ms step_avg:92.53ms
step:1270/1645 train_time:117515ms step_avg:92.53ms
step:1271/1645 train_time:117608ms step_avg:92.53ms
step:1272/1645 train_time:117702ms step_avg:92.53ms
step:1273/1645 train_time:117797ms step_avg:92.53ms
step:1274/1645 train_time:117891ms step_avg:92.54ms
step:1275/1645 train_time:117985ms step_avg:92.54ms
step:1276/1645 train_time:118078ms step_avg:92.54ms
step:1277/1645 train_time:118172ms step_avg:92.54ms
step:1278/1645 train_time:118266ms step_avg:92.54ms
step:1279/1645 train_time:118359ms step_avg:92.54ms
step:1280/1645 train_time:118452ms step_avg:92.54ms
step:1281/1645 train_time:118544ms step_avg:92.54ms
step:1282/1645 train_time:118638ms step_avg:92.54ms
step:1283/1645 train_time:118732ms step_avg:92.54ms
step:1284/1645 train_time:118826ms step_avg:92.54ms
step:1285/1645 train_time:118921ms step_avg:92.55ms
step:1286/1645 train_time:119015ms step_avg:92.55ms
step:1287/1645 train_time:119109ms step_avg:92.55ms
step:1288/1645 train_time:119202ms step_avg:92.55ms
step:1289/1645 train_time:119296ms step_avg:92.55ms
step:1290/1645 train_time:119389ms step_avg:92.55ms
step:1291/1645 train_time:119483ms step_avg:92.55ms
step:1292/1645 train_time:119576ms step_avg:92.55ms
step:1293/1645 train_time:119669ms step_avg:92.55ms
step:1294/1645 train_time:119763ms step_avg:92.55ms
step:1295/1645 train_time:119859ms step_avg:92.56ms
step:1296/1645 train_time:119953ms step_avg:92.56ms
step:1297/1645 train_time:120047ms step_avg:92.56ms
step:1298/1645 train_time:120141ms step_avg:92.56ms
step:1299/1645 train_time:120234ms step_avg:92.56ms
step:1300/1645 train_time:120328ms step_avg:92.56ms
step:1301/1645 train_time:120421ms step_avg:92.56ms
step:1302/1645 train_time:120514ms step_avg:92.56ms
step:1303/1645 train_time:120607ms step_avg:92.56ms
step:1304/1645 train_time:120700ms step_avg:92.56ms
step:1305/1645 train_time:120794ms step_avg:92.56ms
step:1306/1645 train_time:120888ms step_avg:92.56ms
step:1307/1645 train_time:120981ms step_avg:92.56ms
step:1308/1645 train_time:121076ms step_avg:92.57ms
step:1309/1645 train_time:121169ms step_avg:92.57ms
step:1310/1645 train_time:121263ms step_avg:92.57ms
step:1311/1645 train_time:121357ms step_avg:92.57ms
step:1312/1645 train_time:121451ms step_avg:92.57ms
step:1313/1645 train_time:121544ms step_avg:92.57ms
step:1314/1645 train_time:121637ms step_avg:92.57ms
step:1315/1645 train_time:121730ms step_avg:92.57ms
step:1316/1645 train_time:121824ms step_avg:92.57ms
step:1317/1645 train_time:121917ms step_avg:92.57ms
step:1318/1645 train_time:122011ms step_avg:92.57ms
step:1319/1645 train_time:122105ms step_avg:92.57ms
step:1320/1645 train_time:122199ms step_avg:92.57ms
step:1321/1645 train_time:122293ms step_avg:92.58ms
step:1322/1645 train_time:122386ms step_avg:92.58ms
step:1323/1645 train_time:122480ms step_avg:92.58ms
step:1324/1645 train_time:122574ms step_avg:92.58ms
step:1325/1645 train_time:122668ms step_avg:92.58ms
step:1326/1645 train_time:122762ms step_avg:92.58ms
step:1327/1645 train_time:122855ms step_avg:92.58ms
step:1328/1645 train_time:122949ms step_avg:92.58ms
step:1329/1645 train_time:123042ms step_avg:92.58ms
step:1330/1645 train_time:123136ms step_avg:92.58ms
step:1331/1645 train_time:123230ms step_avg:92.58ms
step:1332/1645 train_time:123324ms step_avg:92.59ms
step:1333/1645 train_time:123418ms step_avg:92.59ms
step:1334/1645 train_time:123510ms step_avg:92.59ms
step:1335/1645 train_time:123604ms step_avg:92.59ms
step:1336/1645 train_time:123698ms step_avg:92.59ms
step:1337/1645 train_time:123792ms step_avg:92.59ms
step:1338/1645 train_time:123885ms step_avg:92.59ms
step:1339/1645 train_time:123979ms step_avg:92.59ms
step:1340/1645 train_time:124073ms step_avg:92.59ms
step:1341/1645 train_time:124169ms step_avg:92.59ms
step:1342/1645 train_time:124262ms step_avg:92.59ms
step:1343/1645 train_time:124356ms step_avg:92.60ms
step:1344/1645 train_time:124449ms step_avg:92.60ms
step:1345/1645 train_time:124542ms step_avg:92.60ms
step:1346/1645 train_time:124636ms step_avg:92.60ms
step:1347/1645 train_time:124730ms step_avg:92.60ms
step:1348/1645 train_time:124823ms step_avg:92.60ms
step:1349/1645 train_time:124917ms step_avg:92.60ms
step:1350/1645 train_time:125010ms step_avg:92.60ms
step:1351/1645 train_time:125104ms step_avg:92.60ms
step:1352/1645 train_time:125197ms step_avg:92.60ms
step:1353/1645 train_time:125291ms step_avg:92.60ms
step:1354/1645 train_time:125385ms step_avg:92.60ms
step:1355/1645 train_time:125478ms step_avg:92.60ms
step:1356/1645 train_time:125572ms step_avg:92.60ms
step:1357/1645 train_time:125665ms step_avg:92.61ms
step:1358/1645 train_time:125759ms step_avg:92.61ms
step:1359/1645 train_time:125852ms step_avg:92.61ms
step:1360/1645 train_time:125945ms step_avg:92.61ms
step:1361/1645 train_time:126039ms step_avg:92.61ms
step:1362/1645 train_time:126132ms step_avg:92.61ms
step:1363/1645 train_time:126226ms step_avg:92.61ms
step:1364/1645 train_time:126319ms step_avg:92.61ms
step:1365/1645 train_time:126412ms step_avg:92.61ms
step:1366/1645 train_time:126506ms step_avg:92.61ms
step:1367/1645 train_time:126599ms step_avg:92.61ms
step:1368/1645 train_time:126693ms step_avg:92.61ms
step:1369/1645 train_time:126787ms step_avg:92.61ms
step:1370/1645 train_time:126880ms step_avg:92.61ms
step:1371/1645 train_time:126974ms step_avg:92.61ms
step:1372/1645 train_time:127068ms step_avg:92.62ms
step:1373/1645 train_time:127162ms step_avg:92.62ms
step:1374/1645 train_time:127256ms step_avg:92.62ms
step:1375/1645 train_time:127350ms step_avg:92.62ms
step:1375/1645 val_loss:3.3377 train_time:127444ms step_avg:92.69ms
step:1376/1645 train_time:127470ms step_avg:92.64ms
step:1377/1645 train_time:127543ms step_avg:92.62ms
step:1378/1645 train_time:127638ms step_avg:92.63ms
step:1379/1645 train_time:127731ms step_avg:92.63ms
step:1380/1645 train_time:127825ms step_avg:92.63ms
step:1381/1645 train_time:127917ms step_avg:92.63ms
step:1382/1645 train_time:128009ms step_avg:92.63ms
step:1383/1645 train_time:128102ms step_avg:92.63ms
step:1384/1645 train_time:128197ms step_avg:92.63ms
step:1385/1645 train_time:128291ms step_avg:92.63ms
step:1386/1645 train_time:128385ms step_avg:92.63ms
step:1387/1645 train_time:128480ms step_avg:92.63ms
step:1388/1645 train_time:128575ms step_avg:92.63ms
step:1389/1645 train_time:128669ms step_avg:92.63ms
step:1390/1645 train_time:128762ms step_avg:92.63ms
step:1391/1645 train_time:128855ms step_avg:92.63ms
step:1392/1645 train_time:128948ms step_avg:92.64ms
step:1393/1645 train_time:129041ms step_avg:92.64ms
step:1394/1645 train_time:129135ms step_avg:92.64ms
step:1395/1645 train_time:129229ms step_avg:92.64ms
step:1396/1645 train_time:129323ms step_avg:92.64ms
step:1397/1645 train_time:129417ms step_avg:92.64ms
step:1398/1645 train_time:129511ms step_avg:92.64ms
step:1399/1645 train_time:129606ms step_avg:92.64ms
step:1400/1645 train_time:129699ms step_avg:92.64ms
step:1401/1645 train_time:129793ms step_avg:92.64ms
step:1402/1645 train_time:129887ms step_avg:92.64ms
step:1403/1645 train_time:129980ms step_avg:92.64ms
step:1404/1645 train_time:130072ms step_avg:92.64ms
step:1405/1645 train_time:130166ms step_avg:92.64ms
step:1406/1645 train_time:130260ms step_avg:92.65ms
step:1407/1645 train_time:130355ms step_avg:92.65ms
step:1408/1645 train_time:130449ms step_avg:92.65ms
step:1409/1645 train_time:130543ms step_avg:92.65ms
step:1410/1645 train_time:130639ms step_avg:92.65ms
step:1411/1645 train_time:130731ms step_avg:92.65ms
step:1412/1645 train_time:130826ms step_avg:92.65ms
step:1413/1645 train_time:130918ms step_avg:92.65ms
step:1414/1645 train_time:131012ms step_avg:92.65ms
step:1415/1645 train_time:131105ms step_avg:92.65ms
step:1416/1645 train_time:131199ms step_avg:92.65ms
step:1417/1645 train_time:131292ms step_avg:92.66ms
step:1418/1645 train_time:131386ms step_avg:92.66ms
step:1419/1645 train_time:131481ms step_avg:92.66ms
step:1420/1645 train_time:131575ms step_avg:92.66ms
step:1421/1645 train_time:131668ms step_avg:92.66ms
step:1422/1645 train_time:131762ms step_avg:92.66ms
step:1423/1645 train_time:131856ms step_avg:92.66ms
step:1424/1645 train_time:131950ms step_avg:92.66ms
step:1425/1645 train_time:132044ms step_avg:92.66ms
step:1426/1645 train_time:132136ms step_avg:92.66ms
step:1427/1645 train_time:132229ms step_avg:92.66ms
step:1428/1645 train_time:132323ms step_avg:92.66ms
step:1429/1645 train_time:132417ms step_avg:92.66ms
step:1430/1645 train_time:132510ms step_avg:92.66ms
step:1431/1645 train_time:132605ms step_avg:92.67ms
step:1432/1645 train_time:132699ms step_avg:92.67ms
step:1433/1645 train_time:132792ms step_avg:92.67ms
step:1434/1645 train_time:132886ms step_avg:92.67ms
step:1435/1645 train_time:132980ms step_avg:92.67ms
step:1436/1645 train_time:133074ms step_avg:92.67ms
step:1437/1645 train_time:133167ms step_avg:92.67ms
step:1438/1645 train_time:133261ms step_avg:92.67ms
step:1439/1645 train_time:133355ms step_avg:92.67ms
step:1440/1645 train_time:133450ms step_avg:92.67ms
step:1441/1645 train_time:133542ms step_avg:92.67ms
step:1442/1645 train_time:133636ms step_avg:92.67ms
step:1443/1645 train_time:133729ms step_avg:92.67ms
step:1444/1645 train_time:133823ms step_avg:92.68ms
step:1445/1645 train_time:133916ms step_avg:92.68ms
step:1446/1645 train_time:134011ms step_avg:92.68ms
step:1447/1645 train_time:134104ms step_avg:92.68ms
step:1448/1645 train_time:134197ms step_avg:92.68ms
step:1449/1645 train_time:134292ms step_avg:92.68ms
step:1450/1645 train_time:134386ms step_avg:92.68ms
step:1451/1645 train_time:134479ms step_avg:92.68ms
step:1452/1645 train_time:134573ms step_avg:92.68ms
step:1453/1645 train_time:134666ms step_avg:92.68ms
step:1454/1645 train_time:134760ms step_avg:92.68ms
step:1455/1645 train_time:134853ms step_avg:92.68ms
step:1456/1645 train_time:134947ms step_avg:92.68ms
step:1457/1645 train_time:135042ms step_avg:92.69ms
step:1458/1645 train_time:135136ms step_avg:92.69ms
step:1459/1645 train_time:135229ms step_avg:92.69ms
step:1460/1645 train_time:135323ms step_avg:92.69ms
step:1461/1645 train_time:135417ms step_avg:92.69ms
step:1462/1645 train_time:135511ms step_avg:92.69ms
step:1463/1645 train_time:135605ms step_avg:92.69ms
step:1464/1645 train_time:135700ms step_avg:92.69ms
step:1465/1645 train_time:135794ms step_avg:92.69ms
step:1466/1645 train_time:135887ms step_avg:92.69ms
step:1467/1645 train_time:135981ms step_avg:92.69ms
step:1468/1645 train_time:136075ms step_avg:92.69ms
step:1469/1645 train_time:136168ms step_avg:92.69ms
step:1470/1645 train_time:136263ms step_avg:92.70ms
step:1471/1645 train_time:136356ms step_avg:92.70ms
step:1472/1645 train_time:136449ms step_avg:92.70ms
step:1473/1645 train_time:136543ms step_avg:92.70ms
step:1474/1645 train_time:136637ms step_avg:92.70ms
step:1475/1645 train_time:136731ms step_avg:92.70ms
step:1476/1645 train_time:136825ms step_avg:92.70ms
step:1477/1645 train_time:136918ms step_avg:92.70ms
step:1478/1645 train_time:137012ms step_avg:92.70ms
step:1479/1645 train_time:137106ms step_avg:92.70ms
step:1480/1645 train_time:137200ms step_avg:92.70ms
step:1481/1645 train_time:137294ms step_avg:92.70ms
step:1482/1645 train_time:137387ms step_avg:92.70ms
step:1483/1645 train_time:137481ms step_avg:92.70ms
step:1484/1645 train_time:137575ms step_avg:92.71ms
step:1485/1645 train_time:137669ms step_avg:92.71ms
step:1486/1645 train_time:137762ms step_avg:92.71ms
step:1487/1645 train_time:137856ms step_avg:92.71ms
step:1488/1645 train_time:137950ms step_avg:92.71ms
step:1489/1645 train_time:138044ms step_avg:92.71ms
step:1490/1645 train_time:138137ms step_avg:92.71ms
step:1491/1645 train_time:138231ms step_avg:92.71ms
step:1492/1645 train_time:138325ms step_avg:92.71ms
step:1493/1645 train_time:138418ms step_avg:92.71ms
step:1494/1645 train_time:138511ms step_avg:92.71ms
step:1495/1645 train_time:138605ms step_avg:92.71ms
step:1496/1645 train_time:138698ms step_avg:92.71ms
step:1497/1645 train_time:138792ms step_avg:92.71ms
step:1498/1645 train_time:138886ms step_avg:92.71ms
step:1499/1645 train_time:138980ms step_avg:92.72ms
step:1500/1645 train_time:139074ms step_avg:92.72ms
step:1500/1645 val_loss:3.3079 train_time:139168ms step_avg:92.78ms
step:1501/1645 train_time:139194ms step_avg:92.73ms
step:1502/1645 train_time:139266ms step_avg:92.72ms
step:1503/1645 train_time:139362ms step_avg:92.72ms
step:1504/1645 train_time:139455ms step_avg:92.72ms
step:1505/1645 train_time:139548ms step_avg:92.72ms
step:1506/1645 train_time:139640ms step_avg:92.72ms
step:1507/1645 train_time:139732ms step_avg:92.72ms
step:1508/1645 train_time:139827ms step_avg:92.72ms
step:1509/1645 train_time:139920ms step_avg:92.72ms
step:1510/1645 train_time:140014ms step_avg:92.72ms
step:1511/1645 train_time:140109ms step_avg:92.73ms
step:1512/1645 train_time:140206ms step_avg:92.73ms
step:1513/1645 train_time:140299ms step_avg:92.73ms
step:1514/1645 train_time:140394ms step_avg:92.73ms
step:1515/1645 train_time:140487ms step_avg:92.73ms
step:1516/1645 train_time:140581ms step_avg:92.73ms
step:1517/1645 train_time:140674ms step_avg:92.73ms
step:1518/1645 train_time:140767ms step_avg:92.73ms
step:1519/1645 train_time:140861ms step_avg:92.73ms
step:1520/1645 train_time:140954ms step_avg:92.73ms
step:1521/1645 train_time:141047ms step_avg:92.73ms
step:1522/1645 train_time:141143ms step_avg:92.73ms
step:1523/1645 train_time:141237ms step_avg:92.74ms
step:1524/1645 train_time:141332ms step_avg:92.74ms
step:1525/1645 train_time:141426ms step_avg:92.74ms
step:1526/1645 train_time:141519ms step_avg:92.74ms
step:1527/1645 train_time:141612ms step_avg:92.74ms
step:1528/1645 train_time:141706ms step_avg:92.74ms
step:1529/1645 train_time:141799ms step_avg:92.74ms
step:1530/1645 train_time:141892ms step_avg:92.74ms
step:1531/1645 train_time:141985ms step_avg:92.74ms
step:1532/1645 train_time:142078ms step_avg:92.74ms
step:1533/1645 train_time:142172ms step_avg:92.74ms
step:1534/1645 train_time:142266ms step_avg:92.74ms
step:1535/1645 train_time:142360ms step_avg:92.74ms
step:1536/1645 train_time:142454ms step_avg:92.74ms
step:1537/1645 train_time:142548ms step_avg:92.74ms
step:1538/1645 train_time:142641ms step_avg:92.74ms
step:1539/1645 train_time:142734ms step_avg:92.74ms
step:1540/1645 train_time:142828ms step_avg:92.75ms
step:1541/1645 train_time:142922ms step_avg:92.75ms
step:1542/1645 train_time:143015ms step_avg:92.75ms
step:1543/1645 train_time:143109ms step_avg:92.75ms
step:1544/1645 train_time:143203ms step_avg:92.75ms
step:1545/1645 train_time:143297ms step_avg:92.75ms
step:1546/1645 train_time:143391ms step_avg:92.75ms
step:1547/1645 train_time:143485ms step_avg:92.75ms
step:1548/1645 train_time:143578ms step_avg:92.75ms
step:1549/1645 train_time:143671ms step_avg:92.75ms
step:1550/1645 train_time:143765ms step_avg:92.75ms
step:1551/1645 train_time:143859ms step_avg:92.75ms
step:1552/1645 train_time:143952ms step_avg:92.75ms
step:1553/1645 train_time:144047ms step_avg:92.75ms
step:1554/1645 train_time:144141ms step_avg:92.75ms
step:1555/1645 train_time:144234ms step_avg:92.76ms
step:1556/1645 train_time:144329ms step_avg:92.76ms
step:1557/1645 train_time:144423ms step_avg:92.76ms
step:1558/1645 train_time:144517ms step_avg:92.76ms
step:1559/1645 train_time:144611ms step_avg:92.76ms
step:1560/1645 train_time:144705ms step_avg:92.76ms
step:1561/1645 train_time:144798ms step_avg:92.76ms
step:1562/1645 train_time:144892ms step_avg:92.76ms
step:1563/1645 train_time:144986ms step_avg:92.76ms
step:1564/1645 train_time:145079ms step_avg:92.76ms
step:1565/1645 train_time:145172ms step_avg:92.76ms
step:1566/1645 train_time:145267ms step_avg:92.76ms
step:1567/1645 train_time:145362ms step_avg:92.76ms
step:1568/1645 train_time:145456ms step_avg:92.77ms
step:1569/1645 train_time:145549ms step_avg:92.77ms
step:1570/1645 train_time:145644ms step_avg:92.77ms
step:1571/1645 train_time:145737ms step_avg:92.77ms
step:1572/1645 train_time:145830ms step_avg:92.77ms
step:1573/1645 train_time:145924ms step_avg:92.77ms
step:1574/1645 train_time:146019ms step_avg:92.77ms
step:1575/1645 train_time:146111ms step_avg:92.77ms
step:1576/1645 train_time:146204ms step_avg:92.77ms
step:1577/1645 train_time:146298ms step_avg:92.77ms
step:1578/1645 train_time:146392ms step_avg:92.77ms
step:1579/1645 train_time:146486ms step_avg:92.77ms
step:1580/1645 train_time:146581ms step_avg:92.77ms
step:1581/1645 train_time:146675ms step_avg:92.77ms
step:1582/1645 train_time:146768ms step_avg:92.77ms
step:1583/1645 train_time:146861ms step_avg:92.77ms
step:1584/1645 train_time:146955ms step_avg:92.77ms
step:1585/1645 train_time:147049ms step_avg:92.78ms
step:1586/1645 train_time:147142ms step_avg:92.78ms
step:1587/1645 train_time:147236ms step_avg:92.78ms
step:1588/1645 train_time:147329ms step_avg:92.78ms
step:1589/1645 train_time:147423ms step_avg:92.78ms
step:1590/1645 train_time:147516ms step_avg:92.78ms
step:1591/1645 train_time:147611ms step_avg:92.78ms
step:1592/1645 train_time:147705ms step_avg:92.78ms
step:1593/1645 train_time:147798ms step_avg:92.78ms
step:1594/1645 train_time:147892ms step_avg:92.78ms
step:1595/1645 train_time:147985ms step_avg:92.78ms
step:1596/1645 train_time:148079ms step_avg:92.78ms
step:1597/1645 train_time:148172ms step_avg:92.78ms
step:1598/1645 train_time:148266ms step_avg:92.78ms
step:1599/1645 train_time:148360ms step_avg:92.78ms
step:1600/1645 train_time:148454ms step_avg:92.78ms
step:1601/1645 train_time:148548ms step_avg:92.78ms
step:1602/1645 train_time:148642ms step_avg:92.79ms
step:1603/1645 train_time:148736ms step_avg:92.79ms
step:1604/1645 train_time:148830ms step_avg:92.79ms
step:1605/1645 train_time:148924ms step_avg:92.79ms
step:1606/1645 train_time:149017ms step_avg:92.79ms
step:1607/1645 train_time:149111ms step_avg:92.79ms
step:1608/1645 train_time:149204ms step_avg:92.79ms
step:1609/1645 train_time:149297ms step_avg:92.79ms
step:1610/1645 train_time:149391ms step_avg:92.79ms
step:1611/1645 train_time:149485ms step_avg:92.79ms
step:1612/1645 train_time:149579ms step_avg:92.79ms
step:1613/1645 train_time:149673ms step_avg:92.79ms
step:1614/1645 train_time:149767ms step_avg:92.79ms
step:1615/1645 train_time:149861ms step_avg:92.79ms
step:1616/1645 train_time:149955ms step_avg:92.79ms
step:1617/1645 train_time:150049ms step_avg:92.79ms
step:1618/1645 train_time:150143ms step_avg:92.80ms
step:1619/1645 train_time:150236ms step_avg:92.80ms
step:1620/1645 train_time:150330ms step_avg:92.80ms
step:1621/1645 train_time:150424ms step_avg:92.80ms
step:1622/1645 train_time:150518ms step_avg:92.80ms
step:1623/1645 train_time:150611ms step_avg:92.80ms
step:1624/1645 train_time:150705ms step_avg:92.80ms
step:1625/1645 train_time:150798ms step_avg:92.80ms
step:1625/1645 val_loss:3.2838 train_time:150893ms step_avg:92.86ms
step:1626/1645 train_time:150918ms step_avg:92.82ms
step:1627/1645 train_time:150991ms step_avg:92.80ms
step:1628/1645 train_time:151087ms step_avg:92.81ms
step:1629/1645 train_time:151181ms step_avg:92.81ms
step:1630/1645 train_time:151273ms step_avg:92.81ms
step:1631/1645 train_time:151366ms step_avg:92.81ms
step:1632/1645 train_time:151459ms step_avg:92.81ms
step:1633/1645 train_time:151552ms step_avg:92.81ms
step:1634/1645 train_time:151645ms step_avg:92.81ms
step:1635/1645 train_time:151739ms step_avg:92.81ms
step:1636/1645 train_time:151833ms step_avg:92.81ms
step:1637/1645 train_time:151928ms step_avg:92.81ms
step:1638/1645 train_time:152023ms step_avg:92.81ms
step:1639/1645 train_time:152117ms step_avg:92.81ms
step:1640/1645 train_time:152211ms step_avg:92.81ms
step:1641/1645 train_time:152305ms step_avg:92.81ms
step:1642/1645 train_time:152398ms step_avg:92.81ms
step:1643/1645 train_time:152491ms step_avg:92.81ms
step:1644/1645 train_time:152583ms step_avg:92.81ms
step:1645/1645 train_time:152677ms step_avg:92.81ms
step:1645/1645 val_loss:3.2781 train_time:152771ms step_avg:92.87ms
peak memory allocated: 31659 MiB reserved: 46796 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:16:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:137ms step_avg:136.88ms
step:2/1645 train_time:156ms step_avg:78.07ms
step:3/1645 train_time:227ms step_avg:75.80ms
step:4/1645 train_time:317ms step_avg:79.27ms
step:5/1645 train_time:408ms step_avg:81.54ms
step:6/1645 train_time:499ms step_avg:83.17ms
step:7/1645 train_time:590ms step_avg:84.26ms
step:8/1645 train_time:681ms step_avg:85.17ms
step:9/1645 train_time:772ms step_avg:85.82ms
step:10/1645 train_time:863ms step_avg:86.33ms
step:11/1645 train_time:954ms step_avg:86.73ms
step:12/1645 train_time:1048ms step_avg:87.33ms
step:13/1645 train_time:1144ms step_avg:87.99ms
step:14/1645 train_time:1238ms step_avg:88.44ms
step:15/1645 train_time:1333ms step_avg:88.84ms
step:16/1645 train_time:1425ms step_avg:89.06ms
step:17/1645 train_time:1516ms step_avg:89.20ms
step:18/1645 train_time:1608ms step_avg:89.31ms
step:19/1645 train_time:1699ms step_avg:89.44ms
step:20/1645 train_time:1791ms step_avg:89.53ms
step:21/1645 train_time:1881ms step_avg:89.59ms
step:22/1645 train_time:1974ms step_avg:89.71ms
step:23/1645 train_time:2066ms step_avg:89.84ms
step:24/1645 train_time:2160ms step_avg:89.99ms
step:25/1645 train_time:2253ms step_avg:90.13ms
step:26/1645 train_time:2346ms step_avg:90.24ms
step:27/1645 train_time:2439ms step_avg:90.32ms
step:28/1645 train_time:2531ms step_avg:90.39ms
step:29/1645 train_time:2622ms step_avg:90.43ms
step:30/1645 train_time:2714ms step_avg:90.48ms
step:31/1645 train_time:2806ms step_avg:90.50ms
step:32/1645 train_time:2897ms step_avg:90.54ms
step:33/1645 train_time:2990ms step_avg:90.60ms
step:34/1645 train_time:3081ms step_avg:90.63ms
step:35/1645 train_time:3174ms step_avg:90.69ms
step:36/1645 train_time:3266ms step_avg:90.73ms
step:37/1645 train_time:3358ms step_avg:90.76ms
step:38/1645 train_time:3450ms step_avg:90.79ms
step:39/1645 train_time:3542ms step_avg:90.81ms
step:40/1645 train_time:3633ms step_avg:90.82ms
step:41/1645 train_time:3726ms step_avg:90.87ms
step:42/1645 train_time:3817ms step_avg:90.88ms
step:43/1645 train_time:3909ms step_avg:90.91ms
step:44/1645 train_time:4001ms step_avg:90.92ms
step:45/1645 train_time:4093ms step_avg:90.95ms
step:46/1645 train_time:4187ms step_avg:91.02ms
step:47/1645 train_time:4278ms step_avg:91.02ms
step:48/1645 train_time:4370ms step_avg:91.03ms
step:49/1645 train_time:4461ms step_avg:91.04ms
step:50/1645 train_time:4553ms step_avg:91.06ms
step:51/1645 train_time:4645ms step_avg:91.08ms
step:52/1645 train_time:4736ms step_avg:91.09ms
step:53/1645 train_time:4828ms step_avg:91.10ms
step:54/1645 train_time:4920ms step_avg:91.11ms
step:55/1645 train_time:5012ms step_avg:91.13ms
step:56/1645 train_time:5104ms step_avg:91.14ms
step:57/1645 train_time:5195ms step_avg:91.15ms
step:58/1645 train_time:5287ms step_avg:91.16ms
step:59/1645 train_time:5379ms step_avg:91.16ms
step:60/1645 train_time:5470ms step_avg:91.17ms
step:61/1645 train_time:5562ms step_avg:91.17ms
step:62/1645 train_time:5653ms step_avg:91.18ms
step:63/1645 train_time:5745ms step_avg:91.19ms
step:64/1645 train_time:5837ms step_avg:91.20ms
step:65/1645 train_time:5929ms step_avg:91.22ms
step:66/1645 train_time:6022ms step_avg:91.25ms
step:67/1645 train_time:6114ms step_avg:91.26ms
step:68/1645 train_time:6206ms step_avg:91.26ms
step:69/1645 train_time:6298ms step_avg:91.28ms
step:70/1645 train_time:6390ms step_avg:91.29ms
step:71/1645 train_time:6482ms step_avg:91.30ms
step:72/1645 train_time:6574ms step_avg:91.30ms
step:73/1645 train_time:6665ms step_avg:91.30ms
step:74/1645 train_time:6756ms step_avg:91.30ms
step:75/1645 train_time:6848ms step_avg:91.31ms
step:76/1645 train_time:6939ms step_avg:91.31ms
step:77/1645 train_time:7033ms step_avg:91.34ms
step:78/1645 train_time:7126ms step_avg:91.36ms
step:79/1645 train_time:7217ms step_avg:91.36ms
step:80/1645 train_time:7310ms step_avg:91.38ms
step:81/1645 train_time:7401ms step_avg:91.37ms
step:82/1645 train_time:7493ms step_avg:91.37ms
step:83/1645 train_time:7584ms step_avg:91.37ms
step:84/1645 train_time:7675ms step_avg:91.37ms
step:85/1645 train_time:7766ms step_avg:91.36ms
step:86/1645 train_time:7858ms step_avg:91.37ms
step:87/1645 train_time:7950ms step_avg:91.38ms
step:88/1645 train_time:8042ms step_avg:91.39ms
step:89/1645 train_time:8134ms step_avg:91.39ms
step:90/1645 train_time:8226ms step_avg:91.40ms
step:91/1645 train_time:8319ms step_avg:91.41ms
step:92/1645 train_time:8410ms step_avg:91.42ms
step:93/1645 train_time:8503ms step_avg:91.43ms
step:94/1645 train_time:8595ms step_avg:91.44ms
step:95/1645 train_time:8686ms step_avg:91.43ms
step:96/1645 train_time:8778ms step_avg:91.43ms
step:97/1645 train_time:8869ms step_avg:91.44ms
step:98/1645 train_time:8961ms step_avg:91.44ms
step:99/1645 train_time:9053ms step_avg:91.44ms
step:100/1645 train_time:9144ms step_avg:91.44ms
step:101/1645 train_time:9236ms step_avg:91.45ms
step:102/1645 train_time:9328ms step_avg:91.45ms
step:103/1645 train_time:9420ms step_avg:91.46ms
step:104/1645 train_time:9513ms step_avg:91.47ms
step:105/1645 train_time:9605ms step_avg:91.47ms
step:106/1645 train_time:9696ms step_avg:91.47ms
step:107/1645 train_time:9788ms step_avg:91.47ms
step:108/1645 train_time:9880ms step_avg:91.48ms
step:109/1645 train_time:9971ms step_avg:91.48ms
step:110/1645 train_time:10063ms step_avg:91.48ms
step:111/1645 train_time:10155ms step_avg:91.49ms
step:112/1645 train_time:10246ms step_avg:91.48ms
step:113/1645 train_time:10338ms step_avg:91.48ms
step:114/1645 train_time:10430ms step_avg:91.49ms
step:115/1645 train_time:10522ms step_avg:91.50ms
step:116/1645 train_time:10615ms step_avg:91.51ms
step:117/1645 train_time:10707ms step_avg:91.51ms
step:118/1645 train_time:10799ms step_avg:91.51ms
step:119/1645 train_time:10890ms step_avg:91.51ms
step:120/1645 train_time:10982ms step_avg:91.52ms
step:121/1645 train_time:11074ms step_avg:91.52ms
step:122/1645 train_time:11165ms step_avg:91.52ms
step:123/1645 train_time:11256ms step_avg:91.51ms
step:124/1645 train_time:11349ms step_avg:91.52ms
step:125/1645 train_time:11440ms step_avg:91.52ms
step:125/1645 val_loss:4.2992 train_time:11533ms step_avg:92.26ms
step:126/1645 train_time:11548ms step_avg:91.65ms
step:127/1645 train_time:11634ms step_avg:91.61ms
step:128/1645 train_time:11736ms step_avg:91.68ms
step:129/1645 train_time:11828ms step_avg:91.69ms
step:130/1645 train_time:11919ms step_avg:91.69ms
step:131/1645 train_time:12010ms step_avg:91.68ms
step:132/1645 train_time:12101ms step_avg:91.67ms
step:133/1645 train_time:12191ms step_avg:91.66ms
step:134/1645 train_time:12282ms step_avg:91.66ms
step:135/1645 train_time:12372ms step_avg:91.65ms
step:136/1645 train_time:12463ms step_avg:91.64ms
step:137/1645 train_time:12556ms step_avg:91.65ms
step:138/1645 train_time:12652ms step_avg:91.68ms
step:139/1645 train_time:12746ms step_avg:91.70ms
step:140/1645 train_time:12839ms step_avg:91.71ms
step:141/1645 train_time:12931ms step_avg:91.71ms
step:142/1645 train_time:13022ms step_avg:91.71ms
step:143/1645 train_time:13113ms step_avg:91.70ms
step:144/1645 train_time:13204ms step_avg:91.69ms
step:145/1645 train_time:13295ms step_avg:91.69ms
step:146/1645 train_time:13386ms step_avg:91.68ms
step:147/1645 train_time:13476ms step_avg:91.68ms
step:148/1645 train_time:13570ms step_avg:91.69ms
step:149/1645 train_time:13664ms step_avg:91.71ms
step:150/1645 train_time:13757ms step_avg:91.71ms
step:151/1645 train_time:13849ms step_avg:91.71ms
step:152/1645 train_time:13941ms step_avg:91.71ms
step:153/1645 train_time:14032ms step_avg:91.71ms
step:154/1645 train_time:14123ms step_avg:91.71ms
step:155/1645 train_time:14214ms step_avg:91.70ms
step:156/1645 train_time:14305ms step_avg:91.70ms
step:157/1645 train_time:14396ms step_avg:91.69ms
step:158/1645 train_time:14488ms step_avg:91.69ms
step:159/1645 train_time:14579ms step_avg:91.69ms
step:160/1645 train_time:14671ms step_avg:91.70ms
step:161/1645 train_time:14763ms step_avg:91.70ms
step:162/1645 train_time:14856ms step_avg:91.70ms
step:163/1645 train_time:14948ms step_avg:91.71ms
step:164/1645 train_time:15040ms step_avg:91.71ms
step:165/1645 train_time:15131ms step_avg:91.70ms
step:166/1645 train_time:15222ms step_avg:91.70ms
step:167/1645 train_time:15314ms step_avg:91.70ms
step:168/1645 train_time:15405ms step_avg:91.70ms
step:169/1645 train_time:15496ms step_avg:91.69ms
step:170/1645 train_time:15588ms step_avg:91.69ms
step:171/1645 train_time:15680ms step_avg:91.69ms
step:172/1645 train_time:15772ms step_avg:91.70ms
step:173/1645 train_time:15865ms step_avg:91.71ms
step:174/1645 train_time:15958ms step_avg:91.71ms
step:175/1645 train_time:16049ms step_avg:91.71ms
step:176/1645 train_time:16141ms step_avg:91.71ms
step:177/1645 train_time:16231ms step_avg:91.70ms
step:178/1645 train_time:16323ms step_avg:91.70ms
step:179/1645 train_time:16414ms step_avg:91.70ms
step:180/1645 train_time:16505ms step_avg:91.70ms
step:181/1645 train_time:16597ms step_avg:91.70ms
step:182/1645 train_time:16689ms step_avg:91.70ms
step:183/1645 train_time:16781ms step_avg:91.70ms
step:184/1645 train_time:16872ms step_avg:91.70ms
step:185/1645 train_time:16965ms step_avg:91.70ms
step:186/1645 train_time:17056ms step_avg:91.70ms
step:187/1645 train_time:17148ms step_avg:91.70ms
step:188/1645 train_time:17240ms step_avg:91.70ms
step:189/1645 train_time:17331ms step_avg:91.70ms
step:190/1645 train_time:17422ms step_avg:91.69ms
step:191/1645 train_time:17514ms step_avg:91.69ms
step:192/1645 train_time:17605ms step_avg:91.69ms
step:193/1645 train_time:17697ms step_avg:91.70ms
step:194/1645 train_time:17790ms step_avg:91.70ms
step:195/1645 train_time:17881ms step_avg:91.70ms
step:196/1645 train_time:17972ms step_avg:91.70ms
step:197/1645 train_time:18066ms step_avg:91.71ms
step:198/1645 train_time:18158ms step_avg:91.71ms
step:199/1645 train_time:18249ms step_avg:91.71ms
step:200/1645 train_time:18340ms step_avg:91.70ms
step:201/1645 train_time:18432ms step_avg:91.70ms
step:202/1645 train_time:18523ms step_avg:91.70ms
step:203/1645 train_time:18615ms step_avg:91.70ms
step:204/1645 train_time:18707ms step_avg:91.70ms
step:205/1645 train_time:18799ms step_avg:91.70ms
step:206/1645 train_time:18890ms step_avg:91.70ms
step:207/1645 train_time:18981ms step_avg:91.70ms
step:208/1645 train_time:19073ms step_avg:91.70ms
step:209/1645 train_time:19166ms step_avg:91.70ms
step:210/1645 train_time:19259ms step_avg:91.71ms
step:211/1645 train_time:19350ms step_avg:91.71ms
step:212/1645 train_time:19441ms step_avg:91.70ms
step:213/1645 train_time:19534ms step_avg:91.71ms
step:214/1645 train_time:19625ms step_avg:91.70ms
step:215/1645 train_time:19716ms step_avg:91.70ms
step:216/1645 train_time:19807ms step_avg:91.70ms
step:217/1645 train_time:19898ms step_avg:91.70ms
step:218/1645 train_time:19991ms step_avg:91.70ms
step:219/1645 train_time:20081ms step_avg:91.69ms
step:220/1645 train_time:20173ms step_avg:91.69ms
step:221/1645 train_time:20266ms step_avg:91.70ms
step:222/1645 train_time:20357ms step_avg:91.70ms
step:223/1645 train_time:20448ms step_avg:91.70ms
step:224/1645 train_time:20541ms step_avg:91.70ms
step:225/1645 train_time:20633ms step_avg:91.70ms
step:226/1645 train_time:20724ms step_avg:91.70ms
step:227/1645 train_time:20816ms step_avg:91.70ms
step:228/1645 train_time:20908ms step_avg:91.70ms
step:229/1645 train_time:21000ms step_avg:91.70ms
step:230/1645 train_time:21091ms step_avg:91.70ms
step:231/1645 train_time:21183ms step_avg:91.70ms
step:232/1645 train_time:21275ms step_avg:91.70ms
step:233/1645 train_time:21367ms step_avg:91.70ms
step:234/1645 train_time:21459ms step_avg:91.71ms
step:235/1645 train_time:21551ms step_avg:91.71ms
step:236/1645 train_time:21643ms step_avg:91.71ms
step:237/1645 train_time:21733ms step_avg:91.70ms
step:238/1645 train_time:21825ms step_avg:91.70ms
step:239/1645 train_time:21916ms step_avg:91.70ms
step:240/1645 train_time:22008ms step_avg:91.70ms
step:241/1645 train_time:22100ms step_avg:91.70ms
step:242/1645 train_time:22191ms step_avg:91.70ms
step:243/1645 train_time:22282ms step_avg:91.69ms
step:244/1645 train_time:22373ms step_avg:91.69ms
step:245/1645 train_time:22465ms step_avg:91.70ms
step:246/1645 train_time:22558ms step_avg:91.70ms
step:247/1645 train_time:22649ms step_avg:91.70ms
step:248/1645 train_time:22741ms step_avg:91.70ms
step:249/1645 train_time:22833ms step_avg:91.70ms
step:250/1645 train_time:22924ms step_avg:91.70ms
step:250/1645 val_loss:3.9651 train_time:23016ms step_avg:92.07ms
step:251/1645 train_time:23031ms step_avg:91.76ms
step:252/1645 train_time:23109ms step_avg:91.70ms
step:253/1645 train_time:23203ms step_avg:91.71ms
step:254/1645 train_time:23294ms step_avg:91.71ms
step:255/1645 train_time:23386ms step_avg:91.71ms
step:256/1645 train_time:23477ms step_avg:91.71ms
step:257/1645 train_time:23567ms step_avg:91.70ms
step:258/1645 train_time:23659ms step_avg:91.70ms
step:259/1645 train_time:23750ms step_avg:91.70ms
step:260/1645 train_time:23842ms step_avg:91.70ms
step:261/1645 train_time:23934ms step_avg:91.70ms
step:262/1645 train_time:24027ms step_avg:91.71ms
step:263/1645 train_time:24120ms step_avg:91.71ms
step:264/1645 train_time:24212ms step_avg:91.71ms
step:265/1645 train_time:24305ms step_avg:91.72ms
step:266/1645 train_time:24396ms step_avg:91.72ms
step:267/1645 train_time:24488ms step_avg:91.71ms
step:268/1645 train_time:24579ms step_avg:91.71ms
step:269/1645 train_time:24670ms step_avg:91.71ms
step:270/1645 train_time:24761ms step_avg:91.71ms
step:271/1645 train_time:24852ms step_avg:91.70ms
step:272/1645 train_time:24944ms step_avg:91.71ms
step:273/1645 train_time:25036ms step_avg:91.71ms
step:274/1645 train_time:25129ms step_avg:91.71ms
step:275/1645 train_time:25221ms step_avg:91.71ms
step:276/1645 train_time:25313ms step_avg:91.71ms
step:277/1645 train_time:25404ms step_avg:91.71ms
step:278/1645 train_time:25496ms step_avg:91.71ms
step:279/1645 train_time:25588ms step_avg:91.71ms
step:280/1645 train_time:25679ms step_avg:91.71ms
step:281/1645 train_time:25771ms step_avg:91.71ms
step:282/1645 train_time:25862ms step_avg:91.71ms
step:283/1645 train_time:25954ms step_avg:91.71ms
step:284/1645 train_time:26046ms step_avg:91.71ms
step:285/1645 train_time:26139ms step_avg:91.72ms
step:286/1645 train_time:26230ms step_avg:91.71ms
step:287/1645 train_time:26322ms step_avg:91.71ms
step:288/1645 train_time:26413ms step_avg:91.71ms
step:289/1645 train_time:26505ms step_avg:91.71ms
step:290/1645 train_time:26596ms step_avg:91.71ms
step:291/1645 train_time:26688ms step_avg:91.71ms
step:292/1645 train_time:26781ms step_avg:91.72ms
step:293/1645 train_time:26873ms step_avg:91.72ms
step:294/1645 train_time:26964ms step_avg:91.71ms
step:295/1645 train_time:27056ms step_avg:91.71ms
step:296/1645 train_time:27147ms step_avg:91.71ms
step:297/1645 train_time:27239ms step_avg:91.71ms
step:298/1645 train_time:27331ms step_avg:91.71ms
step:299/1645 train_time:27422ms step_avg:91.71ms
step:300/1645 train_time:27514ms step_avg:91.71ms
step:301/1645 train_time:27606ms step_avg:91.72ms
step:302/1645 train_time:27698ms step_avg:91.71ms
step:303/1645 train_time:27790ms step_avg:91.72ms
step:304/1645 train_time:27881ms step_avg:91.71ms
step:305/1645 train_time:27972ms step_avg:91.71ms
step:306/1645 train_time:28064ms step_avg:91.71ms
step:307/1645 train_time:28155ms step_avg:91.71ms
step:308/1645 train_time:28247ms step_avg:91.71ms
step:309/1645 train_time:28339ms step_avg:91.71ms
step:310/1645 train_time:28431ms step_avg:91.71ms
step:311/1645 train_time:28522ms step_avg:91.71ms
step:312/1645 train_time:28615ms step_avg:91.71ms
step:313/1645 train_time:28707ms step_avg:91.72ms
step:314/1645 train_time:28799ms step_avg:91.72ms
step:315/1645 train_time:28891ms step_avg:91.72ms
step:316/1645 train_time:28983ms step_avg:91.72ms
step:317/1645 train_time:29074ms step_avg:91.72ms
step:318/1645 train_time:29165ms step_avg:91.71ms
step:319/1645 train_time:29257ms step_avg:91.71ms
step:320/1645 train_time:29348ms step_avg:91.71ms
step:321/1645 train_time:29441ms step_avg:91.72ms
step:322/1645 train_time:29532ms step_avg:91.72ms
step:323/1645 train_time:29624ms step_avg:91.71ms
step:324/1645 train_time:29716ms step_avg:91.72ms
step:325/1645 train_time:29808ms step_avg:91.72ms
step:326/1645 train_time:29900ms step_avg:91.72ms
step:327/1645 train_time:29992ms step_avg:91.72ms
step:328/1645 train_time:30082ms step_avg:91.71ms
step:329/1645 train_time:30174ms step_avg:91.71ms
step:330/1645 train_time:30265ms step_avg:91.71ms
step:331/1645 train_time:30356ms step_avg:91.71ms
step:332/1645 train_time:30449ms step_avg:91.71ms
step:333/1645 train_time:30540ms step_avg:91.71ms
step:334/1645 train_time:30632ms step_avg:91.71ms
step:335/1645 train_time:30723ms step_avg:91.71ms
step:336/1645 train_time:30816ms step_avg:91.72ms
step:337/1645 train_time:30909ms step_avg:91.72ms
step:338/1645 train_time:31001ms step_avg:91.72ms
step:339/1645 train_time:31093ms step_avg:91.72ms
step:340/1645 train_time:31184ms step_avg:91.72ms
step:341/1645 train_time:31275ms step_avg:91.72ms
step:342/1645 train_time:31367ms step_avg:91.72ms
step:343/1645 train_time:31459ms step_avg:91.72ms
step:344/1645 train_time:31550ms step_avg:91.71ms
step:345/1645 train_time:31642ms step_avg:91.71ms
step:346/1645 train_time:31733ms step_avg:91.72ms
step:347/1645 train_time:31824ms step_avg:91.71ms
step:348/1645 train_time:31918ms step_avg:91.72ms
step:349/1645 train_time:32010ms step_avg:91.72ms
step:350/1645 train_time:32102ms step_avg:91.72ms
step:351/1645 train_time:32193ms step_avg:91.72ms
step:352/1645 train_time:32285ms step_avg:91.72ms
step:353/1645 train_time:32376ms step_avg:91.72ms
step:354/1645 train_time:32467ms step_avg:91.72ms
step:355/1645 train_time:32559ms step_avg:91.72ms
step:356/1645 train_time:32650ms step_avg:91.71ms
step:357/1645 train_time:32742ms step_avg:91.71ms
step:358/1645 train_time:32834ms step_avg:91.72ms
step:359/1645 train_time:32926ms step_avg:91.72ms
step:360/1645 train_time:33019ms step_avg:91.72ms
step:361/1645 train_time:33112ms step_avg:91.72ms
step:362/1645 train_time:33204ms step_avg:91.72ms
step:363/1645 train_time:33295ms step_avg:91.72ms
step:364/1645 train_time:33387ms step_avg:91.72ms
step:365/1645 train_time:33479ms step_avg:91.72ms
step:366/1645 train_time:33570ms step_avg:91.72ms
step:367/1645 train_time:33662ms step_avg:91.72ms
step:368/1645 train_time:33753ms step_avg:91.72ms
step:369/1645 train_time:33844ms step_avg:91.72ms
step:370/1645 train_time:33936ms step_avg:91.72ms
step:371/1645 train_time:34028ms step_avg:91.72ms
step:372/1645 train_time:34120ms step_avg:91.72ms
step:373/1645 train_time:34213ms step_avg:91.72ms
step:374/1645 train_time:34304ms step_avg:91.72ms
step:375/1645 train_time:34396ms step_avg:91.72ms
step:375/1645 val_loss:3.8148 train_time:34488ms step_avg:91.97ms
step:376/1645 train_time:34508ms step_avg:91.78ms
step:377/1645 train_time:34585ms step_avg:91.74ms
step:378/1645 train_time:34680ms step_avg:91.75ms
step:379/1645 train_time:34772ms step_avg:91.75ms
step:380/1645 train_time:34864ms step_avg:91.75ms
step:381/1645 train_time:34954ms step_avg:91.74ms
step:382/1645 train_time:35045ms step_avg:91.74ms
step:383/1645 train_time:35136ms step_avg:91.74ms
step:384/1645 train_time:35227ms step_avg:91.74ms
step:385/1645 train_time:35317ms step_avg:91.73ms
step:386/1645 train_time:35409ms step_avg:91.73ms
step:387/1645 train_time:35502ms step_avg:91.74ms
step:388/1645 train_time:35597ms step_avg:91.74ms
step:389/1645 train_time:35690ms step_avg:91.75ms
step:390/1645 train_time:35783ms step_avg:91.75ms
step:391/1645 train_time:35874ms step_avg:91.75ms
step:392/1645 train_time:35965ms step_avg:91.75ms
step:393/1645 train_time:36056ms step_avg:91.74ms
step:394/1645 train_time:36147ms step_avg:91.74ms
step:395/1645 train_time:36237ms step_avg:91.74ms
step:396/1645 train_time:36329ms step_avg:91.74ms
step:397/1645 train_time:36421ms step_avg:91.74ms
step:398/1645 train_time:36513ms step_avg:91.74ms
step:399/1645 train_time:36605ms step_avg:91.74ms
step:400/1645 train_time:36697ms step_avg:91.74ms
step:401/1645 train_time:36790ms step_avg:91.75ms
step:402/1645 train_time:36882ms step_avg:91.75ms
step:403/1645 train_time:36975ms step_avg:91.75ms
step:404/1645 train_time:37065ms step_avg:91.74ms
step:405/1645 train_time:37156ms step_avg:91.74ms
step:406/1645 train_time:37247ms step_avg:91.74ms
step:407/1645 train_time:37339ms step_avg:91.74ms
step:408/1645 train_time:37431ms step_avg:91.74ms
step:409/1645 train_time:37523ms step_avg:91.74ms
step:410/1645 train_time:37615ms step_avg:91.74ms
step:411/1645 train_time:37708ms step_avg:91.75ms
step:412/1645 train_time:37800ms step_avg:91.75ms
step:413/1645 train_time:37891ms step_avg:91.75ms
step:414/1645 train_time:37982ms step_avg:91.74ms
step:415/1645 train_time:38073ms step_avg:91.74ms
step:416/1645 train_time:38165ms step_avg:91.74ms
step:417/1645 train_time:38256ms step_avg:91.74ms
step:418/1645 train_time:38348ms step_avg:91.74ms
step:419/1645 train_time:38440ms step_avg:91.74ms
step:420/1645 train_time:38532ms step_avg:91.74ms
step:421/1645 train_time:38624ms step_avg:91.74ms
step:422/1645 train_time:38715ms step_avg:91.74ms
step:423/1645 train_time:38807ms step_avg:91.74ms
step:424/1645 train_time:38898ms step_avg:91.74ms
step:425/1645 train_time:38993ms step_avg:91.75ms
step:426/1645 train_time:39086ms step_avg:91.75ms
step:427/1645 train_time:39176ms step_avg:91.75ms
step:428/1645 train_time:39266ms step_avg:91.74ms
step:429/1645 train_time:39358ms step_avg:91.74ms
step:430/1645 train_time:39449ms step_avg:91.74ms
step:431/1645 train_time:39542ms step_avg:91.74ms
step:432/1645 train_time:39633ms step_avg:91.74ms
step:433/1645 train_time:39726ms step_avg:91.75ms
step:434/1645 train_time:39817ms step_avg:91.74ms
step:435/1645 train_time:39908ms step_avg:91.74ms
step:436/1645 train_time:40000ms step_avg:91.74ms
step:437/1645 train_time:40093ms step_avg:91.75ms
step:438/1645 train_time:40184ms step_avg:91.74ms
step:439/1645 train_time:40276ms step_avg:91.75ms
step:440/1645 train_time:40367ms step_avg:91.74ms
step:441/1645 train_time:40459ms step_avg:91.74ms
step:442/1645 train_time:40551ms step_avg:91.75ms
step:443/1645 train_time:40643ms step_avg:91.75ms
step:444/1645 train_time:40735ms step_avg:91.75ms
step:445/1645 train_time:40826ms step_avg:91.74ms
step:446/1645 train_time:40918ms step_avg:91.74ms
step:447/1645 train_time:41010ms step_avg:91.74ms
step:448/1645 train_time:41102ms step_avg:91.74ms
step:449/1645 train_time:41194ms step_avg:91.75ms
step:450/1645 train_time:41284ms step_avg:91.74ms
step:451/1645 train_time:41376ms step_avg:91.74ms
step:452/1645 train_time:41467ms step_avg:91.74ms
step:453/1645 train_time:41560ms step_avg:91.74ms
step:454/1645 train_time:41652ms step_avg:91.74ms
step:455/1645 train_time:41743ms step_avg:91.74ms
step:456/1645 train_time:41835ms step_avg:91.74ms
step:457/1645 train_time:41927ms step_avg:91.74ms
step:458/1645 train_time:42019ms step_avg:91.74ms
step:459/1645 train_time:42110ms step_avg:91.74ms
step:460/1645 train_time:42203ms step_avg:91.75ms
step:461/1645 train_time:42295ms step_avg:91.75ms
step:462/1645 train_time:42384ms step_avg:91.74ms
step:463/1645 train_time:42476ms step_avg:91.74ms
step:464/1645 train_time:42568ms step_avg:91.74ms
step:465/1645 train_time:42660ms step_avg:91.74ms
step:466/1645 train_time:42752ms step_avg:91.74ms
step:467/1645 train_time:42844ms step_avg:91.74ms
step:468/1645 train_time:42936ms step_avg:91.74ms
step:469/1645 train_time:43028ms step_avg:91.74ms
step:470/1645 train_time:43120ms step_avg:91.75ms
step:471/1645 train_time:43212ms step_avg:91.75ms
step:472/1645 train_time:43303ms step_avg:91.74ms
step:473/1645 train_time:43395ms step_avg:91.74ms
step:474/1645 train_time:43486ms step_avg:91.74ms
step:475/1645 train_time:43578ms step_avg:91.74ms
step:476/1645 train_time:43672ms step_avg:91.75ms
step:477/1645 train_time:43764ms step_avg:91.75ms
step:478/1645 train_time:43855ms step_avg:91.75ms
step:479/1645 train_time:43947ms step_avg:91.75ms
step:480/1645 train_time:44038ms step_avg:91.74ms
step:481/1645 train_time:44129ms step_avg:91.74ms
step:482/1645 train_time:44221ms step_avg:91.74ms
step:483/1645 train_time:44312ms step_avg:91.74ms
step:484/1645 train_time:44404ms step_avg:91.74ms
step:485/1645 train_time:44494ms step_avg:91.74ms
step:486/1645 train_time:44587ms step_avg:91.74ms
step:487/1645 train_time:44678ms step_avg:91.74ms
step:488/1645 train_time:44770ms step_avg:91.74ms
step:489/1645 train_time:44862ms step_avg:91.74ms
step:490/1645 train_time:44953ms step_avg:91.74ms
step:491/1645 train_time:45045ms step_avg:91.74ms
step:492/1645 train_time:45137ms step_avg:91.74ms
step:493/1645 train_time:45228ms step_avg:91.74ms
step:494/1645 train_time:45319ms step_avg:91.74ms
step:495/1645 train_time:45413ms step_avg:91.74ms
step:496/1645 train_time:45503ms step_avg:91.74ms
step:497/1645 train_time:45594ms step_avg:91.74ms
step:498/1645 train_time:45686ms step_avg:91.74ms
step:499/1645 train_time:45777ms step_avg:91.74ms
step:500/1645 train_time:45870ms step_avg:91.74ms
step:500/1645 val_loss:3.7130 train_time:45961ms step_avg:91.92ms
step:501/1645 train_time:45982ms step_avg:91.78ms
step:502/1645 train_time:46058ms step_avg:91.75ms
step:503/1645 train_time:46153ms step_avg:91.75ms
step:504/1645 train_time:46244ms step_avg:91.75ms
step:505/1645 train_time:46335ms step_avg:91.75ms
step:506/1645 train_time:46426ms step_avg:91.75ms
step:507/1645 train_time:46516ms step_avg:91.75ms
step:508/1645 train_time:46607ms step_avg:91.75ms
step:509/1645 train_time:46697ms step_avg:91.74ms
step:510/1645 train_time:46788ms step_avg:91.74ms
step:511/1645 train_time:46880ms step_avg:91.74ms
step:512/1645 train_time:46974ms step_avg:91.75ms
step:513/1645 train_time:47068ms step_avg:91.75ms
step:514/1645 train_time:47161ms step_avg:91.75ms
step:515/1645 train_time:47253ms step_avg:91.75ms
step:516/1645 train_time:47344ms step_avg:91.75ms
step:517/1645 train_time:47435ms step_avg:91.75ms
step:518/1645 train_time:47527ms step_avg:91.75ms
step:519/1645 train_time:47618ms step_avg:91.75ms
step:520/1645 train_time:47708ms step_avg:91.75ms
step:521/1645 train_time:47799ms step_avg:91.75ms
step:522/1645 train_time:47891ms step_avg:91.75ms
step:523/1645 train_time:47986ms step_avg:91.75ms
step:524/1645 train_time:48080ms step_avg:91.76ms
step:525/1645 train_time:48171ms step_avg:91.75ms
step:526/1645 train_time:48263ms step_avg:91.75ms
step:527/1645 train_time:48353ms step_avg:91.75ms
step:528/1645 train_time:48445ms step_avg:91.75ms
step:529/1645 train_time:48536ms step_avg:91.75ms
step:530/1645 train_time:48627ms step_avg:91.75ms
step:531/1645 train_time:48717ms step_avg:91.75ms
step:532/1645 train_time:48809ms step_avg:91.75ms
step:533/1645 train_time:48900ms step_avg:91.74ms
step:534/1645 train_time:48992ms step_avg:91.75ms
step:535/1645 train_time:49085ms step_avg:91.75ms
step:536/1645 train_time:49178ms step_avg:91.75ms
step:537/1645 train_time:49270ms step_avg:91.75ms
step:538/1645 train_time:49360ms step_avg:91.75ms
step:539/1645 train_time:49451ms step_avg:91.75ms
step:540/1645 train_time:49543ms step_avg:91.75ms
step:541/1645 train_time:49634ms step_avg:91.75ms
step:542/1645 train_time:49726ms step_avg:91.75ms
step:543/1645 train_time:49817ms step_avg:91.74ms
step:544/1645 train_time:49908ms step_avg:91.74ms
step:545/1645 train_time:50001ms step_avg:91.75ms
step:546/1645 train_time:50094ms step_avg:91.75ms
step:547/1645 train_time:50186ms step_avg:91.75ms
step:548/1645 train_time:50278ms step_avg:91.75ms
step:549/1645 train_time:50369ms step_avg:91.75ms
step:550/1645 train_time:50461ms step_avg:91.75ms
step:551/1645 train_time:50553ms step_avg:91.75ms
step:552/1645 train_time:50646ms step_avg:91.75ms
step:553/1645 train_time:50739ms step_avg:91.75ms
step:554/1645 train_time:50831ms step_avg:91.75ms
step:555/1645 train_time:50925ms step_avg:91.76ms
step:556/1645 train_time:51019ms step_avg:91.76ms
step:557/1645 train_time:51112ms step_avg:91.76ms
step:558/1645 train_time:51206ms step_avg:91.77ms
step:559/1645 train_time:51299ms step_avg:91.77ms
step:560/1645 train_time:51393ms step_avg:91.77ms
step:561/1645 train_time:51485ms step_avg:91.77ms
step:562/1645 train_time:51577ms step_avg:91.77ms
step:563/1645 train_time:51669ms step_avg:91.78ms
step:564/1645 train_time:51762ms step_avg:91.78ms
step:565/1645 train_time:51855ms step_avg:91.78ms
step:566/1645 train_time:51949ms step_avg:91.78ms
step:567/1645 train_time:52043ms step_avg:91.79ms
step:568/1645 train_time:52136ms step_avg:91.79ms
step:569/1645 train_time:52229ms step_avg:91.79ms
step:570/1645 train_time:52322ms step_avg:91.79ms
step:571/1645 train_time:52415ms step_avg:91.79ms
step:572/1645 train_time:52508ms step_avg:91.80ms
step:573/1645 train_time:52601ms step_avg:91.80ms
step:574/1645 train_time:52693ms step_avg:91.80ms
step:575/1645 train_time:52786ms step_avg:91.80ms
step:576/1645 train_time:52878ms step_avg:91.80ms
step:577/1645 train_time:52972ms step_avg:91.81ms
step:578/1645 train_time:53065ms step_avg:91.81ms
step:579/1645 train_time:53159ms step_avg:91.81ms
step:580/1645 train_time:53251ms step_avg:91.81ms
step:581/1645 train_time:53345ms step_avg:91.82ms
step:582/1645 train_time:53437ms step_avg:91.82ms
step:583/1645 train_time:53530ms step_avg:91.82ms
step:584/1645 train_time:53623ms step_avg:91.82ms
step:585/1645 train_time:53716ms step_avg:91.82ms
step:586/1645 train_time:53809ms step_avg:91.82ms
step:587/1645 train_time:53904ms step_avg:91.83ms
step:588/1645 train_time:53996ms step_avg:91.83ms
step:589/1645 train_time:54089ms step_avg:91.83ms
step:590/1645 train_time:54182ms step_avg:91.83ms
step:591/1645 train_time:54275ms step_avg:91.84ms
step:592/1645 train_time:54368ms step_avg:91.84ms
step:593/1645 train_time:54460ms step_avg:91.84ms
step:594/1645 train_time:54553ms step_avg:91.84ms
step:595/1645 train_time:54646ms step_avg:91.84ms
step:596/1645 train_time:54739ms step_avg:91.84ms
step:597/1645 train_time:54832ms step_avg:91.85ms
step:598/1645 train_time:54925ms step_avg:91.85ms
step:599/1645 train_time:55018ms step_avg:91.85ms
step:600/1645 train_time:55111ms step_avg:91.85ms
step:601/1645 train_time:55205ms step_avg:91.86ms
step:602/1645 train_time:55298ms step_avg:91.86ms
step:603/1645 train_time:55391ms step_avg:91.86ms
step:604/1645 train_time:55484ms step_avg:91.86ms
step:605/1645 train_time:55577ms step_avg:91.86ms
step:606/1645 train_time:55670ms step_avg:91.86ms
step:607/1645 train_time:55763ms step_avg:91.87ms
step:608/1645 train_time:55855ms step_avg:91.87ms
step:609/1645 train_time:55948ms step_avg:91.87ms
step:610/1645 train_time:56041ms step_avg:91.87ms
step:611/1645 train_time:56135ms step_avg:91.87ms
step:612/1645 train_time:56229ms step_avg:91.88ms
step:613/1645 train_time:56322ms step_avg:91.88ms
step:614/1645 train_time:56416ms step_avg:91.88ms
step:615/1645 train_time:56509ms step_avg:91.88ms
step:616/1645 train_time:56601ms step_avg:91.89ms
step:617/1645 train_time:56695ms step_avg:91.89ms
step:618/1645 train_time:56787ms step_avg:91.89ms
step:619/1645 train_time:56880ms step_avg:91.89ms
step:620/1645 train_time:56972ms step_avg:91.89ms
step:621/1645 train_time:57065ms step_avg:91.89ms
step:622/1645 train_time:57158ms step_avg:91.89ms
step:623/1645 train_time:57251ms step_avg:91.90ms
step:624/1645 train_time:57344ms step_avg:91.90ms
step:625/1645 train_time:57437ms step_avg:91.90ms
step:625/1645 val_loss:3.6119 train_time:57530ms step_avg:92.05ms
step:626/1645 train_time:57551ms step_avg:91.93ms
step:627/1645 train_time:57629ms step_avg:91.91ms
step:628/1645 train_time:57731ms step_avg:91.93ms
step:629/1645 train_time:57824ms step_avg:91.93ms
step:630/1645 train_time:57916ms step_avg:91.93ms
step:631/1645 train_time:58008ms step_avg:91.93ms
step:632/1645 train_time:58099ms step_avg:91.93ms
step:633/1645 train_time:58191ms step_avg:91.93ms
step:634/1645 train_time:58283ms step_avg:91.93ms
step:635/1645 train_time:58374ms step_avg:91.93ms
step:636/1645 train_time:58466ms step_avg:91.93ms
step:637/1645 train_time:58562ms step_avg:91.93ms
step:638/1645 train_time:58659ms step_avg:91.94ms
step:639/1645 train_time:58754ms step_avg:91.95ms
step:640/1645 train_time:58847ms step_avg:91.95ms
step:641/1645 train_time:58940ms step_avg:91.95ms
step:642/1645 train_time:59032ms step_avg:91.95ms
step:643/1645 train_time:59124ms step_avg:91.95ms
step:644/1645 train_time:59215ms step_avg:91.95ms
step:645/1645 train_time:59307ms step_avg:91.95ms
step:646/1645 train_time:59398ms step_avg:91.95ms
step:647/1645 train_time:59491ms step_avg:91.95ms
step:648/1645 train_time:59588ms step_avg:91.96ms
step:649/1645 train_time:59683ms step_avg:91.96ms
step:650/1645 train_time:59776ms step_avg:91.96ms
step:651/1645 train_time:59870ms step_avg:91.97ms
step:652/1645 train_time:59964ms step_avg:91.97ms
step:653/1645 train_time:60057ms step_avg:91.97ms
step:654/1645 train_time:60149ms step_avg:91.97ms
step:655/1645 train_time:60241ms step_avg:91.97ms
step:656/1645 train_time:60333ms step_avg:91.97ms
step:657/1645 train_time:60425ms step_avg:91.97ms
step:658/1645 train_time:60519ms step_avg:91.97ms
step:659/1645 train_time:60613ms step_avg:91.98ms
step:660/1645 train_time:60707ms step_avg:91.98ms
step:661/1645 train_time:60800ms step_avg:91.98ms
step:662/1645 train_time:60894ms step_avg:91.98ms
step:663/1645 train_time:60987ms step_avg:91.99ms
step:664/1645 train_time:61080ms step_avg:91.99ms
step:665/1645 train_time:61173ms step_avg:91.99ms
step:666/1645 train_time:61265ms step_avg:91.99ms
step:667/1645 train_time:61357ms step_avg:91.99ms
step:668/1645 train_time:61450ms step_avg:91.99ms
step:669/1645 train_time:61544ms step_avg:91.99ms
step:670/1645 train_time:61636ms step_avg:91.99ms
step:671/1645 train_time:61729ms step_avg:92.00ms
step:672/1645 train_time:61823ms step_avg:92.00ms
step:673/1645 train_time:61916ms step_avg:92.00ms
step:674/1645 train_time:62009ms step_avg:92.00ms
step:675/1645 train_time:62102ms step_avg:92.00ms
step:676/1645 train_time:62194ms step_avg:92.00ms
step:677/1645 train_time:62287ms step_avg:92.00ms
step:678/1645 train_time:62380ms step_avg:92.01ms
step:679/1645 train_time:62474ms step_avg:92.01ms
step:680/1645 train_time:62565ms step_avg:92.01ms
step:681/1645 train_time:62659ms step_avg:92.01ms
step:682/1645 train_time:62752ms step_avg:92.01ms
step:683/1645 train_time:62845ms step_avg:92.01ms
step:684/1645 train_time:62938ms step_avg:92.02ms
step:685/1645 train_time:63031ms step_avg:92.02ms
step:686/1645 train_time:63124ms step_avg:92.02ms
step:687/1645 train_time:63216ms step_avg:92.02ms
step:688/1645 train_time:63308ms step_avg:92.02ms
step:689/1645 train_time:63401ms step_avg:92.02ms
step:690/1645 train_time:63495ms step_avg:92.02ms
step:691/1645 train_time:63588ms step_avg:92.02ms
step:692/1645 train_time:63680ms step_avg:92.02ms
step:693/1645 train_time:63773ms step_avg:92.02ms
step:694/1645 train_time:63865ms step_avg:92.02ms
step:695/1645 train_time:63959ms step_avg:92.03ms
step:696/1645 train_time:64053ms step_avg:92.03ms
step:697/1645 train_time:64146ms step_avg:92.03ms
step:698/1645 train_time:64238ms step_avg:92.03ms
step:699/1645 train_time:64331ms step_avg:92.03ms
step:700/1645 train_time:64424ms step_avg:92.03ms
step:701/1645 train_time:64516ms step_avg:92.03ms
step:702/1645 train_time:64609ms step_avg:92.04ms
step:703/1645 train_time:64703ms step_avg:92.04ms
step:704/1645 train_time:64796ms step_avg:92.04ms
step:705/1645 train_time:64889ms step_avg:92.04ms
step:706/1645 train_time:64983ms step_avg:92.04ms
step:707/1645 train_time:65075ms step_avg:92.04ms
step:708/1645 train_time:65167ms step_avg:92.04ms
step:709/1645 train_time:65260ms step_avg:92.04ms
step:710/1645 train_time:65353ms step_avg:92.05ms
step:711/1645 train_time:65446ms step_avg:92.05ms
step:712/1645 train_time:65538ms step_avg:92.05ms
step:713/1645 train_time:65631ms step_avg:92.05ms
step:714/1645 train_time:65724ms step_avg:92.05ms
step:715/1645 train_time:65818ms step_avg:92.05ms
step:716/1645 train_time:65910ms step_avg:92.05ms
step:717/1645 train_time:66003ms step_avg:92.05ms
step:718/1645 train_time:66096ms step_avg:92.06ms
step:719/1645 train_time:66188ms step_avg:92.06ms
step:720/1645 train_time:66281ms step_avg:92.06ms
step:721/1645 train_time:66374ms step_avg:92.06ms
step:722/1645 train_time:66466ms step_avg:92.06ms
step:723/1645 train_time:66559ms step_avg:92.06ms
step:724/1645 train_time:66652ms step_avg:92.06ms
step:725/1645 train_time:66745ms step_avg:92.06ms
step:726/1645 train_time:66838ms step_avg:92.06ms
step:727/1645 train_time:66930ms step_avg:92.06ms
step:728/1645 train_time:67024ms step_avg:92.07ms
step:729/1645 train_time:67117ms step_avg:92.07ms
step:730/1645 train_time:67210ms step_avg:92.07ms
step:731/1645 train_time:67304ms step_avg:92.07ms
step:732/1645 train_time:67396ms step_avg:92.07ms
step:733/1645 train_time:67488ms step_avg:92.07ms
step:734/1645 train_time:67581ms step_avg:92.07ms
step:735/1645 train_time:67674ms step_avg:92.07ms
step:736/1645 train_time:67767ms step_avg:92.07ms
step:737/1645 train_time:67860ms step_avg:92.08ms
step:738/1645 train_time:67953ms step_avg:92.08ms
step:739/1645 train_time:68046ms step_avg:92.08ms
step:740/1645 train_time:68139ms step_avg:92.08ms
step:741/1645 train_time:68231ms step_avg:92.08ms
step:742/1645 train_time:68324ms step_avg:92.08ms
step:743/1645 train_time:68417ms step_avg:92.08ms
step:744/1645 train_time:68510ms step_avg:92.08ms
step:745/1645 train_time:68603ms step_avg:92.08ms
step:746/1645 train_time:68696ms step_avg:92.09ms
step:747/1645 train_time:68788ms step_avg:92.09ms
step:748/1645 train_time:68882ms step_avg:92.09ms
step:749/1645 train_time:68975ms step_avg:92.09ms
step:750/1645 train_time:69067ms step_avg:92.09ms
step:750/1645 val_loss:3.5612 train_time:69161ms step_avg:92.21ms
step:751/1645 train_time:69182ms step_avg:92.12ms
step:752/1645 train_time:69263ms step_avg:92.11ms
step:753/1645 train_time:69363ms step_avg:92.11ms
step:754/1645 train_time:69455ms step_avg:92.12ms
step:755/1645 train_time:69547ms step_avg:92.11ms
step:756/1645 train_time:69639ms step_avg:92.12ms
step:757/1645 train_time:69731ms step_avg:92.11ms
step:758/1645 train_time:69823ms step_avg:92.11ms
step:759/1645 train_time:69915ms step_avg:92.11ms
step:760/1645 train_time:70007ms step_avg:92.11ms
step:761/1645 train_time:70100ms step_avg:92.12ms
step:762/1645 train_time:70195ms step_avg:92.12ms
step:763/1645 train_time:70290ms step_avg:92.12ms
step:764/1645 train_time:70384ms step_avg:92.13ms
step:765/1645 train_time:70478ms step_avg:92.13ms
step:766/1645 train_time:70571ms step_avg:92.13ms
step:767/1645 train_time:70664ms step_avg:92.13ms
step:768/1645 train_time:70756ms step_avg:92.13ms
step:769/1645 train_time:70848ms step_avg:92.13ms
step:770/1645 train_time:70939ms step_avg:92.13ms
step:771/1645 train_time:71031ms step_avg:92.13ms
step:772/1645 train_time:71124ms step_avg:92.13ms
step:773/1645 train_time:71218ms step_avg:92.13ms
step:774/1645 train_time:71312ms step_avg:92.13ms
step:775/1645 train_time:71406ms step_avg:92.14ms
step:776/1645 train_time:71499ms step_avg:92.14ms
step:777/1645 train_time:71592ms step_avg:92.14ms
step:778/1645 train_time:71686ms step_avg:92.14ms
step:779/1645 train_time:71779ms step_avg:92.14ms
step:780/1645 train_time:71871ms step_avg:92.14ms
step:781/1645 train_time:71964ms step_avg:92.14ms
step:782/1645 train_time:72056ms step_avg:92.14ms
step:783/1645 train_time:72149ms step_avg:92.14ms
step:784/1645 train_time:72242ms step_avg:92.15ms
step:785/1645 train_time:72336ms step_avg:92.15ms
step:786/1645 train_time:72429ms step_avg:92.15ms
step:787/1645 train_time:72522ms step_avg:92.15ms
step:788/1645 train_time:72615ms step_avg:92.15ms
step:789/1645 train_time:72708ms step_avg:92.15ms
step:790/1645 train_time:72801ms step_avg:92.15ms
step:791/1645 train_time:72893ms step_avg:92.15ms
step:792/1645 train_time:72985ms step_avg:92.15ms
step:793/1645 train_time:73077ms step_avg:92.15ms
step:794/1645 train_time:73171ms step_avg:92.15ms
step:795/1645 train_time:73263ms step_avg:92.15ms
step:796/1645 train_time:73356ms step_avg:92.16ms
step:797/1645 train_time:73449ms step_avg:92.16ms
step:798/1645 train_time:73543ms step_avg:92.16ms
step:799/1645 train_time:73636ms step_avg:92.16ms
step:800/1645 train_time:73729ms step_avg:92.16ms
step:801/1645 train_time:73821ms step_avg:92.16ms
step:802/1645 train_time:73914ms step_avg:92.16ms
step:803/1645 train_time:74007ms step_avg:92.16ms
step:804/1645 train_time:74100ms step_avg:92.16ms
step:805/1645 train_time:74193ms step_avg:92.17ms
step:806/1645 train_time:74287ms step_avg:92.17ms
step:807/1645 train_time:74381ms step_avg:92.17ms
step:808/1645 train_time:74475ms step_avg:92.17ms
step:809/1645 train_time:74568ms step_avg:92.17ms
step:810/1645 train_time:74660ms step_avg:92.17ms
step:811/1645 train_time:74753ms step_avg:92.17ms
step:812/1645 train_time:74846ms step_avg:92.18ms
step:813/1645 train_time:74938ms step_avg:92.17ms
step:814/1645 train_time:75031ms step_avg:92.18ms
step:815/1645 train_time:75124ms step_avg:92.18ms
step:816/1645 train_time:75217ms step_avg:92.18ms
step:817/1645 train_time:75309ms step_avg:92.18ms
step:818/1645 train_time:75402ms step_avg:92.18ms
step:819/1645 train_time:75495ms step_avg:92.18ms
step:820/1645 train_time:75588ms step_avg:92.18ms
step:821/1645 train_time:75682ms step_avg:92.18ms
step:822/1645 train_time:75775ms step_avg:92.18ms
step:823/1645 train_time:75868ms step_avg:92.18ms
step:824/1645 train_time:75960ms step_avg:92.18ms
step:825/1645 train_time:76054ms step_avg:92.19ms
step:826/1645 train_time:76147ms step_avg:92.19ms
step:827/1645 train_time:76240ms step_avg:92.19ms
step:828/1645 train_time:76332ms step_avg:92.19ms
step:829/1645 train_time:76425ms step_avg:92.19ms
step:830/1645 train_time:76518ms step_avg:92.19ms
step:831/1645 train_time:76611ms step_avg:92.19ms
step:832/1645 train_time:76704ms step_avg:92.19ms
step:833/1645 train_time:76797ms step_avg:92.19ms
step:834/1645 train_time:76890ms step_avg:92.19ms
step:835/1645 train_time:76983ms step_avg:92.19ms
step:836/1645 train_time:77076ms step_avg:92.20ms
step:837/1645 train_time:77168ms step_avg:92.20ms
step:838/1645 train_time:77261ms step_avg:92.20ms
step:839/1645 train_time:77354ms step_avg:92.20ms
step:840/1645 train_time:77446ms step_avg:92.20ms
step:841/1645 train_time:77539ms step_avg:92.20ms
step:842/1645 train_time:77631ms step_avg:92.20ms
step:843/1645 train_time:77725ms step_avg:92.20ms
step:844/1645 train_time:77818ms step_avg:92.20ms
step:845/1645 train_time:77911ms step_avg:92.20ms
step:846/1645 train_time:78005ms step_avg:92.20ms
step:847/1645 train_time:78099ms step_avg:92.21ms
step:848/1645 train_time:78193ms step_avg:92.21ms
step:849/1645 train_time:78285ms step_avg:92.21ms
step:850/1645 train_time:78378ms step_avg:92.21ms
step:851/1645 train_time:78471ms step_avg:92.21ms
step:852/1645 train_time:78564ms step_avg:92.21ms
step:853/1645 train_time:78657ms step_avg:92.21ms
step:854/1645 train_time:78749ms step_avg:92.21ms
step:855/1645 train_time:78843ms step_avg:92.21ms
step:856/1645 train_time:78934ms step_avg:92.21ms
step:857/1645 train_time:79028ms step_avg:92.21ms
step:858/1645 train_time:79121ms step_avg:92.22ms
step:859/1645 train_time:79214ms step_avg:92.22ms
step:860/1645 train_time:79307ms step_avg:92.22ms
step:861/1645 train_time:79401ms step_avg:92.22ms
step:862/1645 train_time:79493ms step_avg:92.22ms
step:863/1645 train_time:79586ms step_avg:92.22ms
step:864/1645 train_time:79678ms step_avg:92.22ms
step:865/1645 train_time:79771ms step_avg:92.22ms
step:866/1645 train_time:79865ms step_avg:92.22ms
step:867/1645 train_time:79957ms step_avg:92.22ms
step:868/1645 train_time:80049ms step_avg:92.22ms
step:869/1645 train_time:80142ms step_avg:92.22ms
step:870/1645 train_time:80235ms step_avg:92.22ms
step:871/1645 train_time:80328ms step_avg:92.22ms
step:872/1645 train_time:80421ms step_avg:92.23ms
step:873/1645 train_time:80514ms step_avg:92.23ms
step:874/1645 train_time:80607ms step_avg:92.23ms
step:875/1645 train_time:80700ms step_avg:92.23ms
step:875/1645 val_loss:3.5125 train_time:80793ms step_avg:92.33ms
step:876/1645 train_time:80814ms step_avg:92.25ms
step:877/1645 train_time:80890ms step_avg:92.24ms
step:878/1645 train_time:80986ms step_avg:92.24ms
step:879/1645 train_time:81078ms step_avg:92.24ms
step:880/1645 train_time:81170ms step_avg:92.24ms
step:881/1645 train_time:81261ms step_avg:92.24ms
step:882/1645 train_time:81353ms step_avg:92.24ms
step:883/1645 train_time:81446ms step_avg:92.24ms
step:884/1645 train_time:81538ms step_avg:92.24ms
step:885/1645 train_time:81631ms step_avg:92.24ms
step:886/1645 train_time:81725ms step_avg:92.24ms
step:887/1645 train_time:81820ms step_avg:92.24ms
step:888/1645 train_time:81916ms step_avg:92.25ms
step:889/1645 train_time:82011ms step_avg:92.25ms
step:890/1645 train_time:82103ms step_avg:92.25ms
step:891/1645 train_time:82195ms step_avg:92.25ms
step:892/1645 train_time:82287ms step_avg:92.25ms
step:893/1645 train_time:82379ms step_avg:92.25ms
step:894/1645 train_time:82471ms step_avg:92.25ms
step:895/1645 train_time:82563ms step_avg:92.25ms
step:896/1645 train_time:82656ms step_avg:92.25ms
step:897/1645 train_time:82751ms step_avg:92.25ms
step:898/1645 train_time:82845ms step_avg:92.25ms
step:899/1645 train_time:82939ms step_avg:92.26ms
step:900/1645 train_time:83033ms step_avg:92.26ms
step:901/1645 train_time:83127ms step_avg:92.26ms
step:902/1645 train_time:83219ms step_avg:92.26ms
step:903/1645 train_time:83312ms step_avg:92.26ms
step:904/1645 train_time:83404ms step_avg:92.26ms
step:905/1645 train_time:83496ms step_avg:92.26ms
step:906/1645 train_time:83589ms step_avg:92.26ms
step:907/1645 train_time:83682ms step_avg:92.26ms
step:908/1645 train_time:83775ms step_avg:92.26ms
step:909/1645 train_time:83869ms step_avg:92.26ms
step:910/1645 train_time:83963ms step_avg:92.27ms
step:911/1645 train_time:84058ms step_avg:92.27ms
step:912/1645 train_time:84151ms step_avg:92.27ms
step:913/1645 train_time:84244ms step_avg:92.27ms
step:914/1645 train_time:84337ms step_avg:92.27ms
step:915/1645 train_time:84428ms step_avg:92.27ms
step:916/1645 train_time:84521ms step_avg:92.27ms
step:917/1645 train_time:84614ms step_avg:92.27ms
step:918/1645 train_time:84706ms step_avg:92.27ms
step:919/1645 train_time:84799ms step_avg:92.27ms
step:920/1645 train_time:84893ms step_avg:92.27ms
step:921/1645 train_time:84986ms step_avg:92.28ms
step:922/1645 train_time:85079ms step_avg:92.28ms
step:923/1645 train_time:85173ms step_avg:92.28ms
step:924/1645 train_time:85266ms step_avg:92.28ms
step:925/1645 train_time:85358ms step_avg:92.28ms
step:926/1645 train_time:85451ms step_avg:92.28ms
step:927/1645 train_time:85544ms step_avg:92.28ms
step:928/1645 train_time:85637ms step_avg:92.28ms
step:929/1645 train_time:85730ms step_avg:92.28ms
step:930/1645 train_time:85824ms step_avg:92.28ms
step:931/1645 train_time:85917ms step_avg:92.28ms
step:932/1645 train_time:86010ms step_avg:92.29ms
step:933/1645 train_time:86103ms step_avg:92.29ms
step:934/1645 train_time:86196ms step_avg:92.29ms
step:935/1645 train_time:86289ms step_avg:92.29ms
step:936/1645 train_time:86381ms step_avg:92.29ms
step:937/1645 train_time:86474ms step_avg:92.29ms
step:938/1645 train_time:86568ms step_avg:92.29ms
step:939/1645 train_time:86659ms step_avg:92.29ms
step:940/1645 train_time:86752ms step_avg:92.29ms
step:941/1645 train_time:86846ms step_avg:92.29ms
step:942/1645 train_time:86939ms step_avg:92.29ms
step:943/1645 train_time:87033ms step_avg:92.29ms
step:944/1645 train_time:87126ms step_avg:92.29ms
step:945/1645 train_time:87219ms step_avg:92.29ms
step:946/1645 train_time:87311ms step_avg:92.29ms
step:947/1645 train_time:87404ms step_avg:92.30ms
step:948/1645 train_time:87496ms step_avg:92.30ms
step:949/1645 train_time:87589ms step_avg:92.30ms
step:950/1645 train_time:87682ms step_avg:92.30ms
step:951/1645 train_time:87775ms step_avg:92.30ms
step:952/1645 train_time:87867ms step_avg:92.30ms
step:953/1645 train_time:87961ms step_avg:92.30ms
step:954/1645 train_time:88054ms step_avg:92.30ms
step:955/1645 train_time:88146ms step_avg:92.30ms
step:956/1645 train_time:88238ms step_avg:92.30ms
step:957/1645 train_time:88333ms step_avg:92.30ms
step:958/1645 train_time:88426ms step_avg:92.30ms
step:959/1645 train_time:88518ms step_avg:92.30ms
step:960/1645 train_time:88611ms step_avg:92.30ms
step:961/1645 train_time:88705ms step_avg:92.30ms
step:962/1645 train_time:88798ms step_avg:92.31ms
step:963/1645 train_time:88891ms step_avg:92.31ms
step:964/1645 train_time:88984ms step_avg:92.31ms
step:965/1645 train_time:89077ms step_avg:92.31ms
step:966/1645 train_time:89170ms step_avg:92.31ms
step:967/1645 train_time:89263ms step_avg:92.31ms
step:968/1645 train_time:89356ms step_avg:92.31ms
step:969/1645 train_time:89448ms step_avg:92.31ms
step:970/1645 train_time:89542ms step_avg:92.31ms
step:971/1645 train_time:89635ms step_avg:92.31ms
step:972/1645 train_time:89729ms step_avg:92.31ms
step:973/1645 train_time:89823ms step_avg:92.32ms
step:974/1645 train_time:89915ms step_avg:92.32ms
step:975/1645 train_time:90007ms step_avg:92.32ms
step:976/1645 train_time:90102ms step_avg:92.32ms
step:977/1645 train_time:90195ms step_avg:92.32ms
step:978/1645 train_time:90288ms step_avg:92.32ms
step:979/1645 train_time:90380ms step_avg:92.32ms
step:980/1645 train_time:90473ms step_avg:92.32ms
step:981/1645 train_time:90566ms step_avg:92.32ms
step:982/1645 train_time:90659ms step_avg:92.32ms
step:983/1645 train_time:90752ms step_avg:92.32ms
step:984/1645 train_time:90846ms step_avg:92.32ms
step:985/1645 train_time:90938ms step_avg:92.32ms
step:986/1645 train_time:91032ms step_avg:92.32ms
step:987/1645 train_time:91125ms step_avg:92.33ms
step:988/1645 train_time:91218ms step_avg:92.33ms
step:989/1645 train_time:91311ms step_avg:92.33ms
step:990/1645 train_time:91404ms step_avg:92.33ms
step:991/1645 train_time:91498ms step_avg:92.33ms
step:992/1645 train_time:91592ms step_avg:92.33ms
step:993/1645 train_time:91684ms step_avg:92.33ms
step:994/1645 train_time:91777ms step_avg:92.33ms
step:995/1645 train_time:91870ms step_avg:92.33ms
step:996/1645 train_time:91963ms step_avg:92.33ms
step:997/1645 train_time:92056ms step_avg:92.33ms
step:998/1645 train_time:92149ms step_avg:92.33ms
step:999/1645 train_time:92243ms step_avg:92.34ms
step:1000/1645 train_time:92336ms step_avg:92.34ms
step:1000/1645 val_loss:3.4642 train_time:92429ms step_avg:92.43ms
step:1001/1645 train_time:92449ms step_avg:92.36ms
step:1002/1645 train_time:92526ms step_avg:92.34ms
step:1003/1645 train_time:92621ms step_avg:92.34ms
step:1004/1645 train_time:92713ms step_avg:92.34ms
step:1005/1645 train_time:92805ms step_avg:92.34ms
step:1006/1645 train_time:92897ms step_avg:92.34ms
step:1007/1645 train_time:92988ms step_avg:92.34ms
step:1008/1645 train_time:93081ms step_avg:92.34ms
step:1009/1645 train_time:93174ms step_avg:92.34ms
step:1010/1645 train_time:93266ms step_avg:92.34ms
step:1011/1645 train_time:93362ms step_avg:92.35ms
step:1012/1645 train_time:93457ms step_avg:92.35ms
step:1013/1645 train_time:93551ms step_avg:92.35ms
step:1014/1645 train_time:93644ms step_avg:92.35ms
step:1015/1645 train_time:93737ms step_avg:92.35ms
step:1016/1645 train_time:93831ms step_avg:92.35ms
step:1017/1645 train_time:93924ms step_avg:92.35ms
step:1018/1645 train_time:94015ms step_avg:92.35ms
step:1019/1645 train_time:94107ms step_avg:92.35ms
step:1020/1645 train_time:94199ms step_avg:92.35ms
step:1021/1645 train_time:94293ms step_avg:92.35ms
step:1022/1645 train_time:94386ms step_avg:92.35ms
step:1023/1645 train_time:94481ms step_avg:92.36ms
step:1024/1645 train_time:94574ms step_avg:92.36ms
step:1025/1645 train_time:94667ms step_avg:92.36ms
step:1026/1645 train_time:94761ms step_avg:92.36ms
step:1027/1645 train_time:94854ms step_avg:92.36ms
step:1028/1645 train_time:94947ms step_avg:92.36ms
step:1029/1645 train_time:95039ms step_avg:92.36ms
step:1030/1645 train_time:95130ms step_avg:92.36ms
step:1031/1645 train_time:95223ms step_avg:92.36ms
step:1032/1645 train_time:95316ms step_avg:92.36ms
step:1033/1645 train_time:95408ms step_avg:92.36ms
step:1034/1645 train_time:95502ms step_avg:92.36ms
step:1035/1645 train_time:95595ms step_avg:92.36ms
step:1036/1645 train_time:95689ms step_avg:92.36ms
step:1037/1645 train_time:95782ms step_avg:92.36ms
step:1038/1645 train_time:95875ms step_avg:92.37ms
step:1039/1645 train_time:95969ms step_avg:92.37ms
step:1040/1645 train_time:96062ms step_avg:92.37ms
step:1041/1645 train_time:96154ms step_avg:92.37ms
step:1042/1645 train_time:96247ms step_avg:92.37ms
step:1043/1645 train_time:96340ms step_avg:92.37ms
step:1044/1645 train_time:96432ms step_avg:92.37ms
step:1045/1645 train_time:96525ms step_avg:92.37ms
step:1046/1645 train_time:96618ms step_avg:92.37ms
step:1047/1645 train_time:96711ms step_avg:92.37ms
step:1048/1645 train_time:96805ms step_avg:92.37ms
step:1049/1645 train_time:96901ms step_avg:92.37ms
step:1050/1645 train_time:96993ms step_avg:92.37ms
step:1051/1645 train_time:97085ms step_avg:92.37ms
step:1052/1645 train_time:97177ms step_avg:92.37ms
step:1053/1645 train_time:97270ms step_avg:92.37ms
step:1054/1645 train_time:97363ms step_avg:92.37ms
step:1055/1645 train_time:97456ms step_avg:92.38ms
step:1056/1645 train_time:97548ms step_avg:92.38ms
step:1057/1645 train_time:97641ms step_avg:92.38ms
step:1058/1645 train_time:97734ms step_avg:92.38ms
step:1059/1645 train_time:97827ms step_avg:92.38ms
step:1060/1645 train_time:97921ms step_avg:92.38ms
step:1061/1645 train_time:98014ms step_avg:92.38ms
step:1062/1645 train_time:98106ms step_avg:92.38ms
step:1063/1645 train_time:98199ms step_avg:92.38ms
step:1064/1645 train_time:98292ms step_avg:92.38ms
step:1065/1645 train_time:98384ms step_avg:92.38ms
step:1066/1645 train_time:98477ms step_avg:92.38ms
step:1067/1645 train_time:98570ms step_avg:92.38ms
step:1068/1645 train_time:98665ms step_avg:92.38ms
step:1069/1645 train_time:98758ms step_avg:92.38ms
step:1070/1645 train_time:98851ms step_avg:92.38ms
step:1071/1645 train_time:98944ms step_avg:92.38ms
step:1072/1645 train_time:99037ms step_avg:92.39ms
step:1073/1645 train_time:99129ms step_avg:92.39ms
step:1074/1645 train_time:99222ms step_avg:92.39ms
step:1075/1645 train_time:99315ms step_avg:92.39ms
step:1076/1645 train_time:99407ms step_avg:92.39ms
step:1077/1645 train_time:99500ms step_avg:92.39ms
step:1078/1645 train_time:99592ms step_avg:92.39ms
step:1079/1645 train_time:99684ms step_avg:92.39ms
step:1080/1645 train_time:99779ms step_avg:92.39ms
step:1081/1645 train_time:99872ms step_avg:92.39ms
step:1082/1645 train_time:99965ms step_avg:92.39ms
step:1083/1645 train_time:100058ms step_avg:92.39ms
step:1084/1645 train_time:100150ms step_avg:92.39ms
step:1085/1645 train_time:100243ms step_avg:92.39ms
step:1086/1645 train_time:100336ms step_avg:92.39ms
step:1087/1645 train_time:100428ms step_avg:92.39ms
step:1088/1645 train_time:100520ms step_avg:92.39ms
step:1089/1645 train_time:100613ms step_avg:92.39ms
step:1090/1645 train_time:100706ms step_avg:92.39ms
step:1091/1645 train_time:100799ms step_avg:92.39ms
step:1092/1645 train_time:100892ms step_avg:92.39ms
step:1093/1645 train_time:100985ms step_avg:92.39ms
step:1094/1645 train_time:101078ms step_avg:92.39ms
step:1095/1645 train_time:101173ms step_avg:92.40ms
step:1096/1645 train_time:101265ms step_avg:92.39ms
step:1097/1645 train_time:101358ms step_avg:92.40ms
step:1098/1645 train_time:101451ms step_avg:92.40ms
step:1099/1645 train_time:101543ms step_avg:92.40ms
step:1100/1645 train_time:101637ms step_avg:92.40ms
step:1101/1645 train_time:101731ms step_avg:92.40ms
step:1102/1645 train_time:101824ms step_avg:92.40ms
step:1103/1645 train_time:101917ms step_avg:92.40ms
step:1104/1645 train_time:102011ms step_avg:92.40ms
step:1105/1645 train_time:102105ms step_avg:92.40ms
step:1106/1645 train_time:102199ms step_avg:92.40ms
step:1107/1645 train_time:102292ms step_avg:92.40ms
step:1108/1645 train_time:102385ms step_avg:92.41ms
step:1109/1645 train_time:102480ms step_avg:92.41ms
step:1110/1645 train_time:102573ms step_avg:92.41ms
step:1111/1645 train_time:102666ms step_avg:92.41ms
step:1112/1645 train_time:102760ms step_avg:92.41ms
step:1113/1645 train_time:102853ms step_avg:92.41ms
step:1114/1645 train_time:102946ms step_avg:92.41ms
step:1115/1645 train_time:103039ms step_avg:92.41ms
step:1116/1645 train_time:103134ms step_avg:92.41ms
step:1117/1645 train_time:103227ms step_avg:92.41ms
step:1118/1645 train_time:103321ms step_avg:92.42ms
step:1119/1645 train_time:103414ms step_avg:92.42ms
step:1120/1645 train_time:103507ms step_avg:92.42ms
step:1121/1645 train_time:103601ms step_avg:92.42ms
step:1122/1645 train_time:103695ms step_avg:92.42ms
step:1123/1645 train_time:103789ms step_avg:92.42ms
step:1124/1645 train_time:103882ms step_avg:92.42ms
step:1125/1645 train_time:103975ms step_avg:92.42ms
step:1125/1645 val_loss:3.4111 train_time:104069ms step_avg:92.51ms
step:1126/1645 train_time:104090ms step_avg:92.44ms
step:1127/1645 train_time:104171ms step_avg:92.43ms
step:1128/1645 train_time:104272ms step_avg:92.44ms
step:1129/1645 train_time:104366ms step_avg:92.44ms
step:1130/1645 train_time:104459ms step_avg:92.44ms
step:1131/1645 train_time:104551ms step_avg:92.44ms
step:1132/1645 train_time:104643ms step_avg:92.44ms
step:1133/1645 train_time:104736ms step_avg:92.44ms
step:1134/1645 train_time:104829ms step_avg:92.44ms
step:1135/1645 train_time:104920ms step_avg:92.44ms
step:1136/1645 train_time:105013ms step_avg:92.44ms
step:1137/1645 train_time:105107ms step_avg:92.44ms
step:1138/1645 train_time:105205ms step_avg:92.45ms
step:1139/1645 train_time:105301ms step_avg:92.45ms
step:1140/1645 train_time:105395ms step_avg:92.45ms
step:1141/1645 train_time:105489ms step_avg:92.45ms
step:1142/1645 train_time:105582ms step_avg:92.45ms
step:1143/1645 train_time:105675ms step_avg:92.45ms
step:1144/1645 train_time:105767ms step_avg:92.45ms
step:1145/1645 train_time:105860ms step_avg:92.45ms
step:1146/1645 train_time:105953ms step_avg:92.45ms
step:1147/1645 train_time:106046ms step_avg:92.46ms
step:1148/1645 train_time:106140ms step_avg:92.46ms
step:1149/1645 train_time:106235ms step_avg:92.46ms
step:1150/1645 train_time:106330ms step_avg:92.46ms
step:1151/1645 train_time:106425ms step_avg:92.46ms
step:1152/1645 train_time:106519ms step_avg:92.46ms
step:1153/1645 train_time:106612ms step_avg:92.46ms
step:1154/1645 train_time:106705ms step_avg:92.47ms
step:1155/1645 train_time:106798ms step_avg:92.47ms
step:1156/1645 train_time:106891ms step_avg:92.47ms
step:1157/1645 train_time:106983ms step_avg:92.47ms
step:1158/1645 train_time:107078ms step_avg:92.47ms
step:1159/1645 train_time:107172ms step_avg:92.47ms
step:1160/1645 train_time:107267ms step_avg:92.47ms
step:1161/1645 train_time:107361ms step_avg:92.47ms
step:1162/1645 train_time:107455ms step_avg:92.47ms
step:1163/1645 train_time:107550ms step_avg:92.48ms
step:1164/1645 train_time:107642ms step_avg:92.48ms
step:1165/1645 train_time:107735ms step_avg:92.48ms
step:1166/1645 train_time:107828ms step_avg:92.48ms
step:1167/1645 train_time:107922ms step_avg:92.48ms
step:1168/1645 train_time:108014ms step_avg:92.48ms
step:1169/1645 train_time:108108ms step_avg:92.48ms
step:1170/1645 train_time:108202ms step_avg:92.48ms
step:1171/1645 train_time:108296ms step_avg:92.48ms
step:1172/1645 train_time:108390ms step_avg:92.48ms
step:1173/1645 train_time:108483ms step_avg:92.48ms
step:1174/1645 train_time:108577ms step_avg:92.48ms
step:1175/1645 train_time:108671ms step_avg:92.49ms
step:1176/1645 train_time:108764ms step_avg:92.49ms
step:1177/1645 train_time:108858ms step_avg:92.49ms
step:1178/1645 train_time:108951ms step_avg:92.49ms
step:1179/1645 train_time:109045ms step_avg:92.49ms
step:1180/1645 train_time:109139ms step_avg:92.49ms
step:1181/1645 train_time:109233ms step_avg:92.49ms
step:1182/1645 train_time:109327ms step_avg:92.49ms
step:1183/1645 train_time:109422ms step_avg:92.49ms
step:1184/1645 train_time:109515ms step_avg:92.50ms
step:1185/1645 train_time:109610ms step_avg:92.50ms
step:1186/1645 train_time:109703ms step_avg:92.50ms
step:1187/1645 train_time:109796ms step_avg:92.50ms
step:1188/1645 train_time:109889ms step_avg:92.50ms
step:1189/1645 train_time:109982ms step_avg:92.50ms
step:1190/1645 train_time:110077ms step_avg:92.50ms
step:1191/1645 train_time:110171ms step_avg:92.50ms
step:1192/1645 train_time:110266ms step_avg:92.50ms
step:1193/1645 train_time:110359ms step_avg:92.51ms
step:1194/1645 train_time:110453ms step_avg:92.51ms
step:1195/1645 train_time:110547ms step_avg:92.51ms
step:1196/1645 train_time:110640ms step_avg:92.51ms
step:1197/1645 train_time:110733ms step_avg:92.51ms
step:1198/1645 train_time:110827ms step_avg:92.51ms
step:1199/1645 train_time:110920ms step_avg:92.51ms
step:1200/1645 train_time:111013ms step_avg:92.51ms
step:1201/1645 train_time:111106ms step_avg:92.51ms
step:1202/1645 train_time:111200ms step_avg:92.51ms
step:1203/1645 train_time:111294ms step_avg:92.51ms
step:1204/1645 train_time:111388ms step_avg:92.51ms
step:1205/1645 train_time:111482ms step_avg:92.52ms
step:1206/1645 train_time:111575ms step_avg:92.52ms
step:1207/1645 train_time:111670ms step_avg:92.52ms
step:1208/1645 train_time:111762ms step_avg:92.52ms
step:1209/1645 train_time:111855ms step_avg:92.52ms
step:1210/1645 train_time:111949ms step_avg:92.52ms
step:1211/1645 train_time:112042ms step_avg:92.52ms
step:1212/1645 train_time:112135ms step_avg:92.52ms
step:1213/1645 train_time:112229ms step_avg:92.52ms
step:1214/1645 train_time:112323ms step_avg:92.52ms
step:1215/1645 train_time:112417ms step_avg:92.52ms
step:1216/1645 train_time:112511ms step_avg:92.53ms
step:1217/1645 train_time:112604ms step_avg:92.53ms
step:1218/1645 train_time:112698ms step_avg:92.53ms
step:1219/1645 train_time:112792ms step_avg:92.53ms
step:1220/1645 train_time:112884ms step_avg:92.53ms
step:1221/1645 train_time:112978ms step_avg:92.53ms
step:1222/1645 train_time:113071ms step_avg:92.53ms
step:1223/1645 train_time:113164ms step_avg:92.53ms
step:1224/1645 train_time:113259ms step_avg:92.53ms
step:1225/1645 train_time:113352ms step_avg:92.53ms
step:1226/1645 train_time:113446ms step_avg:92.53ms
step:1227/1645 train_time:113540ms step_avg:92.53ms
step:1228/1645 train_time:113633ms step_avg:92.54ms
step:1229/1645 train_time:113728ms step_avg:92.54ms
step:1230/1645 train_time:113822ms step_avg:92.54ms
step:1231/1645 train_time:113915ms step_avg:92.54ms
step:1232/1645 train_time:114008ms step_avg:92.54ms
step:1233/1645 train_time:114102ms step_avg:92.54ms
step:1234/1645 train_time:114195ms step_avg:92.54ms
step:1235/1645 train_time:114289ms step_avg:92.54ms
step:1236/1645 train_time:114384ms step_avg:92.54ms
step:1237/1645 train_time:114478ms step_avg:92.54ms
step:1238/1645 train_time:114572ms step_avg:92.55ms
step:1239/1645 train_time:114665ms step_avg:92.55ms
step:1240/1645 train_time:114758ms step_avg:92.55ms
step:1241/1645 train_time:114852ms step_avg:92.55ms
step:1242/1645 train_time:114946ms step_avg:92.55ms
step:1243/1645 train_time:115039ms step_avg:92.55ms
step:1244/1645 train_time:115132ms step_avg:92.55ms
step:1245/1645 train_time:115226ms step_avg:92.55ms
step:1246/1645 train_time:115320ms step_avg:92.55ms
step:1247/1645 train_time:115413ms step_avg:92.55ms
step:1248/1645 train_time:115507ms step_avg:92.55ms
step:1249/1645 train_time:115600ms step_avg:92.55ms
step:1250/1645 train_time:115693ms step_avg:92.55ms
step:1250/1645 val_loss:3.3728 train_time:115787ms step_avg:92.63ms
step:1251/1645 train_time:115807ms step_avg:92.57ms
step:1252/1645 train_time:115886ms step_avg:92.56ms
step:1253/1645 train_time:115981ms step_avg:92.56ms
step:1254/1645 train_time:116075ms step_avg:92.56ms
step:1255/1645 train_time:116167ms step_avg:92.56ms
step:1256/1645 train_time:116262ms step_avg:92.57ms
step:1257/1645 train_time:116355ms step_avg:92.57ms
step:1258/1645 train_time:116448ms step_avg:92.57ms
step:1259/1645 train_time:116541ms step_avg:92.57ms
step:1260/1645 train_time:116634ms step_avg:92.57ms
step:1261/1645 train_time:116728ms step_avg:92.57ms
step:1262/1645 train_time:116823ms step_avg:92.57ms
step:1263/1645 train_time:116918ms step_avg:92.57ms
step:1264/1645 train_time:117012ms step_avg:92.57ms
step:1265/1645 train_time:117107ms step_avg:92.57ms
step:1266/1645 train_time:117200ms step_avg:92.57ms
step:1267/1645 train_time:117293ms step_avg:92.58ms
step:1268/1645 train_time:117386ms step_avg:92.58ms
step:1269/1645 train_time:117480ms step_avg:92.58ms
step:1270/1645 train_time:117573ms step_avg:92.58ms
step:1271/1645 train_time:117666ms step_avg:92.58ms
step:1272/1645 train_time:117760ms step_avg:92.58ms
step:1273/1645 train_time:117855ms step_avg:92.58ms
step:1274/1645 train_time:117950ms step_avg:92.58ms
step:1275/1645 train_time:118044ms step_avg:92.58ms
step:1276/1645 train_time:118139ms step_avg:92.59ms
step:1277/1645 train_time:118232ms step_avg:92.59ms
step:1278/1645 train_time:118325ms step_avg:92.59ms
step:1279/1645 train_time:118418ms step_avg:92.59ms
step:1280/1645 train_time:118511ms step_avg:92.59ms
step:1281/1645 train_time:118603ms step_avg:92.59ms
step:1282/1645 train_time:118697ms step_avg:92.59ms
step:1283/1645 train_time:118791ms step_avg:92.59ms
step:1284/1645 train_time:118886ms step_avg:92.59ms
step:1285/1645 train_time:118979ms step_avg:92.59ms
step:1286/1645 train_time:119073ms step_avg:92.59ms
step:1287/1645 train_time:119167ms step_avg:92.59ms
step:1288/1645 train_time:119260ms step_avg:92.59ms
step:1289/1645 train_time:119354ms step_avg:92.59ms
step:1290/1645 train_time:119447ms step_avg:92.59ms
step:1291/1645 train_time:119541ms step_avg:92.60ms
step:1292/1645 train_time:119633ms step_avg:92.60ms
step:1293/1645 train_time:119726ms step_avg:92.60ms
step:1294/1645 train_time:119820ms step_avg:92.60ms
step:1295/1645 train_time:119914ms step_avg:92.60ms
step:1296/1645 train_time:120007ms step_avg:92.60ms
step:1297/1645 train_time:120102ms step_avg:92.60ms
step:1298/1645 train_time:120196ms step_avg:92.60ms
step:1299/1645 train_time:120289ms step_avg:92.60ms
step:1300/1645 train_time:120382ms step_avg:92.60ms
step:1301/1645 train_time:120475ms step_avg:92.60ms
step:1302/1645 train_time:120568ms step_avg:92.60ms
step:1303/1645 train_time:120662ms step_avg:92.60ms
step:1304/1645 train_time:120756ms step_avg:92.60ms
step:1305/1645 train_time:120850ms step_avg:92.61ms
step:1306/1645 train_time:120944ms step_avg:92.61ms
step:1307/1645 train_time:121038ms step_avg:92.61ms
step:1308/1645 train_time:121133ms step_avg:92.61ms
step:1309/1645 train_time:121227ms step_avg:92.61ms
step:1310/1645 train_time:121321ms step_avg:92.61ms
step:1311/1645 train_time:121413ms step_avg:92.61ms
step:1312/1645 train_time:121506ms step_avg:92.61ms
step:1313/1645 train_time:121599ms step_avg:92.61ms
step:1314/1645 train_time:121692ms step_avg:92.61ms
step:1315/1645 train_time:121786ms step_avg:92.61ms
step:1316/1645 train_time:121880ms step_avg:92.61ms
step:1317/1645 train_time:121974ms step_avg:92.61ms
step:1318/1645 train_time:122067ms step_avg:92.62ms
step:1319/1645 train_time:122162ms step_avg:92.62ms
step:1320/1645 train_time:122256ms step_avg:92.62ms
step:1321/1645 train_time:122350ms step_avg:92.62ms
step:1322/1645 train_time:122443ms step_avg:92.62ms
step:1323/1645 train_time:122536ms step_avg:92.62ms
step:1324/1645 train_time:122630ms step_avg:92.62ms
step:1325/1645 train_time:122725ms step_avg:92.62ms
step:1326/1645 train_time:122819ms step_avg:92.62ms
step:1327/1645 train_time:122913ms step_avg:92.62ms
step:1328/1645 train_time:123006ms step_avg:92.63ms
step:1329/1645 train_time:123099ms step_avg:92.63ms
step:1330/1645 train_time:123193ms step_avg:92.63ms
step:1331/1645 train_time:123286ms step_avg:92.63ms
step:1332/1645 train_time:123380ms step_avg:92.63ms
step:1333/1645 train_time:123473ms step_avg:92.63ms
step:1334/1645 train_time:123566ms step_avg:92.63ms
step:1335/1645 train_time:123660ms step_avg:92.63ms
step:1336/1645 train_time:123754ms step_avg:92.63ms
step:1337/1645 train_time:123849ms step_avg:92.63ms
step:1338/1645 train_time:123943ms step_avg:92.63ms
step:1339/1645 train_time:124037ms step_avg:92.63ms
step:1340/1645 train_time:124131ms step_avg:92.63ms
step:1341/1645 train_time:124224ms step_avg:92.64ms
step:1342/1645 train_time:124318ms step_avg:92.64ms
step:1343/1645 train_time:124412ms step_avg:92.64ms
step:1344/1645 train_time:124505ms step_avg:92.64ms
step:1345/1645 train_time:124597ms step_avg:92.64ms
step:1346/1645 train_time:124691ms step_avg:92.64ms
step:1347/1645 train_time:124784ms step_avg:92.64ms
step:1348/1645 train_time:124878ms step_avg:92.64ms
step:1349/1645 train_time:124973ms step_avg:92.64ms
step:1350/1645 train_time:125066ms step_avg:92.64ms
step:1351/1645 train_time:125160ms step_avg:92.64ms
step:1352/1645 train_time:125253ms step_avg:92.64ms
step:1353/1645 train_time:125347ms step_avg:92.64ms
step:1354/1645 train_time:125440ms step_avg:92.64ms
step:1355/1645 train_time:125533ms step_avg:92.64ms
step:1356/1645 train_time:125627ms step_avg:92.65ms
step:1357/1645 train_time:125722ms step_avg:92.65ms
step:1358/1645 train_time:125816ms step_avg:92.65ms
step:1359/1645 train_time:125909ms step_avg:92.65ms
step:1360/1645 train_time:126002ms step_avg:92.65ms
step:1361/1645 train_time:126096ms step_avg:92.65ms
step:1362/1645 train_time:126189ms step_avg:92.65ms
step:1363/1645 train_time:126283ms step_avg:92.65ms
step:1364/1645 train_time:126376ms step_avg:92.65ms
step:1365/1645 train_time:126471ms step_avg:92.65ms
step:1366/1645 train_time:126563ms step_avg:92.65ms
step:1367/1645 train_time:126658ms step_avg:92.65ms
step:1368/1645 train_time:126753ms step_avg:92.66ms
step:1369/1645 train_time:126846ms step_avg:92.66ms
step:1370/1645 train_time:126940ms step_avg:92.66ms
step:1371/1645 train_time:127034ms step_avg:92.66ms
step:1372/1645 train_time:127127ms step_avg:92.66ms
step:1373/1645 train_time:127221ms step_avg:92.66ms
step:1374/1645 train_time:127314ms step_avg:92.66ms
step:1375/1645 train_time:127407ms step_avg:92.66ms
step:1375/1645 val_loss:3.3387 train_time:127501ms step_avg:92.73ms
step:1376/1645 train_time:127522ms step_avg:92.68ms
step:1377/1645 train_time:127598ms step_avg:92.66ms
step:1378/1645 train_time:127694ms step_avg:92.67ms
step:1379/1645 train_time:127788ms step_avg:92.67ms
step:1380/1645 train_time:127880ms step_avg:92.67ms
step:1381/1645 train_time:127973ms step_avg:92.67ms
step:1382/1645 train_time:128066ms step_avg:92.67ms
step:1383/1645 train_time:128159ms step_avg:92.67ms
step:1384/1645 train_time:128253ms step_avg:92.67ms
step:1385/1645 train_time:128347ms step_avg:92.67ms
step:1386/1645 train_time:128440ms step_avg:92.67ms
step:1387/1645 train_time:128536ms step_avg:92.67ms
step:1388/1645 train_time:128631ms step_avg:92.67ms
step:1389/1645 train_time:128727ms step_avg:92.68ms
step:1390/1645 train_time:128820ms step_avg:92.68ms
step:1391/1645 train_time:128914ms step_avg:92.68ms
step:1392/1645 train_time:129006ms step_avg:92.68ms
step:1393/1645 train_time:129099ms step_avg:92.68ms
step:1394/1645 train_time:129193ms step_avg:92.68ms
step:1395/1645 train_time:129286ms step_avg:92.68ms
step:1396/1645 train_time:129379ms step_avg:92.68ms
step:1397/1645 train_time:129474ms step_avg:92.68ms
step:1398/1645 train_time:129569ms step_avg:92.68ms
step:1399/1645 train_time:129663ms step_avg:92.68ms
step:1400/1645 train_time:129757ms step_avg:92.68ms
step:1401/1645 train_time:129850ms step_avg:92.68ms
step:1402/1645 train_time:129943ms step_avg:92.68ms
step:1403/1645 train_time:130037ms step_avg:92.68ms
step:1404/1645 train_time:130130ms step_avg:92.69ms
step:1405/1645 train_time:130224ms step_avg:92.69ms
step:1406/1645 train_time:130318ms step_avg:92.69ms
step:1407/1645 train_time:130410ms step_avg:92.69ms
step:1408/1645 train_time:130505ms step_avg:92.69ms
step:1409/1645 train_time:130599ms step_avg:92.69ms
step:1410/1645 train_time:130694ms step_avg:92.69ms
step:1411/1645 train_time:130787ms step_avg:92.69ms
step:1412/1645 train_time:130881ms step_avg:92.69ms
step:1413/1645 train_time:130974ms step_avg:92.69ms
step:1414/1645 train_time:131068ms step_avg:92.69ms
step:1415/1645 train_time:131161ms step_avg:92.69ms
step:1416/1645 train_time:131255ms step_avg:92.69ms
step:1417/1645 train_time:131348ms step_avg:92.69ms
step:1418/1645 train_time:131443ms step_avg:92.70ms
step:1419/1645 train_time:131536ms step_avg:92.70ms
step:1420/1645 train_time:131631ms step_avg:92.70ms
step:1421/1645 train_time:131724ms step_avg:92.70ms
step:1422/1645 train_time:131818ms step_avg:92.70ms
step:1423/1645 train_time:131912ms step_avg:92.70ms
step:1424/1645 train_time:132004ms step_avg:92.70ms
step:1425/1645 train_time:132097ms step_avg:92.70ms
step:1426/1645 train_time:132192ms step_avg:92.70ms
step:1427/1645 train_time:132286ms step_avg:92.70ms
step:1428/1645 train_time:132380ms step_avg:92.70ms
step:1429/1645 train_time:132474ms step_avg:92.70ms
step:1430/1645 train_time:132568ms step_avg:92.70ms
step:1431/1645 train_time:132661ms step_avg:92.71ms
step:1432/1645 train_time:132755ms step_avg:92.71ms
step:1433/1645 train_time:132849ms step_avg:92.71ms
step:1434/1645 train_time:132943ms step_avg:92.71ms
step:1435/1645 train_time:133037ms step_avg:92.71ms
step:1436/1645 train_time:133132ms step_avg:92.71ms
step:1437/1645 train_time:133225ms step_avg:92.71ms
step:1438/1645 train_time:133318ms step_avg:92.71ms
step:1439/1645 train_time:133412ms step_avg:92.71ms
step:1440/1645 train_time:133505ms step_avg:92.71ms
step:1441/1645 train_time:133599ms step_avg:92.71ms
step:1442/1645 train_time:133693ms step_avg:92.71ms
step:1443/1645 train_time:133787ms step_avg:92.71ms
step:1444/1645 train_time:133881ms step_avg:92.72ms
step:1445/1645 train_time:133975ms step_avg:92.72ms
step:1446/1645 train_time:134069ms step_avg:92.72ms
step:1447/1645 train_time:134162ms step_avg:92.72ms
step:1448/1645 train_time:134255ms step_avg:92.72ms
step:1449/1645 train_time:134348ms step_avg:92.72ms
step:1450/1645 train_time:134441ms step_avg:92.72ms
step:1451/1645 train_time:134535ms step_avg:92.72ms
step:1452/1645 train_time:134628ms step_avg:92.72ms
step:1453/1645 train_time:134722ms step_avg:92.72ms
step:1454/1645 train_time:134816ms step_avg:92.72ms
step:1455/1645 train_time:134910ms step_avg:92.72ms
step:1456/1645 train_time:135003ms step_avg:92.72ms
step:1457/1645 train_time:135096ms step_avg:92.72ms
step:1458/1645 train_time:135190ms step_avg:92.72ms
step:1459/1645 train_time:135283ms step_avg:92.72ms
step:1460/1645 train_time:135377ms step_avg:92.72ms
step:1461/1645 train_time:135471ms step_avg:92.73ms
step:1462/1645 train_time:135565ms step_avg:92.73ms
step:1463/1645 train_time:135659ms step_avg:92.73ms
step:1464/1645 train_time:135753ms step_avg:92.73ms
step:1465/1645 train_time:135846ms step_avg:92.73ms
step:1466/1645 train_time:135940ms step_avg:92.73ms
step:1467/1645 train_time:136033ms step_avg:92.73ms
step:1468/1645 train_time:136126ms step_avg:92.73ms
step:1469/1645 train_time:136220ms step_avg:92.73ms
step:1470/1645 train_time:136313ms step_avg:92.73ms
step:1471/1645 train_time:136407ms step_avg:92.73ms
step:1472/1645 train_time:136500ms step_avg:92.73ms
step:1473/1645 train_time:136594ms step_avg:92.73ms
step:1474/1645 train_time:136688ms step_avg:92.73ms
step:1475/1645 train_time:136782ms step_avg:92.73ms
step:1476/1645 train_time:136876ms step_avg:92.73ms
step:1477/1645 train_time:136970ms step_avg:92.74ms
step:1478/1645 train_time:137064ms step_avg:92.74ms
step:1479/1645 train_time:137157ms step_avg:92.74ms
step:1480/1645 train_time:137251ms step_avg:92.74ms
step:1481/1645 train_time:137345ms step_avg:92.74ms
step:1482/1645 train_time:137438ms step_avg:92.74ms
step:1483/1645 train_time:137532ms step_avg:92.74ms
step:1484/1645 train_time:137626ms step_avg:92.74ms
step:1485/1645 train_time:137720ms step_avg:92.74ms
step:1486/1645 train_time:137813ms step_avg:92.74ms
step:1487/1645 train_time:137907ms step_avg:92.74ms
step:1488/1645 train_time:138000ms step_avg:92.74ms
step:1489/1645 train_time:138095ms step_avg:92.74ms
step:1490/1645 train_time:138189ms step_avg:92.74ms
step:1491/1645 train_time:138283ms step_avg:92.75ms
step:1492/1645 train_time:138376ms step_avg:92.75ms
step:1493/1645 train_time:138469ms step_avg:92.75ms
step:1494/1645 train_time:138564ms step_avg:92.75ms
step:1495/1645 train_time:138658ms step_avg:92.75ms
step:1496/1645 train_time:138751ms step_avg:92.75ms
step:1497/1645 train_time:138845ms step_avg:92.75ms
step:1498/1645 train_time:138938ms step_avg:92.75ms
step:1499/1645 train_time:139032ms step_avg:92.75ms
step:1500/1645 train_time:139126ms step_avg:92.75ms
step:1500/1645 val_loss:3.3087 train_time:139219ms step_avg:92.81ms
step:1501/1645 train_time:139241ms step_avg:92.77ms
step:1502/1645 train_time:139316ms step_avg:92.75ms
step:1503/1645 train_time:139412ms step_avg:92.76ms
step:1504/1645 train_time:139505ms step_avg:92.76ms
step:1505/1645 train_time:139598ms step_avg:92.76ms
step:1506/1645 train_time:139690ms step_avg:92.76ms
step:1507/1645 train_time:139784ms step_avg:92.76ms
step:1508/1645 train_time:139877ms step_avg:92.76ms
step:1509/1645 train_time:139970ms step_avg:92.76ms
step:1510/1645 train_time:140063ms step_avg:92.76ms
step:1511/1645 train_time:140158ms step_avg:92.76ms
step:1512/1645 train_time:140255ms step_avg:92.76ms
step:1513/1645 train_time:140351ms step_avg:92.76ms
step:1514/1645 train_time:140446ms step_avg:92.76ms
step:1515/1645 train_time:140539ms step_avg:92.76ms
step:1516/1645 train_time:140632ms step_avg:92.76ms
step:1517/1645 train_time:140725ms step_avg:92.77ms
step:1518/1645 train_time:140819ms step_avg:92.77ms
step:1519/1645 train_time:140912ms step_avg:92.77ms
step:1520/1645 train_time:141005ms step_avg:92.77ms
step:1521/1645 train_time:141099ms step_avg:92.77ms
step:1522/1645 train_time:141193ms step_avg:92.77ms
step:1523/1645 train_time:141288ms step_avg:92.77ms
step:1524/1645 train_time:141382ms step_avg:92.77ms
step:1525/1645 train_time:141476ms step_avg:92.77ms
step:1526/1645 train_time:141569ms step_avg:92.77ms
step:1527/1645 train_time:141663ms step_avg:92.77ms
step:1528/1645 train_time:141756ms step_avg:92.77ms
step:1529/1645 train_time:141849ms step_avg:92.77ms
step:1530/1645 train_time:141942ms step_avg:92.77ms
step:1531/1645 train_time:142035ms step_avg:92.77ms
step:1532/1645 train_time:142129ms step_avg:92.77ms
step:1533/1645 train_time:142224ms step_avg:92.77ms
step:1534/1645 train_time:142318ms step_avg:92.78ms
step:1535/1645 train_time:142412ms step_avg:92.78ms
step:1536/1645 train_time:142506ms step_avg:92.78ms
step:1537/1645 train_time:142600ms step_avg:92.78ms
step:1538/1645 train_time:142693ms step_avg:92.78ms
step:1539/1645 train_time:142787ms step_avg:92.78ms
step:1540/1645 train_time:142880ms step_avg:92.78ms
step:1541/1645 train_time:142974ms step_avg:92.78ms
step:1542/1645 train_time:143066ms step_avg:92.78ms
step:1543/1645 train_time:143161ms step_avg:92.78ms
step:1544/1645 train_time:143255ms step_avg:92.78ms
step:1545/1645 train_time:143349ms step_avg:92.78ms
step:1546/1645 train_time:143442ms step_avg:92.78ms
step:1547/1645 train_time:143536ms step_avg:92.78ms
step:1548/1645 train_time:143629ms step_avg:92.78ms
step:1549/1645 train_time:143723ms step_avg:92.78ms
step:1550/1645 train_time:143817ms step_avg:92.79ms
step:1551/1645 train_time:143910ms step_avg:92.79ms
step:1552/1645 train_time:144005ms step_avg:92.79ms
step:1553/1645 train_time:144098ms step_avg:92.79ms
step:1554/1645 train_time:144191ms step_avg:92.79ms
step:1555/1645 train_time:144285ms step_avg:92.79ms
step:1556/1645 train_time:144379ms step_avg:92.79ms
step:1557/1645 train_time:144473ms step_avg:92.79ms
step:1558/1645 train_time:144566ms step_avg:92.79ms
step:1559/1645 train_time:144659ms step_avg:92.79ms
step:1560/1645 train_time:144753ms step_avg:92.79ms
step:1561/1645 train_time:144846ms step_avg:92.79ms
step:1562/1645 train_time:144940ms step_avg:92.79ms
step:1563/1645 train_time:145033ms step_avg:92.79ms
step:1564/1645 train_time:145127ms step_avg:92.79ms
step:1565/1645 train_time:145221ms step_avg:92.79ms
step:1566/1645 train_time:145315ms step_avg:92.79ms
step:1567/1645 train_time:145409ms step_avg:92.79ms
step:1568/1645 train_time:145502ms step_avg:92.79ms
step:1569/1645 train_time:145596ms step_avg:92.80ms
step:1570/1645 train_time:145689ms step_avg:92.80ms
step:1571/1645 train_time:145783ms step_avg:92.80ms
step:1572/1645 train_time:145876ms step_avg:92.80ms
step:1573/1645 train_time:145969ms step_avg:92.80ms
step:1574/1645 train_time:146062ms step_avg:92.80ms
step:1575/1645 train_time:146157ms step_avg:92.80ms
step:1576/1645 train_time:146250ms step_avg:92.80ms
step:1577/1645 train_time:146344ms step_avg:92.80ms
step:1578/1645 train_time:146438ms step_avg:92.80ms
step:1579/1645 train_time:146532ms step_avg:92.80ms
step:1580/1645 train_time:146627ms step_avg:92.80ms
step:1581/1645 train_time:146722ms step_avg:92.80ms
step:1582/1645 train_time:146817ms step_avg:92.80ms
step:1583/1645 train_time:146910ms step_avg:92.80ms
step:1584/1645 train_time:147003ms step_avg:92.81ms
step:1585/1645 train_time:147097ms step_avg:92.81ms
step:1586/1645 train_time:147191ms step_avg:92.81ms
step:1587/1645 train_time:147285ms step_avg:92.81ms
step:1588/1645 train_time:147378ms step_avg:92.81ms
step:1589/1645 train_time:147472ms step_avg:92.81ms
step:1590/1645 train_time:147565ms step_avg:92.81ms
step:1591/1645 train_time:147658ms step_avg:92.81ms
step:1592/1645 train_time:147751ms step_avg:92.81ms
step:1593/1645 train_time:147844ms step_avg:92.81ms
step:1594/1645 train_time:147938ms step_avg:92.81ms
step:1595/1645 train_time:148031ms step_avg:92.81ms
step:1596/1645 train_time:148126ms step_avg:92.81ms
step:1597/1645 train_time:148221ms step_avg:92.81ms
step:1598/1645 train_time:148315ms step_avg:92.81ms
step:1599/1645 train_time:148408ms step_avg:92.81ms
step:1600/1645 train_time:148502ms step_avg:92.81ms
step:1601/1645 train_time:148596ms step_avg:92.81ms
step:1602/1645 train_time:148689ms step_avg:92.81ms
step:1603/1645 train_time:148783ms step_avg:92.82ms
step:1604/1645 train_time:148877ms step_avg:92.82ms
step:1605/1645 train_time:148970ms step_avg:92.82ms
step:1606/1645 train_time:149063ms step_avg:92.82ms
step:1607/1645 train_time:149157ms step_avg:92.82ms
step:1608/1645 train_time:149251ms step_avg:92.82ms
step:1609/1645 train_time:149345ms step_avg:92.82ms
step:1610/1645 train_time:149439ms step_avg:92.82ms
step:1611/1645 train_time:149534ms step_avg:92.82ms
step:1612/1645 train_time:149628ms step_avg:92.82ms
step:1613/1645 train_time:149722ms step_avg:92.82ms
step:1614/1645 train_time:149816ms step_avg:92.82ms
step:1615/1645 train_time:149909ms step_avg:92.82ms
step:1616/1645 train_time:150003ms step_avg:92.82ms
step:1617/1645 train_time:150096ms step_avg:92.82ms
step:1618/1645 train_time:150190ms step_avg:92.82ms
step:1619/1645 train_time:150283ms step_avg:92.82ms
step:1620/1645 train_time:150378ms step_avg:92.83ms
step:1621/1645 train_time:150472ms step_avg:92.83ms
step:1622/1645 train_time:150565ms step_avg:92.83ms
step:1623/1645 train_time:150658ms step_avg:92.83ms
step:1624/1645 train_time:150752ms step_avg:92.83ms
step:1625/1645 train_time:150846ms step_avg:92.83ms
step:1625/1645 val_loss:3.2849 train_time:150940ms step_avg:92.89ms
step:1626/1645 train_time:150963ms step_avg:92.84ms
step:1627/1645 train_time:151038ms step_avg:92.83ms
step:1628/1645 train_time:151133ms step_avg:92.83ms
step:1629/1645 train_time:151225ms step_avg:92.83ms
step:1630/1645 train_time:151317ms step_avg:92.83ms
step:1631/1645 train_time:151410ms step_avg:92.83ms
step:1632/1645 train_time:151504ms step_avg:92.83ms
step:1633/1645 train_time:151597ms step_avg:92.83ms
step:1634/1645 train_time:151689ms step_avg:92.83ms
step:1635/1645 train_time:151781ms step_avg:92.83ms
step:1636/1645 train_time:151877ms step_avg:92.83ms
step:1637/1645 train_time:151973ms step_avg:92.84ms
step:1638/1645 train_time:152068ms step_avg:92.84ms
step:1639/1645 train_time:152162ms step_avg:92.84ms
step:1640/1645 train_time:152255ms step_avg:92.84ms
step:1641/1645 train_time:152349ms step_avg:92.84ms
step:1642/1645 train_time:152442ms step_avg:92.84ms
step:1643/1645 train_time:152536ms step_avg:92.84ms
step:1644/1645 train_time:152629ms step_avg:92.84ms
step:1645/1645 train_time:152722ms step_avg:92.84ms
step:1645/1645 val_loss:3.2792 train_time:152816ms step_avg:92.90ms
peak memory allocated: 32074 MiB reserved: 47734 MiB

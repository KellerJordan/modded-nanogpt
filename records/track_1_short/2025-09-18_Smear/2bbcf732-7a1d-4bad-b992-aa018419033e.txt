import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:36:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:128ms step_avg:128.44ms
step:2/1645 train_time:146ms step_avg:73.18ms
step:3/1645 train_time:217ms step_avg:72.17ms
step:4/1645 train_time:306ms step_avg:76.51ms
step:5/1645 train_time:397ms step_avg:79.33ms
step:6/1645 train_time:487ms step_avg:81.20ms
step:7/1645 train_time:578ms step_avg:82.61ms
step:8/1645 train_time:669ms step_avg:83.61ms
step:9/1645 train_time:760ms step_avg:84.46ms
step:10/1645 train_time:851ms step_avg:85.12ms
step:11/1645 train_time:942ms step_avg:85.65ms
step:12/1645 train_time:1035ms step_avg:86.21ms
step:13/1645 train_time:1131ms step_avg:86.98ms
step:14/1645 train_time:1226ms step_avg:87.59ms
step:15/1645 train_time:1319ms step_avg:87.90ms
step:16/1645 train_time:1410ms step_avg:88.14ms
step:17/1645 train_time:1502ms step_avg:88.34ms
step:18/1645 train_time:1594ms step_avg:88.53ms
step:19/1645 train_time:1684ms step_avg:88.65ms
step:20/1645 train_time:1775ms step_avg:88.76ms
step:21/1645 train_time:1867ms step_avg:88.90ms
step:22/1645 train_time:1959ms step_avg:89.04ms
step:23/1645 train_time:2051ms step_avg:89.18ms
step:24/1645 train_time:2145ms step_avg:89.38ms
step:25/1645 train_time:2238ms step_avg:89.53ms
step:26/1645 train_time:2331ms step_avg:89.64ms
step:27/1645 train_time:2423ms step_avg:89.74ms
step:28/1645 train_time:2515ms step_avg:89.81ms
step:29/1645 train_time:2606ms step_avg:89.86ms
step:30/1645 train_time:2697ms step_avg:89.88ms
step:31/1645 train_time:2787ms step_avg:89.90ms
step:32/1645 train_time:2878ms step_avg:89.94ms
step:33/1645 train_time:2969ms step_avg:89.98ms
step:34/1645 train_time:3062ms step_avg:90.07ms
step:35/1645 train_time:3154ms step_avg:90.12ms
step:36/1645 train_time:3247ms step_avg:90.20ms
step:37/1645 train_time:3340ms step_avg:90.26ms
step:38/1645 train_time:3432ms step_avg:90.32ms
step:39/1645 train_time:3525ms step_avg:90.37ms
step:40/1645 train_time:3617ms step_avg:90.41ms
step:41/1645 train_time:3707ms step_avg:90.43ms
step:42/1645 train_time:3799ms step_avg:90.44ms
step:43/1645 train_time:3890ms step_avg:90.47ms
step:44/1645 train_time:3982ms step_avg:90.49ms
step:45/1645 train_time:4074ms step_avg:90.53ms
step:46/1645 train_time:4167ms step_avg:90.58ms
step:47/1645 train_time:4260ms step_avg:90.63ms
step:48/1645 train_time:4352ms step_avg:90.67ms
step:49/1645 train_time:4445ms step_avg:90.71ms
step:50/1645 train_time:4536ms step_avg:90.72ms
step:51/1645 train_time:4628ms step_avg:90.75ms
step:52/1645 train_time:4720ms step_avg:90.76ms
step:53/1645 train_time:4811ms step_avg:90.77ms
step:54/1645 train_time:4902ms step_avg:90.78ms
step:55/1645 train_time:4993ms step_avg:90.79ms
step:56/1645 train_time:5085ms step_avg:90.81ms
step:57/1645 train_time:5177ms step_avg:90.82ms
step:58/1645 train_time:5270ms step_avg:90.86ms
step:59/1645 train_time:5363ms step_avg:90.90ms
step:60/1645 train_time:5455ms step_avg:90.91ms
step:61/1645 train_time:5546ms step_avg:90.92ms
step:62/1645 train_time:5638ms step_avg:90.94ms
step:63/1645 train_time:5730ms step_avg:90.95ms
step:64/1645 train_time:5821ms step_avg:90.96ms
step:65/1645 train_time:5913ms step_avg:90.96ms
step:66/1645 train_time:6004ms step_avg:90.97ms
step:67/1645 train_time:6095ms step_avg:90.97ms
step:68/1645 train_time:6188ms step_avg:91.00ms
step:69/1645 train_time:6280ms step_avg:91.02ms
step:70/1645 train_time:6372ms step_avg:91.03ms
step:71/1645 train_time:6464ms step_avg:91.04ms
step:72/1645 train_time:6556ms step_avg:91.05ms
step:73/1645 train_time:6648ms step_avg:91.06ms
step:74/1645 train_time:6740ms step_avg:91.08ms
step:75/1645 train_time:6832ms step_avg:91.09ms
step:76/1645 train_time:6924ms step_avg:91.10ms
step:77/1645 train_time:7015ms step_avg:91.10ms
step:78/1645 train_time:7106ms step_avg:91.11ms
step:79/1645 train_time:7199ms step_avg:91.13ms
step:80/1645 train_time:7291ms step_avg:91.14ms
step:81/1645 train_time:7383ms step_avg:91.14ms
step:82/1645 train_time:7474ms step_avg:91.15ms
step:83/1645 train_time:7566ms step_avg:91.16ms
step:84/1645 train_time:7659ms step_avg:91.18ms
step:85/1645 train_time:7751ms step_avg:91.19ms
step:86/1645 train_time:7843ms step_avg:91.20ms
step:87/1645 train_time:7936ms step_avg:91.22ms
step:88/1645 train_time:8027ms step_avg:91.22ms
step:89/1645 train_time:8118ms step_avg:91.22ms
step:90/1645 train_time:8210ms step_avg:91.22ms
step:91/1645 train_time:8301ms step_avg:91.22ms
step:92/1645 train_time:8393ms step_avg:91.23ms
step:93/1645 train_time:8484ms step_avg:91.23ms
step:94/1645 train_time:8576ms step_avg:91.23ms
step:95/1645 train_time:8669ms step_avg:91.25ms
step:96/1645 train_time:8761ms step_avg:91.27ms
step:97/1645 train_time:8853ms step_avg:91.26ms
step:98/1645 train_time:8945ms step_avg:91.28ms
step:99/1645 train_time:9037ms step_avg:91.28ms
step:100/1645 train_time:9128ms step_avg:91.28ms
step:101/1645 train_time:9220ms step_avg:91.29ms
step:102/1645 train_time:9312ms step_avg:91.29ms
step:103/1645 train_time:9402ms step_avg:91.28ms
step:104/1645 train_time:9494ms step_avg:91.29ms
step:105/1645 train_time:9587ms step_avg:91.30ms
step:106/1645 train_time:9679ms step_avg:91.31ms
step:107/1645 train_time:9771ms step_avg:91.31ms
step:108/1645 train_time:9863ms step_avg:91.33ms
step:109/1645 train_time:9955ms step_avg:91.33ms
step:110/1645 train_time:10047ms step_avg:91.34ms
step:111/1645 train_time:10140ms step_avg:91.35ms
step:112/1645 train_time:10231ms step_avg:91.35ms
step:113/1645 train_time:10323ms step_avg:91.35ms
step:114/1645 train_time:10414ms step_avg:91.35ms
step:115/1645 train_time:10506ms step_avg:91.35ms
step:116/1645 train_time:10598ms step_avg:91.36ms
step:117/1645 train_time:10689ms step_avg:91.36ms
step:118/1645 train_time:10781ms step_avg:91.36ms
step:119/1645 train_time:10872ms step_avg:91.36ms
step:120/1645 train_time:10964ms step_avg:91.37ms
step:121/1645 train_time:11056ms step_avg:91.37ms
step:122/1645 train_time:11149ms step_avg:91.38ms
step:123/1645 train_time:11241ms step_avg:91.39ms
step:124/1645 train_time:11332ms step_avg:91.39ms
step:125/1645 train_time:11423ms step_avg:91.38ms
step:125/1645 val_loss:4.3027 train_time:11514ms step_avg:92.11ms
step:126/1645 train_time:11531ms step_avg:91.51ms
step:127/1645 train_time:11611ms step_avg:91.43ms
step:128/1645 train_time:11713ms step_avg:91.51ms
step:129/1645 train_time:11809ms step_avg:91.54ms
step:130/1645 train_time:11900ms step_avg:91.54ms
step:131/1645 train_time:11991ms step_avg:91.53ms
step:132/1645 train_time:12082ms step_avg:91.53ms
step:133/1645 train_time:12172ms step_avg:91.52ms
step:134/1645 train_time:12263ms step_avg:91.52ms
step:135/1645 train_time:12354ms step_avg:91.51ms
step:136/1645 train_time:12445ms step_avg:91.50ms
step:137/1645 train_time:12537ms step_avg:91.51ms
step:138/1645 train_time:12630ms step_avg:91.52ms
step:139/1645 train_time:12724ms step_avg:91.54ms
step:140/1645 train_time:12817ms step_avg:91.55ms
step:141/1645 train_time:12909ms step_avg:91.56ms
step:142/1645 train_time:13001ms step_avg:91.56ms
step:143/1645 train_time:13092ms step_avg:91.55ms
step:144/1645 train_time:13183ms step_avg:91.55ms
step:145/1645 train_time:13275ms step_avg:91.55ms
step:146/1645 train_time:13366ms step_avg:91.55ms
step:147/1645 train_time:13457ms step_avg:91.54ms
step:148/1645 train_time:13548ms step_avg:91.54ms
step:149/1645 train_time:13641ms step_avg:91.55ms
step:150/1645 train_time:13734ms step_avg:91.56ms
step:151/1645 train_time:13826ms step_avg:91.56ms
step:152/1645 train_time:13918ms step_avg:91.56ms
step:153/1645 train_time:14010ms step_avg:91.57ms
step:154/1645 train_time:14102ms step_avg:91.57ms
step:155/1645 train_time:14194ms step_avg:91.57ms
step:156/1645 train_time:14285ms step_avg:91.57ms
step:157/1645 train_time:14376ms step_avg:91.57ms
step:158/1645 train_time:14468ms step_avg:91.57ms
step:159/1645 train_time:14559ms step_avg:91.57ms
step:160/1645 train_time:14652ms step_avg:91.57ms
step:161/1645 train_time:14745ms step_avg:91.58ms
step:162/1645 train_time:14838ms step_avg:91.59ms
step:163/1645 train_time:14929ms step_avg:91.59ms
step:164/1645 train_time:15021ms step_avg:91.59ms
step:165/1645 train_time:15112ms step_avg:91.59ms
step:166/1645 train_time:15206ms step_avg:91.60ms
step:167/1645 train_time:15296ms step_avg:91.60ms
step:168/1645 train_time:15387ms step_avg:91.59ms
step:169/1645 train_time:15479ms step_avg:91.59ms
step:170/1645 train_time:15570ms step_avg:91.59ms
step:171/1645 train_time:15663ms step_avg:91.59ms
step:172/1645 train_time:15755ms step_avg:91.60ms
step:173/1645 train_time:15847ms step_avg:91.60ms
step:174/1645 train_time:15939ms step_avg:91.60ms
step:175/1645 train_time:16030ms step_avg:91.60ms
step:176/1645 train_time:16122ms step_avg:91.60ms
step:177/1645 train_time:16213ms step_avg:91.60ms
step:178/1645 train_time:16305ms step_avg:91.60ms
step:179/1645 train_time:16397ms step_avg:91.60ms
step:180/1645 train_time:16489ms step_avg:91.60ms
step:181/1645 train_time:16581ms step_avg:91.61ms
step:182/1645 train_time:16673ms step_avg:91.61ms
step:183/1645 train_time:16765ms step_avg:91.61ms
step:184/1645 train_time:16857ms step_avg:91.61ms
step:185/1645 train_time:16948ms step_avg:91.61ms
step:186/1645 train_time:17039ms step_avg:91.61ms
step:187/1645 train_time:17130ms step_avg:91.61ms
step:188/1645 train_time:17223ms step_avg:91.61ms
step:189/1645 train_time:17314ms step_avg:91.61ms
step:190/1645 train_time:17405ms step_avg:91.61ms
step:191/1645 train_time:17498ms step_avg:91.61ms
step:192/1645 train_time:17590ms step_avg:91.61ms
step:193/1645 train_time:17682ms step_avg:91.62ms
step:194/1645 train_time:17774ms step_avg:91.62ms
step:195/1645 train_time:17866ms step_avg:91.62ms
step:196/1645 train_time:17958ms step_avg:91.62ms
step:197/1645 train_time:18049ms step_avg:91.62ms
step:198/1645 train_time:18140ms step_avg:91.62ms
step:199/1645 train_time:18231ms step_avg:91.61ms
step:200/1645 train_time:18322ms step_avg:91.61ms
step:201/1645 train_time:18413ms step_avg:91.61ms
step:202/1645 train_time:18505ms step_avg:91.61ms
step:203/1645 train_time:18597ms step_avg:91.61ms
step:204/1645 train_time:18689ms step_avg:91.61ms
step:205/1645 train_time:18782ms step_avg:91.62ms
step:206/1645 train_time:18873ms step_avg:91.62ms
step:207/1645 train_time:18964ms step_avg:91.61ms
step:208/1645 train_time:19056ms step_avg:91.62ms
step:209/1645 train_time:19148ms step_avg:91.62ms
step:210/1645 train_time:19239ms step_avg:91.61ms
step:211/1645 train_time:19330ms step_avg:91.61ms
step:212/1645 train_time:19422ms step_avg:91.61ms
step:213/1645 train_time:19513ms step_avg:91.61ms
step:214/1645 train_time:19606ms step_avg:91.62ms
step:215/1645 train_time:19698ms step_avg:91.62ms
step:216/1645 train_time:19790ms step_avg:91.62ms
step:217/1645 train_time:19882ms step_avg:91.62ms
step:218/1645 train_time:19974ms step_avg:91.62ms
step:219/1645 train_time:20066ms step_avg:91.62ms
step:220/1645 train_time:20157ms step_avg:91.62ms
step:221/1645 train_time:20248ms step_avg:91.62ms
step:222/1645 train_time:20339ms step_avg:91.62ms
step:223/1645 train_time:20430ms step_avg:91.61ms
step:224/1645 train_time:20521ms step_avg:91.61ms
step:225/1645 train_time:20612ms step_avg:91.61ms
step:226/1645 train_time:20705ms step_avg:91.61ms
step:227/1645 train_time:20797ms step_avg:91.61ms
step:228/1645 train_time:20888ms step_avg:91.61ms
step:229/1645 train_time:20980ms step_avg:91.61ms
step:230/1645 train_time:21072ms step_avg:91.62ms
step:231/1645 train_time:21164ms step_avg:91.62ms
step:232/1645 train_time:21255ms step_avg:91.62ms
step:233/1645 train_time:21347ms step_avg:91.62ms
step:234/1645 train_time:21437ms step_avg:91.61ms
step:235/1645 train_time:21529ms step_avg:91.61ms
step:236/1645 train_time:21621ms step_avg:91.61ms
step:237/1645 train_time:21712ms step_avg:91.61ms
step:238/1645 train_time:21804ms step_avg:91.62ms
step:239/1645 train_time:21897ms step_avg:91.62ms
step:240/1645 train_time:21989ms step_avg:91.62ms
step:241/1645 train_time:22081ms step_avg:91.62ms
step:242/1645 train_time:22172ms step_avg:91.62ms
step:243/1645 train_time:22264ms step_avg:91.62ms
step:244/1645 train_time:22356ms step_avg:91.62ms
step:245/1645 train_time:22447ms step_avg:91.62ms
step:246/1645 train_time:22538ms step_avg:91.62ms
step:247/1645 train_time:22629ms step_avg:91.62ms
step:248/1645 train_time:22721ms step_avg:91.62ms
step:249/1645 train_time:22812ms step_avg:91.62ms
step:250/1645 train_time:22906ms step_avg:91.62ms
step:250/1645 val_loss:3.9618 train_time:22998ms step_avg:91.99ms
step:251/1645 train_time:23014ms step_avg:91.69ms
step:252/1645 train_time:23095ms step_avg:91.65ms
step:253/1645 train_time:23188ms step_avg:91.65ms
step:254/1645 train_time:23280ms step_avg:91.66ms
step:255/1645 train_time:23371ms step_avg:91.65ms
step:256/1645 train_time:23462ms step_avg:91.65ms
step:257/1645 train_time:23552ms step_avg:91.64ms
step:258/1645 train_time:23644ms step_avg:91.64ms
step:259/1645 train_time:23736ms step_avg:91.64ms
step:260/1645 train_time:23826ms step_avg:91.64ms
step:261/1645 train_time:23918ms step_avg:91.64ms
step:262/1645 train_time:24011ms step_avg:91.64ms
step:263/1645 train_time:24105ms step_avg:91.65ms
step:264/1645 train_time:24198ms step_avg:91.66ms
step:265/1645 train_time:24290ms step_avg:91.66ms
step:266/1645 train_time:24381ms step_avg:91.66ms
step:267/1645 train_time:24472ms step_avg:91.65ms
step:268/1645 train_time:24564ms step_avg:91.66ms
step:269/1645 train_time:24655ms step_avg:91.66ms
step:270/1645 train_time:24746ms step_avg:91.65ms
step:271/1645 train_time:24837ms step_avg:91.65ms
step:272/1645 train_time:24928ms step_avg:91.65ms
step:273/1645 train_time:25021ms step_avg:91.65ms
step:274/1645 train_time:25113ms step_avg:91.65ms
step:275/1645 train_time:25205ms step_avg:91.66ms
step:276/1645 train_time:25297ms step_avg:91.66ms
step:277/1645 train_time:25389ms step_avg:91.66ms
step:278/1645 train_time:25481ms step_avg:91.66ms
step:279/1645 train_time:25572ms step_avg:91.66ms
step:280/1645 train_time:25664ms step_avg:91.66ms
step:281/1645 train_time:25756ms step_avg:91.66ms
step:282/1645 train_time:25847ms step_avg:91.66ms
step:283/1645 train_time:25938ms step_avg:91.65ms
step:284/1645 train_time:26030ms step_avg:91.65ms
step:285/1645 train_time:26122ms step_avg:91.65ms
step:286/1645 train_time:26213ms step_avg:91.66ms
step:287/1645 train_time:26305ms step_avg:91.65ms
step:288/1645 train_time:26396ms step_avg:91.65ms
step:289/1645 train_time:26488ms step_avg:91.65ms
step:290/1645 train_time:26579ms step_avg:91.65ms
step:291/1645 train_time:26670ms step_avg:91.65ms
step:292/1645 train_time:26763ms step_avg:91.65ms
step:293/1645 train_time:26855ms step_avg:91.65ms
step:294/1645 train_time:26946ms step_avg:91.65ms
step:295/1645 train_time:27039ms step_avg:91.66ms
step:296/1645 train_time:27130ms step_avg:91.66ms
step:297/1645 train_time:27223ms step_avg:91.66ms
step:298/1645 train_time:27313ms step_avg:91.66ms
step:299/1645 train_time:27405ms step_avg:91.66ms
step:300/1645 train_time:27497ms step_avg:91.66ms
step:301/1645 train_time:27589ms step_avg:91.66ms
step:302/1645 train_time:27680ms step_avg:91.66ms
step:303/1645 train_time:27771ms step_avg:91.65ms
step:304/1645 train_time:27863ms step_avg:91.66ms
step:305/1645 train_time:27956ms step_avg:91.66ms
step:306/1645 train_time:28047ms step_avg:91.66ms
step:307/1645 train_time:28139ms step_avg:91.66ms
step:308/1645 train_time:28230ms step_avg:91.65ms
step:309/1645 train_time:28323ms step_avg:91.66ms
step:310/1645 train_time:28414ms step_avg:91.66ms
step:311/1645 train_time:28505ms step_avg:91.66ms
step:312/1645 train_time:28597ms step_avg:91.66ms
step:313/1645 train_time:28689ms step_avg:91.66ms
step:314/1645 train_time:28780ms step_avg:91.66ms
step:315/1645 train_time:28872ms step_avg:91.66ms
step:316/1645 train_time:28965ms step_avg:91.66ms
step:317/1645 train_time:29057ms step_avg:91.66ms
step:318/1645 train_time:29148ms step_avg:91.66ms
step:319/1645 train_time:29240ms step_avg:91.66ms
step:320/1645 train_time:29333ms step_avg:91.66ms
step:321/1645 train_time:29425ms step_avg:91.67ms
step:322/1645 train_time:29516ms step_avg:91.66ms
step:323/1645 train_time:29607ms step_avg:91.66ms
step:324/1645 train_time:29698ms step_avg:91.66ms
step:325/1645 train_time:29789ms step_avg:91.66ms
step:326/1645 train_time:29881ms step_avg:91.66ms
step:327/1645 train_time:29972ms step_avg:91.66ms
step:328/1645 train_time:30064ms step_avg:91.66ms
step:329/1645 train_time:30157ms step_avg:91.66ms
step:330/1645 train_time:30250ms step_avg:91.67ms
step:331/1645 train_time:30341ms step_avg:91.67ms
step:332/1645 train_time:30432ms step_avg:91.66ms
step:333/1645 train_time:30525ms step_avg:91.67ms
step:334/1645 train_time:30616ms step_avg:91.66ms
step:335/1645 train_time:30707ms step_avg:91.66ms
step:336/1645 train_time:30798ms step_avg:91.66ms
step:337/1645 train_time:30890ms step_avg:91.66ms
step:338/1645 train_time:30982ms step_avg:91.66ms
step:339/1645 train_time:31074ms step_avg:91.66ms
step:340/1645 train_time:31167ms step_avg:91.67ms
step:341/1645 train_time:31259ms step_avg:91.67ms
step:342/1645 train_time:31350ms step_avg:91.67ms
step:343/1645 train_time:31443ms step_avg:91.67ms
step:344/1645 train_time:31536ms step_avg:91.67ms
step:345/1645 train_time:31626ms step_avg:91.67ms
step:346/1645 train_time:31718ms step_avg:91.67ms
step:347/1645 train_time:31810ms step_avg:91.67ms
step:348/1645 train_time:31901ms step_avg:91.67ms
step:349/1645 train_time:31992ms step_avg:91.67ms
step:350/1645 train_time:32084ms step_avg:91.67ms
step:351/1645 train_time:32175ms step_avg:91.67ms
step:352/1645 train_time:32266ms step_avg:91.67ms
step:353/1645 train_time:32359ms step_avg:91.67ms
step:354/1645 train_time:32451ms step_avg:91.67ms
step:355/1645 train_time:32545ms step_avg:91.67ms
step:356/1645 train_time:32636ms step_avg:91.67ms
step:357/1645 train_time:32728ms step_avg:91.67ms
step:358/1645 train_time:32819ms step_avg:91.67ms
step:359/1645 train_time:32911ms step_avg:91.67ms
step:360/1645 train_time:33001ms step_avg:91.67ms
step:361/1645 train_time:33092ms step_avg:91.67ms
step:362/1645 train_time:33184ms step_avg:91.67ms
step:363/1645 train_time:33275ms step_avg:91.67ms
step:364/1645 train_time:33367ms step_avg:91.67ms
step:365/1645 train_time:33460ms step_avg:91.67ms
step:366/1645 train_time:33552ms step_avg:91.67ms
step:367/1645 train_time:33645ms step_avg:91.68ms
step:368/1645 train_time:33736ms step_avg:91.67ms
step:369/1645 train_time:33828ms step_avg:91.67ms
step:370/1645 train_time:33918ms step_avg:91.67ms
step:371/1645 train_time:34010ms step_avg:91.67ms
step:372/1645 train_time:34101ms step_avg:91.67ms
step:373/1645 train_time:34193ms step_avg:91.67ms
step:374/1645 train_time:34285ms step_avg:91.67ms
step:375/1645 train_time:34377ms step_avg:91.67ms
step:375/1645 val_loss:3.8105 train_time:34469ms step_avg:91.92ms
step:376/1645 train_time:34485ms step_avg:91.72ms
step:377/1645 train_time:34564ms step_avg:91.68ms
step:378/1645 train_time:34658ms step_avg:91.69ms
step:379/1645 train_time:34751ms step_avg:91.69ms
step:380/1645 train_time:34842ms step_avg:91.69ms
step:381/1645 train_time:34932ms step_avg:91.69ms
step:382/1645 train_time:35023ms step_avg:91.68ms
step:383/1645 train_time:35114ms step_avg:91.68ms
step:384/1645 train_time:35205ms step_avg:91.68ms
step:385/1645 train_time:35296ms step_avg:91.68ms
step:386/1645 train_time:35388ms step_avg:91.68ms
step:387/1645 train_time:35481ms step_avg:91.68ms
step:388/1645 train_time:35575ms step_avg:91.69ms
step:389/1645 train_time:35667ms step_avg:91.69ms
step:390/1645 train_time:35759ms step_avg:91.69ms
step:391/1645 train_time:35850ms step_avg:91.69ms
step:392/1645 train_time:35941ms step_avg:91.69ms
step:393/1645 train_time:36032ms step_avg:91.68ms
step:394/1645 train_time:36123ms step_avg:91.68ms
step:395/1645 train_time:36215ms step_avg:91.68ms
step:396/1645 train_time:36305ms step_avg:91.68ms
step:397/1645 train_time:36397ms step_avg:91.68ms
step:398/1645 train_time:36490ms step_avg:91.68ms
step:399/1645 train_time:36583ms step_avg:91.69ms
step:400/1645 train_time:36675ms step_avg:91.69ms
step:401/1645 train_time:36767ms step_avg:91.69ms
step:402/1645 train_time:36859ms step_avg:91.69ms
step:403/1645 train_time:36951ms step_avg:91.69ms
step:404/1645 train_time:37042ms step_avg:91.69ms
step:405/1645 train_time:37133ms step_avg:91.69ms
step:406/1645 train_time:37225ms step_avg:91.69ms
step:407/1645 train_time:37316ms step_avg:91.69ms
step:408/1645 train_time:37408ms step_avg:91.69ms
step:409/1645 train_time:37501ms step_avg:91.69ms
step:410/1645 train_time:37593ms step_avg:91.69ms
step:411/1645 train_time:37685ms step_avg:91.69ms
step:412/1645 train_time:37777ms step_avg:91.69ms
step:413/1645 train_time:37868ms step_avg:91.69ms
step:414/1645 train_time:37959ms step_avg:91.69ms
step:415/1645 train_time:38050ms step_avg:91.69ms
step:416/1645 train_time:38141ms step_avg:91.69ms
step:417/1645 train_time:38232ms step_avg:91.68ms
step:418/1645 train_time:38325ms step_avg:91.69ms
step:419/1645 train_time:38416ms step_avg:91.68ms
step:420/1645 train_time:38507ms step_avg:91.68ms
step:421/1645 train_time:38600ms step_avg:91.69ms
step:422/1645 train_time:38693ms step_avg:91.69ms
step:423/1645 train_time:38786ms step_avg:91.69ms
step:424/1645 train_time:38877ms step_avg:91.69ms
step:425/1645 train_time:38969ms step_avg:91.69ms
step:426/1645 train_time:39061ms step_avg:91.69ms
step:427/1645 train_time:39152ms step_avg:91.69ms
step:428/1645 train_time:39243ms step_avg:91.69ms
step:429/1645 train_time:39335ms step_avg:91.69ms
step:430/1645 train_time:39426ms step_avg:91.69ms
step:431/1645 train_time:39518ms step_avg:91.69ms
step:432/1645 train_time:39610ms step_avg:91.69ms
step:433/1645 train_time:39702ms step_avg:91.69ms
step:434/1645 train_time:39794ms step_avg:91.69ms
step:435/1645 train_time:39886ms step_avg:91.69ms
step:436/1645 train_time:39978ms step_avg:91.69ms
step:437/1645 train_time:40069ms step_avg:91.69ms
step:438/1645 train_time:40159ms step_avg:91.69ms
step:439/1645 train_time:40251ms step_avg:91.69ms
step:440/1645 train_time:40342ms step_avg:91.69ms
step:441/1645 train_time:40433ms step_avg:91.69ms
step:442/1645 train_time:40525ms step_avg:91.69ms
step:443/1645 train_time:40617ms step_avg:91.69ms
step:444/1645 train_time:40709ms step_avg:91.69ms
step:445/1645 train_time:40801ms step_avg:91.69ms
step:446/1645 train_time:40893ms step_avg:91.69ms
step:447/1645 train_time:40985ms step_avg:91.69ms
step:448/1645 train_time:41077ms step_avg:91.69ms
step:449/1645 train_time:41168ms step_avg:91.69ms
step:450/1645 train_time:41259ms step_avg:91.69ms
step:451/1645 train_time:41350ms step_avg:91.69ms
step:452/1645 train_time:41442ms step_avg:91.68ms
step:453/1645 train_time:41533ms step_avg:91.69ms
step:454/1645 train_time:41625ms step_avg:91.68ms
step:455/1645 train_time:41718ms step_avg:91.69ms
step:456/1645 train_time:41810ms step_avg:91.69ms
step:457/1645 train_time:41902ms step_avg:91.69ms
step:458/1645 train_time:41994ms step_avg:91.69ms
step:459/1645 train_time:42085ms step_avg:91.69ms
step:460/1645 train_time:42177ms step_avg:91.69ms
step:461/1645 train_time:42269ms step_avg:91.69ms
step:462/1645 train_time:42360ms step_avg:91.69ms
step:463/1645 train_time:42452ms step_avg:91.69ms
step:464/1645 train_time:42543ms step_avg:91.69ms
step:465/1645 train_time:42635ms step_avg:91.69ms
step:466/1645 train_time:42727ms step_avg:91.69ms
step:467/1645 train_time:42820ms step_avg:91.69ms
step:468/1645 train_time:42913ms step_avg:91.69ms
step:469/1645 train_time:43004ms step_avg:91.69ms
step:470/1645 train_time:43095ms step_avg:91.69ms
step:471/1645 train_time:43186ms step_avg:91.69ms
step:472/1645 train_time:43277ms step_avg:91.69ms
step:473/1645 train_time:43368ms step_avg:91.69ms
step:474/1645 train_time:43460ms step_avg:91.69ms
step:475/1645 train_time:43552ms step_avg:91.69ms
step:476/1645 train_time:43643ms step_avg:91.69ms
step:477/1645 train_time:43736ms step_avg:91.69ms
step:478/1645 train_time:43828ms step_avg:91.69ms
step:479/1645 train_time:43920ms step_avg:91.69ms
step:480/1645 train_time:44013ms step_avg:91.69ms
step:481/1645 train_time:44104ms step_avg:91.69ms
step:482/1645 train_time:44195ms step_avg:91.69ms
step:483/1645 train_time:44287ms step_avg:91.69ms
step:484/1645 train_time:44379ms step_avg:91.69ms
step:485/1645 train_time:44471ms step_avg:91.69ms
step:486/1645 train_time:44562ms step_avg:91.69ms
step:487/1645 train_time:44654ms step_avg:91.69ms
step:488/1645 train_time:44746ms step_avg:91.69ms
step:489/1645 train_time:44838ms step_avg:91.69ms
step:490/1645 train_time:44929ms step_avg:91.69ms
step:491/1645 train_time:45021ms step_avg:91.69ms
step:492/1645 train_time:45113ms step_avg:91.69ms
step:493/1645 train_time:45205ms step_avg:91.69ms
step:494/1645 train_time:45297ms step_avg:91.69ms
step:495/1645 train_time:45389ms step_avg:91.69ms
step:496/1645 train_time:45481ms step_avg:91.70ms
step:497/1645 train_time:45573ms step_avg:91.70ms
step:498/1645 train_time:45664ms step_avg:91.70ms
step:499/1645 train_time:45756ms step_avg:91.70ms
step:500/1645 train_time:45846ms step_avg:91.69ms
step:500/1645 val_loss:3.7085 train_time:45938ms step_avg:91.88ms
step:501/1645 train_time:45954ms step_avg:91.72ms
step:502/1645 train_time:46037ms step_avg:91.71ms
step:503/1645 train_time:46130ms step_avg:91.71ms
step:504/1645 train_time:46221ms step_avg:91.71ms
step:505/1645 train_time:46311ms step_avg:91.71ms
step:506/1645 train_time:46402ms step_avg:91.70ms
step:507/1645 train_time:46492ms step_avg:91.70ms
step:508/1645 train_time:46583ms step_avg:91.70ms
step:509/1645 train_time:46677ms step_avg:91.70ms
step:510/1645 train_time:46767ms step_avg:91.70ms
step:511/1645 train_time:46859ms step_avg:91.70ms
step:512/1645 train_time:46952ms step_avg:91.70ms
step:513/1645 train_time:47047ms step_avg:91.71ms
step:514/1645 train_time:47140ms step_avg:91.71ms
step:515/1645 train_time:47232ms step_avg:91.71ms
step:516/1645 train_time:47323ms step_avg:91.71ms
step:517/1645 train_time:47414ms step_avg:91.71ms
step:518/1645 train_time:47504ms step_avg:91.71ms
step:519/1645 train_time:47596ms step_avg:91.71ms
step:520/1645 train_time:47687ms step_avg:91.71ms
step:521/1645 train_time:47779ms step_avg:91.71ms
step:522/1645 train_time:47870ms step_avg:91.71ms
step:523/1645 train_time:47963ms step_avg:91.71ms
step:524/1645 train_time:48057ms step_avg:91.71ms
step:525/1645 train_time:48150ms step_avg:91.71ms
step:526/1645 train_time:48242ms step_avg:91.72ms
step:527/1645 train_time:48334ms step_avg:91.72ms
step:528/1645 train_time:48425ms step_avg:91.71ms
step:529/1645 train_time:48516ms step_avg:91.71ms
step:530/1645 train_time:48607ms step_avg:91.71ms
step:531/1645 train_time:48697ms step_avg:91.71ms
step:532/1645 train_time:48788ms step_avg:91.71ms
step:533/1645 train_time:48879ms step_avg:91.71ms
step:534/1645 train_time:48972ms step_avg:91.71ms
step:535/1645 train_time:49064ms step_avg:91.71ms
step:536/1645 train_time:49156ms step_avg:91.71ms
step:537/1645 train_time:49249ms step_avg:91.71ms
step:538/1645 train_time:49342ms step_avg:91.71ms
step:539/1645 train_time:49433ms step_avg:91.71ms
step:540/1645 train_time:49524ms step_avg:91.71ms
step:541/1645 train_time:49615ms step_avg:91.71ms
step:542/1645 train_time:49706ms step_avg:91.71ms
step:543/1645 train_time:49797ms step_avg:91.71ms
step:544/1645 train_time:49888ms step_avg:91.71ms
step:545/1645 train_time:49980ms step_avg:91.71ms
step:546/1645 train_time:50072ms step_avg:91.71ms
step:547/1645 train_time:50164ms step_avg:91.71ms
step:548/1645 train_time:50257ms step_avg:91.71ms
step:549/1645 train_time:50350ms step_avg:91.71ms
step:550/1645 train_time:50442ms step_avg:91.71ms
step:551/1645 train_time:50536ms step_avg:91.72ms
step:552/1645 train_time:50628ms step_avg:91.72ms
step:553/1645 train_time:50720ms step_avg:91.72ms
step:554/1645 train_time:50813ms step_avg:91.72ms
step:555/1645 train_time:50906ms step_avg:91.72ms
step:556/1645 train_time:50999ms step_avg:91.73ms
step:557/1645 train_time:51092ms step_avg:91.73ms
step:558/1645 train_time:51184ms step_avg:91.73ms
step:559/1645 train_time:51279ms step_avg:91.73ms
step:560/1645 train_time:51373ms step_avg:91.74ms
step:561/1645 train_time:51465ms step_avg:91.74ms
step:562/1645 train_time:51558ms step_avg:91.74ms
step:563/1645 train_time:51651ms step_avg:91.74ms
step:564/1645 train_time:51743ms step_avg:91.74ms
step:565/1645 train_time:51836ms step_avg:91.75ms
step:566/1645 train_time:51929ms step_avg:91.75ms
step:567/1645 train_time:52022ms step_avg:91.75ms
step:568/1645 train_time:52115ms step_avg:91.75ms
step:569/1645 train_time:52209ms step_avg:91.75ms
step:570/1645 train_time:52302ms step_avg:91.76ms
step:571/1645 train_time:52395ms step_avg:91.76ms
step:572/1645 train_time:52488ms step_avg:91.76ms
step:573/1645 train_time:52581ms step_avg:91.76ms
step:574/1645 train_time:52674ms step_avg:91.77ms
step:575/1645 train_time:52766ms step_avg:91.77ms
step:576/1645 train_time:52859ms step_avg:91.77ms
step:577/1645 train_time:52952ms step_avg:91.77ms
step:578/1645 train_time:53045ms step_avg:91.77ms
step:579/1645 train_time:53137ms step_avg:91.77ms
step:580/1645 train_time:53231ms step_avg:91.78ms
step:581/1645 train_time:53324ms step_avg:91.78ms
step:582/1645 train_time:53417ms step_avg:91.78ms
step:583/1645 train_time:53511ms step_avg:91.78ms
step:584/1645 train_time:53603ms step_avg:91.79ms
step:585/1645 train_time:53697ms step_avg:91.79ms
step:586/1645 train_time:53790ms step_avg:91.79ms
step:587/1645 train_time:53882ms step_avg:91.79ms
step:588/1645 train_time:53977ms step_avg:91.80ms
step:589/1645 train_time:54069ms step_avg:91.80ms
step:590/1645 train_time:54162ms step_avg:91.80ms
step:591/1645 train_time:54255ms step_avg:91.80ms
step:592/1645 train_time:54348ms step_avg:91.80ms
step:593/1645 train_time:54441ms step_avg:91.81ms
step:594/1645 train_time:54535ms step_avg:91.81ms
step:595/1645 train_time:54628ms step_avg:91.81ms
step:596/1645 train_time:54720ms step_avg:91.81ms
step:597/1645 train_time:54813ms step_avg:91.81ms
step:598/1645 train_time:54905ms step_avg:91.82ms
step:599/1645 train_time:54998ms step_avg:91.82ms
step:600/1645 train_time:55091ms step_avg:91.82ms
step:601/1645 train_time:55183ms step_avg:91.82ms
step:602/1645 train_time:55277ms step_avg:91.82ms
step:603/1645 train_time:55370ms step_avg:91.82ms
step:604/1645 train_time:55464ms step_avg:91.83ms
step:605/1645 train_time:55557ms step_avg:91.83ms
step:606/1645 train_time:55649ms step_avg:91.83ms
step:607/1645 train_time:55742ms step_avg:91.83ms
step:608/1645 train_time:55835ms step_avg:91.83ms
step:609/1645 train_time:55928ms step_avg:91.84ms
step:610/1645 train_time:56021ms step_avg:91.84ms
step:611/1645 train_time:56114ms step_avg:91.84ms
step:612/1645 train_time:56207ms step_avg:91.84ms
step:613/1645 train_time:56300ms step_avg:91.84ms
step:614/1645 train_time:56394ms step_avg:91.85ms
step:615/1645 train_time:56487ms step_avg:91.85ms
step:616/1645 train_time:56580ms step_avg:91.85ms
step:617/1645 train_time:56673ms step_avg:91.85ms
step:618/1645 train_time:56766ms step_avg:91.85ms
step:619/1645 train_time:56859ms step_avg:91.86ms
step:620/1645 train_time:56951ms step_avg:91.86ms
step:621/1645 train_time:57043ms step_avg:91.86ms
step:622/1645 train_time:57137ms step_avg:91.86ms
step:623/1645 train_time:57229ms step_avg:91.86ms
step:624/1645 train_time:57322ms step_avg:91.86ms
step:625/1645 train_time:57416ms step_avg:91.87ms
step:625/1645 val_loss:3.6095 train_time:57509ms step_avg:92.01ms
step:626/1645 train_time:57526ms step_avg:91.89ms
step:627/1645 train_time:57611ms step_avg:91.88ms
step:628/1645 train_time:57714ms step_avg:91.90ms
step:629/1645 train_time:57812ms step_avg:91.91ms
step:630/1645 train_time:57904ms step_avg:91.91ms
step:631/1645 train_time:57995ms step_avg:91.91ms
step:632/1645 train_time:58087ms step_avg:91.91ms
step:633/1645 train_time:58178ms step_avg:91.91ms
step:634/1645 train_time:58269ms step_avg:91.91ms
step:635/1645 train_time:58361ms step_avg:91.91ms
step:636/1645 train_time:58454ms step_avg:91.91ms
step:637/1645 train_time:58546ms step_avg:91.91ms
step:638/1645 train_time:58643ms step_avg:91.92ms
step:639/1645 train_time:58739ms step_avg:91.92ms
step:640/1645 train_time:58834ms step_avg:91.93ms
step:641/1645 train_time:58926ms step_avg:91.93ms
step:642/1645 train_time:59020ms step_avg:91.93ms
step:643/1645 train_time:59112ms step_avg:91.93ms
step:644/1645 train_time:59203ms step_avg:91.93ms
step:645/1645 train_time:59296ms step_avg:91.93ms
step:646/1645 train_time:59387ms step_avg:91.93ms
step:647/1645 train_time:59479ms step_avg:91.93ms
step:648/1645 train_time:59573ms step_avg:91.93ms
step:649/1645 train_time:59668ms step_avg:91.94ms
step:650/1645 train_time:59763ms step_avg:91.94ms
step:651/1645 train_time:59857ms step_avg:91.95ms
step:652/1645 train_time:59950ms step_avg:91.95ms
step:653/1645 train_time:60044ms step_avg:91.95ms
step:654/1645 train_time:60135ms step_avg:91.95ms
step:655/1645 train_time:60227ms step_avg:91.95ms
step:656/1645 train_time:60320ms step_avg:91.95ms
step:657/1645 train_time:60412ms step_avg:91.95ms
step:658/1645 train_time:60505ms step_avg:91.95ms
step:659/1645 train_time:60599ms step_avg:91.96ms
step:660/1645 train_time:60692ms step_avg:91.96ms
step:661/1645 train_time:60786ms step_avg:91.96ms
step:662/1645 train_time:60879ms step_avg:91.96ms
step:663/1645 train_time:60972ms step_avg:91.96ms
step:664/1645 train_time:61065ms step_avg:91.97ms
step:665/1645 train_time:61158ms step_avg:91.97ms
step:666/1645 train_time:61252ms step_avg:91.97ms
step:667/1645 train_time:61343ms step_avg:91.97ms
step:668/1645 train_time:61436ms step_avg:91.97ms
step:669/1645 train_time:61529ms step_avg:91.97ms
step:670/1645 train_time:61622ms step_avg:91.97ms
step:671/1645 train_time:61715ms step_avg:91.98ms
step:672/1645 train_time:61808ms step_avg:91.98ms
step:673/1645 train_time:61902ms step_avg:91.98ms
step:674/1645 train_time:61995ms step_avg:91.98ms
step:675/1645 train_time:62088ms step_avg:91.98ms
step:676/1645 train_time:62181ms step_avg:91.98ms
step:677/1645 train_time:62273ms step_avg:91.98ms
step:678/1645 train_time:62365ms step_avg:91.98ms
step:679/1645 train_time:62457ms step_avg:91.98ms
step:680/1645 train_time:62550ms step_avg:91.98ms
step:681/1645 train_time:62644ms step_avg:91.99ms
step:682/1645 train_time:62737ms step_avg:91.99ms
step:683/1645 train_time:62830ms step_avg:91.99ms
step:684/1645 train_time:62923ms step_avg:91.99ms
step:685/1645 train_time:63017ms step_avg:92.00ms
step:686/1645 train_time:63109ms step_avg:92.00ms
step:687/1645 train_time:63203ms step_avg:92.00ms
step:688/1645 train_time:63295ms step_avg:92.00ms
step:689/1645 train_time:63388ms step_avg:92.00ms
step:690/1645 train_time:63480ms step_avg:92.00ms
step:691/1645 train_time:63573ms step_avg:92.00ms
step:692/1645 train_time:63667ms step_avg:92.00ms
step:693/1645 train_time:63761ms step_avg:92.01ms
step:694/1645 train_time:63854ms step_avg:92.01ms
step:695/1645 train_time:63947ms step_avg:92.01ms
step:696/1645 train_time:64041ms step_avg:92.01ms
step:697/1645 train_time:64134ms step_avg:92.01ms
step:698/1645 train_time:64227ms step_avg:92.02ms
step:699/1645 train_time:64320ms step_avg:92.02ms
step:700/1645 train_time:64414ms step_avg:92.02ms
step:701/1645 train_time:64506ms step_avg:92.02ms
step:702/1645 train_time:64601ms step_avg:92.02ms
step:703/1645 train_time:64694ms step_avg:92.03ms
step:704/1645 train_time:64786ms step_avg:92.03ms
step:705/1645 train_time:64878ms step_avg:92.03ms
step:706/1645 train_time:64971ms step_avg:92.03ms
step:707/1645 train_time:65064ms step_avg:92.03ms
step:708/1645 train_time:65157ms step_avg:92.03ms
step:709/1645 train_time:65249ms step_avg:92.03ms
step:710/1645 train_time:65343ms step_avg:92.03ms
step:711/1645 train_time:65436ms step_avg:92.03ms
step:712/1645 train_time:65529ms step_avg:92.03ms
step:713/1645 train_time:65623ms step_avg:92.04ms
step:714/1645 train_time:65716ms step_avg:92.04ms
step:715/1645 train_time:65808ms step_avg:92.04ms
step:716/1645 train_time:65901ms step_avg:92.04ms
step:717/1645 train_time:65994ms step_avg:92.04ms
step:718/1645 train_time:66088ms step_avg:92.04ms
step:719/1645 train_time:66180ms step_avg:92.05ms
step:720/1645 train_time:66274ms step_avg:92.05ms
step:721/1645 train_time:66367ms step_avg:92.05ms
step:722/1645 train_time:66461ms step_avg:92.05ms
step:723/1645 train_time:66554ms step_avg:92.05ms
step:724/1645 train_time:66646ms step_avg:92.05ms
step:725/1645 train_time:66739ms step_avg:92.05ms
step:726/1645 train_time:66832ms step_avg:92.05ms
step:727/1645 train_time:66925ms step_avg:92.06ms
step:728/1645 train_time:67019ms step_avg:92.06ms
step:729/1645 train_time:67111ms step_avg:92.06ms
step:730/1645 train_time:67205ms step_avg:92.06ms
step:731/1645 train_time:67297ms step_avg:92.06ms
step:732/1645 train_time:67391ms step_avg:92.06ms
step:733/1645 train_time:67484ms step_avg:92.07ms
step:734/1645 train_time:67579ms step_avg:92.07ms
step:735/1645 train_time:67670ms step_avg:92.07ms
step:736/1645 train_time:67763ms step_avg:92.07ms
step:737/1645 train_time:67856ms step_avg:92.07ms
step:738/1645 train_time:67949ms step_avg:92.07ms
step:739/1645 train_time:68042ms step_avg:92.07ms
step:740/1645 train_time:68136ms step_avg:92.08ms
step:741/1645 train_time:68229ms step_avg:92.08ms
step:742/1645 train_time:68322ms step_avg:92.08ms
step:743/1645 train_time:68415ms step_avg:92.08ms
step:744/1645 train_time:68508ms step_avg:92.08ms
step:745/1645 train_time:68602ms step_avg:92.08ms
step:746/1645 train_time:68695ms step_avg:92.08ms
step:747/1645 train_time:68787ms step_avg:92.08ms
step:748/1645 train_time:68881ms step_avg:92.09ms
step:749/1645 train_time:68974ms step_avg:92.09ms
step:750/1645 train_time:69067ms step_avg:92.09ms
step:750/1645 val_loss:3.5597 train_time:69161ms step_avg:92.21ms
step:751/1645 train_time:69177ms step_avg:92.11ms
step:752/1645 train_time:69258ms step_avg:92.10ms
step:753/1645 train_time:69352ms step_avg:92.10ms
step:754/1645 train_time:69446ms step_avg:92.10ms
step:755/1645 train_time:69538ms step_avg:92.10ms
step:756/1645 train_time:69631ms step_avg:92.10ms
step:757/1645 train_time:69722ms step_avg:92.10ms
step:758/1645 train_time:69815ms step_avg:92.10ms
step:759/1645 train_time:69907ms step_avg:92.10ms
step:760/1645 train_time:70000ms step_avg:92.10ms
step:761/1645 train_time:70093ms step_avg:92.11ms
step:762/1645 train_time:70188ms step_avg:92.11ms
step:763/1645 train_time:70283ms step_avg:92.11ms
step:764/1645 train_time:70377ms step_avg:92.12ms
step:765/1645 train_time:70470ms step_avg:92.12ms
step:766/1645 train_time:70563ms step_avg:92.12ms
step:767/1645 train_time:70655ms step_avg:92.12ms
step:768/1645 train_time:70748ms step_avg:92.12ms
step:769/1645 train_time:70841ms step_avg:92.12ms
step:770/1645 train_time:70933ms step_avg:92.12ms
step:771/1645 train_time:71026ms step_avg:92.12ms
step:772/1645 train_time:71119ms step_avg:92.12ms
step:773/1645 train_time:71213ms step_avg:92.13ms
step:774/1645 train_time:71307ms step_avg:92.13ms
step:775/1645 train_time:71401ms step_avg:92.13ms
step:776/1645 train_time:71494ms step_avg:92.13ms
step:777/1645 train_time:71587ms step_avg:92.13ms
step:778/1645 train_time:71680ms step_avg:92.13ms
step:779/1645 train_time:71773ms step_avg:92.14ms
step:780/1645 train_time:71866ms step_avg:92.14ms
step:781/1645 train_time:71958ms step_avg:92.14ms
step:782/1645 train_time:72051ms step_avg:92.14ms
step:783/1645 train_time:72145ms step_avg:92.14ms
step:784/1645 train_time:72238ms step_avg:92.14ms
step:785/1645 train_time:72332ms step_avg:92.14ms
step:786/1645 train_time:72425ms step_avg:92.14ms
step:787/1645 train_time:72518ms step_avg:92.14ms
step:788/1645 train_time:72611ms step_avg:92.15ms
step:789/1645 train_time:72704ms step_avg:92.15ms
step:790/1645 train_time:72796ms step_avg:92.15ms
step:791/1645 train_time:72889ms step_avg:92.15ms
step:792/1645 train_time:72981ms step_avg:92.15ms
step:793/1645 train_time:73075ms step_avg:92.15ms
step:794/1645 train_time:73169ms step_avg:92.15ms
step:795/1645 train_time:73261ms step_avg:92.15ms
step:796/1645 train_time:73355ms step_avg:92.15ms
step:797/1645 train_time:73448ms step_avg:92.16ms
step:798/1645 train_time:73541ms step_avg:92.16ms
step:799/1645 train_time:73634ms step_avg:92.16ms
step:800/1645 train_time:73727ms step_avg:92.16ms
step:801/1645 train_time:73820ms step_avg:92.16ms
step:802/1645 train_time:73912ms step_avg:92.16ms
step:803/1645 train_time:74005ms step_avg:92.16ms
step:804/1645 train_time:74099ms step_avg:92.16ms
step:805/1645 train_time:74192ms step_avg:92.16ms
step:806/1645 train_time:74286ms step_avg:92.17ms
step:807/1645 train_time:74380ms step_avg:92.17ms
step:808/1645 train_time:74473ms step_avg:92.17ms
step:809/1645 train_time:74567ms step_avg:92.17ms
step:810/1645 train_time:74661ms step_avg:92.17ms
step:811/1645 train_time:74753ms step_avg:92.17ms
step:812/1645 train_time:74846ms step_avg:92.17ms
step:813/1645 train_time:74938ms step_avg:92.17ms
step:814/1645 train_time:75032ms step_avg:92.18ms
step:815/1645 train_time:75124ms step_avg:92.18ms
step:816/1645 train_time:75217ms step_avg:92.18ms
step:817/1645 train_time:75309ms step_avg:92.18ms
step:818/1645 train_time:75402ms step_avg:92.18ms
step:819/1645 train_time:75495ms step_avg:92.18ms
step:820/1645 train_time:75588ms step_avg:92.18ms
step:821/1645 train_time:75682ms step_avg:92.18ms
step:822/1645 train_time:75774ms step_avg:92.18ms
step:823/1645 train_time:75868ms step_avg:92.18ms
step:824/1645 train_time:75962ms step_avg:92.19ms
step:825/1645 train_time:76054ms step_avg:92.19ms
step:826/1645 train_time:76148ms step_avg:92.19ms
step:827/1645 train_time:76241ms step_avg:92.19ms
step:828/1645 train_time:76334ms step_avg:92.19ms
step:829/1645 train_time:76426ms step_avg:92.19ms
step:830/1645 train_time:76519ms step_avg:92.19ms
step:831/1645 train_time:76612ms step_avg:92.19ms
step:832/1645 train_time:76705ms step_avg:92.19ms
step:833/1645 train_time:76798ms step_avg:92.19ms
step:834/1645 train_time:76891ms step_avg:92.19ms
step:835/1645 train_time:76984ms step_avg:92.20ms
step:836/1645 train_time:77077ms step_avg:92.20ms
step:837/1645 train_time:77170ms step_avg:92.20ms
step:838/1645 train_time:77263ms step_avg:92.20ms
step:839/1645 train_time:77355ms step_avg:92.20ms
step:840/1645 train_time:77449ms step_avg:92.20ms
step:841/1645 train_time:77542ms step_avg:92.20ms
step:842/1645 train_time:77635ms step_avg:92.20ms
step:843/1645 train_time:77728ms step_avg:92.20ms
step:844/1645 train_time:77821ms step_avg:92.20ms
step:845/1645 train_time:77913ms step_avg:92.20ms
step:846/1645 train_time:78007ms step_avg:92.21ms
step:847/1645 train_time:78101ms step_avg:92.21ms
step:848/1645 train_time:78193ms step_avg:92.21ms
step:849/1645 train_time:78286ms step_avg:92.21ms
step:850/1645 train_time:78380ms step_avg:92.21ms
step:851/1645 train_time:78474ms step_avg:92.21ms
step:852/1645 train_time:78566ms step_avg:92.21ms
step:853/1645 train_time:78659ms step_avg:92.21ms
step:854/1645 train_time:78752ms step_avg:92.22ms
step:855/1645 train_time:78844ms step_avg:92.22ms
step:856/1645 train_time:78937ms step_avg:92.22ms
step:857/1645 train_time:79030ms step_avg:92.22ms
step:858/1645 train_time:79123ms step_avg:92.22ms
step:859/1645 train_time:79215ms step_avg:92.22ms
step:860/1645 train_time:79308ms step_avg:92.22ms
step:861/1645 train_time:79401ms step_avg:92.22ms
step:862/1645 train_time:79494ms step_avg:92.22ms
step:863/1645 train_time:79588ms step_avg:92.22ms
step:864/1645 train_time:79680ms step_avg:92.22ms
step:865/1645 train_time:79773ms step_avg:92.22ms
step:866/1645 train_time:79866ms step_avg:92.22ms
step:867/1645 train_time:79959ms step_avg:92.23ms
step:868/1645 train_time:80052ms step_avg:92.23ms
step:869/1645 train_time:80145ms step_avg:92.23ms
step:870/1645 train_time:80238ms step_avg:92.23ms
step:871/1645 train_time:80331ms step_avg:92.23ms
step:872/1645 train_time:80424ms step_avg:92.23ms
step:873/1645 train_time:80517ms step_avg:92.23ms
step:874/1645 train_time:80610ms step_avg:92.23ms
step:875/1645 train_time:80704ms step_avg:92.23ms
step:875/1645 val_loss:3.5121 train_time:80797ms step_avg:92.34ms
step:876/1645 train_time:80818ms step_avg:92.26ms
step:877/1645 train_time:80895ms step_avg:92.24ms
step:878/1645 train_time:80988ms step_avg:92.24ms
step:879/1645 train_time:81081ms step_avg:92.24ms
step:880/1645 train_time:81173ms step_avg:92.24ms
step:881/1645 train_time:81265ms step_avg:92.24ms
step:882/1645 train_time:81357ms step_avg:92.24ms
step:883/1645 train_time:81449ms step_avg:92.24ms
step:884/1645 train_time:81542ms step_avg:92.24ms
step:885/1645 train_time:81634ms step_avg:92.24ms
step:886/1645 train_time:81730ms step_avg:92.25ms
step:887/1645 train_time:81824ms step_avg:92.25ms
step:888/1645 train_time:81918ms step_avg:92.25ms
step:889/1645 train_time:82011ms step_avg:92.25ms
step:890/1645 train_time:82104ms step_avg:92.25ms
step:891/1645 train_time:82196ms step_avg:92.25ms
step:892/1645 train_time:82289ms step_avg:92.25ms
step:893/1645 train_time:82381ms step_avg:92.25ms
step:894/1645 train_time:82474ms step_avg:92.25ms
step:895/1645 train_time:82566ms step_avg:92.25ms
step:896/1645 train_time:82659ms step_avg:92.25ms
step:897/1645 train_time:82753ms step_avg:92.26ms
step:898/1645 train_time:82847ms step_avg:92.26ms
step:899/1645 train_time:82940ms step_avg:92.26ms
step:900/1645 train_time:83034ms step_avg:92.26ms
step:901/1645 train_time:83127ms step_avg:92.26ms
step:902/1645 train_time:83220ms step_avg:92.26ms
step:903/1645 train_time:83312ms step_avg:92.26ms
step:904/1645 train_time:83405ms step_avg:92.26ms
step:905/1645 train_time:83497ms step_avg:92.26ms
step:906/1645 train_time:83590ms step_avg:92.26ms
step:907/1645 train_time:83684ms step_avg:92.26ms
step:908/1645 train_time:83778ms step_avg:92.27ms
step:909/1645 train_time:83870ms step_avg:92.27ms
step:910/1645 train_time:83964ms step_avg:92.27ms
step:911/1645 train_time:84056ms step_avg:92.27ms
step:912/1645 train_time:84149ms step_avg:92.27ms
step:913/1645 train_time:84242ms step_avg:92.27ms
step:914/1645 train_time:84334ms step_avg:92.27ms
step:915/1645 train_time:84427ms step_avg:92.27ms
step:916/1645 train_time:84520ms step_avg:92.27ms
step:917/1645 train_time:84613ms step_avg:92.27ms
step:918/1645 train_time:84706ms step_avg:92.27ms
step:919/1645 train_time:84799ms step_avg:92.27ms
step:920/1645 train_time:84892ms step_avg:92.27ms
step:921/1645 train_time:84985ms step_avg:92.27ms
step:922/1645 train_time:85078ms step_avg:92.28ms
step:923/1645 train_time:85171ms step_avg:92.28ms
step:924/1645 train_time:85264ms step_avg:92.28ms
step:925/1645 train_time:85356ms step_avg:92.28ms
step:926/1645 train_time:85449ms step_avg:92.28ms
step:927/1645 train_time:85542ms step_avg:92.28ms
step:928/1645 train_time:85635ms step_avg:92.28ms
step:929/1645 train_time:85728ms step_avg:92.28ms
step:930/1645 train_time:85822ms step_avg:92.28ms
step:931/1645 train_time:85915ms step_avg:92.28ms
step:932/1645 train_time:86008ms step_avg:92.28ms
step:933/1645 train_time:86101ms step_avg:92.28ms
step:934/1645 train_time:86194ms step_avg:92.29ms
step:935/1645 train_time:86287ms step_avg:92.29ms
step:936/1645 train_time:86380ms step_avg:92.29ms
step:937/1645 train_time:86472ms step_avg:92.29ms
step:938/1645 train_time:86565ms step_avg:92.29ms
step:939/1645 train_time:86658ms step_avg:92.29ms
step:940/1645 train_time:86752ms step_avg:92.29ms
step:941/1645 train_time:86844ms step_avg:92.29ms
step:942/1645 train_time:86938ms step_avg:92.29ms
step:943/1645 train_time:87031ms step_avg:92.29ms
step:944/1645 train_time:87124ms step_avg:92.29ms
step:945/1645 train_time:87216ms step_avg:92.29ms
step:946/1645 train_time:87310ms step_avg:92.29ms
step:947/1645 train_time:87403ms step_avg:92.30ms
step:948/1645 train_time:87496ms step_avg:92.29ms
step:949/1645 train_time:87589ms step_avg:92.30ms
step:950/1645 train_time:87682ms step_avg:92.30ms
step:951/1645 train_time:87775ms step_avg:92.30ms
step:952/1645 train_time:87867ms step_avg:92.30ms
step:953/1645 train_time:87960ms step_avg:92.30ms
step:954/1645 train_time:88054ms step_avg:92.30ms
step:955/1645 train_time:88146ms step_avg:92.30ms
step:956/1645 train_time:88240ms step_avg:92.30ms
step:957/1645 train_time:88333ms step_avg:92.30ms
step:958/1645 train_time:88427ms step_avg:92.30ms
step:959/1645 train_time:88520ms step_avg:92.30ms
step:960/1645 train_time:88612ms step_avg:92.30ms
step:961/1645 train_time:88705ms step_avg:92.31ms
step:962/1645 train_time:88799ms step_avg:92.31ms
step:963/1645 train_time:88891ms step_avg:92.31ms
step:964/1645 train_time:88984ms step_avg:92.31ms
step:965/1645 train_time:89077ms step_avg:92.31ms
step:966/1645 train_time:89169ms step_avg:92.31ms
step:967/1645 train_time:89264ms step_avg:92.31ms
step:968/1645 train_time:89356ms step_avg:92.31ms
step:969/1645 train_time:89449ms step_avg:92.31ms
step:970/1645 train_time:89542ms step_avg:92.31ms
step:971/1645 train_time:89635ms step_avg:92.31ms
step:972/1645 train_time:89728ms step_avg:92.31ms
step:973/1645 train_time:89821ms step_avg:92.31ms
step:974/1645 train_time:89913ms step_avg:92.31ms
step:975/1645 train_time:90006ms step_avg:92.31ms
step:976/1645 train_time:90099ms step_avg:92.31ms
step:977/1645 train_time:90191ms step_avg:92.31ms
step:978/1645 train_time:90284ms step_avg:92.32ms
step:979/1645 train_time:90378ms step_avg:92.32ms
step:980/1645 train_time:90471ms step_avg:92.32ms
step:981/1645 train_time:90564ms step_avg:92.32ms
step:982/1645 train_time:90656ms step_avg:92.32ms
step:983/1645 train_time:90750ms step_avg:92.32ms
step:984/1645 train_time:90843ms step_avg:92.32ms
step:985/1645 train_time:90935ms step_avg:92.32ms
step:986/1645 train_time:91029ms step_avg:92.32ms
step:987/1645 train_time:91122ms step_avg:92.32ms
step:988/1645 train_time:91215ms step_avg:92.32ms
step:989/1645 train_time:91308ms step_avg:92.32ms
step:990/1645 train_time:91402ms step_avg:92.33ms
step:991/1645 train_time:91495ms step_avg:92.33ms
step:992/1645 train_time:91587ms step_avg:92.33ms
step:993/1645 train_time:91681ms step_avg:92.33ms
step:994/1645 train_time:91773ms step_avg:92.33ms
step:995/1645 train_time:91865ms step_avg:92.33ms
step:996/1645 train_time:91958ms step_avg:92.33ms
step:997/1645 train_time:92051ms step_avg:92.33ms
step:998/1645 train_time:92144ms step_avg:92.33ms
step:999/1645 train_time:92236ms step_avg:92.33ms
step:1000/1645 train_time:92331ms step_avg:92.33ms
step:1000/1645 val_loss:3.4634 train_time:92424ms step_avg:92.42ms
step:1001/1645 train_time:92446ms step_avg:92.35ms
step:1002/1645 train_time:92522ms step_avg:92.34ms
step:1003/1645 train_time:92619ms step_avg:92.34ms
step:1004/1645 train_time:92712ms step_avg:92.34ms
step:1005/1645 train_time:92805ms step_avg:92.34ms
step:1006/1645 train_time:92897ms step_avg:92.34ms
step:1007/1645 train_time:92988ms step_avg:92.34ms
step:1008/1645 train_time:93079ms step_avg:92.34ms
step:1009/1645 train_time:93171ms step_avg:92.34ms
step:1010/1645 train_time:93265ms step_avg:92.34ms
step:1011/1645 train_time:93358ms step_avg:92.34ms
step:1012/1645 train_time:93453ms step_avg:92.34ms
step:1013/1645 train_time:93549ms step_avg:92.35ms
step:1014/1645 train_time:93643ms step_avg:92.35ms
step:1015/1645 train_time:93736ms step_avg:92.35ms
step:1016/1645 train_time:93830ms step_avg:92.35ms
step:1017/1645 train_time:93921ms step_avg:92.35ms
step:1018/1645 train_time:94013ms step_avg:92.35ms
step:1019/1645 train_time:94105ms step_avg:92.35ms
step:1020/1645 train_time:94197ms step_avg:92.35ms
step:1021/1645 train_time:94289ms step_avg:92.35ms
step:1022/1645 train_time:94382ms step_avg:92.35ms
step:1023/1645 train_time:94476ms step_avg:92.35ms
step:1024/1645 train_time:94571ms step_avg:92.35ms
step:1025/1645 train_time:94666ms step_avg:92.36ms
step:1026/1645 train_time:94759ms step_avg:92.36ms
step:1027/1645 train_time:94852ms step_avg:92.36ms
step:1028/1645 train_time:94945ms step_avg:92.36ms
step:1029/1645 train_time:95037ms step_avg:92.36ms
step:1030/1645 train_time:95129ms step_avg:92.36ms
step:1031/1645 train_time:95221ms step_avg:92.36ms
step:1032/1645 train_time:95313ms step_avg:92.36ms
step:1033/1645 train_time:95407ms step_avg:92.36ms
step:1034/1645 train_time:95500ms step_avg:92.36ms
step:1035/1645 train_time:95595ms step_avg:92.36ms
step:1036/1645 train_time:95689ms step_avg:92.36ms
step:1037/1645 train_time:95781ms step_avg:92.36ms
step:1038/1645 train_time:95875ms step_avg:92.36ms
step:1039/1645 train_time:95968ms step_avg:92.37ms
step:1040/1645 train_time:96060ms step_avg:92.37ms
step:1041/1645 train_time:96153ms step_avg:92.37ms
step:1042/1645 train_time:96245ms step_avg:92.37ms
step:1043/1645 train_time:96338ms step_avg:92.37ms
step:1044/1645 train_time:96431ms step_avg:92.37ms
step:1045/1645 train_time:96524ms step_avg:92.37ms
step:1046/1645 train_time:96618ms step_avg:92.37ms
step:1047/1645 train_time:96711ms step_avg:92.37ms
step:1048/1645 train_time:96804ms step_avg:92.37ms
step:1049/1645 train_time:96897ms step_avg:92.37ms
step:1050/1645 train_time:96991ms step_avg:92.37ms
step:1051/1645 train_time:97082ms step_avg:92.37ms
step:1052/1645 train_time:97175ms step_avg:92.37ms
step:1053/1645 train_time:97268ms step_avg:92.37ms
step:1054/1645 train_time:97360ms step_avg:92.37ms
step:1055/1645 train_time:97454ms step_avg:92.37ms
step:1056/1645 train_time:97547ms step_avg:92.37ms
step:1057/1645 train_time:97640ms step_avg:92.37ms
step:1058/1645 train_time:97735ms step_avg:92.38ms
step:1059/1645 train_time:97828ms step_avg:92.38ms
step:1060/1645 train_time:97921ms step_avg:92.38ms
step:1061/1645 train_time:98013ms step_avg:92.38ms
step:1062/1645 train_time:98106ms step_avg:92.38ms
step:1063/1645 train_time:98199ms step_avg:92.38ms
step:1064/1645 train_time:98292ms step_avg:92.38ms
step:1065/1645 train_time:98384ms step_avg:92.38ms
step:1066/1645 train_time:98477ms step_avg:92.38ms
step:1067/1645 train_time:98571ms step_avg:92.38ms
step:1068/1645 train_time:98665ms step_avg:92.38ms
step:1069/1645 train_time:98758ms step_avg:92.38ms
step:1070/1645 train_time:98852ms step_avg:92.38ms
step:1071/1645 train_time:98945ms step_avg:92.39ms
step:1072/1645 train_time:99038ms step_avg:92.39ms
step:1073/1645 train_time:99132ms step_avg:92.39ms
step:1074/1645 train_time:99224ms step_avg:92.39ms
step:1075/1645 train_time:99316ms step_avg:92.39ms
step:1076/1645 train_time:99409ms step_avg:92.39ms
step:1077/1645 train_time:99501ms step_avg:92.39ms
step:1078/1645 train_time:99594ms step_avg:92.39ms
step:1079/1645 train_time:99687ms step_avg:92.39ms
step:1080/1645 train_time:99780ms step_avg:92.39ms
step:1081/1645 train_time:99874ms step_avg:92.39ms
step:1082/1645 train_time:99967ms step_avg:92.39ms
step:1083/1645 train_time:100060ms step_avg:92.39ms
step:1084/1645 train_time:100152ms step_avg:92.39ms
step:1085/1645 train_time:100245ms step_avg:92.39ms
step:1086/1645 train_time:100337ms step_avg:92.39ms
step:1087/1645 train_time:100430ms step_avg:92.39ms
step:1088/1645 train_time:100523ms step_avg:92.39ms
step:1089/1645 train_time:100616ms step_avg:92.39ms
step:1090/1645 train_time:100709ms step_avg:92.39ms
step:1091/1645 train_time:100802ms step_avg:92.39ms
step:1092/1645 train_time:100896ms step_avg:92.40ms
step:1093/1645 train_time:100990ms step_avg:92.40ms
step:1094/1645 train_time:101081ms step_avg:92.40ms
step:1095/1645 train_time:101175ms step_avg:92.40ms
step:1096/1645 train_time:101268ms step_avg:92.40ms
step:1097/1645 train_time:101362ms step_avg:92.40ms
step:1098/1645 train_time:101455ms step_avg:92.40ms
step:1099/1645 train_time:101548ms step_avg:92.40ms
step:1100/1645 train_time:101641ms step_avg:92.40ms
step:1101/1645 train_time:101735ms step_avg:92.40ms
step:1102/1645 train_time:101829ms step_avg:92.40ms
step:1103/1645 train_time:101922ms step_avg:92.40ms
step:1104/1645 train_time:102016ms step_avg:92.41ms
step:1105/1645 train_time:102109ms step_avg:92.41ms
step:1106/1645 train_time:102203ms step_avg:92.41ms
step:1107/1645 train_time:102297ms step_avg:92.41ms
step:1108/1645 train_time:102390ms step_avg:92.41ms
step:1109/1645 train_time:102484ms step_avg:92.41ms
step:1110/1645 train_time:102577ms step_avg:92.41ms
step:1111/1645 train_time:102672ms step_avg:92.41ms
step:1112/1645 train_time:102765ms step_avg:92.41ms
step:1113/1645 train_time:102859ms step_avg:92.42ms
step:1114/1645 train_time:102953ms step_avg:92.42ms
step:1115/1645 train_time:103046ms step_avg:92.42ms
step:1116/1645 train_time:103139ms step_avg:92.42ms
step:1117/1645 train_time:103232ms step_avg:92.42ms
step:1118/1645 train_time:103327ms step_avg:92.42ms
step:1119/1645 train_time:103421ms step_avg:92.42ms
step:1120/1645 train_time:103514ms step_avg:92.42ms
step:1121/1645 train_time:103609ms step_avg:92.43ms
step:1122/1645 train_time:103702ms step_avg:92.43ms
step:1123/1645 train_time:103795ms step_avg:92.43ms
step:1124/1645 train_time:103889ms step_avg:92.43ms
step:1125/1645 train_time:103982ms step_avg:92.43ms
step:1125/1645 val_loss:3.4102 train_time:104076ms step_avg:92.51ms
step:1126/1645 train_time:104092ms step_avg:92.44ms
step:1127/1645 train_time:104175ms step_avg:92.44ms
step:1128/1645 train_time:104276ms step_avg:92.44ms
step:1129/1645 train_time:104369ms step_avg:92.44ms
step:1130/1645 train_time:104462ms step_avg:92.44ms
step:1131/1645 train_time:104555ms step_avg:92.44ms
step:1132/1645 train_time:104647ms step_avg:92.44ms
step:1133/1645 train_time:104739ms step_avg:92.44ms
step:1134/1645 train_time:104832ms step_avg:92.44ms
step:1135/1645 train_time:104924ms step_avg:92.44ms
step:1136/1645 train_time:105017ms step_avg:92.44ms
step:1137/1645 train_time:105112ms step_avg:92.45ms
step:1138/1645 train_time:105213ms step_avg:92.45ms
step:1139/1645 train_time:105308ms step_avg:92.46ms
step:1140/1645 train_time:105402ms step_avg:92.46ms
step:1141/1645 train_time:105495ms step_avg:92.46ms
step:1142/1645 train_time:105588ms step_avg:92.46ms
step:1143/1645 train_time:105680ms step_avg:92.46ms
step:1144/1645 train_time:105772ms step_avg:92.46ms
step:1145/1645 train_time:105865ms step_avg:92.46ms
step:1146/1645 train_time:105957ms step_avg:92.46ms
step:1147/1645 train_time:106051ms step_avg:92.46ms
step:1148/1645 train_time:106145ms step_avg:92.46ms
step:1149/1645 train_time:106240ms step_avg:92.46ms
step:1150/1645 train_time:106337ms step_avg:92.47ms
step:1151/1645 train_time:106430ms step_avg:92.47ms
step:1152/1645 train_time:106523ms step_avg:92.47ms
step:1153/1645 train_time:106617ms step_avg:92.47ms
step:1154/1645 train_time:106709ms step_avg:92.47ms
step:1155/1645 train_time:106802ms step_avg:92.47ms
step:1156/1645 train_time:106895ms step_avg:92.47ms
step:1157/1645 train_time:106988ms step_avg:92.47ms
step:1158/1645 train_time:107082ms step_avg:92.47ms
step:1159/1645 train_time:107176ms step_avg:92.47ms
step:1160/1645 train_time:107271ms step_avg:92.47ms
step:1161/1645 train_time:107366ms step_avg:92.48ms
step:1162/1645 train_time:107459ms step_avg:92.48ms
step:1163/1645 train_time:107553ms step_avg:92.48ms
step:1164/1645 train_time:107647ms step_avg:92.48ms
step:1165/1645 train_time:107739ms step_avg:92.48ms
step:1166/1645 train_time:107833ms step_avg:92.48ms
step:1167/1645 train_time:107926ms step_avg:92.48ms
step:1168/1645 train_time:108019ms step_avg:92.48ms
step:1169/1645 train_time:108113ms step_avg:92.48ms
step:1170/1645 train_time:108207ms step_avg:92.48ms
step:1171/1645 train_time:108302ms step_avg:92.49ms
step:1172/1645 train_time:108396ms step_avg:92.49ms
step:1173/1645 train_time:108491ms step_avg:92.49ms
step:1174/1645 train_time:108584ms step_avg:92.49ms
step:1175/1645 train_time:108677ms step_avg:92.49ms
step:1176/1645 train_time:108770ms step_avg:92.49ms
step:1177/1645 train_time:108863ms step_avg:92.49ms
step:1178/1645 train_time:108958ms step_avg:92.49ms
step:1179/1645 train_time:109051ms step_avg:92.49ms
step:1180/1645 train_time:109144ms step_avg:92.50ms
step:1181/1645 train_time:109237ms step_avg:92.50ms
step:1182/1645 train_time:109332ms step_avg:92.50ms
step:1183/1645 train_time:109427ms step_avg:92.50ms
step:1184/1645 train_time:109520ms step_avg:92.50ms
step:1185/1645 train_time:109614ms step_avg:92.50ms
step:1186/1645 train_time:109707ms step_avg:92.50ms
step:1187/1645 train_time:109800ms step_avg:92.50ms
step:1188/1645 train_time:109893ms step_avg:92.50ms
step:1189/1645 train_time:109988ms step_avg:92.50ms
step:1190/1645 train_time:110080ms step_avg:92.50ms
step:1191/1645 train_time:110173ms step_avg:92.51ms
step:1192/1645 train_time:110269ms step_avg:92.51ms
step:1193/1645 train_time:110362ms step_avg:92.51ms
step:1194/1645 train_time:110456ms step_avg:92.51ms
step:1195/1645 train_time:110551ms step_avg:92.51ms
step:1196/1645 train_time:110644ms step_avg:92.51ms
step:1197/1645 train_time:110737ms step_avg:92.51ms
step:1198/1645 train_time:110831ms step_avg:92.51ms
step:1199/1645 train_time:110924ms step_avg:92.51ms
step:1200/1645 train_time:111017ms step_avg:92.51ms
step:1201/1645 train_time:111110ms step_avg:92.51ms
step:1202/1645 train_time:111204ms step_avg:92.52ms
step:1203/1645 train_time:111297ms step_avg:92.52ms
step:1204/1645 train_time:111391ms step_avg:92.52ms
step:1205/1645 train_time:111485ms step_avg:92.52ms
step:1206/1645 train_time:111579ms step_avg:92.52ms
step:1207/1645 train_time:111672ms step_avg:92.52ms
step:1208/1645 train_time:111768ms step_avg:92.52ms
step:1209/1645 train_time:111860ms step_avg:92.52ms
step:1210/1645 train_time:111954ms step_avg:92.52ms
step:1211/1645 train_time:112047ms step_avg:92.52ms
step:1212/1645 train_time:112140ms step_avg:92.52ms
step:1213/1645 train_time:112234ms step_avg:92.53ms
step:1214/1645 train_time:112327ms step_avg:92.53ms
step:1215/1645 train_time:112420ms step_avg:92.53ms
step:1216/1645 train_time:112514ms step_avg:92.53ms
step:1217/1645 train_time:112608ms step_avg:92.53ms
step:1218/1645 train_time:112701ms step_avg:92.53ms
step:1219/1645 train_time:112795ms step_avg:92.53ms
step:1220/1645 train_time:112887ms step_avg:92.53ms
step:1221/1645 train_time:112981ms step_avg:92.53ms
step:1222/1645 train_time:113075ms step_avg:92.53ms
step:1223/1645 train_time:113167ms step_avg:92.53ms
step:1224/1645 train_time:113261ms step_avg:92.53ms
step:1225/1645 train_time:113355ms step_avg:92.53ms
step:1226/1645 train_time:113448ms step_avg:92.54ms
step:1227/1645 train_time:113541ms step_avg:92.54ms
step:1228/1645 train_time:113636ms step_avg:92.54ms
step:1229/1645 train_time:113729ms step_avg:92.54ms
step:1230/1645 train_time:113823ms step_avg:92.54ms
step:1231/1645 train_time:113916ms step_avg:92.54ms
step:1232/1645 train_time:114010ms step_avg:92.54ms
step:1233/1645 train_time:114104ms step_avg:92.54ms
step:1234/1645 train_time:114197ms step_avg:92.54ms
step:1235/1645 train_time:114292ms step_avg:92.54ms
step:1236/1645 train_time:114386ms step_avg:92.54ms
step:1237/1645 train_time:114480ms step_avg:92.55ms
step:1238/1645 train_time:114572ms step_avg:92.55ms
step:1239/1645 train_time:114665ms step_avg:92.55ms
step:1240/1645 train_time:114758ms step_avg:92.55ms
step:1241/1645 train_time:114853ms step_avg:92.55ms
step:1242/1645 train_time:114946ms step_avg:92.55ms
step:1243/1645 train_time:115039ms step_avg:92.55ms
step:1244/1645 train_time:115133ms step_avg:92.55ms
step:1245/1645 train_time:115228ms step_avg:92.55ms
step:1246/1645 train_time:115322ms step_avg:92.55ms
step:1247/1645 train_time:115415ms step_avg:92.55ms
step:1248/1645 train_time:115508ms step_avg:92.55ms
step:1249/1645 train_time:115602ms step_avg:92.56ms
step:1250/1645 train_time:115695ms step_avg:92.56ms
step:1250/1645 val_loss:3.3726 train_time:115789ms step_avg:92.63ms
step:1251/1645 train_time:115811ms step_avg:92.57ms
step:1252/1645 train_time:115887ms step_avg:92.56ms
step:1253/1645 train_time:115983ms step_avg:92.56ms
step:1254/1645 train_time:116076ms step_avg:92.56ms
step:1255/1645 train_time:116169ms step_avg:92.56ms
step:1256/1645 train_time:116261ms step_avg:92.56ms
step:1257/1645 train_time:116353ms step_avg:92.56ms
step:1258/1645 train_time:116446ms step_avg:92.56ms
step:1259/1645 train_time:116539ms step_avg:92.57ms
step:1260/1645 train_time:116632ms step_avg:92.57ms
step:1261/1645 train_time:116727ms step_avg:92.57ms
step:1262/1645 train_time:116821ms step_avg:92.57ms
step:1263/1645 train_time:116916ms step_avg:92.57ms
step:1264/1645 train_time:117011ms step_avg:92.57ms
step:1265/1645 train_time:117105ms step_avg:92.57ms
step:1266/1645 train_time:117198ms step_avg:92.57ms
step:1267/1645 train_time:117292ms step_avg:92.57ms
step:1268/1645 train_time:117385ms step_avg:92.57ms
step:1269/1645 train_time:117477ms step_avg:92.57ms
step:1270/1645 train_time:117570ms step_avg:92.57ms
step:1271/1645 train_time:117664ms step_avg:92.58ms
step:1272/1645 train_time:117757ms step_avg:92.58ms
step:1273/1645 train_time:117851ms step_avg:92.58ms
step:1274/1645 train_time:117945ms step_avg:92.58ms
step:1275/1645 train_time:118039ms step_avg:92.58ms
step:1276/1645 train_time:118133ms step_avg:92.58ms
step:1277/1645 train_time:118227ms step_avg:92.58ms
step:1278/1645 train_time:118320ms step_avg:92.58ms
step:1279/1645 train_time:118413ms step_avg:92.58ms
step:1280/1645 train_time:118506ms step_avg:92.58ms
step:1281/1645 train_time:118599ms step_avg:92.58ms
step:1282/1645 train_time:118694ms step_avg:92.58ms
step:1283/1645 train_time:118788ms step_avg:92.59ms
step:1284/1645 train_time:118882ms step_avg:92.59ms
step:1285/1645 train_time:118975ms step_avg:92.59ms
step:1286/1645 train_time:119070ms step_avg:92.59ms
step:1287/1645 train_time:119164ms step_avg:92.59ms
step:1288/1645 train_time:119258ms step_avg:92.59ms
step:1289/1645 train_time:119351ms step_avg:92.59ms
step:1290/1645 train_time:119444ms step_avg:92.59ms
step:1291/1645 train_time:119538ms step_avg:92.59ms
step:1292/1645 train_time:119631ms step_avg:92.59ms
step:1293/1645 train_time:119725ms step_avg:92.59ms
step:1294/1645 train_time:119819ms step_avg:92.60ms
step:1295/1645 train_time:119912ms step_avg:92.60ms
step:1296/1645 train_time:120007ms step_avg:92.60ms
step:1297/1645 train_time:120102ms step_avg:92.60ms
step:1298/1645 train_time:120196ms step_avg:92.60ms
step:1299/1645 train_time:120290ms step_avg:92.60ms
step:1300/1645 train_time:120382ms step_avg:92.60ms
step:1301/1645 train_time:120475ms step_avg:92.60ms
step:1302/1645 train_time:120570ms step_avg:92.60ms
step:1303/1645 train_time:120663ms step_avg:92.60ms
step:1304/1645 train_time:120755ms step_avg:92.60ms
step:1305/1645 train_time:120849ms step_avg:92.60ms
step:1306/1645 train_time:120945ms step_avg:92.61ms
step:1307/1645 train_time:121038ms step_avg:92.61ms
step:1308/1645 train_time:121132ms step_avg:92.61ms
step:1309/1645 train_time:121226ms step_avg:92.61ms
step:1310/1645 train_time:121320ms step_avg:92.61ms
step:1311/1645 train_time:121413ms step_avg:92.61ms
step:1312/1645 train_time:121507ms step_avg:92.61ms
step:1313/1645 train_time:121600ms step_avg:92.61ms
step:1314/1645 train_time:121693ms step_avg:92.61ms
step:1315/1645 train_time:121786ms step_avg:92.61ms
step:1316/1645 train_time:121879ms step_avg:92.61ms
step:1317/1645 train_time:121973ms step_avg:92.61ms
step:1318/1645 train_time:122069ms step_avg:92.62ms
step:1319/1645 train_time:122163ms step_avg:92.62ms
step:1320/1645 train_time:122257ms step_avg:92.62ms
step:1321/1645 train_time:122351ms step_avg:92.62ms
step:1322/1645 train_time:122445ms step_avg:92.62ms
step:1323/1645 train_time:122538ms step_avg:92.62ms
step:1324/1645 train_time:122632ms step_avg:92.62ms
step:1325/1645 train_time:122725ms step_avg:92.62ms
step:1326/1645 train_time:122818ms step_avg:92.62ms
step:1327/1645 train_time:122911ms step_avg:92.62ms
step:1328/1645 train_time:123005ms step_avg:92.62ms
step:1329/1645 train_time:123098ms step_avg:92.62ms
step:1330/1645 train_time:123193ms step_avg:92.63ms
step:1331/1645 train_time:123286ms step_avg:92.63ms
step:1332/1645 train_time:123380ms step_avg:92.63ms
step:1333/1645 train_time:123473ms step_avg:92.63ms
step:1334/1645 train_time:123568ms step_avg:92.63ms
step:1335/1645 train_time:123662ms step_avg:92.63ms
step:1336/1645 train_time:123756ms step_avg:92.63ms
step:1337/1645 train_time:123850ms step_avg:92.63ms
step:1338/1645 train_time:123943ms step_avg:92.63ms
step:1339/1645 train_time:124037ms step_avg:92.63ms
step:1340/1645 train_time:124130ms step_avg:92.63ms
step:1341/1645 train_time:124224ms step_avg:92.64ms
step:1342/1645 train_time:124317ms step_avg:92.64ms
step:1343/1645 train_time:124410ms step_avg:92.64ms
step:1344/1645 train_time:124503ms step_avg:92.64ms
step:1345/1645 train_time:124597ms step_avg:92.64ms
step:1346/1645 train_time:124691ms step_avg:92.64ms
step:1347/1645 train_time:124784ms step_avg:92.64ms
step:1348/1645 train_time:124877ms step_avg:92.64ms
step:1349/1645 train_time:124971ms step_avg:92.64ms
step:1350/1645 train_time:125067ms step_avg:92.64ms
step:1351/1645 train_time:125161ms step_avg:92.64ms
step:1352/1645 train_time:125254ms step_avg:92.64ms
step:1353/1645 train_time:125347ms step_avg:92.64ms
step:1354/1645 train_time:125441ms step_avg:92.64ms
step:1355/1645 train_time:125534ms step_avg:92.64ms
step:1356/1645 train_time:125629ms step_avg:92.65ms
step:1357/1645 train_time:125723ms step_avg:92.65ms
step:1358/1645 train_time:125815ms step_avg:92.65ms
step:1359/1645 train_time:125909ms step_avg:92.65ms
step:1360/1645 train_time:126002ms step_avg:92.65ms
step:1361/1645 train_time:126095ms step_avg:92.65ms
step:1362/1645 train_time:126189ms step_avg:92.65ms
step:1363/1645 train_time:126283ms step_avg:92.65ms
step:1364/1645 train_time:126376ms step_avg:92.65ms
step:1365/1645 train_time:126472ms step_avg:92.65ms
step:1366/1645 train_time:126564ms step_avg:92.65ms
step:1367/1645 train_time:126659ms step_avg:92.65ms
step:1368/1645 train_time:126751ms step_avg:92.65ms
step:1369/1645 train_time:126845ms step_avg:92.65ms
step:1370/1645 train_time:126938ms step_avg:92.66ms
step:1371/1645 train_time:127032ms step_avg:92.66ms
step:1372/1645 train_time:127125ms step_avg:92.66ms
step:1373/1645 train_time:127218ms step_avg:92.66ms
step:1374/1645 train_time:127311ms step_avg:92.66ms
step:1375/1645 train_time:127405ms step_avg:92.66ms
step:1375/1645 val_loss:3.3378 train_time:127499ms step_avg:92.73ms
step:1376/1645 train_time:127521ms step_avg:92.67ms
step:1377/1645 train_time:127597ms step_avg:92.66ms
step:1378/1645 train_time:127691ms step_avg:92.66ms
step:1379/1645 train_time:127785ms step_avg:92.66ms
step:1380/1645 train_time:127878ms step_avg:92.66ms
step:1381/1645 train_time:127971ms step_avg:92.67ms
step:1382/1645 train_time:128064ms step_avg:92.67ms
step:1383/1645 train_time:128157ms step_avg:92.67ms
step:1384/1645 train_time:128250ms step_avg:92.67ms
step:1385/1645 train_time:128343ms step_avg:92.67ms
step:1386/1645 train_time:128437ms step_avg:92.67ms
step:1387/1645 train_time:128532ms step_avg:92.67ms
step:1388/1645 train_time:128627ms step_avg:92.67ms
step:1389/1645 train_time:128721ms step_avg:92.67ms
step:1390/1645 train_time:128816ms step_avg:92.67ms
step:1391/1645 train_time:128908ms step_avg:92.67ms
step:1392/1645 train_time:129001ms step_avg:92.67ms
step:1393/1645 train_time:129094ms step_avg:92.67ms
step:1394/1645 train_time:129188ms step_avg:92.67ms
step:1395/1645 train_time:129281ms step_avg:92.67ms
step:1396/1645 train_time:129375ms step_avg:92.68ms
step:1397/1645 train_time:129468ms step_avg:92.68ms
step:1398/1645 train_time:129563ms step_avg:92.68ms
step:1399/1645 train_time:129657ms step_avg:92.68ms
step:1400/1645 train_time:129751ms step_avg:92.68ms
step:1401/1645 train_time:129844ms step_avg:92.68ms
step:1402/1645 train_time:129937ms step_avg:92.68ms
step:1403/1645 train_time:130030ms step_avg:92.68ms
step:1404/1645 train_time:130123ms step_avg:92.68ms
step:1405/1645 train_time:130217ms step_avg:92.68ms
step:1406/1645 train_time:130310ms step_avg:92.68ms
step:1407/1645 train_time:130404ms step_avg:92.68ms
step:1408/1645 train_time:130498ms step_avg:92.68ms
step:1409/1645 train_time:130593ms step_avg:92.68ms
step:1410/1645 train_time:130687ms step_avg:92.69ms
step:1411/1645 train_time:130781ms step_avg:92.69ms
step:1412/1645 train_time:130874ms step_avg:92.69ms
step:1413/1645 train_time:130968ms step_avg:92.69ms
step:1414/1645 train_time:131062ms step_avg:92.69ms
step:1415/1645 train_time:131154ms step_avg:92.69ms
step:1416/1645 train_time:131247ms step_avg:92.69ms
step:1417/1645 train_time:131340ms step_avg:92.69ms
step:1418/1645 train_time:131434ms step_avg:92.69ms
step:1419/1645 train_time:131528ms step_avg:92.69ms
step:1420/1645 train_time:131622ms step_avg:92.69ms
step:1421/1645 train_time:131716ms step_avg:92.69ms
step:1422/1645 train_time:131809ms step_avg:92.69ms
step:1423/1645 train_time:131904ms step_avg:92.69ms
step:1424/1645 train_time:131998ms step_avg:92.70ms
step:1425/1645 train_time:132091ms step_avg:92.70ms
step:1426/1645 train_time:132184ms step_avg:92.70ms
step:1427/1645 train_time:132279ms step_avg:92.70ms
step:1428/1645 train_time:132372ms step_avg:92.70ms
step:1429/1645 train_time:132466ms step_avg:92.70ms
step:1430/1645 train_time:132560ms step_avg:92.70ms
step:1431/1645 train_time:132654ms step_avg:92.70ms
step:1432/1645 train_time:132748ms step_avg:92.70ms
step:1433/1645 train_time:132841ms step_avg:92.70ms
step:1434/1645 train_time:132935ms step_avg:92.70ms
step:1435/1645 train_time:133029ms step_avg:92.70ms
step:1436/1645 train_time:133123ms step_avg:92.70ms
step:1437/1645 train_time:133216ms step_avg:92.70ms
step:1438/1645 train_time:133309ms step_avg:92.70ms
step:1439/1645 train_time:133403ms step_avg:92.71ms
step:1440/1645 train_time:133498ms step_avg:92.71ms
step:1441/1645 train_time:133591ms step_avg:92.71ms
step:1442/1645 train_time:133684ms step_avg:92.71ms
step:1443/1645 train_time:133777ms step_avg:92.71ms
step:1444/1645 train_time:133870ms step_avg:92.71ms
step:1445/1645 train_time:133964ms step_avg:92.71ms
step:1446/1645 train_time:134058ms step_avg:92.71ms
step:1447/1645 train_time:134151ms step_avg:92.71ms
step:1448/1645 train_time:134245ms step_avg:92.71ms
step:1449/1645 train_time:134342ms step_avg:92.71ms
step:1450/1645 train_time:134434ms step_avg:92.71ms
step:1451/1645 train_time:134528ms step_avg:92.71ms
step:1452/1645 train_time:134622ms step_avg:92.72ms
step:1453/1645 train_time:134716ms step_avg:92.72ms
step:1454/1645 train_time:134808ms step_avg:92.72ms
step:1455/1645 train_time:134903ms step_avg:92.72ms
step:1456/1645 train_time:134997ms step_avg:92.72ms
step:1457/1645 train_time:135090ms step_avg:92.72ms
step:1458/1645 train_time:135184ms step_avg:92.72ms
step:1459/1645 train_time:135277ms step_avg:92.72ms
step:1460/1645 train_time:135370ms step_avg:92.72ms
step:1461/1645 train_time:135464ms step_avg:92.72ms
step:1462/1645 train_time:135561ms step_avg:92.72ms
step:1463/1645 train_time:135656ms step_avg:92.72ms
step:1464/1645 train_time:135748ms step_avg:92.72ms
step:1465/1645 train_time:135842ms step_avg:92.73ms
step:1466/1645 train_time:135937ms step_avg:92.73ms
step:1467/1645 train_time:136030ms step_avg:92.73ms
step:1468/1645 train_time:136123ms step_avg:92.73ms
step:1469/1645 train_time:136218ms step_avg:92.73ms
step:1470/1645 train_time:136312ms step_avg:92.73ms
step:1471/1645 train_time:136405ms step_avg:92.73ms
step:1472/1645 train_time:136501ms step_avg:92.73ms
step:1473/1645 train_time:136595ms step_avg:92.73ms
step:1474/1645 train_time:136688ms step_avg:92.73ms
step:1475/1645 train_time:136782ms step_avg:92.73ms
step:1476/1645 train_time:136876ms step_avg:92.73ms
step:1477/1645 train_time:136969ms step_avg:92.73ms
step:1478/1645 train_time:137063ms step_avg:92.74ms
step:1479/1645 train_time:137156ms step_avg:92.74ms
step:1480/1645 train_time:137251ms step_avg:92.74ms
step:1481/1645 train_time:137342ms step_avg:92.74ms
step:1482/1645 train_time:137437ms step_avg:92.74ms
step:1483/1645 train_time:137532ms step_avg:92.74ms
step:1484/1645 train_time:137626ms step_avg:92.74ms
step:1485/1645 train_time:137720ms step_avg:92.74ms
step:1486/1645 train_time:137813ms step_avg:92.74ms
step:1487/1645 train_time:137907ms step_avg:92.74ms
step:1488/1645 train_time:138001ms step_avg:92.74ms
step:1489/1645 train_time:138094ms step_avg:92.74ms
step:1490/1645 train_time:138188ms step_avg:92.74ms
step:1491/1645 train_time:138282ms step_avg:92.74ms
step:1492/1645 train_time:138374ms step_avg:92.74ms
step:1493/1645 train_time:138468ms step_avg:92.74ms
step:1494/1645 train_time:138563ms step_avg:92.75ms
step:1495/1645 train_time:138658ms step_avg:92.75ms
step:1496/1645 train_time:138750ms step_avg:92.75ms
step:1497/1645 train_time:138844ms step_avg:92.75ms
step:1498/1645 train_time:138937ms step_avg:92.75ms
step:1499/1645 train_time:139032ms step_avg:92.75ms
step:1500/1645 train_time:139125ms step_avg:92.75ms
step:1500/1645 val_loss:3.3082 train_time:139220ms step_avg:92.81ms
step:1501/1645 train_time:139242ms step_avg:92.77ms
step:1502/1645 train_time:139319ms step_avg:92.76ms
step:1503/1645 train_time:139417ms step_avg:92.76ms
step:1504/1645 train_time:139510ms step_avg:92.76ms
step:1505/1645 train_time:139603ms step_avg:92.76ms
step:1506/1645 train_time:139695ms step_avg:92.76ms
step:1507/1645 train_time:139787ms step_avg:92.76ms
step:1508/1645 train_time:139880ms step_avg:92.76ms
step:1509/1645 train_time:139973ms step_avg:92.76ms
step:1510/1645 train_time:140066ms step_avg:92.76ms
step:1511/1645 train_time:140160ms step_avg:92.76ms
step:1512/1645 train_time:140256ms step_avg:92.76ms
step:1513/1645 train_time:140352ms step_avg:92.76ms
step:1514/1645 train_time:140446ms step_avg:92.77ms
step:1515/1645 train_time:140540ms step_avg:92.77ms
step:1516/1645 train_time:140634ms step_avg:92.77ms
step:1517/1645 train_time:140727ms step_avg:92.77ms
step:1518/1645 train_time:140820ms step_avg:92.77ms
step:1519/1645 train_time:140914ms step_avg:92.77ms
step:1520/1645 train_time:141006ms step_avg:92.77ms
step:1521/1645 train_time:141101ms step_avg:92.77ms
step:1522/1645 train_time:141195ms step_avg:92.77ms
step:1523/1645 train_time:141289ms step_avg:92.77ms
step:1524/1645 train_time:141383ms step_avg:92.77ms
step:1525/1645 train_time:141477ms step_avg:92.77ms
step:1526/1645 train_time:141571ms step_avg:92.77ms
step:1527/1645 train_time:141665ms step_avg:92.77ms
step:1528/1645 train_time:141758ms step_avg:92.77ms
step:1529/1645 train_time:141851ms step_avg:92.77ms
step:1530/1645 train_time:141944ms step_avg:92.77ms
step:1531/1645 train_time:142037ms step_avg:92.77ms
step:1532/1645 train_time:142130ms step_avg:92.77ms
step:1533/1645 train_time:142224ms step_avg:92.78ms
step:1534/1645 train_time:142319ms step_avg:92.78ms
step:1535/1645 train_time:142414ms step_avg:92.78ms
step:1536/1645 train_time:142508ms step_avg:92.78ms
step:1537/1645 train_time:142601ms step_avg:92.78ms
step:1538/1645 train_time:142695ms step_avg:92.78ms
step:1539/1645 train_time:142789ms step_avg:92.78ms
step:1540/1645 train_time:142882ms step_avg:92.78ms
step:1541/1645 train_time:142975ms step_avg:92.78ms
step:1542/1645 train_time:143068ms step_avg:92.78ms
step:1543/1645 train_time:143162ms step_avg:92.78ms
step:1544/1645 train_time:143256ms step_avg:92.78ms
step:1545/1645 train_time:143350ms step_avg:92.78ms
step:1546/1645 train_time:143444ms step_avg:92.78ms
step:1547/1645 train_time:143538ms step_avg:92.78ms
step:1548/1645 train_time:143634ms step_avg:92.79ms
step:1549/1645 train_time:143727ms step_avg:92.79ms
step:1550/1645 train_time:143820ms step_avg:92.79ms
step:1551/1645 train_time:143914ms step_avg:92.79ms
step:1552/1645 train_time:144008ms step_avg:92.79ms
step:1553/1645 train_time:144101ms step_avg:92.79ms
step:1554/1645 train_time:144195ms step_avg:92.79ms
step:1555/1645 train_time:144289ms step_avg:92.79ms
step:1556/1645 train_time:144383ms step_avg:92.79ms
step:1557/1645 train_time:144476ms step_avg:92.79ms
step:1558/1645 train_time:144571ms step_avg:92.79ms
step:1559/1645 train_time:144664ms step_avg:92.79ms
step:1560/1645 train_time:144758ms step_avg:92.79ms
step:1561/1645 train_time:144851ms step_avg:92.79ms
step:1562/1645 train_time:144945ms step_avg:92.79ms
step:1563/1645 train_time:145038ms step_avg:92.79ms
step:1564/1645 train_time:145133ms step_avg:92.80ms
step:1565/1645 train_time:145227ms step_avg:92.80ms
step:1566/1645 train_time:145323ms step_avg:92.80ms
step:1567/1645 train_time:145415ms step_avg:92.80ms
step:1568/1645 train_time:145509ms step_avg:92.80ms
step:1569/1645 train_time:145602ms step_avg:92.80ms
step:1570/1645 train_time:145697ms step_avg:92.80ms
step:1571/1645 train_time:145790ms step_avg:92.80ms
step:1572/1645 train_time:145883ms step_avg:92.80ms
step:1573/1645 train_time:145976ms step_avg:92.80ms
step:1574/1645 train_time:146070ms step_avg:92.80ms
step:1575/1645 train_time:146164ms step_avg:92.80ms
step:1576/1645 train_time:146257ms step_avg:92.80ms
step:1577/1645 train_time:146351ms step_avg:92.80ms
step:1578/1645 train_time:146444ms step_avg:92.80ms
step:1579/1645 train_time:146538ms step_avg:92.80ms
step:1580/1645 train_time:146632ms step_avg:92.81ms
step:1581/1645 train_time:146726ms step_avg:92.81ms
step:1582/1645 train_time:146819ms step_avg:92.81ms
step:1583/1645 train_time:146913ms step_avg:92.81ms
step:1584/1645 train_time:147007ms step_avg:92.81ms
step:1585/1645 train_time:147100ms step_avg:92.81ms
step:1586/1645 train_time:147194ms step_avg:92.81ms
step:1587/1645 train_time:147287ms step_avg:92.81ms
step:1588/1645 train_time:147381ms step_avg:92.81ms
step:1589/1645 train_time:147474ms step_avg:92.81ms
step:1590/1645 train_time:147567ms step_avg:92.81ms
step:1591/1645 train_time:147662ms step_avg:92.81ms
step:1592/1645 train_time:147756ms step_avg:92.81ms
step:1593/1645 train_time:147849ms step_avg:92.81ms
step:1594/1645 train_time:147942ms step_avg:92.81ms
step:1595/1645 train_time:148037ms step_avg:92.81ms
step:1596/1645 train_time:148132ms step_avg:92.81ms
step:1597/1645 train_time:148225ms step_avg:92.81ms
step:1598/1645 train_time:148319ms step_avg:92.82ms
step:1599/1645 train_time:148412ms step_avg:92.82ms
step:1600/1645 train_time:148506ms step_avg:92.82ms
step:1601/1645 train_time:148599ms step_avg:92.82ms
step:1602/1645 train_time:148693ms step_avg:92.82ms
step:1603/1645 train_time:148787ms step_avg:92.82ms
step:1604/1645 train_time:148881ms step_avg:92.82ms
step:1605/1645 train_time:148974ms step_avg:92.82ms
step:1606/1645 train_time:149068ms step_avg:92.82ms
step:1607/1645 train_time:149161ms step_avg:92.82ms
step:1608/1645 train_time:149255ms step_avg:92.82ms
step:1609/1645 train_time:149348ms step_avg:92.82ms
step:1610/1645 train_time:149442ms step_avg:92.82ms
step:1611/1645 train_time:149535ms step_avg:92.82ms
step:1612/1645 train_time:149630ms step_avg:92.82ms
step:1613/1645 train_time:149724ms step_avg:92.82ms
step:1614/1645 train_time:149818ms step_avg:92.82ms
step:1615/1645 train_time:149911ms step_avg:92.82ms
step:1616/1645 train_time:150003ms step_avg:92.82ms
step:1617/1645 train_time:150097ms step_avg:92.82ms
step:1618/1645 train_time:150191ms step_avg:92.83ms
step:1619/1645 train_time:150284ms step_avg:92.83ms
step:1620/1645 train_time:150377ms step_avg:92.83ms
step:1621/1645 train_time:150472ms step_avg:92.83ms
step:1622/1645 train_time:150565ms step_avg:92.83ms
step:1623/1645 train_time:150659ms step_avg:92.83ms
step:1624/1645 train_time:150754ms step_avg:92.83ms
step:1625/1645 train_time:150848ms step_avg:92.83ms
step:1625/1645 val_loss:3.2840 train_time:150940ms step_avg:92.89ms
step:1626/1645 train_time:150962ms step_avg:92.84ms
step:1627/1645 train_time:151039ms step_avg:92.83ms
step:1628/1645 train_time:151135ms step_avg:92.83ms
step:1629/1645 train_time:151228ms step_avg:92.84ms
step:1630/1645 train_time:151321ms step_avg:92.84ms
step:1631/1645 train_time:151414ms step_avg:92.84ms
step:1632/1645 train_time:151507ms step_avg:92.83ms
step:1633/1645 train_time:151599ms step_avg:92.83ms
step:1634/1645 train_time:151691ms step_avg:92.83ms
step:1635/1645 train_time:151785ms step_avg:92.83ms
step:1636/1645 train_time:151878ms step_avg:92.84ms
step:1637/1645 train_time:151974ms step_avg:92.84ms
step:1638/1645 train_time:152070ms step_avg:92.84ms
step:1639/1645 train_time:152165ms step_avg:92.84ms
step:1640/1645 train_time:152258ms step_avg:92.84ms
step:1641/1645 train_time:152351ms step_avg:92.84ms
step:1642/1645 train_time:152444ms step_avg:92.84ms
step:1643/1645 train_time:152536ms step_avg:92.84ms
step:1644/1645 train_time:152629ms step_avg:92.84ms
step:1645/1645 train_time:152722ms step_avg:92.84ms
step:1645/1645 val_loss:3.2787 train_time:152815ms step_avg:92.90ms
peak memory allocated: 31497 MiB reserved: 47034 MiB

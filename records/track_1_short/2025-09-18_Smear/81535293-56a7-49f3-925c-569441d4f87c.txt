import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:20:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:128ms step_avg:128.03ms
step:2/1645 train_time:145ms step_avg:72.74ms
step:3/1645 train_time:219ms step_avg:72.96ms
step:4/1645 train_time:308ms step_avg:77.09ms
step:5/1645 train_time:399ms step_avg:79.82ms
step:6/1645 train_time:490ms step_avg:81.65ms
step:7/1645 train_time:581ms step_avg:82.97ms
step:8/1645 train_time:671ms step_avg:83.92ms
step:9/1645 train_time:762ms step_avg:84.67ms
step:10/1645 train_time:853ms step_avg:85.31ms
step:11/1645 train_time:944ms step_avg:85.81ms
step:12/1645 train_time:1038ms step_avg:86.52ms
step:13/1645 train_time:1136ms step_avg:87.40ms
step:14/1645 train_time:1232ms step_avg:88.02ms
step:15/1645 train_time:1324ms step_avg:88.28ms
step:16/1645 train_time:1415ms step_avg:88.46ms
step:17/1645 train_time:1508ms step_avg:88.68ms
step:18/1645 train_time:1599ms step_avg:88.81ms
step:19/1645 train_time:1691ms step_avg:89.00ms
step:20/1645 train_time:1782ms step_avg:89.11ms
step:21/1645 train_time:1873ms step_avg:89.19ms
step:22/1645 train_time:1965ms step_avg:89.33ms
step:23/1645 train_time:2058ms step_avg:89.47ms
step:24/1645 train_time:2154ms step_avg:89.75ms
step:25/1645 train_time:2247ms step_avg:89.89ms
step:26/1645 train_time:2339ms step_avg:89.98ms
step:27/1645 train_time:2431ms step_avg:90.04ms
step:28/1645 train_time:2523ms step_avg:90.10ms
step:29/1645 train_time:2614ms step_avg:90.13ms
step:30/1645 train_time:2706ms step_avg:90.19ms
step:31/1645 train_time:2797ms step_avg:90.23ms
step:32/1645 train_time:2889ms step_avg:90.27ms
step:33/1645 train_time:2980ms step_avg:90.30ms
step:34/1645 train_time:3074ms step_avg:90.41ms
step:35/1645 train_time:3168ms step_avg:90.52ms
step:36/1645 train_time:3261ms step_avg:90.57ms
step:37/1645 train_time:3353ms step_avg:90.62ms
step:38/1645 train_time:3446ms step_avg:90.68ms
step:39/1645 train_time:3537ms step_avg:90.69ms
step:40/1645 train_time:3629ms step_avg:90.72ms
step:41/1645 train_time:3721ms step_avg:90.74ms
step:42/1645 train_time:3812ms step_avg:90.75ms
step:43/1645 train_time:3903ms step_avg:90.76ms
step:44/1645 train_time:3995ms step_avg:90.79ms
step:45/1645 train_time:4087ms step_avg:90.82ms
step:46/1645 train_time:4179ms step_avg:90.85ms
step:47/1645 train_time:4272ms step_avg:90.90ms
step:48/1645 train_time:4365ms step_avg:90.93ms
step:49/1645 train_time:4456ms step_avg:90.95ms
step:50/1645 train_time:4548ms step_avg:90.96ms
step:51/1645 train_time:4640ms step_avg:90.98ms
step:52/1645 train_time:4732ms step_avg:91.00ms
step:53/1645 train_time:4823ms step_avg:91.01ms
step:54/1645 train_time:4915ms step_avg:91.01ms
step:55/1645 train_time:5006ms step_avg:91.02ms
step:56/1645 train_time:5098ms step_avg:91.04ms
step:57/1645 train_time:5190ms step_avg:91.06ms
step:58/1645 train_time:5283ms step_avg:91.08ms
step:59/1645 train_time:5375ms step_avg:91.10ms
step:60/1645 train_time:5468ms step_avg:91.14ms
step:61/1645 train_time:5560ms step_avg:91.15ms
step:62/1645 train_time:5652ms step_avg:91.16ms
step:63/1645 train_time:5744ms step_avg:91.17ms
step:64/1645 train_time:5835ms step_avg:91.18ms
step:65/1645 train_time:5927ms step_avg:91.19ms
step:66/1645 train_time:6018ms step_avg:91.19ms
step:67/1645 train_time:6110ms step_avg:91.20ms
step:68/1645 train_time:6202ms step_avg:91.21ms
step:69/1645 train_time:6295ms step_avg:91.23ms
step:70/1645 train_time:6387ms step_avg:91.24ms
step:71/1645 train_time:6478ms step_avg:91.24ms
step:72/1645 train_time:6572ms step_avg:91.28ms
step:73/1645 train_time:6664ms step_avg:91.29ms
step:74/1645 train_time:6756ms step_avg:91.30ms
step:75/1645 train_time:6849ms step_avg:91.32ms
step:76/1645 train_time:6941ms step_avg:91.33ms
step:77/1645 train_time:7033ms step_avg:91.33ms
step:78/1645 train_time:7124ms step_avg:91.33ms
step:79/1645 train_time:7216ms step_avg:91.34ms
step:80/1645 train_time:7307ms step_avg:91.34ms
step:81/1645 train_time:7398ms step_avg:91.33ms
step:82/1645 train_time:7490ms step_avg:91.35ms
step:83/1645 train_time:7582ms step_avg:91.35ms
step:84/1645 train_time:7674ms step_avg:91.35ms
step:85/1645 train_time:7766ms step_avg:91.36ms
step:86/1645 train_time:7858ms step_avg:91.37ms
step:87/1645 train_time:7950ms step_avg:91.38ms
step:88/1645 train_time:8042ms step_avg:91.39ms
step:89/1645 train_time:8134ms step_avg:91.39ms
step:90/1645 train_time:8225ms step_avg:91.39ms
step:91/1645 train_time:8316ms step_avg:91.39ms
step:92/1645 train_time:8408ms step_avg:91.39ms
step:93/1645 train_time:8499ms step_avg:91.39ms
step:94/1645 train_time:8591ms step_avg:91.40ms
step:95/1645 train_time:8683ms step_avg:91.40ms
step:96/1645 train_time:8775ms step_avg:91.40ms
step:97/1645 train_time:8867ms step_avg:91.41ms
step:98/1645 train_time:8959ms step_avg:91.42ms
step:99/1645 train_time:9052ms step_avg:91.43ms
step:100/1645 train_time:9144ms step_avg:91.44ms
step:101/1645 train_time:9235ms step_avg:91.43ms
step:102/1645 train_time:9326ms step_avg:91.43ms
step:103/1645 train_time:9418ms step_avg:91.44ms
step:104/1645 train_time:9510ms step_avg:91.44ms
step:105/1645 train_time:9602ms step_avg:91.44ms
step:106/1645 train_time:9693ms step_avg:91.45ms
step:107/1645 train_time:9786ms step_avg:91.46ms
step:108/1645 train_time:9877ms step_avg:91.45ms
step:109/1645 train_time:9969ms step_avg:91.46ms
step:110/1645 train_time:10061ms step_avg:91.46ms
step:111/1645 train_time:10153ms step_avg:91.47ms
step:112/1645 train_time:10245ms step_avg:91.47ms
step:113/1645 train_time:10336ms step_avg:91.47ms
step:114/1645 train_time:10427ms step_avg:91.47ms
step:115/1645 train_time:10519ms step_avg:91.47ms
step:116/1645 train_time:10611ms step_avg:91.47ms
step:117/1645 train_time:10702ms step_avg:91.47ms
step:118/1645 train_time:10794ms step_avg:91.47ms
step:119/1645 train_time:10886ms step_avg:91.48ms
step:120/1645 train_time:10977ms step_avg:91.47ms
step:121/1645 train_time:11069ms step_avg:91.48ms
step:122/1645 train_time:11161ms step_avg:91.49ms
step:123/1645 train_time:11253ms step_avg:91.49ms
step:124/1645 train_time:11345ms step_avg:91.49ms
step:125/1645 train_time:11437ms step_avg:91.49ms
step:125/1645 val_loss:4.3196 train_time:11529ms step_avg:92.23ms
step:126/1645 train_time:11552ms step_avg:91.68ms
step:127/1645 train_time:11626ms step_avg:91.54ms
step:128/1645 train_time:11727ms step_avg:91.62ms
step:129/1645 train_time:11821ms step_avg:91.64ms
step:130/1645 train_time:11913ms step_avg:91.64ms
step:131/1645 train_time:12004ms step_avg:91.63ms
step:132/1645 train_time:12095ms step_avg:91.63ms
step:133/1645 train_time:12185ms step_avg:91.62ms
step:134/1645 train_time:12276ms step_avg:91.61ms
step:135/1645 train_time:12367ms step_avg:91.60ms
step:136/1645 train_time:12457ms step_avg:91.59ms
step:137/1645 train_time:12549ms step_avg:91.60ms
step:138/1645 train_time:12643ms step_avg:91.62ms
step:139/1645 train_time:12736ms step_avg:91.63ms
step:140/1645 train_time:12829ms step_avg:91.64ms
step:141/1645 train_time:12920ms step_avg:91.63ms
step:142/1645 train_time:13012ms step_avg:91.63ms
step:143/1645 train_time:13103ms step_avg:91.63ms
step:144/1645 train_time:13195ms step_avg:91.63ms
step:145/1645 train_time:13286ms step_avg:91.63ms
step:146/1645 train_time:13377ms step_avg:91.62ms
step:147/1645 train_time:13467ms step_avg:91.62ms
step:148/1645 train_time:13560ms step_avg:91.62ms
step:149/1645 train_time:13653ms step_avg:91.63ms
step:150/1645 train_time:13747ms step_avg:91.64ms
step:151/1645 train_time:13838ms step_avg:91.64ms
step:152/1645 train_time:13930ms step_avg:91.65ms
step:153/1645 train_time:14021ms step_avg:91.64ms
step:154/1645 train_time:14114ms step_avg:91.65ms
step:155/1645 train_time:14206ms step_avg:91.65ms
step:156/1645 train_time:14297ms step_avg:91.65ms
step:157/1645 train_time:14388ms step_avg:91.64ms
step:158/1645 train_time:14479ms step_avg:91.64ms
step:159/1645 train_time:14570ms step_avg:91.64ms
step:160/1645 train_time:14663ms step_avg:91.64ms
step:161/1645 train_time:14756ms step_avg:91.65ms
step:162/1645 train_time:14847ms step_avg:91.65ms
step:163/1645 train_time:14939ms step_avg:91.65ms
step:164/1645 train_time:15031ms step_avg:91.65ms
step:165/1645 train_time:15122ms step_avg:91.65ms
step:166/1645 train_time:15215ms step_avg:91.66ms
step:167/1645 train_time:15307ms step_avg:91.66ms
step:168/1645 train_time:15398ms step_avg:91.65ms
step:169/1645 train_time:15489ms step_avg:91.65ms
step:170/1645 train_time:15580ms step_avg:91.65ms
step:171/1645 train_time:15673ms step_avg:91.65ms
step:172/1645 train_time:15765ms step_avg:91.66ms
step:173/1645 train_time:15857ms step_avg:91.66ms
step:174/1645 train_time:15948ms step_avg:91.65ms
step:175/1645 train_time:16039ms step_avg:91.65ms
step:176/1645 train_time:16131ms step_avg:91.65ms
step:177/1645 train_time:16222ms step_avg:91.65ms
step:178/1645 train_time:16315ms step_avg:91.66ms
step:179/1645 train_time:16408ms step_avg:91.66ms
step:180/1645 train_time:16499ms step_avg:91.66ms
step:181/1645 train_time:16591ms step_avg:91.66ms
step:182/1645 train_time:16683ms step_avg:91.66ms
step:183/1645 train_time:16774ms step_avg:91.66ms
step:184/1645 train_time:16865ms step_avg:91.66ms
step:185/1645 train_time:16956ms step_avg:91.66ms
step:186/1645 train_time:17048ms step_avg:91.66ms
step:187/1645 train_time:17140ms step_avg:91.66ms
step:188/1645 train_time:17231ms step_avg:91.65ms
step:189/1645 train_time:17322ms step_avg:91.65ms
step:190/1645 train_time:17414ms step_avg:91.65ms
step:191/1645 train_time:17506ms step_avg:91.65ms
step:192/1645 train_time:17598ms step_avg:91.65ms
step:193/1645 train_time:17689ms step_avg:91.65ms
step:194/1645 train_time:17781ms step_avg:91.65ms
step:195/1645 train_time:17873ms step_avg:91.65ms
step:196/1645 train_time:17964ms step_avg:91.65ms
step:197/1645 train_time:18056ms step_avg:91.65ms
step:198/1645 train_time:18147ms step_avg:91.65ms
step:199/1645 train_time:18239ms step_avg:91.65ms
step:200/1645 train_time:18331ms step_avg:91.65ms
step:201/1645 train_time:18422ms step_avg:91.65ms
step:202/1645 train_time:18514ms step_avg:91.65ms
step:203/1645 train_time:18606ms step_avg:91.65ms
step:204/1645 train_time:18697ms step_avg:91.65ms
step:205/1645 train_time:18790ms step_avg:91.66ms
step:206/1645 train_time:18881ms step_avg:91.65ms
step:207/1645 train_time:18974ms step_avg:91.66ms
step:208/1645 train_time:19064ms step_avg:91.65ms
step:209/1645 train_time:19156ms step_avg:91.65ms
step:210/1645 train_time:19248ms step_avg:91.66ms
step:211/1645 train_time:19339ms step_avg:91.66ms
step:212/1645 train_time:19432ms step_avg:91.66ms
step:213/1645 train_time:19523ms step_avg:91.66ms
step:214/1645 train_time:19616ms step_avg:91.66ms
step:215/1645 train_time:19708ms step_avg:91.67ms
step:216/1645 train_time:19800ms step_avg:91.67ms
step:217/1645 train_time:19893ms step_avg:91.67ms
step:218/1645 train_time:19984ms step_avg:91.67ms
step:219/1645 train_time:20077ms step_avg:91.67ms
step:220/1645 train_time:20168ms step_avg:91.67ms
step:221/1645 train_time:20259ms step_avg:91.67ms
step:222/1645 train_time:20350ms step_avg:91.67ms
step:223/1645 train_time:20442ms step_avg:91.67ms
step:224/1645 train_time:20534ms step_avg:91.67ms
step:225/1645 train_time:20626ms step_avg:91.67ms
step:226/1645 train_time:20718ms step_avg:91.67ms
step:227/1645 train_time:20811ms step_avg:91.68ms
step:228/1645 train_time:20903ms step_avg:91.68ms
step:229/1645 train_time:20994ms step_avg:91.68ms
step:230/1645 train_time:21086ms step_avg:91.68ms
step:231/1645 train_time:21177ms step_avg:91.68ms
step:232/1645 train_time:21269ms step_avg:91.68ms
step:233/1645 train_time:21360ms step_avg:91.67ms
step:234/1645 train_time:21453ms step_avg:91.68ms
step:235/1645 train_time:21543ms step_avg:91.67ms
step:236/1645 train_time:21635ms step_avg:91.67ms
step:237/1645 train_time:21727ms step_avg:91.67ms
step:238/1645 train_time:21818ms step_avg:91.67ms
step:239/1645 train_time:21911ms step_avg:91.68ms
step:240/1645 train_time:22002ms step_avg:91.67ms
step:241/1645 train_time:22094ms step_avg:91.68ms
step:242/1645 train_time:22185ms step_avg:91.67ms
step:243/1645 train_time:22277ms step_avg:91.67ms
step:244/1645 train_time:22368ms step_avg:91.67ms
step:245/1645 train_time:22460ms step_avg:91.67ms
step:246/1645 train_time:22551ms step_avg:91.67ms
step:247/1645 train_time:22642ms step_avg:91.67ms
step:248/1645 train_time:22735ms step_avg:91.67ms
step:249/1645 train_time:22827ms step_avg:91.67ms
step:250/1645 train_time:22919ms step_avg:91.68ms
step:250/1645 val_loss:3.9665 train_time:23011ms step_avg:92.04ms
step:251/1645 train_time:23032ms step_avg:91.76ms
step:252/1645 train_time:23108ms step_avg:91.70ms
step:253/1645 train_time:23200ms step_avg:91.70ms
step:254/1645 train_time:23292ms step_avg:91.70ms
step:255/1645 train_time:23382ms step_avg:91.70ms
step:256/1645 train_time:23474ms step_avg:91.70ms
step:257/1645 train_time:23565ms step_avg:91.69ms
step:258/1645 train_time:23656ms step_avg:91.69ms
step:259/1645 train_time:23748ms step_avg:91.69ms
step:260/1645 train_time:23839ms step_avg:91.69ms
step:261/1645 train_time:23932ms step_avg:91.69ms
step:262/1645 train_time:24025ms step_avg:91.70ms
step:263/1645 train_time:24118ms step_avg:91.70ms
step:264/1645 train_time:24210ms step_avg:91.70ms
step:265/1645 train_time:24301ms step_avg:91.70ms
step:266/1645 train_time:24392ms step_avg:91.70ms
step:267/1645 train_time:24483ms step_avg:91.70ms
step:268/1645 train_time:24574ms step_avg:91.70ms
step:269/1645 train_time:24665ms step_avg:91.69ms
step:270/1645 train_time:24756ms step_avg:91.69ms
step:271/1645 train_time:24848ms step_avg:91.69ms
step:272/1645 train_time:24940ms step_avg:91.69ms
step:273/1645 train_time:25032ms step_avg:91.69ms
step:274/1645 train_time:25125ms step_avg:91.70ms
step:275/1645 train_time:25217ms step_avg:91.70ms
step:276/1645 train_time:25308ms step_avg:91.70ms
step:277/1645 train_time:25400ms step_avg:91.69ms
step:278/1645 train_time:25490ms step_avg:91.69ms
step:279/1645 train_time:25582ms step_avg:91.69ms
step:280/1645 train_time:25674ms step_avg:91.69ms
step:281/1645 train_time:25765ms step_avg:91.69ms
step:282/1645 train_time:25856ms step_avg:91.69ms
step:283/1645 train_time:25949ms step_avg:91.69ms
step:284/1645 train_time:26042ms step_avg:91.70ms
step:285/1645 train_time:26134ms step_avg:91.70ms
step:286/1645 train_time:26226ms step_avg:91.70ms
step:287/1645 train_time:26318ms step_avg:91.70ms
step:288/1645 train_time:26409ms step_avg:91.70ms
step:289/1645 train_time:26500ms step_avg:91.70ms
step:290/1645 train_time:26592ms step_avg:91.70ms
step:291/1645 train_time:26683ms step_avg:91.70ms
step:292/1645 train_time:26775ms step_avg:91.69ms
step:293/1645 train_time:26866ms step_avg:91.69ms
step:294/1645 train_time:26957ms step_avg:91.69ms
step:295/1645 train_time:27049ms step_avg:91.69ms
step:296/1645 train_time:27142ms step_avg:91.70ms
step:297/1645 train_time:27233ms step_avg:91.69ms
step:298/1645 train_time:27325ms step_avg:91.69ms
step:299/1645 train_time:27416ms step_avg:91.69ms
step:300/1645 train_time:27507ms step_avg:91.69ms
step:301/1645 train_time:27598ms step_avg:91.69ms
step:302/1645 train_time:27690ms step_avg:91.69ms
step:303/1645 train_time:27782ms step_avg:91.69ms
step:304/1645 train_time:27874ms step_avg:91.69ms
step:305/1645 train_time:27967ms step_avg:91.70ms
step:306/1645 train_time:28058ms step_avg:91.69ms
step:307/1645 train_time:28150ms step_avg:91.69ms
step:308/1645 train_time:28242ms step_avg:91.70ms
step:309/1645 train_time:28334ms step_avg:91.70ms
step:310/1645 train_time:28425ms step_avg:91.69ms
step:311/1645 train_time:28516ms step_avg:91.69ms
step:312/1645 train_time:28608ms step_avg:91.69ms
step:313/1645 train_time:28699ms step_avg:91.69ms
step:314/1645 train_time:28791ms step_avg:91.69ms
step:315/1645 train_time:28883ms step_avg:91.69ms
step:316/1645 train_time:28975ms step_avg:91.69ms
step:317/1645 train_time:29068ms step_avg:91.70ms
step:318/1645 train_time:29161ms step_avg:91.70ms
step:319/1645 train_time:29253ms step_avg:91.70ms
step:320/1645 train_time:29344ms step_avg:91.70ms
step:321/1645 train_time:29435ms step_avg:91.70ms
step:322/1645 train_time:29526ms step_avg:91.70ms
step:323/1645 train_time:29618ms step_avg:91.70ms
step:324/1645 train_time:29709ms step_avg:91.69ms
step:325/1645 train_time:29800ms step_avg:91.69ms
step:326/1645 train_time:29892ms step_avg:91.69ms
step:327/1645 train_time:29984ms step_avg:91.69ms
step:328/1645 train_time:30075ms step_avg:91.69ms
step:329/1645 train_time:30168ms step_avg:91.70ms
step:330/1645 train_time:30261ms step_avg:91.70ms
step:331/1645 train_time:30353ms step_avg:91.70ms
step:332/1645 train_time:30445ms step_avg:91.70ms
step:333/1645 train_time:30536ms step_avg:91.70ms
step:334/1645 train_time:30628ms step_avg:91.70ms
step:335/1645 train_time:30719ms step_avg:91.70ms
step:336/1645 train_time:30811ms step_avg:91.70ms
step:337/1645 train_time:30901ms step_avg:91.70ms
step:338/1645 train_time:30993ms step_avg:91.70ms
step:339/1645 train_time:31085ms step_avg:91.70ms
step:340/1645 train_time:31177ms step_avg:91.70ms
step:341/1645 train_time:31271ms step_avg:91.70ms
step:342/1645 train_time:31362ms step_avg:91.70ms
step:343/1645 train_time:31454ms step_avg:91.70ms
step:344/1645 train_time:31546ms step_avg:91.70ms
step:345/1645 train_time:31638ms step_avg:91.70ms
step:346/1645 train_time:31729ms step_avg:91.70ms
step:347/1645 train_time:31821ms step_avg:91.70ms
step:348/1645 train_time:31911ms step_avg:91.70ms
step:349/1645 train_time:32003ms step_avg:91.70ms
step:350/1645 train_time:32095ms step_avg:91.70ms
step:351/1645 train_time:32188ms step_avg:91.70ms
step:352/1645 train_time:32279ms step_avg:91.70ms
step:353/1645 train_time:32372ms step_avg:91.70ms
step:354/1645 train_time:32464ms step_avg:91.71ms
step:355/1645 train_time:32555ms step_avg:91.71ms
step:356/1645 train_time:32647ms step_avg:91.71ms
step:357/1645 train_time:32738ms step_avg:91.70ms
step:358/1645 train_time:32830ms step_avg:91.70ms
step:359/1645 train_time:32921ms step_avg:91.70ms
step:360/1645 train_time:33012ms step_avg:91.70ms
step:361/1645 train_time:33104ms step_avg:91.70ms
step:362/1645 train_time:33195ms step_avg:91.70ms
step:363/1645 train_time:33287ms step_avg:91.70ms
step:364/1645 train_time:33379ms step_avg:91.70ms
step:365/1645 train_time:33472ms step_avg:91.70ms
step:366/1645 train_time:33563ms step_avg:91.70ms
step:367/1645 train_time:33655ms step_avg:91.70ms
step:368/1645 train_time:33747ms step_avg:91.70ms
step:369/1645 train_time:33839ms step_avg:91.71ms
step:370/1645 train_time:33931ms step_avg:91.70ms
step:371/1645 train_time:34022ms step_avg:91.70ms
step:372/1645 train_time:34113ms step_avg:91.70ms
step:373/1645 train_time:34205ms step_avg:91.70ms
step:374/1645 train_time:34297ms step_avg:91.70ms
step:375/1645 train_time:34389ms step_avg:91.70ms
step:375/1645 val_loss:3.8146 train_time:34481ms step_avg:91.95ms
step:376/1645 train_time:34500ms step_avg:91.75ms
step:377/1645 train_time:34579ms step_avg:91.72ms
step:378/1645 train_time:34672ms step_avg:91.72ms
step:379/1645 train_time:34763ms step_avg:91.72ms
step:380/1645 train_time:34853ms step_avg:91.72ms
step:381/1645 train_time:34944ms step_avg:91.72ms
step:382/1645 train_time:35034ms step_avg:91.71ms
step:383/1645 train_time:35126ms step_avg:91.71ms
step:384/1645 train_time:35217ms step_avg:91.71ms
step:385/1645 train_time:35308ms step_avg:91.71ms
step:386/1645 train_time:35400ms step_avg:91.71ms
step:387/1645 train_time:35494ms step_avg:91.72ms
step:388/1645 train_time:35587ms step_avg:91.72ms
step:389/1645 train_time:35679ms step_avg:91.72ms
step:390/1645 train_time:35771ms step_avg:91.72ms
step:391/1645 train_time:35862ms step_avg:91.72ms
step:392/1645 train_time:35954ms step_avg:91.72ms
step:393/1645 train_time:36044ms step_avg:91.72ms
step:394/1645 train_time:36135ms step_avg:91.71ms
step:395/1645 train_time:36226ms step_avg:91.71ms
step:396/1645 train_time:36318ms step_avg:91.71ms
step:397/1645 train_time:36411ms step_avg:91.72ms
step:398/1645 train_time:36504ms step_avg:91.72ms
step:399/1645 train_time:36597ms step_avg:91.72ms
step:400/1645 train_time:36690ms step_avg:91.73ms
step:401/1645 train_time:36781ms step_avg:91.72ms
step:402/1645 train_time:36872ms step_avg:91.72ms
step:403/1645 train_time:36964ms step_avg:91.72ms
step:404/1645 train_time:37054ms step_avg:91.72ms
step:405/1645 train_time:37145ms step_avg:91.72ms
step:406/1645 train_time:37236ms step_avg:91.71ms
step:407/1645 train_time:37327ms step_avg:91.71ms
step:408/1645 train_time:37419ms step_avg:91.71ms
step:409/1645 train_time:37512ms step_avg:91.72ms
step:410/1645 train_time:37605ms step_avg:91.72ms
step:411/1645 train_time:37697ms step_avg:91.72ms
step:412/1645 train_time:37790ms step_avg:91.72ms
step:413/1645 train_time:37881ms step_avg:91.72ms
step:414/1645 train_time:37973ms step_avg:91.72ms
step:415/1645 train_time:38064ms step_avg:91.72ms
step:416/1645 train_time:38154ms step_avg:91.72ms
step:417/1645 train_time:38246ms step_avg:91.72ms
step:418/1645 train_time:38338ms step_avg:91.72ms
step:419/1645 train_time:38430ms step_avg:91.72ms
step:420/1645 train_time:38524ms step_avg:91.72ms
step:421/1645 train_time:38615ms step_avg:91.72ms
step:422/1645 train_time:38708ms step_avg:91.73ms
step:423/1645 train_time:38800ms step_avg:91.73ms
step:424/1645 train_time:38891ms step_avg:91.72ms
step:425/1645 train_time:38983ms step_avg:91.72ms
step:426/1645 train_time:39075ms step_avg:91.72ms
step:427/1645 train_time:39165ms step_avg:91.72ms
step:428/1645 train_time:39256ms step_avg:91.72ms
step:429/1645 train_time:39347ms step_avg:91.72ms
step:430/1645 train_time:39439ms step_avg:91.72ms
step:431/1645 train_time:39531ms step_avg:91.72ms
step:432/1645 train_time:39623ms step_avg:91.72ms
step:433/1645 train_time:39714ms step_avg:91.72ms
step:434/1645 train_time:39807ms step_avg:91.72ms
step:435/1645 train_time:39899ms step_avg:91.72ms
step:436/1645 train_time:39990ms step_avg:91.72ms
step:437/1645 train_time:40081ms step_avg:91.72ms
step:438/1645 train_time:40172ms step_avg:91.72ms
step:439/1645 train_time:40263ms step_avg:91.72ms
step:440/1645 train_time:40355ms step_avg:91.72ms
step:441/1645 train_time:40446ms step_avg:91.71ms
step:442/1645 train_time:40538ms step_avg:91.72ms
step:443/1645 train_time:40630ms step_avg:91.72ms
step:444/1645 train_time:40722ms step_avg:91.72ms
step:445/1645 train_time:40815ms step_avg:91.72ms
step:446/1645 train_time:40907ms step_avg:91.72ms
step:447/1645 train_time:40999ms step_avg:91.72ms
step:448/1645 train_time:41090ms step_avg:91.72ms
step:449/1645 train_time:41181ms step_avg:91.72ms
step:450/1645 train_time:41272ms step_avg:91.72ms
step:451/1645 train_time:41363ms step_avg:91.71ms
step:452/1645 train_time:41455ms step_avg:91.71ms
step:453/1645 train_time:41547ms step_avg:91.71ms
step:454/1645 train_time:41639ms step_avg:91.71ms
step:455/1645 train_time:41731ms step_avg:91.72ms
step:456/1645 train_time:41824ms step_avg:91.72ms
step:457/1645 train_time:41916ms step_avg:91.72ms
step:458/1645 train_time:42008ms step_avg:91.72ms
step:459/1645 train_time:42099ms step_avg:91.72ms
step:460/1645 train_time:42191ms step_avg:91.72ms
step:461/1645 train_time:42282ms step_avg:91.72ms
step:462/1645 train_time:42373ms step_avg:91.72ms
step:463/1645 train_time:42464ms step_avg:91.71ms
step:464/1645 train_time:42555ms step_avg:91.71ms
step:465/1645 train_time:42647ms step_avg:91.71ms
step:466/1645 train_time:42739ms step_avg:91.71ms
step:467/1645 train_time:42831ms step_avg:91.71ms
step:468/1645 train_time:42923ms step_avg:91.72ms
step:469/1645 train_time:43015ms step_avg:91.72ms
step:470/1645 train_time:43107ms step_avg:91.72ms
step:471/1645 train_time:43198ms step_avg:91.72ms
step:472/1645 train_time:43289ms step_avg:91.71ms
step:473/1645 train_time:43381ms step_avg:91.71ms
step:474/1645 train_time:43472ms step_avg:91.71ms
step:475/1645 train_time:43564ms step_avg:91.71ms
step:476/1645 train_time:43655ms step_avg:91.71ms
step:477/1645 train_time:43747ms step_avg:91.71ms
step:478/1645 train_time:43839ms step_avg:91.71ms
step:479/1645 train_time:43931ms step_avg:91.71ms
step:480/1645 train_time:44023ms step_avg:91.71ms
step:481/1645 train_time:44115ms step_avg:91.72ms
step:482/1645 train_time:44207ms step_avg:91.72ms
step:483/1645 train_time:44299ms step_avg:91.72ms
step:484/1645 train_time:44390ms step_avg:91.71ms
step:485/1645 train_time:44481ms step_avg:91.71ms
step:486/1645 train_time:44573ms step_avg:91.71ms
step:487/1645 train_time:44664ms step_avg:91.71ms
step:488/1645 train_time:44755ms step_avg:91.71ms
step:489/1645 train_time:44847ms step_avg:91.71ms
step:490/1645 train_time:44939ms step_avg:91.71ms
step:491/1645 train_time:45030ms step_avg:91.71ms
step:492/1645 train_time:45123ms step_avg:91.71ms
step:493/1645 train_time:45214ms step_avg:91.71ms
step:494/1645 train_time:45306ms step_avg:91.71ms
step:495/1645 train_time:45399ms step_avg:91.71ms
step:496/1645 train_time:45490ms step_avg:91.71ms
step:497/1645 train_time:45581ms step_avg:91.71ms
step:498/1645 train_time:45674ms step_avg:91.71ms
step:499/1645 train_time:45764ms step_avg:91.71ms
step:500/1645 train_time:45855ms step_avg:91.71ms
step:500/1645 val_loss:3.7127 train_time:45948ms step_avg:91.90ms
step:501/1645 train_time:45967ms step_avg:91.75ms
step:502/1645 train_time:46044ms step_avg:91.72ms
step:503/1645 train_time:46140ms step_avg:91.73ms
step:504/1645 train_time:46232ms step_avg:91.73ms
step:505/1645 train_time:46323ms step_avg:91.73ms
step:506/1645 train_time:46413ms step_avg:91.73ms
step:507/1645 train_time:46503ms step_avg:91.72ms
step:508/1645 train_time:46594ms step_avg:91.72ms
step:509/1645 train_time:46685ms step_avg:91.72ms
step:510/1645 train_time:46775ms step_avg:91.72ms
step:511/1645 train_time:46867ms step_avg:91.72ms
step:512/1645 train_time:46961ms step_avg:91.72ms
step:513/1645 train_time:47054ms step_avg:91.72ms
step:514/1645 train_time:47148ms step_avg:91.73ms
step:515/1645 train_time:47240ms step_avg:91.73ms
step:516/1645 train_time:47332ms step_avg:91.73ms
step:517/1645 train_time:47423ms step_avg:91.73ms
step:518/1645 train_time:47513ms step_avg:91.72ms
step:519/1645 train_time:47605ms step_avg:91.72ms
step:520/1645 train_time:47696ms step_avg:91.72ms
step:521/1645 train_time:47788ms step_avg:91.72ms
step:522/1645 train_time:47879ms step_avg:91.72ms
step:523/1645 train_time:47972ms step_avg:91.72ms
step:524/1645 train_time:48065ms step_avg:91.73ms
step:525/1645 train_time:48158ms step_avg:91.73ms
step:526/1645 train_time:48251ms step_avg:91.73ms
step:527/1645 train_time:48342ms step_avg:91.73ms
step:528/1645 train_time:48433ms step_avg:91.73ms
step:529/1645 train_time:48524ms step_avg:91.73ms
step:530/1645 train_time:48616ms step_avg:91.73ms
step:531/1645 train_time:48706ms step_avg:91.73ms
step:532/1645 train_time:48797ms step_avg:91.72ms
step:533/1645 train_time:48889ms step_avg:91.72ms
step:534/1645 train_time:48982ms step_avg:91.73ms
step:535/1645 train_time:49075ms step_avg:91.73ms
step:536/1645 train_time:49167ms step_avg:91.73ms
step:537/1645 train_time:49260ms step_avg:91.73ms
step:538/1645 train_time:49353ms step_avg:91.73ms
step:539/1645 train_time:49444ms step_avg:91.73ms
step:540/1645 train_time:49535ms step_avg:91.73ms
step:541/1645 train_time:49626ms step_avg:91.73ms
step:542/1645 train_time:49716ms step_avg:91.73ms
step:543/1645 train_time:49807ms step_avg:91.73ms
step:544/1645 train_time:49898ms step_avg:91.72ms
step:545/1645 train_time:49990ms step_avg:91.72ms
step:546/1645 train_time:50084ms step_avg:91.73ms
step:547/1645 train_time:50177ms step_avg:91.73ms
step:548/1645 train_time:50269ms step_avg:91.73ms
step:549/1645 train_time:50361ms step_avg:91.73ms
step:550/1645 train_time:50453ms step_avg:91.73ms
step:551/1645 train_time:50546ms step_avg:91.73ms
step:552/1645 train_time:50638ms step_avg:91.74ms
step:553/1645 train_time:50730ms step_avg:91.74ms
step:554/1645 train_time:50823ms step_avg:91.74ms
step:555/1645 train_time:50916ms step_avg:91.74ms
step:556/1645 train_time:51009ms step_avg:91.74ms
step:557/1645 train_time:51102ms step_avg:91.75ms
step:558/1645 train_time:51197ms step_avg:91.75ms
step:559/1645 train_time:51290ms step_avg:91.75ms
step:560/1645 train_time:51385ms step_avg:91.76ms
step:561/1645 train_time:51477ms step_avg:91.76ms
step:562/1645 train_time:51570ms step_avg:91.76ms
step:563/1645 train_time:51663ms step_avg:91.76ms
step:564/1645 train_time:51756ms step_avg:91.77ms
step:565/1645 train_time:51848ms step_avg:91.77ms
step:566/1645 train_time:51941ms step_avg:91.77ms
step:567/1645 train_time:52034ms step_avg:91.77ms
step:568/1645 train_time:52127ms step_avg:91.77ms
step:569/1645 train_time:52221ms step_avg:91.78ms
step:570/1645 train_time:52313ms step_avg:91.78ms
step:571/1645 train_time:52406ms step_avg:91.78ms
step:572/1645 train_time:52499ms step_avg:91.78ms
step:573/1645 train_time:52592ms step_avg:91.78ms
step:574/1645 train_time:52685ms step_avg:91.79ms
step:575/1645 train_time:52778ms step_avg:91.79ms
step:576/1645 train_time:52870ms step_avg:91.79ms
step:577/1645 train_time:52964ms step_avg:91.79ms
step:578/1645 train_time:53057ms step_avg:91.79ms
step:579/1645 train_time:53149ms step_avg:91.80ms
step:580/1645 train_time:53242ms step_avg:91.80ms
step:581/1645 train_time:53336ms step_avg:91.80ms
step:582/1645 train_time:53429ms step_avg:91.80ms
step:583/1645 train_time:53522ms step_avg:91.80ms
step:584/1645 train_time:53616ms step_avg:91.81ms
step:585/1645 train_time:53709ms step_avg:91.81ms
step:586/1645 train_time:53802ms step_avg:91.81ms
step:587/1645 train_time:53895ms step_avg:91.81ms
step:588/1645 train_time:53987ms step_avg:91.82ms
step:589/1645 train_time:54081ms step_avg:91.82ms
step:590/1645 train_time:54174ms step_avg:91.82ms
step:591/1645 train_time:54268ms step_avg:91.82ms
step:592/1645 train_time:54360ms step_avg:91.83ms
step:593/1645 train_time:54453ms step_avg:91.83ms
step:594/1645 train_time:54547ms step_avg:91.83ms
step:595/1645 train_time:54640ms step_avg:91.83ms
step:596/1645 train_time:54733ms step_avg:91.83ms
step:597/1645 train_time:54825ms step_avg:91.83ms
step:598/1645 train_time:54918ms step_avg:91.84ms
step:599/1645 train_time:55011ms step_avg:91.84ms
step:600/1645 train_time:55104ms step_avg:91.84ms
step:601/1645 train_time:55198ms step_avg:91.84ms
step:602/1645 train_time:55291ms step_avg:91.84ms
step:603/1645 train_time:55384ms step_avg:91.85ms
step:604/1645 train_time:55477ms step_avg:91.85ms
step:605/1645 train_time:55569ms step_avg:91.85ms
step:606/1645 train_time:55663ms step_avg:91.85ms
step:607/1645 train_time:55755ms step_avg:91.85ms
step:608/1645 train_time:55848ms step_avg:91.86ms
step:609/1645 train_time:55941ms step_avg:91.86ms
step:610/1645 train_time:56033ms step_avg:91.86ms
step:611/1645 train_time:56127ms step_avg:91.86ms
step:612/1645 train_time:56219ms step_avg:91.86ms
step:613/1645 train_time:56314ms step_avg:91.87ms
step:614/1645 train_time:56408ms step_avg:91.87ms
step:615/1645 train_time:56499ms step_avg:91.87ms
step:616/1645 train_time:56592ms step_avg:91.87ms
step:617/1645 train_time:56685ms step_avg:91.87ms
step:618/1645 train_time:56779ms step_avg:91.87ms
step:619/1645 train_time:56871ms step_avg:91.88ms
step:620/1645 train_time:56964ms step_avg:91.88ms
step:621/1645 train_time:57056ms step_avg:91.88ms
step:622/1645 train_time:57149ms step_avg:91.88ms
step:623/1645 train_time:57242ms step_avg:91.88ms
step:624/1645 train_time:57336ms step_avg:91.88ms
step:625/1645 train_time:57429ms step_avg:91.89ms
step:625/1645 val_loss:3.6109 train_time:57522ms step_avg:92.03ms
step:626/1645 train_time:57542ms step_avg:91.92ms
step:627/1645 train_time:57624ms step_avg:91.90ms
step:628/1645 train_time:57724ms step_avg:91.92ms
step:629/1645 train_time:57819ms step_avg:91.92ms
step:630/1645 train_time:57913ms step_avg:91.93ms
step:631/1645 train_time:58005ms step_avg:91.93ms
step:632/1645 train_time:58097ms step_avg:91.93ms
step:633/1645 train_time:58188ms step_avg:91.92ms
step:634/1645 train_time:58280ms step_avg:91.92ms
step:635/1645 train_time:58371ms step_avg:91.92ms
step:636/1645 train_time:58464ms step_avg:91.93ms
step:637/1645 train_time:58559ms step_avg:91.93ms
step:638/1645 train_time:58653ms step_avg:91.93ms
step:639/1645 train_time:58748ms step_avg:91.94ms
step:640/1645 train_time:58842ms step_avg:91.94ms
step:641/1645 train_time:58936ms step_avg:91.94ms
step:642/1645 train_time:59028ms step_avg:91.94ms
step:643/1645 train_time:59121ms step_avg:91.94ms
step:644/1645 train_time:59212ms step_avg:91.94ms
step:645/1645 train_time:59304ms step_avg:91.94ms
step:646/1645 train_time:59395ms step_avg:91.94ms
step:647/1645 train_time:59488ms step_avg:91.94ms
step:648/1645 train_time:59581ms step_avg:91.95ms
step:649/1645 train_time:59674ms step_avg:91.95ms
step:650/1645 train_time:59769ms step_avg:91.95ms
step:651/1645 train_time:59863ms step_avg:91.96ms
step:652/1645 train_time:59956ms step_avg:91.96ms
step:653/1645 train_time:60049ms step_avg:91.96ms
step:654/1645 train_time:60142ms step_avg:91.96ms
step:655/1645 train_time:60235ms step_avg:91.96ms
step:656/1645 train_time:60326ms step_avg:91.96ms
step:657/1645 train_time:60419ms step_avg:91.96ms
step:658/1645 train_time:60512ms step_avg:91.96ms
step:659/1645 train_time:60605ms step_avg:91.97ms
step:660/1645 train_time:60699ms step_avg:91.97ms
step:661/1645 train_time:60793ms step_avg:91.97ms
step:662/1645 train_time:60886ms step_avg:91.97ms
step:663/1645 train_time:60979ms step_avg:91.97ms
step:664/1645 train_time:61072ms step_avg:91.98ms
step:665/1645 train_time:61166ms step_avg:91.98ms
step:666/1645 train_time:61257ms step_avg:91.98ms
step:667/1645 train_time:61350ms step_avg:91.98ms
step:668/1645 train_time:61443ms step_avg:91.98ms
step:669/1645 train_time:61537ms step_avg:91.98ms
step:670/1645 train_time:61630ms step_avg:91.98ms
step:671/1645 train_time:61723ms step_avg:91.99ms
step:672/1645 train_time:61815ms step_avg:91.99ms
step:673/1645 train_time:61909ms step_avg:91.99ms
step:674/1645 train_time:62003ms step_avg:91.99ms
step:675/1645 train_time:62097ms step_avg:92.00ms
step:676/1645 train_time:62189ms step_avg:92.00ms
step:677/1645 train_time:62281ms step_avg:92.00ms
step:678/1645 train_time:62373ms step_avg:92.00ms
step:679/1645 train_time:62466ms step_avg:92.00ms
step:680/1645 train_time:62559ms step_avg:92.00ms
step:681/1645 train_time:62651ms step_avg:92.00ms
step:682/1645 train_time:62745ms step_avg:92.00ms
step:683/1645 train_time:62838ms step_avg:92.00ms
step:684/1645 train_time:62932ms step_avg:92.01ms
step:685/1645 train_time:63025ms step_avg:92.01ms
step:686/1645 train_time:63118ms step_avg:92.01ms
step:687/1645 train_time:63211ms step_avg:92.01ms
step:688/1645 train_time:63305ms step_avg:92.01ms
step:689/1645 train_time:63398ms step_avg:92.01ms
step:690/1645 train_time:63490ms step_avg:92.01ms
step:691/1645 train_time:63583ms step_avg:92.02ms
step:692/1645 train_time:63675ms step_avg:92.02ms
step:693/1645 train_time:63768ms step_avg:92.02ms
step:694/1645 train_time:63861ms step_avg:92.02ms
step:695/1645 train_time:63955ms step_avg:92.02ms
step:696/1645 train_time:64048ms step_avg:92.02ms
step:697/1645 train_time:64141ms step_avg:92.02ms
step:698/1645 train_time:64234ms step_avg:92.03ms
step:699/1645 train_time:64327ms step_avg:92.03ms
step:700/1645 train_time:64420ms step_avg:92.03ms
step:701/1645 train_time:64512ms step_avg:92.03ms
step:702/1645 train_time:64606ms step_avg:92.03ms
step:703/1645 train_time:64698ms step_avg:92.03ms
step:704/1645 train_time:64791ms step_avg:92.03ms
step:705/1645 train_time:64885ms step_avg:92.03ms
step:706/1645 train_time:64978ms step_avg:92.04ms
step:707/1645 train_time:65071ms step_avg:92.04ms
step:708/1645 train_time:65164ms step_avg:92.04ms
step:709/1645 train_time:65257ms step_avg:92.04ms
step:710/1645 train_time:65350ms step_avg:92.04ms
step:711/1645 train_time:65444ms step_avg:92.04ms
step:712/1645 train_time:65536ms step_avg:92.04ms
step:713/1645 train_time:65629ms step_avg:92.05ms
step:714/1645 train_time:65722ms step_avg:92.05ms
step:715/1645 train_time:65816ms step_avg:92.05ms
step:716/1645 train_time:65909ms step_avg:92.05ms
step:717/1645 train_time:66002ms step_avg:92.05ms
step:718/1645 train_time:66095ms step_avg:92.05ms
step:719/1645 train_time:66187ms step_avg:92.05ms
step:720/1645 train_time:66280ms step_avg:92.06ms
step:721/1645 train_time:66372ms step_avg:92.06ms
step:722/1645 train_time:66466ms step_avg:92.06ms
step:723/1645 train_time:66558ms step_avg:92.06ms
step:724/1645 train_time:66651ms step_avg:92.06ms
step:725/1645 train_time:66744ms step_avg:92.06ms
step:726/1645 train_time:66837ms step_avg:92.06ms
step:727/1645 train_time:66931ms step_avg:92.07ms
step:728/1645 train_time:67025ms step_avg:92.07ms
step:729/1645 train_time:67117ms step_avg:92.07ms
step:730/1645 train_time:67210ms step_avg:92.07ms
step:731/1645 train_time:67303ms step_avg:92.07ms
step:732/1645 train_time:67396ms step_avg:92.07ms
step:733/1645 train_time:67490ms step_avg:92.07ms
step:734/1645 train_time:67583ms step_avg:92.07ms
step:735/1645 train_time:67675ms step_avg:92.07ms
step:736/1645 train_time:67769ms step_avg:92.08ms
step:737/1645 train_time:67862ms step_avg:92.08ms
step:738/1645 train_time:67954ms step_avg:92.08ms
step:739/1645 train_time:68048ms step_avg:92.08ms
step:740/1645 train_time:68141ms step_avg:92.08ms
step:741/1645 train_time:68233ms step_avg:92.08ms
step:742/1645 train_time:68326ms step_avg:92.08ms
step:743/1645 train_time:68419ms step_avg:92.08ms
step:744/1645 train_time:68512ms step_avg:92.09ms
step:745/1645 train_time:68604ms step_avg:92.09ms
step:746/1645 train_time:68697ms step_avg:92.09ms
step:747/1645 train_time:68790ms step_avg:92.09ms
step:748/1645 train_time:68883ms step_avg:92.09ms
step:749/1645 train_time:68976ms step_avg:92.09ms
step:750/1645 train_time:69069ms step_avg:92.09ms
step:750/1645 val_loss:3.5579 train_time:69163ms step_avg:92.22ms
step:751/1645 train_time:69183ms step_avg:92.12ms
step:752/1645 train_time:69262ms step_avg:92.10ms
step:753/1645 train_time:69357ms step_avg:92.11ms
step:754/1645 train_time:69449ms step_avg:92.11ms
step:755/1645 train_time:69541ms step_avg:92.11ms
step:756/1645 train_time:69633ms step_avg:92.11ms
step:757/1645 train_time:69724ms step_avg:92.11ms
step:758/1645 train_time:69816ms step_avg:92.10ms
step:759/1645 train_time:69908ms step_avg:92.11ms
step:760/1645 train_time:70000ms step_avg:92.11ms
step:761/1645 train_time:70093ms step_avg:92.11ms
step:762/1645 train_time:70189ms step_avg:92.11ms
step:763/1645 train_time:70286ms step_avg:92.12ms
step:764/1645 train_time:70381ms step_avg:92.12ms
step:765/1645 train_time:70474ms step_avg:92.12ms
step:766/1645 train_time:70566ms step_avg:92.12ms
step:767/1645 train_time:70658ms step_avg:92.12ms
step:768/1645 train_time:70750ms step_avg:92.12ms
step:769/1645 train_time:70842ms step_avg:92.12ms
step:770/1645 train_time:70934ms step_avg:92.12ms
step:771/1645 train_time:71026ms step_avg:92.12ms
step:772/1645 train_time:71121ms step_avg:92.13ms
step:773/1645 train_time:71215ms step_avg:92.13ms
step:774/1645 train_time:71311ms step_avg:92.13ms
step:775/1645 train_time:71404ms step_avg:92.13ms
step:776/1645 train_time:71497ms step_avg:92.14ms
step:777/1645 train_time:71591ms step_avg:92.14ms
step:778/1645 train_time:71685ms step_avg:92.14ms
step:779/1645 train_time:71775ms step_avg:92.14ms
step:780/1645 train_time:71868ms step_avg:92.14ms
step:781/1645 train_time:71960ms step_avg:92.14ms
step:782/1645 train_time:72052ms step_avg:92.14ms
step:783/1645 train_time:72145ms step_avg:92.14ms
step:784/1645 train_time:72238ms step_avg:92.14ms
step:785/1645 train_time:72332ms step_avg:92.14ms
step:786/1645 train_time:72425ms step_avg:92.14ms
step:787/1645 train_time:72517ms step_avg:92.14ms
step:788/1645 train_time:72612ms step_avg:92.15ms
step:789/1645 train_time:72703ms step_avg:92.15ms
step:790/1645 train_time:72795ms step_avg:92.15ms
step:791/1645 train_time:72889ms step_avg:92.15ms
step:792/1645 train_time:72981ms step_avg:92.15ms
step:793/1645 train_time:73073ms step_avg:92.15ms
step:794/1645 train_time:73167ms step_avg:92.15ms
step:795/1645 train_time:73260ms step_avg:92.15ms
step:796/1645 train_time:73354ms step_avg:92.15ms
step:797/1645 train_time:73448ms step_avg:92.16ms
step:798/1645 train_time:73541ms step_avg:92.16ms
step:799/1645 train_time:73633ms step_avg:92.16ms
step:800/1645 train_time:73727ms step_avg:92.16ms
step:801/1645 train_time:73818ms step_avg:92.16ms
step:802/1645 train_time:73911ms step_avg:92.16ms
step:803/1645 train_time:74004ms step_avg:92.16ms
step:804/1645 train_time:74097ms step_avg:92.16ms
step:805/1645 train_time:74192ms step_avg:92.16ms
step:806/1645 train_time:74285ms step_avg:92.16ms
step:807/1645 train_time:74378ms step_avg:92.17ms
step:808/1645 train_time:74471ms step_avg:92.17ms
step:809/1645 train_time:74564ms step_avg:92.17ms
step:810/1645 train_time:74657ms step_avg:92.17ms
step:811/1645 train_time:74750ms step_avg:92.17ms
step:812/1645 train_time:74843ms step_avg:92.17ms
step:813/1645 train_time:74936ms step_avg:92.17ms
step:814/1645 train_time:75028ms step_avg:92.17ms
step:815/1645 train_time:75121ms step_avg:92.17ms
step:816/1645 train_time:75214ms step_avg:92.17ms
step:817/1645 train_time:75308ms step_avg:92.18ms
step:818/1645 train_time:75402ms step_avg:92.18ms
step:819/1645 train_time:75495ms step_avg:92.18ms
step:820/1645 train_time:75588ms step_avg:92.18ms
step:821/1645 train_time:75681ms step_avg:92.18ms
step:822/1645 train_time:75774ms step_avg:92.18ms
step:823/1645 train_time:75867ms step_avg:92.18ms
step:824/1645 train_time:75959ms step_avg:92.18ms
step:825/1645 train_time:76052ms step_avg:92.18ms
step:826/1645 train_time:76145ms step_avg:92.19ms
step:827/1645 train_time:76237ms step_avg:92.19ms
step:828/1645 train_time:76331ms step_avg:92.19ms
step:829/1645 train_time:76424ms step_avg:92.19ms
step:830/1645 train_time:76516ms step_avg:92.19ms
step:831/1645 train_time:76610ms step_avg:92.19ms
step:832/1645 train_time:76703ms step_avg:92.19ms
step:833/1645 train_time:76795ms step_avg:92.19ms
step:834/1645 train_time:76888ms step_avg:92.19ms
step:835/1645 train_time:76982ms step_avg:92.19ms
step:836/1645 train_time:77075ms step_avg:92.19ms
step:837/1645 train_time:77168ms step_avg:92.20ms
step:838/1645 train_time:77261ms step_avg:92.20ms
step:839/1645 train_time:77354ms step_avg:92.20ms
step:840/1645 train_time:77447ms step_avg:92.20ms
step:841/1645 train_time:77541ms step_avg:92.20ms
step:842/1645 train_time:77633ms step_avg:92.20ms
step:843/1645 train_time:77727ms step_avg:92.20ms
step:844/1645 train_time:77820ms step_avg:92.20ms
step:845/1645 train_time:77912ms step_avg:92.20ms
step:846/1645 train_time:78005ms step_avg:92.20ms
step:847/1645 train_time:78098ms step_avg:92.21ms
step:848/1645 train_time:78191ms step_avg:92.21ms
step:849/1645 train_time:78283ms step_avg:92.21ms
step:850/1645 train_time:78377ms step_avg:92.21ms
step:851/1645 train_time:78471ms step_avg:92.21ms
step:852/1645 train_time:78563ms step_avg:92.21ms
step:853/1645 train_time:78656ms step_avg:92.21ms
step:854/1645 train_time:78750ms step_avg:92.21ms
step:855/1645 train_time:78843ms step_avg:92.21ms
step:856/1645 train_time:78935ms step_avg:92.21ms
step:857/1645 train_time:79028ms step_avg:92.21ms
step:858/1645 train_time:79121ms step_avg:92.22ms
step:859/1645 train_time:79216ms step_avg:92.22ms
step:860/1645 train_time:79308ms step_avg:92.22ms
step:861/1645 train_time:79400ms step_avg:92.22ms
step:862/1645 train_time:79493ms step_avg:92.22ms
step:863/1645 train_time:79587ms step_avg:92.22ms
step:864/1645 train_time:79680ms step_avg:92.22ms
step:865/1645 train_time:79773ms step_avg:92.22ms
step:866/1645 train_time:79865ms step_avg:92.22ms
step:867/1645 train_time:79958ms step_avg:92.22ms
step:868/1645 train_time:80051ms step_avg:92.23ms
step:869/1645 train_time:80144ms step_avg:92.23ms
step:870/1645 train_time:80236ms step_avg:92.23ms
step:871/1645 train_time:80329ms step_avg:92.23ms
step:872/1645 train_time:80424ms step_avg:92.23ms
step:873/1645 train_time:80515ms step_avg:92.23ms
step:874/1645 train_time:80609ms step_avg:92.23ms
step:875/1645 train_time:80701ms step_avg:92.23ms
step:875/1645 val_loss:3.5128 train_time:80794ms step_avg:92.34ms
step:876/1645 train_time:80816ms step_avg:92.26ms
step:877/1645 train_time:80892ms step_avg:92.24ms
step:878/1645 train_time:80988ms step_avg:92.24ms
step:879/1645 train_time:81081ms step_avg:92.24ms
step:880/1645 train_time:81172ms step_avg:92.24ms
step:881/1645 train_time:81265ms step_avg:92.24ms
step:882/1645 train_time:81356ms step_avg:92.24ms
step:883/1645 train_time:81449ms step_avg:92.24ms
step:884/1645 train_time:81541ms step_avg:92.24ms
step:885/1645 train_time:81632ms step_avg:92.24ms
step:886/1645 train_time:81728ms step_avg:92.24ms
step:887/1645 train_time:81824ms step_avg:92.25ms
step:888/1645 train_time:81920ms step_avg:92.25ms
step:889/1645 train_time:82014ms step_avg:92.25ms
step:890/1645 train_time:82107ms step_avg:92.26ms
step:891/1645 train_time:82200ms step_avg:92.26ms
step:892/1645 train_time:82293ms step_avg:92.26ms
step:893/1645 train_time:82385ms step_avg:92.26ms
step:894/1645 train_time:82477ms step_avg:92.26ms
step:895/1645 train_time:82568ms step_avg:92.26ms
step:896/1645 train_time:82661ms step_avg:92.26ms
step:897/1645 train_time:82755ms step_avg:92.26ms
step:898/1645 train_time:82849ms step_avg:92.26ms
step:899/1645 train_time:82944ms step_avg:92.26ms
step:900/1645 train_time:83037ms step_avg:92.26ms
step:901/1645 train_time:83130ms step_avg:92.26ms
step:902/1645 train_time:83224ms step_avg:92.27ms
step:903/1645 train_time:83316ms step_avg:92.27ms
step:904/1645 train_time:83409ms step_avg:92.27ms
step:905/1645 train_time:83501ms step_avg:92.27ms
step:906/1645 train_time:83593ms step_avg:92.27ms
step:907/1645 train_time:83686ms step_avg:92.27ms
step:908/1645 train_time:83779ms step_avg:92.27ms
step:909/1645 train_time:83872ms step_avg:92.27ms
step:910/1645 train_time:83965ms step_avg:92.27ms
step:911/1645 train_time:84058ms step_avg:92.27ms
step:912/1645 train_time:84151ms step_avg:92.27ms
step:913/1645 train_time:84244ms step_avg:92.27ms
step:914/1645 train_time:84336ms step_avg:92.27ms
step:915/1645 train_time:84428ms step_avg:92.27ms
step:916/1645 train_time:84520ms step_avg:92.27ms
step:917/1645 train_time:84613ms step_avg:92.27ms
step:918/1645 train_time:84706ms step_avg:92.27ms
step:919/1645 train_time:84798ms step_avg:92.27ms
step:920/1645 train_time:84892ms step_avg:92.27ms
step:921/1645 train_time:84988ms step_avg:92.28ms
step:922/1645 train_time:85079ms step_avg:92.28ms
step:923/1645 train_time:85171ms step_avg:92.28ms
step:924/1645 train_time:85264ms step_avg:92.28ms
step:925/1645 train_time:85357ms step_avg:92.28ms
step:926/1645 train_time:85449ms step_avg:92.28ms
step:927/1645 train_time:85541ms step_avg:92.28ms
step:928/1645 train_time:85635ms step_avg:92.28ms
step:929/1645 train_time:85728ms step_avg:92.28ms
step:930/1645 train_time:85822ms step_avg:92.28ms
step:931/1645 train_time:85915ms step_avg:92.28ms
step:932/1645 train_time:86008ms step_avg:92.28ms
step:933/1645 train_time:86101ms step_avg:92.28ms
step:934/1645 train_time:86194ms step_avg:92.29ms
step:935/1645 train_time:86287ms step_avg:92.29ms
step:936/1645 train_time:86379ms step_avg:92.29ms
step:937/1645 train_time:86472ms step_avg:92.29ms
step:938/1645 train_time:86565ms step_avg:92.29ms
step:939/1645 train_time:86658ms step_avg:92.29ms
step:940/1645 train_time:86751ms step_avg:92.29ms
step:941/1645 train_time:86845ms step_avg:92.29ms
step:942/1645 train_time:86937ms step_avg:92.29ms
step:943/1645 train_time:87032ms step_avg:92.29ms
step:944/1645 train_time:87125ms step_avg:92.29ms
step:945/1645 train_time:87218ms step_avg:92.29ms
step:946/1645 train_time:87311ms step_avg:92.29ms
step:947/1645 train_time:87403ms step_avg:92.30ms
step:948/1645 train_time:87496ms step_avg:92.30ms
step:949/1645 train_time:87589ms step_avg:92.30ms
step:950/1645 train_time:87681ms step_avg:92.30ms
step:951/1645 train_time:87774ms step_avg:92.30ms
step:952/1645 train_time:87867ms step_avg:92.30ms
step:953/1645 train_time:87960ms step_avg:92.30ms
step:954/1645 train_time:88053ms step_avg:92.30ms
step:955/1645 train_time:88147ms step_avg:92.30ms
step:956/1645 train_time:88240ms step_avg:92.30ms
step:957/1645 train_time:88333ms step_avg:92.30ms
step:958/1645 train_time:88426ms step_avg:92.30ms
step:959/1645 train_time:88519ms step_avg:92.30ms
step:960/1645 train_time:88612ms step_avg:92.30ms
step:961/1645 train_time:88705ms step_avg:92.30ms
step:962/1645 train_time:88799ms step_avg:92.31ms
step:963/1645 train_time:88891ms step_avg:92.31ms
step:964/1645 train_time:88984ms step_avg:92.31ms
step:965/1645 train_time:89077ms step_avg:92.31ms
step:966/1645 train_time:89170ms step_avg:92.31ms
step:967/1645 train_time:89264ms step_avg:92.31ms
step:968/1645 train_time:89357ms step_avg:92.31ms
step:969/1645 train_time:89452ms step_avg:92.31ms
step:970/1645 train_time:89545ms step_avg:92.31ms
step:971/1645 train_time:89637ms step_avg:92.31ms
step:972/1645 train_time:89730ms step_avg:92.32ms
step:973/1645 train_time:89824ms step_avg:92.32ms
step:974/1645 train_time:89916ms step_avg:92.32ms
step:975/1645 train_time:90010ms step_avg:92.32ms
step:976/1645 train_time:90102ms step_avg:92.32ms
step:977/1645 train_time:90195ms step_avg:92.32ms
step:978/1645 train_time:90287ms step_avg:92.32ms
step:979/1645 train_time:90380ms step_avg:92.32ms
step:980/1645 train_time:90473ms step_avg:92.32ms
step:981/1645 train_time:90566ms step_avg:92.32ms
step:982/1645 train_time:90659ms step_avg:92.32ms
step:983/1645 train_time:90753ms step_avg:92.32ms
step:984/1645 train_time:90846ms step_avg:92.32ms
step:985/1645 train_time:90938ms step_avg:92.32ms
step:986/1645 train_time:91032ms step_avg:92.32ms
step:987/1645 train_time:91124ms step_avg:92.32ms
step:988/1645 train_time:91217ms step_avg:92.32ms
step:989/1645 train_time:91310ms step_avg:92.33ms
step:990/1645 train_time:91403ms step_avg:92.33ms
step:991/1645 train_time:91496ms step_avg:92.33ms
step:992/1645 train_time:91590ms step_avg:92.33ms
step:993/1645 train_time:91683ms step_avg:92.33ms
step:994/1645 train_time:91776ms step_avg:92.33ms
step:995/1645 train_time:91869ms step_avg:92.33ms
step:996/1645 train_time:91961ms step_avg:92.33ms
step:997/1645 train_time:92055ms step_avg:92.33ms
step:998/1645 train_time:92148ms step_avg:92.33ms
step:999/1645 train_time:92241ms step_avg:92.33ms
step:1000/1645 train_time:92334ms step_avg:92.33ms
step:1000/1645 val_loss:3.4611 train_time:92428ms step_avg:92.43ms
step:1001/1645 train_time:92449ms step_avg:92.36ms
step:1002/1645 train_time:92526ms step_avg:92.34ms
step:1003/1645 train_time:92621ms step_avg:92.34ms
step:1004/1645 train_time:92714ms step_avg:92.34ms
step:1005/1645 train_time:92805ms step_avg:92.34ms
step:1006/1645 train_time:92897ms step_avg:92.34ms
step:1007/1645 train_time:92989ms step_avg:92.34ms
step:1008/1645 train_time:93082ms step_avg:92.34ms
step:1009/1645 train_time:93175ms step_avg:92.34ms
step:1010/1645 train_time:93268ms step_avg:92.34ms
step:1011/1645 train_time:93362ms step_avg:92.35ms
step:1012/1645 train_time:93456ms step_avg:92.35ms
step:1013/1645 train_time:93551ms step_avg:92.35ms
step:1014/1645 train_time:93644ms step_avg:92.35ms
step:1015/1645 train_time:93737ms step_avg:92.35ms
step:1016/1645 train_time:93829ms step_avg:92.35ms
step:1017/1645 train_time:93921ms step_avg:92.35ms
step:1018/1645 train_time:94014ms step_avg:92.35ms
step:1019/1645 train_time:94106ms step_avg:92.35ms
step:1020/1645 train_time:94198ms step_avg:92.35ms
step:1021/1645 train_time:94291ms step_avg:92.35ms
step:1022/1645 train_time:94385ms step_avg:92.35ms
step:1023/1645 train_time:94479ms step_avg:92.35ms
step:1024/1645 train_time:94574ms step_avg:92.36ms
step:1025/1645 train_time:94668ms step_avg:92.36ms
step:1026/1645 train_time:94760ms step_avg:92.36ms
step:1027/1645 train_time:94853ms step_avg:92.36ms
step:1028/1645 train_time:94945ms step_avg:92.36ms
step:1029/1645 train_time:95037ms step_avg:92.36ms
step:1030/1645 train_time:95130ms step_avg:92.36ms
step:1031/1645 train_time:95222ms step_avg:92.36ms
step:1032/1645 train_time:95315ms step_avg:92.36ms
step:1033/1645 train_time:95408ms step_avg:92.36ms
step:1034/1645 train_time:95501ms step_avg:92.36ms
step:1035/1645 train_time:95595ms step_avg:92.36ms
step:1036/1645 train_time:95688ms step_avg:92.36ms
step:1037/1645 train_time:95781ms step_avg:92.36ms
step:1038/1645 train_time:95876ms step_avg:92.37ms
step:1039/1645 train_time:95966ms step_avg:92.36ms
step:1040/1645 train_time:96059ms step_avg:92.36ms
step:1041/1645 train_time:96152ms step_avg:92.37ms
step:1042/1645 train_time:96245ms step_avg:92.37ms
step:1043/1645 train_time:96337ms step_avg:92.37ms
step:1044/1645 train_time:96431ms step_avg:92.37ms
step:1045/1645 train_time:96525ms step_avg:92.37ms
step:1046/1645 train_time:96617ms step_avg:92.37ms
step:1047/1645 train_time:96710ms step_avg:92.37ms
step:1048/1645 train_time:96803ms step_avg:92.37ms
step:1049/1645 train_time:96896ms step_avg:92.37ms
step:1050/1645 train_time:96989ms step_avg:92.37ms
step:1051/1645 train_time:97081ms step_avg:92.37ms
step:1052/1645 train_time:97174ms step_avg:92.37ms
step:1053/1645 train_time:97267ms step_avg:92.37ms
step:1054/1645 train_time:97360ms step_avg:92.37ms
step:1055/1645 train_time:97452ms step_avg:92.37ms
step:1056/1645 train_time:97546ms step_avg:92.37ms
step:1057/1645 train_time:97639ms step_avg:92.37ms
step:1058/1645 train_time:97733ms step_avg:92.38ms
step:1059/1645 train_time:97826ms step_avg:92.38ms
step:1060/1645 train_time:97918ms step_avg:92.38ms
step:1061/1645 train_time:98011ms step_avg:92.38ms
step:1062/1645 train_time:98104ms step_avg:92.38ms
step:1063/1645 train_time:98197ms step_avg:92.38ms
step:1064/1645 train_time:98290ms step_avg:92.38ms
step:1065/1645 train_time:98385ms step_avg:92.38ms
step:1066/1645 train_time:98477ms step_avg:92.38ms
step:1067/1645 train_time:98570ms step_avg:92.38ms
step:1068/1645 train_time:98663ms step_avg:92.38ms
step:1069/1645 train_time:98756ms step_avg:92.38ms
step:1070/1645 train_time:98849ms step_avg:92.38ms
step:1071/1645 train_time:98942ms step_avg:92.38ms
step:1072/1645 train_time:99035ms step_avg:92.38ms
step:1073/1645 train_time:99127ms step_avg:92.38ms
step:1074/1645 train_time:99219ms step_avg:92.38ms
step:1075/1645 train_time:99312ms step_avg:92.38ms
step:1076/1645 train_time:99405ms step_avg:92.38ms
step:1077/1645 train_time:99498ms step_avg:92.38ms
step:1078/1645 train_time:99592ms step_avg:92.39ms
step:1079/1645 train_time:99685ms step_avg:92.39ms
step:1080/1645 train_time:99778ms step_avg:92.39ms
step:1081/1645 train_time:99871ms step_avg:92.39ms
step:1082/1645 train_time:99965ms step_avg:92.39ms
step:1083/1645 train_time:100058ms step_avg:92.39ms
step:1084/1645 train_time:100151ms step_avg:92.39ms
step:1085/1645 train_time:100243ms step_avg:92.39ms
step:1086/1645 train_time:100337ms step_avg:92.39ms
step:1087/1645 train_time:100429ms step_avg:92.39ms
step:1088/1645 train_time:100523ms step_avg:92.39ms
step:1089/1645 train_time:100616ms step_avg:92.39ms
step:1090/1645 train_time:100709ms step_avg:92.39ms
step:1091/1645 train_time:100801ms step_avg:92.39ms
step:1092/1645 train_time:100896ms step_avg:92.40ms
step:1093/1645 train_time:100989ms step_avg:92.40ms
step:1094/1645 train_time:101082ms step_avg:92.40ms
step:1095/1645 train_time:101175ms step_avg:92.40ms
step:1096/1645 train_time:101268ms step_avg:92.40ms
step:1097/1645 train_time:101360ms step_avg:92.40ms
step:1098/1645 train_time:101453ms step_avg:92.40ms
step:1099/1645 train_time:101546ms step_avg:92.40ms
step:1100/1645 train_time:101639ms step_avg:92.40ms
step:1101/1645 train_time:101733ms step_avg:92.40ms
step:1102/1645 train_time:101827ms step_avg:92.40ms
step:1103/1645 train_time:101920ms step_avg:92.40ms
step:1104/1645 train_time:102014ms step_avg:92.40ms
step:1105/1645 train_time:102108ms step_avg:92.41ms
step:1106/1645 train_time:102202ms step_avg:92.41ms
step:1107/1645 train_time:102295ms step_avg:92.41ms
step:1108/1645 train_time:102388ms step_avg:92.41ms
step:1109/1645 train_time:102482ms step_avg:92.41ms
step:1110/1645 train_time:102576ms step_avg:92.41ms
step:1111/1645 train_time:102670ms step_avg:92.41ms
step:1112/1645 train_time:102764ms step_avg:92.41ms
step:1113/1645 train_time:102857ms step_avg:92.41ms
step:1114/1645 train_time:102951ms step_avg:92.42ms
step:1115/1645 train_time:103045ms step_avg:92.42ms
step:1116/1645 train_time:103139ms step_avg:92.42ms
step:1117/1645 train_time:103233ms step_avg:92.42ms
step:1118/1645 train_time:103327ms step_avg:92.42ms
step:1119/1645 train_time:103419ms step_avg:92.42ms
step:1120/1645 train_time:103513ms step_avg:92.42ms
step:1121/1645 train_time:103608ms step_avg:92.42ms
step:1122/1645 train_time:103700ms step_avg:92.42ms
step:1123/1645 train_time:103794ms step_avg:92.43ms
step:1124/1645 train_time:103887ms step_avg:92.43ms
step:1125/1645 train_time:103980ms step_avg:92.43ms
step:1125/1645 val_loss:3.4085 train_time:104075ms step_avg:92.51ms
step:1126/1645 train_time:104099ms step_avg:92.45ms
step:1127/1645 train_time:104177ms step_avg:92.44ms
step:1128/1645 train_time:104279ms step_avg:92.45ms
step:1129/1645 train_time:104375ms step_avg:92.45ms
step:1130/1645 train_time:104467ms step_avg:92.45ms
step:1131/1645 train_time:104559ms step_avg:92.45ms
step:1132/1645 train_time:104652ms step_avg:92.45ms
step:1133/1645 train_time:104744ms step_avg:92.45ms
step:1134/1645 train_time:104836ms step_avg:92.45ms
step:1135/1645 train_time:104929ms step_avg:92.45ms
step:1136/1645 train_time:105025ms step_avg:92.45ms
step:1137/1645 train_time:105120ms step_avg:92.45ms
step:1138/1645 train_time:105218ms step_avg:92.46ms
step:1139/1645 train_time:105314ms step_avg:92.46ms
step:1140/1645 train_time:105409ms step_avg:92.46ms
step:1141/1645 train_time:105501ms step_avg:92.46ms
step:1142/1645 train_time:105594ms step_avg:92.46ms
step:1143/1645 train_time:105687ms step_avg:92.46ms
step:1144/1645 train_time:105780ms step_avg:92.46ms
step:1145/1645 train_time:105873ms step_avg:92.47ms
step:1146/1645 train_time:105966ms step_avg:92.47ms
step:1147/1645 train_time:106061ms step_avg:92.47ms
step:1148/1645 train_time:106155ms step_avg:92.47ms
step:1149/1645 train_time:106250ms step_avg:92.47ms
step:1150/1645 train_time:106344ms step_avg:92.47ms
step:1151/1645 train_time:106438ms step_avg:92.47ms
step:1152/1645 train_time:106531ms step_avg:92.47ms
step:1153/1645 train_time:106625ms step_avg:92.48ms
step:1154/1645 train_time:106717ms step_avg:92.48ms
step:1155/1645 train_time:106810ms step_avg:92.48ms
step:1156/1645 train_time:106904ms step_avg:92.48ms
step:1157/1645 train_time:106997ms step_avg:92.48ms
step:1158/1645 train_time:107090ms step_avg:92.48ms
step:1159/1645 train_time:107185ms step_avg:92.48ms
step:1160/1645 train_time:107280ms step_avg:92.48ms
step:1161/1645 train_time:107375ms step_avg:92.49ms
step:1162/1645 train_time:107468ms step_avg:92.49ms
step:1163/1645 train_time:107563ms step_avg:92.49ms
step:1164/1645 train_time:107655ms step_avg:92.49ms
step:1165/1645 train_time:107747ms step_avg:92.49ms
step:1166/1645 train_time:107840ms step_avg:92.49ms
step:1167/1645 train_time:107933ms step_avg:92.49ms
step:1168/1645 train_time:108026ms step_avg:92.49ms
step:1169/1645 train_time:108120ms step_avg:92.49ms
step:1170/1645 train_time:108214ms step_avg:92.49ms
step:1171/1645 train_time:108308ms step_avg:92.49ms
step:1172/1645 train_time:108401ms step_avg:92.49ms
step:1173/1645 train_time:108495ms step_avg:92.49ms
step:1174/1645 train_time:108588ms step_avg:92.49ms
step:1175/1645 train_time:108681ms step_avg:92.49ms
step:1176/1645 train_time:108774ms step_avg:92.50ms
step:1177/1645 train_time:108868ms step_avg:92.50ms
step:1178/1645 train_time:108961ms step_avg:92.50ms
step:1179/1645 train_time:109054ms step_avg:92.50ms
step:1180/1645 train_time:109148ms step_avg:92.50ms
step:1181/1645 train_time:109242ms step_avg:92.50ms
step:1182/1645 train_time:109336ms step_avg:92.50ms
step:1183/1645 train_time:109429ms step_avg:92.50ms
step:1184/1645 train_time:109523ms step_avg:92.50ms
step:1185/1645 train_time:109618ms step_avg:92.50ms
step:1186/1645 train_time:109711ms step_avg:92.51ms
step:1187/1645 train_time:109804ms step_avg:92.51ms
step:1188/1645 train_time:109897ms step_avg:92.51ms
step:1189/1645 train_time:109991ms step_avg:92.51ms
step:1190/1645 train_time:110085ms step_avg:92.51ms
step:1191/1645 train_time:110178ms step_avg:92.51ms
step:1192/1645 train_time:110271ms step_avg:92.51ms
step:1193/1645 train_time:110364ms step_avg:92.51ms
step:1194/1645 train_time:110457ms step_avg:92.51ms
step:1195/1645 train_time:110551ms step_avg:92.51ms
step:1196/1645 train_time:110644ms step_avg:92.51ms
step:1197/1645 train_time:110738ms step_avg:92.51ms
step:1198/1645 train_time:110831ms step_avg:92.51ms
step:1199/1645 train_time:110923ms step_avg:92.51ms
step:1200/1645 train_time:111017ms step_avg:92.51ms
step:1201/1645 train_time:111110ms step_avg:92.51ms
step:1202/1645 train_time:111204ms step_avg:92.52ms
step:1203/1645 train_time:111297ms step_avg:92.52ms
step:1204/1645 train_time:111392ms step_avg:92.52ms
step:1205/1645 train_time:111485ms step_avg:92.52ms
step:1206/1645 train_time:111578ms step_avg:92.52ms
step:1207/1645 train_time:111672ms step_avg:92.52ms
step:1208/1645 train_time:111766ms step_avg:92.52ms
step:1209/1645 train_time:111859ms step_avg:92.52ms
step:1210/1645 train_time:111953ms step_avg:92.52ms
step:1211/1645 train_time:112046ms step_avg:92.52ms
step:1212/1645 train_time:112140ms step_avg:92.52ms
step:1213/1645 train_time:112233ms step_avg:92.53ms
step:1214/1645 train_time:112327ms step_avg:92.53ms
step:1215/1645 train_time:112423ms step_avg:92.53ms
step:1216/1645 train_time:112517ms step_avg:92.53ms
step:1217/1645 train_time:112609ms step_avg:92.53ms
step:1218/1645 train_time:112703ms step_avg:92.53ms
step:1219/1645 train_time:112796ms step_avg:92.53ms
step:1220/1645 train_time:112889ms step_avg:92.53ms
step:1221/1645 train_time:112983ms step_avg:92.53ms
step:1222/1645 train_time:113077ms step_avg:92.53ms
step:1223/1645 train_time:113170ms step_avg:92.53ms
step:1224/1645 train_time:113263ms step_avg:92.54ms
step:1225/1645 train_time:113357ms step_avg:92.54ms
step:1226/1645 train_time:113451ms step_avg:92.54ms
step:1227/1645 train_time:113545ms step_avg:92.54ms
step:1228/1645 train_time:113639ms step_avg:92.54ms
step:1229/1645 train_time:113732ms step_avg:92.54ms
step:1230/1645 train_time:113826ms step_avg:92.54ms
step:1231/1645 train_time:113920ms step_avg:92.54ms
step:1232/1645 train_time:114015ms step_avg:92.54ms
step:1233/1645 train_time:114108ms step_avg:92.54ms
step:1234/1645 train_time:114201ms step_avg:92.55ms
step:1235/1645 train_time:114295ms step_avg:92.55ms
step:1236/1645 train_time:114389ms step_avg:92.55ms
step:1237/1645 train_time:114483ms step_avg:92.55ms
step:1238/1645 train_time:114577ms step_avg:92.55ms
step:1239/1645 train_time:114671ms step_avg:92.55ms
step:1240/1645 train_time:114764ms step_avg:92.55ms
step:1241/1645 train_time:114859ms step_avg:92.55ms
step:1242/1645 train_time:114953ms step_avg:92.55ms
step:1243/1645 train_time:115046ms step_avg:92.56ms
step:1244/1645 train_time:115140ms step_avg:92.56ms
step:1245/1645 train_time:115233ms step_avg:92.56ms
step:1246/1645 train_time:115327ms step_avg:92.56ms
step:1247/1645 train_time:115422ms step_avg:92.56ms
step:1248/1645 train_time:115517ms step_avg:92.56ms
step:1249/1645 train_time:115609ms step_avg:92.56ms
step:1250/1645 train_time:115702ms step_avg:92.56ms
step:1250/1645 val_loss:3.3704 train_time:115795ms step_avg:92.64ms
step:1251/1645 train_time:115816ms step_avg:92.58ms
step:1252/1645 train_time:115895ms step_avg:92.57ms
step:1253/1645 train_time:115989ms step_avg:92.57ms
step:1254/1645 train_time:116082ms step_avg:92.57ms
step:1255/1645 train_time:116175ms step_avg:92.57ms
step:1256/1645 train_time:116267ms step_avg:92.57ms
step:1257/1645 train_time:116359ms step_avg:92.57ms
step:1258/1645 train_time:116452ms step_avg:92.57ms
step:1259/1645 train_time:116547ms step_avg:92.57ms
step:1260/1645 train_time:116640ms step_avg:92.57ms
step:1261/1645 train_time:116734ms step_avg:92.57ms
step:1262/1645 train_time:116829ms step_avg:92.57ms
step:1263/1645 train_time:116924ms step_avg:92.58ms
step:1264/1645 train_time:117018ms step_avg:92.58ms
step:1265/1645 train_time:117111ms step_avg:92.58ms
step:1266/1645 train_time:117204ms step_avg:92.58ms
step:1267/1645 train_time:117298ms step_avg:92.58ms
step:1268/1645 train_time:117391ms step_avg:92.58ms
step:1269/1645 train_time:117486ms step_avg:92.58ms
step:1270/1645 train_time:117578ms step_avg:92.58ms
step:1271/1645 train_time:117672ms step_avg:92.58ms
step:1272/1645 train_time:117766ms step_avg:92.58ms
step:1273/1645 train_time:117861ms step_avg:92.58ms
step:1274/1645 train_time:117956ms step_avg:92.59ms
step:1275/1645 train_time:118049ms step_avg:92.59ms
step:1276/1645 train_time:118143ms step_avg:92.59ms
step:1277/1645 train_time:118236ms step_avg:92.59ms
step:1278/1645 train_time:118329ms step_avg:92.59ms
step:1279/1645 train_time:118422ms step_avg:92.59ms
step:1280/1645 train_time:118516ms step_avg:92.59ms
step:1281/1645 train_time:118609ms step_avg:92.59ms
step:1282/1645 train_time:118703ms step_avg:92.59ms
step:1283/1645 train_time:118797ms step_avg:92.59ms
step:1284/1645 train_time:118893ms step_avg:92.60ms
step:1285/1645 train_time:118988ms step_avg:92.60ms
step:1286/1645 train_time:119082ms step_avg:92.60ms
step:1287/1645 train_time:119175ms step_avg:92.60ms
step:1288/1645 train_time:119268ms step_avg:92.60ms
step:1289/1645 train_time:119361ms step_avg:92.60ms
step:1290/1645 train_time:119455ms step_avg:92.60ms
step:1291/1645 train_time:119548ms step_avg:92.60ms
step:1292/1645 train_time:119642ms step_avg:92.60ms
step:1293/1645 train_time:119736ms step_avg:92.60ms
step:1294/1645 train_time:119830ms step_avg:92.60ms
step:1295/1645 train_time:119925ms step_avg:92.61ms
step:1296/1645 train_time:120020ms step_avg:92.61ms
step:1297/1645 train_time:120115ms step_avg:92.61ms
step:1298/1645 train_time:120209ms step_avg:92.61ms
step:1299/1645 train_time:120302ms step_avg:92.61ms
step:1300/1645 train_time:120395ms step_avg:92.61ms
step:1301/1645 train_time:120489ms step_avg:92.61ms
step:1302/1645 train_time:120582ms step_avg:92.61ms
step:1303/1645 train_time:120676ms step_avg:92.61ms
step:1304/1645 train_time:120770ms step_avg:92.62ms
step:1305/1645 train_time:120864ms step_avg:92.62ms
step:1306/1645 train_time:120958ms step_avg:92.62ms
step:1307/1645 train_time:121054ms step_avg:92.62ms
step:1308/1645 train_time:121148ms step_avg:92.62ms
step:1309/1645 train_time:121241ms step_avg:92.62ms
step:1310/1645 train_time:121335ms step_avg:92.62ms
step:1311/1645 train_time:121428ms step_avg:92.62ms
step:1312/1645 train_time:121521ms step_avg:92.62ms
step:1313/1645 train_time:121614ms step_avg:92.62ms
step:1314/1645 train_time:121707ms step_avg:92.62ms
step:1315/1645 train_time:121801ms step_avg:92.62ms
step:1316/1645 train_time:121895ms step_avg:92.63ms
step:1317/1645 train_time:121988ms step_avg:92.63ms
step:1318/1645 train_time:122082ms step_avg:92.63ms
step:1319/1645 train_time:122177ms step_avg:92.63ms
step:1320/1645 train_time:122272ms step_avg:92.63ms
step:1321/1645 train_time:122365ms step_avg:92.63ms
step:1322/1645 train_time:122458ms step_avg:92.63ms
step:1323/1645 train_time:122552ms step_avg:92.63ms
step:1324/1645 train_time:122646ms step_avg:92.63ms
step:1325/1645 train_time:122740ms step_avg:92.63ms
step:1326/1645 train_time:122833ms step_avg:92.63ms
step:1327/1645 train_time:122926ms step_avg:92.63ms
step:1328/1645 train_time:123019ms step_avg:92.64ms
step:1329/1645 train_time:123114ms step_avg:92.64ms
step:1330/1645 train_time:123207ms step_avg:92.64ms
step:1331/1645 train_time:123301ms step_avg:92.64ms
step:1332/1645 train_time:123394ms step_avg:92.64ms
step:1333/1645 train_time:123488ms step_avg:92.64ms
step:1334/1645 train_time:123581ms step_avg:92.64ms
step:1335/1645 train_time:123676ms step_avg:92.64ms
step:1336/1645 train_time:123769ms step_avg:92.64ms
step:1337/1645 train_time:123863ms step_avg:92.64ms
step:1338/1645 train_time:123956ms step_avg:92.64ms
step:1339/1645 train_time:124050ms step_avg:92.64ms
step:1340/1645 train_time:124144ms step_avg:92.64ms
step:1341/1645 train_time:124237ms step_avg:92.65ms
step:1342/1645 train_time:124332ms step_avg:92.65ms
step:1343/1645 train_time:124426ms step_avg:92.65ms
step:1344/1645 train_time:124519ms step_avg:92.65ms
step:1345/1645 train_time:124612ms step_avg:92.65ms
step:1346/1645 train_time:124706ms step_avg:92.65ms
step:1347/1645 train_time:124799ms step_avg:92.65ms
step:1348/1645 train_time:124893ms step_avg:92.65ms
step:1349/1645 train_time:124988ms step_avg:92.65ms
step:1350/1645 train_time:125082ms step_avg:92.65ms
step:1351/1645 train_time:125176ms step_avg:92.65ms
step:1352/1645 train_time:125269ms step_avg:92.65ms
step:1353/1645 train_time:125363ms step_avg:92.66ms
step:1354/1645 train_time:125457ms step_avg:92.66ms
step:1355/1645 train_time:125551ms step_avg:92.66ms
step:1356/1645 train_time:125646ms step_avg:92.66ms
step:1357/1645 train_time:125740ms step_avg:92.66ms
step:1358/1645 train_time:125833ms step_avg:92.66ms
step:1359/1645 train_time:125927ms step_avg:92.66ms
step:1360/1645 train_time:126020ms step_avg:92.66ms
step:1361/1645 train_time:126114ms step_avg:92.66ms
step:1362/1645 train_time:126208ms step_avg:92.66ms
step:1363/1645 train_time:126301ms step_avg:92.66ms
step:1364/1645 train_time:126394ms step_avg:92.66ms
step:1365/1645 train_time:126489ms step_avg:92.67ms
step:1366/1645 train_time:126582ms step_avg:92.67ms
step:1367/1645 train_time:126675ms step_avg:92.67ms
step:1368/1645 train_time:126770ms step_avg:92.67ms
step:1369/1645 train_time:126863ms step_avg:92.67ms
step:1370/1645 train_time:126957ms step_avg:92.67ms
step:1371/1645 train_time:127051ms step_avg:92.67ms
step:1372/1645 train_time:127145ms step_avg:92.67ms
step:1373/1645 train_time:127239ms step_avg:92.67ms
step:1374/1645 train_time:127332ms step_avg:92.67ms
step:1375/1645 train_time:127425ms step_avg:92.67ms
step:1375/1645 val_loss:3.3356 train_time:127519ms step_avg:92.74ms
step:1376/1645 train_time:127541ms step_avg:92.69ms
step:1377/1645 train_time:127618ms step_avg:92.68ms
step:1378/1645 train_time:127713ms step_avg:92.68ms
step:1379/1645 train_time:127806ms step_avg:92.68ms
step:1380/1645 train_time:127899ms step_avg:92.68ms
step:1381/1645 train_time:127992ms step_avg:92.68ms
step:1382/1645 train_time:128085ms step_avg:92.68ms
step:1383/1645 train_time:128178ms step_avg:92.68ms
step:1384/1645 train_time:128271ms step_avg:92.68ms
step:1385/1645 train_time:128366ms step_avg:92.68ms
step:1386/1645 train_time:128460ms step_avg:92.68ms
step:1387/1645 train_time:128556ms step_avg:92.69ms
step:1388/1645 train_time:128650ms step_avg:92.69ms
step:1389/1645 train_time:128744ms step_avg:92.69ms
step:1390/1645 train_time:128838ms step_avg:92.69ms
step:1391/1645 train_time:128931ms step_avg:92.69ms
step:1392/1645 train_time:129024ms step_avg:92.69ms
step:1393/1645 train_time:129118ms step_avg:92.69ms
step:1394/1645 train_time:129211ms step_avg:92.69ms
step:1395/1645 train_time:129304ms step_avg:92.69ms
step:1396/1645 train_time:129398ms step_avg:92.69ms
step:1397/1645 train_time:129492ms step_avg:92.69ms
step:1398/1645 train_time:129587ms step_avg:92.69ms
step:1399/1645 train_time:129681ms step_avg:92.70ms
step:1400/1645 train_time:129774ms step_avg:92.70ms
step:1401/1645 train_time:129867ms step_avg:92.70ms
step:1402/1645 train_time:129961ms step_avg:92.70ms
step:1403/1645 train_time:130054ms step_avg:92.70ms
step:1404/1645 train_time:130147ms step_avg:92.70ms
step:1405/1645 train_time:130241ms step_avg:92.70ms
step:1406/1645 train_time:130335ms step_avg:92.70ms
step:1407/1645 train_time:130429ms step_avg:92.70ms
step:1408/1645 train_time:130525ms step_avg:92.70ms
step:1409/1645 train_time:130620ms step_avg:92.70ms
step:1410/1645 train_time:130713ms step_avg:92.70ms
step:1411/1645 train_time:130806ms step_avg:92.70ms
step:1412/1645 train_time:130899ms step_avg:92.70ms
step:1413/1645 train_time:130993ms step_avg:92.71ms
step:1414/1645 train_time:131087ms step_avg:92.71ms
step:1415/1645 train_time:131180ms step_avg:92.71ms
step:1416/1645 train_time:131274ms step_avg:92.71ms
step:1417/1645 train_time:131367ms step_avg:92.71ms
step:1418/1645 train_time:131460ms step_avg:92.71ms
step:1419/1645 train_time:131554ms step_avg:92.71ms
step:1420/1645 train_time:131649ms step_avg:92.71ms
step:1421/1645 train_time:131743ms step_avg:92.71ms
step:1422/1645 train_time:131837ms step_avg:92.71ms
step:1423/1645 train_time:131930ms step_avg:92.71ms
step:1424/1645 train_time:132023ms step_avg:92.71ms
step:1425/1645 train_time:132116ms step_avg:92.71ms
step:1426/1645 train_time:132209ms step_avg:92.71ms
step:1427/1645 train_time:132303ms step_avg:92.71ms
step:1428/1645 train_time:132396ms step_avg:92.71ms
step:1429/1645 train_time:132490ms step_avg:92.72ms
step:1430/1645 train_time:132585ms step_avg:92.72ms
step:1431/1645 train_time:132679ms step_avg:92.72ms
step:1432/1645 train_time:132772ms step_avg:92.72ms
step:1433/1645 train_time:132866ms step_avg:92.72ms
step:1434/1645 train_time:132961ms step_avg:92.72ms
step:1435/1645 train_time:133055ms step_avg:92.72ms
step:1436/1645 train_time:133148ms step_avg:92.72ms
step:1437/1645 train_time:133241ms step_avg:92.72ms
step:1438/1645 train_time:133335ms step_avg:92.72ms
step:1439/1645 train_time:133429ms step_avg:92.72ms
step:1440/1645 train_time:133523ms step_avg:92.72ms
step:1441/1645 train_time:133617ms step_avg:92.72ms
step:1442/1645 train_time:133710ms step_avg:92.73ms
step:1443/1645 train_time:133804ms step_avg:92.73ms
step:1444/1645 train_time:133898ms step_avg:92.73ms
step:1445/1645 train_time:133991ms step_avg:92.73ms
step:1446/1645 train_time:134085ms step_avg:92.73ms
step:1447/1645 train_time:134179ms step_avg:92.73ms
step:1448/1645 train_time:134272ms step_avg:92.73ms
step:1449/1645 train_time:134366ms step_avg:92.73ms
step:1450/1645 train_time:134459ms step_avg:92.73ms
step:1451/1645 train_time:134553ms step_avg:92.73ms
step:1452/1645 train_time:134647ms step_avg:92.73ms
step:1453/1645 train_time:134741ms step_avg:92.73ms
step:1454/1645 train_time:134835ms step_avg:92.73ms
step:1455/1645 train_time:134928ms step_avg:92.73ms
step:1456/1645 train_time:135022ms step_avg:92.73ms
step:1457/1645 train_time:135115ms step_avg:92.74ms
step:1458/1645 train_time:135209ms step_avg:92.74ms
step:1459/1645 train_time:135302ms step_avg:92.74ms
step:1460/1645 train_time:135396ms step_avg:92.74ms
step:1461/1645 train_time:135489ms step_avg:92.74ms
step:1462/1645 train_time:135583ms step_avg:92.74ms
step:1463/1645 train_time:135677ms step_avg:92.74ms
step:1464/1645 train_time:135770ms step_avg:92.74ms
step:1465/1645 train_time:135863ms step_avg:92.74ms
step:1466/1645 train_time:135957ms step_avg:92.74ms
step:1467/1645 train_time:136051ms step_avg:92.74ms
step:1468/1645 train_time:136145ms step_avg:92.74ms
step:1469/1645 train_time:136238ms step_avg:92.74ms
step:1470/1645 train_time:136331ms step_avg:92.74ms
step:1471/1645 train_time:136425ms step_avg:92.74ms
step:1472/1645 train_time:136519ms step_avg:92.74ms
step:1473/1645 train_time:136612ms step_avg:92.74ms
step:1474/1645 train_time:136706ms step_avg:92.74ms
step:1475/1645 train_time:136799ms step_avg:92.75ms
step:1476/1645 train_time:136893ms step_avg:92.75ms
step:1477/1645 train_time:136987ms step_avg:92.75ms
step:1478/1645 train_time:137081ms step_avg:92.75ms
step:1479/1645 train_time:137176ms step_avg:92.75ms
step:1480/1645 train_time:137270ms step_avg:92.75ms
step:1481/1645 train_time:137363ms step_avg:92.75ms
step:1482/1645 train_time:137457ms step_avg:92.75ms
step:1483/1645 train_time:137550ms step_avg:92.75ms
step:1484/1645 train_time:137645ms step_avg:92.75ms
step:1485/1645 train_time:137740ms step_avg:92.75ms
step:1486/1645 train_time:137832ms step_avg:92.75ms
step:1487/1645 train_time:137926ms step_avg:92.75ms
step:1488/1645 train_time:138020ms step_avg:92.76ms
step:1489/1645 train_time:138113ms step_avg:92.76ms
step:1490/1645 train_time:138207ms step_avg:92.76ms
step:1491/1645 train_time:138301ms step_avg:92.76ms
step:1492/1645 train_time:138394ms step_avg:92.76ms
step:1493/1645 train_time:138488ms step_avg:92.76ms
step:1494/1645 train_time:138582ms step_avg:92.76ms
step:1495/1645 train_time:138675ms step_avg:92.76ms
step:1496/1645 train_time:138769ms step_avg:92.76ms
step:1497/1645 train_time:138863ms step_avg:92.76ms
step:1498/1645 train_time:138956ms step_avg:92.76ms
step:1499/1645 train_time:139051ms step_avg:92.76ms
step:1500/1645 train_time:139145ms step_avg:92.76ms
step:1500/1645 val_loss:3.3060 train_time:139238ms step_avg:92.83ms
step:1501/1645 train_time:139260ms step_avg:92.78ms
step:1502/1645 train_time:139336ms step_avg:92.77ms
step:1503/1645 train_time:139434ms step_avg:92.77ms
step:1504/1645 train_time:139527ms step_avg:92.77ms
step:1505/1645 train_time:139620ms step_avg:92.77ms
step:1506/1645 train_time:139713ms step_avg:92.77ms
step:1507/1645 train_time:139805ms step_avg:92.77ms
step:1508/1645 train_time:139899ms step_avg:92.77ms
step:1509/1645 train_time:139992ms step_avg:92.77ms
step:1510/1645 train_time:140085ms step_avg:92.77ms
step:1511/1645 train_time:140179ms step_avg:92.77ms
step:1512/1645 train_time:140275ms step_avg:92.77ms
step:1513/1645 train_time:140369ms step_avg:92.78ms
step:1514/1645 train_time:140464ms step_avg:92.78ms
step:1515/1645 train_time:140557ms step_avg:92.78ms
step:1516/1645 train_time:140651ms step_avg:92.78ms
step:1517/1645 train_time:140743ms step_avg:92.78ms
step:1518/1645 train_time:140836ms step_avg:92.78ms
step:1519/1645 train_time:140929ms step_avg:92.78ms
step:1520/1645 train_time:141023ms step_avg:92.78ms
step:1521/1645 train_time:141117ms step_avg:92.78ms
step:1522/1645 train_time:141211ms step_avg:92.78ms
step:1523/1645 train_time:141305ms step_avg:92.78ms
step:1524/1645 train_time:141400ms step_avg:92.78ms
step:1525/1645 train_time:141493ms step_avg:92.78ms
step:1526/1645 train_time:141587ms step_avg:92.78ms
step:1527/1645 train_time:141680ms step_avg:92.78ms
step:1528/1645 train_time:141773ms step_avg:92.78ms
step:1529/1645 train_time:141866ms step_avg:92.78ms
step:1530/1645 train_time:141959ms step_avg:92.78ms
step:1531/1645 train_time:142053ms step_avg:92.78ms
step:1532/1645 train_time:142146ms step_avg:92.78ms
step:1533/1645 train_time:142240ms step_avg:92.79ms
step:1534/1645 train_time:142335ms step_avg:92.79ms
step:1535/1645 train_time:142429ms step_avg:92.79ms
step:1536/1645 train_time:142524ms step_avg:92.79ms
step:1537/1645 train_time:142617ms step_avg:92.79ms
step:1538/1645 train_time:142712ms step_avg:92.79ms
step:1539/1645 train_time:142804ms step_avg:92.79ms
step:1540/1645 train_time:142897ms step_avg:92.79ms
step:1541/1645 train_time:142990ms step_avg:92.79ms
step:1542/1645 train_time:143083ms step_avg:92.79ms
step:1543/1645 train_time:143177ms step_avg:92.79ms
step:1544/1645 train_time:143272ms step_avg:92.79ms
step:1545/1645 train_time:143365ms step_avg:92.79ms
step:1546/1645 train_time:143459ms step_avg:92.79ms
step:1547/1645 train_time:143552ms step_avg:92.79ms
step:1548/1645 train_time:143646ms step_avg:92.79ms
step:1549/1645 train_time:143740ms step_avg:92.80ms
step:1550/1645 train_time:143833ms step_avg:92.80ms
step:1551/1645 train_time:143926ms step_avg:92.80ms
step:1552/1645 train_time:144020ms step_avg:92.80ms
step:1553/1645 train_time:144114ms step_avg:92.80ms
step:1554/1645 train_time:144208ms step_avg:92.80ms
step:1555/1645 train_time:144302ms step_avg:92.80ms
step:1556/1645 train_time:144396ms step_avg:92.80ms
step:1557/1645 train_time:144490ms step_avg:92.80ms
step:1558/1645 train_time:144585ms step_avg:92.80ms
step:1559/1645 train_time:144678ms step_avg:92.80ms
step:1560/1645 train_time:144772ms step_avg:92.80ms
step:1561/1645 train_time:144866ms step_avg:92.80ms
step:1562/1645 train_time:144959ms step_avg:92.80ms
step:1563/1645 train_time:145054ms step_avg:92.80ms
step:1564/1645 train_time:145148ms step_avg:92.81ms
step:1565/1645 train_time:145242ms step_avg:92.81ms
step:1566/1645 train_time:145336ms step_avg:92.81ms
step:1567/1645 train_time:145430ms step_avg:92.81ms
step:1568/1645 train_time:145523ms step_avg:92.81ms
step:1569/1645 train_time:145617ms step_avg:92.81ms
step:1570/1645 train_time:145711ms step_avg:92.81ms
step:1571/1645 train_time:145804ms step_avg:92.81ms
step:1572/1645 train_time:145897ms step_avg:92.81ms
step:1573/1645 train_time:145991ms step_avg:92.81ms
step:1574/1645 train_time:146084ms step_avg:92.81ms
step:1575/1645 train_time:146178ms step_avg:92.81ms
step:1576/1645 train_time:146272ms step_avg:92.81ms
step:1577/1645 train_time:146365ms step_avg:92.81ms
step:1578/1645 train_time:146459ms step_avg:92.81ms
step:1579/1645 train_time:146552ms step_avg:92.81ms
step:1580/1645 train_time:146647ms step_avg:92.81ms
step:1581/1645 train_time:146741ms step_avg:92.82ms
step:1582/1645 train_time:146834ms step_avg:92.82ms
step:1583/1645 train_time:146927ms step_avg:92.82ms
step:1584/1645 train_time:147021ms step_avg:92.82ms
step:1585/1645 train_time:147114ms step_avg:92.82ms
step:1586/1645 train_time:147207ms step_avg:92.82ms
step:1587/1645 train_time:147301ms step_avg:92.82ms
step:1588/1645 train_time:147395ms step_avg:92.82ms
step:1589/1645 train_time:147488ms step_avg:92.82ms
step:1590/1645 train_time:147582ms step_avg:92.82ms
step:1591/1645 train_time:147676ms step_avg:92.82ms
step:1592/1645 train_time:147769ms step_avg:92.82ms
step:1593/1645 train_time:147864ms step_avg:92.82ms
step:1594/1645 train_time:147957ms step_avg:92.82ms
step:1595/1645 train_time:148052ms step_avg:92.82ms
step:1596/1645 train_time:148145ms step_avg:92.82ms
step:1597/1645 train_time:148239ms step_avg:92.82ms
step:1598/1645 train_time:148332ms step_avg:92.82ms
step:1599/1645 train_time:148425ms step_avg:92.82ms
step:1600/1645 train_time:148519ms step_avg:92.82ms
step:1601/1645 train_time:148613ms step_avg:92.82ms
step:1602/1645 train_time:148706ms step_avg:92.83ms
step:1603/1645 train_time:148800ms step_avg:92.83ms
step:1604/1645 train_time:148894ms step_avg:92.83ms
step:1605/1645 train_time:148988ms step_avg:92.83ms
step:1606/1645 train_time:149081ms step_avg:92.83ms
step:1607/1645 train_time:149174ms step_avg:92.83ms
step:1608/1645 train_time:149268ms step_avg:92.83ms
step:1609/1645 train_time:149362ms step_avg:92.83ms
step:1610/1645 train_time:149456ms step_avg:92.83ms
step:1611/1645 train_time:149551ms step_avg:92.83ms
step:1612/1645 train_time:149644ms step_avg:92.83ms
step:1613/1645 train_time:149737ms step_avg:92.83ms
step:1614/1645 train_time:149831ms step_avg:92.83ms
step:1615/1645 train_time:149924ms step_avg:92.83ms
step:1616/1645 train_time:150019ms step_avg:92.83ms
step:1617/1645 train_time:150112ms step_avg:92.83ms
step:1618/1645 train_time:150206ms step_avg:92.83ms
step:1619/1645 train_time:150300ms step_avg:92.83ms
step:1620/1645 train_time:150393ms step_avg:92.84ms
step:1621/1645 train_time:150487ms step_avg:92.84ms
step:1622/1645 train_time:150581ms step_avg:92.84ms
step:1623/1645 train_time:150675ms step_avg:92.84ms
step:1624/1645 train_time:150768ms step_avg:92.84ms
step:1625/1645 train_time:150862ms step_avg:92.84ms
step:1625/1645 val_loss:3.2821 train_time:150956ms step_avg:92.90ms
step:1626/1645 train_time:150976ms step_avg:92.85ms
step:1627/1645 train_time:151053ms step_avg:92.84ms
step:1628/1645 train_time:151149ms step_avg:92.84ms
step:1629/1645 train_time:151242ms step_avg:92.84ms
step:1630/1645 train_time:151335ms step_avg:92.84ms
step:1631/1645 train_time:151427ms step_avg:92.84ms
step:1632/1645 train_time:151520ms step_avg:92.84ms
step:1633/1645 train_time:151613ms step_avg:92.84ms
step:1634/1645 train_time:151706ms step_avg:92.84ms
step:1635/1645 train_time:151799ms step_avg:92.84ms
step:1636/1645 train_time:151896ms step_avg:92.85ms
step:1637/1645 train_time:151991ms step_avg:92.85ms
step:1638/1645 train_time:152085ms step_avg:92.85ms
step:1639/1645 train_time:152179ms step_avg:92.85ms
step:1640/1645 train_time:152272ms step_avg:92.85ms
step:1641/1645 train_time:152366ms step_avg:92.85ms
step:1642/1645 train_time:152459ms step_avg:92.85ms
step:1643/1645 train_time:152552ms step_avg:92.85ms
step:1644/1645 train_time:152645ms step_avg:92.85ms
step:1645/1645 train_time:152739ms step_avg:92.85ms
step:1645/1645 val_loss:3.2765 train_time:152834ms step_avg:92.91ms
peak memory allocated: 32074 MiB reserved: 46896 MiB

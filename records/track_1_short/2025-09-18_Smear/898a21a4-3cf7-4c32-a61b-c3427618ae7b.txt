import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:44:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:129ms step_avg:129.06ms
step:2/1645 train_time:149ms step_avg:74.73ms
step:3/1645 train_time:218ms step_avg:72.57ms
step:4/1645 train_time:307ms step_avg:76.85ms
step:5/1645 train_time:398ms step_avg:79.68ms
step:6/1645 train_time:489ms step_avg:81.52ms
step:7/1645 train_time:580ms step_avg:82.83ms
step:8/1645 train_time:671ms step_avg:83.83ms
step:9/1645 train_time:761ms step_avg:84.59ms
step:10/1645 train_time:852ms step_avg:85.16ms
step:11/1645 train_time:942ms step_avg:85.68ms
step:12/1645 train_time:1037ms step_avg:86.41ms
step:13/1645 train_time:1132ms step_avg:87.06ms
step:14/1645 train_time:1226ms step_avg:87.54ms
step:15/1645 train_time:1318ms step_avg:87.88ms
step:16/1645 train_time:1411ms step_avg:88.16ms
step:17/1645 train_time:1502ms step_avg:88.35ms
step:18/1645 train_time:1594ms step_avg:88.54ms
step:19/1645 train_time:1685ms step_avg:88.67ms
step:20/1645 train_time:1776ms step_avg:88.79ms
step:21/1645 train_time:1867ms step_avg:88.89ms
step:22/1645 train_time:1959ms step_avg:89.05ms
step:23/1645 train_time:2052ms step_avg:89.22ms
step:24/1645 train_time:2145ms step_avg:89.36ms
step:25/1645 train_time:2238ms step_avg:89.50ms
step:26/1645 train_time:2330ms step_avg:89.61ms
step:27/1645 train_time:2422ms step_avg:89.71ms
step:28/1645 train_time:2515ms step_avg:89.81ms
step:29/1645 train_time:2607ms step_avg:89.88ms
step:30/1645 train_time:2698ms step_avg:89.94ms
step:31/1645 train_time:2790ms step_avg:90.00ms
step:32/1645 train_time:2882ms step_avg:90.05ms
step:33/1645 train_time:2974ms step_avg:90.11ms
step:34/1645 train_time:3066ms step_avg:90.18ms
step:35/1645 train_time:3160ms step_avg:90.29ms
step:36/1645 train_time:3253ms step_avg:90.36ms
step:37/1645 train_time:3345ms step_avg:90.41ms
step:38/1645 train_time:3438ms step_avg:90.48ms
step:39/1645 train_time:3530ms step_avg:90.51ms
step:40/1645 train_time:3623ms step_avg:90.56ms
step:41/1645 train_time:3715ms step_avg:90.61ms
step:42/1645 train_time:3806ms step_avg:90.62ms
step:43/1645 train_time:3898ms step_avg:90.65ms
step:44/1645 train_time:3989ms step_avg:90.67ms
step:45/1645 train_time:4082ms step_avg:90.71ms
step:46/1645 train_time:4174ms step_avg:90.73ms
step:47/1645 train_time:4267ms step_avg:90.78ms
step:48/1645 train_time:4359ms step_avg:90.81ms
step:49/1645 train_time:4451ms step_avg:90.83ms
step:50/1645 train_time:4543ms step_avg:90.86ms
step:51/1645 train_time:4635ms step_avg:90.89ms
step:52/1645 train_time:4727ms step_avg:90.91ms
step:53/1645 train_time:4819ms step_avg:90.93ms
step:54/1645 train_time:4910ms step_avg:90.93ms
step:55/1645 train_time:5002ms step_avg:90.94ms
step:56/1645 train_time:5094ms step_avg:90.97ms
step:57/1645 train_time:5186ms step_avg:90.98ms
step:58/1645 train_time:5278ms step_avg:90.99ms
step:59/1645 train_time:5370ms step_avg:91.02ms
step:60/1645 train_time:5462ms step_avg:91.03ms
step:61/1645 train_time:5554ms step_avg:91.05ms
step:62/1645 train_time:5646ms step_avg:91.07ms
step:63/1645 train_time:5738ms step_avg:91.09ms
step:64/1645 train_time:5830ms step_avg:91.10ms
step:65/1645 train_time:5921ms step_avg:91.10ms
step:66/1645 train_time:6014ms step_avg:91.12ms
step:67/1645 train_time:6106ms step_avg:91.13ms
step:68/1645 train_time:6199ms step_avg:91.16ms
step:69/1645 train_time:6290ms step_avg:91.16ms
step:70/1645 train_time:6383ms step_avg:91.18ms
step:71/1645 train_time:6474ms step_avg:91.19ms
step:72/1645 train_time:6566ms step_avg:91.20ms
step:73/1645 train_time:6658ms step_avg:91.21ms
step:74/1645 train_time:6750ms step_avg:91.21ms
step:75/1645 train_time:6842ms step_avg:91.22ms
step:76/1645 train_time:6934ms step_avg:91.24ms
step:77/1645 train_time:7025ms step_avg:91.24ms
step:78/1645 train_time:7119ms step_avg:91.26ms
step:79/1645 train_time:7210ms step_avg:91.27ms
step:80/1645 train_time:7302ms step_avg:91.28ms
step:81/1645 train_time:7394ms step_avg:91.29ms
step:82/1645 train_time:7485ms step_avg:91.29ms
step:83/1645 train_time:7578ms step_avg:91.31ms
step:84/1645 train_time:7669ms step_avg:91.30ms
step:85/1645 train_time:7760ms step_avg:91.30ms
step:86/1645 train_time:7852ms step_avg:91.31ms
step:87/1645 train_time:7944ms step_avg:91.31ms
step:88/1645 train_time:8036ms step_avg:91.32ms
step:89/1645 train_time:8128ms step_avg:91.32ms
step:90/1645 train_time:8221ms step_avg:91.34ms
step:91/1645 train_time:8312ms step_avg:91.34ms
step:92/1645 train_time:8404ms step_avg:91.35ms
step:93/1645 train_time:8495ms step_avg:91.35ms
step:94/1645 train_time:8586ms step_avg:91.34ms
step:95/1645 train_time:8678ms step_avg:91.34ms
step:96/1645 train_time:8769ms step_avg:91.34ms
step:97/1645 train_time:8861ms step_avg:91.35ms
step:98/1645 train_time:8952ms step_avg:91.35ms
step:99/1645 train_time:9044ms step_avg:91.35ms
step:100/1645 train_time:9136ms step_avg:91.36ms
step:101/1645 train_time:9228ms step_avg:91.37ms
step:102/1645 train_time:9321ms step_avg:91.38ms
step:103/1645 train_time:9413ms step_avg:91.39ms
step:104/1645 train_time:9504ms step_avg:91.39ms
step:105/1645 train_time:9596ms step_avg:91.39ms
step:106/1645 train_time:9687ms step_avg:91.39ms
step:107/1645 train_time:9778ms step_avg:91.38ms
step:108/1645 train_time:9869ms step_avg:91.38ms
step:109/1645 train_time:9961ms step_avg:91.39ms
step:110/1645 train_time:10052ms step_avg:91.39ms
step:111/1645 train_time:10144ms step_avg:91.39ms
step:112/1645 train_time:10237ms step_avg:91.40ms
step:113/1645 train_time:10330ms step_avg:91.41ms
step:114/1645 train_time:10422ms step_avg:91.42ms
step:115/1645 train_time:10513ms step_avg:91.42ms
step:116/1645 train_time:10604ms step_avg:91.41ms
step:117/1645 train_time:10696ms step_avg:91.42ms
step:118/1645 train_time:10789ms step_avg:91.43ms
step:119/1645 train_time:10881ms step_avg:91.43ms
step:120/1645 train_time:10972ms step_avg:91.44ms
step:121/1645 train_time:11063ms step_avg:91.43ms
step:122/1645 train_time:11154ms step_avg:91.43ms
step:123/1645 train_time:11246ms step_avg:91.43ms
step:124/1645 train_time:11339ms step_avg:91.44ms
step:125/1645 train_time:11431ms step_avg:91.45ms
step:125/1645 val_loss:4.3213 train_time:11524ms step_avg:92.19ms
step:126/1645 train_time:11545ms step_avg:91.63ms
step:127/1645 train_time:11620ms step_avg:91.50ms
step:128/1645 train_time:11722ms step_avg:91.58ms
step:129/1645 train_time:11820ms step_avg:91.63ms
step:130/1645 train_time:11913ms step_avg:91.64ms
step:131/1645 train_time:12004ms step_avg:91.63ms
step:132/1645 train_time:12094ms step_avg:91.62ms
step:133/1645 train_time:12185ms step_avg:91.62ms
step:134/1645 train_time:12275ms step_avg:91.61ms
step:135/1645 train_time:12366ms step_avg:91.60ms
step:136/1645 train_time:12457ms step_avg:91.59ms
step:137/1645 train_time:12548ms step_avg:91.59ms
step:138/1645 train_time:12641ms step_avg:91.60ms
step:139/1645 train_time:12735ms step_avg:91.62ms
step:140/1645 train_time:12828ms step_avg:91.63ms
step:141/1645 train_time:12921ms step_avg:91.64ms
step:142/1645 train_time:13013ms step_avg:91.64ms
step:143/1645 train_time:13104ms step_avg:91.63ms
step:144/1645 train_time:13195ms step_avg:91.63ms
step:145/1645 train_time:13286ms step_avg:91.62ms
step:146/1645 train_time:13376ms step_avg:91.62ms
step:147/1645 train_time:13467ms step_avg:91.61ms
step:148/1645 train_time:13558ms step_avg:91.61ms
step:149/1645 train_time:13650ms step_avg:91.61ms
step:150/1645 train_time:13743ms step_avg:91.62ms
step:151/1645 train_time:13835ms step_avg:91.62ms
step:152/1645 train_time:13928ms step_avg:91.63ms
step:153/1645 train_time:14020ms step_avg:91.63ms
step:154/1645 train_time:14113ms step_avg:91.64ms
step:155/1645 train_time:14205ms step_avg:91.65ms
step:156/1645 train_time:14296ms step_avg:91.64ms
step:157/1645 train_time:14387ms step_avg:91.63ms
step:158/1645 train_time:14478ms step_avg:91.63ms
step:159/1645 train_time:14569ms step_avg:91.63ms
step:160/1645 train_time:14661ms step_avg:91.63ms
step:161/1645 train_time:14754ms step_avg:91.64ms
step:162/1645 train_time:14847ms step_avg:91.65ms
step:163/1645 train_time:14938ms step_avg:91.65ms
step:164/1645 train_time:15031ms step_avg:91.65ms
step:165/1645 train_time:15123ms step_avg:91.65ms
step:166/1645 train_time:15215ms step_avg:91.65ms
step:167/1645 train_time:15306ms step_avg:91.65ms
step:168/1645 train_time:15397ms step_avg:91.65ms
step:169/1645 train_time:15488ms step_avg:91.64ms
step:170/1645 train_time:15579ms step_avg:91.64ms
step:171/1645 train_time:15670ms step_avg:91.64ms
step:172/1645 train_time:15763ms step_avg:91.64ms
step:173/1645 train_time:15855ms step_avg:91.64ms
step:174/1645 train_time:15947ms step_avg:91.65ms
step:175/1645 train_time:16039ms step_avg:91.65ms
step:176/1645 train_time:16131ms step_avg:91.65ms
step:177/1645 train_time:16222ms step_avg:91.65ms
step:178/1645 train_time:16313ms step_avg:91.65ms
step:179/1645 train_time:16404ms step_avg:91.64ms
step:180/1645 train_time:16495ms step_avg:91.64ms
step:181/1645 train_time:16586ms step_avg:91.64ms
step:182/1645 train_time:16679ms step_avg:91.64ms
step:183/1645 train_time:16771ms step_avg:91.64ms
step:184/1645 train_time:16863ms step_avg:91.65ms
step:185/1645 train_time:16956ms step_avg:91.65ms
step:186/1645 train_time:17048ms step_avg:91.66ms
step:187/1645 train_time:17139ms step_avg:91.65ms
step:188/1645 train_time:17230ms step_avg:91.65ms
step:189/1645 train_time:17321ms step_avg:91.65ms
step:190/1645 train_time:17413ms step_avg:91.64ms
step:191/1645 train_time:17503ms step_avg:91.64ms
step:192/1645 train_time:17595ms step_avg:91.64ms
step:193/1645 train_time:17686ms step_avg:91.64ms
step:194/1645 train_time:17779ms step_avg:91.65ms
step:195/1645 train_time:17871ms step_avg:91.65ms
step:196/1645 train_time:17964ms step_avg:91.65ms
step:197/1645 train_time:18057ms step_avg:91.66ms
step:198/1645 train_time:18148ms step_avg:91.66ms
step:199/1645 train_time:18240ms step_avg:91.66ms
step:200/1645 train_time:18332ms step_avg:91.66ms
step:201/1645 train_time:18423ms step_avg:91.66ms
step:202/1645 train_time:18514ms step_avg:91.65ms
step:203/1645 train_time:18605ms step_avg:91.65ms
step:204/1645 train_time:18696ms step_avg:91.65ms
step:205/1645 train_time:18787ms step_avg:91.65ms
step:206/1645 train_time:18880ms step_avg:91.65ms
step:207/1645 train_time:18972ms step_avg:91.65ms
step:208/1645 train_time:19066ms step_avg:91.66ms
step:209/1645 train_time:19156ms step_avg:91.66ms
step:210/1645 train_time:19248ms step_avg:91.66ms
step:211/1645 train_time:19340ms step_avg:91.66ms
step:212/1645 train_time:19431ms step_avg:91.65ms
step:213/1645 train_time:19523ms step_avg:91.66ms
step:214/1645 train_time:19614ms step_avg:91.65ms
step:215/1645 train_time:19705ms step_avg:91.65ms
step:216/1645 train_time:19797ms step_avg:91.65ms
step:217/1645 train_time:19888ms step_avg:91.65ms
step:218/1645 train_time:19980ms step_avg:91.65ms
step:219/1645 train_time:20073ms step_avg:91.66ms
step:220/1645 train_time:20165ms step_avg:91.66ms
step:221/1645 train_time:20257ms step_avg:91.66ms
step:222/1645 train_time:20348ms step_avg:91.66ms
step:223/1645 train_time:20440ms step_avg:91.66ms
step:224/1645 train_time:20532ms step_avg:91.66ms
step:225/1645 train_time:20622ms step_avg:91.65ms
step:226/1645 train_time:20713ms step_avg:91.65ms
step:227/1645 train_time:20804ms step_avg:91.65ms
step:228/1645 train_time:20896ms step_avg:91.65ms
step:229/1645 train_time:20988ms step_avg:91.65ms
step:230/1645 train_time:21083ms step_avg:91.67ms
step:231/1645 train_time:21177ms step_avg:91.67ms
step:232/1645 train_time:21267ms step_avg:91.67ms
step:233/1645 train_time:21358ms step_avg:91.67ms
step:234/1645 train_time:21450ms step_avg:91.67ms
step:235/1645 train_time:21542ms step_avg:91.67ms
step:236/1645 train_time:21633ms step_avg:91.66ms
step:237/1645 train_time:21724ms step_avg:91.66ms
step:238/1645 train_time:21815ms step_avg:91.66ms
step:239/1645 train_time:21906ms step_avg:91.66ms
step:240/1645 train_time:21998ms step_avg:91.66ms
step:241/1645 train_time:22089ms step_avg:91.66ms
step:242/1645 train_time:22183ms step_avg:91.67ms
step:243/1645 train_time:22276ms step_avg:91.67ms
step:244/1645 train_time:22367ms step_avg:91.67ms
step:245/1645 train_time:22460ms step_avg:91.67ms
step:246/1645 train_time:22551ms step_avg:91.67ms
step:247/1645 train_time:22642ms step_avg:91.67ms
step:248/1645 train_time:22734ms step_avg:91.67ms
step:249/1645 train_time:22824ms step_avg:91.66ms
step:250/1645 train_time:22916ms step_avg:91.66ms
step:250/1645 val_loss:3.9739 train_time:23007ms step_avg:92.03ms
step:251/1645 train_time:23027ms step_avg:91.74ms
step:252/1645 train_time:23103ms step_avg:91.68ms
step:253/1645 train_time:23197ms step_avg:91.69ms
step:254/1645 train_time:23290ms step_avg:91.69ms
step:255/1645 train_time:23380ms step_avg:91.69ms
step:256/1645 train_time:23471ms step_avg:91.68ms
step:257/1645 train_time:23561ms step_avg:91.68ms
step:258/1645 train_time:23652ms step_avg:91.67ms
step:259/1645 train_time:23743ms step_avg:91.67ms
step:260/1645 train_time:23834ms step_avg:91.67ms
step:261/1645 train_time:23927ms step_avg:91.67ms
step:262/1645 train_time:24021ms step_avg:91.68ms
step:263/1645 train_time:24115ms step_avg:91.69ms
step:264/1645 train_time:24207ms step_avg:91.69ms
step:265/1645 train_time:24299ms step_avg:91.69ms
step:266/1645 train_time:24390ms step_avg:91.69ms
step:267/1645 train_time:24481ms step_avg:91.69ms
step:268/1645 train_time:24572ms step_avg:91.69ms
step:269/1645 train_time:24663ms step_avg:91.68ms
step:270/1645 train_time:24754ms step_avg:91.68ms
step:271/1645 train_time:24847ms step_avg:91.69ms
step:272/1645 train_time:24939ms step_avg:91.69ms
step:273/1645 train_time:25031ms step_avg:91.69ms
step:274/1645 train_time:25124ms step_avg:91.69ms
step:275/1645 train_time:25217ms step_avg:91.70ms
step:276/1645 train_time:25309ms step_avg:91.70ms
step:277/1645 train_time:25400ms step_avg:91.70ms
step:278/1645 train_time:25491ms step_avg:91.70ms
step:279/1645 train_time:25582ms step_avg:91.69ms
step:280/1645 train_time:25673ms step_avg:91.69ms
step:281/1645 train_time:25763ms step_avg:91.69ms
step:282/1645 train_time:25855ms step_avg:91.68ms
step:283/1645 train_time:25947ms step_avg:91.68ms
step:284/1645 train_time:26039ms step_avg:91.69ms
step:285/1645 train_time:26132ms step_avg:91.69ms
step:286/1645 train_time:26225ms step_avg:91.70ms
step:287/1645 train_time:26317ms step_avg:91.70ms
step:288/1645 train_time:26409ms step_avg:91.70ms
step:289/1645 train_time:26500ms step_avg:91.70ms
step:290/1645 train_time:26591ms step_avg:91.69ms
step:291/1645 train_time:26682ms step_avg:91.69ms
step:292/1645 train_time:26773ms step_avg:91.69ms
step:293/1645 train_time:26864ms step_avg:91.69ms
step:294/1645 train_time:26957ms step_avg:91.69ms
step:295/1645 train_time:27048ms step_avg:91.69ms
step:296/1645 train_time:27141ms step_avg:91.69ms
step:297/1645 train_time:27234ms step_avg:91.70ms
step:298/1645 train_time:27326ms step_avg:91.70ms
step:299/1645 train_time:27417ms step_avg:91.70ms
step:300/1645 train_time:27509ms step_avg:91.70ms
step:301/1645 train_time:27600ms step_avg:91.69ms
step:302/1645 train_time:27692ms step_avg:91.69ms
step:303/1645 train_time:27783ms step_avg:91.69ms
step:304/1645 train_time:27874ms step_avg:91.69ms
step:305/1645 train_time:27965ms step_avg:91.69ms
step:306/1645 train_time:28057ms step_avg:91.69ms
step:307/1645 train_time:28149ms step_avg:91.69ms
step:308/1645 train_time:28241ms step_avg:91.69ms
step:309/1645 train_time:28334ms step_avg:91.69ms
step:310/1645 train_time:28425ms step_avg:91.69ms
step:311/1645 train_time:28518ms step_avg:91.70ms
step:312/1645 train_time:28610ms step_avg:91.70ms
step:313/1645 train_time:28701ms step_avg:91.69ms
step:314/1645 train_time:28792ms step_avg:91.69ms
step:315/1645 train_time:28883ms step_avg:91.69ms
step:316/1645 train_time:28974ms step_avg:91.69ms
step:317/1645 train_time:29067ms step_avg:91.69ms
step:318/1645 train_time:29159ms step_avg:91.69ms
step:319/1645 train_time:29250ms step_avg:91.69ms
step:320/1645 train_time:29342ms step_avg:91.69ms
step:321/1645 train_time:29434ms step_avg:91.69ms
step:322/1645 train_time:29526ms step_avg:91.70ms
step:323/1645 train_time:29618ms step_avg:91.70ms
step:324/1645 train_time:29709ms step_avg:91.70ms
step:325/1645 train_time:29800ms step_avg:91.69ms
step:326/1645 train_time:29891ms step_avg:91.69ms
step:327/1645 train_time:29982ms step_avg:91.69ms
step:328/1645 train_time:30075ms step_avg:91.69ms
step:329/1645 train_time:30166ms step_avg:91.69ms
step:330/1645 train_time:30258ms step_avg:91.69ms
step:331/1645 train_time:30350ms step_avg:91.69ms
step:332/1645 train_time:30441ms step_avg:91.69ms
step:333/1645 train_time:30535ms step_avg:91.70ms
step:334/1645 train_time:30628ms step_avg:91.70ms
step:335/1645 train_time:30719ms step_avg:91.70ms
step:336/1645 train_time:30811ms step_avg:91.70ms
step:337/1645 train_time:30901ms step_avg:91.70ms
step:338/1645 train_time:30993ms step_avg:91.70ms
step:339/1645 train_time:31084ms step_avg:91.69ms
step:340/1645 train_time:31176ms step_avg:91.69ms
step:341/1645 train_time:31267ms step_avg:91.69ms
step:342/1645 train_time:31359ms step_avg:91.69ms
step:343/1645 train_time:31450ms step_avg:91.69ms
step:344/1645 train_time:31542ms step_avg:91.69ms
step:345/1645 train_time:31636ms step_avg:91.70ms
step:346/1645 train_time:31728ms step_avg:91.70ms
step:347/1645 train_time:31820ms step_avg:91.70ms
step:348/1645 train_time:31911ms step_avg:91.70ms
step:349/1645 train_time:32002ms step_avg:91.70ms
step:350/1645 train_time:32094ms step_avg:91.70ms
step:351/1645 train_time:32185ms step_avg:91.70ms
step:352/1645 train_time:32277ms step_avg:91.70ms
step:353/1645 train_time:32369ms step_avg:91.70ms
step:354/1645 train_time:32460ms step_avg:91.70ms
step:355/1645 train_time:32553ms step_avg:91.70ms
step:356/1645 train_time:32645ms step_avg:91.70ms
step:357/1645 train_time:32736ms step_avg:91.70ms
step:358/1645 train_time:32829ms step_avg:91.70ms
step:359/1645 train_time:32920ms step_avg:91.70ms
step:360/1645 train_time:33012ms step_avg:91.70ms
step:361/1645 train_time:33104ms step_avg:91.70ms
step:362/1645 train_time:33195ms step_avg:91.70ms
step:363/1645 train_time:33287ms step_avg:91.70ms
step:364/1645 train_time:33377ms step_avg:91.70ms
step:365/1645 train_time:33469ms step_avg:91.70ms
step:366/1645 train_time:33561ms step_avg:91.70ms
step:367/1645 train_time:33653ms step_avg:91.70ms
step:368/1645 train_time:33744ms step_avg:91.69ms
step:369/1645 train_time:33837ms step_avg:91.70ms
step:370/1645 train_time:33929ms step_avg:91.70ms
step:371/1645 train_time:34021ms step_avg:91.70ms
step:372/1645 train_time:34113ms step_avg:91.70ms
step:373/1645 train_time:34204ms step_avg:91.70ms
step:374/1645 train_time:34297ms step_avg:91.70ms
step:375/1645 train_time:34389ms step_avg:91.70ms
step:375/1645 val_loss:3.8159 train_time:34480ms step_avg:91.95ms
step:376/1645 train_time:34500ms step_avg:91.76ms
step:377/1645 train_time:34575ms step_avg:91.71ms
step:378/1645 train_time:34669ms step_avg:91.72ms
step:379/1645 train_time:34761ms step_avg:91.72ms
step:380/1645 train_time:34852ms step_avg:91.72ms
step:381/1645 train_time:34944ms step_avg:91.72ms
step:382/1645 train_time:35034ms step_avg:91.71ms
step:383/1645 train_time:35125ms step_avg:91.71ms
step:384/1645 train_time:35216ms step_avg:91.71ms
step:385/1645 train_time:35306ms step_avg:91.71ms
step:386/1645 train_time:35398ms step_avg:91.70ms
step:387/1645 train_time:35491ms step_avg:91.71ms
step:388/1645 train_time:35585ms step_avg:91.71ms
step:389/1645 train_time:35678ms step_avg:91.72ms
step:390/1645 train_time:35769ms step_avg:91.72ms
step:391/1645 train_time:35861ms step_avg:91.72ms
step:392/1645 train_time:35953ms step_avg:91.72ms
step:393/1645 train_time:36044ms step_avg:91.71ms
step:394/1645 train_time:36134ms step_avg:91.71ms
step:395/1645 train_time:36225ms step_avg:91.71ms
step:396/1645 train_time:36316ms step_avg:91.71ms
step:397/1645 train_time:36408ms step_avg:91.71ms
step:398/1645 train_time:36499ms step_avg:91.71ms
step:399/1645 train_time:36593ms step_avg:91.71ms
step:400/1645 train_time:36686ms step_avg:91.71ms
step:401/1645 train_time:36777ms step_avg:91.71ms
step:402/1645 train_time:36870ms step_avg:91.72ms
step:403/1645 train_time:36962ms step_avg:91.72ms
step:404/1645 train_time:37054ms step_avg:91.72ms
step:405/1645 train_time:37145ms step_avg:91.72ms
step:406/1645 train_time:37236ms step_avg:91.71ms
step:407/1645 train_time:37327ms step_avg:91.71ms
step:408/1645 train_time:37419ms step_avg:91.71ms
step:409/1645 train_time:37511ms step_avg:91.71ms
step:410/1645 train_time:37603ms step_avg:91.71ms
step:411/1645 train_time:37695ms step_avg:91.72ms
step:412/1645 train_time:37787ms step_avg:91.72ms
step:413/1645 train_time:37879ms step_avg:91.72ms
step:414/1645 train_time:37971ms step_avg:91.72ms
step:415/1645 train_time:38063ms step_avg:91.72ms
step:416/1645 train_time:38154ms step_avg:91.72ms
step:417/1645 train_time:38245ms step_avg:91.71ms
step:418/1645 train_time:38336ms step_avg:91.71ms
step:419/1645 train_time:38428ms step_avg:91.71ms
step:420/1645 train_time:38519ms step_avg:91.71ms
step:421/1645 train_time:38611ms step_avg:91.71ms
step:422/1645 train_time:38703ms step_avg:91.71ms
step:423/1645 train_time:38795ms step_avg:91.71ms
step:424/1645 train_time:38887ms step_avg:91.72ms
step:425/1645 train_time:38980ms step_avg:91.72ms
step:426/1645 train_time:39072ms step_avg:91.72ms
step:427/1645 train_time:39165ms step_avg:91.72ms
step:428/1645 train_time:39256ms step_avg:91.72ms
step:429/1645 train_time:39347ms step_avg:91.72ms
step:430/1645 train_time:39438ms step_avg:91.72ms
step:431/1645 train_time:39529ms step_avg:91.72ms
step:432/1645 train_time:39621ms step_avg:91.71ms
step:433/1645 train_time:39713ms step_avg:91.72ms
step:434/1645 train_time:39805ms step_avg:91.72ms
step:435/1645 train_time:39896ms step_avg:91.71ms
step:436/1645 train_time:39989ms step_avg:91.72ms
step:437/1645 train_time:40081ms step_avg:91.72ms
step:438/1645 train_time:40174ms step_avg:91.72ms
step:439/1645 train_time:40266ms step_avg:91.72ms
step:440/1645 train_time:40356ms step_avg:91.72ms
step:441/1645 train_time:40448ms step_avg:91.72ms
step:442/1645 train_time:40539ms step_avg:91.72ms
step:443/1645 train_time:40631ms step_avg:91.72ms
step:444/1645 train_time:40722ms step_avg:91.72ms
step:445/1645 train_time:40814ms step_avg:91.72ms
step:446/1645 train_time:40907ms step_avg:91.72ms
step:447/1645 train_time:40998ms step_avg:91.72ms
step:448/1645 train_time:41091ms step_avg:91.72ms
step:449/1645 train_time:41183ms step_avg:91.72ms
step:450/1645 train_time:41274ms step_avg:91.72ms
step:451/1645 train_time:41365ms step_avg:91.72ms
step:452/1645 train_time:41456ms step_avg:91.72ms
step:453/1645 train_time:41548ms step_avg:91.72ms
step:454/1645 train_time:41639ms step_avg:91.72ms
step:455/1645 train_time:41731ms step_avg:91.72ms
step:456/1645 train_time:41822ms step_avg:91.71ms
step:457/1645 train_time:41913ms step_avg:91.71ms
step:458/1645 train_time:42005ms step_avg:91.71ms
step:459/1645 train_time:42097ms step_avg:91.71ms
step:460/1645 train_time:42191ms step_avg:91.72ms
step:461/1645 train_time:42282ms step_avg:91.72ms
step:462/1645 train_time:42373ms step_avg:91.72ms
step:463/1645 train_time:42465ms step_avg:91.72ms
step:464/1645 train_time:42556ms step_avg:91.72ms
step:465/1645 train_time:42649ms step_avg:91.72ms
step:466/1645 train_time:42741ms step_avg:91.72ms
step:467/1645 train_time:42832ms step_avg:91.72ms
step:468/1645 train_time:42923ms step_avg:91.72ms
step:469/1645 train_time:43015ms step_avg:91.72ms
step:470/1645 train_time:43106ms step_avg:91.72ms
step:471/1645 train_time:43198ms step_avg:91.72ms
step:472/1645 train_time:43291ms step_avg:91.72ms
step:473/1645 train_time:43384ms step_avg:91.72ms
step:474/1645 train_time:43476ms step_avg:91.72ms
step:475/1645 train_time:43567ms step_avg:91.72ms
step:476/1645 train_time:43659ms step_avg:91.72ms
step:477/1645 train_time:43750ms step_avg:91.72ms
step:478/1645 train_time:43842ms step_avg:91.72ms
step:479/1645 train_time:43933ms step_avg:91.72ms
step:480/1645 train_time:44024ms step_avg:91.72ms
step:481/1645 train_time:44117ms step_avg:91.72ms
step:482/1645 train_time:44208ms step_avg:91.72ms
step:483/1645 train_time:44299ms step_avg:91.72ms
step:484/1645 train_time:44392ms step_avg:91.72ms
step:485/1645 train_time:44485ms step_avg:91.72ms
step:486/1645 train_time:44576ms step_avg:91.72ms
step:487/1645 train_time:44667ms step_avg:91.72ms
step:488/1645 train_time:44759ms step_avg:91.72ms
step:489/1645 train_time:44851ms step_avg:91.72ms
step:490/1645 train_time:44943ms step_avg:91.72ms
step:491/1645 train_time:45034ms step_avg:91.72ms
step:492/1645 train_time:45125ms step_avg:91.72ms
step:493/1645 train_time:45217ms step_avg:91.72ms
step:494/1645 train_time:45309ms step_avg:91.72ms
step:495/1645 train_time:45400ms step_avg:91.72ms
step:496/1645 train_time:45493ms step_avg:91.72ms
step:497/1645 train_time:45585ms step_avg:91.72ms
step:498/1645 train_time:45676ms step_avg:91.72ms
step:499/1645 train_time:45769ms step_avg:91.72ms
step:500/1645 train_time:45862ms step_avg:91.72ms
step:500/1645 val_loss:3.7151 train_time:45954ms step_avg:91.91ms
step:501/1645 train_time:45974ms step_avg:91.76ms
step:502/1645 train_time:46049ms step_avg:91.73ms
step:503/1645 train_time:46143ms step_avg:91.74ms
step:504/1645 train_time:46234ms step_avg:91.73ms
step:505/1645 train_time:46325ms step_avg:91.73ms
step:506/1645 train_time:46416ms step_avg:91.73ms
step:507/1645 train_time:46506ms step_avg:91.73ms
step:508/1645 train_time:46597ms step_avg:91.73ms
step:509/1645 train_time:46687ms step_avg:91.72ms
step:510/1645 train_time:46778ms step_avg:91.72ms
step:511/1645 train_time:46869ms step_avg:91.72ms
step:512/1645 train_time:46962ms step_avg:91.72ms
step:513/1645 train_time:47055ms step_avg:91.73ms
step:514/1645 train_time:47148ms step_avg:91.73ms
step:515/1645 train_time:47241ms step_avg:91.73ms
step:516/1645 train_time:47332ms step_avg:91.73ms
step:517/1645 train_time:47425ms step_avg:91.73ms
step:518/1645 train_time:47515ms step_avg:91.73ms
step:519/1645 train_time:47606ms step_avg:91.73ms
step:520/1645 train_time:47697ms step_avg:91.73ms
step:521/1645 train_time:47788ms step_avg:91.72ms
step:522/1645 train_time:47880ms step_avg:91.72ms
step:523/1645 train_time:47972ms step_avg:91.72ms
step:524/1645 train_time:48064ms step_avg:91.72ms
step:525/1645 train_time:48157ms step_avg:91.73ms
step:526/1645 train_time:48249ms step_avg:91.73ms
step:527/1645 train_time:48341ms step_avg:91.73ms
step:528/1645 train_time:48433ms step_avg:91.73ms
step:529/1645 train_time:48524ms step_avg:91.73ms
step:530/1645 train_time:48616ms step_avg:91.73ms
step:531/1645 train_time:48706ms step_avg:91.73ms
step:532/1645 train_time:48798ms step_avg:91.72ms
step:533/1645 train_time:48889ms step_avg:91.72ms
step:534/1645 train_time:48980ms step_avg:91.72ms
step:535/1645 train_time:49071ms step_avg:91.72ms
step:536/1645 train_time:49163ms step_avg:91.72ms
step:537/1645 train_time:49255ms step_avg:91.72ms
step:538/1645 train_time:49347ms step_avg:91.72ms
step:539/1645 train_time:49438ms step_avg:91.72ms
step:540/1645 train_time:49530ms step_avg:91.72ms
step:541/1645 train_time:49621ms step_avg:91.72ms
step:542/1645 train_time:49713ms step_avg:91.72ms
step:543/1645 train_time:49805ms step_avg:91.72ms
step:544/1645 train_time:49896ms step_avg:91.72ms
step:545/1645 train_time:49987ms step_avg:91.72ms
step:546/1645 train_time:50079ms step_avg:91.72ms
step:547/1645 train_time:50171ms step_avg:91.72ms
step:548/1645 train_time:50263ms step_avg:91.72ms
step:549/1645 train_time:50354ms step_avg:91.72ms
step:550/1645 train_time:50446ms step_avg:91.72ms
step:551/1645 train_time:50539ms step_avg:91.72ms
step:552/1645 train_time:50631ms step_avg:91.72ms
step:553/1645 train_time:50725ms step_avg:91.73ms
step:554/1645 train_time:50818ms step_avg:91.73ms
step:555/1645 train_time:50910ms step_avg:91.73ms
step:556/1645 train_time:51004ms step_avg:91.73ms
step:557/1645 train_time:51097ms step_avg:91.74ms
step:558/1645 train_time:51190ms step_avg:91.74ms
step:559/1645 train_time:51282ms step_avg:91.74ms
step:560/1645 train_time:51375ms step_avg:91.74ms
step:561/1645 train_time:51468ms step_avg:91.74ms
step:562/1645 train_time:51561ms step_avg:91.74ms
step:563/1645 train_time:51653ms step_avg:91.75ms
step:564/1645 train_time:51747ms step_avg:91.75ms
step:565/1645 train_time:51840ms step_avg:91.75ms
step:566/1645 train_time:51932ms step_avg:91.75ms
step:567/1645 train_time:52025ms step_avg:91.76ms
step:568/1645 train_time:52118ms step_avg:91.76ms
step:569/1645 train_time:52210ms step_avg:91.76ms
step:570/1645 train_time:52303ms step_avg:91.76ms
step:571/1645 train_time:52396ms step_avg:91.76ms
step:572/1645 train_time:52489ms step_avg:91.76ms
step:573/1645 train_time:52582ms step_avg:91.77ms
step:574/1645 train_time:52675ms step_avg:91.77ms
step:575/1645 train_time:52768ms step_avg:91.77ms
step:576/1645 train_time:52861ms step_avg:91.77ms
step:577/1645 train_time:52954ms step_avg:91.77ms
step:578/1645 train_time:53048ms step_avg:91.78ms
step:579/1645 train_time:53140ms step_avg:91.78ms
step:580/1645 train_time:53233ms step_avg:91.78ms
step:581/1645 train_time:53326ms step_avg:91.78ms
step:582/1645 train_time:53419ms step_avg:91.78ms
step:583/1645 train_time:53511ms step_avg:91.79ms
step:584/1645 train_time:53604ms step_avg:91.79ms
step:585/1645 train_time:53697ms step_avg:91.79ms
step:586/1645 train_time:53790ms step_avg:91.79ms
step:587/1645 train_time:53882ms step_avg:91.79ms
step:588/1645 train_time:53975ms step_avg:91.79ms
step:589/1645 train_time:54068ms step_avg:91.80ms
step:590/1645 train_time:54161ms step_avg:91.80ms
step:591/1645 train_time:54253ms step_avg:91.80ms
step:592/1645 train_time:54347ms step_avg:91.80ms
step:593/1645 train_time:54440ms step_avg:91.80ms
step:594/1645 train_time:54533ms step_avg:91.81ms
step:595/1645 train_time:54626ms step_avg:91.81ms
step:596/1645 train_time:54719ms step_avg:91.81ms
step:597/1645 train_time:54812ms step_avg:91.81ms
step:598/1645 train_time:54905ms step_avg:91.81ms
step:599/1645 train_time:54998ms step_avg:91.82ms
step:600/1645 train_time:55090ms step_avg:91.82ms
step:601/1645 train_time:55184ms step_avg:91.82ms
step:602/1645 train_time:55276ms step_avg:91.82ms
step:603/1645 train_time:55368ms step_avg:91.82ms
step:604/1645 train_time:55462ms step_avg:91.82ms
step:605/1645 train_time:55554ms step_avg:91.83ms
step:606/1645 train_time:55648ms step_avg:91.83ms
step:607/1645 train_time:55742ms step_avg:91.83ms
step:608/1645 train_time:55835ms step_avg:91.83ms
step:609/1645 train_time:55928ms step_avg:91.84ms
step:610/1645 train_time:56022ms step_avg:91.84ms
step:611/1645 train_time:56114ms step_avg:91.84ms
step:612/1645 train_time:56207ms step_avg:91.84ms
step:613/1645 train_time:56299ms step_avg:91.84ms
step:614/1645 train_time:56393ms step_avg:91.85ms
step:615/1645 train_time:56485ms step_avg:91.85ms
step:616/1645 train_time:56577ms step_avg:91.85ms
step:617/1645 train_time:56670ms step_avg:91.85ms
step:618/1645 train_time:56763ms step_avg:91.85ms
step:619/1645 train_time:56856ms step_avg:91.85ms
step:620/1645 train_time:56949ms step_avg:91.85ms
step:621/1645 train_time:57042ms step_avg:91.85ms
step:622/1645 train_time:57135ms step_avg:91.86ms
step:623/1645 train_time:57227ms step_avg:91.86ms
step:624/1645 train_time:57320ms step_avg:91.86ms
step:625/1645 train_time:57412ms step_avg:91.86ms
step:625/1645 val_loss:3.6132 train_time:57505ms step_avg:92.01ms
step:626/1645 train_time:57526ms step_avg:91.89ms
step:627/1645 train_time:57604ms step_avg:91.87ms
step:628/1645 train_time:57706ms step_avg:91.89ms
step:629/1645 train_time:57798ms step_avg:91.89ms
step:630/1645 train_time:57890ms step_avg:91.89ms
step:631/1645 train_time:57982ms step_avg:91.89ms
step:632/1645 train_time:58073ms step_avg:91.89ms
step:633/1645 train_time:58165ms step_avg:91.89ms
step:634/1645 train_time:58257ms step_avg:91.89ms
step:635/1645 train_time:58349ms step_avg:91.89ms
step:636/1645 train_time:58441ms step_avg:91.89ms
step:637/1645 train_time:58537ms step_avg:91.89ms
step:638/1645 train_time:58632ms step_avg:91.90ms
step:639/1645 train_time:58727ms step_avg:91.91ms
step:640/1645 train_time:58821ms step_avg:91.91ms
step:641/1645 train_time:58914ms step_avg:91.91ms
step:642/1645 train_time:59006ms step_avg:91.91ms
step:643/1645 train_time:59099ms step_avg:91.91ms
step:644/1645 train_time:59192ms step_avg:91.91ms
step:645/1645 train_time:59284ms step_avg:91.91ms
step:646/1645 train_time:59376ms step_avg:91.91ms
step:647/1645 train_time:59469ms step_avg:91.92ms
step:648/1645 train_time:59564ms step_avg:91.92ms
step:649/1645 train_time:59658ms step_avg:91.92ms
step:650/1645 train_time:59750ms step_avg:91.92ms
step:651/1645 train_time:59844ms step_avg:91.93ms
step:652/1645 train_time:59937ms step_avg:91.93ms
step:653/1645 train_time:60029ms step_avg:91.93ms
step:654/1645 train_time:60121ms step_avg:91.93ms
step:655/1645 train_time:60213ms step_avg:91.93ms
step:656/1645 train_time:60307ms step_avg:91.93ms
step:657/1645 train_time:60399ms step_avg:91.93ms
step:658/1645 train_time:60492ms step_avg:91.93ms
step:659/1645 train_time:60586ms step_avg:91.94ms
step:660/1645 train_time:60680ms step_avg:91.94ms
step:661/1645 train_time:60774ms step_avg:91.94ms
step:662/1645 train_time:60866ms step_avg:91.94ms
step:663/1645 train_time:60959ms step_avg:91.94ms
step:664/1645 train_time:61052ms step_avg:91.95ms
step:665/1645 train_time:61144ms step_avg:91.95ms
step:666/1645 train_time:61237ms step_avg:91.95ms
step:667/1645 train_time:61330ms step_avg:91.95ms
step:668/1645 train_time:61423ms step_avg:91.95ms
step:669/1645 train_time:61515ms step_avg:91.95ms
step:670/1645 train_time:61609ms step_avg:91.95ms
step:671/1645 train_time:61702ms step_avg:91.96ms
step:672/1645 train_time:61795ms step_avg:91.96ms
step:673/1645 train_time:61888ms step_avg:91.96ms
step:674/1645 train_time:61982ms step_avg:91.96ms
step:675/1645 train_time:62075ms step_avg:91.96ms
step:676/1645 train_time:62167ms step_avg:91.96ms
step:677/1645 train_time:62260ms step_avg:91.96ms
step:678/1645 train_time:62353ms step_avg:91.97ms
step:679/1645 train_time:62445ms step_avg:91.97ms
step:680/1645 train_time:62537ms step_avg:91.97ms
step:681/1645 train_time:62630ms step_avg:91.97ms
step:682/1645 train_time:62723ms step_avg:91.97ms
step:683/1645 train_time:62817ms step_avg:91.97ms
step:684/1645 train_time:62909ms step_avg:91.97ms
step:685/1645 train_time:63003ms step_avg:91.98ms
step:686/1645 train_time:63096ms step_avg:91.98ms
step:687/1645 train_time:63189ms step_avg:91.98ms
step:688/1645 train_time:63281ms step_avg:91.98ms
step:689/1645 train_time:63374ms step_avg:91.98ms
step:690/1645 train_time:63467ms step_avg:91.98ms
step:691/1645 train_time:63559ms step_avg:91.98ms
step:692/1645 train_time:63651ms step_avg:91.98ms
step:693/1645 train_time:63744ms step_avg:91.98ms
step:694/1645 train_time:63837ms step_avg:91.98ms
step:695/1645 train_time:63930ms step_avg:91.99ms
step:696/1645 train_time:64024ms step_avg:91.99ms
step:697/1645 train_time:64117ms step_avg:91.99ms
step:698/1645 train_time:64210ms step_avg:91.99ms
step:699/1645 train_time:64303ms step_avg:91.99ms
step:700/1645 train_time:64397ms step_avg:92.00ms
step:701/1645 train_time:64489ms step_avg:92.00ms
step:702/1645 train_time:64583ms step_avg:92.00ms
step:703/1645 train_time:64675ms step_avg:92.00ms
step:704/1645 train_time:64767ms step_avg:92.00ms
step:705/1645 train_time:64861ms step_avg:92.00ms
step:706/1645 train_time:64953ms step_avg:92.00ms
step:707/1645 train_time:65047ms step_avg:92.00ms
step:708/1645 train_time:65139ms step_avg:92.00ms
step:709/1645 train_time:65232ms step_avg:92.01ms
step:710/1645 train_time:65325ms step_avg:92.01ms
step:711/1645 train_time:65418ms step_avg:92.01ms
step:712/1645 train_time:65511ms step_avg:92.01ms
step:713/1645 train_time:65605ms step_avg:92.01ms
step:714/1645 train_time:65698ms step_avg:92.01ms
step:715/1645 train_time:65791ms step_avg:92.02ms
step:716/1645 train_time:65884ms step_avg:92.02ms
step:717/1645 train_time:65977ms step_avg:92.02ms
step:718/1645 train_time:66070ms step_avg:92.02ms
step:719/1645 train_time:66164ms step_avg:92.02ms
step:720/1645 train_time:66257ms step_avg:92.02ms
step:721/1645 train_time:66349ms step_avg:92.02ms
step:722/1645 train_time:66442ms step_avg:92.03ms
step:723/1645 train_time:66535ms step_avg:92.03ms
step:724/1645 train_time:66627ms step_avg:92.03ms
step:725/1645 train_time:66720ms step_avg:92.03ms
step:726/1645 train_time:66812ms step_avg:92.03ms
step:727/1645 train_time:66905ms step_avg:92.03ms
step:728/1645 train_time:67000ms step_avg:92.03ms
step:729/1645 train_time:67092ms step_avg:92.03ms
step:730/1645 train_time:67186ms step_avg:92.04ms
step:731/1645 train_time:67279ms step_avg:92.04ms
step:732/1645 train_time:67371ms step_avg:92.04ms
step:733/1645 train_time:67465ms step_avg:92.04ms
step:734/1645 train_time:67558ms step_avg:92.04ms
step:735/1645 train_time:67651ms step_avg:92.04ms
step:736/1645 train_time:67743ms step_avg:92.04ms
step:737/1645 train_time:67836ms step_avg:92.04ms
step:738/1645 train_time:67928ms step_avg:92.04ms
step:739/1645 train_time:68021ms step_avg:92.05ms
step:740/1645 train_time:68114ms step_avg:92.05ms
step:741/1645 train_time:68209ms step_avg:92.05ms
step:742/1645 train_time:68302ms step_avg:92.05ms
step:743/1645 train_time:68394ms step_avg:92.05ms
step:744/1645 train_time:68488ms step_avg:92.05ms
step:745/1645 train_time:68580ms step_avg:92.05ms
step:746/1645 train_time:68673ms step_avg:92.06ms
step:747/1645 train_time:68765ms step_avg:92.06ms
step:748/1645 train_time:68859ms step_avg:92.06ms
step:749/1645 train_time:68951ms step_avg:92.06ms
step:750/1645 train_time:69044ms step_avg:92.06ms
step:750/1645 val_loss:3.5603 train_time:69137ms step_avg:92.18ms
step:751/1645 train_time:69158ms step_avg:92.09ms
step:752/1645 train_time:69234ms step_avg:92.07ms
step:753/1645 train_time:69329ms step_avg:92.07ms
step:754/1645 train_time:69422ms step_avg:92.07ms
step:755/1645 train_time:69513ms step_avg:92.07ms
step:756/1645 train_time:69606ms step_avg:92.07ms
step:757/1645 train_time:69698ms step_avg:92.07ms
step:758/1645 train_time:69789ms step_avg:92.07ms
step:759/1645 train_time:69881ms step_avg:92.07ms
step:760/1645 train_time:69974ms step_avg:92.07ms
step:761/1645 train_time:70067ms step_avg:92.07ms
step:762/1645 train_time:70161ms step_avg:92.08ms
step:763/1645 train_time:70256ms step_avg:92.08ms
step:764/1645 train_time:70350ms step_avg:92.08ms
step:765/1645 train_time:70444ms step_avg:92.08ms
step:766/1645 train_time:70536ms step_avg:92.08ms
step:767/1645 train_time:70628ms step_avg:92.08ms
step:768/1645 train_time:70720ms step_avg:92.08ms
step:769/1645 train_time:70812ms step_avg:92.08ms
step:770/1645 train_time:70905ms step_avg:92.08ms
step:771/1645 train_time:70997ms step_avg:92.08ms
step:772/1645 train_time:71090ms step_avg:92.09ms
step:773/1645 train_time:71184ms step_avg:92.09ms
step:774/1645 train_time:71277ms step_avg:92.09ms
step:775/1645 train_time:71371ms step_avg:92.09ms
step:776/1645 train_time:71465ms step_avg:92.09ms
step:777/1645 train_time:71557ms step_avg:92.09ms
step:778/1645 train_time:71650ms step_avg:92.09ms
step:779/1645 train_time:71743ms step_avg:92.10ms
step:780/1645 train_time:71835ms step_avg:92.10ms
step:781/1645 train_time:71928ms step_avg:92.10ms
step:782/1645 train_time:72020ms step_avg:92.10ms
step:783/1645 train_time:72114ms step_avg:92.10ms
step:784/1645 train_time:72207ms step_avg:92.10ms
step:785/1645 train_time:72300ms step_avg:92.10ms
step:786/1645 train_time:72393ms step_avg:92.10ms
step:787/1645 train_time:72486ms step_avg:92.10ms
step:788/1645 train_time:72578ms step_avg:92.10ms
step:789/1645 train_time:72672ms step_avg:92.11ms
step:790/1645 train_time:72766ms step_avg:92.11ms
step:791/1645 train_time:72858ms step_avg:92.11ms
step:792/1645 train_time:72951ms step_avg:92.11ms
step:793/1645 train_time:73044ms step_avg:92.11ms
step:794/1645 train_time:73138ms step_avg:92.11ms
step:795/1645 train_time:73231ms step_avg:92.11ms
step:796/1645 train_time:73325ms step_avg:92.12ms
step:797/1645 train_time:73418ms step_avg:92.12ms
step:798/1645 train_time:73511ms step_avg:92.12ms
step:799/1645 train_time:73604ms step_avg:92.12ms
step:800/1645 train_time:73696ms step_avg:92.12ms
step:801/1645 train_time:73789ms step_avg:92.12ms
step:802/1645 train_time:73882ms step_avg:92.12ms
step:803/1645 train_time:73975ms step_avg:92.12ms
step:804/1645 train_time:74069ms step_avg:92.13ms
step:805/1645 train_time:74162ms step_avg:92.13ms
step:806/1645 train_time:74253ms step_avg:92.13ms
step:807/1645 train_time:74347ms step_avg:92.13ms
step:808/1645 train_time:74440ms step_avg:92.13ms
step:809/1645 train_time:74533ms step_avg:92.13ms
step:810/1645 train_time:74627ms step_avg:92.13ms
step:811/1645 train_time:74719ms step_avg:92.13ms
step:812/1645 train_time:74812ms step_avg:92.13ms
step:813/1645 train_time:74904ms step_avg:92.13ms
step:814/1645 train_time:74997ms step_avg:92.13ms
step:815/1645 train_time:75090ms step_avg:92.13ms
step:816/1645 train_time:75183ms step_avg:92.14ms
step:817/1645 train_time:75275ms step_avg:92.14ms
step:818/1645 train_time:75369ms step_avg:92.14ms
step:819/1645 train_time:75462ms step_avg:92.14ms
step:820/1645 train_time:75554ms step_avg:92.14ms
step:821/1645 train_time:75648ms step_avg:92.14ms
step:822/1645 train_time:75742ms step_avg:92.14ms
step:823/1645 train_time:75836ms step_avg:92.15ms
step:824/1645 train_time:75928ms step_avg:92.15ms
step:825/1645 train_time:76021ms step_avg:92.15ms
step:826/1645 train_time:76113ms step_avg:92.15ms
step:827/1645 train_time:76207ms step_avg:92.15ms
step:828/1645 train_time:76300ms step_avg:92.15ms
step:829/1645 train_time:76393ms step_avg:92.15ms
step:830/1645 train_time:76485ms step_avg:92.15ms
step:831/1645 train_time:76577ms step_avg:92.15ms
step:832/1645 train_time:76671ms step_avg:92.15ms
step:833/1645 train_time:76764ms step_avg:92.15ms
step:834/1645 train_time:76857ms step_avg:92.16ms
step:835/1645 train_time:76950ms step_avg:92.16ms
step:836/1645 train_time:77043ms step_avg:92.16ms
step:837/1645 train_time:77136ms step_avg:92.16ms
step:838/1645 train_time:77229ms step_avg:92.16ms
step:839/1645 train_time:77322ms step_avg:92.16ms
step:840/1645 train_time:77415ms step_avg:92.16ms
step:841/1645 train_time:77509ms step_avg:92.16ms
step:842/1645 train_time:77602ms step_avg:92.16ms
step:843/1645 train_time:77694ms step_avg:92.16ms
step:844/1645 train_time:77789ms step_avg:92.17ms
step:845/1645 train_time:77881ms step_avg:92.17ms
step:846/1645 train_time:77974ms step_avg:92.17ms
step:847/1645 train_time:78066ms step_avg:92.17ms
step:848/1645 train_time:78159ms step_avg:92.17ms
step:849/1645 train_time:78252ms step_avg:92.17ms
step:850/1645 train_time:78347ms step_avg:92.17ms
step:851/1645 train_time:78441ms step_avg:92.17ms
step:852/1645 train_time:78534ms step_avg:92.18ms
step:853/1645 train_time:78626ms step_avg:92.18ms
step:854/1645 train_time:78719ms step_avg:92.18ms
step:855/1645 train_time:78811ms step_avg:92.18ms
step:856/1645 train_time:78905ms step_avg:92.18ms
step:857/1645 train_time:78998ms step_avg:92.18ms
step:858/1645 train_time:79090ms step_avg:92.18ms
step:859/1645 train_time:79184ms step_avg:92.18ms
step:860/1645 train_time:79276ms step_avg:92.18ms
step:861/1645 train_time:79369ms step_avg:92.18ms
step:862/1645 train_time:79463ms step_avg:92.18ms
step:863/1645 train_time:79555ms step_avg:92.18ms
step:864/1645 train_time:79648ms step_avg:92.19ms
step:865/1645 train_time:79742ms step_avg:92.19ms
step:866/1645 train_time:79835ms step_avg:92.19ms
step:867/1645 train_time:79928ms step_avg:92.19ms
step:868/1645 train_time:80021ms step_avg:92.19ms
step:869/1645 train_time:80113ms step_avg:92.19ms
step:870/1645 train_time:80207ms step_avg:92.19ms
step:871/1645 train_time:80300ms step_avg:92.19ms
step:872/1645 train_time:80393ms step_avg:92.19ms
step:873/1645 train_time:80486ms step_avg:92.19ms
step:874/1645 train_time:80579ms step_avg:92.20ms
step:875/1645 train_time:80672ms step_avg:92.20ms
step:875/1645 val_loss:3.5135 train_time:80766ms step_avg:92.30ms
step:876/1645 train_time:80792ms step_avg:92.23ms
step:877/1645 train_time:80869ms step_avg:92.21ms
step:878/1645 train_time:80964ms step_avg:92.21ms
step:879/1645 train_time:81057ms step_avg:92.22ms
step:880/1645 train_time:81149ms step_avg:92.21ms
step:881/1645 train_time:81241ms step_avg:92.22ms
step:882/1645 train_time:81333ms step_avg:92.21ms
step:883/1645 train_time:81424ms step_avg:92.21ms
step:884/1645 train_time:81516ms step_avg:92.21ms
step:885/1645 train_time:81608ms step_avg:92.21ms
step:886/1645 train_time:81701ms step_avg:92.21ms
step:887/1645 train_time:81796ms step_avg:92.22ms
step:888/1645 train_time:81891ms step_avg:92.22ms
step:889/1645 train_time:81986ms step_avg:92.22ms
step:890/1645 train_time:82080ms step_avg:92.22ms
step:891/1645 train_time:82172ms step_avg:92.22ms
step:892/1645 train_time:82265ms step_avg:92.23ms
step:893/1645 train_time:82357ms step_avg:92.23ms
step:894/1645 train_time:82450ms step_avg:92.23ms
step:895/1645 train_time:82542ms step_avg:92.23ms
step:896/1645 train_time:82634ms step_avg:92.22ms
step:897/1645 train_time:82727ms step_avg:92.23ms
step:898/1645 train_time:82821ms step_avg:92.23ms
step:899/1645 train_time:82915ms step_avg:92.23ms
step:900/1645 train_time:83010ms step_avg:92.23ms
step:901/1645 train_time:83105ms step_avg:92.24ms
step:902/1645 train_time:83197ms step_avg:92.24ms
step:903/1645 train_time:83290ms step_avg:92.24ms
step:904/1645 train_time:83383ms step_avg:92.24ms
step:905/1645 train_time:83476ms step_avg:92.24ms
step:906/1645 train_time:83569ms step_avg:92.24ms
step:907/1645 train_time:83661ms step_avg:92.24ms
step:908/1645 train_time:83753ms step_avg:92.24ms
step:909/1645 train_time:83847ms step_avg:92.24ms
step:910/1645 train_time:83941ms step_avg:92.24ms
step:911/1645 train_time:84035ms step_avg:92.24ms
step:912/1645 train_time:84127ms step_avg:92.24ms
step:913/1645 train_time:84219ms step_avg:92.24ms
step:914/1645 train_time:84312ms step_avg:92.24ms
step:915/1645 train_time:84405ms step_avg:92.25ms
step:916/1645 train_time:84498ms step_avg:92.25ms
step:917/1645 train_time:84590ms step_avg:92.25ms
step:918/1645 train_time:84683ms step_avg:92.25ms
step:919/1645 train_time:84776ms step_avg:92.25ms
step:920/1645 train_time:84870ms step_avg:92.25ms
step:921/1645 train_time:84964ms step_avg:92.25ms
step:922/1645 train_time:85059ms step_avg:92.25ms
step:923/1645 train_time:85150ms step_avg:92.25ms
step:924/1645 train_time:85243ms step_avg:92.25ms
step:925/1645 train_time:85336ms step_avg:92.26ms
step:926/1645 train_time:85429ms step_avg:92.26ms
step:927/1645 train_time:85521ms step_avg:92.26ms
step:928/1645 train_time:85614ms step_avg:92.26ms
step:929/1645 train_time:85707ms step_avg:92.26ms
step:930/1645 train_time:85800ms step_avg:92.26ms
step:931/1645 train_time:85892ms step_avg:92.26ms
step:932/1645 train_time:85987ms step_avg:92.26ms
step:933/1645 train_time:86081ms step_avg:92.26ms
step:934/1645 train_time:86174ms step_avg:92.26ms
step:935/1645 train_time:86267ms step_avg:92.26ms
step:936/1645 train_time:86360ms step_avg:92.27ms
step:937/1645 train_time:86453ms step_avg:92.27ms
step:938/1645 train_time:86546ms step_avg:92.27ms
step:939/1645 train_time:86639ms step_avg:92.27ms
step:940/1645 train_time:86731ms step_avg:92.27ms
step:941/1645 train_time:86825ms step_avg:92.27ms
step:942/1645 train_time:86917ms step_avg:92.27ms
step:943/1645 train_time:87010ms step_avg:92.27ms
step:944/1645 train_time:87104ms step_avg:92.27ms
step:945/1645 train_time:87197ms step_avg:92.27ms
step:946/1645 train_time:87290ms step_avg:92.27ms
step:947/1645 train_time:87384ms step_avg:92.27ms
step:948/1645 train_time:87477ms step_avg:92.28ms
step:949/1645 train_time:87569ms step_avg:92.28ms
step:950/1645 train_time:87662ms step_avg:92.28ms
step:951/1645 train_time:87755ms step_avg:92.28ms
step:952/1645 train_time:87848ms step_avg:92.28ms
step:953/1645 train_time:87941ms step_avg:92.28ms
step:954/1645 train_time:88034ms step_avg:92.28ms
step:955/1645 train_time:88127ms step_avg:92.28ms
step:956/1645 train_time:88220ms step_avg:92.28ms
step:957/1645 train_time:88312ms step_avg:92.28ms
step:958/1645 train_time:88406ms step_avg:92.28ms
step:959/1645 train_time:88499ms step_avg:92.28ms
step:960/1645 train_time:88593ms step_avg:92.28ms
step:961/1645 train_time:88685ms step_avg:92.28ms
step:962/1645 train_time:88779ms step_avg:92.29ms
step:963/1645 train_time:88871ms step_avg:92.29ms
step:964/1645 train_time:88964ms step_avg:92.29ms
step:965/1645 train_time:89057ms step_avg:92.29ms
step:966/1645 train_time:89150ms step_avg:92.29ms
step:967/1645 train_time:89243ms step_avg:92.29ms
step:968/1645 train_time:89337ms step_avg:92.29ms
step:969/1645 train_time:89429ms step_avg:92.29ms
step:970/1645 train_time:89522ms step_avg:92.29ms
step:971/1645 train_time:89615ms step_avg:92.29ms
step:972/1645 train_time:89708ms step_avg:92.29ms
step:973/1645 train_time:89802ms step_avg:92.29ms
step:974/1645 train_time:89894ms step_avg:92.29ms
step:975/1645 train_time:89987ms step_avg:92.29ms
step:976/1645 train_time:90081ms step_avg:92.30ms
step:977/1645 train_time:90173ms step_avg:92.30ms
step:978/1645 train_time:90266ms step_avg:92.30ms
step:979/1645 train_time:90359ms step_avg:92.30ms
step:980/1645 train_time:90452ms step_avg:92.30ms
step:981/1645 train_time:90545ms step_avg:92.30ms
step:982/1645 train_time:90638ms step_avg:92.30ms
step:983/1645 train_time:90731ms step_avg:92.30ms
step:984/1645 train_time:90824ms step_avg:92.30ms
step:985/1645 train_time:90917ms step_avg:92.30ms
step:986/1645 train_time:91009ms step_avg:92.30ms
step:987/1645 train_time:91103ms step_avg:92.30ms
step:988/1645 train_time:91196ms step_avg:92.30ms
step:989/1645 train_time:91290ms step_avg:92.31ms
step:990/1645 train_time:91384ms step_avg:92.31ms
step:991/1645 train_time:91476ms step_avg:92.31ms
step:992/1645 train_time:91569ms step_avg:92.31ms
step:993/1645 train_time:91661ms step_avg:92.31ms
step:994/1645 train_time:91754ms step_avg:92.31ms
step:995/1645 train_time:91847ms step_avg:92.31ms
step:996/1645 train_time:91940ms step_avg:92.31ms
step:997/1645 train_time:92032ms step_avg:92.31ms
step:998/1645 train_time:92125ms step_avg:92.31ms
step:999/1645 train_time:92218ms step_avg:92.31ms
step:1000/1645 train_time:92311ms step_avg:92.31ms
step:1000/1645 val_loss:3.4640 train_time:92404ms step_avg:92.40ms
step:1001/1645 train_time:92430ms step_avg:92.34ms
step:1002/1645 train_time:92504ms step_avg:92.32ms
step:1003/1645 train_time:92600ms step_avg:92.32ms
step:1004/1645 train_time:92693ms step_avg:92.32ms
step:1005/1645 train_time:92784ms step_avg:92.32ms
step:1006/1645 train_time:92876ms step_avg:92.32ms
step:1007/1645 train_time:92968ms step_avg:92.32ms
step:1008/1645 train_time:93059ms step_avg:92.32ms
step:1009/1645 train_time:93151ms step_avg:92.32ms
step:1010/1645 train_time:93244ms step_avg:92.32ms
step:1011/1645 train_time:93336ms step_avg:92.32ms
step:1012/1645 train_time:93432ms step_avg:92.32ms
step:1013/1645 train_time:93526ms step_avg:92.33ms
step:1014/1645 train_time:93621ms step_avg:92.33ms
step:1015/1645 train_time:93714ms step_avg:92.33ms
step:1016/1645 train_time:93806ms step_avg:92.33ms
step:1017/1645 train_time:93899ms step_avg:92.33ms
step:1018/1645 train_time:93991ms step_avg:92.33ms
step:1019/1645 train_time:94083ms step_avg:92.33ms
step:1020/1645 train_time:94175ms step_avg:92.33ms
step:1021/1645 train_time:94267ms step_avg:92.33ms
step:1022/1645 train_time:94360ms step_avg:92.33ms
step:1023/1645 train_time:94454ms step_avg:92.33ms
step:1024/1645 train_time:94547ms step_avg:92.33ms
step:1025/1645 train_time:94640ms step_avg:92.33ms
step:1026/1645 train_time:94733ms step_avg:92.33ms
step:1027/1645 train_time:94826ms step_avg:92.33ms
step:1028/1645 train_time:94918ms step_avg:92.33ms
step:1029/1645 train_time:95010ms step_avg:92.33ms
step:1030/1645 train_time:95103ms step_avg:92.33ms
step:1031/1645 train_time:95196ms step_avg:92.33ms
step:1032/1645 train_time:95288ms step_avg:92.33ms
step:1033/1645 train_time:95381ms step_avg:92.33ms
step:1034/1645 train_time:95474ms step_avg:92.33ms
step:1035/1645 train_time:95567ms step_avg:92.34ms
step:1036/1645 train_time:95661ms step_avg:92.34ms
step:1037/1645 train_time:95753ms step_avg:92.34ms
step:1038/1645 train_time:95846ms step_avg:92.34ms
step:1039/1645 train_time:95939ms step_avg:92.34ms
step:1040/1645 train_time:96032ms step_avg:92.34ms
step:1041/1645 train_time:96124ms step_avg:92.34ms
step:1042/1645 train_time:96217ms step_avg:92.34ms
step:1043/1645 train_time:96310ms step_avg:92.34ms
step:1044/1645 train_time:96403ms step_avg:92.34ms
step:1045/1645 train_time:96496ms step_avg:92.34ms
step:1046/1645 train_time:96589ms step_avg:92.34ms
step:1047/1645 train_time:96682ms step_avg:92.34ms
step:1048/1645 train_time:96775ms step_avg:92.34ms
step:1049/1645 train_time:96868ms step_avg:92.34ms
step:1050/1645 train_time:96961ms step_avg:92.34ms
step:1051/1645 train_time:97053ms step_avg:92.34ms
step:1052/1645 train_time:97146ms step_avg:92.34ms
step:1053/1645 train_time:97238ms step_avg:92.34ms
step:1054/1645 train_time:97331ms step_avg:92.34ms
step:1055/1645 train_time:97424ms step_avg:92.35ms
step:1056/1645 train_time:97518ms step_avg:92.35ms
step:1057/1645 train_time:97611ms step_avg:92.35ms
step:1058/1645 train_time:97704ms step_avg:92.35ms
step:1059/1645 train_time:97796ms step_avg:92.35ms
step:1060/1645 train_time:97889ms step_avg:92.35ms
step:1061/1645 train_time:97983ms step_avg:92.35ms
step:1062/1645 train_time:98076ms step_avg:92.35ms
step:1063/1645 train_time:98168ms step_avg:92.35ms
step:1064/1645 train_time:98260ms step_avg:92.35ms
step:1065/1645 train_time:98354ms step_avg:92.35ms
step:1066/1645 train_time:98446ms step_avg:92.35ms
step:1067/1645 train_time:98539ms step_avg:92.35ms
step:1068/1645 train_time:98632ms step_avg:92.35ms
step:1069/1645 train_time:98726ms step_avg:92.35ms
step:1070/1645 train_time:98820ms step_avg:92.35ms
step:1071/1645 train_time:98913ms step_avg:92.36ms
step:1072/1645 train_time:99006ms step_avg:92.36ms
step:1073/1645 train_time:99098ms step_avg:92.36ms
step:1074/1645 train_time:99191ms step_avg:92.36ms
step:1075/1645 train_time:99284ms step_avg:92.36ms
step:1076/1645 train_time:99376ms step_avg:92.36ms
step:1077/1645 train_time:99469ms step_avg:92.36ms
step:1078/1645 train_time:99561ms step_avg:92.36ms
step:1079/1645 train_time:99654ms step_avg:92.36ms
step:1080/1645 train_time:99747ms step_avg:92.36ms
step:1081/1645 train_time:99840ms step_avg:92.36ms
step:1082/1645 train_time:99933ms step_avg:92.36ms
step:1083/1645 train_time:100026ms step_avg:92.36ms
step:1084/1645 train_time:100120ms step_avg:92.36ms
step:1085/1645 train_time:100212ms step_avg:92.36ms
step:1086/1645 train_time:100304ms step_avg:92.36ms
step:1087/1645 train_time:100397ms step_avg:92.36ms
step:1088/1645 train_time:100490ms step_avg:92.36ms
step:1089/1645 train_time:100583ms step_avg:92.36ms
step:1090/1645 train_time:100676ms step_avg:92.36ms
step:1091/1645 train_time:100769ms step_avg:92.36ms
step:1092/1645 train_time:100864ms step_avg:92.37ms
step:1093/1645 train_time:100956ms step_avg:92.37ms
step:1094/1645 train_time:101048ms step_avg:92.37ms
step:1095/1645 train_time:101141ms step_avg:92.37ms
step:1096/1645 train_time:101233ms step_avg:92.37ms
step:1097/1645 train_time:101327ms step_avg:92.37ms
step:1098/1645 train_time:101420ms step_avg:92.37ms
step:1099/1645 train_time:101512ms step_avg:92.37ms
step:1100/1645 train_time:101606ms step_avg:92.37ms
step:1101/1645 train_time:101700ms step_avg:92.37ms
step:1102/1645 train_time:101793ms step_avg:92.37ms
step:1103/1645 train_time:101887ms step_avg:92.37ms
step:1104/1645 train_time:101980ms step_avg:92.37ms
step:1105/1645 train_time:102074ms step_avg:92.37ms
step:1106/1645 train_time:102167ms step_avg:92.38ms
step:1107/1645 train_time:102260ms step_avg:92.38ms
step:1108/1645 train_time:102354ms step_avg:92.38ms
step:1109/1645 train_time:102447ms step_avg:92.38ms
step:1110/1645 train_time:102541ms step_avg:92.38ms
step:1111/1645 train_time:102633ms step_avg:92.38ms
step:1112/1645 train_time:102728ms step_avg:92.38ms
step:1113/1645 train_time:102823ms step_avg:92.38ms
step:1114/1645 train_time:102916ms step_avg:92.38ms
step:1115/1645 train_time:103009ms step_avg:92.38ms
step:1116/1645 train_time:103103ms step_avg:92.39ms
step:1117/1645 train_time:103196ms step_avg:92.39ms
step:1118/1645 train_time:103289ms step_avg:92.39ms
step:1119/1645 train_time:103382ms step_avg:92.39ms
step:1120/1645 train_time:103476ms step_avg:92.39ms
step:1121/1645 train_time:103569ms step_avg:92.39ms
step:1122/1645 train_time:103663ms step_avg:92.39ms
step:1123/1645 train_time:103756ms step_avg:92.39ms
step:1124/1645 train_time:103850ms step_avg:92.39ms
step:1125/1645 train_time:103944ms step_avg:92.39ms
step:1125/1645 val_loss:3.4105 train_time:104037ms step_avg:92.48ms
step:1126/1645 train_time:104063ms step_avg:92.42ms
step:1127/1645 train_time:104142ms step_avg:92.41ms
step:1128/1645 train_time:104241ms step_avg:92.41ms
step:1129/1645 train_time:104336ms step_avg:92.41ms
step:1130/1645 train_time:104429ms step_avg:92.42ms
step:1131/1645 train_time:104521ms step_avg:92.41ms
step:1132/1645 train_time:104614ms step_avg:92.42ms
step:1133/1645 train_time:104706ms step_avg:92.42ms
step:1134/1645 train_time:104798ms step_avg:92.41ms
step:1135/1645 train_time:104891ms step_avg:92.41ms
step:1136/1645 train_time:104984ms step_avg:92.42ms
step:1137/1645 train_time:105078ms step_avg:92.42ms
step:1138/1645 train_time:105176ms step_avg:92.42ms
step:1139/1645 train_time:105272ms step_avg:92.42ms
step:1140/1645 train_time:105366ms step_avg:92.43ms
step:1141/1645 train_time:105459ms step_avg:92.43ms
step:1142/1645 train_time:105551ms step_avg:92.43ms
step:1143/1645 train_time:105645ms step_avg:92.43ms
step:1144/1645 train_time:105737ms step_avg:92.43ms
step:1145/1645 train_time:105830ms step_avg:92.43ms
step:1146/1645 train_time:105923ms step_avg:92.43ms
step:1147/1645 train_time:106016ms step_avg:92.43ms
step:1148/1645 train_time:106111ms step_avg:92.43ms
step:1149/1645 train_time:106207ms step_avg:92.43ms
step:1150/1645 train_time:106301ms step_avg:92.44ms
step:1151/1645 train_time:106395ms step_avg:92.44ms
step:1152/1645 train_time:106489ms step_avg:92.44ms
step:1153/1645 train_time:106581ms step_avg:92.44ms
step:1154/1645 train_time:106675ms step_avg:92.44ms
step:1155/1645 train_time:106768ms step_avg:92.44ms
step:1156/1645 train_time:106861ms step_avg:92.44ms
step:1157/1645 train_time:106953ms step_avg:92.44ms
step:1158/1645 train_time:107047ms step_avg:92.44ms
step:1159/1645 train_time:107141ms step_avg:92.44ms
step:1160/1645 train_time:107235ms step_avg:92.44ms
step:1161/1645 train_time:107330ms step_avg:92.45ms
step:1162/1645 train_time:107423ms step_avg:92.45ms
step:1163/1645 train_time:107516ms step_avg:92.45ms
step:1164/1645 train_time:107610ms step_avg:92.45ms
step:1165/1645 train_time:107703ms step_avg:92.45ms
step:1166/1645 train_time:107796ms step_avg:92.45ms
step:1167/1645 train_time:107889ms step_avg:92.45ms
step:1168/1645 train_time:107982ms step_avg:92.45ms
step:1169/1645 train_time:108076ms step_avg:92.45ms
step:1170/1645 train_time:108170ms step_avg:92.45ms
step:1171/1645 train_time:108265ms step_avg:92.45ms
step:1172/1645 train_time:108359ms step_avg:92.46ms
step:1173/1645 train_time:108453ms step_avg:92.46ms
step:1174/1645 train_time:108547ms step_avg:92.46ms
step:1175/1645 train_time:108639ms step_avg:92.46ms
step:1176/1645 train_time:108733ms step_avg:92.46ms
step:1177/1645 train_time:108826ms step_avg:92.46ms
step:1178/1645 train_time:108919ms step_avg:92.46ms
step:1179/1645 train_time:109013ms step_avg:92.46ms
step:1180/1645 train_time:109108ms step_avg:92.46ms
step:1181/1645 train_time:109202ms step_avg:92.47ms
step:1182/1645 train_time:109295ms step_avg:92.47ms
step:1183/1645 train_time:109389ms step_avg:92.47ms
step:1184/1645 train_time:109483ms step_avg:92.47ms
step:1185/1645 train_time:109576ms step_avg:92.47ms
step:1186/1645 train_time:109670ms step_avg:92.47ms
step:1187/1645 train_time:109763ms step_avg:92.47ms
step:1188/1645 train_time:109856ms step_avg:92.47ms
step:1189/1645 train_time:109950ms step_avg:92.47ms
step:1190/1645 train_time:110044ms step_avg:92.47ms
step:1191/1645 train_time:110138ms step_avg:92.47ms
step:1192/1645 train_time:110233ms step_avg:92.48ms
step:1193/1645 train_time:110326ms step_avg:92.48ms
step:1194/1645 train_time:110420ms step_avg:92.48ms
step:1195/1645 train_time:110513ms step_avg:92.48ms
step:1196/1645 train_time:110606ms step_avg:92.48ms
step:1197/1645 train_time:110700ms step_avg:92.48ms
step:1198/1645 train_time:110793ms step_avg:92.48ms
step:1199/1645 train_time:110887ms step_avg:92.48ms
step:1200/1645 train_time:110980ms step_avg:92.48ms
step:1201/1645 train_time:111073ms step_avg:92.48ms
step:1202/1645 train_time:111167ms step_avg:92.49ms
step:1203/1645 train_time:111261ms step_avg:92.49ms
step:1204/1645 train_time:111354ms step_avg:92.49ms
step:1205/1645 train_time:111448ms step_avg:92.49ms
step:1206/1645 train_time:111542ms step_avg:92.49ms
step:1207/1645 train_time:111635ms step_avg:92.49ms
step:1208/1645 train_time:111729ms step_avg:92.49ms
step:1209/1645 train_time:111822ms step_avg:92.49ms
step:1210/1645 train_time:111916ms step_avg:92.49ms
step:1211/1645 train_time:112010ms step_avg:92.49ms
step:1212/1645 train_time:112103ms step_avg:92.49ms
step:1213/1645 train_time:112197ms step_avg:92.50ms
step:1214/1645 train_time:112291ms step_avg:92.50ms
step:1215/1645 train_time:112385ms step_avg:92.50ms
step:1216/1645 train_time:112478ms step_avg:92.50ms
step:1217/1645 train_time:112573ms step_avg:92.50ms
step:1218/1645 train_time:112667ms step_avg:92.50ms
step:1219/1645 train_time:112761ms step_avg:92.50ms
step:1220/1645 train_time:112853ms step_avg:92.50ms
step:1221/1645 train_time:112946ms step_avg:92.50ms
step:1222/1645 train_time:113039ms step_avg:92.50ms
step:1223/1645 train_time:113133ms step_avg:92.50ms
step:1224/1645 train_time:113227ms step_avg:92.51ms
step:1225/1645 train_time:113321ms step_avg:92.51ms
step:1226/1645 train_time:113415ms step_avg:92.51ms
step:1227/1645 train_time:113508ms step_avg:92.51ms
step:1228/1645 train_time:113601ms step_avg:92.51ms
step:1229/1645 train_time:113695ms step_avg:92.51ms
step:1230/1645 train_time:113789ms step_avg:92.51ms
step:1231/1645 train_time:113882ms step_avg:92.51ms
step:1232/1645 train_time:113975ms step_avg:92.51ms
step:1233/1645 train_time:114070ms step_avg:92.51ms
step:1234/1645 train_time:114163ms step_avg:92.51ms
step:1235/1645 train_time:114257ms step_avg:92.52ms
step:1236/1645 train_time:114350ms step_avg:92.52ms
step:1237/1645 train_time:114444ms step_avg:92.52ms
step:1238/1645 train_time:114538ms step_avg:92.52ms
step:1239/1645 train_time:114632ms step_avg:92.52ms
step:1240/1645 train_time:114726ms step_avg:92.52ms
step:1241/1645 train_time:114820ms step_avg:92.52ms
step:1242/1645 train_time:114913ms step_avg:92.52ms
step:1243/1645 train_time:115006ms step_avg:92.52ms
step:1244/1645 train_time:115101ms step_avg:92.52ms
step:1245/1645 train_time:115194ms step_avg:92.53ms
step:1246/1645 train_time:115289ms step_avg:92.53ms
step:1247/1645 train_time:115382ms step_avg:92.53ms
step:1248/1645 train_time:115477ms step_avg:92.53ms
step:1249/1645 train_time:115570ms step_avg:92.53ms
step:1250/1645 train_time:115664ms step_avg:92.53ms
step:1250/1645 val_loss:3.3726 train_time:115758ms step_avg:92.61ms
step:1251/1645 train_time:115779ms step_avg:92.55ms
step:1252/1645 train_time:115859ms step_avg:92.54ms
step:1253/1645 train_time:115953ms step_avg:92.54ms
step:1254/1645 train_time:116046ms step_avg:92.54ms
step:1255/1645 train_time:116138ms step_avg:92.54ms
step:1256/1645 train_time:116230ms step_avg:92.54ms
step:1257/1645 train_time:116322ms step_avg:92.54ms
step:1258/1645 train_time:116414ms step_avg:92.54ms
step:1259/1645 train_time:116507ms step_avg:92.54ms
step:1260/1645 train_time:116602ms step_avg:92.54ms
step:1261/1645 train_time:116697ms step_avg:92.54ms
step:1262/1645 train_time:116794ms step_avg:92.55ms
step:1263/1645 train_time:116889ms step_avg:92.55ms
step:1264/1645 train_time:116985ms step_avg:92.55ms
step:1265/1645 train_time:117079ms step_avg:92.55ms
step:1266/1645 train_time:117171ms step_avg:92.55ms
step:1267/1645 train_time:117264ms step_avg:92.55ms
step:1268/1645 train_time:117357ms step_avg:92.55ms
step:1269/1645 train_time:117449ms step_avg:92.55ms
step:1270/1645 train_time:117542ms step_avg:92.55ms
step:1271/1645 train_time:117636ms step_avg:92.55ms
step:1272/1645 train_time:117730ms step_avg:92.56ms
step:1273/1645 train_time:117825ms step_avg:92.56ms
step:1274/1645 train_time:117920ms step_avg:92.56ms
step:1275/1645 train_time:118015ms step_avg:92.56ms
step:1276/1645 train_time:118108ms step_avg:92.56ms
step:1277/1645 train_time:118202ms step_avg:92.56ms
step:1278/1645 train_time:118295ms step_avg:92.56ms
step:1279/1645 train_time:118388ms step_avg:92.56ms
step:1280/1645 train_time:118481ms step_avg:92.56ms
step:1281/1645 train_time:118573ms step_avg:92.56ms
step:1282/1645 train_time:118667ms step_avg:92.56ms
step:1283/1645 train_time:118762ms step_avg:92.57ms
step:1284/1645 train_time:118856ms step_avg:92.57ms
step:1285/1645 train_time:118951ms step_avg:92.57ms
step:1286/1645 train_time:119045ms step_avg:92.57ms
step:1287/1645 train_time:119138ms step_avg:92.57ms
step:1288/1645 train_time:119232ms step_avg:92.57ms
step:1289/1645 train_time:119325ms step_avg:92.57ms
step:1290/1645 train_time:119419ms step_avg:92.57ms
step:1291/1645 train_time:119512ms step_avg:92.57ms
step:1292/1645 train_time:119607ms step_avg:92.57ms
step:1293/1645 train_time:119701ms step_avg:92.58ms
step:1294/1645 train_time:119797ms step_avg:92.58ms
step:1295/1645 train_time:119891ms step_avg:92.58ms
step:1296/1645 train_time:119985ms step_avg:92.58ms
step:1297/1645 train_time:120079ms step_avg:92.58ms
step:1298/1645 train_time:120172ms step_avg:92.58ms
step:1299/1645 train_time:120265ms step_avg:92.58ms
step:1300/1645 train_time:120359ms step_avg:92.58ms
step:1301/1645 train_time:120452ms step_avg:92.58ms
step:1302/1645 train_time:120544ms step_avg:92.58ms
step:1303/1645 train_time:120638ms step_avg:92.58ms
step:1304/1645 train_time:120732ms step_avg:92.59ms
step:1305/1645 train_time:120825ms step_avg:92.59ms
step:1306/1645 train_time:120920ms step_avg:92.59ms
step:1307/1645 train_time:121013ms step_avg:92.59ms
step:1308/1645 train_time:121106ms step_avg:92.59ms
step:1309/1645 train_time:121200ms step_avg:92.59ms
step:1310/1645 train_time:121293ms step_avg:92.59ms
step:1311/1645 train_time:121387ms step_avg:92.59ms
step:1312/1645 train_time:121480ms step_avg:92.59ms
step:1313/1645 train_time:121574ms step_avg:92.59ms
step:1314/1645 train_time:121667ms step_avg:92.59ms
step:1315/1645 train_time:121761ms step_avg:92.59ms
step:1316/1645 train_time:121854ms step_avg:92.59ms
step:1317/1645 train_time:121948ms step_avg:92.59ms
step:1318/1645 train_time:122041ms step_avg:92.60ms
step:1319/1645 train_time:122135ms step_avg:92.60ms
step:1320/1645 train_time:122229ms step_avg:92.60ms
step:1321/1645 train_time:122323ms step_avg:92.60ms
step:1322/1645 train_time:122417ms step_avg:92.60ms
step:1323/1645 train_time:122510ms step_avg:92.60ms
step:1324/1645 train_time:122604ms step_avg:92.60ms
step:1325/1645 train_time:122699ms step_avg:92.60ms
step:1326/1645 train_time:122793ms step_avg:92.60ms
step:1327/1645 train_time:122885ms step_avg:92.60ms
step:1328/1645 train_time:122979ms step_avg:92.60ms
step:1329/1645 train_time:123073ms step_avg:92.61ms
step:1330/1645 train_time:123167ms step_avg:92.61ms
step:1331/1645 train_time:123261ms step_avg:92.61ms
step:1332/1645 train_time:123356ms step_avg:92.61ms
step:1333/1645 train_time:123449ms step_avg:92.61ms
step:1334/1645 train_time:123542ms step_avg:92.61ms
step:1335/1645 train_time:123636ms step_avg:92.61ms
step:1336/1645 train_time:123729ms step_avg:92.61ms
step:1337/1645 train_time:123823ms step_avg:92.61ms
step:1338/1645 train_time:123917ms step_avg:92.61ms
step:1339/1645 train_time:124010ms step_avg:92.61ms
step:1340/1645 train_time:124105ms step_avg:92.62ms
step:1341/1645 train_time:124200ms step_avg:92.62ms
step:1342/1645 train_time:124295ms step_avg:92.62ms
step:1343/1645 train_time:124388ms step_avg:92.62ms
step:1344/1645 train_time:124482ms step_avg:92.62ms
step:1345/1645 train_time:124575ms step_avg:92.62ms
step:1346/1645 train_time:124668ms step_avg:92.62ms
step:1347/1645 train_time:124761ms step_avg:92.62ms
step:1348/1645 train_time:124855ms step_avg:92.62ms
step:1349/1645 train_time:124949ms step_avg:92.62ms
step:1350/1645 train_time:125042ms step_avg:92.62ms
step:1351/1645 train_time:125136ms step_avg:92.62ms
step:1352/1645 train_time:125230ms step_avg:92.63ms
step:1353/1645 train_time:125324ms step_avg:92.63ms
step:1354/1645 train_time:125418ms step_avg:92.63ms
step:1355/1645 train_time:125512ms step_avg:92.63ms
step:1356/1645 train_time:125605ms step_avg:92.63ms
step:1357/1645 train_time:125699ms step_avg:92.63ms
step:1358/1645 train_time:125793ms step_avg:92.63ms
step:1359/1645 train_time:125887ms step_avg:92.63ms
step:1360/1645 train_time:125980ms step_avg:92.63ms
step:1361/1645 train_time:126074ms step_avg:92.63ms
step:1362/1645 train_time:126167ms step_avg:92.63ms
step:1363/1645 train_time:126262ms step_avg:92.64ms
step:1364/1645 train_time:126356ms step_avg:92.64ms
step:1365/1645 train_time:126449ms step_avg:92.64ms
step:1366/1645 train_time:126542ms step_avg:92.64ms
step:1367/1645 train_time:126635ms step_avg:92.64ms
step:1368/1645 train_time:126729ms step_avg:92.64ms
step:1369/1645 train_time:126822ms step_avg:92.64ms
step:1370/1645 train_time:126916ms step_avg:92.64ms
step:1371/1645 train_time:127010ms step_avg:92.64ms
step:1372/1645 train_time:127104ms step_avg:92.64ms
step:1373/1645 train_time:127199ms step_avg:92.64ms
step:1374/1645 train_time:127292ms step_avg:92.64ms
step:1375/1645 train_time:127386ms step_avg:92.64ms
step:1375/1645 val_loss:3.3375 train_time:127479ms step_avg:92.71ms
step:1376/1645 train_time:127505ms step_avg:92.66ms
step:1377/1645 train_time:127579ms step_avg:92.65ms
step:1378/1645 train_time:127675ms step_avg:92.65ms
step:1379/1645 train_time:127768ms step_avg:92.65ms
step:1380/1645 train_time:127861ms step_avg:92.65ms
step:1381/1645 train_time:127954ms step_avg:92.65ms
step:1382/1645 train_time:128045ms step_avg:92.65ms
step:1383/1645 train_time:128138ms step_avg:92.65ms
step:1384/1645 train_time:128231ms step_avg:92.65ms
step:1385/1645 train_time:128325ms step_avg:92.65ms
step:1386/1645 train_time:128419ms step_avg:92.65ms
step:1387/1645 train_time:128515ms step_avg:92.66ms
step:1388/1645 train_time:128611ms step_avg:92.66ms
step:1389/1645 train_time:128706ms step_avg:92.66ms
step:1390/1645 train_time:128800ms step_avg:92.66ms
step:1391/1645 train_time:128894ms step_avg:92.66ms
step:1392/1645 train_time:128988ms step_avg:92.66ms
step:1393/1645 train_time:129080ms step_avg:92.66ms
step:1394/1645 train_time:129173ms step_avg:92.66ms
step:1395/1645 train_time:129267ms step_avg:92.66ms
step:1396/1645 train_time:129360ms step_avg:92.66ms
step:1397/1645 train_time:129456ms step_avg:92.67ms
step:1398/1645 train_time:129551ms step_avg:92.67ms
step:1399/1645 train_time:129645ms step_avg:92.67ms
step:1400/1645 train_time:129739ms step_avg:92.67ms
step:1401/1645 train_time:129833ms step_avg:92.67ms
step:1402/1645 train_time:129926ms step_avg:92.67ms
step:1403/1645 train_time:130020ms step_avg:92.67ms
step:1404/1645 train_time:130113ms step_avg:92.67ms
step:1405/1645 train_time:130205ms step_avg:92.67ms
step:1406/1645 train_time:130298ms step_avg:92.67ms
step:1407/1645 train_time:130392ms step_avg:92.67ms
step:1408/1645 train_time:130486ms step_avg:92.67ms
step:1409/1645 train_time:130580ms step_avg:92.68ms
step:1410/1645 train_time:130674ms step_avg:92.68ms
step:1411/1645 train_time:130768ms step_avg:92.68ms
step:1412/1645 train_time:130862ms step_avg:92.68ms
step:1413/1645 train_time:130955ms step_avg:92.68ms
step:1414/1645 train_time:131049ms step_avg:92.68ms
step:1415/1645 train_time:131142ms step_avg:92.68ms
step:1416/1645 train_time:131236ms step_avg:92.68ms
step:1417/1645 train_time:131329ms step_avg:92.68ms
step:1418/1645 train_time:131422ms step_avg:92.68ms
step:1419/1645 train_time:131516ms step_avg:92.68ms
step:1420/1645 train_time:131610ms step_avg:92.68ms
step:1421/1645 train_time:131703ms step_avg:92.68ms
step:1422/1645 train_time:131797ms step_avg:92.68ms
step:1423/1645 train_time:131891ms step_avg:92.69ms
step:1424/1645 train_time:131984ms step_avg:92.69ms
step:1425/1645 train_time:132077ms step_avg:92.69ms
step:1426/1645 train_time:132172ms step_avg:92.69ms
step:1427/1645 train_time:132266ms step_avg:92.69ms
step:1428/1645 train_time:132360ms step_avg:92.69ms
step:1429/1645 train_time:132454ms step_avg:92.69ms
step:1430/1645 train_time:132548ms step_avg:92.69ms
step:1431/1645 train_time:132641ms step_avg:92.69ms
step:1432/1645 train_time:132735ms step_avg:92.69ms
step:1433/1645 train_time:132829ms step_avg:92.69ms
step:1434/1645 train_time:132923ms step_avg:92.69ms
step:1435/1645 train_time:133018ms step_avg:92.70ms
step:1436/1645 train_time:133110ms step_avg:92.69ms
step:1437/1645 train_time:133204ms step_avg:92.70ms
step:1438/1645 train_time:133296ms step_avg:92.70ms
step:1439/1645 train_time:133391ms step_avg:92.70ms
step:1440/1645 train_time:133484ms step_avg:92.70ms
step:1441/1645 train_time:133577ms step_avg:92.70ms
step:1442/1645 train_time:133671ms step_avg:92.70ms
step:1443/1645 train_time:133765ms step_avg:92.70ms
step:1444/1645 train_time:133859ms step_avg:92.70ms
step:1445/1645 train_time:133953ms step_avg:92.70ms
step:1446/1645 train_time:134047ms step_avg:92.70ms
step:1447/1645 train_time:134140ms step_avg:92.70ms
step:1448/1645 train_time:134234ms step_avg:92.70ms
step:1449/1645 train_time:134328ms step_avg:92.70ms
step:1450/1645 train_time:134422ms step_avg:92.70ms
step:1451/1645 train_time:134516ms step_avg:92.71ms
step:1452/1645 train_time:134610ms step_avg:92.71ms
step:1453/1645 train_time:134704ms step_avg:92.71ms
step:1454/1645 train_time:134798ms step_avg:92.71ms
step:1455/1645 train_time:134893ms step_avg:92.71ms
step:1456/1645 train_time:134987ms step_avg:92.71ms
step:1457/1645 train_time:135080ms step_avg:92.71ms
step:1458/1645 train_time:135172ms step_avg:92.71ms
step:1459/1645 train_time:135266ms step_avg:92.71ms
step:1460/1645 train_time:135360ms step_avg:92.71ms
step:1461/1645 train_time:135454ms step_avg:92.71ms
step:1462/1645 train_time:135548ms step_avg:92.71ms
step:1463/1645 train_time:135640ms step_avg:92.71ms
step:1464/1645 train_time:135735ms step_avg:92.72ms
step:1465/1645 train_time:135829ms step_avg:92.72ms
step:1466/1645 train_time:135924ms step_avg:92.72ms
step:1467/1645 train_time:136018ms step_avg:92.72ms
step:1468/1645 train_time:136111ms step_avg:92.72ms
step:1469/1645 train_time:136205ms step_avg:92.72ms
step:1470/1645 train_time:136298ms step_avg:92.72ms
step:1471/1645 train_time:136392ms step_avg:92.72ms
step:1472/1645 train_time:136486ms step_avg:92.72ms
step:1473/1645 train_time:136578ms step_avg:92.72ms
step:1474/1645 train_time:136672ms step_avg:92.72ms
step:1475/1645 train_time:136766ms step_avg:92.72ms
step:1476/1645 train_time:136860ms step_avg:92.72ms
step:1477/1645 train_time:136953ms step_avg:92.72ms
step:1478/1645 train_time:137046ms step_avg:92.72ms
step:1479/1645 train_time:137140ms step_avg:92.72ms
step:1480/1645 train_time:137233ms step_avg:92.73ms
step:1481/1645 train_time:137327ms step_avg:92.73ms
step:1482/1645 train_time:137421ms step_avg:92.73ms
step:1483/1645 train_time:137514ms step_avg:92.73ms
step:1484/1645 train_time:137608ms step_avg:92.73ms
step:1485/1645 train_time:137702ms step_avg:92.73ms
step:1486/1645 train_time:137795ms step_avg:92.73ms
step:1487/1645 train_time:137889ms step_avg:92.73ms
step:1488/1645 train_time:137983ms step_avg:92.73ms
step:1489/1645 train_time:138077ms step_avg:92.73ms
step:1490/1645 train_time:138171ms step_avg:92.73ms
step:1491/1645 train_time:138265ms step_avg:92.73ms
step:1492/1645 train_time:138358ms step_avg:92.73ms
step:1493/1645 train_time:138451ms step_avg:92.73ms
step:1494/1645 train_time:138545ms step_avg:92.73ms
step:1495/1645 train_time:138640ms step_avg:92.74ms
step:1496/1645 train_time:138733ms step_avg:92.74ms
step:1497/1645 train_time:138828ms step_avg:92.74ms
step:1498/1645 train_time:138921ms step_avg:92.74ms
step:1499/1645 train_time:139016ms step_avg:92.74ms
step:1500/1645 train_time:139109ms step_avg:92.74ms
step:1500/1645 val_loss:3.3082 train_time:139202ms step_avg:92.80ms
step:1501/1645 train_time:139227ms step_avg:92.76ms
step:1502/1645 train_time:139300ms step_avg:92.74ms
step:1503/1645 train_time:139394ms step_avg:92.74ms
step:1504/1645 train_time:139487ms step_avg:92.74ms
step:1505/1645 train_time:139580ms step_avg:92.74ms
step:1506/1645 train_time:139674ms step_avg:92.74ms
step:1507/1645 train_time:139766ms step_avg:92.74ms
step:1508/1645 train_time:139859ms step_avg:92.74ms
step:1509/1645 train_time:139952ms step_avg:92.75ms
step:1510/1645 train_time:140046ms step_avg:92.75ms
step:1511/1645 train_time:140141ms step_avg:92.75ms
step:1512/1645 train_time:140238ms step_avg:92.75ms
step:1513/1645 train_time:140332ms step_avg:92.75ms
step:1514/1645 train_time:140426ms step_avg:92.75ms
step:1515/1645 train_time:140519ms step_avg:92.75ms
step:1516/1645 train_time:140613ms step_avg:92.75ms
step:1517/1645 train_time:140705ms step_avg:92.75ms
step:1518/1645 train_time:140797ms step_avg:92.75ms
step:1519/1645 train_time:140891ms step_avg:92.75ms
step:1520/1645 train_time:140984ms step_avg:92.75ms
step:1521/1645 train_time:141078ms step_avg:92.75ms
step:1522/1645 train_time:141174ms step_avg:92.76ms
step:1523/1645 train_time:141268ms step_avg:92.76ms
step:1524/1645 train_time:141363ms step_avg:92.76ms
step:1525/1645 train_time:141456ms step_avg:92.76ms
step:1526/1645 train_time:141550ms step_avg:92.76ms
step:1527/1645 train_time:141643ms step_avg:92.76ms
step:1528/1645 train_time:141736ms step_avg:92.76ms
step:1529/1645 train_time:141828ms step_avg:92.76ms
step:1530/1645 train_time:141921ms step_avg:92.76ms
step:1531/1645 train_time:142016ms step_avg:92.76ms
step:1532/1645 train_time:142110ms step_avg:92.76ms
step:1533/1645 train_time:142203ms step_avg:92.76ms
step:1534/1645 train_time:142297ms step_avg:92.76ms
step:1535/1645 train_time:142393ms step_avg:92.76ms
step:1536/1645 train_time:142486ms step_avg:92.76ms
step:1537/1645 train_time:142580ms step_avg:92.77ms
step:1538/1645 train_time:142673ms step_avg:92.77ms
step:1539/1645 train_time:142766ms step_avg:92.77ms
step:1540/1645 train_time:142859ms step_avg:92.77ms
step:1541/1645 train_time:142953ms step_avg:92.77ms
step:1542/1645 train_time:143046ms step_avg:92.77ms
step:1543/1645 train_time:143140ms step_avg:92.77ms
step:1544/1645 train_time:143234ms step_avg:92.77ms
step:1545/1645 train_time:143328ms step_avg:92.77ms
step:1546/1645 train_time:143422ms step_avg:92.77ms
step:1547/1645 train_time:143517ms step_avg:92.77ms
step:1548/1645 train_time:143610ms step_avg:92.77ms
step:1549/1645 train_time:143704ms step_avg:92.77ms
step:1550/1645 train_time:143798ms step_avg:92.77ms
step:1551/1645 train_time:143892ms step_avg:92.77ms
step:1552/1645 train_time:143986ms step_avg:92.77ms
step:1553/1645 train_time:144079ms step_avg:92.77ms
step:1554/1645 train_time:144173ms step_avg:92.78ms
step:1555/1645 train_time:144267ms step_avg:92.78ms
step:1556/1645 train_time:144360ms step_avg:92.78ms
step:1557/1645 train_time:144455ms step_avg:92.78ms
step:1558/1645 train_time:144550ms step_avg:92.78ms
step:1559/1645 train_time:144643ms step_avg:92.78ms
step:1560/1645 train_time:144736ms step_avg:92.78ms
step:1561/1645 train_time:144830ms step_avg:92.78ms
step:1562/1645 train_time:144923ms step_avg:92.78ms
step:1563/1645 train_time:145017ms step_avg:92.78ms
step:1564/1645 train_time:145110ms step_avg:92.78ms
step:1565/1645 train_time:145204ms step_avg:92.78ms
step:1566/1645 train_time:145299ms step_avg:92.78ms
step:1567/1645 train_time:145393ms step_avg:92.78ms
step:1568/1645 train_time:145487ms step_avg:92.78ms
step:1569/1645 train_time:145581ms step_avg:92.79ms
step:1570/1645 train_time:145674ms step_avg:92.79ms
step:1571/1645 train_time:145767ms step_avg:92.79ms
step:1572/1645 train_time:145860ms step_avg:92.79ms
step:1573/1645 train_time:145955ms step_avg:92.79ms
step:1574/1645 train_time:146048ms step_avg:92.79ms
step:1575/1645 train_time:146142ms step_avg:92.79ms
step:1576/1645 train_time:146236ms step_avg:92.79ms
step:1577/1645 train_time:146330ms step_avg:92.79ms
step:1578/1645 train_time:146424ms step_avg:92.79ms
step:1579/1645 train_time:146518ms step_avg:92.79ms
step:1580/1645 train_time:146611ms step_avg:92.79ms
step:1581/1645 train_time:146704ms step_avg:92.79ms
step:1582/1645 train_time:146797ms step_avg:92.79ms
step:1583/1645 train_time:146891ms step_avg:92.79ms
step:1584/1645 train_time:146985ms step_avg:92.79ms
step:1585/1645 train_time:147077ms step_avg:92.79ms
step:1586/1645 train_time:147171ms step_avg:92.79ms
step:1587/1645 train_time:147264ms step_avg:92.79ms
step:1588/1645 train_time:147358ms step_avg:92.79ms
step:1589/1645 train_time:147452ms step_avg:92.80ms
step:1590/1645 train_time:147546ms step_avg:92.80ms
step:1591/1645 train_time:147640ms step_avg:92.80ms
step:1592/1645 train_time:147733ms step_avg:92.80ms
step:1593/1645 train_time:147827ms step_avg:92.80ms
step:1594/1645 train_time:147920ms step_avg:92.80ms
step:1595/1645 train_time:148014ms step_avg:92.80ms
step:1596/1645 train_time:148108ms step_avg:92.80ms
step:1597/1645 train_time:148202ms step_avg:92.80ms
step:1598/1645 train_time:148295ms step_avg:92.80ms
step:1599/1645 train_time:148389ms step_avg:92.80ms
step:1600/1645 train_time:148482ms step_avg:92.80ms
step:1601/1645 train_time:148576ms step_avg:92.80ms
step:1602/1645 train_time:148670ms step_avg:92.80ms
step:1603/1645 train_time:148762ms step_avg:92.80ms
step:1604/1645 train_time:148856ms step_avg:92.80ms
step:1605/1645 train_time:148950ms step_avg:92.80ms
step:1606/1645 train_time:149044ms step_avg:92.80ms
step:1607/1645 train_time:149137ms step_avg:92.80ms
step:1608/1645 train_time:149230ms step_avg:92.80ms
step:1609/1645 train_time:149324ms step_avg:92.81ms
step:1610/1645 train_time:149419ms step_avg:92.81ms
step:1611/1645 train_time:149512ms step_avg:92.81ms
step:1612/1645 train_time:149607ms step_avg:92.81ms
step:1613/1645 train_time:149700ms step_avg:92.81ms
step:1614/1645 train_time:149793ms step_avg:92.81ms
step:1615/1645 train_time:149888ms step_avg:92.81ms
step:1616/1645 train_time:149982ms step_avg:92.81ms
step:1617/1645 train_time:150076ms step_avg:92.81ms
step:1618/1645 train_time:150169ms step_avg:92.81ms
step:1619/1645 train_time:150263ms step_avg:92.81ms
step:1620/1645 train_time:150356ms step_avg:92.81ms
step:1621/1645 train_time:150450ms step_avg:92.81ms
step:1622/1645 train_time:150544ms step_avg:92.81ms
step:1623/1645 train_time:150638ms step_avg:92.81ms
step:1624/1645 train_time:150731ms step_avg:92.81ms
step:1625/1645 train_time:150825ms step_avg:92.82ms
step:1625/1645 val_loss:3.2844 train_time:150919ms step_avg:92.87ms
step:1626/1645 train_time:150939ms step_avg:92.83ms
step:1627/1645 train_time:151016ms step_avg:92.82ms
step:1628/1645 train_time:151113ms step_avg:92.82ms
step:1629/1645 train_time:151206ms step_avg:92.82ms
step:1630/1645 train_time:151299ms step_avg:92.82ms
step:1631/1645 train_time:151392ms step_avg:92.82ms
step:1632/1645 train_time:151485ms step_avg:92.82ms
step:1633/1645 train_time:151578ms step_avg:92.82ms
step:1634/1645 train_time:151671ms step_avg:92.82ms
step:1635/1645 train_time:151764ms step_avg:92.82ms
step:1636/1645 train_time:151859ms step_avg:92.82ms
step:1637/1645 train_time:151954ms step_avg:92.82ms
step:1638/1645 train_time:152049ms step_avg:92.83ms
step:1639/1645 train_time:152143ms step_avg:92.83ms
step:1640/1645 train_time:152237ms step_avg:92.83ms
step:1641/1645 train_time:152330ms step_avg:92.83ms
step:1642/1645 train_time:152423ms step_avg:92.83ms
step:1643/1645 train_time:152517ms step_avg:92.83ms
step:1644/1645 train_time:152610ms step_avg:92.83ms
step:1645/1645 train_time:152704ms step_avg:92.83ms
step:1645/1645 val_loss:3.2787 train_time:152798ms step_avg:92.89ms
peak memory allocated: 31659 MiB reserved: 46816 MiB

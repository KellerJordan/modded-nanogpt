import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:40:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1645 train_time:137ms step_avg:137.41ms
step:2/1645 train_time:159ms step_avg:79.52ms
step:3/1645 train_time:226ms step_avg:75.39ms
step:4/1645 train_time:316ms step_avg:78.97ms
step:5/1645 train_time:407ms step_avg:81.36ms
step:6/1645 train_time:497ms step_avg:82.90ms
step:7/1645 train_time:588ms step_avg:84.02ms
step:8/1645 train_time:680ms step_avg:84.96ms
step:9/1645 train_time:771ms step_avg:85.63ms
step:10/1645 train_time:861ms step_avg:86.13ms
step:11/1645 train_time:952ms step_avg:86.56ms
step:12/1645 train_time:1044ms step_avg:86.96ms
step:13/1645 train_time:1137ms step_avg:87.47ms
step:14/1645 train_time:1230ms step_avg:87.85ms
step:15/1645 train_time:1322ms step_avg:88.13ms
step:16/1645 train_time:1414ms step_avg:88.36ms
step:17/1645 train_time:1505ms step_avg:88.55ms
step:18/1645 train_time:1597ms step_avg:88.70ms
step:19/1645 train_time:1688ms step_avg:88.87ms
step:20/1645 train_time:1780ms step_avg:89.00ms
step:21/1645 train_time:1871ms step_avg:89.09ms
step:22/1645 train_time:1963ms step_avg:89.21ms
step:23/1645 train_time:2054ms step_avg:89.30ms
step:24/1645 train_time:2147ms step_avg:89.46ms
step:25/1645 train_time:2240ms step_avg:89.58ms
step:26/1645 train_time:2332ms step_avg:89.68ms
step:27/1645 train_time:2425ms step_avg:89.82ms
step:28/1645 train_time:2516ms step_avg:89.87ms
step:29/1645 train_time:2608ms step_avg:89.94ms
step:30/1645 train_time:2700ms step_avg:89.99ms
step:31/1645 train_time:2792ms step_avg:90.05ms
step:32/1645 train_time:2884ms step_avg:90.13ms
step:33/1645 train_time:2975ms step_avg:90.15ms
step:34/1645 train_time:3067ms step_avg:90.20ms
step:35/1645 train_time:3159ms step_avg:90.26ms
step:36/1645 train_time:3251ms step_avg:90.31ms
step:37/1645 train_time:3344ms step_avg:90.37ms
step:38/1645 train_time:3435ms step_avg:90.41ms
step:39/1645 train_time:3527ms step_avg:90.44ms
step:40/1645 train_time:3620ms step_avg:90.49ms
step:41/1645 train_time:3711ms step_avg:90.52ms
step:42/1645 train_time:3804ms step_avg:90.57ms
step:43/1645 train_time:3896ms step_avg:90.60ms
step:44/1645 train_time:3987ms step_avg:90.62ms
step:45/1645 train_time:4079ms step_avg:90.64ms
step:46/1645 train_time:4171ms step_avg:90.68ms
step:47/1645 train_time:4263ms step_avg:90.70ms
step:48/1645 train_time:4354ms step_avg:90.72ms
step:49/1645 train_time:4446ms step_avg:90.74ms
step:50/1645 train_time:4538ms step_avg:90.75ms
step:51/1645 train_time:4629ms step_avg:90.77ms
step:52/1645 train_time:4721ms step_avg:90.79ms
step:53/1645 train_time:4813ms step_avg:90.82ms
step:54/1645 train_time:4906ms step_avg:90.85ms
step:55/1645 train_time:4998ms step_avg:90.87ms
step:56/1645 train_time:5089ms step_avg:90.88ms
step:57/1645 train_time:5181ms step_avg:90.90ms
step:58/1645 train_time:5273ms step_avg:90.92ms
step:59/1645 train_time:5365ms step_avg:90.94ms
step:60/1645 train_time:5458ms step_avg:90.97ms
step:61/1645 train_time:5549ms step_avg:90.97ms
step:62/1645 train_time:5641ms step_avg:90.98ms
step:63/1645 train_time:5733ms step_avg:91.00ms
step:64/1645 train_time:5826ms step_avg:91.03ms
step:65/1645 train_time:5918ms step_avg:91.05ms
step:66/1645 train_time:6010ms step_avg:91.06ms
step:67/1645 train_time:6102ms step_avg:91.07ms
step:68/1645 train_time:6195ms step_avg:91.10ms
step:69/1645 train_time:6287ms step_avg:91.12ms
step:70/1645 train_time:6379ms step_avg:91.13ms
step:71/1645 train_time:6470ms step_avg:91.12ms
step:72/1645 train_time:6561ms step_avg:91.13ms
step:73/1645 train_time:6653ms step_avg:91.13ms
step:74/1645 train_time:6744ms step_avg:91.14ms
step:75/1645 train_time:6836ms step_avg:91.15ms
step:76/1645 train_time:6929ms step_avg:91.17ms
step:77/1645 train_time:7021ms step_avg:91.19ms
step:78/1645 train_time:7113ms step_avg:91.20ms
step:79/1645 train_time:7205ms step_avg:91.21ms
step:80/1645 train_time:7298ms step_avg:91.22ms
step:81/1645 train_time:7390ms step_avg:91.23ms
step:82/1645 train_time:7481ms step_avg:91.23ms
step:83/1645 train_time:7572ms step_avg:91.23ms
step:84/1645 train_time:7663ms step_avg:91.23ms
step:85/1645 train_time:7754ms step_avg:91.23ms
step:86/1645 train_time:7846ms step_avg:91.24ms
step:87/1645 train_time:7938ms step_avg:91.24ms
step:88/1645 train_time:8030ms step_avg:91.25ms
step:89/1645 train_time:8123ms step_avg:91.26ms
step:90/1645 train_time:8215ms step_avg:91.28ms
step:91/1645 train_time:8307ms step_avg:91.29ms
step:92/1645 train_time:8400ms step_avg:91.30ms
step:93/1645 train_time:8491ms step_avg:91.30ms
step:94/1645 train_time:8582ms step_avg:91.30ms
step:95/1645 train_time:8674ms step_avg:91.31ms
step:96/1645 train_time:8765ms step_avg:91.30ms
step:97/1645 train_time:8857ms step_avg:91.31ms
step:98/1645 train_time:8949ms step_avg:91.31ms
step:99/1645 train_time:9040ms step_avg:91.31ms
step:100/1645 train_time:9132ms step_avg:91.32ms
step:101/1645 train_time:9224ms step_avg:91.33ms
step:102/1645 train_time:9316ms step_avg:91.33ms
step:103/1645 train_time:9408ms step_avg:91.34ms
step:104/1645 train_time:9501ms step_avg:91.35ms
step:105/1645 train_time:9592ms step_avg:91.35ms
step:106/1645 train_time:9684ms step_avg:91.35ms
step:107/1645 train_time:9775ms step_avg:91.36ms
step:108/1645 train_time:9866ms step_avg:91.35ms
step:109/1645 train_time:9959ms step_avg:91.36ms
step:110/1645 train_time:10050ms step_avg:91.37ms
step:111/1645 train_time:10143ms step_avg:91.38ms
step:112/1645 train_time:10234ms step_avg:91.38ms
step:113/1645 train_time:10327ms step_avg:91.39ms
step:114/1645 train_time:10421ms step_avg:91.41ms
step:115/1645 train_time:10512ms step_avg:91.41ms
step:116/1645 train_time:10603ms step_avg:91.40ms
step:117/1645 train_time:10695ms step_avg:91.41ms
step:118/1645 train_time:10786ms step_avg:91.41ms
step:119/1645 train_time:10877ms step_avg:91.40ms
step:120/1645 train_time:10968ms step_avg:91.40ms
step:121/1645 train_time:11059ms step_avg:91.40ms
step:122/1645 train_time:11152ms step_avg:91.41ms
step:123/1645 train_time:11243ms step_avg:91.41ms
step:124/1645 train_time:11335ms step_avg:91.41ms
step:125/1645 train_time:11428ms step_avg:91.43ms
step:125/1645 val_loss:4.3128 train_time:11521ms step_avg:92.16ms
step:126/1645 train_time:11535ms step_avg:91.55ms
step:127/1645 train_time:11616ms step_avg:91.46ms
step:128/1645 train_time:11718ms step_avg:91.55ms
step:129/1645 train_time:11812ms step_avg:91.57ms
step:130/1645 train_time:11904ms step_avg:91.57ms
step:131/1645 train_time:11995ms step_avg:91.57ms
step:132/1645 train_time:12086ms step_avg:91.56ms
step:133/1645 train_time:12176ms step_avg:91.55ms
step:134/1645 train_time:12267ms step_avg:91.54ms
step:135/1645 train_time:12357ms step_avg:91.54ms
step:136/1645 train_time:12448ms step_avg:91.53ms
step:137/1645 train_time:12540ms step_avg:91.53ms
step:138/1645 train_time:12633ms step_avg:91.55ms
step:139/1645 train_time:12729ms step_avg:91.57ms
step:140/1645 train_time:12822ms step_avg:91.58ms
step:141/1645 train_time:12913ms step_avg:91.58ms
step:142/1645 train_time:13005ms step_avg:91.58ms
step:143/1645 train_time:13095ms step_avg:91.58ms
step:144/1645 train_time:13187ms step_avg:91.57ms
step:145/1645 train_time:13278ms step_avg:91.57ms
step:146/1645 train_time:13369ms step_avg:91.57ms
step:147/1645 train_time:13460ms step_avg:91.56ms
step:148/1645 train_time:13552ms step_avg:91.56ms
step:149/1645 train_time:13645ms step_avg:91.58ms
step:150/1645 train_time:13738ms step_avg:91.59ms
step:151/1645 train_time:13831ms step_avg:91.60ms
step:152/1645 train_time:13924ms step_avg:91.60ms
step:153/1645 train_time:14014ms step_avg:91.60ms
step:154/1645 train_time:14106ms step_avg:91.60ms
step:155/1645 train_time:14198ms step_avg:91.60ms
step:156/1645 train_time:14288ms step_avg:91.59ms
step:157/1645 train_time:14380ms step_avg:91.59ms
step:158/1645 train_time:14470ms step_avg:91.58ms
step:159/1645 train_time:14562ms step_avg:91.58ms
step:160/1645 train_time:14655ms step_avg:91.59ms
step:161/1645 train_time:14749ms step_avg:91.61ms
step:162/1645 train_time:14844ms step_avg:91.63ms
step:163/1645 train_time:14934ms step_avg:91.62ms
step:164/1645 train_time:15026ms step_avg:91.62ms
step:165/1645 train_time:15117ms step_avg:91.62ms
step:166/1645 train_time:15208ms step_avg:91.62ms
step:167/1645 train_time:15299ms step_avg:91.61ms
step:168/1645 train_time:15390ms step_avg:91.61ms
step:169/1645 train_time:15481ms step_avg:91.61ms
step:170/1645 train_time:15572ms step_avg:91.60ms
step:171/1645 train_time:15665ms step_avg:91.61ms
step:172/1645 train_time:15758ms step_avg:91.62ms
step:173/1645 train_time:15850ms step_avg:91.62ms
step:174/1645 train_time:15942ms step_avg:91.62ms
step:175/1645 train_time:16033ms step_avg:91.62ms
step:176/1645 train_time:16126ms step_avg:91.62ms
step:177/1645 train_time:16217ms step_avg:91.62ms
step:178/1645 train_time:16308ms step_avg:91.62ms
step:179/1645 train_time:16399ms step_avg:91.62ms
step:180/1645 train_time:16490ms step_avg:91.61ms
step:181/1645 train_time:16582ms step_avg:91.61ms
step:182/1645 train_time:16673ms step_avg:91.61ms
step:183/1645 train_time:16765ms step_avg:91.61ms
step:184/1645 train_time:16857ms step_avg:91.62ms
step:185/1645 train_time:16949ms step_avg:91.62ms
step:186/1645 train_time:17041ms step_avg:91.62ms
step:187/1645 train_time:17133ms step_avg:91.62ms
step:188/1645 train_time:17225ms step_avg:91.62ms
step:189/1645 train_time:17316ms step_avg:91.62ms
step:190/1645 train_time:17407ms step_avg:91.61ms
step:191/1645 train_time:17498ms step_avg:91.61ms
step:192/1645 train_time:17590ms step_avg:91.61ms
step:193/1645 train_time:17681ms step_avg:91.61ms
step:194/1645 train_time:17773ms step_avg:91.62ms
step:195/1645 train_time:17867ms step_avg:91.63ms
step:196/1645 train_time:17960ms step_avg:91.63ms
step:197/1645 train_time:18051ms step_avg:91.63ms
step:198/1645 train_time:18143ms step_avg:91.63ms
step:199/1645 train_time:18234ms step_avg:91.63ms
step:200/1645 train_time:18325ms step_avg:91.63ms
step:201/1645 train_time:18416ms step_avg:91.62ms
step:202/1645 train_time:18507ms step_avg:91.62ms
step:203/1645 train_time:18598ms step_avg:91.62ms
step:204/1645 train_time:18689ms step_avg:91.61ms
step:205/1645 train_time:18781ms step_avg:91.61ms
step:206/1645 train_time:18873ms step_avg:91.62ms
step:207/1645 train_time:18966ms step_avg:91.62ms
step:208/1645 train_time:19059ms step_avg:91.63ms
step:209/1645 train_time:19150ms step_avg:91.63ms
step:210/1645 train_time:19242ms step_avg:91.63ms
step:211/1645 train_time:19333ms step_avg:91.63ms
step:212/1645 train_time:19424ms step_avg:91.62ms
step:213/1645 train_time:19516ms step_avg:91.63ms
step:214/1645 train_time:19608ms step_avg:91.62ms
step:215/1645 train_time:19700ms step_avg:91.63ms
step:216/1645 train_time:19791ms step_avg:91.63ms
step:217/1645 train_time:19884ms step_avg:91.63ms
step:218/1645 train_time:19976ms step_avg:91.63ms
step:219/1645 train_time:20068ms step_avg:91.64ms
step:220/1645 train_time:20160ms step_avg:91.64ms
step:221/1645 train_time:20253ms step_avg:91.64ms
step:222/1645 train_time:20344ms step_avg:91.64ms
step:223/1645 train_time:20436ms step_avg:91.64ms
step:224/1645 train_time:20528ms step_avg:91.64ms
step:225/1645 train_time:20619ms step_avg:91.64ms
step:226/1645 train_time:20710ms step_avg:91.64ms
step:227/1645 train_time:20801ms step_avg:91.64ms
step:228/1645 train_time:20894ms step_avg:91.64ms
step:229/1645 train_time:20986ms step_avg:91.64ms
step:230/1645 train_time:21077ms step_avg:91.64ms
step:231/1645 train_time:21170ms step_avg:91.64ms
step:232/1645 train_time:21261ms step_avg:91.64ms
step:233/1645 train_time:21353ms step_avg:91.64ms
step:234/1645 train_time:21445ms step_avg:91.64ms
step:235/1645 train_time:21536ms step_avg:91.64ms
step:236/1645 train_time:21627ms step_avg:91.64ms
step:237/1645 train_time:21719ms step_avg:91.64ms
step:238/1645 train_time:21810ms step_avg:91.64ms
step:239/1645 train_time:21901ms step_avg:91.64ms
step:240/1645 train_time:21993ms step_avg:91.64ms
step:241/1645 train_time:22085ms step_avg:91.64ms
step:242/1645 train_time:22177ms step_avg:91.64ms
step:243/1645 train_time:22269ms step_avg:91.64ms
step:244/1645 train_time:22360ms step_avg:91.64ms
step:245/1645 train_time:22452ms step_avg:91.64ms
step:246/1645 train_time:22544ms step_avg:91.64ms
step:247/1645 train_time:22636ms step_avg:91.65ms
step:248/1645 train_time:22728ms step_avg:91.65ms
step:249/1645 train_time:22819ms step_avg:91.64ms
step:250/1645 train_time:22911ms step_avg:91.64ms
step:250/1645 val_loss:3.9734 train_time:23002ms step_avg:92.01ms
step:251/1645 train_time:23017ms step_avg:91.70ms
step:252/1645 train_time:23096ms step_avg:91.65ms
step:253/1645 train_time:23190ms step_avg:91.66ms
step:254/1645 train_time:23281ms step_avg:91.66ms
step:255/1645 train_time:23372ms step_avg:91.66ms
step:256/1645 train_time:23463ms step_avg:91.65ms
step:257/1645 train_time:23553ms step_avg:91.65ms
step:258/1645 train_time:23644ms step_avg:91.64ms
step:259/1645 train_time:23735ms step_avg:91.64ms
step:260/1645 train_time:23826ms step_avg:91.64ms
step:261/1645 train_time:23919ms step_avg:91.64ms
step:262/1645 train_time:24013ms step_avg:91.65ms
step:263/1645 train_time:24106ms step_avg:91.66ms
step:264/1645 train_time:24198ms step_avg:91.66ms
step:265/1645 train_time:24290ms step_avg:91.66ms
step:266/1645 train_time:24380ms step_avg:91.66ms
step:267/1645 train_time:24472ms step_avg:91.66ms
step:268/1645 train_time:24563ms step_avg:91.65ms
step:269/1645 train_time:24654ms step_avg:91.65ms
step:270/1645 train_time:24744ms step_avg:91.65ms
step:271/1645 train_time:24837ms step_avg:91.65ms
step:272/1645 train_time:24928ms step_avg:91.65ms
step:273/1645 train_time:25021ms step_avg:91.65ms
step:274/1645 train_time:25114ms step_avg:91.66ms
step:275/1645 train_time:25205ms step_avg:91.65ms
step:276/1645 train_time:25298ms step_avg:91.66ms
step:277/1645 train_time:25389ms step_avg:91.66ms
step:278/1645 train_time:25481ms step_avg:91.66ms
step:279/1645 train_time:25572ms step_avg:91.66ms
step:280/1645 train_time:25663ms step_avg:91.65ms
step:281/1645 train_time:25753ms step_avg:91.65ms
step:282/1645 train_time:25845ms step_avg:91.65ms
step:283/1645 train_time:25937ms step_avg:91.65ms
step:284/1645 train_time:26029ms step_avg:91.65ms
step:285/1645 train_time:26121ms step_avg:91.65ms
step:286/1645 train_time:26214ms step_avg:91.66ms
step:287/1645 train_time:26307ms step_avg:91.66ms
step:288/1645 train_time:26399ms step_avg:91.66ms
step:289/1645 train_time:26491ms step_avg:91.67ms
step:290/1645 train_time:26582ms step_avg:91.66ms
step:291/1645 train_time:26673ms step_avg:91.66ms
step:292/1645 train_time:26764ms step_avg:91.66ms
step:293/1645 train_time:26856ms step_avg:91.66ms
step:294/1645 train_time:26948ms step_avg:91.66ms
step:295/1645 train_time:27039ms step_avg:91.66ms
step:296/1645 train_time:27131ms step_avg:91.66ms
step:297/1645 train_time:27223ms step_avg:91.66ms
step:298/1645 train_time:27316ms step_avg:91.66ms
step:299/1645 train_time:27408ms step_avg:91.66ms
step:300/1645 train_time:27500ms step_avg:91.67ms
step:301/1645 train_time:27591ms step_avg:91.67ms
step:302/1645 train_time:27683ms step_avg:91.66ms
step:303/1645 train_time:27773ms step_avg:91.66ms
step:304/1645 train_time:27864ms step_avg:91.66ms
step:305/1645 train_time:27955ms step_avg:91.66ms
step:306/1645 train_time:28047ms step_avg:91.66ms
step:307/1645 train_time:28139ms step_avg:91.66ms
step:308/1645 train_time:28231ms step_avg:91.66ms
step:309/1645 train_time:28323ms step_avg:91.66ms
step:310/1645 train_time:28416ms step_avg:91.66ms
step:311/1645 train_time:28507ms step_avg:91.66ms
step:312/1645 train_time:28599ms step_avg:91.66ms
step:313/1645 train_time:28691ms step_avg:91.66ms
step:314/1645 train_time:28782ms step_avg:91.66ms
step:315/1645 train_time:28874ms step_avg:91.66ms
step:316/1645 train_time:28965ms step_avg:91.66ms
step:317/1645 train_time:29056ms step_avg:91.66ms
step:318/1645 train_time:29147ms step_avg:91.66ms
step:319/1645 train_time:29239ms step_avg:91.66ms
step:320/1645 train_time:29331ms step_avg:91.66ms
step:321/1645 train_time:29422ms step_avg:91.66ms
step:322/1645 train_time:29514ms step_avg:91.66ms
step:323/1645 train_time:29606ms step_avg:91.66ms
step:324/1645 train_time:29698ms step_avg:91.66ms
step:325/1645 train_time:29790ms step_avg:91.66ms
step:326/1645 train_time:29881ms step_avg:91.66ms
step:327/1645 train_time:29973ms step_avg:91.66ms
step:328/1645 train_time:30064ms step_avg:91.66ms
step:329/1645 train_time:30155ms step_avg:91.66ms
step:330/1645 train_time:30247ms step_avg:91.66ms
step:331/1645 train_time:30339ms step_avg:91.66ms
step:332/1645 train_time:30430ms step_avg:91.66ms
step:333/1645 train_time:30521ms step_avg:91.66ms
step:334/1645 train_time:30613ms step_avg:91.66ms
step:335/1645 train_time:30705ms step_avg:91.66ms
step:336/1645 train_time:30796ms step_avg:91.65ms
step:337/1645 train_time:30888ms step_avg:91.66ms
step:338/1645 train_time:30980ms step_avg:91.66ms
step:339/1645 train_time:31072ms step_avg:91.66ms
step:340/1645 train_time:31162ms step_avg:91.65ms
step:341/1645 train_time:31254ms step_avg:91.65ms
step:342/1645 train_time:31345ms step_avg:91.65ms
step:343/1645 train_time:31437ms step_avg:91.65ms
step:344/1645 train_time:31528ms step_avg:91.65ms
step:345/1645 train_time:31621ms step_avg:91.65ms
step:346/1645 train_time:31714ms step_avg:91.66ms
step:347/1645 train_time:31805ms step_avg:91.66ms
step:348/1645 train_time:31898ms step_avg:91.66ms
step:349/1645 train_time:31989ms step_avg:91.66ms
step:350/1645 train_time:32080ms step_avg:91.66ms
step:351/1645 train_time:32174ms step_avg:91.66ms
step:352/1645 train_time:32264ms step_avg:91.66ms
step:353/1645 train_time:32354ms step_avg:91.66ms
step:354/1645 train_time:32446ms step_avg:91.65ms
step:355/1645 train_time:32538ms step_avg:91.66ms
step:356/1645 train_time:32629ms step_avg:91.66ms
step:357/1645 train_time:32721ms step_avg:91.66ms
step:358/1645 train_time:32813ms step_avg:91.66ms
step:359/1645 train_time:32905ms step_avg:91.66ms
step:360/1645 train_time:32998ms step_avg:91.66ms
step:361/1645 train_time:33088ms step_avg:91.66ms
step:362/1645 train_time:33180ms step_avg:91.66ms
step:363/1645 train_time:33272ms step_avg:91.66ms
step:364/1645 train_time:33363ms step_avg:91.66ms
step:365/1645 train_time:33454ms step_avg:91.65ms
step:366/1645 train_time:33545ms step_avg:91.65ms
step:367/1645 train_time:33637ms step_avg:91.65ms
step:368/1645 train_time:33729ms step_avg:91.65ms
step:369/1645 train_time:33821ms step_avg:91.66ms
step:370/1645 train_time:33913ms step_avg:91.66ms
step:371/1645 train_time:34006ms step_avg:91.66ms
step:372/1645 train_time:34098ms step_avg:91.66ms
step:373/1645 train_time:34189ms step_avg:91.66ms
step:374/1645 train_time:34282ms step_avg:91.66ms
step:375/1645 train_time:34372ms step_avg:91.66ms
step:375/1645 val_loss:3.8190 train_time:34463ms step_avg:91.90ms
step:376/1645 train_time:34478ms step_avg:91.70ms
step:377/1645 train_time:34558ms step_avg:91.67ms
step:378/1645 train_time:34653ms step_avg:91.67ms
step:379/1645 train_time:34745ms step_avg:91.68ms
step:380/1645 train_time:34836ms step_avg:91.67ms
step:381/1645 train_time:34927ms step_avg:91.67ms
step:382/1645 train_time:35017ms step_avg:91.67ms
step:383/1645 train_time:35108ms step_avg:91.67ms
step:384/1645 train_time:35199ms step_avg:91.66ms
step:385/1645 train_time:35290ms step_avg:91.66ms
step:386/1645 train_time:35381ms step_avg:91.66ms
step:387/1645 train_time:35474ms step_avg:91.67ms
step:388/1645 train_time:35569ms step_avg:91.67ms
step:389/1645 train_time:35661ms step_avg:91.67ms
step:390/1645 train_time:35754ms step_avg:91.68ms
step:391/1645 train_time:35846ms step_avg:91.68ms
step:392/1645 train_time:35937ms step_avg:91.68ms
step:393/1645 train_time:36028ms step_avg:91.67ms
step:394/1645 train_time:36119ms step_avg:91.67ms
step:395/1645 train_time:36209ms step_avg:91.67ms
step:396/1645 train_time:36300ms step_avg:91.67ms
step:397/1645 train_time:36392ms step_avg:91.67ms
step:398/1645 train_time:36483ms step_avg:91.67ms
step:399/1645 train_time:36576ms step_avg:91.67ms
step:400/1645 train_time:36669ms step_avg:91.67ms
step:401/1645 train_time:36761ms step_avg:91.67ms
step:402/1645 train_time:36855ms step_avg:91.68ms
step:403/1645 train_time:36948ms step_avg:91.68ms
step:404/1645 train_time:37038ms step_avg:91.68ms
step:405/1645 train_time:37129ms step_avg:91.68ms
step:406/1645 train_time:37220ms step_avg:91.67ms
step:407/1645 train_time:37310ms step_avg:91.67ms
step:408/1645 train_time:37402ms step_avg:91.67ms
step:409/1645 train_time:37494ms step_avg:91.67ms
step:410/1645 train_time:37586ms step_avg:91.67ms
step:411/1645 train_time:37678ms step_avg:91.67ms
step:412/1645 train_time:37770ms step_avg:91.68ms
step:413/1645 train_time:37862ms step_avg:91.67ms
step:414/1645 train_time:37954ms step_avg:91.68ms
step:415/1645 train_time:38046ms step_avg:91.68ms
step:416/1645 train_time:38137ms step_avg:91.67ms
step:417/1645 train_time:38228ms step_avg:91.67ms
step:418/1645 train_time:38319ms step_avg:91.67ms
step:419/1645 train_time:38411ms step_avg:91.67ms
step:420/1645 train_time:38503ms step_avg:91.67ms
step:421/1645 train_time:38595ms step_avg:91.67ms
step:422/1645 train_time:38687ms step_avg:91.67ms
step:423/1645 train_time:38779ms step_avg:91.68ms
step:424/1645 train_time:38871ms step_avg:91.68ms
step:425/1645 train_time:38962ms step_avg:91.68ms
step:426/1645 train_time:39054ms step_avg:91.68ms
step:427/1645 train_time:39145ms step_avg:91.67ms
step:428/1645 train_time:39237ms step_avg:91.67ms
step:429/1645 train_time:39328ms step_avg:91.67ms
step:430/1645 train_time:39419ms step_avg:91.67ms
step:431/1645 train_time:39511ms step_avg:91.67ms
step:432/1645 train_time:39602ms step_avg:91.67ms
step:433/1645 train_time:39694ms step_avg:91.67ms
step:434/1645 train_time:39786ms step_avg:91.67ms
step:435/1645 train_time:39878ms step_avg:91.67ms
step:436/1645 train_time:39969ms step_avg:91.67ms
step:437/1645 train_time:40060ms step_avg:91.67ms
step:438/1645 train_time:40152ms step_avg:91.67ms
step:439/1645 train_time:40244ms step_avg:91.67ms
step:440/1645 train_time:40336ms step_avg:91.67ms
step:441/1645 train_time:40427ms step_avg:91.67ms
step:442/1645 train_time:40519ms step_avg:91.67ms
step:443/1645 train_time:40611ms step_avg:91.67ms
step:444/1645 train_time:40703ms step_avg:91.67ms
step:445/1645 train_time:40795ms step_avg:91.67ms
step:446/1645 train_time:40886ms step_avg:91.67ms
step:447/1645 train_time:40978ms step_avg:91.67ms
step:448/1645 train_time:41070ms step_avg:91.67ms
step:449/1645 train_time:41162ms step_avg:91.67ms
step:450/1645 train_time:41253ms step_avg:91.67ms
step:451/1645 train_time:41345ms step_avg:91.67ms
step:452/1645 train_time:41436ms step_avg:91.67ms
step:453/1645 train_time:41528ms step_avg:91.67ms
step:454/1645 train_time:41619ms step_avg:91.67ms
step:455/1645 train_time:41711ms step_avg:91.67ms
step:456/1645 train_time:41802ms step_avg:91.67ms
step:457/1645 train_time:41894ms step_avg:91.67ms
step:458/1645 train_time:41986ms step_avg:91.67ms
step:459/1645 train_time:42077ms step_avg:91.67ms
step:460/1645 train_time:42170ms step_avg:91.67ms
step:461/1645 train_time:42260ms step_avg:91.67ms
step:462/1645 train_time:42351ms step_avg:91.67ms
step:463/1645 train_time:42443ms step_avg:91.67ms
step:464/1645 train_time:42535ms step_avg:91.67ms
step:465/1645 train_time:42626ms step_avg:91.67ms
step:466/1645 train_time:42718ms step_avg:91.67ms
step:467/1645 train_time:42811ms step_avg:91.67ms
step:468/1645 train_time:42902ms step_avg:91.67ms
step:469/1645 train_time:42994ms step_avg:91.67ms
step:470/1645 train_time:43086ms step_avg:91.67ms
step:471/1645 train_time:43177ms step_avg:91.67ms
step:472/1645 train_time:43269ms step_avg:91.67ms
step:473/1645 train_time:43362ms step_avg:91.67ms
step:474/1645 train_time:43453ms step_avg:91.67ms
step:475/1645 train_time:43544ms step_avg:91.67ms
step:476/1645 train_time:43635ms step_avg:91.67ms
step:477/1645 train_time:43727ms step_avg:91.67ms
step:478/1645 train_time:43819ms step_avg:91.67ms
step:479/1645 train_time:43910ms step_avg:91.67ms
step:480/1645 train_time:44001ms step_avg:91.67ms
step:481/1645 train_time:44093ms step_avg:91.67ms
step:482/1645 train_time:44186ms step_avg:91.67ms
step:483/1645 train_time:44278ms step_avg:91.67ms
step:484/1645 train_time:44370ms step_avg:91.67ms
step:485/1645 train_time:44463ms step_avg:91.68ms
step:486/1645 train_time:44554ms step_avg:91.67ms
step:487/1645 train_time:44645ms step_avg:91.67ms
step:488/1645 train_time:44737ms step_avg:91.67ms
step:489/1645 train_time:44828ms step_avg:91.67ms
step:490/1645 train_time:44919ms step_avg:91.67ms
step:491/1645 train_time:45010ms step_avg:91.67ms
step:492/1645 train_time:45102ms step_avg:91.67ms
step:493/1645 train_time:45194ms step_avg:91.67ms
step:494/1645 train_time:45286ms step_avg:91.67ms
step:495/1645 train_time:45377ms step_avg:91.67ms
step:496/1645 train_time:45469ms step_avg:91.67ms
step:497/1645 train_time:45561ms step_avg:91.67ms
step:498/1645 train_time:45654ms step_avg:91.67ms
step:499/1645 train_time:45745ms step_avg:91.67ms
step:500/1645 train_time:45837ms step_avg:91.67ms
step:500/1645 val_loss:3.7160 train_time:45929ms step_avg:91.86ms
step:501/1645 train_time:45949ms step_avg:91.71ms
step:502/1645 train_time:46024ms step_avg:91.68ms
step:503/1645 train_time:46118ms step_avg:91.69ms
step:504/1645 train_time:46209ms step_avg:91.69ms
step:505/1645 train_time:46300ms step_avg:91.68ms
step:506/1645 train_time:46391ms step_avg:91.68ms
step:507/1645 train_time:46481ms step_avg:91.68ms
step:508/1645 train_time:46572ms step_avg:91.68ms
step:509/1645 train_time:46663ms step_avg:91.68ms
step:510/1645 train_time:46754ms step_avg:91.67ms
step:511/1645 train_time:46846ms step_avg:91.68ms
step:512/1645 train_time:46939ms step_avg:91.68ms
step:513/1645 train_time:47032ms step_avg:91.68ms
step:514/1645 train_time:47125ms step_avg:91.68ms
step:515/1645 train_time:47217ms step_avg:91.68ms
step:516/1645 train_time:47308ms step_avg:91.68ms
step:517/1645 train_time:47399ms step_avg:91.68ms
step:518/1645 train_time:47490ms step_avg:91.68ms
step:519/1645 train_time:47581ms step_avg:91.68ms
step:520/1645 train_time:47671ms step_avg:91.68ms
step:521/1645 train_time:47762ms step_avg:91.67ms
step:522/1645 train_time:47854ms step_avg:91.67ms
step:523/1645 train_time:47946ms step_avg:91.68ms
step:524/1645 train_time:48039ms step_avg:91.68ms
step:525/1645 train_time:48131ms step_avg:91.68ms
step:526/1645 train_time:48223ms step_avg:91.68ms
step:527/1645 train_time:48315ms step_avg:91.68ms
step:528/1645 train_time:48407ms step_avg:91.68ms
step:529/1645 train_time:48498ms step_avg:91.68ms
step:530/1645 train_time:48589ms step_avg:91.68ms
step:531/1645 train_time:48680ms step_avg:91.68ms
step:532/1645 train_time:48771ms step_avg:91.67ms
step:533/1645 train_time:48862ms step_avg:91.67ms
step:534/1645 train_time:48955ms step_avg:91.68ms
step:535/1645 train_time:49046ms step_avg:91.67ms
step:536/1645 train_time:49137ms step_avg:91.67ms
step:537/1645 train_time:49230ms step_avg:91.68ms
step:538/1645 train_time:49323ms step_avg:91.68ms
step:539/1645 train_time:49414ms step_avg:91.68ms
step:540/1645 train_time:49506ms step_avg:91.68ms
step:541/1645 train_time:49598ms step_avg:91.68ms
step:542/1645 train_time:49689ms step_avg:91.68ms
step:543/1645 train_time:49780ms step_avg:91.68ms
step:544/1645 train_time:49871ms step_avg:91.67ms
step:545/1645 train_time:49963ms step_avg:91.68ms
step:546/1645 train_time:50054ms step_avg:91.67ms
step:547/1645 train_time:50146ms step_avg:91.67ms
step:548/1645 train_time:50237ms step_avg:91.67ms
step:549/1645 train_time:50329ms step_avg:91.67ms
step:550/1645 train_time:50422ms step_avg:91.68ms
step:551/1645 train_time:50515ms step_avg:91.68ms
step:552/1645 train_time:50609ms step_avg:91.68ms
step:553/1645 train_time:50701ms step_avg:91.68ms
step:554/1645 train_time:50794ms step_avg:91.69ms
step:555/1645 train_time:50887ms step_avg:91.69ms
step:556/1645 train_time:50980ms step_avg:91.69ms
step:557/1645 train_time:51072ms step_avg:91.69ms
step:558/1645 train_time:51166ms step_avg:91.70ms
step:559/1645 train_time:51258ms step_avg:91.70ms
step:560/1645 train_time:51352ms step_avg:91.70ms
step:561/1645 train_time:51445ms step_avg:91.70ms
step:562/1645 train_time:51539ms step_avg:91.71ms
step:563/1645 train_time:51631ms step_avg:91.71ms
step:564/1645 train_time:51724ms step_avg:91.71ms
step:565/1645 train_time:51817ms step_avg:91.71ms
step:566/1645 train_time:51911ms step_avg:91.71ms
step:567/1645 train_time:52003ms step_avg:91.72ms
step:568/1645 train_time:52095ms step_avg:91.72ms
step:569/1645 train_time:52189ms step_avg:91.72ms
step:570/1645 train_time:52282ms step_avg:91.72ms
step:571/1645 train_time:52375ms step_avg:91.73ms
step:572/1645 train_time:52469ms step_avg:91.73ms
step:573/1645 train_time:52561ms step_avg:91.73ms
step:574/1645 train_time:52654ms step_avg:91.73ms
step:575/1645 train_time:52748ms step_avg:91.74ms
step:576/1645 train_time:52841ms step_avg:91.74ms
step:577/1645 train_time:52933ms step_avg:91.74ms
step:578/1645 train_time:53026ms step_avg:91.74ms
step:579/1645 train_time:53119ms step_avg:91.74ms
step:580/1645 train_time:53212ms step_avg:91.74ms
step:581/1645 train_time:53306ms step_avg:91.75ms
step:582/1645 train_time:53400ms step_avg:91.75ms
step:583/1645 train_time:53492ms step_avg:91.75ms
step:584/1645 train_time:53585ms step_avg:91.76ms
step:585/1645 train_time:53678ms step_avg:91.76ms
step:586/1645 train_time:53770ms step_avg:91.76ms
step:587/1645 train_time:53863ms step_avg:91.76ms
step:588/1645 train_time:53955ms step_avg:91.76ms
step:589/1645 train_time:54048ms step_avg:91.76ms
step:590/1645 train_time:54142ms step_avg:91.77ms
step:591/1645 train_time:54235ms step_avg:91.77ms
step:592/1645 train_time:54328ms step_avg:91.77ms
step:593/1645 train_time:54422ms step_avg:91.77ms
step:594/1645 train_time:54515ms step_avg:91.78ms
step:595/1645 train_time:54608ms step_avg:91.78ms
step:596/1645 train_time:54701ms step_avg:91.78ms
step:597/1645 train_time:54794ms step_avg:91.78ms
step:598/1645 train_time:54888ms step_avg:91.79ms
step:599/1645 train_time:54981ms step_avg:91.79ms
step:600/1645 train_time:55073ms step_avg:91.79ms
step:601/1645 train_time:55167ms step_avg:91.79ms
step:602/1645 train_time:55259ms step_avg:91.79ms
step:603/1645 train_time:55353ms step_avg:91.80ms
step:604/1645 train_time:55448ms step_avg:91.80ms
step:605/1645 train_time:55541ms step_avg:91.80ms
step:606/1645 train_time:55633ms step_avg:91.80ms
step:607/1645 train_time:55726ms step_avg:91.81ms
step:608/1645 train_time:55820ms step_avg:91.81ms
step:609/1645 train_time:55913ms step_avg:91.81ms
step:610/1645 train_time:56006ms step_avg:91.81ms
step:611/1645 train_time:56099ms step_avg:91.81ms
step:612/1645 train_time:56192ms step_avg:91.82ms
step:613/1645 train_time:56285ms step_avg:91.82ms
step:614/1645 train_time:56378ms step_avg:91.82ms
step:615/1645 train_time:56471ms step_avg:91.82ms
step:616/1645 train_time:56565ms step_avg:91.83ms
step:617/1645 train_time:56659ms step_avg:91.83ms
step:618/1645 train_time:56751ms step_avg:91.83ms
step:619/1645 train_time:56844ms step_avg:91.83ms
step:620/1645 train_time:56936ms step_avg:91.83ms
step:621/1645 train_time:57030ms step_avg:91.84ms
step:622/1645 train_time:57123ms step_avg:91.84ms
step:623/1645 train_time:57216ms step_avg:91.84ms
step:624/1645 train_time:57310ms step_avg:91.84ms
step:625/1645 train_time:57403ms step_avg:91.84ms
step:625/1645 val_loss:3.6127 train_time:57495ms step_avg:91.99ms
step:626/1645 train_time:57510ms step_avg:91.87ms
step:627/1645 train_time:57595ms step_avg:91.86ms
step:628/1645 train_time:57697ms step_avg:91.87ms
step:629/1645 train_time:57793ms step_avg:91.88ms
step:630/1645 train_time:57885ms step_avg:91.88ms
step:631/1645 train_time:57976ms step_avg:91.88ms
step:632/1645 train_time:58068ms step_avg:91.88ms
step:633/1645 train_time:58160ms step_avg:91.88ms
step:634/1645 train_time:58252ms step_avg:91.88ms
step:635/1645 train_time:58344ms step_avg:91.88ms
step:636/1645 train_time:58437ms step_avg:91.88ms
step:637/1645 train_time:58531ms step_avg:91.88ms
step:638/1645 train_time:58626ms step_avg:91.89ms
step:639/1645 train_time:58721ms step_avg:91.89ms
step:640/1645 train_time:58815ms step_avg:91.90ms
step:641/1645 train_time:58909ms step_avg:91.90ms
step:642/1645 train_time:59001ms step_avg:91.90ms
step:643/1645 train_time:59093ms step_avg:91.90ms
step:644/1645 train_time:59185ms step_avg:91.90ms
step:645/1645 train_time:59277ms step_avg:91.90ms
step:646/1645 train_time:59369ms step_avg:91.90ms
step:647/1645 train_time:59462ms step_avg:91.90ms
step:648/1645 train_time:59556ms step_avg:91.91ms
step:649/1645 train_time:59650ms step_avg:91.91ms
step:650/1645 train_time:59743ms step_avg:91.91ms
step:651/1645 train_time:59837ms step_avg:91.91ms
step:652/1645 train_time:59930ms step_avg:91.92ms
step:653/1645 train_time:60023ms step_avg:91.92ms
step:654/1645 train_time:60116ms step_avg:91.92ms
step:655/1645 train_time:60208ms step_avg:91.92ms
step:656/1645 train_time:60299ms step_avg:91.92ms
step:657/1645 train_time:60392ms step_avg:91.92ms
step:658/1645 train_time:60486ms step_avg:91.92ms
step:659/1645 train_time:60580ms step_avg:91.93ms
step:660/1645 train_time:60674ms step_avg:91.93ms
step:661/1645 train_time:60769ms step_avg:91.93ms
step:662/1645 train_time:60861ms step_avg:91.93ms
step:663/1645 train_time:60953ms step_avg:91.94ms
step:664/1645 train_time:61046ms step_avg:91.94ms
step:665/1645 train_time:61139ms step_avg:91.94ms
step:666/1645 train_time:61231ms step_avg:91.94ms
step:667/1645 train_time:61323ms step_avg:91.94ms
step:668/1645 train_time:61416ms step_avg:91.94ms
step:669/1645 train_time:61510ms step_avg:91.94ms
step:670/1645 train_time:61604ms step_avg:91.95ms
step:671/1645 train_time:61697ms step_avg:91.95ms
step:672/1645 train_time:61791ms step_avg:91.95ms
step:673/1645 train_time:61885ms step_avg:91.95ms
step:674/1645 train_time:61977ms step_avg:91.95ms
step:675/1645 train_time:62070ms step_avg:91.96ms
step:676/1645 train_time:62163ms step_avg:91.96ms
step:677/1645 train_time:62255ms step_avg:91.96ms
step:678/1645 train_time:62347ms step_avg:91.96ms
step:679/1645 train_time:62440ms step_avg:91.96ms
step:680/1645 train_time:62532ms step_avg:91.96ms
step:681/1645 train_time:62625ms step_avg:91.96ms
step:682/1645 train_time:62719ms step_avg:91.96ms
step:683/1645 train_time:62812ms step_avg:91.96ms
step:684/1645 train_time:62905ms step_avg:91.97ms
step:685/1645 train_time:62998ms step_avg:91.97ms
step:686/1645 train_time:63091ms step_avg:91.97ms
step:687/1645 train_time:63184ms step_avg:91.97ms
step:688/1645 train_time:63276ms step_avg:91.97ms
step:689/1645 train_time:63369ms step_avg:91.97ms
step:690/1645 train_time:63462ms step_avg:91.97ms
step:691/1645 train_time:63555ms step_avg:91.97ms
step:692/1645 train_time:63648ms step_avg:91.98ms
step:693/1645 train_time:63741ms step_avg:91.98ms
step:694/1645 train_time:63834ms step_avg:91.98ms
step:695/1645 train_time:63927ms step_avg:91.98ms
step:696/1645 train_time:64020ms step_avg:91.98ms
step:697/1645 train_time:64113ms step_avg:91.98ms
step:698/1645 train_time:64206ms step_avg:91.98ms
step:699/1645 train_time:64298ms step_avg:91.99ms
step:700/1645 train_time:64391ms step_avg:91.99ms
step:701/1645 train_time:64484ms step_avg:91.99ms
step:702/1645 train_time:64577ms step_avg:91.99ms
step:703/1645 train_time:64669ms step_avg:91.99ms
step:704/1645 train_time:64763ms step_avg:91.99ms
step:705/1645 train_time:64855ms step_avg:91.99ms
step:706/1645 train_time:64948ms step_avg:91.99ms
step:707/1645 train_time:65040ms step_avg:91.99ms
step:708/1645 train_time:65133ms step_avg:92.00ms
step:709/1645 train_time:65226ms step_avg:92.00ms
step:710/1645 train_time:65318ms step_avg:92.00ms
step:711/1645 train_time:65411ms step_avg:92.00ms
step:712/1645 train_time:65504ms step_avg:92.00ms
step:713/1645 train_time:65597ms step_avg:92.00ms
step:714/1645 train_time:65691ms step_avg:92.00ms
step:715/1645 train_time:65783ms step_avg:92.00ms
step:716/1645 train_time:65876ms step_avg:92.01ms
step:717/1645 train_time:65970ms step_avg:92.01ms
step:718/1645 train_time:66062ms step_avg:92.01ms
step:719/1645 train_time:66154ms step_avg:92.01ms
step:720/1645 train_time:66248ms step_avg:92.01ms
step:721/1645 train_time:66341ms step_avg:92.01ms
step:722/1645 train_time:66434ms step_avg:92.01ms
step:723/1645 train_time:66527ms step_avg:92.02ms
step:724/1645 train_time:66620ms step_avg:92.02ms
step:725/1645 train_time:66712ms step_avg:92.02ms
step:726/1645 train_time:66805ms step_avg:92.02ms
step:727/1645 train_time:66898ms step_avg:92.02ms
step:728/1645 train_time:66991ms step_avg:92.02ms
step:729/1645 train_time:67084ms step_avg:92.02ms
step:730/1645 train_time:67177ms step_avg:92.02ms
step:731/1645 train_time:67269ms step_avg:92.02ms
step:732/1645 train_time:67362ms step_avg:92.03ms
step:733/1645 train_time:67455ms step_avg:92.03ms
step:734/1645 train_time:67549ms step_avg:92.03ms
step:735/1645 train_time:67641ms step_avg:92.03ms
step:736/1645 train_time:67734ms step_avg:92.03ms
step:737/1645 train_time:67826ms step_avg:92.03ms
step:738/1645 train_time:67919ms step_avg:92.03ms
step:739/1645 train_time:68012ms step_avg:92.03ms
step:740/1645 train_time:68105ms step_avg:92.03ms
step:741/1645 train_time:68198ms step_avg:92.04ms
step:742/1645 train_time:68291ms step_avg:92.04ms
step:743/1645 train_time:68385ms step_avg:92.04ms
step:744/1645 train_time:68478ms step_avg:92.04ms
step:745/1645 train_time:68571ms step_avg:92.04ms
step:746/1645 train_time:68664ms step_avg:92.04ms
step:747/1645 train_time:68757ms step_avg:92.04ms
step:748/1645 train_time:68850ms step_avg:92.05ms
step:749/1645 train_time:68943ms step_avg:92.05ms
step:750/1645 train_time:69036ms step_avg:92.05ms
step:750/1645 val_loss:3.5624 train_time:69129ms step_avg:92.17ms
step:751/1645 train_time:69149ms step_avg:92.08ms
step:752/1645 train_time:69227ms step_avg:92.06ms
step:753/1645 train_time:69324ms step_avg:92.06ms
step:754/1645 train_time:69415ms step_avg:92.06ms
step:755/1645 train_time:69508ms step_avg:92.06ms
step:756/1645 train_time:69600ms step_avg:92.06ms
step:757/1645 train_time:69692ms step_avg:92.06ms
step:758/1645 train_time:69785ms step_avg:92.06ms
step:759/1645 train_time:69877ms step_avg:92.06ms
step:760/1645 train_time:69970ms step_avg:92.07ms
step:761/1645 train_time:70063ms step_avg:92.07ms
step:762/1645 train_time:70158ms step_avg:92.07ms
step:763/1645 train_time:70253ms step_avg:92.07ms
step:764/1645 train_time:70347ms step_avg:92.08ms
step:765/1645 train_time:70440ms step_avg:92.08ms
step:766/1645 train_time:70532ms step_avg:92.08ms
step:767/1645 train_time:70624ms step_avg:92.08ms
step:768/1645 train_time:70716ms step_avg:92.08ms
step:769/1645 train_time:70809ms step_avg:92.08ms
step:770/1645 train_time:70901ms step_avg:92.08ms
step:771/1645 train_time:70994ms step_avg:92.08ms
step:772/1645 train_time:71087ms step_avg:92.08ms
step:773/1645 train_time:71181ms step_avg:92.08ms
step:774/1645 train_time:71275ms step_avg:92.09ms
step:775/1645 train_time:71368ms step_avg:92.09ms
step:776/1645 train_time:71462ms step_avg:92.09ms
step:777/1645 train_time:71555ms step_avg:92.09ms
step:778/1645 train_time:71647ms step_avg:92.09ms
step:779/1645 train_time:71739ms step_avg:92.09ms
step:780/1645 train_time:71831ms step_avg:92.09ms
step:781/1645 train_time:71925ms step_avg:92.09ms
step:782/1645 train_time:72018ms step_avg:92.09ms
step:783/1645 train_time:72111ms step_avg:92.10ms
step:784/1645 train_time:72205ms step_avg:92.10ms
step:785/1645 train_time:72298ms step_avg:92.10ms
step:786/1645 train_time:72392ms step_avg:92.10ms
step:787/1645 train_time:72484ms step_avg:92.10ms
step:788/1645 train_time:72578ms step_avg:92.10ms
step:789/1645 train_time:72670ms step_avg:92.10ms
step:790/1645 train_time:72764ms step_avg:92.11ms
step:791/1645 train_time:72855ms step_avg:92.11ms
step:792/1645 train_time:72947ms step_avg:92.11ms
step:793/1645 train_time:73040ms step_avg:92.11ms
step:794/1645 train_time:73134ms step_avg:92.11ms
step:795/1645 train_time:73227ms step_avg:92.11ms
step:796/1645 train_time:73320ms step_avg:92.11ms
step:797/1645 train_time:73414ms step_avg:92.11ms
step:798/1645 train_time:73507ms step_avg:92.11ms
step:799/1645 train_time:73600ms step_avg:92.11ms
step:800/1645 train_time:73692ms step_avg:92.12ms
step:801/1645 train_time:73785ms step_avg:92.12ms
step:802/1645 train_time:73877ms step_avg:92.12ms
step:803/1645 train_time:73971ms step_avg:92.12ms
step:804/1645 train_time:74066ms step_avg:92.12ms
step:805/1645 train_time:74158ms step_avg:92.12ms
step:806/1645 train_time:74251ms step_avg:92.12ms
step:807/1645 train_time:74344ms step_avg:92.12ms
step:808/1645 train_time:74437ms step_avg:92.12ms
step:809/1645 train_time:74530ms step_avg:92.13ms
step:810/1645 train_time:74624ms step_avg:92.13ms
step:811/1645 train_time:74717ms step_avg:92.13ms
step:812/1645 train_time:74810ms step_avg:92.13ms
step:813/1645 train_time:74903ms step_avg:92.13ms
step:814/1645 train_time:74995ms step_avg:92.13ms
step:815/1645 train_time:75088ms step_avg:92.13ms
step:816/1645 train_time:75181ms step_avg:92.13ms
step:817/1645 train_time:75275ms step_avg:92.14ms
step:818/1645 train_time:75368ms step_avg:92.14ms
step:819/1645 train_time:75461ms step_avg:92.14ms
step:820/1645 train_time:75554ms step_avg:92.14ms
step:821/1645 train_time:75646ms step_avg:92.14ms
step:822/1645 train_time:75739ms step_avg:92.14ms
step:823/1645 train_time:75832ms step_avg:92.14ms
step:824/1645 train_time:75925ms step_avg:92.14ms
step:825/1645 train_time:76018ms step_avg:92.14ms
step:826/1645 train_time:76112ms step_avg:92.15ms
step:827/1645 train_time:76204ms step_avg:92.15ms
step:828/1645 train_time:76298ms step_avg:92.15ms
step:829/1645 train_time:76392ms step_avg:92.15ms
step:830/1645 train_time:76486ms step_avg:92.15ms
step:831/1645 train_time:76578ms step_avg:92.15ms
step:832/1645 train_time:76670ms step_avg:92.15ms
step:833/1645 train_time:76762ms step_avg:92.15ms
step:834/1645 train_time:76855ms step_avg:92.15ms
step:835/1645 train_time:76948ms step_avg:92.15ms
step:836/1645 train_time:77041ms step_avg:92.15ms
step:837/1645 train_time:77134ms step_avg:92.16ms
step:838/1645 train_time:77226ms step_avg:92.16ms
step:839/1645 train_time:77320ms step_avg:92.16ms
step:840/1645 train_time:77413ms step_avg:92.16ms
step:841/1645 train_time:77506ms step_avg:92.16ms
step:842/1645 train_time:77599ms step_avg:92.16ms
step:843/1645 train_time:77692ms step_avg:92.16ms
step:844/1645 train_time:77786ms step_avg:92.16ms
step:845/1645 train_time:77877ms step_avg:92.16ms
step:846/1645 train_time:77970ms step_avg:92.16ms
step:847/1645 train_time:78063ms step_avg:92.16ms
step:848/1645 train_time:78156ms step_avg:92.16ms
step:849/1645 train_time:78248ms step_avg:92.17ms
step:850/1645 train_time:78341ms step_avg:92.17ms
step:851/1645 train_time:78435ms step_avg:92.17ms
step:852/1645 train_time:78528ms step_avg:92.17ms
step:853/1645 train_time:78621ms step_avg:92.17ms
step:854/1645 train_time:78713ms step_avg:92.17ms
step:855/1645 train_time:78806ms step_avg:92.17ms
step:856/1645 train_time:78899ms step_avg:92.17ms
step:857/1645 train_time:78991ms step_avg:92.17ms
step:858/1645 train_time:79085ms step_avg:92.17ms
step:859/1645 train_time:79178ms step_avg:92.18ms
step:860/1645 train_time:79271ms step_avg:92.18ms
step:861/1645 train_time:79363ms step_avg:92.18ms
step:862/1645 train_time:79457ms step_avg:92.18ms
step:863/1645 train_time:79550ms step_avg:92.18ms
step:864/1645 train_time:79643ms step_avg:92.18ms
step:865/1645 train_time:79736ms step_avg:92.18ms
step:866/1645 train_time:79829ms step_avg:92.18ms
step:867/1645 train_time:79922ms step_avg:92.18ms
step:868/1645 train_time:80015ms step_avg:92.18ms
step:869/1645 train_time:80107ms step_avg:92.18ms
step:870/1645 train_time:80200ms step_avg:92.18ms
step:871/1645 train_time:80293ms step_avg:92.18ms
step:872/1645 train_time:80386ms step_avg:92.19ms
step:873/1645 train_time:80479ms step_avg:92.19ms
step:874/1645 train_time:80572ms step_avg:92.19ms
step:875/1645 train_time:80665ms step_avg:92.19ms
step:875/1645 val_loss:3.5158 train_time:80758ms step_avg:92.29ms
step:876/1645 train_time:80778ms step_avg:92.21ms
step:877/1645 train_time:80853ms step_avg:92.19ms
step:878/1645 train_time:80948ms step_avg:92.20ms
step:879/1645 train_time:81041ms step_avg:92.20ms
step:880/1645 train_time:81133ms step_avg:92.20ms
step:881/1645 train_time:81226ms step_avg:92.20ms
step:882/1645 train_time:81317ms step_avg:92.20ms
step:883/1645 train_time:81410ms step_avg:92.20ms
step:884/1645 train_time:81502ms step_avg:92.20ms
step:885/1645 train_time:81594ms step_avg:92.20ms
step:886/1645 train_time:81688ms step_avg:92.20ms
step:887/1645 train_time:81782ms step_avg:92.20ms
step:888/1645 train_time:81877ms step_avg:92.20ms
step:889/1645 train_time:81971ms step_avg:92.21ms
step:890/1645 train_time:82064ms step_avg:92.21ms
step:891/1645 train_time:82157ms step_avg:92.21ms
step:892/1645 train_time:82250ms step_avg:92.21ms
step:893/1645 train_time:82342ms step_avg:92.21ms
step:894/1645 train_time:82434ms step_avg:92.21ms
step:895/1645 train_time:82527ms step_avg:92.21ms
step:896/1645 train_time:82619ms step_avg:92.21ms
step:897/1645 train_time:82712ms step_avg:92.21ms
step:898/1645 train_time:82807ms step_avg:92.21ms
step:899/1645 train_time:82900ms step_avg:92.21ms
step:900/1645 train_time:82994ms step_avg:92.22ms
step:901/1645 train_time:83087ms step_avg:92.22ms
step:902/1645 train_time:83180ms step_avg:92.22ms
step:903/1645 train_time:83272ms step_avg:92.22ms
step:904/1645 train_time:83364ms step_avg:92.22ms
step:905/1645 train_time:83457ms step_avg:92.22ms
step:906/1645 train_time:83549ms step_avg:92.22ms
step:907/1645 train_time:83642ms step_avg:92.22ms
step:908/1645 train_time:83735ms step_avg:92.22ms
step:909/1645 train_time:83828ms step_avg:92.22ms
step:910/1645 train_time:83922ms step_avg:92.22ms
step:911/1645 train_time:84015ms step_avg:92.22ms
step:912/1645 train_time:84108ms step_avg:92.22ms
step:913/1645 train_time:84201ms step_avg:92.23ms
step:914/1645 train_time:84294ms step_avg:92.23ms
step:915/1645 train_time:84387ms step_avg:92.23ms
step:916/1645 train_time:84479ms step_avg:92.23ms
step:917/1645 train_time:84571ms step_avg:92.23ms
step:918/1645 train_time:84664ms step_avg:92.23ms
step:919/1645 train_time:84757ms step_avg:92.23ms
step:920/1645 train_time:84852ms step_avg:92.23ms
step:921/1645 train_time:84944ms step_avg:92.23ms
step:922/1645 train_time:85038ms step_avg:92.23ms
step:923/1645 train_time:85131ms step_avg:92.23ms
step:924/1645 train_time:85223ms step_avg:92.23ms
step:925/1645 train_time:85316ms step_avg:92.23ms
step:926/1645 train_time:85408ms step_avg:92.23ms
step:927/1645 train_time:85501ms step_avg:92.23ms
step:928/1645 train_time:85593ms step_avg:92.23ms
step:929/1645 train_time:85686ms step_avg:92.23ms
step:930/1645 train_time:85779ms step_avg:92.24ms
step:931/1645 train_time:85872ms step_avg:92.24ms
step:932/1645 train_time:85966ms step_avg:92.24ms
step:933/1645 train_time:86060ms step_avg:92.24ms
step:934/1645 train_time:86152ms step_avg:92.24ms
step:935/1645 train_time:86246ms step_avg:92.24ms
step:936/1645 train_time:86338ms step_avg:92.24ms
step:937/1645 train_time:86431ms step_avg:92.24ms
step:938/1645 train_time:86524ms step_avg:92.24ms
step:939/1645 train_time:86616ms step_avg:92.24ms
step:940/1645 train_time:86708ms step_avg:92.24ms
step:941/1645 train_time:86801ms step_avg:92.24ms
step:942/1645 train_time:86895ms step_avg:92.24ms
step:943/1645 train_time:86988ms step_avg:92.25ms
step:944/1645 train_time:87081ms step_avg:92.25ms
step:945/1645 train_time:87174ms step_avg:92.25ms
step:946/1645 train_time:87269ms step_avg:92.25ms
step:947/1645 train_time:87363ms step_avg:92.25ms
step:948/1645 train_time:87454ms step_avg:92.25ms
step:949/1645 train_time:87548ms step_avg:92.25ms
step:950/1645 train_time:87641ms step_avg:92.25ms
step:951/1645 train_time:87733ms step_avg:92.25ms
step:952/1645 train_time:87827ms step_avg:92.26ms
step:953/1645 train_time:87920ms step_avg:92.26ms
step:954/1645 train_time:88012ms step_avg:92.26ms
step:955/1645 train_time:88104ms step_avg:92.26ms
step:956/1645 train_time:88197ms step_avg:92.26ms
step:957/1645 train_time:88290ms step_avg:92.26ms
step:958/1645 train_time:88383ms step_avg:92.26ms
step:959/1645 train_time:88475ms step_avg:92.26ms
step:960/1645 train_time:88571ms step_avg:92.26ms
step:961/1645 train_time:88663ms step_avg:92.26ms
step:962/1645 train_time:88756ms step_avg:92.26ms
step:963/1645 train_time:88849ms step_avg:92.26ms
step:964/1645 train_time:88942ms step_avg:92.26ms
step:965/1645 train_time:89035ms step_avg:92.26ms
step:966/1645 train_time:89128ms step_avg:92.26ms
step:967/1645 train_time:89221ms step_avg:92.27ms
step:968/1645 train_time:89313ms step_avg:92.27ms
step:969/1645 train_time:89406ms step_avg:92.27ms
step:970/1645 train_time:89499ms step_avg:92.27ms
step:971/1645 train_time:89592ms step_avg:92.27ms
step:972/1645 train_time:89686ms step_avg:92.27ms
step:973/1645 train_time:89779ms step_avg:92.27ms
step:974/1645 train_time:89872ms step_avg:92.27ms
step:975/1645 train_time:89966ms step_avg:92.27ms
step:976/1645 train_time:90058ms step_avg:92.27ms
step:977/1645 train_time:90151ms step_avg:92.27ms
step:978/1645 train_time:90243ms step_avg:92.27ms
step:979/1645 train_time:90336ms step_avg:92.27ms
step:980/1645 train_time:90431ms step_avg:92.28ms
step:981/1645 train_time:90523ms step_avg:92.28ms
step:982/1645 train_time:90616ms step_avg:92.28ms
step:983/1645 train_time:90709ms step_avg:92.28ms
step:984/1645 train_time:90801ms step_avg:92.28ms
step:985/1645 train_time:90894ms step_avg:92.28ms
step:986/1645 train_time:90987ms step_avg:92.28ms
step:987/1645 train_time:91081ms step_avg:92.28ms
step:988/1645 train_time:91174ms step_avg:92.28ms
step:989/1645 train_time:91267ms step_avg:92.28ms
step:990/1645 train_time:91361ms step_avg:92.28ms
step:991/1645 train_time:91453ms step_avg:92.28ms
step:992/1645 train_time:91546ms step_avg:92.28ms
step:993/1645 train_time:91639ms step_avg:92.28ms
step:994/1645 train_time:91732ms step_avg:92.29ms
step:995/1645 train_time:91825ms step_avg:92.29ms
step:996/1645 train_time:91917ms step_avg:92.29ms
step:997/1645 train_time:92010ms step_avg:92.29ms
step:998/1645 train_time:92103ms step_avg:92.29ms
step:999/1645 train_time:92195ms step_avg:92.29ms
step:1000/1645 train_time:92288ms step_avg:92.29ms
step:1000/1645 val_loss:3.4649 train_time:92382ms step_avg:92.38ms
step:1001/1645 train_time:92403ms step_avg:92.31ms
step:1002/1645 train_time:92481ms step_avg:92.30ms
step:1003/1645 train_time:92576ms step_avg:92.30ms
step:1004/1645 train_time:92668ms step_avg:92.30ms
step:1005/1645 train_time:92760ms step_avg:92.30ms
step:1006/1645 train_time:92852ms step_avg:92.30ms
step:1007/1645 train_time:92944ms step_avg:92.30ms
step:1008/1645 train_time:93036ms step_avg:92.30ms
step:1009/1645 train_time:93129ms step_avg:92.30ms
step:1010/1645 train_time:93221ms step_avg:92.30ms
step:1011/1645 train_time:93315ms step_avg:92.30ms
step:1012/1645 train_time:93410ms step_avg:92.30ms
step:1013/1645 train_time:93506ms step_avg:92.31ms
step:1014/1645 train_time:93600ms step_avg:92.31ms
step:1015/1645 train_time:93694ms step_avg:92.31ms
step:1016/1645 train_time:93786ms step_avg:92.31ms
step:1017/1645 train_time:93878ms step_avg:92.31ms
step:1018/1645 train_time:93970ms step_avg:92.31ms
step:1019/1645 train_time:94062ms step_avg:92.31ms
step:1020/1645 train_time:94155ms step_avg:92.31ms
step:1021/1645 train_time:94248ms step_avg:92.31ms
step:1022/1645 train_time:94341ms step_avg:92.31ms
step:1023/1645 train_time:94435ms step_avg:92.31ms
step:1024/1645 train_time:94530ms step_avg:92.31ms
step:1025/1645 train_time:94624ms step_avg:92.32ms
step:1026/1645 train_time:94716ms step_avg:92.32ms
step:1027/1645 train_time:94809ms step_avg:92.32ms
step:1028/1645 train_time:94901ms step_avg:92.32ms
step:1029/1645 train_time:94993ms step_avg:92.32ms
step:1030/1645 train_time:95085ms step_avg:92.32ms
step:1031/1645 train_time:95178ms step_avg:92.32ms
step:1032/1645 train_time:95270ms step_avg:92.32ms
step:1033/1645 train_time:95363ms step_avg:92.32ms
step:1034/1645 train_time:95457ms step_avg:92.32ms
step:1035/1645 train_time:95550ms step_avg:92.32ms
step:1036/1645 train_time:95643ms step_avg:92.32ms
step:1037/1645 train_time:95737ms step_avg:92.32ms
step:1038/1645 train_time:95830ms step_avg:92.32ms
step:1039/1645 train_time:95923ms step_avg:92.32ms
step:1040/1645 train_time:96016ms step_avg:92.32ms
step:1041/1645 train_time:96108ms step_avg:92.32ms
step:1042/1645 train_time:96200ms step_avg:92.32ms
step:1043/1645 train_time:96293ms step_avg:92.32ms
step:1044/1645 train_time:96387ms step_avg:92.32ms
step:1045/1645 train_time:96480ms step_avg:92.33ms
step:1046/1645 train_time:96573ms step_avg:92.33ms
step:1047/1645 train_time:96666ms step_avg:92.33ms
step:1048/1645 train_time:96759ms step_avg:92.33ms
step:1049/1645 train_time:96852ms step_avg:92.33ms
step:1050/1645 train_time:96944ms step_avg:92.33ms
step:1051/1645 train_time:97037ms step_avg:92.33ms
step:1052/1645 train_time:97129ms step_avg:92.33ms
step:1053/1645 train_time:97222ms step_avg:92.33ms
step:1054/1645 train_time:97314ms step_avg:92.33ms
step:1055/1645 train_time:97409ms step_avg:92.33ms
step:1056/1645 train_time:97503ms step_avg:92.33ms
step:1057/1645 train_time:97596ms step_avg:92.33ms
step:1058/1645 train_time:97691ms step_avg:92.34ms
step:1059/1645 train_time:97784ms step_avg:92.34ms
step:1060/1645 train_time:97876ms step_avg:92.34ms
step:1061/1645 train_time:97970ms step_avg:92.34ms
step:1062/1645 train_time:98062ms step_avg:92.34ms
step:1063/1645 train_time:98156ms step_avg:92.34ms
step:1064/1645 train_time:98249ms step_avg:92.34ms
step:1065/1645 train_time:98341ms step_avg:92.34ms
step:1066/1645 train_time:98434ms step_avg:92.34ms
step:1067/1645 train_time:98528ms step_avg:92.34ms
step:1068/1645 train_time:98621ms step_avg:92.34ms
step:1069/1645 train_time:98713ms step_avg:92.34ms
step:1070/1645 train_time:98806ms step_avg:92.34ms
step:1071/1645 train_time:98898ms step_avg:92.34ms
step:1072/1645 train_time:98991ms step_avg:92.34ms
step:1073/1645 train_time:99083ms step_avg:92.34ms
step:1074/1645 train_time:99176ms step_avg:92.34ms
step:1075/1645 train_time:99269ms step_avg:92.34ms
step:1076/1645 train_time:99362ms step_avg:92.34ms
step:1077/1645 train_time:99456ms step_avg:92.35ms
step:1078/1645 train_time:99548ms step_avg:92.35ms
step:1079/1645 train_time:99642ms step_avg:92.35ms
step:1080/1645 train_time:99735ms step_avg:92.35ms
step:1081/1645 train_time:99828ms step_avg:92.35ms
step:1082/1645 train_time:99922ms step_avg:92.35ms
step:1083/1645 train_time:100014ms step_avg:92.35ms
step:1084/1645 train_time:100107ms step_avg:92.35ms
step:1085/1645 train_time:100200ms step_avg:92.35ms
step:1086/1645 train_time:100293ms step_avg:92.35ms
step:1087/1645 train_time:100386ms step_avg:92.35ms
step:1088/1645 train_time:100479ms step_avg:92.35ms
step:1089/1645 train_time:100572ms step_avg:92.35ms
step:1090/1645 train_time:100665ms step_avg:92.35ms
step:1091/1645 train_time:100758ms step_avg:92.35ms
step:1092/1645 train_time:100850ms step_avg:92.35ms
step:1093/1645 train_time:100944ms step_avg:92.36ms
step:1094/1645 train_time:101037ms step_avg:92.36ms
step:1095/1645 train_time:101130ms step_avg:92.36ms
step:1096/1645 train_time:101222ms step_avg:92.36ms
step:1097/1645 train_time:101315ms step_avg:92.36ms
step:1098/1645 train_time:101408ms step_avg:92.36ms
step:1099/1645 train_time:101501ms step_avg:92.36ms
step:1100/1645 train_time:101595ms step_avg:92.36ms
step:1101/1645 train_time:101689ms step_avg:92.36ms
step:1102/1645 train_time:101783ms step_avg:92.36ms
step:1103/1645 train_time:101877ms step_avg:92.36ms
step:1104/1645 train_time:101970ms step_avg:92.36ms
step:1105/1645 train_time:102064ms step_avg:92.37ms
step:1106/1645 train_time:102157ms step_avg:92.37ms
step:1107/1645 train_time:102250ms step_avg:92.37ms
step:1108/1645 train_time:102343ms step_avg:92.37ms
step:1109/1645 train_time:102437ms step_avg:92.37ms
step:1110/1645 train_time:102531ms step_avg:92.37ms
step:1111/1645 train_time:102625ms step_avg:92.37ms
step:1112/1645 train_time:102718ms step_avg:92.37ms
step:1113/1645 train_time:102812ms step_avg:92.37ms
step:1114/1645 train_time:102906ms step_avg:92.38ms
step:1115/1645 train_time:103000ms step_avg:92.38ms
step:1116/1645 train_time:103094ms step_avg:92.38ms
step:1117/1645 train_time:103187ms step_avg:92.38ms
step:1118/1645 train_time:103281ms step_avg:92.38ms
step:1119/1645 train_time:103375ms step_avg:92.38ms
step:1120/1645 train_time:103468ms step_avg:92.38ms
step:1121/1645 train_time:103561ms step_avg:92.38ms
step:1122/1645 train_time:103655ms step_avg:92.38ms
step:1123/1645 train_time:103748ms step_avg:92.38ms
step:1124/1645 train_time:103841ms step_avg:92.39ms
step:1125/1645 train_time:103935ms step_avg:92.39ms
step:1125/1645 val_loss:3.4131 train_time:104029ms step_avg:92.47ms
step:1126/1645 train_time:104052ms step_avg:92.41ms
step:1127/1645 train_time:104132ms step_avg:92.40ms
step:1128/1645 train_time:104234ms step_avg:92.41ms
step:1129/1645 train_time:104329ms step_avg:92.41ms
step:1130/1645 train_time:104421ms step_avg:92.41ms
step:1131/1645 train_time:104513ms step_avg:92.41ms
step:1132/1645 train_time:104605ms step_avg:92.41ms
step:1133/1645 train_time:104698ms step_avg:92.41ms
step:1134/1645 train_time:104790ms step_avg:92.41ms
step:1135/1645 train_time:104883ms step_avg:92.41ms
step:1136/1645 train_time:104977ms step_avg:92.41ms
step:1137/1645 train_time:105073ms step_avg:92.41ms
step:1138/1645 train_time:105168ms step_avg:92.41ms
step:1139/1645 train_time:105264ms step_avg:92.42ms
step:1140/1645 train_time:105359ms step_avg:92.42ms
step:1141/1645 train_time:105452ms step_avg:92.42ms
step:1142/1645 train_time:105545ms step_avg:92.42ms
step:1143/1645 train_time:105638ms step_avg:92.42ms
step:1144/1645 train_time:105730ms step_avg:92.42ms
step:1145/1645 train_time:105823ms step_avg:92.42ms
step:1146/1645 train_time:105916ms step_avg:92.42ms
step:1147/1645 train_time:106009ms step_avg:92.42ms
step:1148/1645 train_time:106105ms step_avg:92.43ms
step:1149/1645 train_time:106198ms step_avg:92.43ms
step:1150/1645 train_time:106292ms step_avg:92.43ms
step:1151/1645 train_time:106386ms step_avg:92.43ms
step:1152/1645 train_time:106480ms step_avg:92.43ms
step:1153/1645 train_time:106573ms step_avg:92.43ms
step:1154/1645 train_time:106666ms step_avg:92.43ms
step:1155/1645 train_time:106759ms step_avg:92.43ms
step:1156/1645 train_time:106852ms step_avg:92.43ms
step:1157/1645 train_time:106945ms step_avg:92.43ms
step:1158/1645 train_time:107039ms step_avg:92.43ms
step:1159/1645 train_time:107134ms step_avg:92.44ms
step:1160/1645 train_time:107228ms step_avg:92.44ms
step:1161/1645 train_time:107322ms step_avg:92.44ms
step:1162/1645 train_time:107417ms step_avg:92.44ms
step:1163/1645 train_time:107510ms step_avg:92.44ms
step:1164/1645 train_time:107603ms step_avg:92.44ms
step:1165/1645 train_time:107696ms step_avg:92.44ms
step:1166/1645 train_time:107789ms step_avg:92.44ms
step:1167/1645 train_time:107882ms step_avg:92.44ms
step:1168/1645 train_time:107975ms step_avg:92.44ms
step:1169/1645 train_time:108068ms step_avg:92.45ms
step:1170/1645 train_time:108162ms step_avg:92.45ms
step:1171/1645 train_time:108257ms step_avg:92.45ms
step:1172/1645 train_time:108351ms step_avg:92.45ms
step:1173/1645 train_time:108447ms step_avg:92.45ms
step:1174/1645 train_time:108540ms step_avg:92.45ms
step:1175/1645 train_time:108633ms step_avg:92.45ms
step:1176/1645 train_time:108727ms step_avg:92.45ms
step:1177/1645 train_time:108820ms step_avg:92.45ms
step:1178/1645 train_time:108913ms step_avg:92.46ms
step:1179/1645 train_time:109005ms step_avg:92.46ms
step:1180/1645 train_time:109099ms step_avg:92.46ms
step:1181/1645 train_time:109192ms step_avg:92.46ms
step:1182/1645 train_time:109287ms step_avg:92.46ms
step:1183/1645 train_time:109381ms step_avg:92.46ms
step:1184/1645 train_time:109476ms step_avg:92.46ms
step:1185/1645 train_time:109569ms step_avg:92.46ms
step:1186/1645 train_time:109663ms step_avg:92.46ms
step:1187/1645 train_time:109756ms step_avg:92.46ms
step:1188/1645 train_time:109849ms step_avg:92.47ms
step:1189/1645 train_time:109942ms step_avg:92.47ms
step:1190/1645 train_time:110036ms step_avg:92.47ms
step:1191/1645 train_time:110129ms step_avg:92.47ms
step:1192/1645 train_time:110223ms step_avg:92.47ms
step:1193/1645 train_time:110317ms step_avg:92.47ms
step:1194/1645 train_time:110411ms step_avg:92.47ms
step:1195/1645 train_time:110504ms step_avg:92.47ms
step:1196/1645 train_time:110598ms step_avg:92.47ms
step:1197/1645 train_time:110691ms step_avg:92.47ms
step:1198/1645 train_time:110785ms step_avg:92.47ms
step:1199/1645 train_time:110879ms step_avg:92.48ms
step:1200/1645 train_time:110972ms step_avg:92.48ms
step:1201/1645 train_time:111064ms step_avg:92.48ms
step:1202/1645 train_time:111159ms step_avg:92.48ms
step:1203/1645 train_time:111252ms step_avg:92.48ms
step:1204/1645 train_time:111345ms step_avg:92.48ms
step:1205/1645 train_time:111439ms step_avg:92.48ms
step:1206/1645 train_time:111533ms step_avg:92.48ms
step:1207/1645 train_time:111626ms step_avg:92.48ms
step:1208/1645 train_time:111721ms step_avg:92.48ms
step:1209/1645 train_time:111814ms step_avg:92.48ms
step:1210/1645 train_time:111907ms step_avg:92.49ms
step:1211/1645 train_time:112001ms step_avg:92.49ms
step:1212/1645 train_time:112093ms step_avg:92.49ms
step:1213/1645 train_time:112187ms step_avg:92.49ms
step:1214/1645 train_time:112280ms step_avg:92.49ms
step:1215/1645 train_time:112373ms step_avg:92.49ms
step:1216/1645 train_time:112467ms step_avg:92.49ms
step:1217/1645 train_time:112561ms step_avg:92.49ms
step:1218/1645 train_time:112655ms step_avg:92.49ms
step:1219/1645 train_time:112749ms step_avg:92.49ms
step:1220/1645 train_time:112843ms step_avg:92.49ms
step:1221/1645 train_time:112937ms step_avg:92.50ms
step:1222/1645 train_time:113030ms step_avg:92.50ms
step:1223/1645 train_time:113122ms step_avg:92.50ms
step:1224/1645 train_time:113216ms step_avg:92.50ms
step:1225/1645 train_time:113309ms step_avg:92.50ms
step:1226/1645 train_time:113403ms step_avg:92.50ms
step:1227/1645 train_time:113497ms step_avg:92.50ms
step:1228/1645 train_time:113591ms step_avg:92.50ms
step:1229/1645 train_time:113684ms step_avg:92.50ms
step:1230/1645 train_time:113779ms step_avg:92.50ms
step:1231/1645 train_time:113874ms step_avg:92.51ms
step:1232/1645 train_time:113966ms step_avg:92.51ms
step:1233/1645 train_time:114060ms step_avg:92.51ms
step:1234/1645 train_time:114153ms step_avg:92.51ms
step:1235/1645 train_time:114246ms step_avg:92.51ms
step:1236/1645 train_time:114340ms step_avg:92.51ms
step:1237/1645 train_time:114433ms step_avg:92.51ms
step:1238/1645 train_time:114527ms step_avg:92.51ms
step:1239/1645 train_time:114620ms step_avg:92.51ms
step:1240/1645 train_time:114714ms step_avg:92.51ms
step:1241/1645 train_time:114808ms step_avg:92.51ms
step:1242/1645 train_time:114901ms step_avg:92.51ms
step:1243/1645 train_time:114995ms step_avg:92.51ms
step:1244/1645 train_time:115088ms step_avg:92.51ms
step:1245/1645 train_time:115183ms step_avg:92.52ms
step:1246/1645 train_time:115277ms step_avg:92.52ms
step:1247/1645 train_time:115369ms step_avg:92.52ms
step:1248/1645 train_time:115463ms step_avg:92.52ms
step:1249/1645 train_time:115556ms step_avg:92.52ms
step:1250/1645 train_time:115649ms step_avg:92.52ms
step:1250/1645 val_loss:3.3746 train_time:115743ms step_avg:92.59ms
step:1251/1645 train_time:115764ms step_avg:92.54ms
step:1252/1645 train_time:115841ms step_avg:92.52ms
step:1253/1645 train_time:115935ms step_avg:92.53ms
step:1254/1645 train_time:116029ms step_avg:92.53ms
step:1255/1645 train_time:116121ms step_avg:92.53ms
step:1256/1645 train_time:116214ms step_avg:92.53ms
step:1257/1645 train_time:116307ms step_avg:92.53ms
step:1258/1645 train_time:116400ms step_avg:92.53ms
step:1259/1645 train_time:116492ms step_avg:92.53ms
step:1260/1645 train_time:116586ms step_avg:92.53ms
step:1261/1645 train_time:116681ms step_avg:92.53ms
step:1262/1645 train_time:116777ms step_avg:92.53ms
step:1263/1645 train_time:116872ms step_avg:92.54ms
step:1264/1645 train_time:116967ms step_avg:92.54ms
step:1265/1645 train_time:117060ms step_avg:92.54ms
step:1266/1645 train_time:117153ms step_avg:92.54ms
step:1267/1645 train_time:117246ms step_avg:92.54ms
step:1268/1645 train_time:117340ms step_avg:92.54ms
step:1269/1645 train_time:117433ms step_avg:92.54ms
step:1270/1645 train_time:117525ms step_avg:92.54ms
step:1271/1645 train_time:117619ms step_avg:92.54ms
step:1272/1645 train_time:117713ms step_avg:92.54ms
step:1273/1645 train_time:117809ms step_avg:92.54ms
step:1274/1645 train_time:117905ms step_avg:92.55ms
step:1275/1645 train_time:117998ms step_avg:92.55ms
step:1276/1645 train_time:118091ms step_avg:92.55ms
step:1277/1645 train_time:118185ms step_avg:92.55ms
step:1278/1645 train_time:118278ms step_avg:92.55ms
step:1279/1645 train_time:118371ms step_avg:92.55ms
step:1280/1645 train_time:118464ms step_avg:92.55ms
step:1281/1645 train_time:118557ms step_avg:92.55ms
step:1282/1645 train_time:118650ms step_avg:92.55ms
step:1283/1645 train_time:118745ms step_avg:92.55ms
step:1284/1645 train_time:118840ms step_avg:92.55ms
step:1285/1645 train_time:118934ms step_avg:92.56ms
step:1286/1645 train_time:119028ms step_avg:92.56ms
step:1287/1645 train_time:119121ms step_avg:92.56ms
step:1288/1645 train_time:119214ms step_avg:92.56ms
step:1289/1645 train_time:119308ms step_avg:92.56ms
step:1290/1645 train_time:119401ms step_avg:92.56ms
step:1291/1645 train_time:119495ms step_avg:92.56ms
step:1292/1645 train_time:119588ms step_avg:92.56ms
step:1293/1645 train_time:119682ms step_avg:92.56ms
step:1294/1645 train_time:119777ms step_avg:92.56ms
step:1295/1645 train_time:119871ms step_avg:92.56ms
step:1296/1645 train_time:119966ms step_avg:92.57ms
step:1297/1645 train_time:120061ms step_avg:92.57ms
step:1298/1645 train_time:120154ms step_avg:92.57ms
step:1299/1645 train_time:120247ms step_avg:92.57ms
step:1300/1645 train_time:120340ms step_avg:92.57ms
step:1301/1645 train_time:120433ms step_avg:92.57ms
step:1302/1645 train_time:120527ms step_avg:92.57ms
step:1303/1645 train_time:120621ms step_avg:92.57ms
step:1304/1645 train_time:120714ms step_avg:92.57ms
step:1305/1645 train_time:120808ms step_avg:92.57ms
step:1306/1645 train_time:120902ms step_avg:92.57ms
step:1307/1645 train_time:120996ms step_avg:92.58ms
step:1308/1645 train_time:121091ms step_avg:92.58ms
step:1309/1645 train_time:121184ms step_avg:92.58ms
step:1310/1645 train_time:121278ms step_avg:92.58ms
step:1311/1645 train_time:121371ms step_avg:92.58ms
step:1312/1645 train_time:121465ms step_avg:92.58ms
step:1313/1645 train_time:121558ms step_avg:92.58ms
step:1314/1645 train_time:121651ms step_avg:92.58ms
step:1315/1645 train_time:121744ms step_avg:92.58ms
step:1316/1645 train_time:121839ms step_avg:92.58ms
step:1317/1645 train_time:121932ms step_avg:92.58ms
step:1318/1645 train_time:122027ms step_avg:92.58ms
step:1319/1645 train_time:122120ms step_avg:92.59ms
step:1320/1645 train_time:122213ms step_avg:92.59ms
step:1321/1645 train_time:122307ms step_avg:92.59ms
step:1322/1645 train_time:122401ms step_avg:92.59ms
step:1323/1645 train_time:122494ms step_avg:92.59ms
step:1324/1645 train_time:122588ms step_avg:92.59ms
step:1325/1645 train_time:122683ms step_avg:92.59ms
step:1326/1645 train_time:122777ms step_avg:92.59ms
step:1327/1645 train_time:122870ms step_avg:92.59ms
step:1328/1645 train_time:122964ms step_avg:92.59ms
step:1329/1645 train_time:123057ms step_avg:92.59ms
step:1330/1645 train_time:123151ms step_avg:92.60ms
step:1331/1645 train_time:123245ms step_avg:92.60ms
step:1332/1645 train_time:123338ms step_avg:92.60ms
step:1333/1645 train_time:123432ms step_avg:92.60ms
step:1334/1645 train_time:123525ms step_avg:92.60ms
step:1335/1645 train_time:123619ms step_avg:92.60ms
step:1336/1645 train_time:123713ms step_avg:92.60ms
step:1337/1645 train_time:123807ms step_avg:92.60ms
step:1338/1645 train_time:123901ms step_avg:92.60ms
step:1339/1645 train_time:123994ms step_avg:92.60ms
step:1340/1645 train_time:124089ms step_avg:92.60ms
step:1341/1645 train_time:124183ms step_avg:92.60ms
step:1342/1645 train_time:124277ms step_avg:92.61ms
step:1343/1645 train_time:124371ms step_avg:92.61ms
step:1344/1645 train_time:124466ms step_avg:92.61ms
step:1345/1645 train_time:124560ms step_avg:92.61ms
step:1346/1645 train_time:124654ms step_avg:92.61ms
step:1347/1645 train_time:124748ms step_avg:92.61ms
step:1348/1645 train_time:124841ms step_avg:92.61ms
step:1349/1645 train_time:124935ms step_avg:92.61ms
step:1350/1645 train_time:125029ms step_avg:92.61ms
step:1351/1645 train_time:125123ms step_avg:92.61ms
step:1352/1645 train_time:125216ms step_avg:92.62ms
step:1353/1645 train_time:125309ms step_avg:92.62ms
step:1354/1645 train_time:125403ms step_avg:92.62ms
step:1355/1645 train_time:125496ms step_avg:92.62ms
step:1356/1645 train_time:125590ms step_avg:92.62ms
step:1357/1645 train_time:125685ms step_avg:92.62ms
step:1358/1645 train_time:125778ms step_avg:92.62ms
step:1359/1645 train_time:125871ms step_avg:92.62ms
step:1360/1645 train_time:125964ms step_avg:92.62ms
step:1361/1645 train_time:126058ms step_avg:92.62ms
step:1362/1645 train_time:126151ms step_avg:92.62ms
step:1363/1645 train_time:126246ms step_avg:92.62ms
step:1364/1645 train_time:126339ms step_avg:92.62ms
step:1365/1645 train_time:126433ms step_avg:92.62ms
step:1366/1645 train_time:126527ms step_avg:92.63ms
step:1367/1645 train_time:126620ms step_avg:92.63ms
step:1368/1645 train_time:126715ms step_avg:92.63ms
step:1369/1645 train_time:126809ms step_avg:92.63ms
step:1370/1645 train_time:126903ms step_avg:92.63ms
step:1371/1645 train_time:126997ms step_avg:92.63ms
step:1372/1645 train_time:127090ms step_avg:92.63ms
step:1373/1645 train_time:127184ms step_avg:92.63ms
step:1374/1645 train_time:127278ms step_avg:92.63ms
step:1375/1645 train_time:127371ms step_avg:92.63ms
step:1375/1645 val_loss:3.3397 train_time:127466ms step_avg:92.70ms
step:1376/1645 train_time:127486ms step_avg:92.65ms
step:1377/1645 train_time:127565ms step_avg:92.64ms
step:1378/1645 train_time:127660ms step_avg:92.64ms
step:1379/1645 train_time:127754ms step_avg:92.64ms
step:1380/1645 train_time:127846ms step_avg:92.64ms
step:1381/1645 train_time:127939ms step_avg:92.64ms
step:1382/1645 train_time:128033ms step_avg:92.64ms
step:1383/1645 train_time:128126ms step_avg:92.64ms
step:1384/1645 train_time:128221ms step_avg:92.65ms
step:1385/1645 train_time:128317ms step_avg:92.65ms
step:1386/1645 train_time:128411ms step_avg:92.65ms
step:1387/1645 train_time:128506ms step_avg:92.65ms
step:1388/1645 train_time:128601ms step_avg:92.65ms
step:1389/1645 train_time:128695ms step_avg:92.65ms
step:1390/1645 train_time:128789ms step_avg:92.65ms
step:1391/1645 train_time:128883ms step_avg:92.65ms
step:1392/1645 train_time:128977ms step_avg:92.66ms
step:1393/1645 train_time:129071ms step_avg:92.66ms
step:1394/1645 train_time:129163ms step_avg:92.66ms
step:1395/1645 train_time:129257ms step_avg:92.66ms
step:1396/1645 train_time:129352ms step_avg:92.66ms
step:1397/1645 train_time:129446ms step_avg:92.66ms
step:1398/1645 train_time:129541ms step_avg:92.66ms
step:1399/1645 train_time:129635ms step_avg:92.66ms
step:1400/1645 train_time:129729ms step_avg:92.66ms
step:1401/1645 train_time:129822ms step_avg:92.66ms
step:1402/1645 train_time:129915ms step_avg:92.66ms
step:1403/1645 train_time:130009ms step_avg:92.66ms
step:1404/1645 train_time:130102ms step_avg:92.67ms
step:1405/1645 train_time:130196ms step_avg:92.67ms
step:1406/1645 train_time:130290ms step_avg:92.67ms
step:1407/1645 train_time:130384ms step_avg:92.67ms
step:1408/1645 train_time:130479ms step_avg:92.67ms
step:1409/1645 train_time:130574ms step_avg:92.67ms
step:1410/1645 train_time:130668ms step_avg:92.67ms
step:1411/1645 train_time:130760ms step_avg:92.67ms
step:1412/1645 train_time:130854ms step_avg:92.67ms
step:1413/1645 train_time:130947ms step_avg:92.67ms
step:1414/1645 train_time:131041ms step_avg:92.67ms
step:1415/1645 train_time:131134ms step_avg:92.67ms
step:1416/1645 train_time:131227ms step_avg:92.67ms
step:1417/1645 train_time:131322ms step_avg:92.68ms
step:1418/1645 train_time:131416ms step_avg:92.68ms
step:1419/1645 train_time:131510ms step_avg:92.68ms
step:1420/1645 train_time:131603ms step_avg:92.68ms
step:1421/1645 train_time:131697ms step_avg:92.68ms
step:1422/1645 train_time:131791ms step_avg:92.68ms
step:1423/1645 train_time:131885ms step_avg:92.68ms
step:1424/1645 train_time:131978ms step_avg:92.68ms
step:1425/1645 train_time:132072ms step_avg:92.68ms
step:1426/1645 train_time:132164ms step_avg:92.68ms
step:1427/1645 train_time:132258ms step_avg:92.68ms
step:1428/1645 train_time:132353ms step_avg:92.68ms
step:1429/1645 train_time:132447ms step_avg:92.68ms
step:1430/1645 train_time:132541ms step_avg:92.69ms
step:1431/1645 train_time:132635ms step_avg:92.69ms
step:1432/1645 train_time:132728ms step_avg:92.69ms
step:1433/1645 train_time:132821ms step_avg:92.69ms
step:1434/1645 train_time:132915ms step_avg:92.69ms
step:1435/1645 train_time:133008ms step_avg:92.69ms
step:1436/1645 train_time:133102ms step_avg:92.69ms
step:1437/1645 train_time:133195ms step_avg:92.69ms
step:1438/1645 train_time:133289ms step_avg:92.69ms
step:1439/1645 train_time:133383ms step_avg:92.69ms
step:1440/1645 train_time:133476ms step_avg:92.69ms
step:1441/1645 train_time:133570ms step_avg:92.69ms
step:1442/1645 train_time:133664ms step_avg:92.69ms
step:1443/1645 train_time:133757ms step_avg:92.69ms
step:1444/1645 train_time:133851ms step_avg:92.69ms
step:1445/1645 train_time:133944ms step_avg:92.69ms
step:1446/1645 train_time:134038ms step_avg:92.70ms
step:1447/1645 train_time:134132ms step_avg:92.70ms
step:1448/1645 train_time:134225ms step_avg:92.70ms
step:1449/1645 train_time:134319ms step_avg:92.70ms
step:1450/1645 train_time:134412ms step_avg:92.70ms
step:1451/1645 train_time:134506ms step_avg:92.70ms
step:1452/1645 train_time:134600ms step_avg:92.70ms
step:1453/1645 train_time:134693ms step_avg:92.70ms
step:1454/1645 train_time:134787ms step_avg:92.70ms
step:1455/1645 train_time:134881ms step_avg:92.70ms
step:1456/1645 train_time:134975ms step_avg:92.70ms
step:1457/1645 train_time:135067ms step_avg:92.70ms
step:1458/1645 train_time:135161ms step_avg:92.70ms
step:1459/1645 train_time:135255ms step_avg:92.70ms
step:1460/1645 train_time:135348ms step_avg:92.70ms
step:1461/1645 train_time:135442ms step_avg:92.71ms
step:1462/1645 train_time:135536ms step_avg:92.71ms
step:1463/1645 train_time:135631ms step_avg:92.71ms
step:1464/1645 train_time:135723ms step_avg:92.71ms
step:1465/1645 train_time:135818ms step_avg:92.71ms
step:1466/1645 train_time:135911ms step_avg:92.71ms
step:1467/1645 train_time:136005ms step_avg:92.71ms
step:1468/1645 train_time:136099ms step_avg:92.71ms
step:1469/1645 train_time:136193ms step_avg:92.71ms
step:1470/1645 train_time:136286ms step_avg:92.71ms
step:1471/1645 train_time:136380ms step_avg:92.71ms
step:1472/1645 train_time:136474ms step_avg:92.71ms
step:1473/1645 train_time:136567ms step_avg:92.71ms
step:1474/1645 train_time:136660ms step_avg:92.71ms
step:1475/1645 train_time:136754ms step_avg:92.71ms
step:1476/1645 train_time:136848ms step_avg:92.72ms
step:1477/1645 train_time:136941ms step_avg:92.72ms
step:1478/1645 train_time:137036ms step_avg:92.72ms
step:1479/1645 train_time:137128ms step_avg:92.72ms
step:1480/1645 train_time:137222ms step_avg:92.72ms
step:1481/1645 train_time:137316ms step_avg:92.72ms
step:1482/1645 train_time:137410ms step_avg:92.72ms
step:1483/1645 train_time:137504ms step_avg:92.72ms
step:1484/1645 train_time:137599ms step_avg:92.72ms
step:1485/1645 train_time:137694ms step_avg:92.72ms
step:1486/1645 train_time:137786ms step_avg:92.72ms
step:1487/1645 train_time:137880ms step_avg:92.72ms
step:1488/1645 train_time:137973ms step_avg:92.72ms
step:1489/1645 train_time:138067ms step_avg:92.72ms
step:1490/1645 train_time:138160ms step_avg:92.73ms
step:1491/1645 train_time:138255ms step_avg:92.73ms
step:1492/1645 train_time:138348ms step_avg:92.73ms
step:1493/1645 train_time:138443ms step_avg:92.73ms
step:1494/1645 train_time:138536ms step_avg:92.73ms
step:1495/1645 train_time:138629ms step_avg:92.73ms
step:1496/1645 train_time:138722ms step_avg:92.73ms
step:1497/1645 train_time:138817ms step_avg:92.73ms
step:1498/1645 train_time:138910ms step_avg:92.73ms
step:1499/1645 train_time:139003ms step_avg:92.73ms
step:1500/1645 train_time:139098ms step_avg:92.73ms
step:1500/1645 val_loss:3.3096 train_time:139192ms step_avg:92.79ms
step:1501/1645 train_time:139217ms step_avg:92.75ms
step:1502/1645 train_time:139291ms step_avg:92.74ms
step:1503/1645 train_time:139387ms step_avg:92.74ms
step:1504/1645 train_time:139481ms step_avg:92.74ms
step:1505/1645 train_time:139573ms step_avg:92.74ms
step:1506/1645 train_time:139666ms step_avg:92.74ms
step:1507/1645 train_time:139758ms step_avg:92.74ms
step:1508/1645 train_time:139851ms step_avg:92.74ms
step:1509/1645 train_time:139947ms step_avg:92.74ms
step:1510/1645 train_time:140042ms step_avg:92.74ms
step:1511/1645 train_time:140136ms step_avg:92.74ms
step:1512/1645 train_time:140230ms step_avg:92.74ms
step:1513/1645 train_time:140324ms step_avg:92.75ms
step:1514/1645 train_time:140418ms step_avg:92.75ms
step:1515/1645 train_time:140512ms step_avg:92.75ms
step:1516/1645 train_time:140606ms step_avg:92.75ms
step:1517/1645 train_time:140700ms step_avg:92.75ms
step:1518/1645 train_time:140792ms step_avg:92.75ms
step:1519/1645 train_time:140886ms step_avg:92.75ms
step:1520/1645 train_time:140981ms step_avg:92.75ms
step:1521/1645 train_time:141074ms step_avg:92.75ms
step:1522/1645 train_time:141169ms step_avg:92.75ms
step:1523/1645 train_time:141263ms step_avg:92.75ms
step:1524/1645 train_time:141357ms step_avg:92.75ms
step:1525/1645 train_time:141451ms step_avg:92.75ms
step:1526/1645 train_time:141546ms step_avg:92.76ms
step:1527/1645 train_time:141639ms step_avg:92.76ms
step:1528/1645 train_time:141732ms step_avg:92.76ms
step:1529/1645 train_time:141825ms step_avg:92.76ms
step:1530/1645 train_time:141919ms step_avg:92.76ms
step:1531/1645 train_time:142012ms step_avg:92.76ms
step:1532/1645 train_time:142106ms step_avg:92.76ms
step:1533/1645 train_time:142200ms step_avg:92.76ms
step:1534/1645 train_time:142294ms step_avg:92.76ms
step:1535/1645 train_time:142388ms step_avg:92.76ms
step:1536/1645 train_time:142482ms step_avg:92.76ms
step:1537/1645 train_time:142575ms step_avg:92.76ms
step:1538/1645 train_time:142669ms step_avg:92.76ms
step:1539/1645 train_time:142763ms step_avg:92.76ms
step:1540/1645 train_time:142856ms step_avg:92.76ms
step:1541/1645 train_time:142950ms step_avg:92.76ms
step:1542/1645 train_time:143044ms step_avg:92.77ms
step:1543/1645 train_time:143138ms step_avg:92.77ms
step:1544/1645 train_time:143232ms step_avg:92.77ms
step:1545/1645 train_time:143326ms step_avg:92.77ms
step:1546/1645 train_time:143420ms step_avg:92.77ms
step:1547/1645 train_time:143514ms step_avg:92.77ms
step:1548/1645 train_time:143608ms step_avg:92.77ms
step:1549/1645 train_time:143702ms step_avg:92.77ms
step:1550/1645 train_time:143794ms step_avg:92.77ms
step:1551/1645 train_time:143888ms step_avg:92.77ms
step:1552/1645 train_time:143983ms step_avg:92.77ms
step:1553/1645 train_time:144078ms step_avg:92.77ms
step:1554/1645 train_time:144171ms step_avg:92.77ms
step:1555/1645 train_time:144265ms step_avg:92.78ms
step:1556/1645 train_time:144359ms step_avg:92.78ms
step:1557/1645 train_time:144453ms step_avg:92.78ms
step:1558/1645 train_time:144548ms step_avg:92.78ms
step:1559/1645 train_time:144642ms step_avg:92.78ms
step:1560/1645 train_time:144736ms step_avg:92.78ms
step:1561/1645 train_time:144829ms step_avg:92.78ms
step:1562/1645 train_time:144922ms step_avg:92.78ms
step:1563/1645 train_time:145016ms step_avg:92.78ms
step:1564/1645 train_time:145109ms step_avg:92.78ms
step:1565/1645 train_time:145204ms step_avg:92.78ms
step:1566/1645 train_time:145298ms step_avg:92.78ms
step:1567/1645 train_time:145390ms step_avg:92.78ms
step:1568/1645 train_time:145484ms step_avg:92.78ms
step:1569/1645 train_time:145578ms step_avg:92.78ms
step:1570/1645 train_time:145672ms step_avg:92.78ms
step:1571/1645 train_time:145765ms step_avg:92.78ms
step:1572/1645 train_time:145858ms step_avg:92.78ms
step:1573/1645 train_time:145951ms step_avg:92.79ms
step:1574/1645 train_time:146045ms step_avg:92.79ms
step:1575/1645 train_time:146139ms step_avg:92.79ms
step:1576/1645 train_time:146232ms step_avg:92.79ms
step:1577/1645 train_time:146326ms step_avg:92.79ms
step:1578/1645 train_time:146420ms step_avg:92.79ms
step:1579/1645 train_time:146513ms step_avg:92.79ms
step:1580/1645 train_time:146608ms step_avg:92.79ms
step:1581/1645 train_time:146702ms step_avg:92.79ms
step:1582/1645 train_time:146795ms step_avg:92.79ms
step:1583/1645 train_time:146888ms step_avg:92.79ms
step:1584/1645 train_time:146982ms step_avg:92.79ms
step:1585/1645 train_time:147075ms step_avg:92.79ms
step:1586/1645 train_time:147168ms step_avg:92.79ms
step:1587/1645 train_time:147262ms step_avg:92.79ms
step:1588/1645 train_time:147355ms step_avg:92.79ms
step:1589/1645 train_time:147449ms step_avg:92.79ms
step:1590/1645 train_time:147543ms step_avg:92.79ms
step:1591/1645 train_time:147637ms step_avg:92.80ms
step:1592/1645 train_time:147731ms step_avg:92.80ms
step:1593/1645 train_time:147825ms step_avg:92.80ms
step:1594/1645 train_time:147918ms step_avg:92.80ms
step:1595/1645 train_time:148012ms step_avg:92.80ms
step:1596/1645 train_time:148105ms step_avg:92.80ms
step:1597/1645 train_time:148199ms step_avg:92.80ms
step:1598/1645 train_time:148293ms step_avg:92.80ms
step:1599/1645 train_time:148386ms step_avg:92.80ms
step:1600/1645 train_time:148480ms step_avg:92.80ms
step:1601/1645 train_time:148574ms step_avg:92.80ms
step:1602/1645 train_time:148667ms step_avg:92.80ms
step:1603/1645 train_time:148761ms step_avg:92.80ms
step:1604/1645 train_time:148854ms step_avg:92.80ms
step:1605/1645 train_time:148949ms step_avg:92.80ms
step:1606/1645 train_time:149042ms step_avg:92.80ms
step:1607/1645 train_time:149136ms step_avg:92.80ms
step:1608/1645 train_time:149230ms step_avg:92.80ms
step:1609/1645 train_time:149323ms step_avg:92.80ms
step:1610/1645 train_time:149418ms step_avg:92.81ms
step:1611/1645 train_time:149511ms step_avg:92.81ms
step:1612/1645 train_time:149606ms step_avg:92.81ms
step:1613/1645 train_time:149700ms step_avg:92.81ms
step:1614/1645 train_time:149793ms step_avg:92.81ms
step:1615/1645 train_time:149887ms step_avg:92.81ms
step:1616/1645 train_time:149981ms step_avg:92.81ms
step:1617/1645 train_time:150075ms step_avg:92.81ms
step:1618/1645 train_time:150169ms step_avg:92.81ms
step:1619/1645 train_time:150262ms step_avg:92.81ms
step:1620/1645 train_time:150355ms step_avg:92.81ms
step:1621/1645 train_time:150449ms step_avg:92.81ms
step:1622/1645 train_time:150544ms step_avg:92.81ms
step:1623/1645 train_time:150637ms step_avg:92.81ms
step:1624/1645 train_time:150731ms step_avg:92.81ms
step:1625/1645 train_time:150825ms step_avg:92.82ms
step:1625/1645 val_loss:3.2858 train_time:150918ms step_avg:92.87ms
step:1626/1645 train_time:150939ms step_avg:92.83ms
step:1627/1645 train_time:151017ms step_avg:92.82ms
step:1628/1645 train_time:151112ms step_avg:92.82ms
step:1629/1645 train_time:151205ms step_avg:92.82ms
step:1630/1645 train_time:151298ms step_avg:92.82ms
step:1631/1645 train_time:151391ms step_avg:92.82ms
step:1632/1645 train_time:151484ms step_avg:92.82ms
step:1633/1645 train_time:151577ms step_avg:92.82ms
step:1634/1645 train_time:151671ms step_avg:92.82ms
step:1635/1645 train_time:151764ms step_avg:92.82ms
step:1636/1645 train_time:151858ms step_avg:92.82ms
step:1637/1645 train_time:151953ms step_avg:92.82ms
step:1638/1645 train_time:152048ms step_avg:92.83ms
step:1639/1645 train_time:152142ms step_avg:92.83ms
step:1640/1645 train_time:152236ms step_avg:92.83ms
step:1641/1645 train_time:152330ms step_avg:92.83ms
step:1642/1645 train_time:152423ms step_avg:92.83ms
step:1643/1645 train_time:152516ms step_avg:92.83ms
step:1644/1645 train_time:152608ms step_avg:92.83ms
step:1645/1645 train_time:152702ms step_avg:92.83ms
step:1645/1645 val_loss:3.2798 train_time:152796ms step_avg:92.89ms
peak memory allocated: 31659 MiB reserved: 46856 MiB

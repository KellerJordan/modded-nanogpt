import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:33:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   25C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   27C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:134ms step_avg:134.40ms
step:2/1645 train_time:150ms step_avg:75.09ms
step:3/1645 train_time:223ms step_avg:74.32ms
step:4/1645 train_time:313ms step_avg:78.24ms
step:5/1645 train_time:404ms step_avg:80.80ms
step:6/1645 train_time:495ms step_avg:82.42ms
step:7/1645 train_time:585ms step_avg:83.61ms
step:8/1645 train_time:676ms step_avg:84.55ms
step:9/1645 train_time:767ms step_avg:85.27ms
step:10/1645 train_time:859ms step_avg:85.85ms
step:11/1645 train_time:950ms step_avg:86.33ms
step:12/1645 train_time:1045ms step_avg:87.04ms
step:13/1645 train_time:1142ms step_avg:87.86ms
step:14/1645 train_time:1235ms step_avg:88.25ms
step:15/1645 train_time:1327ms step_avg:88.45ms
step:16/1645 train_time:1419ms step_avg:88.71ms
step:17/1645 train_time:1511ms step_avg:88.89ms
step:18/1645 train_time:1602ms step_avg:89.00ms
step:19/1645 train_time:1693ms step_avg:89.12ms
step:20/1645 train_time:1784ms step_avg:89.21ms
step:21/1645 train_time:1875ms step_avg:89.28ms
step:22/1645 train_time:1967ms step_avg:89.42ms
step:23/1645 train_time:2061ms step_avg:89.59ms
step:24/1645 train_time:2155ms step_avg:89.79ms
step:25/1645 train_time:2248ms step_avg:89.90ms
step:26/1645 train_time:2340ms step_avg:89.98ms
step:27/1645 train_time:2432ms step_avg:90.06ms
step:28/1645 train_time:2523ms step_avg:90.12ms
step:29/1645 train_time:2615ms step_avg:90.18ms
step:30/1645 train_time:2707ms step_avg:90.23ms
step:31/1645 train_time:2799ms step_avg:90.28ms
step:32/1645 train_time:2891ms step_avg:90.33ms
step:33/1645 train_time:2983ms step_avg:90.38ms
step:34/1645 train_time:3075ms step_avg:90.45ms
step:35/1645 train_time:3168ms step_avg:90.52ms
step:36/1645 train_time:3260ms step_avg:90.56ms
step:37/1645 train_time:3352ms step_avg:90.61ms
step:38/1645 train_time:3445ms step_avg:90.65ms
step:39/1645 train_time:3536ms step_avg:90.67ms
step:40/1645 train_time:3628ms step_avg:90.70ms
step:41/1645 train_time:3719ms step_avg:90.72ms
step:42/1645 train_time:3812ms step_avg:90.75ms
step:43/1645 train_time:3904ms step_avg:90.79ms
step:44/1645 train_time:3996ms step_avg:90.83ms
step:45/1645 train_time:4089ms step_avg:90.86ms
step:46/1645 train_time:4181ms step_avg:90.90ms
step:47/1645 train_time:4274ms step_avg:90.94ms
step:48/1645 train_time:4366ms step_avg:90.96ms
step:49/1645 train_time:4458ms step_avg:90.99ms
step:50/1645 train_time:4549ms step_avg:90.99ms
step:51/1645 train_time:4641ms step_avg:91.00ms
step:52/1645 train_time:4732ms step_avg:91.01ms
step:53/1645 train_time:4824ms step_avg:91.03ms
step:54/1645 train_time:4917ms step_avg:91.05ms
step:55/1645 train_time:5009ms step_avg:91.07ms
step:56/1645 train_time:5102ms step_avg:91.10ms
step:57/1645 train_time:5196ms step_avg:91.16ms
step:58/1645 train_time:5287ms step_avg:91.15ms
step:59/1645 train_time:5379ms step_avg:91.17ms
step:60/1645 train_time:5470ms step_avg:91.17ms
step:61/1645 train_time:5562ms step_avg:91.18ms
step:62/1645 train_time:5654ms step_avg:91.19ms
step:63/1645 train_time:5745ms step_avg:91.19ms
step:64/1645 train_time:5839ms step_avg:91.23ms
step:65/1645 train_time:5930ms step_avg:91.22ms
step:66/1645 train_time:6023ms step_avg:91.25ms
step:67/1645 train_time:6116ms step_avg:91.28ms
step:68/1645 train_time:6208ms step_avg:91.29ms
step:69/1645 train_time:6302ms step_avg:91.33ms
step:70/1645 train_time:6393ms step_avg:91.33ms
step:71/1645 train_time:6484ms step_avg:91.32ms
step:72/1645 train_time:6575ms step_avg:91.32ms
step:73/1645 train_time:6667ms step_avg:91.33ms
step:74/1645 train_time:6759ms step_avg:91.33ms
step:75/1645 train_time:6849ms step_avg:91.33ms
step:76/1645 train_time:6942ms step_avg:91.34ms
step:77/1645 train_time:7034ms step_avg:91.35ms
step:78/1645 train_time:7127ms step_avg:91.37ms
step:79/1645 train_time:7219ms step_avg:91.38ms
step:80/1645 train_time:7310ms step_avg:91.38ms
step:81/1645 train_time:7403ms step_avg:91.40ms
step:82/1645 train_time:7495ms step_avg:91.41ms
step:83/1645 train_time:7586ms step_avg:91.40ms
step:84/1645 train_time:7678ms step_avg:91.41ms
step:85/1645 train_time:7769ms step_avg:91.40ms
step:86/1645 train_time:7861ms step_avg:91.40ms
step:87/1645 train_time:7953ms step_avg:91.41ms
step:88/1645 train_time:8045ms step_avg:91.42ms
step:89/1645 train_time:8136ms step_avg:91.42ms
step:90/1645 train_time:8228ms step_avg:91.42ms
step:91/1645 train_time:8322ms step_avg:91.45ms
step:92/1645 train_time:8414ms step_avg:91.46ms
step:93/1645 train_time:8505ms step_avg:91.46ms
step:94/1645 train_time:8597ms step_avg:91.45ms
step:95/1645 train_time:8688ms step_avg:91.46ms
step:96/1645 train_time:8780ms step_avg:91.46ms
step:97/1645 train_time:8872ms step_avg:91.46ms
step:98/1645 train_time:8963ms step_avg:91.46ms
step:99/1645 train_time:9055ms step_avg:91.46ms
step:100/1645 train_time:9147ms step_avg:91.47ms
step:101/1645 train_time:9240ms step_avg:91.48ms
step:102/1645 train_time:9331ms step_avg:91.48ms
step:103/1645 train_time:9424ms step_avg:91.50ms
step:104/1645 train_time:9516ms step_avg:91.50ms
step:105/1645 train_time:9607ms step_avg:91.50ms
step:106/1645 train_time:9699ms step_avg:91.50ms
step:107/1645 train_time:9790ms step_avg:91.49ms
step:108/1645 train_time:9882ms step_avg:91.50ms
step:109/1645 train_time:9974ms step_avg:91.51ms
step:110/1645 train_time:10066ms step_avg:91.51ms
step:111/1645 train_time:10158ms step_avg:91.51ms
step:112/1645 train_time:10249ms step_avg:91.51ms
step:113/1645 train_time:10342ms step_avg:91.52ms
step:114/1645 train_time:10434ms step_avg:91.53ms
step:115/1645 train_time:10527ms step_avg:91.54ms
step:116/1645 train_time:10618ms step_avg:91.53ms
step:117/1645 train_time:10709ms step_avg:91.53ms
step:118/1645 train_time:10801ms step_avg:91.54ms
step:119/1645 train_time:10893ms step_avg:91.54ms
step:120/1645 train_time:10985ms step_avg:91.54ms
step:121/1645 train_time:11077ms step_avg:91.54ms
step:122/1645 train_time:11168ms step_avg:91.54ms
step:123/1645 train_time:11259ms step_avg:91.54ms
step:124/1645 train_time:11350ms step_avg:91.54ms
step:125/1645 train_time:11442ms step_avg:91.54ms
step:125/1645 val_loss:4.3125 train_time:11534ms step_avg:92.27ms
step:126/1645 train_time:11549ms step_avg:91.66ms
step:127/1645 train_time:11633ms step_avg:91.59ms
step:128/1645 train_time:11735ms step_avg:91.68ms
step:129/1645 train_time:11833ms step_avg:91.73ms
step:130/1645 train_time:11924ms step_avg:91.72ms
step:131/1645 train_time:12014ms step_avg:91.71ms
step:132/1645 train_time:12104ms step_avg:91.70ms
step:133/1645 train_time:12195ms step_avg:91.69ms
step:134/1645 train_time:12285ms step_avg:91.68ms
step:135/1645 train_time:12376ms step_avg:91.67ms
step:136/1645 train_time:12466ms step_avg:91.66ms
step:137/1645 train_time:12557ms step_avg:91.66ms
step:138/1645 train_time:12652ms step_avg:91.68ms
step:139/1645 train_time:12747ms step_avg:91.70ms
step:140/1645 train_time:12841ms step_avg:91.72ms
step:141/1645 train_time:12934ms step_avg:91.73ms
step:142/1645 train_time:13025ms step_avg:91.73ms
step:143/1645 train_time:13118ms step_avg:91.73ms
step:144/1645 train_time:13208ms step_avg:91.72ms
step:145/1645 train_time:13299ms step_avg:91.72ms
step:146/1645 train_time:13389ms step_avg:91.71ms
step:147/1645 train_time:13482ms step_avg:91.72ms
step:148/1645 train_time:13572ms step_avg:91.70ms
step:149/1645 train_time:13665ms step_avg:91.71ms
step:150/1645 train_time:13759ms step_avg:91.73ms
step:151/1645 train_time:13851ms step_avg:91.73ms
step:152/1645 train_time:13944ms step_avg:91.74ms
step:153/1645 train_time:14036ms step_avg:91.74ms
step:154/1645 train_time:14128ms step_avg:91.74ms
step:155/1645 train_time:14220ms step_avg:91.74ms
step:156/1645 train_time:14311ms step_avg:91.73ms
step:157/1645 train_time:14401ms step_avg:91.73ms
step:158/1645 train_time:14492ms step_avg:91.72ms
step:159/1645 train_time:14584ms step_avg:91.72ms
step:160/1645 train_time:14675ms step_avg:91.72ms
step:161/1645 train_time:14766ms step_avg:91.72ms
step:162/1645 train_time:14859ms step_avg:91.72ms
step:163/1645 train_time:14952ms step_avg:91.73ms
step:164/1645 train_time:15044ms step_avg:91.73ms
step:165/1645 train_time:15136ms step_avg:91.74ms
step:166/1645 train_time:15227ms step_avg:91.73ms
step:167/1645 train_time:15318ms step_avg:91.73ms
step:168/1645 train_time:15409ms step_avg:91.72ms
step:169/1645 train_time:15500ms step_avg:91.72ms
step:170/1645 train_time:15591ms step_avg:91.71ms
step:171/1645 train_time:15682ms step_avg:91.71ms
step:172/1645 train_time:15774ms step_avg:91.71ms
step:173/1645 train_time:15866ms step_avg:91.71ms
step:174/1645 train_time:15960ms step_avg:91.72ms
step:175/1645 train_time:16052ms step_avg:91.73ms
step:176/1645 train_time:16144ms step_avg:91.73ms
step:177/1645 train_time:16235ms step_avg:91.73ms
step:178/1645 train_time:16327ms step_avg:91.72ms
step:179/1645 train_time:16418ms step_avg:91.72ms
step:180/1645 train_time:16509ms step_avg:91.72ms
step:181/1645 train_time:16600ms step_avg:91.71ms
step:182/1645 train_time:16693ms step_avg:91.72ms
step:183/1645 train_time:16783ms step_avg:91.71ms
step:184/1645 train_time:16875ms step_avg:91.71ms
step:185/1645 train_time:16966ms step_avg:91.71ms
step:186/1645 train_time:17059ms step_avg:91.71ms
step:187/1645 train_time:17151ms step_avg:91.72ms
step:188/1645 train_time:17242ms step_avg:91.72ms
step:189/1645 train_time:17334ms step_avg:91.72ms
step:190/1645 train_time:17426ms step_avg:91.72ms
step:191/1645 train_time:17517ms step_avg:91.71ms
step:192/1645 train_time:17608ms step_avg:91.71ms
step:193/1645 train_time:17699ms step_avg:91.70ms
step:194/1645 train_time:17790ms step_avg:91.70ms
step:195/1645 train_time:17882ms step_avg:91.70ms
step:196/1645 train_time:17974ms step_avg:91.71ms
step:197/1645 train_time:18066ms step_avg:91.71ms
step:198/1645 train_time:18159ms step_avg:91.71ms
step:199/1645 train_time:18251ms step_avg:91.71ms
step:200/1645 train_time:18342ms step_avg:91.71ms
step:201/1645 train_time:18434ms step_avg:91.71ms
step:202/1645 train_time:18525ms step_avg:91.71ms
step:203/1645 train_time:18616ms step_avg:91.70ms
step:204/1645 train_time:18707ms step_avg:91.70ms
step:205/1645 train_time:18798ms step_avg:91.70ms
step:206/1645 train_time:18890ms step_avg:91.70ms
step:207/1645 train_time:18982ms step_avg:91.70ms
step:208/1645 train_time:19074ms step_avg:91.70ms
step:209/1645 train_time:19166ms step_avg:91.70ms
step:210/1645 train_time:19259ms step_avg:91.71ms
step:211/1645 train_time:19350ms step_avg:91.71ms
step:212/1645 train_time:19442ms step_avg:91.71ms
step:213/1645 train_time:19533ms step_avg:91.71ms
step:214/1645 train_time:19625ms step_avg:91.70ms
step:215/1645 train_time:19715ms step_avg:91.70ms
step:216/1645 train_time:19809ms step_avg:91.71ms
step:217/1645 train_time:19898ms step_avg:91.70ms
step:218/1645 train_time:19990ms step_avg:91.70ms
step:219/1645 train_time:20081ms step_avg:91.70ms
step:220/1645 train_time:20173ms step_avg:91.70ms
step:221/1645 train_time:20265ms step_avg:91.70ms
step:222/1645 train_time:20357ms step_avg:91.70ms
step:223/1645 train_time:20448ms step_avg:91.70ms
step:224/1645 train_time:20541ms step_avg:91.70ms
step:225/1645 train_time:20633ms step_avg:91.70ms
step:226/1645 train_time:20724ms step_avg:91.70ms
step:227/1645 train_time:20816ms step_avg:91.70ms
step:228/1645 train_time:20906ms step_avg:91.69ms
step:229/1645 train_time:20998ms step_avg:91.69ms
step:230/1645 train_time:21089ms step_avg:91.69ms
step:231/1645 train_time:21181ms step_avg:91.69ms
step:232/1645 train_time:21273ms step_avg:91.69ms
step:233/1645 train_time:21365ms step_avg:91.69ms
step:234/1645 train_time:21458ms step_avg:91.70ms
step:235/1645 train_time:21549ms step_avg:91.70ms
step:236/1645 train_time:21641ms step_avg:91.70ms
step:237/1645 train_time:21732ms step_avg:91.70ms
step:238/1645 train_time:21823ms step_avg:91.69ms
step:239/1645 train_time:21915ms step_avg:91.70ms
step:240/1645 train_time:22005ms step_avg:91.69ms
step:241/1645 train_time:22097ms step_avg:91.69ms
step:242/1645 train_time:22188ms step_avg:91.69ms
step:243/1645 train_time:22280ms step_avg:91.69ms
step:244/1645 train_time:22372ms step_avg:91.69ms
step:245/1645 train_time:22464ms step_avg:91.69ms
step:246/1645 train_time:22556ms step_avg:91.69ms
step:247/1645 train_time:22647ms step_avg:91.69ms
step:248/1645 train_time:22739ms step_avg:91.69ms
step:249/1645 train_time:22831ms step_avg:91.69ms
step:250/1645 train_time:22923ms step_avg:91.69ms
step:250/1645 val_loss:3.9668 train_time:23015ms step_avg:92.06ms
step:251/1645 train_time:23029ms step_avg:91.75ms
step:252/1645 train_time:23110ms step_avg:91.71ms
step:253/1645 train_time:23203ms step_avg:91.71ms
step:254/1645 train_time:23295ms step_avg:91.71ms
step:255/1645 train_time:23386ms step_avg:91.71ms
step:256/1645 train_time:23476ms step_avg:91.71ms
step:257/1645 train_time:23567ms step_avg:91.70ms
step:258/1645 train_time:23658ms step_avg:91.70ms
step:259/1645 train_time:23749ms step_avg:91.70ms
step:260/1645 train_time:23840ms step_avg:91.69ms
step:261/1645 train_time:23932ms step_avg:91.69ms
step:262/1645 train_time:24024ms step_avg:91.70ms
step:263/1645 train_time:24119ms step_avg:91.71ms
step:264/1645 train_time:24212ms step_avg:91.71ms
step:265/1645 train_time:24303ms step_avg:91.71ms
step:266/1645 train_time:24395ms step_avg:91.71ms
step:267/1645 train_time:24487ms step_avg:91.71ms
step:268/1645 train_time:24578ms step_avg:91.71ms
step:269/1645 train_time:24669ms step_avg:91.71ms
step:270/1645 train_time:24760ms step_avg:91.70ms
step:271/1645 train_time:24851ms step_avg:91.70ms
step:272/1645 train_time:24943ms step_avg:91.70ms
step:273/1645 train_time:25035ms step_avg:91.70ms
step:274/1645 train_time:25127ms step_avg:91.71ms
step:275/1645 train_time:25219ms step_avg:91.71ms
step:276/1645 train_time:25311ms step_avg:91.71ms
step:277/1645 train_time:25402ms step_avg:91.70ms
step:278/1645 train_time:25494ms step_avg:91.71ms
step:279/1645 train_time:25586ms step_avg:91.71ms
step:280/1645 train_time:25677ms step_avg:91.70ms
step:281/1645 train_time:25768ms step_avg:91.70ms
step:282/1645 train_time:25859ms step_avg:91.70ms
step:283/1645 train_time:25951ms step_avg:91.70ms
step:284/1645 train_time:26042ms step_avg:91.70ms
step:285/1645 train_time:26135ms step_avg:91.70ms
step:286/1645 train_time:26227ms step_avg:91.70ms
step:287/1645 train_time:26318ms step_avg:91.70ms
step:288/1645 train_time:26410ms step_avg:91.70ms
step:289/1645 train_time:26502ms step_avg:91.70ms
step:290/1645 train_time:26594ms step_avg:91.70ms
step:291/1645 train_time:26686ms step_avg:91.70ms
step:292/1645 train_time:26777ms step_avg:91.70ms
step:293/1645 train_time:26869ms step_avg:91.70ms
step:294/1645 train_time:26959ms step_avg:91.70ms
step:295/1645 train_time:27050ms step_avg:91.70ms
step:296/1645 train_time:27142ms step_avg:91.70ms
step:297/1645 train_time:27235ms step_avg:91.70ms
step:298/1645 train_time:27327ms step_avg:91.70ms
step:299/1645 train_time:27418ms step_avg:91.70ms
step:300/1645 train_time:27509ms step_avg:91.70ms
step:301/1645 train_time:27601ms step_avg:91.70ms
step:302/1645 train_time:27693ms step_avg:91.70ms
step:303/1645 train_time:27785ms step_avg:91.70ms
step:304/1645 train_time:27876ms step_avg:91.70ms
step:305/1645 train_time:27968ms step_avg:91.70ms
step:306/1645 train_time:28059ms step_avg:91.70ms
step:307/1645 train_time:28151ms step_avg:91.70ms
step:308/1645 train_time:28242ms step_avg:91.70ms
step:309/1645 train_time:28334ms step_avg:91.70ms
step:310/1645 train_time:28426ms step_avg:91.70ms
step:311/1645 train_time:28517ms step_avg:91.70ms
step:312/1645 train_time:28610ms step_avg:91.70ms
step:313/1645 train_time:28701ms step_avg:91.70ms
step:314/1645 train_time:28792ms step_avg:91.69ms
step:315/1645 train_time:28884ms step_avg:91.69ms
step:316/1645 train_time:28976ms step_avg:91.70ms
step:317/1645 train_time:29067ms step_avg:91.69ms
step:318/1645 train_time:29159ms step_avg:91.69ms
step:319/1645 train_time:29251ms step_avg:91.69ms
step:320/1645 train_time:29343ms step_avg:91.70ms
step:321/1645 train_time:29434ms step_avg:91.70ms
step:322/1645 train_time:29526ms step_avg:91.69ms
step:323/1645 train_time:29618ms step_avg:91.70ms
step:324/1645 train_time:29710ms step_avg:91.70ms
step:325/1645 train_time:29802ms step_avg:91.70ms
step:326/1645 train_time:29893ms step_avg:91.70ms
step:327/1645 train_time:29985ms step_avg:91.70ms
step:328/1645 train_time:30076ms step_avg:91.70ms
step:329/1645 train_time:30168ms step_avg:91.70ms
step:330/1645 train_time:30260ms step_avg:91.70ms
step:331/1645 train_time:30351ms step_avg:91.69ms
step:332/1645 train_time:30443ms step_avg:91.69ms
step:333/1645 train_time:30535ms step_avg:91.70ms
step:334/1645 train_time:30626ms step_avg:91.70ms
step:335/1645 train_time:30717ms step_avg:91.69ms
step:336/1645 train_time:30809ms step_avg:91.69ms
step:337/1645 train_time:30900ms step_avg:91.69ms
step:338/1645 train_time:30992ms step_avg:91.69ms
step:339/1645 train_time:31083ms step_avg:91.69ms
step:340/1645 train_time:31175ms step_avg:91.69ms
step:341/1645 train_time:31266ms step_avg:91.69ms
step:342/1645 train_time:31358ms step_avg:91.69ms
step:343/1645 train_time:31449ms step_avg:91.69ms
step:344/1645 train_time:31541ms step_avg:91.69ms
step:345/1645 train_time:31632ms step_avg:91.69ms
step:346/1645 train_time:31724ms step_avg:91.69ms
step:347/1645 train_time:31817ms step_avg:91.69ms
step:348/1645 train_time:31909ms step_avg:91.69ms
step:349/1645 train_time:32001ms step_avg:91.69ms
step:350/1645 train_time:32092ms step_avg:91.69ms
step:351/1645 train_time:32182ms step_avg:91.69ms
step:352/1645 train_time:32274ms step_avg:91.69ms
step:353/1645 train_time:32366ms step_avg:91.69ms
step:354/1645 train_time:32457ms step_avg:91.69ms
step:355/1645 train_time:32549ms step_avg:91.69ms
step:356/1645 train_time:32641ms step_avg:91.69ms
step:357/1645 train_time:32732ms step_avg:91.69ms
step:358/1645 train_time:32824ms step_avg:91.69ms
step:359/1645 train_time:32916ms step_avg:91.69ms
step:360/1645 train_time:33008ms step_avg:91.69ms
step:361/1645 train_time:33100ms step_avg:91.69ms
step:362/1645 train_time:33192ms step_avg:91.69ms
step:363/1645 train_time:33283ms step_avg:91.69ms
step:364/1645 train_time:33375ms step_avg:91.69ms
step:365/1645 train_time:33466ms step_avg:91.69ms
step:366/1645 train_time:33557ms step_avg:91.69ms
step:367/1645 train_time:33649ms step_avg:91.69ms
step:368/1645 train_time:33740ms step_avg:91.69ms
step:369/1645 train_time:33833ms step_avg:91.69ms
step:370/1645 train_time:33923ms step_avg:91.68ms
step:371/1645 train_time:34016ms step_avg:91.69ms
step:372/1645 train_time:34108ms step_avg:91.69ms
step:373/1645 train_time:34201ms step_avg:91.69ms
step:374/1645 train_time:34293ms step_avg:91.69ms
step:375/1645 train_time:34385ms step_avg:91.69ms
step:375/1645 val_loss:3.8152 train_time:34477ms step_avg:91.94ms
step:376/1645 train_time:34498ms step_avg:91.75ms
step:377/1645 train_time:34574ms step_avg:91.71ms
step:378/1645 train_time:34669ms step_avg:91.72ms
step:379/1645 train_time:34761ms step_avg:91.72ms
step:380/1645 train_time:34852ms step_avg:91.72ms
step:381/1645 train_time:34942ms step_avg:91.71ms
step:382/1645 train_time:35032ms step_avg:91.71ms
step:383/1645 train_time:35123ms step_avg:91.71ms
step:384/1645 train_time:35214ms step_avg:91.70ms
step:385/1645 train_time:35305ms step_avg:91.70ms
step:386/1645 train_time:35396ms step_avg:91.70ms
step:387/1645 train_time:35490ms step_avg:91.70ms
step:388/1645 train_time:35582ms step_avg:91.71ms
step:389/1645 train_time:35675ms step_avg:91.71ms
step:390/1645 train_time:35767ms step_avg:91.71ms
step:391/1645 train_time:35858ms step_avg:91.71ms
step:392/1645 train_time:35949ms step_avg:91.71ms
step:393/1645 train_time:36040ms step_avg:91.71ms
step:394/1645 train_time:36131ms step_avg:91.70ms
step:395/1645 train_time:36222ms step_avg:91.70ms
step:396/1645 train_time:36313ms step_avg:91.70ms
step:397/1645 train_time:36404ms step_avg:91.70ms
step:398/1645 train_time:36497ms step_avg:91.70ms
step:399/1645 train_time:36588ms step_avg:91.70ms
step:400/1645 train_time:36680ms step_avg:91.70ms
step:401/1645 train_time:36772ms step_avg:91.70ms
step:402/1645 train_time:36864ms step_avg:91.70ms
step:403/1645 train_time:36955ms step_avg:91.70ms
step:404/1645 train_time:37047ms step_avg:91.70ms
step:405/1645 train_time:37138ms step_avg:91.70ms
step:406/1645 train_time:37229ms step_avg:91.70ms
step:407/1645 train_time:37321ms step_avg:91.70ms
step:408/1645 train_time:37412ms step_avg:91.70ms
step:409/1645 train_time:37504ms step_avg:91.70ms
step:410/1645 train_time:37594ms step_avg:91.69ms
step:411/1645 train_time:37687ms step_avg:91.70ms
step:412/1645 train_time:37778ms step_avg:91.69ms
step:413/1645 train_time:37870ms step_avg:91.70ms
step:414/1645 train_time:37962ms step_avg:91.70ms
step:415/1645 train_time:38053ms step_avg:91.69ms
step:416/1645 train_time:38145ms step_avg:91.69ms
step:417/1645 train_time:38236ms step_avg:91.69ms
step:418/1645 train_time:38328ms step_avg:91.69ms
step:419/1645 train_time:38418ms step_avg:91.69ms
step:420/1645 train_time:38509ms step_avg:91.69ms
step:421/1645 train_time:38601ms step_avg:91.69ms
step:422/1645 train_time:38692ms step_avg:91.69ms
step:423/1645 train_time:38783ms step_avg:91.69ms
step:424/1645 train_time:38875ms step_avg:91.69ms
step:425/1645 train_time:38968ms step_avg:91.69ms
step:426/1645 train_time:39059ms step_avg:91.69ms
step:427/1645 train_time:39150ms step_avg:91.69ms
step:428/1645 train_time:39242ms step_avg:91.69ms
step:429/1645 train_time:39334ms step_avg:91.69ms
step:430/1645 train_time:39425ms step_avg:91.69ms
step:431/1645 train_time:39516ms step_avg:91.68ms
step:432/1645 train_time:39609ms step_avg:91.69ms
step:433/1645 train_time:39700ms step_avg:91.69ms
step:434/1645 train_time:39791ms step_avg:91.68ms
step:435/1645 train_time:39884ms step_avg:91.69ms
step:436/1645 train_time:39975ms step_avg:91.69ms
step:437/1645 train_time:40067ms step_avg:91.69ms
step:438/1645 train_time:40158ms step_avg:91.69ms
step:439/1645 train_time:40250ms step_avg:91.69ms
step:440/1645 train_time:40342ms step_avg:91.69ms
step:441/1645 train_time:40433ms step_avg:91.68ms
step:442/1645 train_time:40524ms step_avg:91.68ms
step:443/1645 train_time:40617ms step_avg:91.69ms
step:444/1645 train_time:40707ms step_avg:91.68ms
step:445/1645 train_time:40798ms step_avg:91.68ms
step:446/1645 train_time:40890ms step_avg:91.68ms
step:447/1645 train_time:40982ms step_avg:91.68ms
step:448/1645 train_time:41073ms step_avg:91.68ms
step:449/1645 train_time:41165ms step_avg:91.68ms
step:450/1645 train_time:41257ms step_avg:91.68ms
step:451/1645 train_time:41348ms step_avg:91.68ms
step:452/1645 train_time:41440ms step_avg:91.68ms
step:453/1645 train_time:41532ms step_avg:91.68ms
step:454/1645 train_time:41623ms step_avg:91.68ms
step:455/1645 train_time:41715ms step_avg:91.68ms
step:456/1645 train_time:41806ms step_avg:91.68ms
step:457/1645 train_time:41897ms step_avg:91.68ms
step:458/1645 train_time:41989ms step_avg:91.68ms
step:459/1645 train_time:42080ms step_avg:91.68ms
step:460/1645 train_time:42172ms step_avg:91.68ms
step:461/1645 train_time:42263ms step_avg:91.68ms
step:462/1645 train_time:42355ms step_avg:91.68ms
step:463/1645 train_time:42447ms step_avg:91.68ms
step:464/1645 train_time:42539ms step_avg:91.68ms
step:465/1645 train_time:42632ms step_avg:91.68ms
step:466/1645 train_time:42724ms step_avg:91.68ms
step:467/1645 train_time:42814ms step_avg:91.68ms
step:468/1645 train_time:42906ms step_avg:91.68ms
step:469/1645 train_time:42997ms step_avg:91.68ms
step:470/1645 train_time:43089ms step_avg:91.68ms
step:471/1645 train_time:43180ms step_avg:91.68ms
step:472/1645 train_time:43271ms step_avg:91.68ms
step:473/1645 train_time:43363ms step_avg:91.68ms
step:474/1645 train_time:43455ms step_avg:91.68ms
step:475/1645 train_time:43547ms step_avg:91.68ms
step:476/1645 train_time:43638ms step_avg:91.68ms
step:477/1645 train_time:43731ms step_avg:91.68ms
step:478/1645 train_time:43821ms step_avg:91.68ms
step:479/1645 train_time:43912ms step_avg:91.68ms
step:480/1645 train_time:44004ms step_avg:91.67ms
step:481/1645 train_time:44095ms step_avg:91.67ms
step:482/1645 train_time:44187ms step_avg:91.67ms
step:483/1645 train_time:44278ms step_avg:91.67ms
step:484/1645 train_time:44370ms step_avg:91.67ms
step:485/1645 train_time:44462ms step_avg:91.67ms
step:486/1645 train_time:44554ms step_avg:91.68ms
step:487/1645 train_time:44647ms step_avg:91.68ms
step:488/1645 train_time:44740ms step_avg:91.68ms
step:489/1645 train_time:44831ms step_avg:91.68ms
step:490/1645 train_time:44922ms step_avg:91.68ms
step:491/1645 train_time:45013ms step_avg:91.68ms
step:492/1645 train_time:45104ms step_avg:91.68ms
step:493/1645 train_time:45195ms step_avg:91.67ms
step:494/1645 train_time:45286ms step_avg:91.67ms
step:495/1645 train_time:45378ms step_avg:91.67ms
step:496/1645 train_time:45471ms step_avg:91.68ms
step:497/1645 train_time:45563ms step_avg:91.68ms
step:498/1645 train_time:45656ms step_avg:91.68ms
step:499/1645 train_time:45747ms step_avg:91.68ms
step:500/1645 train_time:45841ms step_avg:91.68ms
step:500/1645 val_loss:3.7132 train_time:45931ms step_avg:91.86ms
step:501/1645 train_time:45952ms step_avg:91.72ms
step:502/1645 train_time:46027ms step_avg:91.69ms
step:503/1645 train_time:46121ms step_avg:91.69ms
step:504/1645 train_time:46213ms step_avg:91.69ms
step:505/1645 train_time:46304ms step_avg:91.69ms
step:506/1645 train_time:46394ms step_avg:91.69ms
step:507/1645 train_time:46485ms step_avg:91.69ms
step:508/1645 train_time:46576ms step_avg:91.69ms
step:509/1645 train_time:46667ms step_avg:91.68ms
step:510/1645 train_time:46758ms step_avg:91.68ms
step:511/1645 train_time:46850ms step_avg:91.68ms
step:512/1645 train_time:46942ms step_avg:91.68ms
step:513/1645 train_time:47034ms step_avg:91.69ms
step:514/1645 train_time:47128ms step_avg:91.69ms
step:515/1645 train_time:47221ms step_avg:91.69ms
step:516/1645 train_time:47313ms step_avg:91.69ms
step:517/1645 train_time:47404ms step_avg:91.69ms
step:518/1645 train_time:47495ms step_avg:91.69ms
step:519/1645 train_time:47586ms step_avg:91.69ms
step:520/1645 train_time:47677ms step_avg:91.69ms
step:521/1645 train_time:47767ms step_avg:91.68ms
step:522/1645 train_time:47858ms step_avg:91.68ms
step:523/1645 train_time:47950ms step_avg:91.68ms
step:524/1645 train_time:48043ms step_avg:91.69ms
step:525/1645 train_time:48136ms step_avg:91.69ms
step:526/1645 train_time:48228ms step_avg:91.69ms
step:527/1645 train_time:48319ms step_avg:91.69ms
step:528/1645 train_time:48411ms step_avg:91.69ms
step:529/1645 train_time:48502ms step_avg:91.69ms
step:530/1645 train_time:48593ms step_avg:91.69ms
step:531/1645 train_time:48684ms step_avg:91.68ms
step:532/1645 train_time:48775ms step_avg:91.68ms
step:533/1645 train_time:48867ms step_avg:91.68ms
step:534/1645 train_time:48959ms step_avg:91.68ms
step:535/1645 train_time:49051ms step_avg:91.68ms
step:536/1645 train_time:49143ms step_avg:91.68ms
step:537/1645 train_time:49235ms step_avg:91.69ms
step:538/1645 train_time:49326ms step_avg:91.68ms
step:539/1645 train_time:49418ms step_avg:91.69ms
step:540/1645 train_time:49509ms step_avg:91.68ms
step:541/1645 train_time:49601ms step_avg:91.68ms
step:542/1645 train_time:49691ms step_avg:91.68ms
step:543/1645 train_time:49783ms step_avg:91.68ms
step:544/1645 train_time:49874ms step_avg:91.68ms
step:545/1645 train_time:49966ms step_avg:91.68ms
step:546/1645 train_time:50058ms step_avg:91.68ms
step:547/1645 train_time:50150ms step_avg:91.68ms
step:548/1645 train_time:50241ms step_avg:91.68ms
step:549/1645 train_time:50333ms step_avg:91.68ms
step:550/1645 train_time:50426ms step_avg:91.68ms
step:551/1645 train_time:50520ms step_avg:91.69ms
step:552/1645 train_time:50612ms step_avg:91.69ms
step:553/1645 train_time:50705ms step_avg:91.69ms
step:554/1645 train_time:50797ms step_avg:91.69ms
step:555/1645 train_time:50890ms step_avg:91.69ms
step:556/1645 train_time:50984ms step_avg:91.70ms
step:557/1645 train_time:51077ms step_avg:91.70ms
step:558/1645 train_time:51170ms step_avg:91.70ms
step:559/1645 train_time:51263ms step_avg:91.70ms
step:560/1645 train_time:51355ms step_avg:91.71ms
step:561/1645 train_time:51449ms step_avg:91.71ms
step:562/1645 train_time:51542ms step_avg:91.71ms
step:563/1645 train_time:51634ms step_avg:91.71ms
step:564/1645 train_time:51728ms step_avg:91.72ms
step:565/1645 train_time:51821ms step_avg:91.72ms
step:566/1645 train_time:51914ms step_avg:91.72ms
step:567/1645 train_time:52007ms step_avg:91.72ms
step:568/1645 train_time:52101ms step_avg:91.73ms
step:569/1645 train_time:52193ms step_avg:91.73ms
step:570/1645 train_time:52286ms step_avg:91.73ms
step:571/1645 train_time:52379ms step_avg:91.73ms
step:572/1645 train_time:52471ms step_avg:91.73ms
step:573/1645 train_time:52563ms step_avg:91.73ms
step:574/1645 train_time:52656ms step_avg:91.74ms
step:575/1645 train_time:52749ms step_avg:91.74ms
step:576/1645 train_time:52842ms step_avg:91.74ms
step:577/1645 train_time:52936ms step_avg:91.74ms
step:578/1645 train_time:53029ms step_avg:91.75ms
step:579/1645 train_time:53122ms step_avg:91.75ms
step:580/1645 train_time:53215ms step_avg:91.75ms
step:581/1645 train_time:53309ms step_avg:91.75ms
step:582/1645 train_time:53402ms step_avg:91.76ms
step:583/1645 train_time:53494ms step_avg:91.76ms
step:584/1645 train_time:53588ms step_avg:91.76ms
step:585/1645 train_time:53680ms step_avg:91.76ms
step:586/1645 train_time:53772ms step_avg:91.76ms
step:587/1645 train_time:53866ms step_avg:91.76ms
step:588/1645 train_time:53959ms step_avg:91.77ms
step:589/1645 train_time:54052ms step_avg:91.77ms
step:590/1645 train_time:54145ms step_avg:91.77ms
step:591/1645 train_time:54238ms step_avg:91.77ms
step:592/1645 train_time:54331ms step_avg:91.77ms
step:593/1645 train_time:54424ms step_avg:91.78ms
step:594/1645 train_time:54518ms step_avg:91.78ms
step:595/1645 train_time:54611ms step_avg:91.78ms
step:596/1645 train_time:54703ms step_avg:91.78ms
step:597/1645 train_time:54795ms step_avg:91.78ms
step:598/1645 train_time:54888ms step_avg:91.79ms
step:599/1645 train_time:54980ms step_avg:91.79ms
step:600/1645 train_time:55073ms step_avg:91.79ms
step:601/1645 train_time:55167ms step_avg:91.79ms
step:602/1645 train_time:55260ms step_avg:91.79ms
step:603/1645 train_time:55352ms step_avg:91.79ms
step:604/1645 train_time:55446ms step_avg:91.80ms
step:605/1645 train_time:55540ms step_avg:91.80ms
step:606/1645 train_time:55632ms step_avg:91.80ms
step:607/1645 train_time:55725ms step_avg:91.80ms
step:608/1645 train_time:55818ms step_avg:91.81ms
step:609/1645 train_time:55911ms step_avg:91.81ms
step:610/1645 train_time:56005ms step_avg:91.81ms
step:611/1645 train_time:56098ms step_avg:91.81ms
step:612/1645 train_time:56191ms step_avg:91.81ms
step:613/1645 train_time:56285ms step_avg:91.82ms
step:614/1645 train_time:56378ms step_avg:91.82ms
step:615/1645 train_time:56471ms step_avg:91.82ms
step:616/1645 train_time:56563ms step_avg:91.82ms
step:617/1645 train_time:56656ms step_avg:91.82ms
step:618/1645 train_time:56750ms step_avg:91.83ms
step:619/1645 train_time:56842ms step_avg:91.83ms
step:620/1645 train_time:56935ms step_avg:91.83ms
step:621/1645 train_time:57029ms step_avg:91.83ms
step:622/1645 train_time:57121ms step_avg:91.84ms
step:623/1645 train_time:57215ms step_avg:91.84ms
step:624/1645 train_time:57308ms step_avg:91.84ms
step:625/1645 train_time:57401ms step_avg:91.84ms
step:625/1645 val_loss:3.6152 train_time:57494ms step_avg:91.99ms
step:626/1645 train_time:57515ms step_avg:91.88ms
step:627/1645 train_time:57589ms step_avg:91.85ms
step:628/1645 train_time:57692ms step_avg:91.87ms
step:629/1645 train_time:57788ms step_avg:91.87ms
step:630/1645 train_time:57880ms step_avg:91.87ms
step:631/1645 train_time:57971ms step_avg:91.87ms
step:632/1645 train_time:58063ms step_avg:91.87ms
step:633/1645 train_time:58155ms step_avg:91.87ms
step:634/1645 train_time:58246ms step_avg:91.87ms
step:635/1645 train_time:58338ms step_avg:91.87ms
step:636/1645 train_time:58432ms step_avg:91.87ms
step:637/1645 train_time:58526ms step_avg:91.88ms
step:638/1645 train_time:58621ms step_avg:91.88ms
step:639/1645 train_time:58717ms step_avg:91.89ms
step:640/1645 train_time:58811ms step_avg:91.89ms
step:641/1645 train_time:58904ms step_avg:91.89ms
step:642/1645 train_time:58996ms step_avg:91.89ms
step:643/1645 train_time:59088ms step_avg:91.89ms
step:644/1645 train_time:59180ms step_avg:91.90ms
step:645/1645 train_time:59273ms step_avg:91.90ms
step:646/1645 train_time:59365ms step_avg:91.90ms
step:647/1645 train_time:59457ms step_avg:91.90ms
step:648/1645 train_time:59551ms step_avg:91.90ms
step:649/1645 train_time:59645ms step_avg:91.90ms
step:650/1645 train_time:59741ms step_avg:91.91ms
step:651/1645 train_time:59834ms step_avg:91.91ms
step:652/1645 train_time:59926ms step_avg:91.91ms
step:653/1645 train_time:60019ms step_avg:91.91ms
step:654/1645 train_time:60111ms step_avg:91.91ms
step:655/1645 train_time:60203ms step_avg:91.91ms
step:656/1645 train_time:60295ms step_avg:91.91ms
step:657/1645 train_time:60387ms step_avg:91.91ms
step:658/1645 train_time:60482ms step_avg:91.92ms
step:659/1645 train_time:60576ms step_avg:91.92ms
step:660/1645 train_time:60669ms step_avg:91.92ms
step:661/1645 train_time:60762ms step_avg:91.92ms
step:662/1645 train_time:60856ms step_avg:91.93ms
step:663/1645 train_time:60948ms step_avg:91.93ms
step:664/1645 train_time:61042ms step_avg:91.93ms
step:665/1645 train_time:61135ms step_avg:91.93ms
step:666/1645 train_time:61228ms step_avg:91.93ms
step:667/1645 train_time:61320ms step_avg:91.93ms
step:668/1645 train_time:61412ms step_avg:91.93ms
step:669/1645 train_time:61505ms step_avg:91.94ms
step:670/1645 train_time:61599ms step_avg:91.94ms
step:671/1645 train_time:61693ms step_avg:91.94ms
step:672/1645 train_time:61785ms step_avg:91.94ms
step:673/1645 train_time:61878ms step_avg:91.94ms
step:674/1645 train_time:61972ms step_avg:91.95ms
step:675/1645 train_time:62065ms step_avg:91.95ms
step:676/1645 train_time:62157ms step_avg:91.95ms
step:677/1645 train_time:62249ms step_avg:91.95ms
step:678/1645 train_time:62341ms step_avg:91.95ms
step:679/1645 train_time:62434ms step_avg:91.95ms
step:680/1645 train_time:62526ms step_avg:91.95ms
step:681/1645 train_time:62619ms step_avg:91.95ms
step:682/1645 train_time:62713ms step_avg:91.95ms
step:683/1645 train_time:62806ms step_avg:91.96ms
step:684/1645 train_time:62900ms step_avg:91.96ms
step:685/1645 train_time:62994ms step_avg:91.96ms
step:686/1645 train_time:63086ms step_avg:91.96ms
step:687/1645 train_time:63179ms step_avg:91.96ms
step:688/1645 train_time:63271ms step_avg:91.96ms
step:689/1645 train_time:63363ms step_avg:91.96ms
step:690/1645 train_time:63456ms step_avg:91.97ms
step:691/1645 train_time:63549ms step_avg:91.97ms
step:692/1645 train_time:63641ms step_avg:91.97ms
step:693/1645 train_time:63735ms step_avg:91.97ms
step:694/1645 train_time:63828ms step_avg:91.97ms
step:695/1645 train_time:63921ms step_avg:91.97ms
step:696/1645 train_time:64014ms step_avg:91.97ms
step:697/1645 train_time:64107ms step_avg:91.98ms
step:698/1645 train_time:64201ms step_avg:91.98ms
step:699/1645 train_time:64294ms step_avg:91.98ms
step:700/1645 train_time:64387ms step_avg:91.98ms
step:701/1645 train_time:64480ms step_avg:91.98ms
step:702/1645 train_time:64573ms step_avg:91.98ms
step:703/1645 train_time:64666ms step_avg:91.99ms
step:704/1645 train_time:64760ms step_avg:91.99ms
step:705/1645 train_time:64853ms step_avg:91.99ms
step:706/1645 train_time:64947ms step_avg:91.99ms
step:707/1645 train_time:65039ms step_avg:91.99ms
step:708/1645 train_time:65132ms step_avg:91.99ms
step:709/1645 train_time:65225ms step_avg:92.00ms
step:710/1645 train_time:65317ms step_avg:92.00ms
step:711/1645 train_time:65409ms step_avg:92.00ms
step:712/1645 train_time:65503ms step_avg:92.00ms
step:713/1645 train_time:65596ms step_avg:92.00ms
step:714/1645 train_time:65688ms step_avg:92.00ms
step:715/1645 train_time:65782ms step_avg:92.00ms
step:716/1645 train_time:65875ms step_avg:92.00ms
step:717/1645 train_time:65968ms step_avg:92.01ms
step:718/1645 train_time:66062ms step_avg:92.01ms
step:719/1645 train_time:66155ms step_avg:92.01ms
step:720/1645 train_time:66247ms step_avg:92.01ms
step:721/1645 train_time:66340ms step_avg:92.01ms
step:722/1645 train_time:66434ms step_avg:92.01ms
step:723/1645 train_time:66526ms step_avg:92.01ms
step:724/1645 train_time:66619ms step_avg:92.02ms
step:725/1645 train_time:66712ms step_avg:92.02ms
step:726/1645 train_time:66805ms step_avg:92.02ms
step:727/1645 train_time:66898ms step_avg:92.02ms
step:728/1645 train_time:66990ms step_avg:92.02ms
step:729/1645 train_time:67084ms step_avg:92.02ms
step:730/1645 train_time:67177ms step_avg:92.02ms
step:731/1645 train_time:67270ms step_avg:92.02ms
step:732/1645 train_time:67363ms step_avg:92.03ms
step:733/1645 train_time:67457ms step_avg:92.03ms
step:734/1645 train_time:67548ms step_avg:92.03ms
step:735/1645 train_time:67641ms step_avg:92.03ms
step:736/1645 train_time:67734ms step_avg:92.03ms
step:737/1645 train_time:67827ms step_avg:92.03ms
step:738/1645 train_time:67921ms step_avg:92.03ms
step:739/1645 train_time:68014ms step_avg:92.03ms
step:740/1645 train_time:68106ms step_avg:92.04ms
step:741/1645 train_time:68199ms step_avg:92.04ms
step:742/1645 train_time:68291ms step_avg:92.04ms
step:743/1645 train_time:68385ms step_avg:92.04ms
step:744/1645 train_time:68477ms step_avg:92.04ms
step:745/1645 train_time:68570ms step_avg:92.04ms
step:746/1645 train_time:68664ms step_avg:92.04ms
step:747/1645 train_time:68756ms step_avg:92.04ms
step:748/1645 train_time:68848ms step_avg:92.04ms
step:749/1645 train_time:68941ms step_avg:92.04ms
step:750/1645 train_time:69035ms step_avg:92.05ms
step:750/1645 val_loss:3.5608 train_time:69127ms step_avg:92.17ms
step:751/1645 train_time:69148ms step_avg:92.07ms
step:752/1645 train_time:69224ms step_avg:92.05ms
step:753/1645 train_time:69320ms step_avg:92.06ms
step:754/1645 train_time:69413ms step_avg:92.06ms
step:755/1645 train_time:69504ms step_avg:92.06ms
step:756/1645 train_time:69596ms step_avg:92.06ms
step:757/1645 train_time:69688ms step_avg:92.06ms
step:758/1645 train_time:69780ms step_avg:92.06ms
step:759/1645 train_time:69873ms step_avg:92.06ms
step:760/1645 train_time:69964ms step_avg:92.06ms
step:761/1645 train_time:70057ms step_avg:92.06ms
step:762/1645 train_time:70151ms step_avg:92.06ms
step:763/1645 train_time:70245ms step_avg:92.06ms
step:764/1645 train_time:70341ms step_avg:92.07ms
step:765/1645 train_time:70435ms step_avg:92.07ms
step:766/1645 train_time:70527ms step_avg:92.07ms
step:767/1645 train_time:70619ms step_avg:92.07ms
step:768/1645 train_time:70712ms step_avg:92.07ms
step:769/1645 train_time:70804ms step_avg:92.07ms
step:770/1645 train_time:70896ms step_avg:92.07ms
step:771/1645 train_time:70988ms step_avg:92.07ms
step:772/1645 train_time:71082ms step_avg:92.08ms
step:773/1645 train_time:71176ms step_avg:92.08ms
step:774/1645 train_time:71270ms step_avg:92.08ms
step:775/1645 train_time:71363ms step_avg:92.08ms
step:776/1645 train_time:71455ms step_avg:92.08ms
step:777/1645 train_time:71548ms step_avg:92.08ms
step:778/1645 train_time:71641ms step_avg:92.08ms
step:779/1645 train_time:71734ms step_avg:92.08ms
step:780/1645 train_time:71827ms step_avg:92.09ms
step:781/1645 train_time:71920ms step_avg:92.09ms
step:782/1645 train_time:72013ms step_avg:92.09ms
step:783/1645 train_time:72106ms step_avg:92.09ms
step:784/1645 train_time:72201ms step_avg:92.09ms
step:785/1645 train_time:72294ms step_avg:92.09ms
step:786/1645 train_time:72387ms step_avg:92.09ms
step:787/1645 train_time:72480ms step_avg:92.10ms
step:788/1645 train_time:72573ms step_avg:92.10ms
step:789/1645 train_time:72666ms step_avg:92.10ms
step:790/1645 train_time:72758ms step_avg:92.10ms
step:791/1645 train_time:72849ms step_avg:92.10ms
step:792/1645 train_time:72942ms step_avg:92.10ms
step:793/1645 train_time:73036ms step_avg:92.10ms
step:794/1645 train_time:73128ms step_avg:92.10ms
step:795/1645 train_time:73221ms step_avg:92.10ms
step:796/1645 train_time:73316ms step_avg:92.11ms
step:797/1645 train_time:73409ms step_avg:92.11ms
step:798/1645 train_time:73503ms step_avg:92.11ms
step:799/1645 train_time:73597ms step_avg:92.11ms
step:800/1645 train_time:73689ms step_avg:92.11ms
step:801/1645 train_time:73782ms step_avg:92.11ms
step:802/1645 train_time:73875ms step_avg:92.11ms
step:803/1645 train_time:73967ms step_avg:92.11ms
step:804/1645 train_time:74060ms step_avg:92.11ms
step:805/1645 train_time:74152ms step_avg:92.11ms
step:806/1645 train_time:74245ms step_avg:92.12ms
step:807/1645 train_time:74338ms step_avg:92.12ms
step:808/1645 train_time:74431ms step_avg:92.12ms
step:809/1645 train_time:74524ms step_avg:92.12ms
step:810/1645 train_time:74617ms step_avg:92.12ms
step:811/1645 train_time:74710ms step_avg:92.12ms
step:812/1645 train_time:74804ms step_avg:92.12ms
step:813/1645 train_time:74896ms step_avg:92.12ms
step:814/1645 train_time:74989ms step_avg:92.12ms
step:815/1645 train_time:75083ms step_avg:92.13ms
step:816/1645 train_time:75175ms step_avg:92.13ms
step:817/1645 train_time:75268ms step_avg:92.13ms
step:818/1645 train_time:75360ms step_avg:92.13ms
step:819/1645 train_time:75453ms step_avg:92.13ms
step:820/1645 train_time:75546ms step_avg:92.13ms
step:821/1645 train_time:75640ms step_avg:92.13ms
step:822/1645 train_time:75734ms step_avg:92.13ms
step:823/1645 train_time:75826ms step_avg:92.13ms
step:824/1645 train_time:75919ms step_avg:92.13ms
step:825/1645 train_time:76012ms step_avg:92.14ms
step:826/1645 train_time:76105ms step_avg:92.14ms
step:827/1645 train_time:76198ms step_avg:92.14ms
step:828/1645 train_time:76291ms step_avg:92.14ms
step:829/1645 train_time:76384ms step_avg:92.14ms
step:830/1645 train_time:76477ms step_avg:92.14ms
step:831/1645 train_time:76570ms step_avg:92.14ms
step:832/1645 train_time:76663ms step_avg:92.14ms
step:833/1645 train_time:76755ms step_avg:92.14ms
step:834/1645 train_time:76848ms step_avg:92.14ms
step:835/1645 train_time:76941ms step_avg:92.15ms
step:836/1645 train_time:77035ms step_avg:92.15ms
step:837/1645 train_time:77128ms step_avg:92.15ms
step:838/1645 train_time:77220ms step_avg:92.15ms
step:839/1645 train_time:77313ms step_avg:92.15ms
step:840/1645 train_time:77406ms step_avg:92.15ms
step:841/1645 train_time:77500ms step_avg:92.15ms
step:842/1645 train_time:77592ms step_avg:92.15ms
step:843/1645 train_time:77686ms step_avg:92.15ms
step:844/1645 train_time:77779ms step_avg:92.15ms
step:845/1645 train_time:77871ms step_avg:92.15ms
step:846/1645 train_time:77963ms step_avg:92.16ms
step:847/1645 train_time:78057ms step_avg:92.16ms
step:848/1645 train_time:78150ms step_avg:92.16ms
step:849/1645 train_time:78244ms step_avg:92.16ms
step:850/1645 train_time:78336ms step_avg:92.16ms
step:851/1645 train_time:78430ms step_avg:92.16ms
step:852/1645 train_time:78523ms step_avg:92.16ms
step:853/1645 train_time:78618ms step_avg:92.17ms
step:854/1645 train_time:78711ms step_avg:92.17ms
step:855/1645 train_time:78803ms step_avg:92.17ms
step:856/1645 train_time:78896ms step_avg:92.17ms
step:857/1645 train_time:78988ms step_avg:92.17ms
step:858/1645 train_time:79081ms step_avg:92.17ms
step:859/1645 train_time:79175ms step_avg:92.17ms
step:860/1645 train_time:79268ms step_avg:92.17ms
step:861/1645 train_time:79360ms step_avg:92.17ms
step:862/1645 train_time:79453ms step_avg:92.17ms
step:863/1645 train_time:79545ms step_avg:92.17ms
step:864/1645 train_time:79638ms step_avg:92.17ms
step:865/1645 train_time:79730ms step_avg:92.17ms
step:866/1645 train_time:79824ms step_avg:92.18ms
step:867/1645 train_time:79917ms step_avg:92.18ms
step:868/1645 train_time:80009ms step_avg:92.18ms
step:869/1645 train_time:80103ms step_avg:92.18ms
step:870/1645 train_time:80196ms step_avg:92.18ms
step:871/1645 train_time:80288ms step_avg:92.18ms
step:872/1645 train_time:80381ms step_avg:92.18ms
step:873/1645 train_time:80474ms step_avg:92.18ms
step:874/1645 train_time:80567ms step_avg:92.18ms
step:875/1645 train_time:80660ms step_avg:92.18ms
step:875/1645 val_loss:3.5166 train_time:80752ms step_avg:92.29ms
step:876/1645 train_time:80772ms step_avg:92.21ms
step:877/1645 train_time:80852ms step_avg:92.19ms
step:878/1645 train_time:80945ms step_avg:92.19ms
step:879/1645 train_time:81038ms step_avg:92.19ms
step:880/1645 train_time:81130ms step_avg:92.19ms
step:881/1645 train_time:81222ms step_avg:92.19ms
step:882/1645 train_time:81314ms step_avg:92.19ms
step:883/1645 train_time:81406ms step_avg:92.19ms
step:884/1645 train_time:81498ms step_avg:92.19ms
step:885/1645 train_time:81590ms step_avg:92.19ms
step:886/1645 train_time:81684ms step_avg:92.19ms
step:887/1645 train_time:81778ms step_avg:92.20ms
step:888/1645 train_time:81872ms step_avg:92.20ms
step:889/1645 train_time:81966ms step_avg:92.20ms
step:890/1645 train_time:82059ms step_avg:92.20ms
step:891/1645 train_time:82152ms step_avg:92.20ms
step:892/1645 train_time:82244ms step_avg:92.20ms
step:893/1645 train_time:82336ms step_avg:92.20ms
step:894/1645 train_time:82429ms step_avg:92.20ms
step:895/1645 train_time:82521ms step_avg:92.20ms
step:896/1645 train_time:82614ms step_avg:92.20ms
step:897/1645 train_time:82708ms step_avg:92.21ms
step:898/1645 train_time:82801ms step_avg:92.21ms
step:899/1645 train_time:82894ms step_avg:92.21ms
step:900/1645 train_time:82988ms step_avg:92.21ms
step:901/1645 train_time:83082ms step_avg:92.21ms
step:902/1645 train_time:83174ms step_avg:92.21ms
step:903/1645 train_time:83267ms step_avg:92.21ms
step:904/1645 train_time:83359ms step_avg:92.21ms
step:905/1645 train_time:83451ms step_avg:92.21ms
step:906/1645 train_time:83543ms step_avg:92.21ms
step:907/1645 train_time:83637ms step_avg:92.21ms
step:908/1645 train_time:83730ms step_avg:92.21ms
step:909/1645 train_time:83824ms step_avg:92.22ms
step:910/1645 train_time:83918ms step_avg:92.22ms
step:911/1645 train_time:84012ms step_avg:92.22ms
step:912/1645 train_time:84105ms step_avg:92.22ms
step:913/1645 train_time:84198ms step_avg:92.22ms
step:914/1645 train_time:84290ms step_avg:92.22ms
step:915/1645 train_time:84383ms step_avg:92.22ms
step:916/1645 train_time:84475ms step_avg:92.22ms
step:917/1645 train_time:84569ms step_avg:92.22ms
step:918/1645 train_time:84661ms step_avg:92.22ms
step:919/1645 train_time:84754ms step_avg:92.22ms
step:920/1645 train_time:84847ms step_avg:92.23ms
step:921/1645 train_time:84941ms step_avg:92.23ms
step:922/1645 train_time:85034ms step_avg:92.23ms
step:923/1645 train_time:85127ms step_avg:92.23ms
step:924/1645 train_time:85221ms step_avg:92.23ms
step:925/1645 train_time:85312ms step_avg:92.23ms
step:926/1645 train_time:85405ms step_avg:92.23ms
step:927/1645 train_time:85497ms step_avg:92.23ms
step:928/1645 train_time:85590ms step_avg:92.23ms
step:929/1645 train_time:85684ms step_avg:92.23ms
step:930/1645 train_time:85777ms step_avg:92.23ms
step:931/1645 train_time:85870ms step_avg:92.23ms
step:932/1645 train_time:85963ms step_avg:92.24ms
step:933/1645 train_time:86057ms step_avg:92.24ms
step:934/1645 train_time:86150ms step_avg:92.24ms
step:935/1645 train_time:86243ms step_avg:92.24ms
step:936/1645 train_time:86337ms step_avg:92.24ms
step:937/1645 train_time:86429ms step_avg:92.24ms
step:938/1645 train_time:86523ms step_avg:92.24ms
step:939/1645 train_time:86615ms step_avg:92.24ms
step:940/1645 train_time:86708ms step_avg:92.24ms
step:941/1645 train_time:86800ms step_avg:92.24ms
step:942/1645 train_time:86893ms step_avg:92.24ms
step:943/1645 train_time:86987ms step_avg:92.24ms
step:944/1645 train_time:87080ms step_avg:92.25ms
step:945/1645 train_time:87172ms step_avg:92.25ms
step:946/1645 train_time:87267ms step_avg:92.25ms
step:947/1645 train_time:87360ms step_avg:92.25ms
step:948/1645 train_time:87452ms step_avg:92.25ms
step:949/1645 train_time:87545ms step_avg:92.25ms
step:950/1645 train_time:87638ms step_avg:92.25ms
step:951/1645 train_time:87731ms step_avg:92.25ms
step:952/1645 train_time:87824ms step_avg:92.25ms
step:953/1645 train_time:87917ms step_avg:92.25ms
step:954/1645 train_time:88010ms step_avg:92.25ms
step:955/1645 train_time:88103ms step_avg:92.25ms
step:956/1645 train_time:88196ms step_avg:92.25ms
step:957/1645 train_time:88288ms step_avg:92.26ms
step:958/1645 train_time:88381ms step_avg:92.26ms
step:959/1645 train_time:88474ms step_avg:92.26ms
step:960/1645 train_time:88566ms step_avg:92.26ms
step:961/1645 train_time:88660ms step_avg:92.26ms
step:962/1645 train_time:88753ms step_avg:92.26ms
step:963/1645 train_time:88846ms step_avg:92.26ms
step:964/1645 train_time:88940ms step_avg:92.26ms
step:965/1645 train_time:89032ms step_avg:92.26ms
step:966/1645 train_time:89126ms step_avg:92.26ms
step:967/1645 train_time:89219ms step_avg:92.26ms
step:968/1645 train_time:89311ms step_avg:92.26ms
step:969/1645 train_time:89404ms step_avg:92.26ms
step:970/1645 train_time:89497ms step_avg:92.26ms
step:971/1645 train_time:89590ms step_avg:92.27ms
step:972/1645 train_time:89683ms step_avg:92.27ms
step:973/1645 train_time:89777ms step_avg:92.27ms
step:974/1645 train_time:89870ms step_avg:92.27ms
step:975/1645 train_time:89963ms step_avg:92.27ms
step:976/1645 train_time:90057ms step_avg:92.27ms
step:977/1645 train_time:90150ms step_avg:92.27ms
step:978/1645 train_time:90243ms step_avg:92.27ms
step:979/1645 train_time:90337ms step_avg:92.27ms
step:980/1645 train_time:90429ms step_avg:92.27ms
step:981/1645 train_time:90522ms step_avg:92.27ms
step:982/1645 train_time:90615ms step_avg:92.28ms
step:983/1645 train_time:90707ms step_avg:92.28ms
step:984/1645 train_time:90799ms step_avg:92.28ms
step:985/1645 train_time:90892ms step_avg:92.28ms
step:986/1645 train_time:90985ms step_avg:92.28ms
step:987/1645 train_time:91078ms step_avg:92.28ms
step:988/1645 train_time:91171ms step_avg:92.28ms
step:989/1645 train_time:91265ms step_avg:92.28ms
step:990/1645 train_time:91357ms step_avg:92.28ms
step:991/1645 train_time:91451ms step_avg:92.28ms
step:992/1645 train_time:91545ms step_avg:92.28ms
step:993/1645 train_time:91638ms step_avg:92.28ms
step:994/1645 train_time:91730ms step_avg:92.28ms
step:995/1645 train_time:91824ms step_avg:92.29ms
step:996/1645 train_time:91917ms step_avg:92.29ms
step:997/1645 train_time:92011ms step_avg:92.29ms
step:998/1645 train_time:92103ms step_avg:92.29ms
step:999/1645 train_time:92196ms step_avg:92.29ms
step:1000/1645 train_time:92289ms step_avg:92.29ms
step:1000/1645 val_loss:3.4662 train_time:92382ms step_avg:92.38ms
step:1001/1645 train_time:92403ms step_avg:92.31ms
step:1002/1645 train_time:92479ms step_avg:92.29ms
step:1003/1645 train_time:92573ms step_avg:92.30ms
step:1004/1645 train_time:92665ms step_avg:92.30ms
step:1005/1645 train_time:92757ms step_avg:92.30ms
step:1006/1645 train_time:92848ms step_avg:92.29ms
step:1007/1645 train_time:92941ms step_avg:92.29ms
step:1008/1645 train_time:93033ms step_avg:92.29ms
step:1009/1645 train_time:93124ms step_avg:92.29ms
step:1010/1645 train_time:93217ms step_avg:92.29ms
step:1011/1645 train_time:93311ms step_avg:92.30ms
step:1012/1645 train_time:93406ms step_avg:92.30ms
step:1013/1645 train_time:93501ms step_avg:92.30ms
step:1014/1645 train_time:93595ms step_avg:92.30ms
step:1015/1645 train_time:93687ms step_avg:92.30ms
step:1016/1645 train_time:93779ms step_avg:92.30ms
step:1017/1645 train_time:93871ms step_avg:92.30ms
step:1018/1645 train_time:93965ms step_avg:92.30ms
step:1019/1645 train_time:94057ms step_avg:92.30ms
step:1020/1645 train_time:94149ms step_avg:92.30ms
step:1021/1645 train_time:94242ms step_avg:92.30ms
step:1022/1645 train_time:94335ms step_avg:92.30ms
step:1023/1645 train_time:94429ms step_avg:92.31ms
step:1024/1645 train_time:94524ms step_avg:92.31ms
step:1025/1645 train_time:94617ms step_avg:92.31ms
step:1026/1645 train_time:94709ms step_avg:92.31ms
step:1027/1645 train_time:94801ms step_avg:92.31ms
step:1028/1645 train_time:94894ms step_avg:92.31ms
step:1029/1645 train_time:94987ms step_avg:92.31ms
step:1030/1645 train_time:95079ms step_avg:92.31ms
step:1031/1645 train_time:95171ms step_avg:92.31ms
step:1032/1645 train_time:95264ms step_avg:92.31ms
step:1033/1645 train_time:95358ms step_avg:92.31ms
step:1034/1645 train_time:95451ms step_avg:92.31ms
step:1035/1645 train_time:95544ms step_avg:92.31ms
step:1036/1645 train_time:95636ms step_avg:92.31ms
step:1037/1645 train_time:95730ms step_avg:92.31ms
step:1038/1645 train_time:95823ms step_avg:92.31ms
step:1039/1645 train_time:95916ms step_avg:92.32ms
step:1040/1645 train_time:96008ms step_avg:92.32ms
step:1041/1645 train_time:96103ms step_avg:92.32ms
step:1042/1645 train_time:96194ms step_avg:92.32ms
step:1043/1645 train_time:96287ms step_avg:92.32ms
step:1044/1645 train_time:96380ms step_avg:92.32ms
step:1045/1645 train_time:96473ms step_avg:92.32ms
step:1046/1645 train_time:96566ms step_avg:92.32ms
step:1047/1645 train_time:96659ms step_avg:92.32ms
step:1048/1645 train_time:96752ms step_avg:92.32ms
step:1049/1645 train_time:96845ms step_avg:92.32ms
step:1050/1645 train_time:96938ms step_avg:92.32ms
step:1051/1645 train_time:97030ms step_avg:92.32ms
step:1052/1645 train_time:97123ms step_avg:92.32ms
step:1053/1645 train_time:97216ms step_avg:92.32ms
step:1054/1645 train_time:97310ms step_avg:92.32ms
step:1055/1645 train_time:97403ms step_avg:92.32ms
step:1056/1645 train_time:97496ms step_avg:92.33ms
step:1057/1645 train_time:97590ms step_avg:92.33ms
step:1058/1645 train_time:97683ms step_avg:92.33ms
step:1059/1645 train_time:97776ms step_avg:92.33ms
step:1060/1645 train_time:97869ms step_avg:92.33ms
step:1061/1645 train_time:97962ms step_avg:92.33ms
step:1062/1645 train_time:98056ms step_avg:92.33ms
step:1063/1645 train_time:98148ms step_avg:92.33ms
step:1064/1645 train_time:98241ms step_avg:92.33ms
step:1065/1645 train_time:98333ms step_avg:92.33ms
step:1066/1645 train_time:98427ms step_avg:92.33ms
step:1067/1645 train_time:98520ms step_avg:92.33ms
step:1068/1645 train_time:98614ms step_avg:92.33ms
step:1069/1645 train_time:98706ms step_avg:92.33ms
step:1070/1645 train_time:98799ms step_avg:92.34ms
step:1071/1645 train_time:98891ms step_avg:92.34ms
step:1072/1645 train_time:98985ms step_avg:92.34ms
step:1073/1645 train_time:99077ms step_avg:92.34ms
step:1074/1645 train_time:99170ms step_avg:92.34ms
step:1075/1645 train_time:99263ms step_avg:92.34ms
step:1076/1645 train_time:99356ms step_avg:92.34ms
step:1077/1645 train_time:99449ms step_avg:92.34ms
step:1078/1645 train_time:99543ms step_avg:92.34ms
step:1079/1645 train_time:99636ms step_avg:92.34ms
step:1080/1645 train_time:99729ms step_avg:92.34ms
step:1081/1645 train_time:99823ms step_avg:92.34ms
step:1082/1645 train_time:99917ms step_avg:92.34ms
step:1083/1645 train_time:100009ms step_avg:92.34ms
step:1084/1645 train_time:100102ms step_avg:92.34ms
step:1085/1645 train_time:100195ms step_avg:92.35ms
step:1086/1645 train_time:100288ms step_avg:92.35ms
step:1087/1645 train_time:100381ms step_avg:92.35ms
step:1088/1645 train_time:100473ms step_avg:92.35ms
step:1089/1645 train_time:100566ms step_avg:92.35ms
step:1090/1645 train_time:100659ms step_avg:92.35ms
step:1091/1645 train_time:100752ms step_avg:92.35ms
step:1092/1645 train_time:100845ms step_avg:92.35ms
step:1093/1645 train_time:100937ms step_avg:92.35ms
step:1094/1645 train_time:101031ms step_avg:92.35ms
step:1095/1645 train_time:101123ms step_avg:92.35ms
step:1096/1645 train_time:101216ms step_avg:92.35ms
step:1097/1645 train_time:101308ms step_avg:92.35ms
step:1098/1645 train_time:101402ms step_avg:92.35ms
step:1099/1645 train_time:101496ms step_avg:92.35ms
step:1100/1645 train_time:101589ms step_avg:92.35ms
step:1101/1645 train_time:101683ms step_avg:92.36ms
step:1102/1645 train_time:101777ms step_avg:92.36ms
step:1103/1645 train_time:101870ms step_avg:92.36ms
step:1104/1645 train_time:101964ms step_avg:92.36ms
step:1105/1645 train_time:102057ms step_avg:92.36ms
step:1106/1645 train_time:102150ms step_avg:92.36ms
step:1107/1645 train_time:102243ms step_avg:92.36ms
step:1108/1645 train_time:102337ms step_avg:92.36ms
step:1109/1645 train_time:102431ms step_avg:92.36ms
step:1110/1645 train_time:102525ms step_avg:92.36ms
step:1111/1645 train_time:102619ms step_avg:92.37ms
step:1112/1645 train_time:102712ms step_avg:92.37ms
step:1113/1645 train_time:102806ms step_avg:92.37ms
step:1114/1645 train_time:102900ms step_avg:92.37ms
step:1115/1645 train_time:102994ms step_avg:92.37ms
step:1116/1645 train_time:103087ms step_avg:92.37ms
step:1117/1645 train_time:103180ms step_avg:92.37ms
step:1118/1645 train_time:103273ms step_avg:92.37ms
step:1119/1645 train_time:103367ms step_avg:92.37ms
step:1120/1645 train_time:103461ms step_avg:92.38ms
step:1121/1645 train_time:103554ms step_avg:92.38ms
step:1122/1645 train_time:103649ms step_avg:92.38ms
step:1123/1645 train_time:103743ms step_avg:92.38ms
step:1124/1645 train_time:103836ms step_avg:92.38ms
step:1125/1645 train_time:103930ms step_avg:92.38ms
step:1125/1645 val_loss:3.4134 train_time:104024ms step_avg:92.47ms
step:1126/1645 train_time:104039ms step_avg:92.40ms
step:1127/1645 train_time:104123ms step_avg:92.39ms
step:1128/1645 train_time:104227ms step_avg:92.40ms
step:1129/1645 train_time:104323ms step_avg:92.40ms
step:1130/1645 train_time:104415ms step_avg:92.40ms
step:1131/1645 train_time:104508ms step_avg:92.40ms
step:1132/1645 train_time:104600ms step_avg:92.40ms
step:1133/1645 train_time:104693ms step_avg:92.40ms
step:1134/1645 train_time:104785ms step_avg:92.40ms
step:1135/1645 train_time:104877ms step_avg:92.40ms
step:1136/1645 train_time:104972ms step_avg:92.41ms
step:1137/1645 train_time:105067ms step_avg:92.41ms
step:1138/1645 train_time:105164ms step_avg:92.41ms
step:1139/1645 train_time:105260ms step_avg:92.41ms
step:1140/1645 train_time:105354ms step_avg:92.42ms
step:1141/1645 train_time:105447ms step_avg:92.42ms
step:1142/1645 train_time:105541ms step_avg:92.42ms
step:1143/1645 train_time:105633ms step_avg:92.42ms
step:1144/1645 train_time:105726ms step_avg:92.42ms
step:1145/1645 train_time:105819ms step_avg:92.42ms
step:1146/1645 train_time:105912ms step_avg:92.42ms
step:1147/1645 train_time:106006ms step_avg:92.42ms
step:1148/1645 train_time:106100ms step_avg:92.42ms
step:1149/1645 train_time:106195ms step_avg:92.42ms
step:1150/1645 train_time:106289ms step_avg:92.43ms
step:1151/1645 train_time:106382ms step_avg:92.43ms
step:1152/1645 train_time:106476ms step_avg:92.43ms
step:1153/1645 train_time:106569ms step_avg:92.43ms
step:1154/1645 train_time:106662ms step_avg:92.43ms
step:1155/1645 train_time:106755ms step_avg:92.43ms
step:1156/1645 train_time:106848ms step_avg:92.43ms
step:1157/1645 train_time:106942ms step_avg:92.43ms
step:1158/1645 train_time:107036ms step_avg:92.43ms
step:1159/1645 train_time:107130ms step_avg:92.43ms
step:1160/1645 train_time:107225ms step_avg:92.43ms
step:1161/1645 train_time:107319ms step_avg:92.44ms
step:1162/1645 train_time:107413ms step_avg:92.44ms
step:1163/1645 train_time:107507ms step_avg:92.44ms
step:1164/1645 train_time:107601ms step_avg:92.44ms
step:1165/1645 train_time:107693ms step_avg:92.44ms
step:1166/1645 train_time:107786ms step_avg:92.44ms
step:1167/1645 train_time:107880ms step_avg:92.44ms
step:1168/1645 train_time:107974ms step_avg:92.44ms
step:1169/1645 train_time:108068ms step_avg:92.44ms
step:1170/1645 train_time:108162ms step_avg:92.45ms
step:1171/1645 train_time:108256ms step_avg:92.45ms
step:1172/1645 train_time:108349ms step_avg:92.45ms
step:1173/1645 train_time:108444ms step_avg:92.45ms
step:1174/1645 train_time:108537ms step_avg:92.45ms
step:1175/1645 train_time:108631ms step_avg:92.45ms
step:1176/1645 train_time:108724ms step_avg:92.45ms
step:1177/1645 train_time:108817ms step_avg:92.45ms
step:1178/1645 train_time:108911ms step_avg:92.45ms
step:1179/1645 train_time:109004ms step_avg:92.45ms
step:1180/1645 train_time:109098ms step_avg:92.46ms
step:1181/1645 train_time:109191ms step_avg:92.46ms
step:1182/1645 train_time:109285ms step_avg:92.46ms
step:1183/1645 train_time:109379ms step_avg:92.46ms
step:1184/1645 train_time:109473ms step_avg:92.46ms
step:1185/1645 train_time:109567ms step_avg:92.46ms
step:1186/1645 train_time:109661ms step_avg:92.46ms
step:1187/1645 train_time:109754ms step_avg:92.46ms
step:1188/1645 train_time:109849ms step_avg:92.47ms
step:1189/1645 train_time:109942ms step_avg:92.47ms
step:1190/1645 train_time:110036ms step_avg:92.47ms
step:1191/1645 train_time:110130ms step_avg:92.47ms
step:1192/1645 train_time:110223ms step_avg:92.47ms
step:1193/1645 train_time:110317ms step_avg:92.47ms
step:1194/1645 train_time:110411ms step_avg:92.47ms
step:1195/1645 train_time:110506ms step_avg:92.47ms
step:1196/1645 train_time:110599ms step_avg:92.47ms
step:1197/1645 train_time:110692ms step_avg:92.47ms
step:1198/1645 train_time:110786ms step_avg:92.48ms
step:1199/1645 train_time:110879ms step_avg:92.48ms
step:1200/1645 train_time:110973ms step_avg:92.48ms
step:1201/1645 train_time:111066ms step_avg:92.48ms
step:1202/1645 train_time:111159ms step_avg:92.48ms
step:1203/1645 train_time:111253ms step_avg:92.48ms
step:1204/1645 train_time:111347ms step_avg:92.48ms
step:1205/1645 train_time:111441ms step_avg:92.48ms
step:1206/1645 train_time:111535ms step_avg:92.48ms
step:1207/1645 train_time:111629ms step_avg:92.48ms
step:1208/1645 train_time:111722ms step_avg:92.49ms
step:1209/1645 train_time:111816ms step_avg:92.49ms
step:1210/1645 train_time:111909ms step_avg:92.49ms
step:1211/1645 train_time:112002ms step_avg:92.49ms
step:1212/1645 train_time:112095ms step_avg:92.49ms
step:1213/1645 train_time:112189ms step_avg:92.49ms
step:1214/1645 train_time:112284ms step_avg:92.49ms
step:1215/1645 train_time:112377ms step_avg:92.49ms
step:1216/1645 train_time:112471ms step_avg:92.49ms
step:1217/1645 train_time:112565ms step_avg:92.49ms
step:1218/1645 train_time:112658ms step_avg:92.49ms
step:1219/1645 train_time:112752ms step_avg:92.50ms
step:1220/1645 train_time:112846ms step_avg:92.50ms
step:1221/1645 train_time:112939ms step_avg:92.50ms
step:1222/1645 train_time:113032ms step_avg:92.50ms
step:1223/1645 train_time:113126ms step_avg:92.50ms
step:1224/1645 train_time:113220ms step_avg:92.50ms
step:1225/1645 train_time:113314ms step_avg:92.50ms
step:1226/1645 train_time:113407ms step_avg:92.50ms
step:1227/1645 train_time:113502ms step_avg:92.50ms
step:1228/1645 train_time:113595ms step_avg:92.50ms
step:1229/1645 train_time:113688ms step_avg:92.50ms
step:1230/1645 train_time:113782ms step_avg:92.51ms
step:1231/1645 train_time:113875ms step_avg:92.51ms
step:1232/1645 train_time:113969ms step_avg:92.51ms
step:1233/1645 train_time:114062ms step_avg:92.51ms
step:1234/1645 train_time:114156ms step_avg:92.51ms
step:1235/1645 train_time:114250ms step_avg:92.51ms
step:1236/1645 train_time:114343ms step_avg:92.51ms
step:1237/1645 train_time:114437ms step_avg:92.51ms
step:1238/1645 train_time:114530ms step_avg:92.51ms
step:1239/1645 train_time:114624ms step_avg:92.51ms
step:1240/1645 train_time:114716ms step_avg:92.51ms
step:1241/1645 train_time:114810ms step_avg:92.51ms
step:1242/1645 train_time:114903ms step_avg:92.51ms
step:1243/1645 train_time:114996ms step_avg:92.52ms
step:1244/1645 train_time:115090ms step_avg:92.52ms
step:1245/1645 train_time:115184ms step_avg:92.52ms
step:1246/1645 train_time:115278ms step_avg:92.52ms
step:1247/1645 train_time:115372ms step_avg:92.52ms
step:1248/1645 train_time:115467ms step_avg:92.52ms
step:1249/1645 train_time:115560ms step_avg:92.52ms
step:1250/1645 train_time:115654ms step_avg:92.52ms
step:1250/1645 val_loss:3.3744 train_time:115749ms step_avg:92.60ms
step:1251/1645 train_time:115769ms step_avg:92.54ms
step:1252/1645 train_time:115848ms step_avg:92.53ms
step:1253/1645 train_time:115942ms step_avg:92.53ms
step:1254/1645 train_time:116036ms step_avg:92.53ms
step:1255/1645 train_time:116127ms step_avg:92.53ms
step:1256/1645 train_time:116221ms step_avg:92.53ms
step:1257/1645 train_time:116313ms step_avg:92.53ms
step:1258/1645 train_time:116405ms step_avg:92.53ms
step:1259/1645 train_time:116498ms step_avg:92.53ms
step:1260/1645 train_time:116590ms step_avg:92.53ms
step:1261/1645 train_time:116685ms step_avg:92.53ms
step:1262/1645 train_time:116783ms step_avg:92.54ms
step:1263/1645 train_time:116879ms step_avg:92.54ms
step:1264/1645 train_time:116972ms step_avg:92.54ms
step:1265/1645 train_time:117066ms step_avg:92.54ms
step:1266/1645 train_time:117158ms step_avg:92.54ms
step:1267/1645 train_time:117251ms step_avg:92.54ms
step:1268/1645 train_time:117345ms step_avg:92.54ms
step:1269/1645 train_time:117438ms step_avg:92.54ms
step:1270/1645 train_time:117531ms step_avg:92.54ms
step:1271/1645 train_time:117624ms step_avg:92.54ms
step:1272/1645 train_time:117718ms step_avg:92.55ms
step:1273/1645 train_time:117812ms step_avg:92.55ms
step:1274/1645 train_time:117907ms step_avg:92.55ms
step:1275/1645 train_time:118002ms step_avg:92.55ms
step:1276/1645 train_time:118096ms step_avg:92.55ms
step:1277/1645 train_time:118189ms step_avg:92.55ms
step:1278/1645 train_time:118282ms step_avg:92.55ms
step:1279/1645 train_time:118375ms step_avg:92.55ms
step:1280/1645 train_time:118470ms step_avg:92.55ms
step:1281/1645 train_time:118562ms step_avg:92.55ms
step:1282/1645 train_time:118656ms step_avg:92.56ms
step:1283/1645 train_time:118749ms step_avg:92.56ms
step:1284/1645 train_time:118844ms step_avg:92.56ms
step:1285/1645 train_time:118938ms step_avg:92.56ms
step:1286/1645 train_time:119032ms step_avg:92.56ms
step:1287/1645 train_time:119126ms step_avg:92.56ms
step:1288/1645 train_time:119219ms step_avg:92.56ms
step:1289/1645 train_time:119313ms step_avg:92.56ms
step:1290/1645 train_time:119406ms step_avg:92.56ms
step:1291/1645 train_time:119499ms step_avg:92.56ms
step:1292/1645 train_time:119592ms step_avg:92.56ms
step:1293/1645 train_time:119686ms step_avg:92.56ms
step:1294/1645 train_time:119780ms step_avg:92.57ms
step:1295/1645 train_time:119874ms step_avg:92.57ms
step:1296/1645 train_time:119969ms step_avg:92.57ms
step:1297/1645 train_time:120063ms step_avg:92.57ms
step:1298/1645 train_time:120158ms step_avg:92.57ms
step:1299/1645 train_time:120252ms step_avg:92.57ms
step:1300/1645 train_time:120345ms step_avg:92.57ms
step:1301/1645 train_time:120438ms step_avg:92.57ms
step:1302/1645 train_time:120530ms step_avg:92.57ms
step:1303/1645 train_time:120624ms step_avg:92.57ms
step:1304/1645 train_time:120717ms step_avg:92.57ms
step:1305/1645 train_time:120810ms step_avg:92.57ms
step:1306/1645 train_time:120905ms step_avg:92.58ms
step:1307/1645 train_time:120998ms step_avg:92.58ms
step:1308/1645 train_time:121090ms step_avg:92.58ms
step:1309/1645 train_time:121184ms step_avg:92.58ms
step:1310/1645 train_time:121278ms step_avg:92.58ms
step:1311/1645 train_time:121371ms step_avg:92.58ms
step:1312/1645 train_time:121465ms step_avg:92.58ms
step:1313/1645 train_time:121558ms step_avg:92.58ms
step:1314/1645 train_time:121652ms step_avg:92.58ms
step:1315/1645 train_time:121745ms step_avg:92.58ms
step:1316/1645 train_time:121839ms step_avg:92.58ms
step:1317/1645 train_time:121931ms step_avg:92.58ms
step:1318/1645 train_time:122025ms step_avg:92.58ms
step:1319/1645 train_time:122119ms step_avg:92.58ms
step:1320/1645 train_time:122213ms step_avg:92.59ms
step:1321/1645 train_time:122307ms step_avg:92.59ms
step:1322/1645 train_time:122401ms step_avg:92.59ms
step:1323/1645 train_time:122496ms step_avg:92.59ms
step:1324/1645 train_time:122589ms step_avg:92.59ms
step:1325/1645 train_time:122682ms step_avg:92.59ms
step:1326/1645 train_time:122775ms step_avg:92.59ms
step:1327/1645 train_time:122869ms step_avg:92.59ms
step:1328/1645 train_time:122962ms step_avg:92.59ms
step:1329/1645 train_time:123055ms step_avg:92.59ms
step:1330/1645 train_time:123150ms step_avg:92.59ms
step:1331/1645 train_time:123244ms step_avg:92.59ms
step:1332/1645 train_time:123337ms step_avg:92.60ms
step:1333/1645 train_time:123430ms step_avg:92.60ms
step:1334/1645 train_time:123523ms step_avg:92.60ms
step:1335/1645 train_time:123617ms step_avg:92.60ms
step:1336/1645 train_time:123710ms step_avg:92.60ms
step:1337/1645 train_time:123805ms step_avg:92.60ms
step:1338/1645 train_time:123898ms step_avg:92.60ms
step:1339/1645 train_time:123991ms step_avg:92.60ms
step:1340/1645 train_time:124087ms step_avg:92.60ms
step:1341/1645 train_time:124180ms step_avg:92.60ms
step:1342/1645 train_time:124274ms step_avg:92.60ms
step:1343/1645 train_time:124368ms step_avg:92.60ms
step:1344/1645 train_time:124462ms step_avg:92.61ms
step:1345/1645 train_time:124556ms step_avg:92.61ms
step:1346/1645 train_time:124649ms step_avg:92.61ms
step:1347/1645 train_time:124742ms step_avg:92.61ms
step:1348/1645 train_time:124836ms step_avg:92.61ms
step:1349/1645 train_time:124929ms step_avg:92.61ms
step:1350/1645 train_time:125023ms step_avg:92.61ms
step:1351/1645 train_time:125117ms step_avg:92.61ms
step:1352/1645 train_time:125210ms step_avg:92.61ms
step:1353/1645 train_time:125304ms step_avg:92.61ms
step:1354/1645 train_time:125397ms step_avg:92.61ms
step:1355/1645 train_time:125492ms step_avg:92.61ms
step:1356/1645 train_time:125586ms step_avg:92.61ms
step:1357/1645 train_time:125679ms step_avg:92.62ms
step:1358/1645 train_time:125772ms step_avg:92.62ms
step:1359/1645 train_time:125865ms step_avg:92.62ms
step:1360/1645 train_time:125958ms step_avg:92.62ms
step:1361/1645 train_time:126052ms step_avg:92.62ms
step:1362/1645 train_time:126146ms step_avg:92.62ms
step:1363/1645 train_time:126240ms step_avg:92.62ms
step:1364/1645 train_time:126333ms step_avg:92.62ms
step:1365/1645 train_time:126426ms step_avg:92.62ms
step:1366/1645 train_time:126519ms step_avg:92.62ms
step:1367/1645 train_time:126614ms step_avg:92.62ms
step:1368/1645 train_time:126707ms step_avg:92.62ms
step:1369/1645 train_time:126801ms step_avg:92.62ms
step:1370/1645 train_time:126896ms step_avg:92.62ms
step:1371/1645 train_time:126988ms step_avg:92.62ms
step:1372/1645 train_time:127082ms step_avg:92.63ms
step:1373/1645 train_time:127176ms step_avg:92.63ms
step:1374/1645 train_time:127269ms step_avg:92.63ms
step:1375/1645 train_time:127364ms step_avg:92.63ms
step:1375/1645 val_loss:3.3396 train_time:127457ms step_avg:92.70ms
step:1376/1645 train_time:127478ms step_avg:92.64ms
step:1377/1645 train_time:127556ms step_avg:92.63ms
step:1378/1645 train_time:127652ms step_avg:92.64ms
step:1379/1645 train_time:127746ms step_avg:92.64ms
step:1380/1645 train_time:127839ms step_avg:92.64ms
step:1381/1645 train_time:127933ms step_avg:92.64ms
step:1382/1645 train_time:128024ms step_avg:92.64ms
step:1383/1645 train_time:128116ms step_avg:92.64ms
step:1384/1645 train_time:128210ms step_avg:92.64ms
step:1385/1645 train_time:128303ms step_avg:92.64ms
step:1386/1645 train_time:128398ms step_avg:92.64ms
step:1387/1645 train_time:128494ms step_avg:92.64ms
step:1388/1645 train_time:128589ms step_avg:92.64ms
step:1389/1645 train_time:128683ms step_avg:92.64ms
step:1390/1645 train_time:128777ms step_avg:92.65ms
step:1391/1645 train_time:128871ms step_avg:92.65ms
step:1392/1645 train_time:128964ms step_avg:92.65ms
step:1393/1645 train_time:129057ms step_avg:92.65ms
step:1394/1645 train_time:129150ms step_avg:92.65ms
step:1395/1645 train_time:129242ms step_avg:92.65ms
step:1396/1645 train_time:129337ms step_avg:92.65ms
step:1397/1645 train_time:129431ms step_avg:92.65ms
step:1398/1645 train_time:129524ms step_avg:92.65ms
step:1399/1645 train_time:129620ms step_avg:92.65ms
step:1400/1645 train_time:129714ms step_avg:92.65ms
step:1401/1645 train_time:129807ms step_avg:92.65ms
step:1402/1645 train_time:129901ms step_avg:92.65ms
step:1403/1645 train_time:129994ms step_avg:92.65ms
step:1404/1645 train_time:130087ms step_avg:92.65ms
step:1405/1645 train_time:130181ms step_avg:92.66ms
step:1406/1645 train_time:130274ms step_avg:92.66ms
step:1407/1645 train_time:130366ms step_avg:92.66ms
step:1408/1645 train_time:130461ms step_avg:92.66ms
step:1409/1645 train_time:130556ms step_avg:92.66ms
step:1410/1645 train_time:130650ms step_avg:92.66ms
step:1411/1645 train_time:130744ms step_avg:92.66ms
step:1412/1645 train_time:130838ms step_avg:92.66ms
step:1413/1645 train_time:130931ms step_avg:92.66ms
step:1414/1645 train_time:131023ms step_avg:92.66ms
step:1415/1645 train_time:131116ms step_avg:92.66ms
step:1416/1645 train_time:131209ms step_avg:92.66ms
step:1417/1645 train_time:131302ms step_avg:92.66ms
step:1418/1645 train_time:131395ms step_avg:92.66ms
step:1419/1645 train_time:131489ms step_avg:92.66ms
step:1420/1645 train_time:131583ms step_avg:92.66ms
step:1421/1645 train_time:131678ms step_avg:92.67ms
step:1422/1645 train_time:131772ms step_avg:92.67ms
step:1423/1645 train_time:131865ms step_avg:92.67ms
step:1424/1645 train_time:131959ms step_avg:92.67ms
step:1425/1645 train_time:132053ms step_avg:92.67ms
step:1426/1645 train_time:132146ms step_avg:92.67ms
step:1427/1645 train_time:132239ms step_avg:92.67ms
step:1428/1645 train_time:132332ms step_avg:92.67ms
step:1429/1645 train_time:132425ms step_avg:92.67ms
step:1430/1645 train_time:132520ms step_avg:92.67ms
step:1431/1645 train_time:132614ms step_avg:92.67ms
step:1432/1645 train_time:132708ms step_avg:92.67ms
step:1433/1645 train_time:132802ms step_avg:92.67ms
step:1434/1645 train_time:132896ms step_avg:92.67ms
step:1435/1645 train_time:132990ms step_avg:92.68ms
step:1436/1645 train_time:133083ms step_avg:92.68ms
step:1437/1645 train_time:133176ms step_avg:92.68ms
step:1438/1645 train_time:133269ms step_avg:92.68ms
step:1439/1645 train_time:133363ms step_avg:92.68ms
step:1440/1645 train_time:133458ms step_avg:92.68ms
step:1441/1645 train_time:133550ms step_avg:92.68ms
step:1442/1645 train_time:133644ms step_avg:92.68ms
step:1443/1645 train_time:133738ms step_avg:92.68ms
step:1444/1645 train_time:133831ms step_avg:92.68ms
step:1445/1645 train_time:133925ms step_avg:92.68ms
step:1446/1645 train_time:134019ms step_avg:92.68ms
step:1447/1645 train_time:134113ms step_avg:92.68ms
step:1448/1645 train_time:134206ms step_avg:92.68ms
step:1449/1645 train_time:134300ms step_avg:92.68ms
step:1450/1645 train_time:134393ms step_avg:92.68ms
step:1451/1645 train_time:134487ms step_avg:92.69ms
step:1452/1645 train_time:134581ms step_avg:92.69ms
step:1453/1645 train_time:134675ms step_avg:92.69ms
step:1454/1645 train_time:134769ms step_avg:92.69ms
step:1455/1645 train_time:134864ms step_avg:92.69ms
step:1456/1645 train_time:134958ms step_avg:92.69ms
step:1457/1645 train_time:135052ms step_avg:92.69ms
step:1458/1645 train_time:135145ms step_avg:92.69ms
step:1459/1645 train_time:135239ms step_avg:92.69ms
step:1460/1645 train_time:135332ms step_avg:92.69ms
step:1461/1645 train_time:135426ms step_avg:92.69ms
step:1462/1645 train_time:135521ms step_avg:92.70ms
step:1463/1645 train_time:135615ms step_avg:92.70ms
step:1464/1645 train_time:135709ms step_avg:92.70ms
step:1465/1645 train_time:135803ms step_avg:92.70ms
step:1466/1645 train_time:135897ms step_avg:92.70ms
step:1467/1645 train_time:135991ms step_avg:92.70ms
step:1468/1645 train_time:136085ms step_avg:92.70ms
step:1469/1645 train_time:136178ms step_avg:92.70ms
step:1470/1645 train_time:136272ms step_avg:92.70ms
step:1471/1645 train_time:136365ms step_avg:92.70ms
step:1472/1645 train_time:136458ms step_avg:92.70ms
step:1473/1645 train_time:136552ms step_avg:92.70ms
step:1474/1645 train_time:136645ms step_avg:92.70ms
step:1475/1645 train_time:136739ms step_avg:92.70ms
step:1476/1645 train_time:136834ms step_avg:92.71ms
step:1477/1645 train_time:136927ms step_avg:92.71ms
step:1478/1645 train_time:137020ms step_avg:92.71ms
step:1479/1645 train_time:137115ms step_avg:92.71ms
step:1480/1645 train_time:137209ms step_avg:92.71ms
step:1481/1645 train_time:137303ms step_avg:92.71ms
step:1482/1645 train_time:137396ms step_avg:92.71ms
step:1483/1645 train_time:137490ms step_avg:92.71ms
step:1484/1645 train_time:137584ms step_avg:92.71ms
step:1485/1645 train_time:137678ms step_avg:92.71ms
step:1486/1645 train_time:137771ms step_avg:92.71ms
step:1487/1645 train_time:137865ms step_avg:92.71ms
step:1488/1645 train_time:137959ms step_avg:92.71ms
step:1489/1645 train_time:138052ms step_avg:92.71ms
step:1490/1645 train_time:138145ms step_avg:92.71ms
step:1491/1645 train_time:138239ms step_avg:92.72ms
step:1492/1645 train_time:138332ms step_avg:92.72ms
step:1493/1645 train_time:138425ms step_avg:92.72ms
step:1494/1645 train_time:138519ms step_avg:92.72ms
step:1495/1645 train_time:138613ms step_avg:92.72ms
step:1496/1645 train_time:138707ms step_avg:92.72ms
step:1497/1645 train_time:138801ms step_avg:92.72ms
step:1498/1645 train_time:138894ms step_avg:92.72ms
step:1499/1645 train_time:138987ms step_avg:92.72ms
step:1500/1645 train_time:139081ms step_avg:92.72ms
step:1500/1645 val_loss:3.3100 train_time:139176ms step_avg:92.78ms
step:1501/1645 train_time:139196ms step_avg:92.74ms
step:1502/1645 train_time:139274ms step_avg:92.73ms
step:1503/1645 train_time:139370ms step_avg:92.73ms
step:1504/1645 train_time:139463ms step_avg:92.73ms
step:1505/1645 train_time:139556ms step_avg:92.73ms
step:1506/1645 train_time:139649ms step_avg:92.73ms
step:1507/1645 train_time:139741ms step_avg:92.73ms
step:1508/1645 train_time:139835ms step_avg:92.73ms
step:1509/1645 train_time:139927ms step_avg:92.73ms
step:1510/1645 train_time:140020ms step_avg:92.73ms
step:1511/1645 train_time:140114ms step_avg:92.73ms
step:1512/1645 train_time:140210ms step_avg:92.73ms
step:1513/1645 train_time:140306ms step_avg:92.73ms
step:1514/1645 train_time:140401ms step_avg:92.73ms
step:1515/1645 train_time:140493ms step_avg:92.73ms
step:1516/1645 train_time:140587ms step_avg:92.74ms
step:1517/1645 train_time:140680ms step_avg:92.74ms
step:1518/1645 train_time:140773ms step_avg:92.74ms
step:1519/1645 train_time:140866ms step_avg:92.74ms
step:1520/1645 train_time:140959ms step_avg:92.74ms
step:1521/1645 train_time:141053ms step_avg:92.74ms
step:1522/1645 train_time:141148ms step_avg:92.74ms
step:1523/1645 train_time:141243ms step_avg:92.74ms
step:1524/1645 train_time:141338ms step_avg:92.74ms
step:1525/1645 train_time:141433ms step_avg:92.74ms
step:1526/1645 train_time:141526ms step_avg:92.74ms
step:1527/1645 train_time:141619ms step_avg:92.74ms
step:1528/1645 train_time:141712ms step_avg:92.74ms
step:1529/1645 train_time:141805ms step_avg:92.74ms
step:1530/1645 train_time:141897ms step_avg:92.74ms
step:1531/1645 train_time:141990ms step_avg:92.74ms
step:1532/1645 train_time:142083ms step_avg:92.74ms
step:1533/1645 train_time:142177ms step_avg:92.74ms
step:1534/1645 train_time:142271ms step_avg:92.75ms
step:1535/1645 train_time:142366ms step_avg:92.75ms
step:1536/1645 train_time:142460ms step_avg:92.75ms
step:1537/1645 train_time:142554ms step_avg:92.75ms
step:1538/1645 train_time:142648ms step_avg:92.75ms
step:1539/1645 train_time:142741ms step_avg:92.75ms
step:1540/1645 train_time:142835ms step_avg:92.75ms
step:1541/1645 train_time:142928ms step_avg:92.75ms
step:1542/1645 train_time:143022ms step_avg:92.75ms
step:1543/1645 train_time:143116ms step_avg:92.75ms
step:1544/1645 train_time:143210ms step_avg:92.75ms
step:1545/1645 train_time:143305ms step_avg:92.75ms
step:1546/1645 train_time:143398ms step_avg:92.75ms
step:1547/1645 train_time:143492ms step_avg:92.75ms
step:1548/1645 train_time:143586ms step_avg:92.76ms
step:1549/1645 train_time:143679ms step_avg:92.76ms
step:1550/1645 train_time:143773ms step_avg:92.76ms
step:1551/1645 train_time:143866ms step_avg:92.76ms
step:1552/1645 train_time:143958ms step_avg:92.76ms
step:1553/1645 train_time:144053ms step_avg:92.76ms
step:1554/1645 train_time:144148ms step_avg:92.76ms
step:1555/1645 train_time:144242ms step_avg:92.76ms
step:1556/1645 train_time:144337ms step_avg:92.76ms
step:1557/1645 train_time:144430ms step_avg:92.76ms
step:1558/1645 train_time:144524ms step_avg:92.76ms
step:1559/1645 train_time:144618ms step_avg:92.76ms
step:1560/1645 train_time:144711ms step_avg:92.76ms
step:1561/1645 train_time:144805ms step_avg:92.76ms
step:1562/1645 train_time:144898ms step_avg:92.76ms
step:1563/1645 train_time:144991ms step_avg:92.76ms
step:1564/1645 train_time:145085ms step_avg:92.77ms
step:1565/1645 train_time:145178ms step_avg:92.77ms
step:1566/1645 train_time:145272ms step_avg:92.77ms
step:1567/1645 train_time:145366ms step_avg:92.77ms
step:1568/1645 train_time:145459ms step_avg:92.77ms
step:1569/1645 train_time:145553ms step_avg:92.77ms
step:1570/1645 train_time:145648ms step_avg:92.77ms
step:1571/1645 train_time:145741ms step_avg:92.77ms
step:1572/1645 train_time:145834ms step_avg:92.77ms
step:1573/1645 train_time:145927ms step_avg:92.77ms
step:1574/1645 train_time:146022ms step_avg:92.77ms
step:1575/1645 train_time:146116ms step_avg:92.77ms
step:1576/1645 train_time:146209ms step_avg:92.77ms
step:1577/1645 train_time:146303ms step_avg:92.77ms
step:1578/1645 train_time:146397ms step_avg:92.77ms
step:1579/1645 train_time:146491ms step_avg:92.77ms
step:1580/1645 train_time:146585ms step_avg:92.78ms
step:1581/1645 train_time:146680ms step_avg:92.78ms
step:1582/1645 train_time:146773ms step_avg:92.78ms
step:1583/1645 train_time:146867ms step_avg:92.78ms
step:1584/1645 train_time:146961ms step_avg:92.78ms
step:1585/1645 train_time:147054ms step_avg:92.78ms
step:1586/1645 train_time:147149ms step_avg:92.78ms
step:1587/1645 train_time:147243ms step_avg:92.78ms
step:1588/1645 train_time:147337ms step_avg:92.78ms
step:1589/1645 train_time:147431ms step_avg:92.78ms
step:1590/1645 train_time:147524ms step_avg:92.78ms
step:1591/1645 train_time:147617ms step_avg:92.78ms
step:1592/1645 train_time:147711ms step_avg:92.78ms
step:1593/1645 train_time:147804ms step_avg:92.78ms
step:1594/1645 train_time:147897ms step_avg:92.78ms
step:1595/1645 train_time:147990ms step_avg:92.78ms
step:1596/1645 train_time:148084ms step_avg:92.78ms
step:1597/1645 train_time:148177ms step_avg:92.78ms
step:1598/1645 train_time:148271ms step_avg:92.79ms
step:1599/1645 train_time:148364ms step_avg:92.79ms
step:1600/1645 train_time:148457ms step_avg:92.79ms
step:1601/1645 train_time:148553ms step_avg:92.79ms
step:1602/1645 train_time:148646ms step_avg:92.79ms
step:1603/1645 train_time:148740ms step_avg:92.79ms
step:1604/1645 train_time:148833ms step_avg:92.79ms
step:1605/1645 train_time:148927ms step_avg:92.79ms
step:1606/1645 train_time:149020ms step_avg:92.79ms
step:1607/1645 train_time:149114ms step_avg:92.79ms
step:1608/1645 train_time:149208ms step_avg:92.79ms
step:1609/1645 train_time:149301ms step_avg:92.79ms
step:1610/1645 train_time:149395ms step_avg:92.79ms
step:1611/1645 train_time:149489ms step_avg:92.79ms
step:1612/1645 train_time:149583ms step_avg:92.79ms
step:1613/1645 train_time:149677ms step_avg:92.79ms
step:1614/1645 train_time:149771ms step_avg:92.79ms
step:1615/1645 train_time:149865ms step_avg:92.80ms
step:1616/1645 train_time:149959ms step_avg:92.80ms
step:1617/1645 train_time:150051ms step_avg:92.80ms
step:1618/1645 train_time:150145ms step_avg:92.80ms
step:1619/1645 train_time:150238ms step_avg:92.80ms
step:1620/1645 train_time:150332ms step_avg:92.80ms
step:1621/1645 train_time:150425ms step_avg:92.80ms
step:1622/1645 train_time:150519ms step_avg:92.80ms
step:1623/1645 train_time:150613ms step_avg:92.80ms
step:1624/1645 train_time:150707ms step_avg:92.80ms
step:1625/1645 train_time:150800ms step_avg:92.80ms
step:1625/1645 val_loss:3.2859 train_time:150894ms step_avg:92.86ms
step:1626/1645 train_time:150914ms step_avg:92.81ms
step:1627/1645 train_time:150991ms step_avg:92.80ms
step:1628/1645 train_time:151088ms step_avg:92.81ms
step:1629/1645 train_time:151182ms step_avg:92.81ms
step:1630/1645 train_time:151275ms step_avg:92.81ms
step:1631/1645 train_time:151367ms step_avg:92.81ms
step:1632/1645 train_time:151461ms step_avg:92.81ms
step:1633/1645 train_time:151554ms step_avg:92.81ms
step:1634/1645 train_time:151648ms step_avg:92.81ms
step:1635/1645 train_time:151740ms step_avg:92.81ms
step:1636/1645 train_time:151834ms step_avg:92.81ms
step:1637/1645 train_time:151929ms step_avg:92.81ms
step:1638/1645 train_time:152024ms step_avg:92.81ms
step:1639/1645 train_time:152119ms step_avg:92.81ms
step:1640/1645 train_time:152213ms step_avg:92.81ms
step:1641/1645 train_time:152306ms step_avg:92.81ms
step:1642/1645 train_time:152399ms step_avg:92.81ms
step:1643/1645 train_time:152492ms step_avg:92.81ms
step:1644/1645 train_time:152586ms step_avg:92.81ms
step:1645/1645 train_time:152678ms step_avg:92.81ms
step:1645/1645 val_loss:3.2801 train_time:152773ms step_avg:92.87ms
peak memory allocated: 32074 MiB reserved: 46756 MiB

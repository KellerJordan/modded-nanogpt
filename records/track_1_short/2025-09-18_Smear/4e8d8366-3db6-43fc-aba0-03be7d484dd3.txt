import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:48:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:137ms step_avg:137.07ms
step:2/1645 train_time:160ms step_avg:79.87ms
step:3/1645 train_time:226ms step_avg:75.21ms
step:4/1645 train_time:315ms step_avg:78.74ms
step:5/1645 train_time:406ms step_avg:81.11ms
step:6/1645 train_time:496ms step_avg:82.67ms
step:7/1645 train_time:587ms step_avg:83.83ms
step:8/1645 train_time:678ms step_avg:84.79ms
step:9/1645 train_time:769ms step_avg:85.45ms
step:10/1645 train_time:859ms step_avg:85.95ms
step:11/1645 train_time:950ms step_avg:86.38ms
step:12/1645 train_time:1046ms step_avg:87.17ms
step:13/1645 train_time:1143ms step_avg:87.93ms
step:14/1645 train_time:1237ms step_avg:88.38ms
step:15/1645 train_time:1329ms step_avg:88.57ms
step:16/1645 train_time:1420ms step_avg:88.75ms
step:17/1645 train_time:1511ms step_avg:88.90ms
step:18/1645 train_time:1602ms step_avg:89.02ms
step:19/1645 train_time:1693ms step_avg:89.12ms
step:20/1645 train_time:1784ms step_avg:89.22ms
step:21/1645 train_time:1877ms step_avg:89.36ms
step:22/1645 train_time:1968ms step_avg:89.45ms
step:23/1645 train_time:2061ms step_avg:89.60ms
step:24/1645 train_time:2155ms step_avg:89.79ms
step:25/1645 train_time:2248ms step_avg:89.93ms
step:26/1645 train_time:2341ms step_avg:90.04ms
step:27/1645 train_time:2433ms step_avg:90.10ms
step:28/1645 train_time:2524ms step_avg:90.15ms
step:29/1645 train_time:2615ms step_avg:90.17ms
step:30/1645 train_time:2706ms step_avg:90.21ms
step:31/1645 train_time:2797ms step_avg:90.23ms
step:32/1645 train_time:2889ms step_avg:90.28ms
step:33/1645 train_time:2982ms step_avg:90.38ms
step:34/1645 train_time:3075ms step_avg:90.45ms
step:35/1645 train_time:3168ms step_avg:90.51ms
step:36/1645 train_time:3262ms step_avg:90.61ms
step:37/1645 train_time:3354ms step_avg:90.64ms
step:38/1645 train_time:3446ms step_avg:90.68ms
step:39/1645 train_time:3537ms step_avg:90.70ms
step:40/1645 train_time:3629ms step_avg:90.72ms
step:41/1645 train_time:3719ms step_avg:90.72ms
step:42/1645 train_time:3811ms step_avg:90.73ms
step:43/1645 train_time:3903ms step_avg:90.76ms
step:44/1645 train_time:3994ms step_avg:90.77ms
step:45/1645 train_time:4087ms step_avg:90.83ms
step:46/1645 train_time:4179ms step_avg:90.85ms
step:47/1645 train_time:4271ms step_avg:90.88ms
step:48/1645 train_time:4363ms step_avg:90.90ms
step:49/1645 train_time:4457ms step_avg:90.95ms
step:50/1645 train_time:4548ms step_avg:90.97ms
step:51/1645 train_time:4640ms step_avg:90.98ms
step:52/1645 train_time:4732ms step_avg:90.99ms
step:53/1645 train_time:4823ms step_avg:90.99ms
step:54/1645 train_time:4914ms step_avg:91.01ms
step:55/1645 train_time:5006ms step_avg:91.01ms
step:56/1645 train_time:5098ms step_avg:91.04ms
step:57/1645 train_time:5190ms step_avg:91.05ms
step:58/1645 train_time:5282ms step_avg:91.07ms
step:59/1645 train_time:5375ms step_avg:91.09ms
step:60/1645 train_time:5467ms step_avg:91.11ms
step:61/1645 train_time:5559ms step_avg:91.14ms
step:62/1645 train_time:5651ms step_avg:91.15ms
step:63/1645 train_time:5742ms step_avg:91.15ms
step:64/1645 train_time:5834ms step_avg:91.15ms
step:65/1645 train_time:5925ms step_avg:91.16ms
step:66/1645 train_time:6017ms step_avg:91.16ms
step:67/1645 train_time:6109ms step_avg:91.17ms
step:68/1645 train_time:6202ms step_avg:91.21ms
step:69/1645 train_time:6293ms step_avg:91.21ms
step:70/1645 train_time:6386ms step_avg:91.23ms
step:71/1645 train_time:6479ms step_avg:91.26ms
step:72/1645 train_time:6571ms step_avg:91.26ms
step:73/1645 train_time:6663ms step_avg:91.27ms
step:74/1645 train_time:6755ms step_avg:91.28ms
step:75/1645 train_time:6847ms step_avg:91.29ms
step:76/1645 train_time:6939ms step_avg:91.30ms
step:77/1645 train_time:7029ms step_avg:91.29ms
step:78/1645 train_time:7121ms step_avg:91.29ms
step:79/1645 train_time:7213ms step_avg:91.30ms
step:80/1645 train_time:7305ms step_avg:91.31ms
step:81/1645 train_time:7397ms step_avg:91.32ms
step:82/1645 train_time:7489ms step_avg:91.33ms
step:83/1645 train_time:7582ms step_avg:91.35ms
step:84/1645 train_time:7674ms step_avg:91.36ms
step:85/1645 train_time:7766ms step_avg:91.36ms
step:86/1645 train_time:7857ms step_avg:91.37ms
step:87/1645 train_time:7950ms step_avg:91.38ms
step:88/1645 train_time:8041ms step_avg:91.38ms
step:89/1645 train_time:8133ms step_avg:91.39ms
step:90/1645 train_time:8224ms step_avg:91.38ms
step:91/1645 train_time:8316ms step_avg:91.38ms
step:92/1645 train_time:8409ms step_avg:91.40ms
step:93/1645 train_time:8500ms step_avg:91.40ms
step:94/1645 train_time:8592ms step_avg:91.40ms
step:95/1645 train_time:8685ms step_avg:91.42ms
step:96/1645 train_time:8778ms step_avg:91.43ms
step:97/1645 train_time:8870ms step_avg:91.44ms
step:98/1645 train_time:8963ms step_avg:91.46ms
step:99/1645 train_time:9054ms step_avg:91.46ms
step:100/1645 train_time:9146ms step_avg:91.46ms
step:101/1645 train_time:9237ms step_avg:91.46ms
step:102/1645 train_time:9328ms step_avg:91.45ms
step:103/1645 train_time:9420ms step_avg:91.45ms
step:104/1645 train_time:9511ms step_avg:91.46ms
step:105/1645 train_time:9604ms step_avg:91.47ms
step:106/1645 train_time:9696ms step_avg:91.47ms
step:107/1645 train_time:9788ms step_avg:91.47ms
step:108/1645 train_time:9880ms step_avg:91.48ms
step:109/1645 train_time:9972ms step_avg:91.48ms
step:110/1645 train_time:10064ms step_avg:91.49ms
step:111/1645 train_time:10155ms step_avg:91.48ms
step:112/1645 train_time:10246ms step_avg:91.48ms
step:113/1645 train_time:10338ms step_avg:91.48ms
step:114/1645 train_time:10429ms step_avg:91.48ms
step:115/1645 train_time:10520ms step_avg:91.48ms
step:116/1645 train_time:10612ms step_avg:91.48ms
step:117/1645 train_time:10704ms step_avg:91.48ms
step:118/1645 train_time:10795ms step_avg:91.49ms
step:119/1645 train_time:10887ms step_avg:91.49ms
step:120/1645 train_time:10981ms step_avg:91.51ms
step:121/1645 train_time:11073ms step_avg:91.51ms
step:122/1645 train_time:11165ms step_avg:91.52ms
step:123/1645 train_time:11257ms step_avg:91.52ms
step:124/1645 train_time:11349ms step_avg:91.52ms
step:125/1645 train_time:11440ms step_avg:91.52ms
step:125/1645 val_loss:4.3122 train_time:11532ms step_avg:92.26ms
step:126/1645 train_time:11554ms step_avg:91.70ms
step:127/1645 train_time:11630ms step_avg:91.58ms
step:128/1645 train_time:11731ms step_avg:91.65ms
step:129/1645 train_time:11824ms step_avg:91.66ms
step:130/1645 train_time:11915ms step_avg:91.66ms
step:131/1645 train_time:12006ms step_avg:91.65ms
step:132/1645 train_time:12097ms step_avg:91.64ms
step:133/1645 train_time:12187ms step_avg:91.63ms
step:134/1645 train_time:12278ms step_avg:91.62ms
step:135/1645 train_time:12368ms step_avg:91.61ms
step:136/1645 train_time:12461ms step_avg:91.62ms
step:137/1645 train_time:12555ms step_avg:91.64ms
step:138/1645 train_time:12648ms step_avg:91.65ms
step:139/1645 train_time:12743ms step_avg:91.67ms
step:140/1645 train_time:12834ms step_avg:91.67ms
step:141/1645 train_time:12925ms step_avg:91.67ms
step:142/1645 train_time:13017ms step_avg:91.67ms
step:143/1645 train_time:13108ms step_avg:91.67ms
step:144/1645 train_time:13199ms step_avg:91.66ms
step:145/1645 train_time:13290ms step_avg:91.66ms
step:146/1645 train_time:13381ms step_avg:91.65ms
step:147/1645 train_time:13474ms step_avg:91.66ms
step:148/1645 train_time:13566ms step_avg:91.66ms
step:149/1645 train_time:13659ms step_avg:91.67ms
step:150/1645 train_time:13752ms step_avg:91.68ms
step:151/1645 train_time:13844ms step_avg:91.68ms
step:152/1645 train_time:13936ms step_avg:91.68ms
step:153/1645 train_time:14027ms step_avg:91.68ms
step:154/1645 train_time:14119ms step_avg:91.68ms
step:155/1645 train_time:14210ms step_avg:91.68ms
step:156/1645 train_time:14301ms step_avg:91.67ms
step:157/1645 train_time:14392ms step_avg:91.67ms
step:158/1645 train_time:14484ms step_avg:91.67ms
step:159/1645 train_time:14577ms step_avg:91.68ms
step:160/1645 train_time:14670ms step_avg:91.69ms
step:161/1645 train_time:14763ms step_avg:91.69ms
step:162/1645 train_time:14856ms step_avg:91.70ms
step:163/1645 train_time:14947ms step_avg:91.70ms
step:164/1645 train_time:15038ms step_avg:91.70ms
step:165/1645 train_time:15129ms step_avg:91.69ms
step:166/1645 train_time:15220ms step_avg:91.69ms
step:167/1645 train_time:15312ms step_avg:91.69ms
step:168/1645 train_time:15402ms step_avg:91.68ms
step:169/1645 train_time:15494ms step_avg:91.68ms
step:170/1645 train_time:15586ms step_avg:91.68ms
step:171/1645 train_time:15678ms step_avg:91.69ms
step:172/1645 train_time:15770ms step_avg:91.69ms
step:173/1645 train_time:15862ms step_avg:91.69ms
step:174/1645 train_time:15954ms step_avg:91.69ms
step:175/1645 train_time:16045ms step_avg:91.68ms
step:176/1645 train_time:16136ms step_avg:91.68ms
step:177/1645 train_time:16226ms step_avg:91.67ms
step:178/1645 train_time:16318ms step_avg:91.67ms
step:179/1645 train_time:16408ms step_avg:91.67ms
step:180/1645 train_time:16500ms step_avg:91.67ms
step:181/1645 train_time:16593ms step_avg:91.67ms
step:182/1645 train_time:16684ms step_avg:91.67ms
step:183/1645 train_time:16777ms step_avg:91.68ms
step:184/1645 train_time:16869ms step_avg:91.68ms
step:185/1645 train_time:16962ms step_avg:91.68ms
step:186/1645 train_time:17053ms step_avg:91.68ms
step:187/1645 train_time:17144ms step_avg:91.68ms
step:188/1645 train_time:17235ms step_avg:91.68ms
step:189/1645 train_time:17327ms step_avg:91.68ms
step:190/1645 train_time:17418ms step_avg:91.67ms
step:191/1645 train_time:17509ms step_avg:91.67ms
step:192/1645 train_time:17602ms step_avg:91.68ms
step:193/1645 train_time:17695ms step_avg:91.68ms
step:194/1645 train_time:17786ms step_avg:91.68ms
step:195/1645 train_time:17879ms step_avg:91.69ms
step:196/1645 train_time:17971ms step_avg:91.69ms
step:197/1645 train_time:18062ms step_avg:91.69ms
step:198/1645 train_time:18154ms step_avg:91.69ms
step:199/1645 train_time:18245ms step_avg:91.68ms
step:200/1645 train_time:18336ms step_avg:91.68ms
step:201/1645 train_time:18428ms step_avg:91.68ms
step:202/1645 train_time:18519ms step_avg:91.68ms
step:203/1645 train_time:18611ms step_avg:91.68ms
step:204/1645 train_time:18704ms step_avg:91.69ms
step:205/1645 train_time:18796ms step_avg:91.69ms
step:206/1645 train_time:18888ms step_avg:91.69ms
step:207/1645 train_time:18981ms step_avg:91.70ms
step:208/1645 train_time:19073ms step_avg:91.70ms
step:209/1645 train_time:19164ms step_avg:91.69ms
step:210/1645 train_time:19255ms step_avg:91.69ms
step:211/1645 train_time:19346ms step_avg:91.69ms
step:212/1645 train_time:19437ms step_avg:91.69ms
step:213/1645 train_time:19529ms step_avg:91.68ms
step:214/1645 train_time:19621ms step_avg:91.68ms
step:215/1645 train_time:19712ms step_avg:91.68ms
step:216/1645 train_time:19805ms step_avg:91.69ms
step:217/1645 train_time:19897ms step_avg:91.69ms
step:218/1645 train_time:19988ms step_avg:91.69ms
step:219/1645 train_time:20080ms step_avg:91.69ms
step:220/1645 train_time:20173ms step_avg:91.69ms
step:221/1645 train_time:20264ms step_avg:91.69ms
step:222/1645 train_time:20356ms step_avg:91.69ms
step:223/1645 train_time:20447ms step_avg:91.69ms
step:224/1645 train_time:20538ms step_avg:91.69ms
step:225/1645 train_time:20630ms step_avg:91.69ms
step:226/1645 train_time:20721ms step_avg:91.69ms
step:227/1645 train_time:20813ms step_avg:91.69ms
step:228/1645 train_time:20904ms step_avg:91.68ms
step:229/1645 train_time:20996ms step_avg:91.69ms
step:230/1645 train_time:21089ms step_avg:91.69ms
step:231/1645 train_time:21182ms step_avg:91.70ms
step:232/1645 train_time:21273ms step_avg:91.69ms
step:233/1645 train_time:21363ms step_avg:91.69ms
step:234/1645 train_time:21456ms step_avg:91.69ms
step:235/1645 train_time:21546ms step_avg:91.69ms
step:236/1645 train_time:21638ms step_avg:91.69ms
step:237/1645 train_time:21730ms step_avg:91.69ms
step:238/1645 train_time:21822ms step_avg:91.69ms
step:239/1645 train_time:21913ms step_avg:91.69ms
step:240/1645 train_time:22004ms step_avg:91.69ms
step:241/1645 train_time:22096ms step_avg:91.69ms
step:242/1645 train_time:22188ms step_avg:91.69ms
step:243/1645 train_time:22280ms step_avg:91.69ms
step:244/1645 train_time:22371ms step_avg:91.69ms
step:245/1645 train_time:22463ms step_avg:91.69ms
step:246/1645 train_time:22554ms step_avg:91.68ms
step:247/1645 train_time:22646ms step_avg:91.69ms
step:248/1645 train_time:22738ms step_avg:91.68ms
step:249/1645 train_time:22829ms step_avg:91.68ms
step:250/1645 train_time:22921ms step_avg:91.68ms
step:250/1645 val_loss:3.9663 train_time:23012ms step_avg:92.05ms
step:251/1645 train_time:23037ms step_avg:91.78ms
step:252/1645 train_time:23107ms step_avg:91.69ms
step:253/1645 train_time:23202ms step_avg:91.71ms
step:254/1645 train_time:23294ms step_avg:91.71ms
step:255/1645 train_time:23385ms step_avg:91.71ms
step:256/1645 train_time:23476ms step_avg:91.70ms
step:257/1645 train_time:23567ms step_avg:91.70ms
step:258/1645 train_time:23657ms step_avg:91.70ms
step:259/1645 train_time:23749ms step_avg:91.70ms
step:260/1645 train_time:23840ms step_avg:91.69ms
step:261/1645 train_time:23931ms step_avg:91.69ms
step:262/1645 train_time:24024ms step_avg:91.70ms
step:263/1645 train_time:24117ms step_avg:91.70ms
step:264/1645 train_time:24210ms step_avg:91.70ms
step:265/1645 train_time:24301ms step_avg:91.70ms
step:266/1645 train_time:24393ms step_avg:91.70ms
step:267/1645 train_time:24483ms step_avg:91.70ms
step:268/1645 train_time:24575ms step_avg:91.70ms
step:269/1645 train_time:24665ms step_avg:91.69ms
step:270/1645 train_time:24757ms step_avg:91.69ms
step:271/1645 train_time:24848ms step_avg:91.69ms
step:272/1645 train_time:24940ms step_avg:91.69ms
step:273/1645 train_time:25033ms step_avg:91.70ms
step:274/1645 train_time:25126ms step_avg:91.70ms
step:275/1645 train_time:25218ms step_avg:91.70ms
step:276/1645 train_time:25310ms step_avg:91.70ms
step:277/1645 train_time:25402ms step_avg:91.70ms
step:278/1645 train_time:25494ms step_avg:91.70ms
step:279/1645 train_time:25585ms step_avg:91.70ms
step:280/1645 train_time:25676ms step_avg:91.70ms
step:281/1645 train_time:25767ms step_avg:91.70ms
step:282/1645 train_time:25858ms step_avg:91.69ms
step:283/1645 train_time:25950ms step_avg:91.70ms
step:284/1645 train_time:26043ms step_avg:91.70ms
step:285/1645 train_time:26135ms step_avg:91.70ms
step:286/1645 train_time:26228ms step_avg:91.71ms
step:287/1645 train_time:26320ms step_avg:91.71ms
step:288/1645 train_time:26411ms step_avg:91.71ms
step:289/1645 train_time:26503ms step_avg:91.71ms
step:290/1645 train_time:26594ms step_avg:91.70ms
step:291/1645 train_time:26685ms step_avg:91.70ms
step:292/1645 train_time:26776ms step_avg:91.70ms
step:293/1645 train_time:26869ms step_avg:91.70ms
step:294/1645 train_time:26959ms step_avg:91.70ms
step:295/1645 train_time:27052ms step_avg:91.70ms
step:296/1645 train_time:27144ms step_avg:91.70ms
step:297/1645 train_time:27236ms step_avg:91.70ms
step:298/1645 train_time:27327ms step_avg:91.70ms
step:299/1645 train_time:27419ms step_avg:91.70ms
step:300/1645 train_time:27513ms step_avg:91.71ms
step:301/1645 train_time:27604ms step_avg:91.71ms
step:302/1645 train_time:27694ms step_avg:91.70ms
step:303/1645 train_time:27785ms step_avg:91.70ms
step:304/1645 train_time:27877ms step_avg:91.70ms
step:305/1645 train_time:27968ms step_avg:91.70ms
step:306/1645 train_time:28060ms step_avg:91.70ms
step:307/1645 train_time:28152ms step_avg:91.70ms
step:308/1645 train_time:28244ms step_avg:91.70ms
step:309/1645 train_time:28336ms step_avg:91.70ms
step:310/1645 train_time:28428ms step_avg:91.70ms
step:311/1645 train_time:28520ms step_avg:91.70ms
step:312/1645 train_time:28613ms step_avg:91.71ms
step:313/1645 train_time:28704ms step_avg:91.70ms
step:314/1645 train_time:28794ms step_avg:91.70ms
step:315/1645 train_time:28886ms step_avg:91.70ms
step:316/1645 train_time:28978ms step_avg:91.70ms
step:317/1645 train_time:29069ms step_avg:91.70ms
step:318/1645 train_time:29161ms step_avg:91.70ms
step:319/1645 train_time:29254ms step_avg:91.71ms
step:320/1645 train_time:29346ms step_avg:91.71ms
step:321/1645 train_time:29437ms step_avg:91.70ms
step:322/1645 train_time:29529ms step_avg:91.70ms
step:323/1645 train_time:29620ms step_avg:91.70ms
step:324/1645 train_time:29712ms step_avg:91.70ms
step:325/1645 train_time:29802ms step_avg:91.70ms
step:326/1645 train_time:29893ms step_avg:91.70ms
step:327/1645 train_time:29985ms step_avg:91.70ms
step:328/1645 train_time:30076ms step_avg:91.70ms
step:329/1645 train_time:30167ms step_avg:91.69ms
step:330/1645 train_time:30260ms step_avg:91.70ms
step:331/1645 train_time:30351ms step_avg:91.70ms
step:332/1645 train_time:30444ms step_avg:91.70ms
step:333/1645 train_time:30536ms step_avg:91.70ms
step:334/1645 train_time:30628ms step_avg:91.70ms
step:335/1645 train_time:30719ms step_avg:91.70ms
step:336/1645 train_time:30811ms step_avg:91.70ms
step:337/1645 train_time:30902ms step_avg:91.70ms
step:338/1645 train_time:30993ms step_avg:91.70ms
step:339/1645 train_time:31084ms step_avg:91.69ms
step:340/1645 train_time:31176ms step_avg:91.69ms
step:341/1645 train_time:31266ms step_avg:91.69ms
step:342/1645 train_time:31358ms step_avg:91.69ms
step:343/1645 train_time:31451ms step_avg:91.69ms
step:344/1645 train_time:31543ms step_avg:91.70ms
step:345/1645 train_time:31635ms step_avg:91.70ms
step:346/1645 train_time:31727ms step_avg:91.70ms
step:347/1645 train_time:31819ms step_avg:91.70ms
step:348/1645 train_time:31910ms step_avg:91.70ms
step:349/1645 train_time:32002ms step_avg:91.70ms
step:350/1645 train_time:32095ms step_avg:91.70ms
step:351/1645 train_time:32185ms step_avg:91.70ms
step:352/1645 train_time:32277ms step_avg:91.70ms
step:353/1645 train_time:32369ms step_avg:91.70ms
step:354/1645 train_time:32461ms step_avg:91.70ms
step:355/1645 train_time:32554ms step_avg:91.70ms
step:356/1645 train_time:32646ms step_avg:91.70ms
step:357/1645 train_time:32737ms step_avg:91.70ms
step:358/1645 train_time:32829ms step_avg:91.70ms
step:359/1645 train_time:32921ms step_avg:91.70ms
step:360/1645 train_time:33012ms step_avg:91.70ms
step:361/1645 train_time:33104ms step_avg:91.70ms
step:362/1645 train_time:33195ms step_avg:91.70ms
step:363/1645 train_time:33286ms step_avg:91.70ms
step:364/1645 train_time:33378ms step_avg:91.70ms
step:365/1645 train_time:33470ms step_avg:91.70ms
step:366/1645 train_time:33562ms step_avg:91.70ms
step:367/1645 train_time:33654ms step_avg:91.70ms
step:368/1645 train_time:33746ms step_avg:91.70ms
step:369/1645 train_time:33837ms step_avg:91.70ms
step:370/1645 train_time:33929ms step_avg:91.70ms
step:371/1645 train_time:34020ms step_avg:91.70ms
step:372/1645 train_time:34112ms step_avg:91.70ms
step:373/1645 train_time:34205ms step_avg:91.70ms
step:374/1645 train_time:34295ms step_avg:91.70ms
step:375/1645 train_time:34387ms step_avg:91.70ms
step:375/1645 val_loss:3.8136 train_time:34479ms step_avg:91.94ms
step:376/1645 train_time:34503ms step_avg:91.76ms
step:377/1645 train_time:34574ms step_avg:91.71ms
step:378/1645 train_time:34669ms step_avg:91.72ms
step:379/1645 train_time:34762ms step_avg:91.72ms
step:380/1645 train_time:34852ms step_avg:91.72ms
step:381/1645 train_time:34943ms step_avg:91.71ms
step:382/1645 train_time:35033ms step_avg:91.71ms
step:383/1645 train_time:35124ms step_avg:91.71ms
step:384/1645 train_time:35215ms step_avg:91.71ms
step:385/1645 train_time:35305ms step_avg:91.70ms
step:386/1645 train_time:35397ms step_avg:91.70ms
step:387/1645 train_time:35490ms step_avg:91.71ms
step:388/1645 train_time:35583ms step_avg:91.71ms
step:389/1645 train_time:35675ms step_avg:91.71ms
step:390/1645 train_time:35768ms step_avg:91.71ms
step:391/1645 train_time:35859ms step_avg:91.71ms
step:392/1645 train_time:35950ms step_avg:91.71ms
step:393/1645 train_time:36041ms step_avg:91.71ms
step:394/1645 train_time:36132ms step_avg:91.70ms
step:395/1645 train_time:36223ms step_avg:91.70ms
step:396/1645 train_time:36314ms step_avg:91.70ms
step:397/1645 train_time:36406ms step_avg:91.70ms
step:398/1645 train_time:36499ms step_avg:91.71ms
step:399/1645 train_time:36591ms step_avg:91.71ms
step:400/1645 train_time:36684ms step_avg:91.71ms
step:401/1645 train_time:36777ms step_avg:91.71ms
step:402/1645 train_time:36869ms step_avg:91.71ms
step:403/1645 train_time:36960ms step_avg:91.71ms
step:404/1645 train_time:37051ms step_avg:91.71ms
step:405/1645 train_time:37142ms step_avg:91.71ms
step:406/1645 train_time:37232ms step_avg:91.71ms
step:407/1645 train_time:37324ms step_avg:91.71ms
step:408/1645 train_time:37416ms step_avg:91.71ms
step:409/1645 train_time:37508ms step_avg:91.71ms
step:410/1645 train_time:37601ms step_avg:91.71ms
step:411/1645 train_time:37693ms step_avg:91.71ms
step:412/1645 train_time:37785ms step_avg:91.71ms
step:413/1645 train_time:37878ms step_avg:91.71ms
step:414/1645 train_time:37969ms step_avg:91.71ms
step:415/1645 train_time:38060ms step_avg:91.71ms
step:416/1645 train_time:38151ms step_avg:91.71ms
step:417/1645 train_time:38243ms step_avg:91.71ms
step:418/1645 train_time:38333ms step_avg:91.71ms
step:419/1645 train_time:38425ms step_avg:91.71ms
step:420/1645 train_time:38517ms step_avg:91.71ms
step:421/1645 train_time:38609ms step_avg:91.71ms
step:422/1645 train_time:38702ms step_avg:91.71ms
step:423/1645 train_time:38794ms step_avg:91.71ms
step:424/1645 train_time:38886ms step_avg:91.71ms
step:425/1645 train_time:38980ms step_avg:91.72ms
step:426/1645 train_time:39070ms step_avg:91.71ms
step:427/1645 train_time:39161ms step_avg:91.71ms
step:428/1645 train_time:39252ms step_avg:91.71ms
step:429/1645 train_time:39343ms step_avg:91.71ms
step:430/1645 train_time:39433ms step_avg:91.71ms
step:431/1645 train_time:39525ms step_avg:91.71ms
step:432/1645 train_time:39616ms step_avg:91.70ms
step:433/1645 train_time:39709ms step_avg:91.71ms
step:434/1645 train_time:39802ms step_avg:91.71ms
step:435/1645 train_time:39893ms step_avg:91.71ms
step:436/1645 train_time:39985ms step_avg:91.71ms
step:437/1645 train_time:40077ms step_avg:91.71ms
step:438/1645 train_time:40168ms step_avg:91.71ms
step:439/1645 train_time:40260ms step_avg:91.71ms
step:440/1645 train_time:40351ms step_avg:91.71ms
step:441/1645 train_time:40442ms step_avg:91.70ms
step:442/1645 train_time:40533ms step_avg:91.70ms
step:443/1645 train_time:40625ms step_avg:91.70ms
step:444/1645 train_time:40716ms step_avg:91.70ms
step:445/1645 train_time:40808ms step_avg:91.70ms
step:446/1645 train_time:40901ms step_avg:91.71ms
step:447/1645 train_time:40994ms step_avg:91.71ms
step:448/1645 train_time:41087ms step_avg:91.71ms
step:449/1645 train_time:41177ms step_avg:91.71ms
step:450/1645 train_time:41269ms step_avg:91.71ms
step:451/1645 train_time:41359ms step_avg:91.71ms
step:452/1645 train_time:41450ms step_avg:91.70ms
step:453/1645 train_time:41541ms step_avg:91.70ms
step:454/1645 train_time:41633ms step_avg:91.70ms
step:455/1645 train_time:41724ms step_avg:91.70ms
step:456/1645 train_time:41817ms step_avg:91.70ms
step:457/1645 train_time:41907ms step_avg:91.70ms
step:458/1645 train_time:42000ms step_avg:91.70ms
step:459/1645 train_time:42093ms step_avg:91.71ms
step:460/1645 train_time:42184ms step_avg:91.70ms
step:461/1645 train_time:42275ms step_avg:91.70ms
step:462/1645 train_time:42367ms step_avg:91.70ms
step:463/1645 train_time:42458ms step_avg:91.70ms
step:464/1645 train_time:42549ms step_avg:91.70ms
step:465/1645 train_time:42641ms step_avg:91.70ms
step:466/1645 train_time:42732ms step_avg:91.70ms
step:467/1645 train_time:42823ms step_avg:91.70ms
step:468/1645 train_time:42914ms step_avg:91.70ms
step:469/1645 train_time:43007ms step_avg:91.70ms
step:470/1645 train_time:43099ms step_avg:91.70ms
step:471/1645 train_time:43192ms step_avg:91.70ms
step:472/1645 train_time:43285ms step_avg:91.71ms
step:473/1645 train_time:43376ms step_avg:91.70ms
step:474/1645 train_time:43467ms step_avg:91.70ms
step:475/1645 train_time:43559ms step_avg:91.70ms
step:476/1645 train_time:43650ms step_avg:91.70ms
step:477/1645 train_time:43742ms step_avg:91.70ms
step:478/1645 train_time:43833ms step_avg:91.70ms
step:479/1645 train_time:43924ms step_avg:91.70ms
step:480/1645 train_time:44016ms step_avg:91.70ms
step:481/1645 train_time:44108ms step_avg:91.70ms
step:482/1645 train_time:44201ms step_avg:91.70ms
step:483/1645 train_time:44293ms step_avg:91.70ms
step:484/1645 train_time:44385ms step_avg:91.70ms
step:485/1645 train_time:44476ms step_avg:91.70ms
step:486/1645 train_time:44567ms step_avg:91.70ms
step:487/1645 train_time:44659ms step_avg:91.70ms
step:488/1645 train_time:44751ms step_avg:91.70ms
step:489/1645 train_time:44842ms step_avg:91.70ms
step:490/1645 train_time:44934ms step_avg:91.70ms
step:491/1645 train_time:45025ms step_avg:91.70ms
step:492/1645 train_time:45116ms step_avg:91.70ms
step:493/1645 train_time:45209ms step_avg:91.70ms
step:494/1645 train_time:45301ms step_avg:91.70ms
step:495/1645 train_time:45394ms step_avg:91.70ms
step:496/1645 train_time:45485ms step_avg:91.70ms
step:497/1645 train_time:45576ms step_avg:91.70ms
step:498/1645 train_time:45668ms step_avg:91.70ms
step:499/1645 train_time:45759ms step_avg:91.70ms
step:500/1645 train_time:45850ms step_avg:91.70ms
step:500/1645 val_loss:3.7121 train_time:45943ms step_avg:91.89ms
step:501/1645 train_time:45963ms step_avg:91.74ms
step:502/1645 train_time:46038ms step_avg:91.71ms
step:503/1645 train_time:46133ms step_avg:91.72ms
step:504/1645 train_time:46225ms step_avg:91.72ms
step:505/1645 train_time:46317ms step_avg:91.72ms
step:506/1645 train_time:46407ms step_avg:91.71ms
step:507/1645 train_time:46498ms step_avg:91.71ms
step:508/1645 train_time:46588ms step_avg:91.71ms
step:509/1645 train_time:46680ms step_avg:91.71ms
step:510/1645 train_time:46771ms step_avg:91.71ms
step:511/1645 train_time:46862ms step_avg:91.71ms
step:512/1645 train_time:46955ms step_avg:91.71ms
step:513/1645 train_time:47049ms step_avg:91.71ms
step:514/1645 train_time:47141ms step_avg:91.71ms
step:515/1645 train_time:47234ms step_avg:91.72ms
step:516/1645 train_time:47326ms step_avg:91.72ms
step:517/1645 train_time:47417ms step_avg:91.72ms
step:518/1645 train_time:47508ms step_avg:91.71ms
step:519/1645 train_time:47598ms step_avg:91.71ms
step:520/1645 train_time:47690ms step_avg:91.71ms
step:521/1645 train_time:47780ms step_avg:91.71ms
step:522/1645 train_time:47873ms step_avg:91.71ms
step:523/1645 train_time:47964ms step_avg:91.71ms
step:524/1645 train_time:48057ms step_avg:91.71ms
step:525/1645 train_time:48150ms step_avg:91.71ms
step:526/1645 train_time:48243ms step_avg:91.72ms
step:527/1645 train_time:48335ms step_avg:91.72ms
step:528/1645 train_time:48426ms step_avg:91.72ms
step:529/1645 train_time:48518ms step_avg:91.72ms
step:530/1645 train_time:48609ms step_avg:91.72ms
step:531/1645 train_time:48700ms step_avg:91.71ms
step:532/1645 train_time:48791ms step_avg:91.71ms
step:533/1645 train_time:48885ms step_avg:91.72ms
step:534/1645 train_time:48976ms step_avg:91.71ms
step:535/1645 train_time:49067ms step_avg:91.71ms
step:536/1645 train_time:49160ms step_avg:91.72ms
step:537/1645 train_time:49252ms step_avg:91.72ms
step:538/1645 train_time:49344ms step_avg:91.72ms
step:539/1645 train_time:49435ms step_avg:91.72ms
step:540/1645 train_time:49526ms step_avg:91.72ms
step:541/1645 train_time:49618ms step_avg:91.72ms
step:542/1645 train_time:49709ms step_avg:91.71ms
step:543/1645 train_time:49800ms step_avg:91.71ms
step:544/1645 train_time:49892ms step_avg:91.71ms
step:545/1645 train_time:49984ms step_avg:91.71ms
step:546/1645 train_time:50076ms step_avg:91.71ms
step:547/1645 train_time:50168ms step_avg:91.71ms
step:548/1645 train_time:50260ms step_avg:91.71ms
step:549/1645 train_time:50353ms step_avg:91.72ms
step:550/1645 train_time:50445ms step_avg:91.72ms
step:551/1645 train_time:50537ms step_avg:91.72ms
step:552/1645 train_time:50630ms step_avg:91.72ms
step:553/1645 train_time:50722ms step_avg:91.72ms
step:554/1645 train_time:50816ms step_avg:91.73ms
step:555/1645 train_time:50908ms step_avg:91.73ms
step:556/1645 train_time:51001ms step_avg:91.73ms
step:557/1645 train_time:51095ms step_avg:91.73ms
step:558/1645 train_time:51188ms step_avg:91.73ms
step:559/1645 train_time:51282ms step_avg:91.74ms
step:560/1645 train_time:51375ms step_avg:91.74ms
step:561/1645 train_time:51468ms step_avg:91.74ms
step:562/1645 train_time:51561ms step_avg:91.74ms
step:563/1645 train_time:51654ms step_avg:91.75ms
step:564/1645 train_time:51746ms step_avg:91.75ms
step:565/1645 train_time:51839ms step_avg:91.75ms
step:566/1645 train_time:51932ms step_avg:91.75ms
step:567/1645 train_time:52024ms step_avg:91.75ms
step:568/1645 train_time:52118ms step_avg:91.76ms
step:569/1645 train_time:52212ms step_avg:91.76ms
step:570/1645 train_time:52305ms step_avg:91.76ms
step:571/1645 train_time:52398ms step_avg:91.76ms
step:572/1645 train_time:52490ms step_avg:91.77ms
step:573/1645 train_time:52583ms step_avg:91.77ms
step:574/1645 train_time:52676ms step_avg:91.77ms
step:575/1645 train_time:52770ms step_avg:91.77ms
step:576/1645 train_time:52861ms step_avg:91.77ms
step:577/1645 train_time:52955ms step_avg:91.78ms
step:578/1645 train_time:53048ms step_avg:91.78ms
step:579/1645 train_time:53141ms step_avg:91.78ms
step:580/1645 train_time:53235ms step_avg:91.78ms
step:581/1645 train_time:53328ms step_avg:91.79ms
step:582/1645 train_time:53421ms step_avg:91.79ms
step:583/1645 train_time:53515ms step_avg:91.79ms
step:584/1645 train_time:53608ms step_avg:91.79ms
step:585/1645 train_time:53702ms step_avg:91.80ms
step:586/1645 train_time:53794ms step_avg:91.80ms
step:587/1645 train_time:53886ms step_avg:91.80ms
step:588/1645 train_time:53979ms step_avg:91.80ms
step:589/1645 train_time:54072ms step_avg:91.80ms
step:590/1645 train_time:54165ms step_avg:91.80ms
step:591/1645 train_time:54258ms step_avg:91.81ms
step:592/1645 train_time:54352ms step_avg:91.81ms
step:593/1645 train_time:54445ms step_avg:91.81ms
step:594/1645 train_time:54538ms step_avg:91.81ms
step:595/1645 train_time:54631ms step_avg:91.82ms
step:596/1645 train_time:54724ms step_avg:91.82ms
step:597/1645 train_time:54817ms step_avg:91.82ms
step:598/1645 train_time:54909ms step_avg:91.82ms
step:599/1645 train_time:55003ms step_avg:91.82ms
step:600/1645 train_time:55096ms step_avg:91.83ms
step:601/1645 train_time:55189ms step_avg:91.83ms
step:602/1645 train_time:55282ms step_avg:91.83ms
step:603/1645 train_time:55375ms step_avg:91.83ms
step:604/1645 train_time:55468ms step_avg:91.84ms
step:605/1645 train_time:55561ms step_avg:91.84ms
step:606/1645 train_time:55655ms step_avg:91.84ms
step:607/1645 train_time:55748ms step_avg:91.84ms
step:608/1645 train_time:55841ms step_avg:91.84ms
step:609/1645 train_time:55933ms step_avg:91.84ms
step:610/1645 train_time:56026ms step_avg:91.85ms
step:611/1645 train_time:56119ms step_avg:91.85ms
step:612/1645 train_time:56212ms step_avg:91.85ms
step:613/1645 train_time:56305ms step_avg:91.85ms
step:614/1645 train_time:56399ms step_avg:91.85ms
step:615/1645 train_time:56493ms step_avg:91.86ms
step:616/1645 train_time:56586ms step_avg:91.86ms
step:617/1645 train_time:56679ms step_avg:91.86ms
step:618/1645 train_time:56772ms step_avg:91.86ms
step:619/1645 train_time:56865ms step_avg:91.87ms
step:620/1645 train_time:56957ms step_avg:91.87ms
step:621/1645 train_time:57050ms step_avg:91.87ms
step:622/1645 train_time:57142ms step_avg:91.87ms
step:623/1645 train_time:57236ms step_avg:91.87ms
step:624/1645 train_time:57329ms step_avg:91.87ms
step:625/1645 train_time:57422ms step_avg:91.88ms
step:625/1645 val_loss:3.6130 train_time:57517ms step_avg:92.03ms
step:626/1645 train_time:57537ms step_avg:91.91ms
step:627/1645 train_time:57612ms step_avg:91.89ms
step:628/1645 train_time:57714ms step_avg:91.90ms
step:629/1645 train_time:57809ms step_avg:91.91ms
step:630/1645 train_time:57902ms step_avg:91.91ms
step:631/1645 train_time:57994ms step_avg:91.91ms
step:632/1645 train_time:58085ms step_avg:91.91ms
step:633/1645 train_time:58177ms step_avg:91.91ms
step:634/1645 train_time:58269ms step_avg:91.91ms
step:635/1645 train_time:58361ms step_avg:91.91ms
step:636/1645 train_time:58456ms step_avg:91.91ms
step:637/1645 train_time:58552ms step_avg:91.92ms
step:638/1645 train_time:58646ms step_avg:91.92ms
step:639/1645 train_time:58740ms step_avg:91.93ms
step:640/1645 train_time:58833ms step_avg:91.93ms
step:641/1645 train_time:58926ms step_avg:91.93ms
step:642/1645 train_time:59018ms step_avg:91.93ms
step:643/1645 train_time:59111ms step_avg:91.93ms
step:644/1645 train_time:59203ms step_avg:91.93ms
step:645/1645 train_time:59295ms step_avg:91.93ms
step:646/1645 train_time:59387ms step_avg:91.93ms
step:647/1645 train_time:59481ms step_avg:91.93ms
step:648/1645 train_time:59577ms step_avg:91.94ms
step:649/1645 train_time:59670ms step_avg:91.94ms
step:650/1645 train_time:59763ms step_avg:91.94ms
step:651/1645 train_time:59856ms step_avg:91.94ms
step:652/1645 train_time:59948ms step_avg:91.95ms
step:653/1645 train_time:60041ms step_avg:91.95ms
step:654/1645 train_time:60133ms step_avg:91.95ms
step:655/1645 train_time:60226ms step_avg:91.95ms
step:656/1645 train_time:60318ms step_avg:91.95ms
step:657/1645 train_time:60412ms step_avg:91.95ms
step:658/1645 train_time:60505ms step_avg:91.95ms
step:659/1645 train_time:60598ms step_avg:91.96ms
step:660/1645 train_time:60694ms step_avg:91.96ms
step:661/1645 train_time:60786ms step_avg:91.96ms
step:662/1645 train_time:60878ms step_avg:91.96ms
step:663/1645 train_time:60972ms step_avg:91.96ms
step:664/1645 train_time:61065ms step_avg:91.96ms
step:665/1645 train_time:61157ms step_avg:91.97ms
step:666/1645 train_time:61250ms step_avg:91.97ms
step:667/1645 train_time:61342ms step_avg:91.97ms
step:668/1645 train_time:61435ms step_avg:91.97ms
step:669/1645 train_time:61529ms step_avg:91.97ms
step:670/1645 train_time:61622ms step_avg:91.97ms
step:671/1645 train_time:61715ms step_avg:91.97ms
step:672/1645 train_time:61808ms step_avg:91.98ms
step:673/1645 train_time:61901ms step_avg:91.98ms
step:674/1645 train_time:61995ms step_avg:91.98ms
step:675/1645 train_time:62088ms step_avg:91.98ms
step:676/1645 train_time:62180ms step_avg:91.98ms
step:677/1645 train_time:62272ms step_avg:91.98ms
step:678/1645 train_time:62365ms step_avg:91.98ms
step:679/1645 train_time:62458ms step_avg:91.98ms
step:680/1645 train_time:62550ms step_avg:91.99ms
step:681/1645 train_time:62643ms step_avg:91.99ms
step:682/1645 train_time:62736ms step_avg:91.99ms
step:683/1645 train_time:62829ms step_avg:91.99ms
step:684/1645 train_time:62922ms step_avg:91.99ms
step:685/1645 train_time:63016ms step_avg:91.99ms
step:686/1645 train_time:63109ms step_avg:92.00ms
step:687/1645 train_time:63201ms step_avg:92.00ms
step:688/1645 train_time:63294ms step_avg:92.00ms
step:689/1645 train_time:63387ms step_avg:92.00ms
step:690/1645 train_time:63480ms step_avg:92.00ms
step:691/1645 train_time:63573ms step_avg:92.00ms
step:692/1645 train_time:63666ms step_avg:92.00ms
step:693/1645 train_time:63760ms step_avg:92.01ms
step:694/1645 train_time:63853ms step_avg:92.01ms
step:695/1645 train_time:63946ms step_avg:92.01ms
step:696/1645 train_time:64039ms step_avg:92.01ms
step:697/1645 train_time:64132ms step_avg:92.01ms
step:698/1645 train_time:64225ms step_avg:92.01ms
step:699/1645 train_time:64318ms step_avg:92.01ms
step:700/1645 train_time:64411ms step_avg:92.02ms
step:701/1645 train_time:64504ms step_avg:92.02ms
step:702/1645 train_time:64597ms step_avg:92.02ms
step:703/1645 train_time:64690ms step_avg:92.02ms
step:704/1645 train_time:64782ms step_avg:92.02ms
step:705/1645 train_time:64875ms step_avg:92.02ms
step:706/1645 train_time:64968ms step_avg:92.02ms
step:707/1645 train_time:65061ms step_avg:92.02ms
step:708/1645 train_time:65154ms step_avg:92.03ms
step:709/1645 train_time:65247ms step_avg:92.03ms
step:710/1645 train_time:65340ms step_avg:92.03ms
step:711/1645 train_time:65432ms step_avg:92.03ms
step:712/1645 train_time:65525ms step_avg:92.03ms
step:713/1645 train_time:65621ms step_avg:92.03ms
step:714/1645 train_time:65713ms step_avg:92.03ms
step:715/1645 train_time:65805ms step_avg:92.04ms
step:716/1645 train_time:65898ms step_avg:92.04ms
step:717/1645 train_time:65990ms step_avg:92.04ms
step:718/1645 train_time:66082ms step_avg:92.04ms
step:719/1645 train_time:66176ms step_avg:92.04ms
step:720/1645 train_time:66269ms step_avg:92.04ms
step:721/1645 train_time:66361ms step_avg:92.04ms
step:722/1645 train_time:66454ms step_avg:92.04ms
step:723/1645 train_time:66547ms step_avg:92.04ms
step:724/1645 train_time:66640ms step_avg:92.04ms
step:725/1645 train_time:66734ms step_avg:92.05ms
step:726/1645 train_time:66827ms step_avg:92.05ms
step:727/1645 train_time:66921ms step_avg:92.05ms
step:728/1645 train_time:67014ms step_avg:92.05ms
step:729/1645 train_time:67106ms step_avg:92.05ms
step:730/1645 train_time:67199ms step_avg:92.05ms
step:731/1645 train_time:67292ms step_avg:92.05ms
step:732/1645 train_time:67385ms step_avg:92.06ms
step:733/1645 train_time:67477ms step_avg:92.06ms
step:734/1645 train_time:67570ms step_avg:92.06ms
step:735/1645 train_time:67663ms step_avg:92.06ms
step:736/1645 train_time:67756ms step_avg:92.06ms
step:737/1645 train_time:67849ms step_avg:92.06ms
step:738/1645 train_time:67941ms step_avg:92.06ms
step:739/1645 train_time:68034ms step_avg:92.06ms
step:740/1645 train_time:68126ms step_avg:92.06ms
step:741/1645 train_time:68220ms step_avg:92.06ms
step:742/1645 train_time:68313ms step_avg:92.07ms
step:743/1645 train_time:68406ms step_avg:92.07ms
step:744/1645 train_time:68500ms step_avg:92.07ms
step:745/1645 train_time:68593ms step_avg:92.07ms
step:746/1645 train_time:68686ms step_avg:92.07ms
step:747/1645 train_time:68778ms step_avg:92.07ms
step:748/1645 train_time:68871ms step_avg:92.07ms
step:749/1645 train_time:68963ms step_avg:92.07ms
step:750/1645 train_time:69055ms step_avg:92.07ms
step:750/1645 val_loss:3.5612 train_time:69148ms step_avg:92.20ms
step:751/1645 train_time:69169ms step_avg:92.10ms
step:752/1645 train_time:69245ms step_avg:92.08ms
step:753/1645 train_time:69338ms step_avg:92.08ms
step:754/1645 train_time:69430ms step_avg:92.08ms
step:755/1645 train_time:69522ms step_avg:92.08ms
step:756/1645 train_time:69614ms step_avg:92.08ms
step:757/1645 train_time:69706ms step_avg:92.08ms
step:758/1645 train_time:69798ms step_avg:92.08ms
step:759/1645 train_time:69891ms step_avg:92.08ms
step:760/1645 train_time:69984ms step_avg:92.08ms
step:761/1645 train_time:70079ms step_avg:92.09ms
step:762/1645 train_time:70173ms step_avg:92.09ms
step:763/1645 train_time:70267ms step_avg:92.09ms
step:764/1645 train_time:70360ms step_avg:92.09ms
step:765/1645 train_time:70454ms step_avg:92.10ms
step:766/1645 train_time:70547ms step_avg:92.10ms
step:767/1645 train_time:70639ms step_avg:92.10ms
step:768/1645 train_time:70731ms step_avg:92.10ms
step:769/1645 train_time:70824ms step_avg:92.10ms
step:770/1645 train_time:70916ms step_avg:92.10ms
step:771/1645 train_time:71009ms step_avg:92.10ms
step:772/1645 train_time:71102ms step_avg:92.10ms
step:773/1645 train_time:71195ms step_avg:92.10ms
step:774/1645 train_time:71288ms step_avg:92.10ms
step:775/1645 train_time:71382ms step_avg:92.11ms
step:776/1645 train_time:71475ms step_avg:92.11ms
step:777/1645 train_time:71567ms step_avg:92.11ms
step:778/1645 train_time:71660ms step_avg:92.11ms
step:779/1645 train_time:71753ms step_avg:92.11ms
step:780/1645 train_time:71845ms step_avg:92.11ms
step:781/1645 train_time:71939ms step_avg:92.11ms
step:782/1645 train_time:72032ms step_avg:92.11ms
step:783/1645 train_time:72125ms step_avg:92.11ms
step:784/1645 train_time:72219ms step_avg:92.12ms
step:785/1645 train_time:72312ms step_avg:92.12ms
step:786/1645 train_time:72405ms step_avg:92.12ms
step:787/1645 train_time:72498ms step_avg:92.12ms
step:788/1645 train_time:72593ms step_avg:92.12ms
step:789/1645 train_time:72683ms step_avg:92.12ms
step:790/1645 train_time:72776ms step_avg:92.12ms
step:791/1645 train_time:72868ms step_avg:92.12ms
step:792/1645 train_time:72961ms step_avg:92.12ms
step:793/1645 train_time:73055ms step_avg:92.12ms
step:794/1645 train_time:73148ms step_avg:92.13ms
step:795/1645 train_time:73241ms step_avg:92.13ms
step:796/1645 train_time:73334ms step_avg:92.13ms
step:797/1645 train_time:73427ms step_avg:92.13ms
step:798/1645 train_time:73520ms step_avg:92.13ms
step:799/1645 train_time:73613ms step_avg:92.13ms
step:800/1645 train_time:73705ms step_avg:92.13ms
step:801/1645 train_time:73799ms step_avg:92.13ms
step:802/1645 train_time:73892ms step_avg:92.13ms
step:803/1645 train_time:73984ms step_avg:92.13ms
step:804/1645 train_time:74078ms step_avg:92.14ms
step:805/1645 train_time:74171ms step_avg:92.14ms
step:806/1645 train_time:74264ms step_avg:92.14ms
step:807/1645 train_time:74358ms step_avg:92.14ms
step:808/1645 train_time:74451ms step_avg:92.14ms
step:809/1645 train_time:74544ms step_avg:92.14ms
step:810/1645 train_time:74636ms step_avg:92.14ms
step:811/1645 train_time:74728ms step_avg:92.14ms
step:812/1645 train_time:74821ms step_avg:92.14ms
step:813/1645 train_time:74914ms step_avg:92.14ms
step:814/1645 train_time:75007ms step_avg:92.15ms
step:815/1645 train_time:75100ms step_avg:92.15ms
step:816/1645 train_time:75193ms step_avg:92.15ms
step:817/1645 train_time:75286ms step_avg:92.15ms
step:818/1645 train_time:75380ms step_avg:92.15ms
step:819/1645 train_time:75473ms step_avg:92.15ms
step:820/1645 train_time:75565ms step_avg:92.15ms
step:821/1645 train_time:75658ms step_avg:92.15ms
step:822/1645 train_time:75751ms step_avg:92.15ms
step:823/1645 train_time:75843ms step_avg:92.15ms
step:824/1645 train_time:75936ms step_avg:92.16ms
step:825/1645 train_time:76029ms step_avg:92.16ms
step:826/1645 train_time:76122ms step_avg:92.16ms
step:827/1645 train_time:76216ms step_avg:92.16ms
step:828/1645 train_time:76311ms step_avg:92.16ms
step:829/1645 train_time:76403ms step_avg:92.16ms
step:830/1645 train_time:76496ms step_avg:92.16ms
step:831/1645 train_time:76588ms step_avg:92.16ms
step:832/1645 train_time:76680ms step_avg:92.16ms
step:833/1645 train_time:76773ms step_avg:92.16ms
step:834/1645 train_time:76865ms step_avg:92.16ms
step:835/1645 train_time:76958ms step_avg:92.17ms
step:836/1645 train_time:77051ms step_avg:92.17ms
step:837/1645 train_time:77144ms step_avg:92.17ms
step:838/1645 train_time:77238ms step_avg:92.17ms
step:839/1645 train_time:77331ms step_avg:92.17ms
step:840/1645 train_time:77424ms step_avg:92.17ms
step:841/1645 train_time:77518ms step_avg:92.17ms
step:842/1645 train_time:77611ms step_avg:92.17ms
step:843/1645 train_time:77703ms step_avg:92.17ms
step:844/1645 train_time:77795ms step_avg:92.17ms
step:845/1645 train_time:77888ms step_avg:92.18ms
step:846/1645 train_time:77980ms step_avg:92.18ms
step:847/1645 train_time:78074ms step_avg:92.18ms
step:848/1645 train_time:78166ms step_avg:92.18ms
step:849/1645 train_time:78260ms step_avg:92.18ms
step:850/1645 train_time:78354ms step_avg:92.18ms
step:851/1645 train_time:78448ms step_avg:92.18ms
step:852/1645 train_time:78541ms step_avg:92.18ms
step:853/1645 train_time:78634ms step_avg:92.18ms
step:854/1645 train_time:78726ms step_avg:92.19ms
step:855/1645 train_time:78821ms step_avg:92.19ms
step:856/1645 train_time:78912ms step_avg:92.19ms
step:857/1645 train_time:79005ms step_avg:92.19ms
step:858/1645 train_time:79097ms step_avg:92.19ms
step:859/1645 train_time:79189ms step_avg:92.19ms
step:860/1645 train_time:79282ms step_avg:92.19ms
step:861/1645 train_time:79376ms step_avg:92.19ms
step:862/1645 train_time:79468ms step_avg:92.19ms
step:863/1645 train_time:79561ms step_avg:92.19ms
step:864/1645 train_time:79654ms step_avg:92.19ms
step:865/1645 train_time:79747ms step_avg:92.19ms
step:866/1645 train_time:79840ms step_avg:92.19ms
step:867/1645 train_time:79933ms step_avg:92.19ms
step:868/1645 train_time:80027ms step_avg:92.20ms
step:869/1645 train_time:80119ms step_avg:92.20ms
step:870/1645 train_time:80211ms step_avg:92.20ms
step:871/1645 train_time:80304ms step_avg:92.20ms
step:872/1645 train_time:80397ms step_avg:92.20ms
step:873/1645 train_time:80489ms step_avg:92.20ms
step:874/1645 train_time:80582ms step_avg:92.20ms
step:875/1645 train_time:80675ms step_avg:92.20ms
step:875/1645 val_loss:3.5148 train_time:80768ms step_avg:92.31ms
step:876/1645 train_time:80789ms step_avg:92.22ms
step:877/1645 train_time:80866ms step_avg:92.21ms
step:878/1645 train_time:80963ms step_avg:92.21ms
step:879/1645 train_time:81055ms step_avg:92.21ms
step:880/1645 train_time:81147ms step_avg:92.21ms
step:881/1645 train_time:81238ms step_avg:92.21ms
step:882/1645 train_time:81330ms step_avg:92.21ms
step:883/1645 train_time:81423ms step_avg:92.21ms
step:884/1645 train_time:81515ms step_avg:92.21ms
step:885/1645 train_time:81607ms step_avg:92.21ms
step:886/1645 train_time:81701ms step_avg:92.21ms
step:887/1645 train_time:81797ms step_avg:92.22ms
step:888/1645 train_time:81892ms step_avg:92.22ms
step:889/1645 train_time:81985ms step_avg:92.22ms
step:890/1645 train_time:82079ms step_avg:92.22ms
step:891/1645 train_time:82170ms step_avg:92.22ms
step:892/1645 train_time:82262ms step_avg:92.22ms
step:893/1645 train_time:82354ms step_avg:92.22ms
step:894/1645 train_time:82447ms step_avg:92.22ms
step:895/1645 train_time:82539ms step_avg:92.22ms
step:896/1645 train_time:82630ms step_avg:92.22ms
step:897/1645 train_time:82724ms step_avg:92.22ms
step:898/1645 train_time:82818ms step_avg:92.23ms
step:899/1645 train_time:82912ms step_avg:92.23ms
step:900/1645 train_time:83005ms step_avg:92.23ms
step:901/1645 train_time:83098ms step_avg:92.23ms
step:902/1645 train_time:83191ms step_avg:92.23ms
step:903/1645 train_time:83283ms step_avg:92.23ms
step:904/1645 train_time:83375ms step_avg:92.23ms
step:905/1645 train_time:83467ms step_avg:92.23ms
step:906/1645 train_time:83560ms step_avg:92.23ms
step:907/1645 train_time:83653ms step_avg:92.23ms
step:908/1645 train_time:83746ms step_avg:92.23ms
step:909/1645 train_time:83840ms step_avg:92.23ms
step:910/1645 train_time:83934ms step_avg:92.23ms
step:911/1645 train_time:84027ms step_avg:92.24ms
step:912/1645 train_time:84119ms step_avg:92.24ms
step:913/1645 train_time:84211ms step_avg:92.24ms
step:914/1645 train_time:84304ms step_avg:92.24ms
step:915/1645 train_time:84397ms step_avg:92.24ms
step:916/1645 train_time:84489ms step_avg:92.24ms
step:917/1645 train_time:84582ms step_avg:92.24ms
step:918/1645 train_time:84675ms step_avg:92.24ms
step:919/1645 train_time:84768ms step_avg:92.24ms
step:920/1645 train_time:84862ms step_avg:92.24ms
step:921/1645 train_time:84956ms step_avg:92.24ms
step:922/1645 train_time:85049ms step_avg:92.24ms
step:923/1645 train_time:85141ms step_avg:92.24ms
step:924/1645 train_time:85234ms step_avg:92.24ms
step:925/1645 train_time:85327ms step_avg:92.25ms
step:926/1645 train_time:85419ms step_avg:92.25ms
step:927/1645 train_time:85511ms step_avg:92.25ms
step:928/1645 train_time:85604ms step_avg:92.25ms
step:929/1645 train_time:85698ms step_avg:92.25ms
step:930/1645 train_time:85791ms step_avg:92.25ms
step:931/1645 train_time:85884ms step_avg:92.25ms
step:932/1645 train_time:85978ms step_avg:92.25ms
step:933/1645 train_time:86070ms step_avg:92.25ms
step:934/1645 train_time:86163ms step_avg:92.25ms
step:935/1645 train_time:86256ms step_avg:92.25ms
step:936/1645 train_time:86349ms step_avg:92.25ms
step:937/1645 train_time:86441ms step_avg:92.25ms
step:938/1645 train_time:86533ms step_avg:92.25ms
step:939/1645 train_time:86626ms step_avg:92.25ms
step:940/1645 train_time:86719ms step_avg:92.25ms
step:941/1645 train_time:86812ms step_avg:92.25ms
step:942/1645 train_time:86905ms step_avg:92.26ms
step:943/1645 train_time:86999ms step_avg:92.26ms
step:944/1645 train_time:87092ms step_avg:92.26ms
step:945/1645 train_time:87185ms step_avg:92.26ms
step:946/1645 train_time:87277ms step_avg:92.26ms
step:947/1645 train_time:87370ms step_avg:92.26ms
step:948/1645 train_time:87463ms step_avg:92.26ms
step:949/1645 train_time:87556ms step_avg:92.26ms
step:950/1645 train_time:87649ms step_avg:92.26ms
step:951/1645 train_time:87742ms step_avg:92.26ms
step:952/1645 train_time:87835ms step_avg:92.26ms
step:953/1645 train_time:87928ms step_avg:92.26ms
step:954/1645 train_time:88021ms step_avg:92.27ms
step:955/1645 train_time:88115ms step_avg:92.27ms
step:956/1645 train_time:88209ms step_avg:92.27ms
step:957/1645 train_time:88300ms step_avg:92.27ms
step:958/1645 train_time:88393ms step_avg:92.27ms
step:959/1645 train_time:88486ms step_avg:92.27ms
step:960/1645 train_time:88579ms step_avg:92.27ms
step:961/1645 train_time:88672ms step_avg:92.27ms
step:962/1645 train_time:88765ms step_avg:92.27ms
step:963/1645 train_time:88857ms step_avg:92.27ms
step:964/1645 train_time:88950ms step_avg:92.27ms
step:965/1645 train_time:89044ms step_avg:92.27ms
step:966/1645 train_time:89137ms step_avg:92.27ms
step:967/1645 train_time:89229ms step_avg:92.27ms
step:968/1645 train_time:89322ms step_avg:92.27ms
step:969/1645 train_time:89415ms step_avg:92.28ms
step:970/1645 train_time:89507ms step_avg:92.27ms
step:971/1645 train_time:89601ms step_avg:92.28ms
step:972/1645 train_time:89694ms step_avg:92.28ms
step:973/1645 train_time:89786ms step_avg:92.28ms
step:974/1645 train_time:89879ms step_avg:92.28ms
step:975/1645 train_time:89972ms step_avg:92.28ms
step:976/1645 train_time:90064ms step_avg:92.28ms
step:977/1645 train_time:90158ms step_avg:92.28ms
step:978/1645 train_time:90251ms step_avg:92.28ms
step:979/1645 train_time:90343ms step_avg:92.28ms
step:980/1645 train_time:90436ms step_avg:92.28ms
step:981/1645 train_time:90529ms step_avg:92.28ms
step:982/1645 train_time:90622ms step_avg:92.28ms
step:983/1645 train_time:90715ms step_avg:92.28ms
step:984/1645 train_time:90808ms step_avg:92.28ms
step:985/1645 train_time:90902ms step_avg:92.29ms
step:986/1645 train_time:90995ms step_avg:92.29ms
step:987/1645 train_time:91087ms step_avg:92.29ms
step:988/1645 train_time:91180ms step_avg:92.29ms
step:989/1645 train_time:91273ms step_avg:92.29ms
step:990/1645 train_time:91365ms step_avg:92.29ms
step:991/1645 train_time:91458ms step_avg:92.29ms
step:992/1645 train_time:91551ms step_avg:92.29ms
step:993/1645 train_time:91643ms step_avg:92.29ms
step:994/1645 train_time:91736ms step_avg:92.29ms
step:995/1645 train_time:91829ms step_avg:92.29ms
step:996/1645 train_time:91922ms step_avg:92.29ms
step:997/1645 train_time:92016ms step_avg:92.29ms
step:998/1645 train_time:92109ms step_avg:92.29ms
step:999/1645 train_time:92203ms step_avg:92.29ms
step:1000/1645 train_time:92295ms step_avg:92.30ms
step:1000/1645 val_loss:3.4650 train_time:92388ms step_avg:92.39ms
step:1001/1645 train_time:92409ms step_avg:92.32ms
step:1002/1645 train_time:92486ms step_avg:92.30ms
step:1003/1645 train_time:92581ms step_avg:92.30ms
step:1004/1645 train_time:92673ms step_avg:92.30ms
step:1005/1645 train_time:92765ms step_avg:92.30ms
step:1006/1645 train_time:92857ms step_avg:92.30ms
step:1007/1645 train_time:92948ms step_avg:92.30ms
step:1008/1645 train_time:93040ms step_avg:92.30ms
step:1009/1645 train_time:93132ms step_avg:92.30ms
step:1010/1645 train_time:93225ms step_avg:92.30ms
step:1011/1645 train_time:93318ms step_avg:92.30ms
step:1012/1645 train_time:93413ms step_avg:92.30ms
step:1013/1645 train_time:93508ms step_avg:92.31ms
step:1014/1645 train_time:93603ms step_avg:92.31ms
step:1015/1645 train_time:93695ms step_avg:92.31ms
step:1016/1645 train_time:93788ms step_avg:92.31ms
step:1017/1645 train_time:93880ms step_avg:92.31ms
step:1018/1645 train_time:93971ms step_avg:92.31ms
step:1019/1645 train_time:94063ms step_avg:92.31ms
step:1020/1645 train_time:94155ms step_avg:92.31ms
step:1021/1645 train_time:94249ms step_avg:92.31ms
step:1022/1645 train_time:94343ms step_avg:92.31ms
step:1023/1645 train_time:94437ms step_avg:92.31ms
step:1024/1645 train_time:94531ms step_avg:92.32ms
step:1025/1645 train_time:94624ms step_avg:92.32ms
step:1026/1645 train_time:94717ms step_avg:92.32ms
step:1027/1645 train_time:94809ms step_avg:92.32ms
step:1028/1645 train_time:94902ms step_avg:92.32ms
step:1029/1645 train_time:94993ms step_avg:92.32ms
step:1030/1645 train_time:95085ms step_avg:92.32ms
step:1031/1645 train_time:95177ms step_avg:92.32ms
step:1032/1645 train_time:95270ms step_avg:92.32ms
step:1033/1645 train_time:95364ms step_avg:92.32ms
step:1034/1645 train_time:95457ms step_avg:92.32ms
step:1035/1645 train_time:95550ms step_avg:92.32ms
step:1036/1645 train_time:95643ms step_avg:92.32ms
step:1037/1645 train_time:95737ms step_avg:92.32ms
step:1038/1645 train_time:95830ms step_avg:92.32ms
step:1039/1645 train_time:95922ms step_avg:92.32ms
step:1040/1645 train_time:96014ms step_avg:92.32ms
step:1041/1645 train_time:96106ms step_avg:92.32ms
step:1042/1645 train_time:96200ms step_avg:92.32ms
step:1043/1645 train_time:96294ms step_avg:92.32ms
step:1044/1645 train_time:96386ms step_avg:92.32ms
step:1045/1645 train_time:96479ms step_avg:92.32ms
step:1046/1645 train_time:96572ms step_avg:92.32ms
step:1047/1645 train_time:96665ms step_avg:92.33ms
step:1048/1645 train_time:96758ms step_avg:92.33ms
step:1049/1645 train_time:96850ms step_avg:92.33ms
step:1050/1645 train_time:96944ms step_avg:92.33ms
step:1051/1645 train_time:97037ms step_avg:92.33ms
step:1052/1645 train_time:97129ms step_avg:92.33ms
step:1053/1645 train_time:97221ms step_avg:92.33ms
step:1054/1645 train_time:97314ms step_avg:92.33ms
step:1055/1645 train_time:97407ms step_avg:92.33ms
step:1056/1645 train_time:97501ms step_avg:92.33ms
step:1057/1645 train_time:97594ms step_avg:92.33ms
step:1058/1645 train_time:97688ms step_avg:92.33ms
step:1059/1645 train_time:97781ms step_avg:92.33ms
step:1060/1645 train_time:97873ms step_avg:92.33ms
step:1061/1645 train_time:97967ms step_avg:92.33ms
step:1062/1645 train_time:98060ms step_avg:92.34ms
step:1063/1645 train_time:98152ms step_avg:92.34ms
step:1064/1645 train_time:98245ms step_avg:92.34ms
step:1065/1645 train_time:98339ms step_avg:92.34ms
step:1066/1645 train_time:98432ms step_avg:92.34ms
step:1067/1645 train_time:98525ms step_avg:92.34ms
step:1068/1645 train_time:98618ms step_avg:92.34ms
step:1069/1645 train_time:98711ms step_avg:92.34ms
step:1070/1645 train_time:98805ms step_avg:92.34ms
step:1071/1645 train_time:98897ms step_avg:92.34ms
step:1072/1645 train_time:98990ms step_avg:92.34ms
step:1073/1645 train_time:99082ms step_avg:92.34ms
step:1074/1645 train_time:99175ms step_avg:92.34ms
step:1075/1645 train_time:99268ms step_avg:92.34ms
step:1076/1645 train_time:99361ms step_avg:92.34ms
step:1077/1645 train_time:99454ms step_avg:92.34ms
step:1078/1645 train_time:99547ms step_avg:92.34ms
step:1079/1645 train_time:99640ms step_avg:92.34ms
step:1080/1645 train_time:99733ms step_avg:92.35ms
step:1081/1645 train_time:99826ms step_avg:92.35ms
step:1082/1645 train_time:99920ms step_avg:92.35ms
step:1083/1645 train_time:100013ms step_avg:92.35ms
step:1084/1645 train_time:100105ms step_avg:92.35ms
step:1085/1645 train_time:100197ms step_avg:92.35ms
step:1086/1645 train_time:100290ms step_avg:92.35ms
step:1087/1645 train_time:100383ms step_avg:92.35ms
step:1088/1645 train_time:100476ms step_avg:92.35ms
step:1089/1645 train_time:100569ms step_avg:92.35ms
step:1090/1645 train_time:100662ms step_avg:92.35ms
step:1091/1645 train_time:100755ms step_avg:92.35ms
step:1092/1645 train_time:100848ms step_avg:92.35ms
step:1093/1645 train_time:100942ms step_avg:92.35ms
step:1094/1645 train_time:101035ms step_avg:92.35ms
step:1095/1645 train_time:101127ms step_avg:92.35ms
step:1096/1645 train_time:101219ms step_avg:92.35ms
step:1097/1645 train_time:101313ms step_avg:92.35ms
step:1098/1645 train_time:101405ms step_avg:92.35ms
step:1099/1645 train_time:101499ms step_avg:92.36ms
step:1100/1645 train_time:101592ms step_avg:92.36ms
step:1101/1645 train_time:101686ms step_avg:92.36ms
step:1102/1645 train_time:101779ms step_avg:92.36ms
step:1103/1645 train_time:101873ms step_avg:92.36ms
step:1104/1645 train_time:101968ms step_avg:92.36ms
step:1105/1645 train_time:102061ms step_avg:92.36ms
step:1106/1645 train_time:102154ms step_avg:92.36ms
step:1107/1645 train_time:102247ms step_avg:92.36ms
step:1108/1645 train_time:102340ms step_avg:92.37ms
step:1109/1645 train_time:102434ms step_avg:92.37ms
step:1110/1645 train_time:102527ms step_avg:92.37ms
step:1111/1645 train_time:102622ms step_avg:92.37ms
step:1112/1645 train_time:102715ms step_avg:92.37ms
step:1113/1645 train_time:102808ms step_avg:92.37ms
step:1114/1645 train_time:102901ms step_avg:92.37ms
step:1115/1645 train_time:102995ms step_avg:92.37ms
step:1116/1645 train_time:103088ms step_avg:92.37ms
step:1117/1645 train_time:103182ms step_avg:92.37ms
step:1118/1645 train_time:103275ms step_avg:92.37ms
step:1119/1645 train_time:103369ms step_avg:92.38ms
step:1120/1645 train_time:103463ms step_avg:92.38ms
step:1121/1645 train_time:103556ms step_avg:92.38ms
step:1122/1645 train_time:103649ms step_avg:92.38ms
step:1123/1645 train_time:103745ms step_avg:92.38ms
step:1124/1645 train_time:103838ms step_avg:92.38ms
step:1125/1645 train_time:103932ms step_avg:92.38ms
step:1125/1645 val_loss:3.4117 train_time:104025ms step_avg:92.47ms
step:1126/1645 train_time:104046ms step_avg:92.40ms
step:1127/1645 train_time:104126ms step_avg:92.39ms
step:1128/1645 train_time:104226ms step_avg:92.40ms
step:1129/1645 train_time:104320ms step_avg:92.40ms
step:1130/1645 train_time:104413ms step_avg:92.40ms
step:1131/1645 train_time:104505ms step_avg:92.40ms
step:1132/1645 train_time:104598ms step_avg:92.40ms
step:1133/1645 train_time:104690ms step_avg:92.40ms
step:1134/1645 train_time:104783ms step_avg:92.40ms
step:1135/1645 train_time:104875ms step_avg:92.40ms
step:1136/1645 train_time:104971ms step_avg:92.40ms
step:1137/1645 train_time:105066ms step_avg:92.41ms
step:1138/1645 train_time:105161ms step_avg:92.41ms
step:1139/1645 train_time:105256ms step_avg:92.41ms
step:1140/1645 train_time:105350ms step_avg:92.41ms
step:1141/1645 train_time:105443ms step_avg:92.41ms
step:1142/1645 train_time:105535ms step_avg:92.41ms
step:1143/1645 train_time:105627ms step_avg:92.41ms
step:1144/1645 train_time:105720ms step_avg:92.41ms
step:1145/1645 train_time:105813ms step_avg:92.41ms
step:1146/1645 train_time:105907ms step_avg:92.41ms
step:1147/1645 train_time:106000ms step_avg:92.41ms
step:1148/1645 train_time:106097ms step_avg:92.42ms
step:1149/1645 train_time:106192ms step_avg:92.42ms
step:1150/1645 train_time:106288ms step_avg:92.42ms
step:1151/1645 train_time:106381ms step_avg:92.42ms
step:1152/1645 train_time:106473ms step_avg:92.42ms
step:1153/1645 train_time:106566ms step_avg:92.42ms
step:1154/1645 train_time:106659ms step_avg:92.43ms
step:1155/1645 train_time:106752ms step_avg:92.43ms
step:1156/1645 train_time:106845ms step_avg:92.43ms
step:1157/1645 train_time:106938ms step_avg:92.43ms
step:1158/1645 train_time:107033ms step_avg:92.43ms
step:1159/1645 train_time:107128ms step_avg:92.43ms
step:1160/1645 train_time:107221ms step_avg:92.43ms
step:1161/1645 train_time:107315ms step_avg:92.43ms
step:1162/1645 train_time:107409ms step_avg:92.43ms
step:1163/1645 train_time:107503ms step_avg:92.44ms
step:1164/1645 train_time:107597ms step_avg:92.44ms
step:1165/1645 train_time:107688ms step_avg:92.44ms
step:1166/1645 train_time:107781ms step_avg:92.44ms
step:1167/1645 train_time:107873ms step_avg:92.44ms
step:1168/1645 train_time:107967ms step_avg:92.44ms
step:1169/1645 train_time:108060ms step_avg:92.44ms
step:1170/1645 train_time:108155ms step_avg:92.44ms
step:1171/1645 train_time:108250ms step_avg:92.44ms
step:1172/1645 train_time:108343ms step_avg:92.44ms
step:1173/1645 train_time:108436ms step_avg:92.44ms
step:1174/1645 train_time:108530ms step_avg:92.44ms
step:1175/1645 train_time:108624ms step_avg:92.45ms
step:1176/1645 train_time:108718ms step_avg:92.45ms
step:1177/1645 train_time:108811ms step_avg:92.45ms
step:1178/1645 train_time:108905ms step_avg:92.45ms
step:1179/1645 train_time:108998ms step_avg:92.45ms
step:1180/1645 train_time:109092ms step_avg:92.45ms
step:1181/1645 train_time:109186ms step_avg:92.45ms
step:1182/1645 train_time:109280ms step_avg:92.45ms
step:1183/1645 train_time:109375ms step_avg:92.46ms
step:1184/1645 train_time:109468ms step_avg:92.46ms
step:1185/1645 train_time:109561ms step_avg:92.46ms
step:1186/1645 train_time:109656ms step_avg:92.46ms
step:1187/1645 train_time:109749ms step_avg:92.46ms
step:1188/1645 train_time:109842ms step_avg:92.46ms
step:1189/1645 train_time:109935ms step_avg:92.46ms
step:1190/1645 train_time:110029ms step_avg:92.46ms
step:1191/1645 train_time:110122ms step_avg:92.46ms
step:1192/1645 train_time:110217ms step_avg:92.46ms
step:1193/1645 train_time:110314ms step_avg:92.47ms
step:1194/1645 train_time:110405ms step_avg:92.47ms
step:1195/1645 train_time:110499ms step_avg:92.47ms
step:1196/1645 train_time:110592ms step_avg:92.47ms
step:1197/1645 train_time:110685ms step_avg:92.47ms
step:1198/1645 train_time:110779ms step_avg:92.47ms
step:1199/1645 train_time:110872ms step_avg:92.47ms
step:1200/1645 train_time:110965ms step_avg:92.47ms
step:1201/1645 train_time:111059ms step_avg:92.47ms
step:1202/1645 train_time:111154ms step_avg:92.47ms
step:1203/1645 train_time:111247ms step_avg:92.47ms
step:1204/1645 train_time:111340ms step_avg:92.48ms
step:1205/1645 train_time:111433ms step_avg:92.48ms
step:1206/1645 train_time:111527ms step_avg:92.48ms
step:1207/1645 train_time:111621ms step_avg:92.48ms
step:1208/1645 train_time:111715ms step_avg:92.48ms
step:1209/1645 train_time:111807ms step_avg:92.48ms
step:1210/1645 train_time:111901ms step_avg:92.48ms
step:1211/1645 train_time:111994ms step_avg:92.48ms
step:1212/1645 train_time:112088ms step_avg:92.48ms
step:1213/1645 train_time:112181ms step_avg:92.48ms
step:1214/1645 train_time:112276ms step_avg:92.48ms
step:1215/1645 train_time:112368ms step_avg:92.48ms
step:1216/1645 train_time:112463ms step_avg:92.49ms
step:1217/1645 train_time:112556ms step_avg:92.49ms
step:1218/1645 train_time:112650ms step_avg:92.49ms
step:1219/1645 train_time:112744ms step_avg:92.49ms
step:1220/1645 train_time:112836ms step_avg:92.49ms
step:1221/1645 train_time:112930ms step_avg:92.49ms
step:1222/1645 train_time:113025ms step_avg:92.49ms
step:1223/1645 train_time:113117ms step_avg:92.49ms
step:1224/1645 train_time:113211ms step_avg:92.49ms
step:1225/1645 train_time:113304ms step_avg:92.49ms
step:1226/1645 train_time:113398ms step_avg:92.49ms
step:1227/1645 train_time:113492ms step_avg:92.50ms
step:1228/1645 train_time:113586ms step_avg:92.50ms
step:1229/1645 train_time:113679ms step_avg:92.50ms
step:1230/1645 train_time:113772ms step_avg:92.50ms
step:1231/1645 train_time:113865ms step_avg:92.50ms
step:1232/1645 train_time:113959ms step_avg:92.50ms
step:1233/1645 train_time:114052ms step_avg:92.50ms
step:1234/1645 train_time:114146ms step_avg:92.50ms
step:1235/1645 train_time:114240ms step_avg:92.50ms
step:1236/1645 train_time:114335ms step_avg:92.50ms
step:1237/1645 train_time:114430ms step_avg:92.51ms
step:1238/1645 train_time:114522ms step_avg:92.51ms
step:1239/1645 train_time:114615ms step_avg:92.51ms
step:1240/1645 train_time:114708ms step_avg:92.51ms
step:1241/1645 train_time:114802ms step_avg:92.51ms
step:1242/1645 train_time:114896ms step_avg:92.51ms
step:1243/1645 train_time:114990ms step_avg:92.51ms
step:1244/1645 train_time:115084ms step_avg:92.51ms
step:1245/1645 train_time:115177ms step_avg:92.51ms
step:1246/1645 train_time:115271ms step_avg:92.51ms
step:1247/1645 train_time:115365ms step_avg:92.51ms
step:1248/1645 train_time:115458ms step_avg:92.51ms
step:1249/1645 train_time:115552ms step_avg:92.52ms
step:1250/1645 train_time:115646ms step_avg:92.52ms
step:1250/1645 val_loss:3.3735 train_time:115739ms step_avg:92.59ms
step:1251/1645 train_time:115760ms step_avg:92.53ms
step:1252/1645 train_time:115838ms step_avg:92.52ms
step:1253/1645 train_time:115933ms step_avg:92.52ms
step:1254/1645 train_time:116028ms step_avg:92.53ms
step:1255/1645 train_time:116120ms step_avg:92.53ms
step:1256/1645 train_time:116213ms step_avg:92.53ms
step:1257/1645 train_time:116305ms step_avg:92.53ms
step:1258/1645 train_time:116398ms step_avg:92.53ms
step:1259/1645 train_time:116491ms step_avg:92.53ms
step:1260/1645 train_time:116584ms step_avg:92.53ms
step:1261/1645 train_time:116679ms step_avg:92.53ms
step:1262/1645 train_time:116774ms step_avg:92.53ms
step:1263/1645 train_time:116869ms step_avg:92.53ms
step:1264/1645 train_time:116964ms step_avg:92.53ms
step:1265/1645 train_time:117057ms step_avg:92.54ms
step:1266/1645 train_time:117151ms step_avg:92.54ms
step:1267/1645 train_time:117243ms step_avg:92.54ms
step:1268/1645 train_time:117336ms step_avg:92.54ms
step:1269/1645 train_time:117430ms step_avg:92.54ms
step:1270/1645 train_time:117523ms step_avg:92.54ms
step:1271/1645 train_time:117616ms step_avg:92.54ms
step:1272/1645 train_time:117710ms step_avg:92.54ms
step:1273/1645 train_time:117805ms step_avg:92.54ms
step:1274/1645 train_time:117900ms step_avg:92.54ms
step:1275/1645 train_time:117994ms step_avg:92.54ms
step:1276/1645 train_time:118088ms step_avg:92.55ms
step:1277/1645 train_time:118183ms step_avg:92.55ms
step:1278/1645 train_time:118275ms step_avg:92.55ms
step:1279/1645 train_time:118368ms step_avg:92.55ms
step:1280/1645 train_time:118462ms step_avg:92.55ms
step:1281/1645 train_time:118554ms step_avg:92.55ms
step:1282/1645 train_time:118648ms step_avg:92.55ms
step:1283/1645 train_time:118742ms step_avg:92.55ms
step:1284/1645 train_time:118836ms step_avg:92.55ms
step:1285/1645 train_time:118930ms step_avg:92.55ms
step:1286/1645 train_time:119024ms step_avg:92.55ms
step:1287/1645 train_time:119118ms step_avg:92.55ms
step:1288/1645 train_time:119211ms step_avg:92.56ms
step:1289/1645 train_time:119305ms step_avg:92.56ms
step:1290/1645 train_time:119398ms step_avg:92.56ms
step:1291/1645 train_time:119493ms step_avg:92.56ms
step:1292/1645 train_time:119585ms step_avg:92.56ms
step:1293/1645 train_time:119678ms step_avg:92.56ms
step:1294/1645 train_time:119772ms step_avg:92.56ms
step:1295/1645 train_time:119867ms step_avg:92.56ms
step:1296/1645 train_time:119961ms step_avg:92.56ms
step:1297/1645 train_time:120055ms step_avg:92.56ms
step:1298/1645 train_time:120150ms step_avg:92.57ms
step:1299/1645 train_time:120243ms step_avg:92.57ms
step:1300/1645 train_time:120337ms step_avg:92.57ms
step:1301/1645 train_time:120430ms step_avg:92.57ms
step:1302/1645 train_time:120525ms step_avg:92.57ms
step:1303/1645 train_time:120618ms step_avg:92.57ms
step:1304/1645 train_time:120712ms step_avg:92.57ms
step:1305/1645 train_time:120806ms step_avg:92.57ms
step:1306/1645 train_time:120900ms step_avg:92.57ms
step:1307/1645 train_time:120995ms step_avg:92.57ms
step:1308/1645 train_time:121088ms step_avg:92.58ms
step:1309/1645 train_time:121181ms step_avg:92.58ms
step:1310/1645 train_time:121274ms step_avg:92.58ms
step:1311/1645 train_time:121367ms step_avg:92.58ms
step:1312/1645 train_time:121461ms step_avg:92.58ms
step:1313/1645 train_time:121555ms step_avg:92.58ms
step:1314/1645 train_time:121648ms step_avg:92.58ms
step:1315/1645 train_time:121741ms step_avg:92.58ms
step:1316/1645 train_time:121835ms step_avg:92.58ms
step:1317/1645 train_time:121929ms step_avg:92.58ms
step:1318/1645 train_time:122023ms step_avg:92.58ms
step:1319/1645 train_time:122117ms step_avg:92.58ms
step:1320/1645 train_time:122210ms step_avg:92.58ms
step:1321/1645 train_time:122305ms step_avg:92.59ms
step:1322/1645 train_time:122398ms step_avg:92.59ms
step:1323/1645 train_time:122493ms step_avg:92.59ms
step:1324/1645 train_time:122587ms step_avg:92.59ms
step:1325/1645 train_time:122681ms step_avg:92.59ms
step:1326/1645 train_time:122774ms step_avg:92.59ms
step:1327/1645 train_time:122868ms step_avg:92.59ms
step:1328/1645 train_time:122962ms step_avg:92.59ms
step:1329/1645 train_time:123056ms step_avg:92.59ms
step:1330/1645 train_time:123149ms step_avg:92.59ms
step:1331/1645 train_time:123243ms step_avg:92.59ms
step:1332/1645 train_time:123336ms step_avg:92.59ms
step:1333/1645 train_time:123430ms step_avg:92.60ms
step:1334/1645 train_time:123524ms step_avg:92.60ms
step:1335/1645 train_time:123618ms step_avg:92.60ms
step:1336/1645 train_time:123712ms step_avg:92.60ms
step:1337/1645 train_time:123805ms step_avg:92.60ms
step:1338/1645 train_time:123899ms step_avg:92.60ms
step:1339/1645 train_time:123993ms step_avg:92.60ms
step:1340/1645 train_time:124086ms step_avg:92.60ms
step:1341/1645 train_time:124180ms step_avg:92.60ms
step:1342/1645 train_time:124273ms step_avg:92.60ms
step:1343/1645 train_time:124366ms step_avg:92.60ms
step:1344/1645 train_time:124460ms step_avg:92.60ms
step:1345/1645 train_time:124554ms step_avg:92.60ms
step:1346/1645 train_time:124649ms step_avg:92.61ms
step:1347/1645 train_time:124742ms step_avg:92.61ms
step:1348/1645 train_time:124836ms step_avg:92.61ms
step:1349/1645 train_time:124930ms step_avg:92.61ms
step:1350/1645 train_time:125023ms step_avg:92.61ms
step:1351/1645 train_time:125116ms step_avg:92.61ms
step:1352/1645 train_time:125211ms step_avg:92.61ms
step:1353/1645 train_time:125304ms step_avg:92.61ms
step:1354/1645 train_time:125397ms step_avg:92.61ms
step:1355/1645 train_time:125491ms step_avg:92.61ms
step:1356/1645 train_time:125584ms step_avg:92.61ms
step:1357/1645 train_time:125678ms step_avg:92.61ms
step:1358/1645 train_time:125771ms step_avg:92.61ms
step:1359/1645 train_time:125864ms step_avg:92.62ms
step:1360/1645 train_time:125957ms step_avg:92.62ms
step:1361/1645 train_time:126051ms step_avg:92.62ms
step:1362/1645 train_time:126144ms step_avg:92.62ms
step:1363/1645 train_time:126238ms step_avg:92.62ms
step:1364/1645 train_time:126332ms step_avg:92.62ms
step:1365/1645 train_time:126426ms step_avg:92.62ms
step:1366/1645 train_time:126521ms step_avg:92.62ms
step:1367/1645 train_time:126614ms step_avg:92.62ms
step:1368/1645 train_time:126708ms step_avg:92.62ms
step:1369/1645 train_time:126802ms step_avg:92.62ms
step:1370/1645 train_time:126895ms step_avg:92.62ms
step:1371/1645 train_time:126989ms step_avg:92.63ms
step:1372/1645 train_time:127083ms step_avg:92.63ms
step:1373/1645 train_time:127177ms step_avg:92.63ms
step:1374/1645 train_time:127269ms step_avg:92.63ms
step:1375/1645 train_time:127364ms step_avg:92.63ms
step:1375/1645 val_loss:3.3383 train_time:127457ms step_avg:92.70ms
step:1376/1645 train_time:127478ms step_avg:92.64ms
step:1377/1645 train_time:127554ms step_avg:92.63ms
step:1378/1645 train_time:127650ms step_avg:92.63ms
step:1379/1645 train_time:127744ms step_avg:92.64ms
step:1380/1645 train_time:127837ms step_avg:92.64ms
step:1381/1645 train_time:127930ms step_avg:92.64ms
step:1382/1645 train_time:128023ms step_avg:92.64ms
step:1383/1645 train_time:128116ms step_avg:92.64ms
step:1384/1645 train_time:128209ms step_avg:92.64ms
step:1385/1645 train_time:128301ms step_avg:92.64ms
step:1386/1645 train_time:128395ms step_avg:92.64ms
step:1387/1645 train_time:128491ms step_avg:92.64ms
step:1388/1645 train_time:128586ms step_avg:92.64ms
step:1389/1645 train_time:128680ms step_avg:92.64ms
step:1390/1645 train_time:128774ms step_avg:92.64ms
step:1391/1645 train_time:128868ms step_avg:92.64ms
step:1392/1645 train_time:128961ms step_avg:92.64ms
step:1393/1645 train_time:129055ms step_avg:92.65ms
step:1394/1645 train_time:129148ms step_avg:92.65ms
step:1395/1645 train_time:129240ms step_avg:92.65ms
step:1396/1645 train_time:129333ms step_avg:92.65ms
step:1397/1645 train_time:129427ms step_avg:92.65ms
step:1398/1645 train_time:129522ms step_avg:92.65ms
step:1399/1645 train_time:129616ms step_avg:92.65ms
step:1400/1645 train_time:129711ms step_avg:92.65ms
step:1401/1645 train_time:129804ms step_avg:92.65ms
step:1402/1645 train_time:129898ms step_avg:92.65ms
step:1403/1645 train_time:129991ms step_avg:92.65ms
step:1404/1645 train_time:130085ms step_avg:92.65ms
step:1405/1645 train_time:130178ms step_avg:92.65ms
step:1406/1645 train_time:130271ms step_avg:92.65ms
step:1407/1645 train_time:130364ms step_avg:92.65ms
step:1408/1645 train_time:130460ms step_avg:92.66ms
step:1409/1645 train_time:130554ms step_avg:92.66ms
step:1410/1645 train_time:130648ms step_avg:92.66ms
step:1411/1645 train_time:130742ms step_avg:92.66ms
step:1412/1645 train_time:130836ms step_avg:92.66ms
step:1413/1645 train_time:130930ms step_avg:92.66ms
step:1414/1645 train_time:131024ms step_avg:92.66ms
step:1415/1645 train_time:131116ms step_avg:92.66ms
step:1416/1645 train_time:131209ms step_avg:92.66ms
step:1417/1645 train_time:131302ms step_avg:92.66ms
step:1418/1645 train_time:131395ms step_avg:92.66ms
step:1419/1645 train_time:131489ms step_avg:92.66ms
step:1420/1645 train_time:131583ms step_avg:92.66ms
step:1421/1645 train_time:131678ms step_avg:92.67ms
step:1422/1645 train_time:131771ms step_avg:92.67ms
step:1423/1645 train_time:131864ms step_avg:92.67ms
step:1424/1645 train_time:131959ms step_avg:92.67ms
step:1425/1645 train_time:132052ms step_avg:92.67ms
step:1426/1645 train_time:132145ms step_avg:92.67ms
step:1427/1645 train_time:132239ms step_avg:92.67ms
step:1428/1645 train_time:132332ms step_avg:92.67ms
step:1429/1645 train_time:132426ms step_avg:92.67ms
step:1430/1645 train_time:132520ms step_avg:92.67ms
step:1431/1645 train_time:132614ms step_avg:92.67ms
step:1432/1645 train_time:132708ms step_avg:92.67ms
step:1433/1645 train_time:132802ms step_avg:92.67ms
step:1434/1645 train_time:132895ms step_avg:92.67ms
step:1435/1645 train_time:132989ms step_avg:92.68ms
step:1436/1645 train_time:133082ms step_avg:92.68ms
step:1437/1645 train_time:133176ms step_avg:92.68ms
step:1438/1645 train_time:133268ms step_avg:92.68ms
step:1439/1645 train_time:133362ms step_avg:92.68ms
step:1440/1645 train_time:133456ms step_avg:92.68ms
step:1441/1645 train_time:133549ms step_avg:92.68ms
step:1442/1645 train_time:133643ms step_avg:92.68ms
step:1443/1645 train_time:133736ms step_avg:92.68ms
step:1444/1645 train_time:133831ms step_avg:92.68ms
step:1445/1645 train_time:133925ms step_avg:92.68ms
step:1446/1645 train_time:134019ms step_avg:92.68ms
step:1447/1645 train_time:134112ms step_avg:92.68ms
step:1448/1645 train_time:134205ms step_avg:92.68ms
step:1449/1645 train_time:134299ms step_avg:92.68ms
step:1450/1645 train_time:134393ms step_avg:92.69ms
step:1451/1645 train_time:134486ms step_avg:92.69ms
step:1452/1645 train_time:134580ms step_avg:92.69ms
step:1453/1645 train_time:134674ms step_avg:92.69ms
step:1454/1645 train_time:134767ms step_avg:92.69ms
step:1455/1645 train_time:134863ms step_avg:92.69ms
step:1456/1645 train_time:134957ms step_avg:92.69ms
step:1457/1645 train_time:135050ms step_avg:92.69ms
step:1458/1645 train_time:135143ms step_avg:92.69ms
step:1459/1645 train_time:135237ms step_avg:92.69ms
step:1460/1645 train_time:135330ms step_avg:92.69ms
step:1461/1645 train_time:135424ms step_avg:92.69ms
step:1462/1645 train_time:135519ms step_avg:92.69ms
step:1463/1645 train_time:135612ms step_avg:92.69ms
step:1464/1645 train_time:135706ms step_avg:92.70ms
step:1465/1645 train_time:135800ms step_avg:92.70ms
step:1466/1645 train_time:135894ms step_avg:92.70ms
step:1467/1645 train_time:135988ms step_avg:92.70ms
step:1468/1645 train_time:136081ms step_avg:92.70ms
step:1469/1645 train_time:136176ms step_avg:92.70ms
step:1470/1645 train_time:136269ms step_avg:92.70ms
step:1471/1645 train_time:136362ms step_avg:92.70ms
step:1472/1645 train_time:136455ms step_avg:92.70ms
step:1473/1645 train_time:136549ms step_avg:92.70ms
step:1474/1645 train_time:136643ms step_avg:92.70ms
step:1475/1645 train_time:136738ms step_avg:92.70ms
step:1476/1645 train_time:136832ms step_avg:92.70ms
step:1477/1645 train_time:136925ms step_avg:92.71ms
step:1478/1645 train_time:137018ms step_avg:92.71ms
step:1479/1645 train_time:137111ms step_avg:92.71ms
step:1480/1645 train_time:137206ms step_avg:92.71ms
step:1481/1645 train_time:137298ms step_avg:92.71ms
step:1482/1645 train_time:137391ms step_avg:92.71ms
step:1483/1645 train_time:137485ms step_avg:92.71ms
step:1484/1645 train_time:137579ms step_avg:92.71ms
step:1485/1645 train_time:137672ms step_avg:92.71ms
step:1486/1645 train_time:137766ms step_avg:92.71ms
step:1487/1645 train_time:137860ms step_avg:92.71ms
step:1488/1645 train_time:137954ms step_avg:92.71ms
step:1489/1645 train_time:138047ms step_avg:92.71ms
step:1490/1645 train_time:138141ms step_avg:92.71ms
step:1491/1645 train_time:138235ms step_avg:92.71ms
step:1492/1645 train_time:138328ms step_avg:92.71ms
step:1493/1645 train_time:138422ms step_avg:92.71ms
step:1494/1645 train_time:138516ms step_avg:92.71ms
step:1495/1645 train_time:138610ms step_avg:92.72ms
step:1496/1645 train_time:138702ms step_avg:92.72ms
step:1497/1645 train_time:138796ms step_avg:92.72ms
step:1498/1645 train_time:138890ms step_avg:92.72ms
step:1499/1645 train_time:138984ms step_avg:92.72ms
step:1500/1645 train_time:139077ms step_avg:92.72ms
step:1500/1645 val_loss:3.3084 train_time:139171ms step_avg:92.78ms
step:1501/1645 train_time:139192ms step_avg:92.73ms
step:1502/1645 train_time:139269ms step_avg:92.72ms
step:1503/1645 train_time:139365ms step_avg:92.72ms
step:1504/1645 train_time:139457ms step_avg:92.72ms
step:1505/1645 train_time:139550ms step_avg:92.72ms
step:1506/1645 train_time:139643ms step_avg:92.72ms
step:1507/1645 train_time:139736ms step_avg:92.72ms
step:1508/1645 train_time:139829ms step_avg:92.72ms
step:1509/1645 train_time:139921ms step_avg:92.72ms
step:1510/1645 train_time:140014ms step_avg:92.72ms
step:1511/1645 train_time:140108ms step_avg:92.73ms
step:1512/1645 train_time:140203ms step_avg:92.73ms
step:1513/1645 train_time:140298ms step_avg:92.73ms
step:1514/1645 train_time:140393ms step_avg:92.73ms
step:1515/1645 train_time:140486ms step_avg:92.73ms
step:1516/1645 train_time:140579ms step_avg:92.73ms
step:1517/1645 train_time:140672ms step_avg:92.73ms
step:1518/1645 train_time:140764ms step_avg:92.73ms
step:1519/1645 train_time:140858ms step_avg:92.73ms
step:1520/1645 train_time:140950ms step_avg:92.73ms
step:1521/1645 train_time:141043ms step_avg:92.73ms
step:1522/1645 train_time:141138ms step_avg:92.73ms
step:1523/1645 train_time:141232ms step_avg:92.73ms
step:1524/1645 train_time:141326ms step_avg:92.73ms
step:1525/1645 train_time:141420ms step_avg:92.73ms
step:1526/1645 train_time:141513ms step_avg:92.73ms
step:1527/1645 train_time:141607ms step_avg:92.74ms
step:1528/1645 train_time:141700ms step_avg:92.74ms
step:1529/1645 train_time:141793ms step_avg:92.74ms
step:1530/1645 train_time:141886ms step_avg:92.74ms
step:1531/1645 train_time:141979ms step_avg:92.74ms
step:1532/1645 train_time:142072ms step_avg:92.74ms
step:1533/1645 train_time:142166ms step_avg:92.74ms
step:1534/1645 train_time:142261ms step_avg:92.74ms
step:1535/1645 train_time:142354ms step_avg:92.74ms
step:1536/1645 train_time:142448ms step_avg:92.74ms
step:1537/1645 train_time:142541ms step_avg:92.74ms
step:1538/1645 train_time:142635ms step_avg:92.74ms
step:1539/1645 train_time:142728ms step_avg:92.74ms
step:1540/1645 train_time:142821ms step_avg:92.74ms
step:1541/1645 train_time:142915ms step_avg:92.74ms
step:1542/1645 train_time:143008ms step_avg:92.74ms
step:1543/1645 train_time:143102ms step_avg:92.74ms
step:1544/1645 train_time:143195ms step_avg:92.74ms
step:1545/1645 train_time:143290ms step_avg:92.74ms
step:1546/1645 train_time:143384ms step_avg:92.74ms
step:1547/1645 train_time:143478ms step_avg:92.75ms
step:1548/1645 train_time:143572ms step_avg:92.75ms
step:1549/1645 train_time:143666ms step_avg:92.75ms
step:1550/1645 train_time:143759ms step_avg:92.75ms
step:1551/1645 train_time:143852ms step_avg:92.75ms
step:1552/1645 train_time:143946ms step_avg:92.75ms
step:1553/1645 train_time:144038ms step_avg:92.75ms
step:1554/1645 train_time:144132ms step_avg:92.75ms
step:1555/1645 train_time:144226ms step_avg:92.75ms
step:1556/1645 train_time:144319ms step_avg:92.75ms
step:1557/1645 train_time:144412ms step_avg:92.75ms
step:1558/1645 train_time:144507ms step_avg:92.75ms
step:1559/1645 train_time:144600ms step_avg:92.75ms
step:1560/1645 train_time:144693ms step_avg:92.75ms
step:1561/1645 train_time:144788ms step_avg:92.75ms
step:1562/1645 train_time:144882ms step_avg:92.75ms
step:1563/1645 train_time:144975ms step_avg:92.75ms
step:1564/1645 train_time:145068ms step_avg:92.75ms
step:1565/1645 train_time:145162ms step_avg:92.76ms
step:1566/1645 train_time:145256ms step_avg:92.76ms
step:1567/1645 train_time:145350ms step_avg:92.76ms
step:1568/1645 train_time:145443ms step_avg:92.76ms
step:1569/1645 train_time:145536ms step_avg:92.76ms
step:1570/1645 train_time:145630ms step_avg:92.76ms
step:1571/1645 train_time:145723ms step_avg:92.76ms
step:1572/1645 train_time:145817ms step_avg:92.76ms
step:1573/1645 train_time:145911ms step_avg:92.76ms
step:1574/1645 train_time:146005ms step_avg:92.76ms
step:1575/1645 train_time:146097ms step_avg:92.76ms
step:1576/1645 train_time:146192ms step_avg:92.76ms
step:1577/1645 train_time:146286ms step_avg:92.76ms
step:1578/1645 train_time:146381ms step_avg:92.76ms
step:1579/1645 train_time:146474ms step_avg:92.76ms
step:1580/1645 train_time:146568ms step_avg:92.76ms
step:1581/1645 train_time:146662ms step_avg:92.77ms
step:1582/1645 train_time:146756ms step_avg:92.77ms
step:1583/1645 train_time:146849ms step_avg:92.77ms
step:1584/1645 train_time:146943ms step_avg:92.77ms
step:1585/1645 train_time:147035ms step_avg:92.77ms
step:1586/1645 train_time:147129ms step_avg:92.77ms
step:1587/1645 train_time:147222ms step_avg:92.77ms
step:1588/1645 train_time:147315ms step_avg:92.77ms
step:1589/1645 train_time:147409ms step_avg:92.77ms
step:1590/1645 train_time:147503ms step_avg:92.77ms
step:1591/1645 train_time:147597ms step_avg:92.77ms
step:1592/1645 train_time:147692ms step_avg:92.77ms
step:1593/1645 train_time:147786ms step_avg:92.77ms
step:1594/1645 train_time:147879ms step_avg:92.77ms
step:1595/1645 train_time:147972ms step_avg:92.77ms
step:1596/1645 train_time:148066ms step_avg:92.77ms
step:1597/1645 train_time:148159ms step_avg:92.77ms
step:1598/1645 train_time:148253ms step_avg:92.77ms
step:1599/1645 train_time:148346ms step_avg:92.77ms
step:1600/1645 train_time:148439ms step_avg:92.77ms
step:1601/1645 train_time:148532ms step_avg:92.77ms
step:1602/1645 train_time:148625ms step_avg:92.77ms
step:1603/1645 train_time:148719ms step_avg:92.78ms
step:1604/1645 train_time:148812ms step_avg:92.78ms
step:1605/1645 train_time:148905ms step_avg:92.78ms
step:1606/1645 train_time:148999ms step_avg:92.78ms
step:1607/1645 train_time:149094ms step_avg:92.78ms
step:1608/1645 train_time:149186ms step_avg:92.78ms
step:1609/1645 train_time:149280ms step_avg:92.78ms
step:1610/1645 train_time:149374ms step_avg:92.78ms
step:1611/1645 train_time:149468ms step_avg:92.78ms
step:1612/1645 train_time:149562ms step_avg:92.78ms
step:1613/1645 train_time:149655ms step_avg:92.78ms
step:1614/1645 train_time:149749ms step_avg:92.78ms
step:1615/1645 train_time:149842ms step_avg:92.78ms
step:1616/1645 train_time:149935ms step_avg:92.78ms
step:1617/1645 train_time:150029ms step_avg:92.78ms
step:1618/1645 train_time:150123ms step_avg:92.78ms
step:1619/1645 train_time:150217ms step_avg:92.78ms
step:1620/1645 train_time:150311ms step_avg:92.78ms
step:1621/1645 train_time:150405ms step_avg:92.79ms
step:1622/1645 train_time:150499ms step_avg:92.79ms
step:1623/1645 train_time:150593ms step_avg:92.79ms
step:1624/1645 train_time:150686ms step_avg:92.79ms
step:1625/1645 train_time:150780ms step_avg:92.79ms
step:1625/1645 val_loss:3.2846 train_time:150873ms step_avg:92.84ms
step:1626/1645 train_time:150893ms step_avg:92.80ms
step:1627/1645 train_time:150971ms step_avg:92.79ms
step:1628/1645 train_time:151066ms step_avg:92.79ms
step:1629/1645 train_time:151159ms step_avg:92.79ms
step:1630/1645 train_time:151251ms step_avg:92.79ms
step:1631/1645 train_time:151344ms step_avg:92.79ms
step:1632/1645 train_time:151437ms step_avg:92.79ms
step:1633/1645 train_time:151530ms step_avg:92.79ms
step:1634/1645 train_time:151623ms step_avg:92.79ms
step:1635/1645 train_time:151716ms step_avg:92.79ms
step:1636/1645 train_time:151811ms step_avg:92.79ms
step:1637/1645 train_time:151907ms step_avg:92.80ms
step:1638/1645 train_time:152005ms step_avg:92.80ms
step:1639/1645 train_time:152100ms step_avg:92.80ms
step:1640/1645 train_time:152194ms step_avg:92.80ms
step:1641/1645 train_time:152287ms step_avg:92.80ms
step:1642/1645 train_time:152380ms step_avg:92.80ms
step:1643/1645 train_time:152473ms step_avg:92.80ms
step:1644/1645 train_time:152567ms step_avg:92.80ms
step:1645/1645 train_time:152659ms step_avg:92.80ms
step:1645/1645 val_loss:3.2786 train_time:152754ms step_avg:92.86ms
peak memory allocated: 32074 MiB reserved: 46736 MiB

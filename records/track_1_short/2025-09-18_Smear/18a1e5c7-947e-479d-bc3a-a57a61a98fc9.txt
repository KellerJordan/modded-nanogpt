import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()

        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        smear_lambda = self.scalars[5 * len(self.blocks)]
        #x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x = self.embed(input_seq)

        # smear token embed forward 1 position
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1645 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"smear/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250721+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 18 17:24:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:91:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1645 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1645 train_time:134ms step_avg:133.56ms
step:2/1645 train_time:152ms step_avg:76.25ms
step:3/1645 train_time:222ms step_avg:74.07ms
step:4/1645 train_time:312ms step_avg:78.03ms
step:5/1645 train_time:402ms step_avg:80.46ms
step:6/1645 train_time:493ms step_avg:82.20ms
step:7/1645 train_time:584ms step_avg:83.44ms
step:8/1645 train_time:675ms step_avg:84.39ms
step:9/1645 train_time:766ms step_avg:85.09ms
step:10/1645 train_time:857ms step_avg:85.67ms
step:11/1645 train_time:948ms step_avg:86.19ms
step:12/1645 train_time:1043ms step_avg:86.90ms
step:13/1645 train_time:1138ms step_avg:87.55ms
step:14/1645 train_time:1231ms step_avg:87.95ms
step:15/1645 train_time:1323ms step_avg:88.20ms
step:16/1645 train_time:1414ms step_avg:88.38ms
step:17/1645 train_time:1506ms step_avg:88.58ms
step:18/1645 train_time:1597ms step_avg:88.71ms
step:19/1645 train_time:1689ms step_avg:88.87ms
step:20/1645 train_time:1779ms step_avg:88.97ms
step:21/1645 train_time:1871ms step_avg:89.08ms
step:22/1645 train_time:1963ms step_avg:89.23ms
step:23/1645 train_time:2056ms step_avg:89.41ms
step:24/1645 train_time:2150ms step_avg:89.59ms
step:25/1645 train_time:2243ms step_avg:89.71ms
step:26/1645 train_time:2335ms step_avg:89.80ms
step:27/1645 train_time:2426ms step_avg:89.87ms
step:28/1645 train_time:2518ms step_avg:89.93ms
step:29/1645 train_time:2610ms step_avg:89.99ms
step:30/1645 train_time:2701ms step_avg:90.03ms
step:31/1645 train_time:2792ms step_avg:90.06ms
step:32/1645 train_time:2884ms step_avg:90.11ms
step:33/1645 train_time:2975ms step_avg:90.15ms
step:34/1645 train_time:3069ms step_avg:90.28ms
step:35/1645 train_time:3163ms step_avg:90.37ms
step:36/1645 train_time:3256ms step_avg:90.43ms
step:37/1645 train_time:3347ms step_avg:90.47ms
step:38/1645 train_time:3440ms step_avg:90.52ms
step:39/1645 train_time:3532ms step_avg:90.56ms
step:40/1645 train_time:3623ms step_avg:90.58ms
step:41/1645 train_time:3715ms step_avg:90.61ms
step:42/1645 train_time:3807ms step_avg:90.64ms
step:43/1645 train_time:3898ms step_avg:90.65ms
step:44/1645 train_time:3990ms step_avg:90.69ms
step:45/1645 train_time:4082ms step_avg:90.72ms
step:46/1645 train_time:4174ms step_avg:90.74ms
step:47/1645 train_time:4267ms step_avg:90.79ms
step:48/1645 train_time:4359ms step_avg:90.82ms
step:49/1645 train_time:4451ms step_avg:90.84ms
step:50/1645 train_time:4544ms step_avg:90.87ms
step:51/1645 train_time:4635ms step_avg:90.88ms
step:52/1645 train_time:4727ms step_avg:90.89ms
step:53/1645 train_time:4820ms step_avg:90.94ms
step:54/1645 train_time:4910ms step_avg:90.93ms
step:55/1645 train_time:5002ms step_avg:90.94ms
step:56/1645 train_time:5093ms step_avg:90.95ms
step:57/1645 train_time:5185ms step_avg:90.96ms
step:58/1645 train_time:5277ms step_avg:90.98ms
step:59/1645 train_time:5369ms step_avg:91.01ms
step:60/1645 train_time:5461ms step_avg:91.02ms
step:61/1645 train_time:5553ms step_avg:91.03ms
step:62/1645 train_time:5645ms step_avg:91.05ms
step:63/1645 train_time:5737ms step_avg:91.07ms
step:64/1645 train_time:5829ms step_avg:91.08ms
step:65/1645 train_time:5922ms step_avg:91.11ms
step:66/1645 train_time:6013ms step_avg:91.10ms
step:67/1645 train_time:6104ms step_avg:91.10ms
step:68/1645 train_time:6195ms step_avg:91.11ms
step:69/1645 train_time:6287ms step_avg:91.12ms
step:70/1645 train_time:6379ms step_avg:91.13ms
step:71/1645 train_time:6471ms step_avg:91.15ms
step:72/1645 train_time:6564ms step_avg:91.16ms
step:73/1645 train_time:6655ms step_avg:91.17ms
step:74/1645 train_time:6747ms step_avg:91.18ms
step:75/1645 train_time:6840ms step_avg:91.20ms
step:76/1645 train_time:6932ms step_avg:91.20ms
step:77/1645 train_time:7023ms step_avg:91.21ms
step:78/1645 train_time:7114ms step_avg:91.21ms
step:79/1645 train_time:7206ms step_avg:91.21ms
step:80/1645 train_time:7298ms step_avg:91.22ms
step:81/1645 train_time:7390ms step_avg:91.24ms
step:82/1645 train_time:7481ms step_avg:91.24ms
step:83/1645 train_time:7574ms step_avg:91.25ms
step:84/1645 train_time:7667ms step_avg:91.27ms
step:85/1645 train_time:7758ms step_avg:91.28ms
step:86/1645 train_time:7850ms step_avg:91.28ms
step:87/1645 train_time:7943ms step_avg:91.29ms
step:88/1645 train_time:8035ms step_avg:91.30ms
step:89/1645 train_time:8126ms step_avg:91.31ms
step:90/1645 train_time:8217ms step_avg:91.30ms
step:91/1645 train_time:8309ms step_avg:91.31ms
step:92/1645 train_time:8401ms step_avg:91.31ms
step:93/1645 train_time:8493ms step_avg:91.32ms
step:94/1645 train_time:8584ms step_avg:91.32ms
step:95/1645 train_time:8675ms step_avg:91.32ms
step:96/1645 train_time:8769ms step_avg:91.34ms
step:97/1645 train_time:8862ms step_avg:91.36ms
step:98/1645 train_time:8954ms step_avg:91.37ms
step:99/1645 train_time:9045ms step_avg:91.37ms
step:100/1645 train_time:9137ms step_avg:91.37ms
step:101/1645 train_time:9229ms step_avg:91.38ms
step:102/1645 train_time:9321ms step_avg:91.38ms
step:103/1645 train_time:9412ms step_avg:91.38ms
step:104/1645 train_time:9503ms step_avg:91.38ms
step:105/1645 train_time:9594ms step_avg:91.38ms
step:106/1645 train_time:9687ms step_avg:91.39ms
step:107/1645 train_time:9778ms step_avg:91.38ms
step:108/1645 train_time:9872ms step_avg:91.40ms
step:109/1645 train_time:9963ms step_avg:91.41ms
step:110/1645 train_time:10055ms step_avg:91.41ms
step:111/1645 train_time:10147ms step_avg:91.41ms
step:112/1645 train_time:10240ms step_avg:91.43ms
step:113/1645 train_time:10331ms step_avg:91.43ms
step:114/1645 train_time:10423ms step_avg:91.43ms
step:115/1645 train_time:10515ms step_avg:91.43ms
step:116/1645 train_time:10607ms step_avg:91.44ms
step:117/1645 train_time:10699ms step_avg:91.44ms
step:118/1645 train_time:10791ms step_avg:91.45ms
step:119/1645 train_time:10882ms step_avg:91.45ms
step:120/1645 train_time:10974ms step_avg:91.45ms
step:121/1645 train_time:11066ms step_avg:91.46ms
step:122/1645 train_time:11158ms step_avg:91.46ms
step:123/1645 train_time:11251ms step_avg:91.47ms
step:124/1645 train_time:11342ms step_avg:91.47ms
step:125/1645 train_time:11433ms step_avg:91.47ms
step:125/1645 val_loss:4.3202 train_time:11525ms step_avg:92.20ms
step:126/1645 train_time:11547ms step_avg:91.64ms
step:127/1645 train_time:11621ms step_avg:91.50ms
step:128/1645 train_time:11720ms step_avg:91.57ms
step:129/1645 train_time:11814ms step_avg:91.58ms
step:130/1645 train_time:11906ms step_avg:91.59ms
step:131/1645 train_time:11997ms step_avg:91.58ms
step:132/1645 train_time:12087ms step_avg:91.57ms
step:133/1645 train_time:12179ms step_avg:91.57ms
step:134/1645 train_time:12269ms step_avg:91.56ms
step:135/1645 train_time:12360ms step_avg:91.56ms
step:136/1645 train_time:12451ms step_avg:91.55ms
step:137/1645 train_time:12544ms step_avg:91.56ms
step:138/1645 train_time:12638ms step_avg:91.58ms
step:139/1645 train_time:12732ms step_avg:91.59ms
step:140/1645 train_time:12825ms step_avg:91.61ms
step:141/1645 train_time:12918ms step_avg:91.61ms
step:142/1645 train_time:13009ms step_avg:91.61ms
step:143/1645 train_time:13100ms step_avg:91.61ms
step:144/1645 train_time:13191ms step_avg:91.60ms
step:145/1645 train_time:13282ms step_avg:91.60ms
step:146/1645 train_time:13373ms step_avg:91.60ms
step:147/1645 train_time:13465ms step_avg:91.60ms
step:148/1645 train_time:13557ms step_avg:91.60ms
step:149/1645 train_time:13649ms step_avg:91.61ms
step:150/1645 train_time:13742ms step_avg:91.61ms
step:151/1645 train_time:13835ms step_avg:91.62ms
step:152/1645 train_time:13927ms step_avg:91.62ms
step:153/1645 train_time:14018ms step_avg:91.62ms
step:154/1645 train_time:14109ms step_avg:91.62ms
step:155/1645 train_time:14200ms step_avg:91.61ms
step:156/1645 train_time:14291ms step_avg:91.61ms
step:157/1645 train_time:14382ms step_avg:91.60ms
step:158/1645 train_time:14474ms step_avg:91.61ms
step:159/1645 train_time:14566ms step_avg:91.61ms
step:160/1645 train_time:14658ms step_avg:91.61ms
step:161/1645 train_time:14750ms step_avg:91.62ms
step:162/1645 train_time:14843ms step_avg:91.63ms
step:163/1645 train_time:14936ms step_avg:91.63ms
step:164/1645 train_time:15027ms step_avg:91.63ms
step:165/1645 train_time:15118ms step_avg:91.62ms
step:166/1645 train_time:15209ms step_avg:91.62ms
step:167/1645 train_time:15301ms step_avg:91.62ms
step:168/1645 train_time:15392ms step_avg:91.62ms
step:169/1645 train_time:15484ms step_avg:91.62ms
step:170/1645 train_time:15576ms step_avg:91.62ms
step:171/1645 train_time:15668ms step_avg:91.63ms
step:172/1645 train_time:15760ms step_avg:91.63ms
step:173/1645 train_time:15852ms step_avg:91.63ms
step:174/1645 train_time:15945ms step_avg:91.64ms
step:175/1645 train_time:16037ms step_avg:91.64ms
step:176/1645 train_time:16128ms step_avg:91.64ms
step:177/1645 train_time:16219ms step_avg:91.63ms
step:178/1645 train_time:16310ms step_avg:91.63ms
step:179/1645 train_time:16401ms step_avg:91.63ms
step:180/1645 train_time:16493ms step_avg:91.63ms
step:181/1645 train_time:16585ms step_avg:91.63ms
step:182/1645 train_time:16677ms step_avg:91.63ms
step:183/1645 train_time:16769ms step_avg:91.64ms
step:184/1645 train_time:16862ms step_avg:91.64ms
step:185/1645 train_time:16953ms step_avg:91.64ms
step:186/1645 train_time:17045ms step_avg:91.64ms
step:187/1645 train_time:17137ms step_avg:91.64ms
step:188/1645 train_time:17228ms step_avg:91.64ms
step:189/1645 train_time:17319ms step_avg:91.63ms
step:190/1645 train_time:17410ms step_avg:91.63ms
step:191/1645 train_time:17502ms step_avg:91.63ms
step:192/1645 train_time:17594ms step_avg:91.64ms
step:193/1645 train_time:17687ms step_avg:91.64ms
step:194/1645 train_time:17778ms step_avg:91.64ms
step:195/1645 train_time:17870ms step_avg:91.64ms
step:196/1645 train_time:17962ms step_avg:91.64ms
step:197/1645 train_time:18055ms step_avg:91.65ms
step:198/1645 train_time:18146ms step_avg:91.65ms
step:199/1645 train_time:18238ms step_avg:91.65ms
step:200/1645 train_time:18329ms step_avg:91.64ms
step:201/1645 train_time:18420ms step_avg:91.64ms
step:202/1645 train_time:18511ms step_avg:91.64ms
step:203/1645 train_time:18604ms step_avg:91.65ms
step:204/1645 train_time:18695ms step_avg:91.64ms
step:205/1645 train_time:18788ms step_avg:91.65ms
step:206/1645 train_time:18880ms step_avg:91.65ms
step:207/1645 train_time:18972ms step_avg:91.65ms
step:208/1645 train_time:19063ms step_avg:91.65ms
step:209/1645 train_time:19156ms step_avg:91.65ms
step:210/1645 train_time:19247ms step_avg:91.65ms
step:211/1645 train_time:19339ms step_avg:91.65ms
step:212/1645 train_time:19429ms step_avg:91.65ms
step:213/1645 train_time:19520ms step_avg:91.65ms
step:214/1645 train_time:19612ms step_avg:91.64ms
step:215/1645 train_time:19704ms step_avg:91.65ms
step:216/1645 train_time:19797ms step_avg:91.65ms
step:217/1645 train_time:19888ms step_avg:91.65ms
step:218/1645 train_time:19980ms step_avg:91.65ms
step:219/1645 train_time:20071ms step_avg:91.65ms
step:220/1645 train_time:20163ms step_avg:91.65ms
step:221/1645 train_time:20255ms step_avg:91.65ms
step:222/1645 train_time:20346ms step_avg:91.65ms
step:223/1645 train_time:20437ms step_avg:91.65ms
step:224/1645 train_time:20528ms step_avg:91.64ms
step:225/1645 train_time:20619ms step_avg:91.64ms
step:226/1645 train_time:20711ms step_avg:91.64ms
step:227/1645 train_time:20805ms step_avg:91.65ms
step:228/1645 train_time:20895ms step_avg:91.64ms
step:229/1645 train_time:20987ms step_avg:91.64ms
step:230/1645 train_time:21079ms step_avg:91.65ms
step:231/1645 train_time:21170ms step_avg:91.64ms
step:232/1645 train_time:21262ms step_avg:91.65ms
step:233/1645 train_time:21353ms step_avg:91.65ms
step:234/1645 train_time:21445ms step_avg:91.65ms
step:235/1645 train_time:21537ms step_avg:91.65ms
step:236/1645 train_time:21628ms step_avg:91.64ms
step:237/1645 train_time:21719ms step_avg:91.64ms
step:238/1645 train_time:21811ms step_avg:91.64ms
step:239/1645 train_time:21902ms step_avg:91.64ms
step:240/1645 train_time:21994ms step_avg:91.64ms
step:241/1645 train_time:22087ms step_avg:91.65ms
step:242/1645 train_time:22179ms step_avg:91.65ms
step:243/1645 train_time:22271ms step_avg:91.65ms
step:244/1645 train_time:22362ms step_avg:91.65ms
step:245/1645 train_time:22454ms step_avg:91.65ms
step:246/1645 train_time:22545ms step_avg:91.65ms
step:247/1645 train_time:22637ms step_avg:91.65ms
step:248/1645 train_time:22728ms step_avg:91.65ms
step:249/1645 train_time:22819ms step_avg:91.64ms
step:250/1645 train_time:22911ms step_avg:91.65ms
step:250/1645 val_loss:3.9696 train_time:23002ms step_avg:92.01ms
step:251/1645 train_time:23023ms step_avg:91.73ms
step:252/1645 train_time:23102ms step_avg:91.67ms
step:253/1645 train_time:23195ms step_avg:91.68ms
step:254/1645 train_time:23286ms step_avg:91.68ms
step:255/1645 train_time:23377ms step_avg:91.68ms
step:256/1645 train_time:23469ms step_avg:91.67ms
step:257/1645 train_time:23559ms step_avg:91.67ms
step:258/1645 train_time:23649ms step_avg:91.66ms
step:259/1645 train_time:23740ms step_avg:91.66ms
step:260/1645 train_time:23831ms step_avg:91.66ms
step:261/1645 train_time:23923ms step_avg:91.66ms
step:262/1645 train_time:24017ms step_avg:91.67ms
step:263/1645 train_time:24112ms step_avg:91.68ms
step:264/1645 train_time:24204ms step_avg:91.68ms
step:265/1645 train_time:24297ms step_avg:91.69ms
step:266/1645 train_time:24388ms step_avg:91.68ms
step:267/1645 train_time:24480ms step_avg:91.68ms
step:268/1645 train_time:24570ms step_avg:91.68ms
step:269/1645 train_time:24661ms step_avg:91.68ms
step:270/1645 train_time:24751ms step_avg:91.67ms
step:271/1645 train_time:24841ms step_avg:91.67ms
step:272/1645 train_time:24933ms step_avg:91.66ms
step:273/1645 train_time:25026ms step_avg:91.67ms
step:274/1645 train_time:25120ms step_avg:91.68ms
step:275/1645 train_time:25212ms step_avg:91.68ms
step:276/1645 train_time:25304ms step_avg:91.68ms
step:277/1645 train_time:25395ms step_avg:91.68ms
step:278/1645 train_time:25487ms step_avg:91.68ms
step:279/1645 train_time:25579ms step_avg:91.68ms
step:280/1645 train_time:25668ms step_avg:91.67ms
step:281/1645 train_time:25760ms step_avg:91.67ms
step:282/1645 train_time:25851ms step_avg:91.67ms
step:283/1645 train_time:25943ms step_avg:91.67ms
step:284/1645 train_time:26035ms step_avg:91.67ms
step:285/1645 train_time:26126ms step_avg:91.67ms
step:286/1645 train_time:26220ms step_avg:91.68ms
step:287/1645 train_time:26312ms step_avg:91.68ms
step:288/1645 train_time:26404ms step_avg:91.68ms
step:289/1645 train_time:26495ms step_avg:91.68ms
step:290/1645 train_time:26587ms step_avg:91.68ms
step:291/1645 train_time:26677ms step_avg:91.67ms
step:292/1645 train_time:26768ms step_avg:91.67ms
step:293/1645 train_time:26860ms step_avg:91.67ms
step:294/1645 train_time:26951ms step_avg:91.67ms
step:295/1645 train_time:27043ms step_avg:91.67ms
step:296/1645 train_time:27135ms step_avg:91.67ms
step:297/1645 train_time:27227ms step_avg:91.67ms
step:298/1645 train_time:27319ms step_avg:91.67ms
step:299/1645 train_time:27411ms step_avg:91.68ms
step:300/1645 train_time:27503ms step_avg:91.68ms
step:301/1645 train_time:27595ms step_avg:91.68ms
step:302/1645 train_time:27686ms step_avg:91.68ms
step:303/1645 train_time:27777ms step_avg:91.67ms
step:304/1645 train_time:27868ms step_avg:91.67ms
step:305/1645 train_time:27960ms step_avg:91.67ms
step:306/1645 train_time:28052ms step_avg:91.67ms
step:307/1645 train_time:28144ms step_avg:91.67ms
step:308/1645 train_time:28237ms step_avg:91.68ms
step:309/1645 train_time:28329ms step_avg:91.68ms
step:310/1645 train_time:28421ms step_avg:91.68ms
step:311/1645 train_time:28512ms step_avg:91.68ms
step:312/1645 train_time:28603ms step_avg:91.68ms
step:313/1645 train_time:28695ms step_avg:91.68ms
step:314/1645 train_time:28786ms step_avg:91.67ms
step:315/1645 train_time:28877ms step_avg:91.67ms
step:316/1645 train_time:28968ms step_avg:91.67ms
step:317/1645 train_time:29060ms step_avg:91.67ms
step:318/1645 train_time:29152ms step_avg:91.67ms
step:319/1645 train_time:29244ms step_avg:91.67ms
step:320/1645 train_time:29337ms step_avg:91.68ms
step:321/1645 train_time:29429ms step_avg:91.68ms
step:322/1645 train_time:29520ms step_avg:91.68ms
step:323/1645 train_time:29612ms step_avg:91.68ms
step:324/1645 train_time:29703ms step_avg:91.68ms
step:325/1645 train_time:29795ms step_avg:91.68ms
step:326/1645 train_time:29886ms step_avg:91.68ms
step:327/1645 train_time:29977ms step_avg:91.67ms
step:328/1645 train_time:30069ms step_avg:91.67ms
step:329/1645 train_time:30161ms step_avg:91.67ms
step:330/1645 train_time:30253ms step_avg:91.67ms
step:331/1645 train_time:30345ms step_avg:91.68ms
step:332/1645 train_time:30438ms step_avg:91.68ms
step:333/1645 train_time:30529ms step_avg:91.68ms
step:334/1645 train_time:30621ms step_avg:91.68ms
step:335/1645 train_time:30712ms step_avg:91.68ms
step:336/1645 train_time:30805ms step_avg:91.68ms
step:337/1645 train_time:30895ms step_avg:91.68ms
step:338/1645 train_time:30986ms step_avg:91.68ms
step:339/1645 train_time:31079ms step_avg:91.68ms
step:340/1645 train_time:31171ms step_avg:91.68ms
step:341/1645 train_time:31262ms step_avg:91.68ms
step:342/1645 train_time:31354ms step_avg:91.68ms
step:343/1645 train_time:31446ms step_avg:91.68ms
step:344/1645 train_time:31538ms step_avg:91.68ms
step:345/1645 train_time:31630ms step_avg:91.68ms
step:346/1645 train_time:31722ms step_avg:91.68ms
step:347/1645 train_time:31814ms step_avg:91.68ms
step:348/1645 train_time:31906ms step_avg:91.68ms
step:349/1645 train_time:31998ms step_avg:91.68ms
step:350/1645 train_time:32089ms step_avg:91.68ms
step:351/1645 train_time:32180ms step_avg:91.68ms
step:352/1645 train_time:32272ms step_avg:91.68ms
step:353/1645 train_time:32364ms step_avg:91.68ms
step:354/1645 train_time:32455ms step_avg:91.68ms
step:355/1645 train_time:32546ms step_avg:91.68ms
step:356/1645 train_time:32639ms step_avg:91.68ms
step:357/1645 train_time:32731ms step_avg:91.68ms
step:358/1645 train_time:32823ms step_avg:91.68ms
step:359/1645 train_time:32914ms step_avg:91.68ms
step:360/1645 train_time:33005ms step_avg:91.68ms
step:361/1645 train_time:33096ms step_avg:91.68ms
step:362/1645 train_time:33188ms step_avg:91.68ms
step:363/1645 train_time:33280ms step_avg:91.68ms
step:364/1645 train_time:33371ms step_avg:91.68ms
step:365/1645 train_time:33463ms step_avg:91.68ms
step:366/1645 train_time:33554ms step_avg:91.68ms
step:367/1645 train_time:33645ms step_avg:91.68ms
step:368/1645 train_time:33739ms step_avg:91.68ms
step:369/1645 train_time:33830ms step_avg:91.68ms
step:370/1645 train_time:33922ms step_avg:91.68ms
step:371/1645 train_time:34016ms step_avg:91.69ms
step:372/1645 train_time:34105ms step_avg:91.68ms
step:373/1645 train_time:34198ms step_avg:91.68ms
step:374/1645 train_time:34290ms step_avg:91.68ms
step:375/1645 train_time:34381ms step_avg:91.68ms
step:375/1645 val_loss:3.8141 train_time:34473ms step_avg:91.93ms
step:376/1645 train_time:34493ms step_avg:91.74ms
step:377/1645 train_time:34567ms step_avg:91.69ms
step:378/1645 train_time:34661ms step_avg:91.69ms
step:379/1645 train_time:34752ms step_avg:91.70ms
step:380/1645 train_time:34843ms step_avg:91.69ms
step:381/1645 train_time:34934ms step_avg:91.69ms
step:382/1645 train_time:35024ms step_avg:91.69ms
step:383/1645 train_time:35115ms step_avg:91.68ms
step:384/1645 train_time:35205ms step_avg:91.68ms
step:385/1645 train_time:35296ms step_avg:91.68ms
step:386/1645 train_time:35389ms step_avg:91.68ms
step:387/1645 train_time:35483ms step_avg:91.69ms
step:388/1645 train_time:35578ms step_avg:91.69ms
step:389/1645 train_time:35670ms step_avg:91.70ms
step:390/1645 train_time:35761ms step_avg:91.70ms
step:391/1645 train_time:35853ms step_avg:91.69ms
step:392/1645 train_time:35944ms step_avg:91.69ms
step:393/1645 train_time:36035ms step_avg:91.69ms
step:394/1645 train_time:36125ms step_avg:91.69ms
step:395/1645 train_time:36216ms step_avg:91.69ms
step:396/1645 train_time:36307ms step_avg:91.68ms
step:397/1645 train_time:36399ms step_avg:91.68ms
step:398/1645 train_time:36493ms step_avg:91.69ms
step:399/1645 train_time:36587ms step_avg:91.70ms
step:400/1645 train_time:36679ms step_avg:91.70ms
step:401/1645 train_time:36772ms step_avg:91.70ms
step:402/1645 train_time:36863ms step_avg:91.70ms
step:403/1645 train_time:36955ms step_avg:91.70ms
step:404/1645 train_time:37045ms step_avg:91.70ms
step:405/1645 train_time:37136ms step_avg:91.69ms
step:406/1645 train_time:37226ms step_avg:91.69ms
step:407/1645 train_time:37318ms step_avg:91.69ms
step:408/1645 train_time:37409ms step_avg:91.69ms
step:409/1645 train_time:37501ms step_avg:91.69ms
step:410/1645 train_time:37595ms step_avg:91.69ms
step:411/1645 train_time:37687ms step_avg:91.70ms
step:412/1645 train_time:37779ms step_avg:91.70ms
step:413/1645 train_time:37872ms step_avg:91.70ms
step:414/1645 train_time:37963ms step_avg:91.70ms
step:415/1645 train_time:38054ms step_avg:91.70ms
step:416/1645 train_time:38145ms step_avg:91.70ms
step:417/1645 train_time:38236ms step_avg:91.69ms
step:418/1645 train_time:38326ms step_avg:91.69ms
step:419/1645 train_time:38418ms step_avg:91.69ms
step:420/1645 train_time:38510ms step_avg:91.69ms
step:421/1645 train_time:38602ms step_avg:91.69ms
step:422/1645 train_time:38695ms step_avg:91.69ms
step:423/1645 train_time:38787ms step_avg:91.70ms
step:424/1645 train_time:38878ms step_avg:91.69ms
step:425/1645 train_time:38971ms step_avg:91.70ms
step:426/1645 train_time:39062ms step_avg:91.70ms
step:427/1645 train_time:39154ms step_avg:91.69ms
step:428/1645 train_time:39245ms step_avg:91.69ms
step:429/1645 train_time:39336ms step_avg:91.69ms
step:430/1645 train_time:39427ms step_avg:91.69ms
step:431/1645 train_time:39518ms step_avg:91.69ms
step:432/1645 train_time:39611ms step_avg:91.69ms
step:433/1645 train_time:39702ms step_avg:91.69ms
step:434/1645 train_time:39794ms step_avg:91.69ms
step:435/1645 train_time:39886ms step_avg:91.69ms
step:436/1645 train_time:39978ms step_avg:91.69ms
step:437/1645 train_time:40070ms step_avg:91.69ms
step:438/1645 train_time:40162ms step_avg:91.69ms
step:439/1645 train_time:40253ms step_avg:91.69ms
step:440/1645 train_time:40345ms step_avg:91.69ms
step:441/1645 train_time:40436ms step_avg:91.69ms
step:442/1645 train_time:40527ms step_avg:91.69ms
step:443/1645 train_time:40618ms step_avg:91.69ms
step:444/1645 train_time:40711ms step_avg:91.69ms
step:445/1645 train_time:40803ms step_avg:91.69ms
step:446/1645 train_time:40894ms step_avg:91.69ms
step:447/1645 train_time:40986ms step_avg:91.69ms
step:448/1645 train_time:41077ms step_avg:91.69ms
step:449/1645 train_time:41170ms step_avg:91.69ms
step:450/1645 train_time:41262ms step_avg:91.69ms
step:451/1645 train_time:41353ms step_avg:91.69ms
step:452/1645 train_time:41445ms step_avg:91.69ms
step:453/1645 train_time:41536ms step_avg:91.69ms
step:454/1645 train_time:41627ms step_avg:91.69ms
step:455/1645 train_time:41718ms step_avg:91.69ms
step:456/1645 train_time:41810ms step_avg:91.69ms
step:457/1645 train_time:41903ms step_avg:91.69ms
step:458/1645 train_time:41994ms step_avg:91.69ms
step:459/1645 train_time:42087ms step_avg:91.69ms
step:460/1645 train_time:42178ms step_avg:91.69ms
step:461/1645 train_time:42271ms step_avg:91.69ms
step:462/1645 train_time:42362ms step_avg:91.69ms
step:463/1645 train_time:42455ms step_avg:91.69ms
step:464/1645 train_time:42546ms step_avg:91.69ms
step:465/1645 train_time:42637ms step_avg:91.69ms
step:466/1645 train_time:42728ms step_avg:91.69ms
step:467/1645 train_time:42820ms step_avg:91.69ms
step:468/1645 train_time:42911ms step_avg:91.69ms
step:469/1645 train_time:43003ms step_avg:91.69ms
step:470/1645 train_time:43095ms step_avg:91.69ms
step:471/1645 train_time:43186ms step_avg:91.69ms
step:472/1645 train_time:43278ms step_avg:91.69ms
step:473/1645 train_time:43370ms step_avg:91.69ms
step:474/1645 train_time:43462ms step_avg:91.69ms
step:475/1645 train_time:43554ms step_avg:91.69ms
step:476/1645 train_time:43645ms step_avg:91.69ms
step:477/1645 train_time:43737ms step_avg:91.69ms
step:478/1645 train_time:43828ms step_avg:91.69ms
step:479/1645 train_time:43920ms step_avg:91.69ms
step:480/1645 train_time:44013ms step_avg:91.69ms
step:481/1645 train_time:44104ms step_avg:91.69ms
step:482/1645 train_time:44196ms step_avg:91.69ms
step:483/1645 train_time:44289ms step_avg:91.70ms
step:484/1645 train_time:44381ms step_avg:91.70ms
step:485/1645 train_time:44473ms step_avg:91.70ms
step:486/1645 train_time:44564ms step_avg:91.70ms
step:487/1645 train_time:44656ms step_avg:91.70ms
step:488/1645 train_time:44748ms step_avg:91.70ms
step:489/1645 train_time:44839ms step_avg:91.70ms
step:490/1645 train_time:44931ms step_avg:91.69ms
step:491/1645 train_time:45023ms step_avg:91.70ms
step:492/1645 train_time:45114ms step_avg:91.70ms
step:493/1645 train_time:45207ms step_avg:91.70ms
step:494/1645 train_time:45298ms step_avg:91.70ms
step:495/1645 train_time:45391ms step_avg:91.70ms
step:496/1645 train_time:45482ms step_avg:91.70ms
step:497/1645 train_time:45574ms step_avg:91.70ms
step:498/1645 train_time:45666ms step_avg:91.70ms
step:499/1645 train_time:45757ms step_avg:91.70ms
step:500/1645 train_time:45848ms step_avg:91.70ms
step:500/1645 val_loss:3.7137 train_time:45941ms step_avg:91.88ms
step:501/1645 train_time:45961ms step_avg:91.74ms
step:502/1645 train_time:46036ms step_avg:91.70ms
step:503/1645 train_time:46131ms step_avg:91.71ms
step:504/1645 train_time:46223ms step_avg:91.71ms
step:505/1645 train_time:46313ms step_avg:91.71ms
step:506/1645 train_time:46404ms step_avg:91.71ms
step:507/1645 train_time:46494ms step_avg:91.70ms
step:508/1645 train_time:46585ms step_avg:91.70ms
step:509/1645 train_time:46677ms step_avg:91.70ms
step:510/1645 train_time:46768ms step_avg:91.70ms
step:511/1645 train_time:46861ms step_avg:91.70ms
step:512/1645 train_time:46954ms step_avg:91.71ms
step:513/1645 train_time:47047ms step_avg:91.71ms
step:514/1645 train_time:47139ms step_avg:91.71ms
step:515/1645 train_time:47232ms step_avg:91.71ms
step:516/1645 train_time:47323ms step_avg:91.71ms
step:517/1645 train_time:47414ms step_avg:91.71ms
step:518/1645 train_time:47505ms step_avg:91.71ms
step:519/1645 train_time:47595ms step_avg:91.71ms
step:520/1645 train_time:47687ms step_avg:91.71ms
step:521/1645 train_time:47778ms step_avg:91.70ms
step:522/1645 train_time:47870ms step_avg:91.70ms
step:523/1645 train_time:47962ms step_avg:91.70ms
step:524/1645 train_time:48054ms step_avg:91.71ms
step:525/1645 train_time:48147ms step_avg:91.71ms
step:526/1645 train_time:48240ms step_avg:91.71ms
step:527/1645 train_time:48331ms step_avg:91.71ms
step:528/1645 train_time:48423ms step_avg:91.71ms
step:529/1645 train_time:48513ms step_avg:91.71ms
step:530/1645 train_time:48605ms step_avg:91.71ms
step:531/1645 train_time:48696ms step_avg:91.71ms
step:532/1645 train_time:48787ms step_avg:91.71ms
step:533/1645 train_time:48878ms step_avg:91.70ms
step:534/1645 train_time:48970ms step_avg:91.70ms
step:535/1645 train_time:49062ms step_avg:91.70ms
step:536/1645 train_time:49154ms step_avg:91.71ms
step:537/1645 train_time:49248ms step_avg:91.71ms
step:538/1645 train_time:49340ms step_avg:91.71ms
step:539/1645 train_time:49432ms step_avg:91.71ms
step:540/1645 train_time:49524ms step_avg:91.71ms
step:541/1645 train_time:49615ms step_avg:91.71ms
step:542/1645 train_time:49707ms step_avg:91.71ms
step:543/1645 train_time:49799ms step_avg:91.71ms
step:544/1645 train_time:49889ms step_avg:91.71ms
step:545/1645 train_time:49980ms step_avg:91.71ms
step:546/1645 train_time:50072ms step_avg:91.71ms
step:547/1645 train_time:50165ms step_avg:91.71ms
step:548/1645 train_time:50257ms step_avg:91.71ms
step:549/1645 train_time:50350ms step_avg:91.71ms
step:550/1645 train_time:50443ms step_avg:91.71ms
step:551/1645 train_time:50535ms step_avg:91.71ms
step:552/1645 train_time:50628ms step_avg:91.72ms
step:553/1645 train_time:50721ms step_avg:91.72ms
step:554/1645 train_time:50813ms step_avg:91.72ms
step:555/1645 train_time:50906ms step_avg:91.72ms
step:556/1645 train_time:50998ms step_avg:91.72ms
step:557/1645 train_time:51091ms step_avg:91.73ms
step:558/1645 train_time:51184ms step_avg:91.73ms
step:559/1645 train_time:51277ms step_avg:91.73ms
step:560/1645 train_time:51371ms step_avg:91.73ms
step:561/1645 train_time:51464ms step_avg:91.74ms
step:562/1645 train_time:51557ms step_avg:91.74ms
step:563/1645 train_time:51650ms step_avg:91.74ms
step:564/1645 train_time:51742ms step_avg:91.74ms
step:565/1645 train_time:51835ms step_avg:91.74ms
step:566/1645 train_time:51928ms step_avg:91.74ms
step:567/1645 train_time:52020ms step_avg:91.75ms
step:568/1645 train_time:52112ms step_avg:91.75ms
step:569/1645 train_time:52207ms step_avg:91.75ms
step:570/1645 train_time:52300ms step_avg:91.75ms
step:571/1645 train_time:52393ms step_avg:91.76ms
step:572/1645 train_time:52486ms step_avg:91.76ms
step:573/1645 train_time:52579ms step_avg:91.76ms
step:574/1645 train_time:52673ms step_avg:91.76ms
step:575/1645 train_time:52767ms step_avg:91.77ms
step:576/1645 train_time:52859ms step_avg:91.77ms
step:577/1645 train_time:52953ms step_avg:91.77ms
step:578/1645 train_time:53045ms step_avg:91.77ms
step:579/1645 train_time:53138ms step_avg:91.78ms
step:580/1645 train_time:53232ms step_avg:91.78ms
step:581/1645 train_time:53325ms step_avg:91.78ms
step:582/1645 train_time:53418ms step_avg:91.78ms
step:583/1645 train_time:53512ms step_avg:91.79ms
step:584/1645 train_time:53604ms step_avg:91.79ms
step:585/1645 train_time:53698ms step_avg:91.79ms
step:586/1645 train_time:53791ms step_avg:91.79ms
step:587/1645 train_time:53884ms step_avg:91.80ms
step:588/1645 train_time:53976ms step_avg:91.80ms
step:589/1645 train_time:54069ms step_avg:91.80ms
step:590/1645 train_time:54161ms step_avg:91.80ms
step:591/1645 train_time:54255ms step_avg:91.80ms
step:592/1645 train_time:54347ms step_avg:91.80ms
step:593/1645 train_time:54441ms step_avg:91.81ms
step:594/1645 train_time:54534ms step_avg:91.81ms
step:595/1645 train_time:54627ms step_avg:91.81ms
step:596/1645 train_time:54720ms step_avg:91.81ms
step:597/1645 train_time:54813ms step_avg:91.81ms
step:598/1645 train_time:54906ms step_avg:91.82ms
step:599/1645 train_time:54999ms step_avg:91.82ms
step:600/1645 train_time:55092ms step_avg:91.82ms
step:601/1645 train_time:55185ms step_avg:91.82ms
step:602/1645 train_time:55278ms step_avg:91.82ms
step:603/1645 train_time:55371ms step_avg:91.83ms
step:604/1645 train_time:55464ms step_avg:91.83ms
step:605/1645 train_time:55557ms step_avg:91.83ms
step:606/1645 train_time:55650ms step_avg:91.83ms
step:607/1645 train_time:55743ms step_avg:91.83ms
step:608/1645 train_time:55836ms step_avg:91.84ms
step:609/1645 train_time:55930ms step_avg:91.84ms
step:610/1645 train_time:56022ms step_avg:91.84ms
step:611/1645 train_time:56114ms step_avg:91.84ms
step:612/1645 train_time:56207ms step_avg:91.84ms
step:613/1645 train_time:56301ms step_avg:91.84ms
step:614/1645 train_time:56392ms step_avg:91.84ms
step:615/1645 train_time:56485ms step_avg:91.85ms
step:616/1645 train_time:56578ms step_avg:91.85ms
step:617/1645 train_time:56671ms step_avg:91.85ms
step:618/1645 train_time:56764ms step_avg:91.85ms
step:619/1645 train_time:56856ms step_avg:91.85ms
step:620/1645 train_time:56951ms step_avg:91.86ms
step:621/1645 train_time:57044ms step_avg:91.86ms
step:622/1645 train_time:57136ms step_avg:91.86ms
step:623/1645 train_time:57231ms step_avg:91.86ms
step:624/1645 train_time:57323ms step_avg:91.86ms
step:625/1645 train_time:57415ms step_avg:91.86ms
step:625/1645 val_loss:3.6114 train_time:57508ms step_avg:92.01ms
step:626/1645 train_time:57530ms step_avg:91.90ms
step:627/1645 train_time:57608ms step_avg:91.88ms
step:628/1645 train_time:57705ms step_avg:91.89ms
step:629/1645 train_time:57798ms step_avg:91.89ms
step:630/1645 train_time:57890ms step_avg:91.89ms
step:631/1645 train_time:57982ms step_avg:91.89ms
step:632/1645 train_time:58074ms step_avg:91.89ms
step:633/1645 train_time:58165ms step_avg:91.89ms
step:634/1645 train_time:58257ms step_avg:91.89ms
step:635/1645 train_time:58348ms step_avg:91.89ms
step:636/1645 train_time:58443ms step_avg:91.89ms
step:637/1645 train_time:58540ms step_avg:91.90ms
step:638/1645 train_time:58635ms step_avg:91.90ms
step:639/1645 train_time:58729ms step_avg:91.91ms
step:640/1645 train_time:58823ms step_avg:91.91ms
step:641/1645 train_time:58915ms step_avg:91.91ms
step:642/1645 train_time:59007ms step_avg:91.91ms
step:643/1645 train_time:59100ms step_avg:91.91ms
step:644/1645 train_time:59193ms step_avg:91.91ms
step:645/1645 train_time:59284ms step_avg:91.91ms
step:646/1645 train_time:59376ms step_avg:91.91ms
step:647/1645 train_time:59469ms step_avg:91.92ms
step:648/1645 train_time:59565ms step_avg:91.92ms
step:649/1645 train_time:59660ms step_avg:91.93ms
step:650/1645 train_time:59753ms step_avg:91.93ms
step:651/1645 train_time:59846ms step_avg:91.93ms
step:652/1645 train_time:59939ms step_avg:91.93ms
step:653/1645 train_time:60032ms step_avg:91.93ms
step:654/1645 train_time:60125ms step_avg:91.93ms
step:655/1645 train_time:60216ms step_avg:91.93ms
step:656/1645 train_time:60309ms step_avg:91.93ms
step:657/1645 train_time:60402ms step_avg:91.94ms
step:658/1645 train_time:60496ms step_avg:91.94ms
step:659/1645 train_time:60589ms step_avg:91.94ms
step:660/1645 train_time:60683ms step_avg:91.94ms
step:661/1645 train_time:60776ms step_avg:91.95ms
step:662/1645 train_time:60869ms step_avg:91.95ms
step:663/1645 train_time:60962ms step_avg:91.95ms
step:664/1645 train_time:61054ms step_avg:91.95ms
step:665/1645 train_time:61148ms step_avg:91.95ms
step:666/1645 train_time:61240ms step_avg:91.95ms
step:667/1645 train_time:61333ms step_avg:91.95ms
step:668/1645 train_time:61426ms step_avg:91.96ms
step:669/1645 train_time:61519ms step_avg:91.96ms
step:670/1645 train_time:61613ms step_avg:91.96ms
step:671/1645 train_time:61706ms step_avg:91.96ms
step:672/1645 train_time:61800ms step_avg:91.96ms
step:673/1645 train_time:61894ms step_avg:91.97ms
step:674/1645 train_time:61986ms step_avg:91.97ms
step:675/1645 train_time:62079ms step_avg:91.97ms
step:676/1645 train_time:62171ms step_avg:91.97ms
step:677/1645 train_time:62263ms step_avg:91.97ms
step:678/1645 train_time:62356ms step_avg:91.97ms
step:679/1645 train_time:62448ms step_avg:91.97ms
step:680/1645 train_time:62541ms step_avg:91.97ms
step:681/1645 train_time:62634ms step_avg:91.97ms
step:682/1645 train_time:62727ms step_avg:91.97ms
step:683/1645 train_time:62821ms step_avg:91.98ms
step:684/1645 train_time:62914ms step_avg:91.98ms
step:685/1645 train_time:63006ms step_avg:91.98ms
step:686/1645 train_time:63100ms step_avg:91.98ms
step:687/1645 train_time:63192ms step_avg:91.98ms
step:688/1645 train_time:63285ms step_avg:91.98ms
step:689/1645 train_time:63378ms step_avg:91.98ms
step:690/1645 train_time:63470ms step_avg:91.98ms
step:691/1645 train_time:63563ms step_avg:91.99ms
step:692/1645 train_time:63656ms step_avg:91.99ms
step:693/1645 train_time:63749ms step_avg:91.99ms
step:694/1645 train_time:63842ms step_avg:91.99ms
step:695/1645 train_time:63934ms step_avg:91.99ms
step:696/1645 train_time:64028ms step_avg:91.99ms
step:697/1645 train_time:64121ms step_avg:92.00ms
step:698/1645 train_time:64215ms step_avg:92.00ms
step:699/1645 train_time:64307ms step_avg:92.00ms
step:700/1645 train_time:64400ms step_avg:92.00ms
step:701/1645 train_time:64493ms step_avg:92.00ms
step:702/1645 train_time:64586ms step_avg:92.00ms
step:703/1645 train_time:64679ms step_avg:92.00ms
step:704/1645 train_time:64772ms step_avg:92.01ms
step:705/1645 train_time:64865ms step_avg:92.01ms
step:706/1645 train_time:64958ms step_avg:92.01ms
step:707/1645 train_time:65050ms step_avg:92.01ms
step:708/1645 train_time:65143ms step_avg:92.01ms
step:709/1645 train_time:65236ms step_avg:92.01ms
step:710/1645 train_time:65328ms step_avg:92.01ms
step:711/1645 train_time:65421ms step_avg:92.01ms
step:712/1645 train_time:65514ms step_avg:92.01ms
step:713/1645 train_time:65607ms step_avg:92.02ms
step:714/1645 train_time:65701ms step_avg:92.02ms
step:715/1645 train_time:65793ms step_avg:92.02ms
step:716/1645 train_time:65886ms step_avg:92.02ms
step:717/1645 train_time:65980ms step_avg:92.02ms
step:718/1645 train_time:66073ms step_avg:92.02ms
step:719/1645 train_time:66165ms step_avg:92.02ms
step:720/1645 train_time:66258ms step_avg:92.02ms
step:721/1645 train_time:66350ms step_avg:92.03ms
step:722/1645 train_time:66443ms step_avg:92.03ms
step:723/1645 train_time:66535ms step_avg:92.03ms
step:724/1645 train_time:66628ms step_avg:92.03ms
step:725/1645 train_time:66722ms step_avg:92.03ms
step:726/1645 train_time:66815ms step_avg:92.03ms
step:727/1645 train_time:66908ms step_avg:92.03ms
step:728/1645 train_time:67001ms step_avg:92.03ms
step:729/1645 train_time:67095ms step_avg:92.04ms
step:730/1645 train_time:67187ms step_avg:92.04ms
step:731/1645 train_time:67280ms step_avg:92.04ms
step:732/1645 train_time:67373ms step_avg:92.04ms
step:733/1645 train_time:67465ms step_avg:92.04ms
step:734/1645 train_time:67558ms step_avg:92.04ms
step:735/1645 train_time:67652ms step_avg:92.04ms
step:736/1645 train_time:67745ms step_avg:92.04ms
step:737/1645 train_time:67838ms step_avg:92.05ms
step:738/1645 train_time:67930ms step_avg:92.05ms
step:739/1645 train_time:68024ms step_avg:92.05ms
step:740/1645 train_time:68117ms step_avg:92.05ms
step:741/1645 train_time:68209ms step_avg:92.05ms
step:742/1645 train_time:68303ms step_avg:92.05ms
step:743/1645 train_time:68395ms step_avg:92.05ms
step:744/1645 train_time:68488ms step_avg:92.05ms
step:745/1645 train_time:68580ms step_avg:92.05ms
step:746/1645 train_time:68674ms step_avg:92.06ms
step:747/1645 train_time:68767ms step_avg:92.06ms
step:748/1645 train_time:68859ms step_avg:92.06ms
step:749/1645 train_time:68953ms step_avg:92.06ms
step:750/1645 train_time:69047ms step_avg:92.06ms
step:750/1645 val_loss:3.5596 train_time:69140ms step_avg:92.19ms
step:751/1645 train_time:69161ms step_avg:92.09ms
step:752/1645 train_time:69237ms step_avg:92.07ms
step:753/1645 train_time:69334ms step_avg:92.08ms
step:754/1645 train_time:69427ms step_avg:92.08ms
step:755/1645 train_time:69518ms step_avg:92.08ms
step:756/1645 train_time:69610ms step_avg:92.08ms
step:757/1645 train_time:69702ms step_avg:92.08ms
step:758/1645 train_time:69794ms step_avg:92.08ms
step:759/1645 train_time:69886ms step_avg:92.08ms
step:760/1645 train_time:69978ms step_avg:92.08ms
step:761/1645 train_time:70070ms step_avg:92.08ms
step:762/1645 train_time:70164ms step_avg:92.08ms
step:763/1645 train_time:70260ms step_avg:92.08ms
step:764/1645 train_time:70353ms step_avg:92.09ms
step:765/1645 train_time:70446ms step_avg:92.09ms
step:766/1645 train_time:70540ms step_avg:92.09ms
step:767/1645 train_time:70632ms step_avg:92.09ms
step:768/1645 train_time:70726ms step_avg:92.09ms
step:769/1645 train_time:70818ms step_avg:92.09ms
step:770/1645 train_time:70910ms step_avg:92.09ms
step:771/1645 train_time:71003ms step_avg:92.09ms
step:772/1645 train_time:71097ms step_avg:92.09ms
step:773/1645 train_time:71192ms step_avg:92.10ms
step:774/1645 train_time:71286ms step_avg:92.10ms
step:775/1645 train_time:71379ms step_avg:92.10ms
step:776/1645 train_time:71472ms step_avg:92.10ms
step:777/1645 train_time:71565ms step_avg:92.10ms
step:778/1645 train_time:71658ms step_avg:92.11ms
step:779/1645 train_time:71750ms step_avg:92.11ms
step:780/1645 train_time:71843ms step_avg:92.11ms
step:781/1645 train_time:71935ms step_avg:92.11ms
step:782/1645 train_time:72028ms step_avg:92.11ms
step:783/1645 train_time:72120ms step_avg:92.11ms
step:784/1645 train_time:72213ms step_avg:92.11ms
step:785/1645 train_time:72307ms step_avg:92.11ms
step:786/1645 train_time:72400ms step_avg:92.11ms
step:787/1645 train_time:72494ms step_avg:92.11ms
step:788/1645 train_time:72587ms step_avg:92.12ms
step:789/1645 train_time:72680ms step_avg:92.12ms
step:790/1645 train_time:72772ms step_avg:92.12ms
step:791/1645 train_time:72864ms step_avg:92.12ms
step:792/1645 train_time:72956ms step_avg:92.12ms
step:793/1645 train_time:73049ms step_avg:92.12ms
step:794/1645 train_time:73143ms step_avg:92.12ms
step:795/1645 train_time:73236ms step_avg:92.12ms
step:796/1645 train_time:73330ms step_avg:92.12ms
step:797/1645 train_time:73423ms step_avg:92.12ms
step:798/1645 train_time:73517ms step_avg:92.13ms
step:799/1645 train_time:73611ms step_avg:92.13ms
step:800/1645 train_time:73703ms step_avg:92.13ms
step:801/1645 train_time:73796ms step_avg:92.13ms
step:802/1645 train_time:73889ms step_avg:92.13ms
step:803/1645 train_time:73980ms step_avg:92.13ms
step:804/1645 train_time:74073ms step_avg:92.13ms
step:805/1645 train_time:74167ms step_avg:92.13ms
step:806/1645 train_time:74260ms step_avg:92.13ms
step:807/1645 train_time:74353ms step_avg:92.13ms
step:808/1645 train_time:74447ms step_avg:92.14ms
step:809/1645 train_time:74539ms step_avg:92.14ms
step:810/1645 train_time:74632ms step_avg:92.14ms
step:811/1645 train_time:74725ms step_avg:92.14ms
step:812/1645 train_time:74819ms step_avg:92.14ms
step:813/1645 train_time:74911ms step_avg:92.14ms
step:814/1645 train_time:75004ms step_avg:92.14ms
step:815/1645 train_time:75097ms step_avg:92.14ms
step:816/1645 train_time:75190ms step_avg:92.14ms
step:817/1645 train_time:75283ms step_avg:92.15ms
step:818/1645 train_time:75375ms step_avg:92.15ms
step:819/1645 train_time:75469ms step_avg:92.15ms
step:820/1645 train_time:75562ms step_avg:92.15ms
step:821/1645 train_time:75654ms step_avg:92.15ms
step:822/1645 train_time:75749ms step_avg:92.15ms
step:823/1645 train_time:75842ms step_avg:92.15ms
step:824/1645 train_time:75934ms step_avg:92.15ms
step:825/1645 train_time:76027ms step_avg:92.15ms
step:826/1645 train_time:76119ms step_avg:92.15ms
step:827/1645 train_time:76212ms step_avg:92.16ms
step:828/1645 train_time:76306ms step_avg:92.16ms
step:829/1645 train_time:76398ms step_avg:92.16ms
step:830/1645 train_time:76491ms step_avg:92.16ms
step:831/1645 train_time:76584ms step_avg:92.16ms
step:832/1645 train_time:76677ms step_avg:92.16ms
step:833/1645 train_time:76771ms step_avg:92.16ms
step:834/1645 train_time:76864ms step_avg:92.16ms
step:835/1645 train_time:76956ms step_avg:92.16ms
step:836/1645 train_time:77049ms step_avg:92.16ms
step:837/1645 train_time:77142ms step_avg:92.17ms
step:838/1645 train_time:77235ms step_avg:92.17ms
step:839/1645 train_time:77329ms step_avg:92.17ms
step:840/1645 train_time:77423ms step_avg:92.17ms
step:841/1645 train_time:77516ms step_avg:92.17ms
step:842/1645 train_time:77609ms step_avg:92.17ms
step:843/1645 train_time:77702ms step_avg:92.17ms
step:844/1645 train_time:77795ms step_avg:92.17ms
step:845/1645 train_time:77887ms step_avg:92.17ms
step:846/1645 train_time:77980ms step_avg:92.17ms
step:847/1645 train_time:78072ms step_avg:92.17ms
step:848/1645 train_time:78165ms step_avg:92.18ms
step:849/1645 train_time:78258ms step_avg:92.18ms
step:850/1645 train_time:78350ms step_avg:92.18ms
step:851/1645 train_time:78445ms step_avg:92.18ms
step:852/1645 train_time:78537ms step_avg:92.18ms
step:853/1645 train_time:78630ms step_avg:92.18ms
step:854/1645 train_time:78725ms step_avg:92.18ms
step:855/1645 train_time:78817ms step_avg:92.18ms
step:856/1645 train_time:78910ms step_avg:92.18ms
step:857/1645 train_time:79003ms step_avg:92.19ms
step:858/1645 train_time:79096ms step_avg:92.19ms
step:859/1645 train_time:79188ms step_avg:92.19ms
step:860/1645 train_time:79281ms step_avg:92.19ms
step:861/1645 train_time:79374ms step_avg:92.19ms
step:862/1645 train_time:79467ms step_avg:92.19ms
step:863/1645 train_time:79559ms step_avg:92.19ms
step:864/1645 train_time:79652ms step_avg:92.19ms
step:865/1645 train_time:79746ms step_avg:92.19ms
step:866/1645 train_time:79839ms step_avg:92.19ms
step:867/1645 train_time:79932ms step_avg:92.19ms
step:868/1645 train_time:80025ms step_avg:92.19ms
step:869/1645 train_time:80117ms step_avg:92.19ms
step:870/1645 train_time:80210ms step_avg:92.20ms
step:871/1645 train_time:80303ms step_avg:92.20ms
step:872/1645 train_time:80396ms step_avg:92.20ms
step:873/1645 train_time:80489ms step_avg:92.20ms
step:874/1645 train_time:80582ms step_avg:92.20ms
step:875/1645 train_time:80674ms step_avg:92.20ms
step:875/1645 val_loss:3.5143 train_time:80768ms step_avg:92.31ms
step:876/1645 train_time:80788ms step_avg:92.22ms
step:877/1645 train_time:80864ms step_avg:92.21ms
step:878/1645 train_time:80960ms step_avg:92.21ms
step:879/1645 train_time:81053ms step_avg:92.21ms
step:880/1645 train_time:81145ms step_avg:92.21ms
step:881/1645 train_time:81237ms step_avg:92.21ms
step:882/1645 train_time:81328ms step_avg:92.21ms
step:883/1645 train_time:81420ms step_avg:92.21ms
step:884/1645 train_time:81512ms step_avg:92.21ms
step:885/1645 train_time:81605ms step_avg:92.21ms
step:886/1645 train_time:81697ms step_avg:92.21ms
step:887/1645 train_time:81791ms step_avg:92.21ms
step:888/1645 train_time:81887ms step_avg:92.21ms
step:889/1645 train_time:81982ms step_avg:92.22ms
step:890/1645 train_time:82076ms step_avg:92.22ms
step:891/1645 train_time:82168ms step_avg:92.22ms
step:892/1645 train_time:82260ms step_avg:92.22ms
step:893/1645 train_time:82352ms step_avg:92.22ms
step:894/1645 train_time:82445ms step_avg:92.22ms
step:895/1645 train_time:82537ms step_avg:92.22ms
step:896/1645 train_time:82629ms step_avg:92.22ms
step:897/1645 train_time:82722ms step_avg:92.22ms
step:898/1645 train_time:82814ms step_avg:92.22ms
step:899/1645 train_time:82910ms step_avg:92.22ms
step:900/1645 train_time:83005ms step_avg:92.23ms
step:901/1645 train_time:83097ms step_avg:92.23ms
step:902/1645 train_time:83190ms step_avg:92.23ms
step:903/1645 train_time:83284ms step_avg:92.23ms
step:904/1645 train_time:83375ms step_avg:92.23ms
step:905/1645 train_time:83468ms step_avg:92.23ms
step:906/1645 train_time:83560ms step_avg:92.23ms
step:907/1645 train_time:83652ms step_avg:92.23ms
step:908/1645 train_time:83745ms step_avg:92.23ms
step:909/1645 train_time:83838ms step_avg:92.23ms
step:910/1645 train_time:83932ms step_avg:92.23ms
step:911/1645 train_time:84026ms step_avg:92.23ms
step:912/1645 train_time:84119ms step_avg:92.24ms
step:913/1645 train_time:84212ms step_avg:92.24ms
step:914/1645 train_time:84305ms step_avg:92.24ms
step:915/1645 train_time:84398ms step_avg:92.24ms
step:916/1645 train_time:84491ms step_avg:92.24ms
step:917/1645 train_time:84583ms step_avg:92.24ms
step:918/1645 train_time:84675ms step_avg:92.24ms
step:919/1645 train_time:84768ms step_avg:92.24ms
step:920/1645 train_time:84861ms step_avg:92.24ms
step:921/1645 train_time:84955ms step_avg:92.24ms
step:922/1645 train_time:85048ms step_avg:92.24ms
step:923/1645 train_time:85141ms step_avg:92.24ms
step:924/1645 train_time:85233ms step_avg:92.24ms
step:925/1645 train_time:85327ms step_avg:92.24ms
step:926/1645 train_time:85419ms step_avg:92.24ms
step:927/1645 train_time:85512ms step_avg:92.25ms
step:928/1645 train_time:85605ms step_avg:92.25ms
step:929/1645 train_time:85698ms step_avg:92.25ms
step:930/1645 train_time:85791ms step_avg:92.25ms
step:931/1645 train_time:85884ms step_avg:92.25ms
step:932/1645 train_time:85978ms step_avg:92.25ms
step:933/1645 train_time:86071ms step_avg:92.25ms
step:934/1645 train_time:86163ms step_avg:92.25ms
step:935/1645 train_time:86256ms step_avg:92.25ms
step:936/1645 train_time:86349ms step_avg:92.25ms
step:937/1645 train_time:86442ms step_avg:92.25ms
step:938/1645 train_time:86534ms step_avg:92.25ms
step:939/1645 train_time:86626ms step_avg:92.25ms
step:940/1645 train_time:86719ms step_avg:92.25ms
step:941/1645 train_time:86812ms step_avg:92.25ms
step:942/1645 train_time:86905ms step_avg:92.26ms
step:943/1645 train_time:86999ms step_avg:92.26ms
step:944/1645 train_time:87091ms step_avg:92.26ms
step:945/1645 train_time:87185ms step_avg:92.26ms
step:946/1645 train_time:87278ms step_avg:92.26ms
step:947/1645 train_time:87371ms step_avg:92.26ms
step:948/1645 train_time:87465ms step_avg:92.26ms
step:949/1645 train_time:87557ms step_avg:92.26ms
step:950/1645 train_time:87650ms step_avg:92.26ms
step:951/1645 train_time:87744ms step_avg:92.26ms
step:952/1645 train_time:87837ms step_avg:92.27ms
step:953/1645 train_time:87929ms step_avg:92.27ms
step:954/1645 train_time:88022ms step_avg:92.27ms
step:955/1645 train_time:88114ms step_avg:92.27ms
step:956/1645 train_time:88208ms step_avg:92.27ms
step:957/1645 train_time:88300ms step_avg:92.27ms
step:958/1645 train_time:88392ms step_avg:92.27ms
step:959/1645 train_time:88486ms step_avg:92.27ms
step:960/1645 train_time:88579ms step_avg:92.27ms
step:961/1645 train_time:88672ms step_avg:92.27ms
step:962/1645 train_time:88766ms step_avg:92.27ms
step:963/1645 train_time:88859ms step_avg:92.27ms
step:964/1645 train_time:88951ms step_avg:92.27ms
step:965/1645 train_time:89045ms step_avg:92.27ms
step:966/1645 train_time:89137ms step_avg:92.27ms
step:967/1645 train_time:89230ms step_avg:92.28ms
step:968/1645 train_time:89323ms step_avg:92.28ms
step:969/1645 train_time:89417ms step_avg:92.28ms
step:970/1645 train_time:89510ms step_avg:92.28ms
step:971/1645 train_time:89603ms step_avg:92.28ms
step:972/1645 train_time:89696ms step_avg:92.28ms
step:973/1645 train_time:89790ms step_avg:92.28ms
step:974/1645 train_time:89883ms step_avg:92.28ms
step:975/1645 train_time:89975ms step_avg:92.28ms
step:976/1645 train_time:90068ms step_avg:92.28ms
step:977/1645 train_time:90161ms step_avg:92.28ms
step:978/1645 train_time:90254ms step_avg:92.28ms
step:979/1645 train_time:90347ms step_avg:92.28ms
step:980/1645 train_time:90440ms step_avg:92.29ms
step:981/1645 train_time:90532ms step_avg:92.29ms
step:982/1645 train_time:90625ms step_avg:92.29ms
step:983/1645 train_time:90718ms step_avg:92.29ms
step:984/1645 train_time:90811ms step_avg:92.29ms
step:985/1645 train_time:90904ms step_avg:92.29ms
step:986/1645 train_time:90997ms step_avg:92.29ms
step:987/1645 train_time:91090ms step_avg:92.29ms
step:988/1645 train_time:91183ms step_avg:92.29ms
step:989/1645 train_time:91276ms step_avg:92.29ms
step:990/1645 train_time:91369ms step_avg:92.29ms
step:991/1645 train_time:91462ms step_avg:92.29ms
step:992/1645 train_time:91555ms step_avg:92.29ms
step:993/1645 train_time:91648ms step_avg:92.29ms
step:994/1645 train_time:91741ms step_avg:92.30ms
step:995/1645 train_time:91835ms step_avg:92.30ms
step:996/1645 train_time:91927ms step_avg:92.30ms
step:997/1645 train_time:92021ms step_avg:92.30ms
step:998/1645 train_time:92113ms step_avg:92.30ms
step:999/1645 train_time:92206ms step_avg:92.30ms
step:1000/1645 train_time:92299ms step_avg:92.30ms
step:1000/1645 val_loss:3.4659 train_time:92392ms step_avg:92.39ms
step:1001/1645 train_time:92417ms step_avg:92.32ms
step:1002/1645 train_time:92490ms step_avg:92.31ms
step:1003/1645 train_time:92586ms step_avg:92.31ms
step:1004/1645 train_time:92679ms step_avg:92.31ms
step:1005/1645 train_time:92771ms step_avg:92.31ms
step:1006/1645 train_time:92862ms step_avg:92.31ms
step:1007/1645 train_time:92954ms step_avg:92.31ms
step:1008/1645 train_time:93046ms step_avg:92.31ms
step:1009/1645 train_time:93138ms step_avg:92.31ms
step:1010/1645 train_time:93230ms step_avg:92.31ms
step:1011/1645 train_time:93323ms step_avg:92.31ms
step:1012/1645 train_time:93417ms step_avg:92.31ms
step:1013/1645 train_time:93513ms step_avg:92.31ms
step:1014/1645 train_time:93607ms step_avg:92.31ms
step:1015/1645 train_time:93700ms step_avg:92.32ms
step:1016/1645 train_time:93793ms step_avg:92.32ms
step:1017/1645 train_time:93885ms step_avg:92.32ms
step:1018/1645 train_time:93977ms step_avg:92.31ms
step:1019/1645 train_time:94069ms step_avg:92.31ms
step:1020/1645 train_time:94161ms step_avg:92.31ms
step:1021/1645 train_time:94253ms step_avg:92.31ms
step:1022/1645 train_time:94347ms step_avg:92.32ms
step:1023/1645 train_time:94441ms step_avg:92.32ms
step:1024/1645 train_time:94534ms step_avg:92.32ms
step:1025/1645 train_time:94627ms step_avg:92.32ms
step:1026/1645 train_time:94720ms step_avg:92.32ms
step:1027/1645 train_time:94813ms step_avg:92.32ms
step:1028/1645 train_time:94906ms step_avg:92.32ms
step:1029/1645 train_time:94998ms step_avg:92.32ms
step:1030/1645 train_time:95090ms step_avg:92.32ms
step:1031/1645 train_time:95183ms step_avg:92.32ms
step:1032/1645 train_time:95275ms step_avg:92.32ms
step:1033/1645 train_time:95368ms step_avg:92.32ms
step:1034/1645 train_time:95463ms step_avg:92.32ms
step:1035/1645 train_time:95555ms step_avg:92.32ms
step:1036/1645 train_time:95649ms step_avg:92.33ms
step:1037/1645 train_time:95741ms step_avg:92.33ms
step:1038/1645 train_time:95834ms step_avg:92.33ms
step:1039/1645 train_time:95928ms step_avg:92.33ms
step:1040/1645 train_time:96020ms step_avg:92.33ms
step:1041/1645 train_time:96112ms step_avg:92.33ms
step:1042/1645 train_time:96205ms step_avg:92.33ms
step:1043/1645 train_time:96298ms step_avg:92.33ms
step:1044/1645 train_time:96390ms step_avg:92.33ms
step:1045/1645 train_time:96485ms step_avg:92.33ms
step:1046/1645 train_time:96578ms step_avg:92.33ms
step:1047/1645 train_time:96671ms step_avg:92.33ms
step:1048/1645 train_time:96763ms step_avg:92.33ms
step:1049/1645 train_time:96856ms step_avg:92.33ms
step:1050/1645 train_time:96949ms step_avg:92.33ms
step:1051/1645 train_time:97041ms step_avg:92.33ms
step:1052/1645 train_time:97133ms step_avg:92.33ms
step:1053/1645 train_time:97226ms step_avg:92.33ms
step:1054/1645 train_time:97319ms step_avg:92.33ms
step:1055/1645 train_time:97413ms step_avg:92.33ms
step:1056/1645 train_time:97507ms step_avg:92.34ms
step:1057/1645 train_time:97600ms step_avg:92.34ms
step:1058/1645 train_time:97693ms step_avg:92.34ms
step:1059/1645 train_time:97788ms step_avg:92.34ms
step:1060/1645 train_time:97880ms step_avg:92.34ms
step:1061/1645 train_time:97972ms step_avg:92.34ms
step:1062/1645 train_time:98066ms step_avg:92.34ms
step:1063/1645 train_time:98158ms step_avg:92.34ms
step:1064/1645 train_time:98250ms step_avg:92.34ms
step:1065/1645 train_time:98344ms step_avg:92.34ms
step:1066/1645 train_time:98436ms step_avg:92.34ms
step:1067/1645 train_time:98530ms step_avg:92.34ms
step:1068/1645 train_time:98623ms step_avg:92.34ms
step:1069/1645 train_time:98716ms step_avg:92.34ms
step:1070/1645 train_time:98810ms step_avg:92.35ms
step:1071/1645 train_time:98902ms step_avg:92.35ms
step:1072/1645 train_time:98996ms step_avg:92.35ms
step:1073/1645 train_time:99088ms step_avg:92.35ms
step:1074/1645 train_time:99180ms step_avg:92.35ms
step:1075/1645 train_time:99272ms step_avg:92.35ms
step:1076/1645 train_time:99365ms step_avg:92.35ms
step:1077/1645 train_time:99458ms step_avg:92.35ms
step:1078/1645 train_time:99551ms step_avg:92.35ms
step:1079/1645 train_time:99644ms step_avg:92.35ms
step:1080/1645 train_time:99737ms step_avg:92.35ms
step:1081/1645 train_time:99830ms step_avg:92.35ms
step:1082/1645 train_time:99924ms step_avg:92.35ms
step:1083/1645 train_time:100018ms step_avg:92.35ms
step:1084/1645 train_time:100110ms step_avg:92.35ms
step:1085/1645 train_time:100203ms step_avg:92.35ms
step:1086/1645 train_time:100296ms step_avg:92.35ms
step:1087/1645 train_time:100389ms step_avg:92.35ms
step:1088/1645 train_time:100482ms step_avg:92.35ms
step:1089/1645 train_time:100574ms step_avg:92.35ms
step:1090/1645 train_time:100667ms step_avg:92.36ms
step:1091/1645 train_time:100760ms step_avg:92.36ms
step:1092/1645 train_time:100853ms step_avg:92.36ms
step:1093/1645 train_time:100946ms step_avg:92.36ms
step:1094/1645 train_time:101039ms step_avg:92.36ms
step:1095/1645 train_time:101132ms step_avg:92.36ms
step:1096/1645 train_time:101225ms step_avg:92.36ms
step:1097/1645 train_time:101318ms step_avg:92.36ms
step:1098/1645 train_time:101411ms step_avg:92.36ms
step:1099/1645 train_time:101506ms step_avg:92.36ms
step:1100/1645 train_time:101599ms step_avg:92.36ms
step:1101/1645 train_time:101692ms step_avg:92.36ms
step:1102/1645 train_time:101786ms step_avg:92.37ms
step:1103/1645 train_time:101879ms step_avg:92.37ms
step:1104/1645 train_time:101973ms step_avg:92.37ms
step:1105/1645 train_time:102067ms step_avg:92.37ms
step:1106/1645 train_time:102159ms step_avg:92.37ms
step:1107/1645 train_time:102253ms step_avg:92.37ms
step:1108/1645 train_time:102347ms step_avg:92.37ms
step:1109/1645 train_time:102440ms step_avg:92.37ms
step:1110/1645 train_time:102533ms step_avg:92.37ms
step:1111/1645 train_time:102628ms step_avg:92.37ms
step:1112/1645 train_time:102722ms step_avg:92.38ms
step:1113/1645 train_time:102814ms step_avg:92.38ms
step:1114/1645 train_time:102909ms step_avg:92.38ms
step:1115/1645 train_time:103002ms step_avg:92.38ms
step:1116/1645 train_time:103096ms step_avg:92.38ms
step:1117/1645 train_time:103189ms step_avg:92.38ms
step:1118/1645 train_time:103282ms step_avg:92.38ms
step:1119/1645 train_time:103375ms step_avg:92.38ms
step:1120/1645 train_time:103469ms step_avg:92.38ms
step:1121/1645 train_time:103562ms step_avg:92.38ms
step:1122/1645 train_time:103656ms step_avg:92.39ms
step:1123/1645 train_time:103750ms step_avg:92.39ms
step:1124/1645 train_time:103843ms step_avg:92.39ms
step:1125/1645 train_time:103936ms step_avg:92.39ms
step:1125/1645 val_loss:3.4121 train_time:104031ms step_avg:92.47ms
step:1126/1645 train_time:104057ms step_avg:92.41ms
step:1127/1645 train_time:104133ms step_avg:92.40ms
step:1128/1645 train_time:104235ms step_avg:92.41ms
step:1129/1645 train_time:104329ms step_avg:92.41ms
step:1130/1645 train_time:104421ms step_avg:92.41ms
step:1131/1645 train_time:104514ms step_avg:92.41ms
step:1132/1645 train_time:104607ms step_avg:92.41ms
step:1133/1645 train_time:104700ms step_avg:92.41ms
step:1134/1645 train_time:104793ms step_avg:92.41ms
step:1135/1645 train_time:104885ms step_avg:92.41ms
step:1136/1645 train_time:104978ms step_avg:92.41ms
step:1137/1645 train_time:105074ms step_avg:92.41ms
step:1138/1645 train_time:105171ms step_avg:92.42ms
step:1139/1645 train_time:105267ms step_avg:92.42ms
step:1140/1645 train_time:105361ms step_avg:92.42ms
step:1141/1645 train_time:105455ms step_avg:92.42ms
step:1142/1645 train_time:105548ms step_avg:92.42ms
step:1143/1645 train_time:105640ms step_avg:92.42ms
step:1144/1645 train_time:105733ms step_avg:92.42ms
step:1145/1645 train_time:105826ms step_avg:92.42ms
step:1146/1645 train_time:105918ms step_avg:92.42ms
step:1147/1645 train_time:106012ms step_avg:92.43ms
step:1148/1645 train_time:106107ms step_avg:92.43ms
step:1149/1645 train_time:106201ms step_avg:92.43ms
step:1150/1645 train_time:106296ms step_avg:92.43ms
step:1151/1645 train_time:106390ms step_avg:92.43ms
step:1152/1645 train_time:106483ms step_avg:92.43ms
step:1153/1645 train_time:106576ms step_avg:92.43ms
step:1154/1645 train_time:106669ms step_avg:92.43ms
step:1155/1645 train_time:106762ms step_avg:92.43ms
step:1156/1645 train_time:106856ms step_avg:92.44ms
step:1157/1645 train_time:106950ms step_avg:92.44ms
step:1158/1645 train_time:107044ms step_avg:92.44ms
step:1159/1645 train_time:107138ms step_avg:92.44ms
step:1160/1645 train_time:107232ms step_avg:92.44ms
step:1161/1645 train_time:107326ms step_avg:92.44ms
step:1162/1645 train_time:107420ms step_avg:92.44ms
step:1163/1645 train_time:107514ms step_avg:92.45ms
step:1164/1645 train_time:107607ms step_avg:92.45ms
step:1165/1645 train_time:107700ms step_avg:92.45ms
step:1166/1645 train_time:107793ms step_avg:92.45ms
step:1167/1645 train_time:107886ms step_avg:92.45ms
step:1168/1645 train_time:107979ms step_avg:92.45ms
step:1169/1645 train_time:108073ms step_avg:92.45ms
step:1170/1645 train_time:108167ms step_avg:92.45ms
step:1171/1645 train_time:108261ms step_avg:92.45ms
step:1172/1645 train_time:108355ms step_avg:92.45ms
step:1173/1645 train_time:108449ms step_avg:92.45ms
step:1174/1645 train_time:108543ms step_avg:92.46ms
step:1175/1645 train_time:108636ms step_avg:92.46ms
step:1176/1645 train_time:108730ms step_avg:92.46ms
step:1177/1645 train_time:108823ms step_avg:92.46ms
step:1178/1645 train_time:108916ms step_avg:92.46ms
step:1179/1645 train_time:109009ms step_avg:92.46ms
step:1180/1645 train_time:109103ms step_avg:92.46ms
step:1181/1645 train_time:109197ms step_avg:92.46ms
step:1182/1645 train_time:109290ms step_avg:92.46ms
step:1183/1645 train_time:109383ms step_avg:92.46ms
step:1184/1645 train_time:109477ms step_avg:92.46ms
step:1185/1645 train_time:109572ms step_avg:92.47ms
step:1186/1645 train_time:109666ms step_avg:92.47ms
step:1187/1645 train_time:109758ms step_avg:92.47ms
step:1188/1645 train_time:109852ms step_avg:92.47ms
step:1189/1645 train_time:109946ms step_avg:92.47ms
step:1190/1645 train_time:110039ms step_avg:92.47ms
step:1191/1645 train_time:110133ms step_avg:92.47ms
step:1192/1645 train_time:110226ms step_avg:92.47ms
step:1193/1645 train_time:110320ms step_avg:92.47ms
step:1194/1645 train_time:110413ms step_avg:92.47ms
step:1195/1645 train_time:110507ms step_avg:92.47ms
step:1196/1645 train_time:110601ms step_avg:92.48ms
step:1197/1645 train_time:110694ms step_avg:92.48ms
step:1198/1645 train_time:110788ms step_avg:92.48ms
step:1199/1645 train_time:110881ms step_avg:92.48ms
step:1200/1645 train_time:110975ms step_avg:92.48ms
step:1201/1645 train_time:111068ms step_avg:92.48ms
step:1202/1645 train_time:111162ms step_avg:92.48ms
step:1203/1645 train_time:111256ms step_avg:92.48ms
step:1204/1645 train_time:111349ms step_avg:92.48ms
step:1205/1645 train_time:111443ms step_avg:92.48ms
step:1206/1645 train_time:111537ms step_avg:92.49ms
step:1207/1645 train_time:111631ms step_avg:92.49ms
step:1208/1645 train_time:111724ms step_avg:92.49ms
step:1209/1645 train_time:111818ms step_avg:92.49ms
step:1210/1645 train_time:111911ms step_avg:92.49ms
step:1211/1645 train_time:112005ms step_avg:92.49ms
step:1212/1645 train_time:112098ms step_avg:92.49ms
step:1213/1645 train_time:112191ms step_avg:92.49ms
step:1214/1645 train_time:112284ms step_avg:92.49ms
step:1215/1645 train_time:112378ms step_avg:92.49ms
step:1216/1645 train_time:112472ms step_avg:92.49ms
step:1217/1645 train_time:112566ms step_avg:92.49ms
step:1218/1645 train_time:112659ms step_avg:92.50ms
step:1219/1645 train_time:112753ms step_avg:92.50ms
step:1220/1645 train_time:112847ms step_avg:92.50ms
step:1221/1645 train_time:112940ms step_avg:92.50ms
step:1222/1645 train_time:113034ms step_avg:92.50ms
step:1223/1645 train_time:113127ms step_avg:92.50ms
step:1224/1645 train_time:113220ms step_avg:92.50ms
step:1225/1645 train_time:113314ms step_avg:92.50ms
step:1226/1645 train_time:113407ms step_avg:92.50ms
step:1227/1645 train_time:113501ms step_avg:92.50ms
step:1228/1645 train_time:113595ms step_avg:92.50ms
step:1229/1645 train_time:113688ms step_avg:92.50ms
step:1230/1645 train_time:113783ms step_avg:92.51ms
step:1231/1645 train_time:113876ms step_avg:92.51ms
step:1232/1645 train_time:113970ms step_avg:92.51ms
step:1233/1645 train_time:114064ms step_avg:92.51ms
step:1234/1645 train_time:114157ms step_avg:92.51ms
step:1235/1645 train_time:114251ms step_avg:92.51ms
step:1236/1645 train_time:114345ms step_avg:92.51ms
step:1237/1645 train_time:114438ms step_avg:92.51ms
step:1238/1645 train_time:114532ms step_avg:92.51ms
step:1239/1645 train_time:114625ms step_avg:92.51ms
step:1240/1645 train_time:114719ms step_avg:92.52ms
step:1241/1645 train_time:114813ms step_avg:92.52ms
step:1242/1645 train_time:114907ms step_avg:92.52ms
step:1243/1645 train_time:115000ms step_avg:92.52ms
step:1244/1645 train_time:115093ms step_avg:92.52ms
step:1245/1645 train_time:115186ms step_avg:92.52ms
step:1246/1645 train_time:115280ms step_avg:92.52ms
step:1247/1645 train_time:115373ms step_avg:92.52ms
step:1248/1645 train_time:115466ms step_avg:92.52ms
step:1249/1645 train_time:115559ms step_avg:92.52ms
step:1250/1645 train_time:115654ms step_avg:92.52ms
step:1250/1645 val_loss:3.3733 train_time:115747ms step_avg:92.60ms
step:1251/1645 train_time:115773ms step_avg:92.54ms
step:1252/1645 train_time:115848ms step_avg:92.53ms
step:1253/1645 train_time:115942ms step_avg:92.53ms
step:1254/1645 train_time:116035ms step_avg:92.53ms
step:1255/1645 train_time:116127ms step_avg:92.53ms
step:1256/1645 train_time:116219ms step_avg:92.53ms
step:1257/1645 train_time:116312ms step_avg:92.53ms
step:1258/1645 train_time:116404ms step_avg:92.53ms
step:1259/1645 train_time:116497ms step_avg:92.53ms
step:1260/1645 train_time:116590ms step_avg:92.53ms
step:1261/1645 train_time:116685ms step_avg:92.53ms
step:1262/1645 train_time:116783ms step_avg:92.54ms
step:1263/1645 train_time:116879ms step_avg:92.54ms
step:1264/1645 train_time:116973ms step_avg:92.54ms
step:1265/1645 train_time:117066ms step_avg:92.54ms
step:1266/1645 train_time:117160ms step_avg:92.54ms
step:1267/1645 train_time:117252ms step_avg:92.54ms
step:1268/1645 train_time:117345ms step_avg:92.54ms
step:1269/1645 train_time:117438ms step_avg:92.54ms
step:1270/1645 train_time:117531ms step_avg:92.54ms
step:1271/1645 train_time:117624ms step_avg:92.54ms
step:1272/1645 train_time:117719ms step_avg:92.55ms
step:1273/1645 train_time:117813ms step_avg:92.55ms
step:1274/1645 train_time:117909ms step_avg:92.55ms
step:1275/1645 train_time:118003ms step_avg:92.55ms
step:1276/1645 train_time:118096ms step_avg:92.55ms
step:1277/1645 train_time:118188ms step_avg:92.55ms
step:1278/1645 train_time:118282ms step_avg:92.55ms
step:1279/1645 train_time:118374ms step_avg:92.55ms
step:1280/1645 train_time:118467ms step_avg:92.55ms
step:1281/1645 train_time:118560ms step_avg:92.55ms
step:1282/1645 train_time:118654ms step_avg:92.55ms
step:1283/1645 train_time:118748ms step_avg:92.55ms
step:1284/1645 train_time:118842ms step_avg:92.56ms
step:1285/1645 train_time:118936ms step_avg:92.56ms
step:1286/1645 train_time:119029ms step_avg:92.56ms
step:1287/1645 train_time:119123ms step_avg:92.56ms
step:1288/1645 train_time:119216ms step_avg:92.56ms
step:1289/1645 train_time:119310ms step_avg:92.56ms
step:1290/1645 train_time:119403ms step_avg:92.56ms
step:1291/1645 train_time:119496ms step_avg:92.56ms
step:1292/1645 train_time:119590ms step_avg:92.56ms
step:1293/1645 train_time:119684ms step_avg:92.56ms
step:1294/1645 train_time:119778ms step_avg:92.56ms
step:1295/1645 train_time:119872ms step_avg:92.57ms
step:1296/1645 train_time:119966ms step_avg:92.57ms
step:1297/1645 train_time:120060ms step_avg:92.57ms
step:1298/1645 train_time:120154ms step_avg:92.57ms
step:1299/1645 train_time:120248ms step_avg:92.57ms
step:1300/1645 train_time:120341ms step_avg:92.57ms
step:1301/1645 train_time:120433ms step_avg:92.57ms
step:1302/1645 train_time:120527ms step_avg:92.57ms
step:1303/1645 train_time:120620ms step_avg:92.57ms
step:1304/1645 train_time:120714ms step_avg:92.57ms
step:1305/1645 train_time:120808ms step_avg:92.57ms
step:1306/1645 train_time:120905ms step_avg:92.58ms
step:1307/1645 train_time:120998ms step_avg:92.58ms
step:1308/1645 train_time:121090ms step_avg:92.58ms
step:1309/1645 train_time:121185ms step_avg:92.58ms
step:1310/1645 train_time:121278ms step_avg:92.58ms
step:1311/1645 train_time:121371ms step_avg:92.58ms
step:1312/1645 train_time:121464ms step_avg:92.58ms
step:1313/1645 train_time:121558ms step_avg:92.58ms
step:1314/1645 train_time:121652ms step_avg:92.58ms
step:1315/1645 train_time:121745ms step_avg:92.58ms
step:1316/1645 train_time:121839ms step_avg:92.58ms
step:1317/1645 train_time:121933ms step_avg:92.58ms
step:1318/1645 train_time:122027ms step_avg:92.58ms
step:1319/1645 train_time:122120ms step_avg:92.59ms
step:1320/1645 train_time:122213ms step_avg:92.59ms
step:1321/1645 train_time:122307ms step_avg:92.59ms
step:1322/1645 train_time:122400ms step_avg:92.59ms
step:1323/1645 train_time:122493ms step_avg:92.59ms
step:1324/1645 train_time:122587ms step_avg:92.59ms
step:1325/1645 train_time:122681ms step_avg:92.59ms
step:1326/1645 train_time:122776ms step_avg:92.59ms
step:1327/1645 train_time:122869ms step_avg:92.59ms
step:1328/1645 train_time:122963ms step_avg:92.59ms
step:1329/1645 train_time:123057ms step_avg:92.59ms
step:1330/1645 train_time:123150ms step_avg:92.59ms
step:1331/1645 train_time:123244ms step_avg:92.59ms
step:1332/1645 train_time:123337ms step_avg:92.60ms
step:1333/1645 train_time:123430ms step_avg:92.60ms
step:1334/1645 train_time:123524ms step_avg:92.60ms
step:1335/1645 train_time:123619ms step_avg:92.60ms
step:1336/1645 train_time:123712ms step_avg:92.60ms
step:1337/1645 train_time:123805ms step_avg:92.60ms
step:1338/1645 train_time:123899ms step_avg:92.60ms
step:1339/1645 train_time:123992ms step_avg:92.60ms
step:1340/1645 train_time:124085ms step_avg:92.60ms
step:1341/1645 train_time:124180ms step_avg:92.60ms
step:1342/1645 train_time:124274ms step_avg:92.60ms
step:1343/1645 train_time:124367ms step_avg:92.60ms
step:1344/1645 train_time:124460ms step_avg:92.60ms
step:1345/1645 train_time:124553ms step_avg:92.60ms
step:1346/1645 train_time:124647ms step_avg:92.61ms
step:1347/1645 train_time:124740ms step_avg:92.61ms
step:1348/1645 train_time:124834ms step_avg:92.61ms
step:1349/1645 train_time:124928ms step_avg:92.61ms
step:1350/1645 train_time:125021ms step_avg:92.61ms
step:1351/1645 train_time:125115ms step_avg:92.61ms
step:1352/1645 train_time:125209ms step_avg:92.61ms
step:1353/1645 train_time:125303ms step_avg:92.61ms
step:1354/1645 train_time:125396ms step_avg:92.61ms
step:1355/1645 train_time:125488ms step_avg:92.61ms
step:1356/1645 train_time:125583ms step_avg:92.61ms
step:1357/1645 train_time:125677ms step_avg:92.61ms
step:1358/1645 train_time:125771ms step_avg:92.62ms
step:1359/1645 train_time:125864ms step_avg:92.62ms
step:1360/1645 train_time:125957ms step_avg:92.62ms
step:1361/1645 train_time:126051ms step_avg:92.62ms
step:1362/1645 train_time:126146ms step_avg:92.62ms
step:1363/1645 train_time:126239ms step_avg:92.62ms
step:1364/1645 train_time:126332ms step_avg:92.62ms
step:1365/1645 train_time:126427ms step_avg:92.62ms
step:1366/1645 train_time:126520ms step_avg:92.62ms
step:1367/1645 train_time:126613ms step_avg:92.62ms
step:1368/1645 train_time:126706ms step_avg:92.62ms
step:1369/1645 train_time:126800ms step_avg:92.62ms
step:1370/1645 train_time:126893ms step_avg:92.62ms
step:1371/1645 train_time:126986ms step_avg:92.62ms
step:1372/1645 train_time:127081ms step_avg:92.62ms
step:1373/1645 train_time:127176ms step_avg:92.63ms
step:1374/1645 train_time:127269ms step_avg:92.63ms
step:1375/1645 train_time:127363ms step_avg:92.63ms
step:1375/1645 val_loss:3.3391 train_time:127456ms step_avg:92.70ms
step:1376/1645 train_time:127477ms step_avg:92.64ms
step:1377/1645 train_time:127555ms step_avg:92.63ms
step:1378/1645 train_time:127650ms step_avg:92.63ms
step:1379/1645 train_time:127743ms step_avg:92.63ms
step:1380/1645 train_time:127836ms step_avg:92.63ms
step:1381/1645 train_time:127928ms step_avg:92.63ms
step:1382/1645 train_time:128021ms step_avg:92.63ms
step:1383/1645 train_time:128113ms step_avg:92.63ms
step:1384/1645 train_time:128206ms step_avg:92.63ms
step:1385/1645 train_time:128300ms step_avg:92.64ms
step:1386/1645 train_time:128394ms step_avg:92.64ms
step:1387/1645 train_time:128489ms step_avg:92.64ms
step:1388/1645 train_time:128584ms step_avg:92.64ms
step:1389/1645 train_time:128679ms step_avg:92.64ms
step:1390/1645 train_time:128773ms step_avg:92.64ms
step:1391/1645 train_time:128866ms step_avg:92.64ms
step:1392/1645 train_time:128960ms step_avg:92.64ms
step:1393/1645 train_time:129052ms step_avg:92.64ms
step:1394/1645 train_time:129145ms step_avg:92.64ms
step:1395/1645 train_time:129237ms step_avg:92.64ms
step:1396/1645 train_time:129330ms step_avg:92.64ms
step:1397/1645 train_time:129424ms step_avg:92.64ms
step:1398/1645 train_time:129522ms step_avg:92.65ms
step:1399/1645 train_time:129617ms step_avg:92.65ms
step:1400/1645 train_time:129710ms step_avg:92.65ms
step:1401/1645 train_time:129804ms step_avg:92.65ms
step:1402/1645 train_time:129898ms step_avg:92.65ms
step:1403/1645 train_time:129992ms step_avg:92.65ms
step:1404/1645 train_time:130085ms step_avg:92.65ms
step:1405/1645 train_time:130177ms step_avg:92.65ms
step:1406/1645 train_time:130270ms step_avg:92.65ms
step:1407/1645 train_time:130363ms step_avg:92.65ms
step:1408/1645 train_time:130457ms step_avg:92.65ms
step:1409/1645 train_time:130552ms step_avg:92.66ms
step:1410/1645 train_time:130645ms step_avg:92.66ms
step:1411/1645 train_time:130739ms step_avg:92.66ms
step:1412/1645 train_time:130832ms step_avg:92.66ms
step:1413/1645 train_time:130925ms step_avg:92.66ms
step:1414/1645 train_time:131019ms step_avg:92.66ms
step:1415/1645 train_time:131112ms step_avg:92.66ms
step:1416/1645 train_time:131206ms step_avg:92.66ms
step:1417/1645 train_time:131299ms step_avg:92.66ms
step:1418/1645 train_time:131391ms step_avg:92.66ms
step:1419/1645 train_time:131485ms step_avg:92.66ms
step:1420/1645 train_time:131579ms step_avg:92.66ms
step:1421/1645 train_time:131673ms step_avg:92.66ms
step:1422/1645 train_time:131766ms step_avg:92.66ms
step:1423/1645 train_time:131860ms step_avg:92.66ms
step:1424/1645 train_time:131953ms step_avg:92.66ms
step:1425/1645 train_time:132047ms step_avg:92.66ms
step:1426/1645 train_time:132141ms step_avg:92.67ms
step:1427/1645 train_time:132235ms step_avg:92.67ms
step:1428/1645 train_time:132327ms step_avg:92.67ms
step:1429/1645 train_time:132421ms step_avg:92.67ms
step:1430/1645 train_time:132516ms step_avg:92.67ms
step:1431/1645 train_time:132609ms step_avg:92.67ms
step:1432/1645 train_time:132703ms step_avg:92.67ms
step:1433/1645 train_time:132797ms step_avg:92.67ms
step:1434/1645 train_time:132892ms step_avg:92.67ms
step:1435/1645 train_time:132985ms step_avg:92.67ms
step:1436/1645 train_time:133079ms step_avg:92.67ms
step:1437/1645 train_time:133172ms step_avg:92.67ms
step:1438/1645 train_time:133264ms step_avg:92.67ms
step:1439/1645 train_time:133358ms step_avg:92.67ms
step:1440/1645 train_time:133453ms step_avg:92.68ms
step:1441/1645 train_time:133546ms step_avg:92.68ms
step:1442/1645 train_time:133641ms step_avg:92.68ms
step:1443/1645 train_time:133734ms step_avg:92.68ms
step:1444/1645 train_time:133828ms step_avg:92.68ms
step:1445/1645 train_time:133922ms step_avg:92.68ms
step:1446/1645 train_time:134017ms step_avg:92.68ms
step:1447/1645 train_time:134110ms step_avg:92.68ms
step:1448/1645 train_time:134203ms step_avg:92.68ms
step:1449/1645 train_time:134297ms step_avg:92.68ms
step:1450/1645 train_time:134391ms step_avg:92.68ms
step:1451/1645 train_time:134484ms step_avg:92.68ms
step:1452/1645 train_time:134578ms step_avg:92.68ms
step:1453/1645 train_time:134672ms step_avg:92.69ms
step:1454/1645 train_time:134765ms step_avg:92.69ms
step:1455/1645 train_time:134860ms step_avg:92.69ms
step:1456/1645 train_time:134955ms step_avg:92.69ms
step:1457/1645 train_time:135049ms step_avg:92.69ms
step:1458/1645 train_time:135141ms step_avg:92.69ms
step:1459/1645 train_time:135235ms step_avg:92.69ms
step:1460/1645 train_time:135329ms step_avg:92.69ms
step:1461/1645 train_time:135422ms step_avg:92.69ms
step:1462/1645 train_time:135518ms step_avg:92.69ms
step:1463/1645 train_time:135611ms step_avg:92.69ms
step:1464/1645 train_time:135705ms step_avg:92.69ms
step:1465/1645 train_time:135799ms step_avg:92.70ms
step:1466/1645 train_time:135893ms step_avg:92.70ms
step:1467/1645 train_time:135986ms step_avg:92.70ms
step:1468/1645 train_time:136079ms step_avg:92.70ms
step:1469/1645 train_time:136173ms step_avg:92.70ms
step:1470/1645 train_time:136266ms step_avg:92.70ms
step:1471/1645 train_time:136360ms step_avg:92.70ms
step:1472/1645 train_time:136455ms step_avg:92.70ms
step:1473/1645 train_time:136549ms step_avg:92.70ms
step:1474/1645 train_time:136642ms step_avg:92.70ms
step:1475/1645 train_time:136736ms step_avg:92.70ms
step:1476/1645 train_time:136829ms step_avg:92.70ms
step:1477/1645 train_time:136923ms step_avg:92.70ms
step:1478/1645 train_time:137017ms step_avg:92.70ms
step:1479/1645 train_time:137111ms step_avg:92.71ms
step:1480/1645 train_time:137204ms step_avg:92.71ms
step:1481/1645 train_time:137297ms step_avg:92.71ms
step:1482/1645 train_time:137390ms step_avg:92.71ms
step:1483/1645 train_time:137483ms step_avg:92.71ms
step:1484/1645 train_time:137577ms step_avg:92.71ms
step:1485/1645 train_time:137671ms step_avg:92.71ms
step:1486/1645 train_time:137766ms step_avg:92.71ms
step:1487/1645 train_time:137860ms step_avg:92.71ms
step:1488/1645 train_time:137953ms step_avg:92.71ms
step:1489/1645 train_time:138046ms step_avg:92.71ms
step:1490/1645 train_time:138139ms step_avg:92.71ms
step:1491/1645 train_time:138232ms step_avg:92.71ms
step:1492/1645 train_time:138325ms step_avg:92.71ms
step:1493/1645 train_time:138418ms step_avg:92.71ms
step:1494/1645 train_time:138512ms step_avg:92.71ms
step:1495/1645 train_time:138605ms step_avg:92.71ms
step:1496/1645 train_time:138698ms step_avg:92.71ms
step:1497/1645 train_time:138792ms step_avg:92.71ms
step:1498/1645 train_time:138885ms step_avg:92.71ms
step:1499/1645 train_time:138979ms step_avg:92.71ms
step:1500/1645 train_time:139073ms step_avg:92.72ms
step:1500/1645 val_loss:3.3094 train_time:139166ms step_avg:92.78ms
step:1501/1645 train_time:139191ms step_avg:92.73ms
step:1502/1645 train_time:139263ms step_avg:92.72ms
step:1503/1645 train_time:139359ms step_avg:92.72ms
step:1504/1645 train_time:139453ms step_avg:92.72ms
step:1505/1645 train_time:139547ms step_avg:92.72ms
step:1506/1645 train_time:139639ms step_avg:92.72ms
step:1507/1645 train_time:139732ms step_avg:92.72ms
step:1508/1645 train_time:139825ms step_avg:92.72ms
step:1509/1645 train_time:139917ms step_avg:92.72ms
step:1510/1645 train_time:140010ms step_avg:92.72ms
step:1511/1645 train_time:140104ms step_avg:92.72ms
step:1512/1645 train_time:140200ms step_avg:92.72ms
step:1513/1645 train_time:140295ms step_avg:92.73ms
step:1514/1645 train_time:140390ms step_avg:92.73ms
step:1515/1645 train_time:140484ms step_avg:92.73ms
step:1516/1645 train_time:140577ms step_avg:92.73ms
step:1517/1645 train_time:140670ms step_avg:92.73ms
step:1518/1645 train_time:140762ms step_avg:92.73ms
step:1519/1645 train_time:140856ms step_avg:92.73ms
step:1520/1645 train_time:140950ms step_avg:92.73ms
step:1521/1645 train_time:141043ms step_avg:92.73ms
step:1522/1645 train_time:141138ms step_avg:92.73ms
step:1523/1645 train_time:141231ms step_avg:92.73ms
step:1524/1645 train_time:141325ms step_avg:92.73ms
step:1525/1645 train_time:141419ms step_avg:92.73ms
step:1526/1645 train_time:141513ms step_avg:92.73ms
step:1527/1645 train_time:141607ms step_avg:92.74ms
step:1528/1645 train_time:141700ms step_avg:92.74ms
step:1529/1645 train_time:141793ms step_avg:92.74ms
step:1530/1645 train_time:141886ms step_avg:92.74ms
step:1531/1645 train_time:141979ms step_avg:92.74ms
step:1532/1645 train_time:142073ms step_avg:92.74ms
step:1533/1645 train_time:142167ms step_avg:92.74ms
step:1534/1645 train_time:142261ms step_avg:92.74ms
step:1535/1645 train_time:142355ms step_avg:92.74ms
step:1536/1645 train_time:142449ms step_avg:92.74ms
step:1537/1645 train_time:142543ms step_avg:92.74ms
step:1538/1645 train_time:142636ms step_avg:92.74ms
step:1539/1645 train_time:142730ms step_avg:92.74ms
step:1540/1645 train_time:142823ms step_avg:92.74ms
step:1541/1645 train_time:142917ms step_avg:92.74ms
step:1542/1645 train_time:143010ms step_avg:92.74ms
step:1543/1645 train_time:143104ms step_avg:92.74ms
step:1544/1645 train_time:143197ms step_avg:92.74ms
step:1545/1645 train_time:143290ms step_avg:92.74ms
step:1546/1645 train_time:143384ms step_avg:92.75ms
step:1547/1645 train_time:143478ms step_avg:92.75ms
step:1548/1645 train_time:143573ms step_avg:92.75ms
step:1549/1645 train_time:143667ms step_avg:92.75ms
step:1550/1645 train_time:143760ms step_avg:92.75ms
step:1551/1645 train_time:143853ms step_avg:92.75ms
step:1552/1645 train_time:143947ms step_avg:92.75ms
step:1553/1645 train_time:144041ms step_avg:92.75ms
step:1554/1645 train_time:144134ms step_avg:92.75ms
step:1555/1645 train_time:144229ms step_avg:92.75ms
step:1556/1645 train_time:144324ms step_avg:92.75ms
step:1557/1645 train_time:144417ms step_avg:92.75ms
step:1558/1645 train_time:144511ms step_avg:92.75ms
step:1559/1645 train_time:144604ms step_avg:92.75ms
step:1560/1645 train_time:144698ms step_avg:92.76ms
step:1561/1645 train_time:144792ms step_avg:92.76ms
step:1562/1645 train_time:144885ms step_avg:92.76ms
step:1563/1645 train_time:144979ms step_avg:92.76ms
step:1564/1645 train_time:145072ms step_avg:92.76ms
step:1565/1645 train_time:145166ms step_avg:92.76ms
step:1566/1645 train_time:145260ms step_avg:92.76ms
step:1567/1645 train_time:145353ms step_avg:92.76ms
step:1568/1645 train_time:145448ms step_avg:92.76ms
step:1569/1645 train_time:145542ms step_avg:92.76ms
step:1570/1645 train_time:145635ms step_avg:92.76ms
step:1571/1645 train_time:145730ms step_avg:92.76ms
step:1572/1645 train_time:145822ms step_avg:92.76ms
step:1573/1645 train_time:145915ms step_avg:92.76ms
step:1574/1645 train_time:146008ms step_avg:92.76ms
step:1575/1645 train_time:146102ms step_avg:92.76ms
step:1576/1645 train_time:146195ms step_avg:92.76ms
step:1577/1645 train_time:146288ms step_avg:92.76ms
step:1578/1645 train_time:146382ms step_avg:92.76ms
step:1579/1645 train_time:146476ms step_avg:92.77ms
step:1580/1645 train_time:146570ms step_avg:92.77ms
step:1581/1645 train_time:146663ms step_avg:92.77ms
step:1582/1645 train_time:146756ms step_avg:92.77ms
step:1583/1645 train_time:146850ms step_avg:92.77ms
step:1584/1645 train_time:146943ms step_avg:92.77ms
step:1585/1645 train_time:147036ms step_avg:92.77ms
step:1586/1645 train_time:147131ms step_avg:92.77ms
step:1587/1645 train_time:147224ms step_avg:92.77ms
step:1588/1645 train_time:147317ms step_avg:92.77ms
step:1589/1645 train_time:147410ms step_avg:92.77ms
step:1590/1645 train_time:147504ms step_avg:92.77ms
step:1591/1645 train_time:147597ms step_avg:92.77ms
step:1592/1645 train_time:147691ms step_avg:92.77ms
step:1593/1645 train_time:147784ms step_avg:92.77ms
step:1594/1645 train_time:147878ms step_avg:92.77ms
step:1595/1645 train_time:147971ms step_avg:92.77ms
step:1596/1645 train_time:148065ms step_avg:92.77ms
step:1597/1645 train_time:148158ms step_avg:92.77ms
step:1598/1645 train_time:148252ms step_avg:92.77ms
step:1599/1645 train_time:148346ms step_avg:92.77ms
step:1600/1645 train_time:148439ms step_avg:92.77ms
step:1601/1645 train_time:148534ms step_avg:92.78ms
step:1602/1645 train_time:148628ms step_avg:92.78ms
step:1603/1645 train_time:148722ms step_avg:92.78ms
step:1604/1645 train_time:148815ms step_avg:92.78ms
step:1605/1645 train_time:148909ms step_avg:92.78ms
step:1606/1645 train_time:149003ms step_avg:92.78ms
step:1607/1645 train_time:149096ms step_avg:92.78ms
step:1608/1645 train_time:149190ms step_avg:92.78ms
step:1609/1645 train_time:149283ms step_avg:92.78ms
step:1610/1645 train_time:149377ms step_avg:92.78ms
step:1611/1645 train_time:149472ms step_avg:92.78ms
step:1612/1645 train_time:149565ms step_avg:92.78ms
step:1613/1645 train_time:149658ms step_avg:92.78ms
step:1614/1645 train_time:149753ms step_avg:92.78ms
step:1615/1645 train_time:149848ms step_avg:92.78ms
step:1616/1645 train_time:149941ms step_avg:92.79ms
step:1617/1645 train_time:150035ms step_avg:92.79ms
step:1618/1645 train_time:150128ms step_avg:92.79ms
step:1619/1645 train_time:150222ms step_avg:92.79ms
step:1620/1645 train_time:150315ms step_avg:92.79ms
step:1621/1645 train_time:150409ms step_avg:92.79ms
step:1622/1645 train_time:150503ms step_avg:92.79ms
step:1623/1645 train_time:150596ms step_avg:92.79ms
step:1624/1645 train_time:150689ms step_avg:92.79ms
step:1625/1645 train_time:150782ms step_avg:92.79ms
step:1625/1645 val_loss:3.2852 train_time:150876ms step_avg:92.85ms
step:1626/1645 train_time:150901ms step_avg:92.81ms
step:1627/1645 train_time:150976ms step_avg:92.79ms
step:1628/1645 train_time:151072ms step_avg:92.80ms
step:1629/1645 train_time:151165ms step_avg:92.80ms
step:1630/1645 train_time:151258ms step_avg:92.80ms
step:1631/1645 train_time:151351ms step_avg:92.80ms
step:1632/1645 train_time:151444ms step_avg:92.80ms
step:1633/1645 train_time:151537ms step_avg:92.80ms
step:1634/1645 train_time:151631ms step_avg:92.80ms
step:1635/1645 train_time:151723ms step_avg:92.80ms
step:1636/1645 train_time:151817ms step_avg:92.80ms
step:1637/1645 train_time:151912ms step_avg:92.80ms
step:1638/1645 train_time:152007ms step_avg:92.80ms
step:1639/1645 train_time:152101ms step_avg:92.80ms
step:1640/1645 train_time:152195ms step_avg:92.80ms
step:1641/1645 train_time:152288ms step_avg:92.80ms
step:1642/1645 train_time:152382ms step_avg:92.80ms
step:1643/1645 train_time:152475ms step_avg:92.80ms
step:1644/1645 train_time:152567ms step_avg:92.80ms
step:1645/1645 train_time:152661ms step_avg:92.80ms
step:1645/1645 val_loss:3.2796 train_time:152755ms step_avg:92.86ms
peak memory allocated: 32074 MiB reserved: 47316 MiB

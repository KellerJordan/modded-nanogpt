import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:31:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:153ms step_avg:152.58ms
step:2/1670 train_time:175ms step_avg:87.36ms
step:3/1670 train_time:237ms step_avg:78.90ms
step:4/1670 train_time:322ms step_avg:80.62ms
step:5/1670 train_time:409ms step_avg:81.70ms
step:6/1670 train_time:495ms step_avg:82.52ms
step:7/1670 train_time:582ms step_avg:83.15ms
step:8/1670 train_time:669ms step_avg:83.63ms
step:9/1670 train_time:756ms step_avg:83.98ms
step:10/1670 train_time:843ms step_avg:84.28ms
step:11/1670 train_time:929ms step_avg:84.50ms
step:12/1670 train_time:1019ms step_avg:84.89ms
step:13/1670 train_time:1110ms step_avg:85.37ms
step:14/1670 train_time:1201ms step_avg:85.75ms
step:15/1670 train_time:1289ms step_avg:85.92ms
step:16/1670 train_time:1377ms step_avg:86.05ms
step:17/1670 train_time:1464ms step_avg:86.14ms
step:18/1670 train_time:1551ms step_avg:86.18ms
step:19/1670 train_time:1638ms step_avg:86.24ms
step:20/1670 train_time:1725ms step_avg:86.26ms
step:21/1670 train_time:1813ms step_avg:86.32ms
step:22/1670 train_time:1900ms step_avg:86.38ms
step:23/1670 train_time:1988ms step_avg:86.43ms
step:24/1670 train_time:2077ms step_avg:86.56ms
step:25/1670 train_time:2167ms step_avg:86.67ms
step:26/1670 train_time:2256ms step_avg:86.77ms
step:27/1670 train_time:2344ms step_avg:86.83ms
step:28/1670 train_time:2433ms step_avg:86.90ms
step:29/1670 train_time:2521ms step_avg:86.92ms
step:30/1670 train_time:2608ms step_avg:86.94ms
step:31/1670 train_time:2695ms step_avg:86.93ms
step:32/1670 train_time:2782ms step_avg:86.94ms
step:33/1670 train_time:2869ms step_avg:86.95ms
step:34/1670 train_time:2957ms step_avg:86.97ms
step:35/1670 train_time:3045ms step_avg:87.01ms
step:36/1670 train_time:3133ms step_avg:87.04ms
step:37/1670 train_time:3223ms step_avg:87.10ms
step:38/1670 train_time:3311ms step_avg:87.12ms
step:39/1670 train_time:3400ms step_avg:87.19ms
step:40/1670 train_time:3488ms step_avg:87.20ms
step:41/1670 train_time:3577ms step_avg:87.24ms
step:42/1670 train_time:3664ms step_avg:87.24ms
step:43/1670 train_time:3752ms step_avg:87.25ms
step:44/1670 train_time:3839ms step_avg:87.26ms
step:45/1670 train_time:3926ms step_avg:87.26ms
step:46/1670 train_time:4014ms step_avg:87.27ms
step:47/1670 train_time:4103ms step_avg:87.30ms
step:48/1670 train_time:4192ms step_avg:87.33ms
step:49/1670 train_time:4280ms step_avg:87.35ms
step:50/1670 train_time:4367ms step_avg:87.35ms
step:51/1670 train_time:4455ms step_avg:87.36ms
step:52/1670 train_time:4544ms step_avg:87.38ms
step:53/1670 train_time:4632ms step_avg:87.39ms
step:54/1670 train_time:4719ms step_avg:87.40ms
step:55/1670 train_time:4807ms step_avg:87.39ms
step:56/1670 train_time:4894ms step_avg:87.40ms
step:57/1670 train_time:4983ms step_avg:87.41ms
step:58/1670 train_time:5070ms step_avg:87.42ms
step:59/1670 train_time:5158ms step_avg:87.42ms
step:60/1670 train_time:5246ms step_avg:87.43ms
step:61/1670 train_time:5333ms step_avg:87.43ms
step:62/1670 train_time:5421ms step_avg:87.44ms
step:63/1670 train_time:5509ms step_avg:87.45ms
step:64/1670 train_time:5598ms step_avg:87.46ms
step:65/1670 train_time:5685ms step_avg:87.47ms
step:66/1670 train_time:5773ms step_avg:87.47ms
step:67/1670 train_time:5861ms step_avg:87.47ms
step:68/1670 train_time:5948ms step_avg:87.47ms
step:69/1670 train_time:6035ms step_avg:87.47ms
step:70/1670 train_time:6123ms step_avg:87.47ms
step:71/1670 train_time:6211ms step_avg:87.48ms
step:72/1670 train_time:6299ms step_avg:87.48ms
step:73/1670 train_time:6387ms step_avg:87.49ms
step:74/1670 train_time:6475ms step_avg:87.50ms
step:75/1670 train_time:6563ms step_avg:87.51ms
step:76/1670 train_time:6651ms step_avg:87.52ms
step:77/1670 train_time:6740ms step_avg:87.53ms
step:78/1670 train_time:6827ms step_avg:87.53ms
step:79/1670 train_time:6916ms step_avg:87.54ms
step:80/1670 train_time:7004ms step_avg:87.55ms
step:81/1670 train_time:7092ms step_avg:87.55ms
step:82/1670 train_time:7180ms step_avg:87.56ms
step:83/1670 train_time:7267ms step_avg:87.56ms
step:84/1670 train_time:7355ms step_avg:87.56ms
step:85/1670 train_time:7443ms step_avg:87.57ms
step:86/1670 train_time:7531ms step_avg:87.57ms
step:87/1670 train_time:7619ms step_avg:87.57ms
step:88/1670 train_time:7707ms step_avg:87.58ms
step:89/1670 train_time:7795ms step_avg:87.58ms
step:90/1670 train_time:7883ms step_avg:87.59ms
step:91/1670 train_time:7970ms step_avg:87.59ms
step:92/1670 train_time:8059ms step_avg:87.59ms
step:93/1670 train_time:8146ms step_avg:87.59ms
step:94/1670 train_time:8235ms step_avg:87.60ms
step:95/1670 train_time:8323ms step_avg:87.61ms
step:96/1670 train_time:8411ms step_avg:87.61ms
step:97/1670 train_time:8499ms step_avg:87.62ms
step:98/1670 train_time:8587ms step_avg:87.62ms
step:99/1670 train_time:8675ms step_avg:87.62ms
step:100/1670 train_time:8762ms step_avg:87.62ms
step:101/1670 train_time:8850ms step_avg:87.62ms
step:102/1670 train_time:8938ms step_avg:87.63ms
step:103/1670 train_time:9026ms step_avg:87.63ms
step:104/1670 train_time:9114ms step_avg:87.63ms
step:105/1670 train_time:9202ms step_avg:87.63ms
step:106/1670 train_time:9289ms step_avg:87.63ms
step:107/1670 train_time:9377ms step_avg:87.64ms
step:108/1670 train_time:9464ms step_avg:87.63ms
step:109/1670 train_time:9552ms step_avg:87.64ms
step:110/1670 train_time:9641ms step_avg:87.64ms
step:111/1670 train_time:9728ms step_avg:87.64ms
step:112/1670 train_time:9815ms step_avg:87.64ms
step:113/1670 train_time:9903ms step_avg:87.64ms
step:114/1670 train_time:9991ms step_avg:87.64ms
step:115/1670 train_time:10079ms step_avg:87.64ms
step:116/1670 train_time:10167ms step_avg:87.64ms
step:117/1670 train_time:10254ms step_avg:87.64ms
step:118/1670 train_time:10342ms step_avg:87.64ms
step:119/1670 train_time:10429ms step_avg:87.64ms
step:120/1670 train_time:10517ms step_avg:87.64ms
step:121/1670 train_time:10605ms step_avg:87.64ms
step:122/1670 train_time:10692ms step_avg:87.64ms
step:123/1670 train_time:10780ms step_avg:87.64ms
step:124/1670 train_time:10867ms step_avg:87.64ms
step:125/1670 train_time:10955ms step_avg:87.64ms
step:125/1670 val_loss:4.3461 train_time:11046ms step_avg:88.37ms
step:126/1670 train_time:11066ms step_avg:87.83ms
step:127/1670 train_time:11134ms step_avg:87.67ms
step:128/1670 train_time:11230ms step_avg:87.73ms
step:129/1670 train_time:11322ms step_avg:87.77ms
step:130/1670 train_time:11410ms step_avg:87.77ms
step:131/1670 train_time:11497ms step_avg:87.76ms
step:132/1670 train_time:11584ms step_avg:87.76ms
step:133/1670 train_time:11671ms step_avg:87.75ms
step:134/1670 train_time:11757ms step_avg:87.74ms
step:135/1670 train_time:11844ms step_avg:87.73ms
step:136/1670 train_time:11930ms step_avg:87.72ms
step:137/1670 train_time:12017ms step_avg:87.72ms
step:138/1670 train_time:12105ms step_avg:87.72ms
step:139/1670 train_time:12195ms step_avg:87.73ms
step:140/1670 train_time:12285ms step_avg:87.75ms
step:141/1670 train_time:12374ms step_avg:87.76ms
step:142/1670 train_time:12461ms step_avg:87.75ms
step:143/1670 train_time:12549ms step_avg:87.76ms
step:144/1670 train_time:12636ms step_avg:87.75ms
step:145/1670 train_time:12723ms step_avg:87.74ms
step:146/1670 train_time:12810ms step_avg:87.74ms
step:147/1670 train_time:12897ms step_avg:87.73ms
step:148/1670 train_time:12984ms step_avg:87.73ms
step:149/1670 train_time:13071ms step_avg:87.72ms
step:150/1670 train_time:13159ms step_avg:87.73ms
step:151/1670 train_time:13247ms step_avg:87.73ms
step:152/1670 train_time:13336ms step_avg:87.73ms
step:153/1670 train_time:13424ms step_avg:87.74ms
step:154/1670 train_time:13510ms step_avg:87.73ms
step:155/1670 train_time:13598ms step_avg:87.73ms
step:156/1670 train_time:13685ms step_avg:87.72ms
step:157/1670 train_time:13772ms step_avg:87.72ms
step:158/1670 train_time:13859ms step_avg:87.72ms
step:159/1670 train_time:13946ms step_avg:87.71ms
step:160/1670 train_time:14034ms step_avg:87.71ms
step:161/1670 train_time:14122ms step_avg:87.71ms
step:162/1670 train_time:14209ms step_avg:87.71ms
step:163/1670 train_time:14299ms step_avg:87.72ms
step:164/1670 train_time:14386ms step_avg:87.72ms
step:165/1670 train_time:14474ms step_avg:87.72ms
step:166/1670 train_time:14562ms step_avg:87.72ms
step:167/1670 train_time:14649ms step_avg:87.72ms
step:168/1670 train_time:14736ms step_avg:87.72ms
step:169/1670 train_time:14823ms step_avg:87.71ms
step:170/1670 train_time:14910ms step_avg:87.70ms
step:171/1670 train_time:14998ms step_avg:87.71ms
step:172/1670 train_time:15085ms step_avg:87.70ms
step:173/1670 train_time:15174ms step_avg:87.71ms
step:174/1670 train_time:15262ms step_avg:87.71ms
step:175/1670 train_time:15350ms step_avg:87.71ms
step:176/1670 train_time:15439ms step_avg:87.72ms
step:177/1670 train_time:15526ms step_avg:87.72ms
step:178/1670 train_time:15614ms step_avg:87.72ms
step:179/1670 train_time:15701ms step_avg:87.72ms
step:180/1670 train_time:15789ms step_avg:87.72ms
step:181/1670 train_time:15877ms step_avg:87.72ms
step:182/1670 train_time:15964ms step_avg:87.71ms
step:183/1670 train_time:16051ms step_avg:87.71ms
step:184/1670 train_time:16139ms step_avg:87.71ms
step:185/1670 train_time:16226ms step_avg:87.71ms
step:186/1670 train_time:16313ms step_avg:87.71ms
step:187/1670 train_time:16402ms step_avg:87.71ms
step:188/1670 train_time:16490ms step_avg:87.71ms
step:189/1670 train_time:16578ms step_avg:87.71ms
step:190/1670 train_time:16665ms step_avg:87.71ms
step:191/1670 train_time:16753ms step_avg:87.71ms
step:192/1670 train_time:16840ms step_avg:87.71ms
step:193/1670 train_time:16928ms step_avg:87.71ms
step:194/1670 train_time:17015ms step_avg:87.71ms
step:195/1670 train_time:17102ms step_avg:87.70ms
step:196/1670 train_time:17189ms step_avg:87.70ms
step:197/1670 train_time:17277ms step_avg:87.70ms
step:198/1670 train_time:17364ms step_avg:87.70ms
step:199/1670 train_time:17452ms step_avg:87.70ms
step:200/1670 train_time:17541ms step_avg:87.70ms
step:201/1670 train_time:17628ms step_avg:87.70ms
step:202/1670 train_time:17716ms step_avg:87.70ms
step:203/1670 train_time:17803ms step_avg:87.70ms
step:204/1670 train_time:17891ms step_avg:87.70ms
step:205/1670 train_time:17979ms step_avg:87.70ms
step:206/1670 train_time:18066ms step_avg:87.70ms
step:207/1670 train_time:18154ms step_avg:87.70ms
step:208/1670 train_time:18241ms step_avg:87.70ms
step:209/1670 train_time:18329ms step_avg:87.70ms
step:210/1670 train_time:18416ms step_avg:87.70ms
step:211/1670 train_time:18504ms step_avg:87.70ms
step:212/1670 train_time:18591ms step_avg:87.69ms
step:213/1670 train_time:18679ms step_avg:87.69ms
step:214/1670 train_time:18766ms step_avg:87.69ms
step:215/1670 train_time:18853ms step_avg:87.69ms
step:216/1670 train_time:18940ms step_avg:87.69ms
step:217/1670 train_time:19028ms step_avg:87.69ms
step:218/1670 train_time:19115ms step_avg:87.69ms
step:219/1670 train_time:19203ms step_avg:87.68ms
step:220/1670 train_time:19290ms step_avg:87.68ms
step:221/1670 train_time:19379ms step_avg:87.69ms
step:222/1670 train_time:19465ms step_avg:87.68ms
step:223/1670 train_time:19553ms step_avg:87.68ms
step:224/1670 train_time:19642ms step_avg:87.69ms
step:225/1670 train_time:19729ms step_avg:87.69ms
step:226/1670 train_time:19817ms step_avg:87.68ms
step:227/1670 train_time:19904ms step_avg:87.68ms
step:228/1670 train_time:19991ms step_avg:87.68ms
step:229/1670 train_time:20079ms step_avg:87.68ms
step:230/1670 train_time:20166ms step_avg:87.68ms
step:231/1670 train_time:20254ms step_avg:87.68ms
step:232/1670 train_time:20342ms step_avg:87.68ms
step:233/1670 train_time:20429ms step_avg:87.68ms
step:234/1670 train_time:20517ms step_avg:87.68ms
step:235/1670 train_time:20604ms step_avg:87.68ms
step:236/1670 train_time:20692ms step_avg:87.68ms
step:237/1670 train_time:20779ms step_avg:87.68ms
step:238/1670 train_time:20866ms step_avg:87.67ms
step:239/1670 train_time:20953ms step_avg:87.67ms
step:240/1670 train_time:21041ms step_avg:87.67ms
step:241/1670 train_time:21129ms step_avg:87.67ms
step:242/1670 train_time:21217ms step_avg:87.67ms
step:243/1670 train_time:21304ms step_avg:87.67ms
step:244/1670 train_time:21391ms step_avg:87.67ms
step:245/1670 train_time:21479ms step_avg:87.67ms
step:246/1670 train_time:21566ms step_avg:87.67ms
step:247/1670 train_time:21654ms step_avg:87.67ms
step:248/1670 train_time:21742ms step_avg:87.67ms
step:249/1670 train_time:21829ms step_avg:87.67ms
step:250/1670 train_time:21916ms step_avg:87.66ms
step:250/1670 val_loss:3.9850 train_time:22005ms step_avg:88.02ms
step:251/1670 train_time:22026ms step_avg:87.75ms
step:252/1670 train_time:22097ms step_avg:87.69ms
step:253/1670 train_time:22193ms step_avg:87.72ms
step:254/1670 train_time:22281ms step_avg:87.72ms
step:255/1670 train_time:22368ms step_avg:87.72ms
step:256/1670 train_time:22454ms step_avg:87.71ms
step:257/1670 train_time:22541ms step_avg:87.71ms
step:258/1670 train_time:22628ms step_avg:87.70ms
step:259/1670 train_time:22714ms step_avg:87.70ms
step:260/1670 train_time:22801ms step_avg:87.70ms
step:261/1670 train_time:22888ms step_avg:87.69ms
step:262/1670 train_time:22976ms step_avg:87.69ms
step:263/1670 train_time:23065ms step_avg:87.70ms
step:264/1670 train_time:23155ms step_avg:87.71ms
step:265/1670 train_time:23245ms step_avg:87.72ms
step:266/1670 train_time:23333ms step_avg:87.72ms
step:267/1670 train_time:23420ms step_avg:87.71ms
step:268/1670 train_time:23506ms step_avg:87.71ms
step:269/1670 train_time:23593ms step_avg:87.71ms
step:270/1670 train_time:23679ms step_avg:87.70ms
step:271/1670 train_time:23766ms step_avg:87.70ms
step:272/1670 train_time:23853ms step_avg:87.69ms
step:273/1670 train_time:23940ms step_avg:87.69ms
step:274/1670 train_time:24030ms step_avg:87.70ms
step:275/1670 train_time:24117ms step_avg:87.70ms
step:276/1670 train_time:24207ms step_avg:87.71ms
step:277/1670 train_time:24295ms step_avg:87.71ms
step:278/1670 train_time:24383ms step_avg:87.71ms
step:279/1670 train_time:24471ms step_avg:87.71ms
step:280/1670 train_time:24558ms step_avg:87.71ms
step:281/1670 train_time:24645ms step_avg:87.70ms
step:282/1670 train_time:24731ms step_avg:87.70ms
step:283/1670 train_time:24818ms step_avg:87.70ms
step:284/1670 train_time:24905ms step_avg:87.69ms
step:285/1670 train_time:24993ms step_avg:87.69ms
step:286/1670 train_time:25081ms step_avg:87.69ms
step:287/1670 train_time:25169ms step_avg:87.70ms
step:288/1670 train_time:25257ms step_avg:87.70ms
step:289/1670 train_time:25345ms step_avg:87.70ms
step:290/1670 train_time:25433ms step_avg:87.70ms
step:291/1670 train_time:25520ms step_avg:87.70ms
step:292/1670 train_time:25607ms step_avg:87.70ms
step:293/1670 train_time:25695ms step_avg:87.70ms
step:294/1670 train_time:25782ms step_avg:87.69ms
step:295/1670 train_time:25869ms step_avg:87.69ms
step:296/1670 train_time:25957ms step_avg:87.69ms
step:297/1670 train_time:26045ms step_avg:87.69ms
step:298/1670 train_time:26132ms step_avg:87.69ms
step:299/1670 train_time:26221ms step_avg:87.70ms
step:300/1670 train_time:26310ms step_avg:87.70ms
step:301/1670 train_time:26397ms step_avg:87.70ms
step:302/1670 train_time:26485ms step_avg:87.70ms
step:303/1670 train_time:26573ms step_avg:87.70ms
step:304/1670 train_time:26660ms step_avg:87.70ms
step:305/1670 train_time:26747ms step_avg:87.70ms
step:306/1670 train_time:26834ms step_avg:87.69ms
step:307/1670 train_time:26922ms step_avg:87.69ms
step:308/1670 train_time:27009ms step_avg:87.69ms
step:309/1670 train_time:27097ms step_avg:87.69ms
step:310/1670 train_time:27185ms step_avg:87.69ms
step:311/1670 train_time:27273ms step_avg:87.69ms
step:312/1670 train_time:27360ms step_avg:87.69ms
step:313/1670 train_time:27448ms step_avg:87.69ms
step:314/1670 train_time:27535ms step_avg:87.69ms
step:315/1670 train_time:27624ms step_avg:87.69ms
step:316/1670 train_time:27710ms step_avg:87.69ms
step:317/1670 train_time:27798ms step_avg:87.69ms
step:318/1670 train_time:27886ms step_avg:87.69ms
step:319/1670 train_time:27973ms step_avg:87.69ms
step:320/1670 train_time:28060ms step_avg:87.69ms
step:321/1670 train_time:28149ms step_avg:87.69ms
step:322/1670 train_time:28236ms step_avg:87.69ms
step:323/1670 train_time:28325ms step_avg:87.69ms
step:324/1670 train_time:28413ms step_avg:87.69ms
step:325/1670 train_time:28500ms step_avg:87.69ms
step:326/1670 train_time:28589ms step_avg:87.69ms
step:327/1670 train_time:28676ms step_avg:87.69ms
step:328/1670 train_time:28764ms step_avg:87.69ms
step:329/1670 train_time:28851ms step_avg:87.69ms
step:330/1670 train_time:28939ms step_avg:87.69ms
step:331/1670 train_time:29026ms step_avg:87.69ms
step:332/1670 train_time:29114ms step_avg:87.69ms
step:333/1670 train_time:29201ms step_avg:87.69ms
step:334/1670 train_time:29289ms step_avg:87.69ms
step:335/1670 train_time:29376ms step_avg:87.69ms
step:336/1670 train_time:29464ms step_avg:87.69ms
step:337/1670 train_time:29552ms step_avg:87.69ms
step:338/1670 train_time:29639ms step_avg:87.69ms
step:339/1670 train_time:29727ms step_avg:87.69ms
step:340/1670 train_time:29814ms step_avg:87.69ms
step:341/1670 train_time:29901ms step_avg:87.69ms
step:342/1670 train_time:29989ms step_avg:87.69ms
step:343/1670 train_time:30077ms step_avg:87.69ms
step:344/1670 train_time:30165ms step_avg:87.69ms
step:345/1670 train_time:30252ms step_avg:87.69ms
step:346/1670 train_time:30339ms step_avg:87.68ms
step:347/1670 train_time:30428ms step_avg:87.69ms
step:348/1670 train_time:30515ms step_avg:87.69ms
step:349/1670 train_time:30602ms step_avg:87.68ms
step:350/1670 train_time:30690ms step_avg:87.69ms
step:351/1670 train_time:30777ms step_avg:87.68ms
step:352/1670 train_time:30865ms step_avg:87.68ms
step:353/1670 train_time:30953ms step_avg:87.68ms
step:354/1670 train_time:31040ms step_avg:87.68ms
step:355/1670 train_time:31128ms step_avg:87.68ms
step:356/1670 train_time:31216ms step_avg:87.68ms
step:357/1670 train_time:31303ms step_avg:87.68ms
step:358/1670 train_time:31392ms step_avg:87.69ms
step:359/1670 train_time:31480ms step_avg:87.69ms
step:360/1670 train_time:31568ms step_avg:87.69ms
step:361/1670 train_time:31655ms step_avg:87.69ms
step:362/1670 train_time:31742ms step_avg:87.69ms
step:363/1670 train_time:31831ms step_avg:87.69ms
step:364/1670 train_time:31918ms step_avg:87.69ms
step:365/1670 train_time:32006ms step_avg:87.69ms
step:366/1670 train_time:32093ms step_avg:87.69ms
step:367/1670 train_time:32181ms step_avg:87.69ms
step:368/1670 train_time:32269ms step_avg:87.69ms
step:369/1670 train_time:32357ms step_avg:87.69ms
step:370/1670 train_time:32444ms step_avg:87.69ms
step:371/1670 train_time:32532ms step_avg:87.69ms
step:372/1670 train_time:32620ms step_avg:87.69ms
step:373/1670 train_time:32708ms step_avg:87.69ms
step:374/1670 train_time:32796ms step_avg:87.69ms
step:375/1670 train_time:32885ms step_avg:87.69ms
step:375/1670 val_loss:3.8254 train_time:32974ms step_avg:87.93ms
step:376/1670 train_time:32993ms step_avg:87.75ms
step:377/1670 train_time:33065ms step_avg:87.71ms
step:378/1670 train_time:33158ms step_avg:87.72ms
step:379/1670 train_time:33246ms step_avg:87.72ms
step:380/1670 train_time:33334ms step_avg:87.72ms
step:381/1670 train_time:33420ms step_avg:87.72ms
step:382/1670 train_time:33506ms step_avg:87.71ms
step:383/1670 train_time:33593ms step_avg:87.71ms
step:384/1670 train_time:33679ms step_avg:87.71ms
step:385/1670 train_time:33766ms step_avg:87.70ms
step:386/1670 train_time:33852ms step_avg:87.70ms
step:387/1670 train_time:33940ms step_avg:87.70ms
step:388/1670 train_time:34030ms step_avg:87.71ms
step:389/1670 train_time:34119ms step_avg:87.71ms
step:390/1670 train_time:34208ms step_avg:87.71ms
step:391/1670 train_time:34297ms step_avg:87.72ms
step:392/1670 train_time:34384ms step_avg:87.71ms
step:393/1670 train_time:34472ms step_avg:87.71ms
step:394/1670 train_time:34558ms step_avg:87.71ms
step:395/1670 train_time:34645ms step_avg:87.71ms
step:396/1670 train_time:34733ms step_avg:87.71ms
step:397/1670 train_time:34819ms step_avg:87.71ms
step:398/1670 train_time:34907ms step_avg:87.71ms
step:399/1670 train_time:34995ms step_avg:87.71ms
step:400/1670 train_time:35084ms step_avg:87.71ms
step:401/1670 train_time:35174ms step_avg:87.72ms
step:402/1670 train_time:35262ms step_avg:87.72ms
step:403/1670 train_time:35350ms step_avg:87.72ms
step:404/1670 train_time:35438ms step_avg:87.72ms
step:405/1670 train_time:35525ms step_avg:87.72ms
step:406/1670 train_time:35612ms step_avg:87.71ms
step:407/1670 train_time:35699ms step_avg:87.71ms
step:408/1670 train_time:35786ms step_avg:87.71ms
step:409/1670 train_time:35874ms step_avg:87.71ms
step:410/1670 train_time:35962ms step_avg:87.71ms
step:411/1670 train_time:36051ms step_avg:87.72ms
step:412/1670 train_time:36139ms step_avg:87.72ms
step:413/1670 train_time:36227ms step_avg:87.72ms
step:414/1670 train_time:36315ms step_avg:87.72ms
step:415/1670 train_time:36403ms step_avg:87.72ms
step:416/1670 train_time:36490ms step_avg:87.72ms
step:417/1670 train_time:36577ms step_avg:87.72ms
step:418/1670 train_time:36664ms step_avg:87.71ms
step:419/1670 train_time:36752ms step_avg:87.71ms
step:420/1670 train_time:36839ms step_avg:87.71ms
step:421/1670 train_time:36927ms step_avg:87.71ms
step:422/1670 train_time:37015ms step_avg:87.71ms
step:423/1670 train_time:37103ms step_avg:87.71ms
step:424/1670 train_time:37192ms step_avg:87.72ms
step:425/1670 train_time:37280ms step_avg:87.72ms
step:426/1670 train_time:37367ms step_avg:87.72ms
step:427/1670 train_time:37455ms step_avg:87.72ms
step:428/1670 train_time:37542ms step_avg:87.72ms
step:429/1670 train_time:37630ms step_avg:87.72ms
step:430/1670 train_time:37717ms step_avg:87.71ms
step:431/1670 train_time:37806ms step_avg:87.72ms
step:432/1670 train_time:37893ms step_avg:87.72ms
step:433/1670 train_time:37981ms step_avg:87.71ms
step:434/1670 train_time:38068ms step_avg:87.72ms
step:435/1670 train_time:38156ms step_avg:87.72ms
step:436/1670 train_time:38244ms step_avg:87.72ms
step:437/1670 train_time:38334ms step_avg:87.72ms
step:438/1670 train_time:38422ms step_avg:87.72ms
step:439/1670 train_time:38510ms step_avg:87.72ms
step:440/1670 train_time:38597ms step_avg:87.72ms
step:441/1670 train_time:38684ms step_avg:87.72ms
step:442/1670 train_time:38773ms step_avg:87.72ms
step:443/1670 train_time:38860ms step_avg:87.72ms
step:444/1670 train_time:38947ms step_avg:87.72ms
step:445/1670 train_time:39036ms step_avg:87.72ms
step:446/1670 train_time:39124ms step_avg:87.72ms
step:447/1670 train_time:39212ms step_avg:87.72ms
step:448/1670 train_time:39299ms step_avg:87.72ms
step:449/1670 train_time:39387ms step_avg:87.72ms
step:450/1670 train_time:39475ms step_avg:87.72ms
step:451/1670 train_time:39562ms step_avg:87.72ms
step:452/1670 train_time:39650ms step_avg:87.72ms
step:453/1670 train_time:39738ms step_avg:87.72ms
step:454/1670 train_time:39826ms step_avg:87.72ms
step:455/1670 train_time:39913ms step_avg:87.72ms
step:456/1670 train_time:40001ms step_avg:87.72ms
step:457/1670 train_time:40088ms step_avg:87.72ms
step:458/1670 train_time:40176ms step_avg:87.72ms
step:459/1670 train_time:40266ms step_avg:87.72ms
step:460/1670 train_time:40354ms step_avg:87.73ms
step:461/1670 train_time:40441ms step_avg:87.73ms
step:462/1670 train_time:40529ms step_avg:87.73ms
step:463/1670 train_time:40617ms step_avg:87.72ms
step:464/1670 train_time:40705ms step_avg:87.73ms
step:465/1670 train_time:40793ms step_avg:87.73ms
step:466/1670 train_time:40880ms step_avg:87.73ms
step:467/1670 train_time:40969ms step_avg:87.73ms
step:468/1670 train_time:41056ms step_avg:87.73ms
step:469/1670 train_time:41144ms step_avg:87.73ms
step:470/1670 train_time:41233ms step_avg:87.73ms
step:471/1670 train_time:41320ms step_avg:87.73ms
step:472/1670 train_time:41408ms step_avg:87.73ms
step:473/1670 train_time:41496ms step_avg:87.73ms
step:474/1670 train_time:41583ms step_avg:87.73ms
step:475/1670 train_time:41672ms step_avg:87.73ms
step:476/1670 train_time:41759ms step_avg:87.73ms
step:477/1670 train_time:41848ms step_avg:87.73ms
step:478/1670 train_time:41935ms step_avg:87.73ms
step:479/1670 train_time:42023ms step_avg:87.73ms
step:480/1670 train_time:42111ms step_avg:87.73ms
step:481/1670 train_time:42199ms step_avg:87.73ms
step:482/1670 train_time:42287ms step_avg:87.73ms
step:483/1670 train_time:42375ms step_avg:87.73ms
step:484/1670 train_time:42462ms step_avg:87.73ms
step:485/1670 train_time:42550ms step_avg:87.73ms
step:486/1670 train_time:42637ms step_avg:87.73ms
step:487/1670 train_time:42725ms step_avg:87.73ms
step:488/1670 train_time:42814ms step_avg:87.73ms
step:489/1670 train_time:42901ms step_avg:87.73ms
step:490/1670 train_time:42989ms step_avg:87.73ms
step:491/1670 train_time:43077ms step_avg:87.73ms
step:492/1670 train_time:43164ms step_avg:87.73ms
step:493/1670 train_time:43252ms step_avg:87.73ms
step:494/1670 train_time:43339ms step_avg:87.73ms
step:495/1670 train_time:43427ms step_avg:87.73ms
step:496/1670 train_time:43515ms step_avg:87.73ms
step:497/1670 train_time:43603ms step_avg:87.73ms
step:498/1670 train_time:43690ms step_avg:87.73ms
step:499/1670 train_time:43778ms step_avg:87.73ms
step:500/1670 train_time:43865ms step_avg:87.73ms
step:500/1670 val_loss:3.7207 train_time:43954ms step_avg:87.91ms
step:501/1670 train_time:43976ms step_avg:87.78ms
step:502/1670 train_time:44046ms step_avg:87.74ms
step:503/1670 train_time:44140ms step_avg:87.75ms
step:504/1670 train_time:44227ms step_avg:87.75ms
step:505/1670 train_time:44314ms step_avg:87.75ms
step:506/1670 train_time:44401ms step_avg:87.75ms
step:507/1670 train_time:44487ms step_avg:87.75ms
step:508/1670 train_time:44574ms step_avg:87.74ms
step:509/1670 train_time:44660ms step_avg:87.74ms
step:510/1670 train_time:44746ms step_avg:87.74ms
step:511/1670 train_time:44833ms step_avg:87.74ms
step:512/1670 train_time:44921ms step_avg:87.74ms
step:513/1670 train_time:45011ms step_avg:87.74ms
step:514/1670 train_time:45102ms step_avg:87.75ms
step:515/1670 train_time:45191ms step_avg:87.75ms
step:516/1670 train_time:45279ms step_avg:87.75ms
step:517/1670 train_time:45366ms step_avg:87.75ms
step:518/1670 train_time:45453ms step_avg:87.75ms
step:519/1670 train_time:45540ms step_avg:87.75ms
step:520/1670 train_time:45626ms step_avg:87.74ms
step:521/1670 train_time:45713ms step_avg:87.74ms
step:522/1670 train_time:45801ms step_avg:87.74ms
step:523/1670 train_time:45888ms step_avg:87.74ms
step:524/1670 train_time:45976ms step_avg:87.74ms
step:525/1670 train_time:46065ms step_avg:87.74ms
step:526/1670 train_time:46155ms step_avg:87.75ms
step:527/1670 train_time:46243ms step_avg:87.75ms
step:528/1670 train_time:46332ms step_avg:87.75ms
step:529/1670 train_time:46420ms step_avg:87.75ms
step:530/1670 train_time:46507ms step_avg:87.75ms
step:531/1670 train_time:46594ms step_avg:87.75ms
step:532/1670 train_time:46681ms step_avg:87.75ms
step:533/1670 train_time:46768ms step_avg:87.74ms
step:534/1670 train_time:46855ms step_avg:87.74ms
step:535/1670 train_time:46942ms step_avg:87.74ms
step:536/1670 train_time:47030ms step_avg:87.74ms
step:537/1670 train_time:47119ms step_avg:87.74ms
step:538/1670 train_time:47207ms step_avg:87.74ms
step:539/1670 train_time:47295ms step_avg:87.75ms
step:540/1670 train_time:47383ms step_avg:87.75ms
step:541/1670 train_time:47471ms step_avg:87.75ms
step:542/1670 train_time:47558ms step_avg:87.75ms
step:543/1670 train_time:47645ms step_avg:87.74ms
step:544/1670 train_time:47733ms step_avg:87.74ms
step:545/1670 train_time:47821ms step_avg:87.75ms
step:546/1670 train_time:47910ms step_avg:87.75ms
step:547/1670 train_time:47999ms step_avg:87.75ms
step:548/1670 train_time:48087ms step_avg:87.75ms
step:549/1670 train_time:48177ms step_avg:87.75ms
step:550/1670 train_time:48266ms step_avg:87.76ms
step:551/1670 train_time:48355ms step_avg:87.76ms
step:552/1670 train_time:48444ms step_avg:87.76ms
step:553/1670 train_time:48532ms step_avg:87.76ms
step:554/1670 train_time:48621ms step_avg:87.76ms
step:555/1670 train_time:48711ms step_avg:87.77ms
step:556/1670 train_time:48800ms step_avg:87.77ms
step:557/1670 train_time:48890ms step_avg:87.77ms
step:558/1670 train_time:48980ms step_avg:87.78ms
step:559/1670 train_time:49069ms step_avg:87.78ms
step:560/1670 train_time:49159ms step_avg:87.78ms
step:561/1670 train_time:49249ms step_avg:87.79ms
step:562/1670 train_time:49339ms step_avg:87.79ms
step:563/1670 train_time:49427ms step_avg:87.79ms
step:564/1670 train_time:49516ms step_avg:87.79ms
step:565/1670 train_time:49604ms step_avg:87.80ms
step:566/1670 train_time:49693ms step_avg:87.80ms
step:567/1670 train_time:49783ms step_avg:87.80ms
step:568/1670 train_time:49872ms step_avg:87.80ms
step:569/1670 train_time:49962ms step_avg:87.81ms
step:570/1670 train_time:50051ms step_avg:87.81ms
step:571/1670 train_time:50140ms step_avg:87.81ms
step:572/1670 train_time:50228ms step_avg:87.81ms
step:573/1670 train_time:50318ms step_avg:87.81ms
step:574/1670 train_time:50406ms step_avg:87.82ms
step:575/1670 train_time:50495ms step_avg:87.82ms
step:576/1670 train_time:50583ms step_avg:87.82ms
step:577/1670 train_time:50673ms step_avg:87.82ms
step:578/1670 train_time:50762ms step_avg:87.82ms
step:579/1670 train_time:50852ms step_avg:87.83ms
step:580/1670 train_time:50941ms step_avg:87.83ms
step:581/1670 train_time:51030ms step_avg:87.83ms
step:582/1670 train_time:51119ms step_avg:87.83ms
step:583/1670 train_time:51209ms step_avg:87.84ms
step:584/1670 train_time:51298ms step_avg:87.84ms
step:585/1670 train_time:51386ms step_avg:87.84ms
step:586/1670 train_time:51476ms step_avg:87.84ms
step:587/1670 train_time:51564ms step_avg:87.84ms
step:588/1670 train_time:51653ms step_avg:87.85ms
step:589/1670 train_time:51742ms step_avg:87.85ms
step:590/1670 train_time:51831ms step_avg:87.85ms
step:591/1670 train_time:51920ms step_avg:87.85ms
step:592/1670 train_time:52009ms step_avg:87.85ms
step:593/1670 train_time:52099ms step_avg:87.86ms
step:594/1670 train_time:52188ms step_avg:87.86ms
step:595/1670 train_time:52278ms step_avg:87.86ms
step:596/1670 train_time:52367ms step_avg:87.86ms
step:597/1670 train_time:52456ms step_avg:87.87ms
step:598/1670 train_time:52546ms step_avg:87.87ms
step:599/1670 train_time:52635ms step_avg:87.87ms
step:600/1670 train_time:52724ms step_avg:87.87ms
step:601/1670 train_time:52813ms step_avg:87.87ms
step:602/1670 train_time:52901ms step_avg:87.88ms
step:603/1670 train_time:52991ms step_avg:87.88ms
step:604/1670 train_time:53081ms step_avg:87.88ms
step:605/1670 train_time:53169ms step_avg:87.88ms
step:606/1670 train_time:53258ms step_avg:87.88ms
step:607/1670 train_time:53347ms step_avg:87.89ms
step:608/1670 train_time:53436ms step_avg:87.89ms
step:609/1670 train_time:53525ms step_avg:87.89ms
step:610/1670 train_time:53615ms step_avg:87.89ms
step:611/1670 train_time:53704ms step_avg:87.89ms
step:612/1670 train_time:53793ms step_avg:87.90ms
step:613/1670 train_time:53882ms step_avg:87.90ms
step:614/1670 train_time:53972ms step_avg:87.90ms
step:615/1670 train_time:54061ms step_avg:87.90ms
step:616/1670 train_time:54149ms step_avg:87.90ms
step:617/1670 train_time:54238ms step_avg:87.91ms
step:618/1670 train_time:54327ms step_avg:87.91ms
step:619/1670 train_time:54417ms step_avg:87.91ms
step:620/1670 train_time:54505ms step_avg:87.91ms
step:621/1670 train_time:54595ms step_avg:87.92ms
step:622/1670 train_time:54684ms step_avg:87.92ms
step:623/1670 train_time:54772ms step_avg:87.92ms
step:624/1670 train_time:54861ms step_avg:87.92ms
step:625/1670 train_time:54949ms step_avg:87.92ms
step:625/1670 val_loss:3.6177 train_time:55040ms step_avg:88.06ms
step:626/1670 train_time:55061ms step_avg:87.96ms
step:627/1670 train_time:55131ms step_avg:87.93ms
step:628/1670 train_time:55220ms step_avg:87.93ms
step:629/1670 train_time:55311ms step_avg:87.94ms
step:630/1670 train_time:55399ms step_avg:87.93ms
step:631/1670 train_time:55487ms step_avg:87.93ms
step:632/1670 train_time:55574ms step_avg:87.93ms
step:633/1670 train_time:55662ms step_avg:87.93ms
step:634/1670 train_time:55751ms step_avg:87.94ms
step:635/1670 train_time:55842ms step_avg:87.94ms
step:636/1670 train_time:55931ms step_avg:87.94ms
step:637/1670 train_time:56022ms step_avg:87.95ms
step:638/1670 train_time:56112ms step_avg:87.95ms
step:639/1670 train_time:56202ms step_avg:87.95ms
step:640/1670 train_time:56291ms step_avg:87.95ms
step:641/1670 train_time:56380ms step_avg:87.96ms
step:642/1670 train_time:56469ms step_avg:87.96ms
step:643/1670 train_time:56557ms step_avg:87.96ms
step:644/1670 train_time:56645ms step_avg:87.96ms
step:645/1670 train_time:56733ms step_avg:87.96ms
step:646/1670 train_time:56823ms step_avg:87.96ms
step:647/1670 train_time:56912ms step_avg:87.96ms
step:648/1670 train_time:57003ms step_avg:87.97ms
step:649/1670 train_time:57092ms step_avg:87.97ms
step:650/1670 train_time:57184ms step_avg:87.97ms
step:651/1670 train_time:57273ms step_avg:87.98ms
step:652/1670 train_time:57363ms step_avg:87.98ms
step:653/1670 train_time:57451ms step_avg:87.98ms
step:654/1670 train_time:57539ms step_avg:87.98ms
step:655/1670 train_time:57627ms step_avg:87.98ms
step:656/1670 train_time:57715ms step_avg:87.98ms
step:657/1670 train_time:57803ms step_avg:87.98ms
step:658/1670 train_time:57892ms step_avg:87.98ms
step:659/1670 train_time:57982ms step_avg:87.98ms
step:660/1670 train_time:58071ms step_avg:87.99ms
step:661/1670 train_time:58161ms step_avg:87.99ms
step:662/1670 train_time:58249ms step_avg:87.99ms
step:663/1670 train_time:58339ms step_avg:87.99ms
step:664/1670 train_time:58427ms step_avg:87.99ms
step:665/1670 train_time:58516ms step_avg:87.99ms
step:666/1670 train_time:58604ms step_avg:87.99ms
step:667/1670 train_time:58692ms step_avg:87.99ms
step:668/1670 train_time:58781ms step_avg:88.00ms
step:669/1670 train_time:58869ms step_avg:88.00ms
step:670/1670 train_time:58958ms step_avg:88.00ms
step:671/1670 train_time:59046ms step_avg:88.00ms
step:672/1670 train_time:59136ms step_avg:88.00ms
step:673/1670 train_time:59226ms step_avg:88.00ms
step:674/1670 train_time:59315ms step_avg:88.00ms
step:675/1670 train_time:59405ms step_avg:88.01ms
step:676/1670 train_time:59494ms step_avg:88.01ms
step:677/1670 train_time:59583ms step_avg:88.01ms
step:678/1670 train_time:59671ms step_avg:88.01ms
step:679/1670 train_time:59759ms step_avg:88.01ms
step:680/1670 train_time:59848ms step_avg:88.01ms
step:681/1670 train_time:59937ms step_avg:88.01ms
step:682/1670 train_time:60027ms step_avg:88.02ms
step:683/1670 train_time:60116ms step_avg:88.02ms
step:684/1670 train_time:60205ms step_avg:88.02ms
step:685/1670 train_time:60295ms step_avg:88.02ms
step:686/1670 train_time:60384ms step_avg:88.02ms
step:687/1670 train_time:60472ms step_avg:88.02ms
step:688/1670 train_time:60560ms step_avg:88.02ms
step:689/1670 train_time:60649ms step_avg:88.02ms
step:690/1670 train_time:60738ms step_avg:88.03ms
step:691/1670 train_time:60827ms step_avg:88.03ms
step:692/1670 train_time:60915ms step_avg:88.03ms
step:693/1670 train_time:61004ms step_avg:88.03ms
step:694/1670 train_time:61094ms step_avg:88.03ms
step:695/1670 train_time:61183ms step_avg:88.03ms
step:696/1670 train_time:61272ms step_avg:88.03ms
step:697/1670 train_time:61362ms step_avg:88.04ms
step:698/1670 train_time:61451ms step_avg:88.04ms
step:699/1670 train_time:61539ms step_avg:88.04ms
step:700/1670 train_time:61628ms step_avg:88.04ms
step:701/1670 train_time:61717ms step_avg:88.04ms
step:702/1670 train_time:61806ms step_avg:88.04ms
step:703/1670 train_time:61895ms step_avg:88.04ms
step:704/1670 train_time:61984ms step_avg:88.05ms
step:705/1670 train_time:62073ms step_avg:88.05ms
step:706/1670 train_time:62163ms step_avg:88.05ms
step:707/1670 train_time:62252ms step_avg:88.05ms
step:708/1670 train_time:62341ms step_avg:88.05ms
step:709/1670 train_time:62430ms step_avg:88.05ms
step:710/1670 train_time:62519ms step_avg:88.05ms
step:711/1670 train_time:62607ms step_avg:88.06ms
step:712/1670 train_time:62697ms step_avg:88.06ms
step:713/1670 train_time:62785ms step_avg:88.06ms
step:714/1670 train_time:62874ms step_avg:88.06ms
step:715/1670 train_time:62964ms step_avg:88.06ms
step:716/1670 train_time:63052ms step_avg:88.06ms
step:717/1670 train_time:63142ms step_avg:88.06ms
step:718/1670 train_time:63231ms step_avg:88.07ms
step:719/1670 train_time:63322ms step_avg:88.07ms
step:720/1670 train_time:63410ms step_avg:88.07ms
step:721/1670 train_time:63500ms step_avg:88.07ms
step:722/1670 train_time:63588ms step_avg:88.07ms
step:723/1670 train_time:63677ms step_avg:88.07ms
step:724/1670 train_time:63765ms step_avg:88.07ms
step:725/1670 train_time:63855ms step_avg:88.08ms
step:726/1670 train_time:63944ms step_avg:88.08ms
step:727/1670 train_time:64033ms step_avg:88.08ms
step:728/1670 train_time:64123ms step_avg:88.08ms
step:729/1670 train_time:64212ms step_avg:88.08ms
step:730/1670 train_time:64301ms step_avg:88.08ms
step:731/1670 train_time:64389ms step_avg:88.08ms
step:732/1670 train_time:64478ms step_avg:88.08ms
step:733/1670 train_time:64567ms step_avg:88.09ms
step:734/1670 train_time:64656ms step_avg:88.09ms
step:735/1670 train_time:64744ms step_avg:88.09ms
step:736/1670 train_time:64834ms step_avg:88.09ms
step:737/1670 train_time:64923ms step_avg:88.09ms
step:738/1670 train_time:65011ms step_avg:88.09ms
step:739/1670 train_time:65101ms step_avg:88.09ms
step:740/1670 train_time:65189ms step_avg:88.09ms
step:741/1670 train_time:65279ms step_avg:88.10ms
step:742/1670 train_time:65368ms step_avg:88.10ms
step:743/1670 train_time:65456ms step_avg:88.10ms
step:744/1670 train_time:65545ms step_avg:88.10ms
step:745/1670 train_time:65635ms step_avg:88.10ms
step:746/1670 train_time:65723ms step_avg:88.10ms
step:747/1670 train_time:65812ms step_avg:88.10ms
step:748/1670 train_time:65902ms step_avg:88.10ms
step:749/1670 train_time:65990ms step_avg:88.10ms
step:750/1670 train_time:66080ms step_avg:88.11ms
step:750/1670 val_loss:3.5670 train_time:66170ms step_avg:88.23ms
step:751/1670 train_time:66189ms step_avg:88.13ms
step:752/1670 train_time:66264ms step_avg:88.12ms
step:753/1670 train_time:66356ms step_avg:88.12ms
step:754/1670 train_time:66444ms step_avg:88.12ms
step:755/1670 train_time:66534ms step_avg:88.12ms
step:756/1670 train_time:66621ms step_avg:88.12ms
step:757/1670 train_time:66709ms step_avg:88.12ms
step:758/1670 train_time:66797ms step_avg:88.12ms
step:759/1670 train_time:66885ms step_avg:88.12ms
step:760/1670 train_time:66973ms step_avg:88.12ms
step:761/1670 train_time:67061ms step_avg:88.12ms
step:762/1670 train_time:67152ms step_avg:88.13ms
step:763/1670 train_time:67244ms step_avg:88.13ms
step:764/1670 train_time:67337ms step_avg:88.14ms
step:765/1670 train_time:67426ms step_avg:88.14ms
step:766/1670 train_time:67515ms step_avg:88.14ms
step:767/1670 train_time:67603ms step_avg:88.14ms
step:768/1670 train_time:67691ms step_avg:88.14ms
step:769/1670 train_time:67779ms step_avg:88.14ms
step:770/1670 train_time:67867ms step_avg:88.14ms
step:771/1670 train_time:67955ms step_avg:88.14ms
step:772/1670 train_time:68043ms step_avg:88.14ms
step:773/1670 train_time:68134ms step_avg:88.14ms
step:774/1670 train_time:68224ms step_avg:88.14ms
step:775/1670 train_time:68314ms step_avg:88.15ms
step:776/1670 train_time:68403ms step_avg:88.15ms
step:777/1670 train_time:68493ms step_avg:88.15ms
step:778/1670 train_time:68581ms step_avg:88.15ms
step:779/1670 train_time:68670ms step_avg:88.15ms
step:780/1670 train_time:68759ms step_avg:88.15ms
step:781/1670 train_time:68847ms step_avg:88.15ms
step:782/1670 train_time:68935ms step_avg:88.15ms
step:783/1670 train_time:69023ms step_avg:88.15ms
step:784/1670 train_time:69113ms step_avg:88.15ms
step:785/1670 train_time:69202ms step_avg:88.16ms
step:786/1670 train_time:69292ms step_avg:88.16ms
step:787/1670 train_time:69381ms step_avg:88.16ms
step:788/1670 train_time:69471ms step_avg:88.16ms
step:789/1670 train_time:69559ms step_avg:88.16ms
step:790/1670 train_time:69648ms step_avg:88.16ms
step:791/1670 train_time:69737ms step_avg:88.16ms
step:792/1670 train_time:69825ms step_avg:88.16ms
step:793/1670 train_time:69914ms step_avg:88.16ms
step:794/1670 train_time:70002ms step_avg:88.16ms
step:795/1670 train_time:70092ms step_avg:88.17ms
step:796/1670 train_time:70181ms step_avg:88.17ms
step:797/1670 train_time:70271ms step_avg:88.17ms
step:798/1670 train_time:70361ms step_avg:88.17ms
step:799/1670 train_time:70451ms step_avg:88.17ms
step:800/1670 train_time:70540ms step_avg:88.17ms
step:801/1670 train_time:70628ms step_avg:88.18ms
step:802/1670 train_time:70717ms step_avg:88.18ms
step:803/1670 train_time:70805ms step_avg:88.18ms
step:804/1670 train_time:70894ms step_avg:88.18ms
step:805/1670 train_time:70982ms step_avg:88.18ms
step:806/1670 train_time:71071ms step_avg:88.18ms
step:807/1670 train_time:71161ms step_avg:88.18ms
step:808/1670 train_time:71250ms step_avg:88.18ms
step:809/1670 train_time:71339ms step_avg:88.18ms
step:810/1670 train_time:71429ms step_avg:88.18ms
step:811/1670 train_time:71518ms step_avg:88.18ms
step:812/1670 train_time:71606ms step_avg:88.19ms
step:813/1670 train_time:71696ms step_avg:88.19ms
step:814/1670 train_time:71784ms step_avg:88.19ms
step:815/1670 train_time:71873ms step_avg:88.19ms
step:816/1670 train_time:71962ms step_avg:88.19ms
step:817/1670 train_time:72051ms step_avg:88.19ms
step:818/1670 train_time:72140ms step_avg:88.19ms
step:819/1670 train_time:72229ms step_avg:88.19ms
step:820/1670 train_time:72318ms step_avg:88.19ms
step:821/1670 train_time:72407ms step_avg:88.19ms
step:822/1670 train_time:72496ms step_avg:88.19ms
step:823/1670 train_time:72584ms step_avg:88.19ms
step:824/1670 train_time:72673ms step_avg:88.20ms
step:825/1670 train_time:72762ms step_avg:88.20ms
step:826/1670 train_time:72851ms step_avg:88.20ms
step:827/1670 train_time:72940ms step_avg:88.20ms
step:828/1670 train_time:73029ms step_avg:88.20ms
step:829/1670 train_time:73118ms step_avg:88.20ms
step:830/1670 train_time:73207ms step_avg:88.20ms
step:831/1670 train_time:73297ms step_avg:88.20ms
step:832/1670 train_time:73386ms step_avg:88.20ms
step:833/1670 train_time:73476ms step_avg:88.21ms
step:834/1670 train_time:73564ms step_avg:88.21ms
step:835/1670 train_time:73654ms step_avg:88.21ms
step:836/1670 train_time:73742ms step_avg:88.21ms
step:837/1670 train_time:73831ms step_avg:88.21ms
step:838/1670 train_time:73920ms step_avg:88.21ms
step:839/1670 train_time:74009ms step_avg:88.21ms
step:840/1670 train_time:74099ms step_avg:88.21ms
step:841/1670 train_time:74188ms step_avg:88.21ms
step:842/1670 train_time:74277ms step_avg:88.22ms
step:843/1670 train_time:74366ms step_avg:88.22ms
step:844/1670 train_time:74455ms step_avg:88.22ms
step:845/1670 train_time:74544ms step_avg:88.22ms
step:846/1670 train_time:74634ms step_avg:88.22ms
step:847/1670 train_time:74722ms step_avg:88.22ms
step:848/1670 train_time:74811ms step_avg:88.22ms
step:849/1670 train_time:74900ms step_avg:88.22ms
step:850/1670 train_time:74989ms step_avg:88.22ms
step:851/1670 train_time:75078ms step_avg:88.22ms
step:852/1670 train_time:75167ms step_avg:88.22ms
step:853/1670 train_time:75258ms step_avg:88.23ms
step:854/1670 train_time:75346ms step_avg:88.23ms
step:855/1670 train_time:75435ms step_avg:88.23ms
step:856/1670 train_time:75524ms step_avg:88.23ms
step:857/1670 train_time:75614ms step_avg:88.23ms
step:858/1670 train_time:75702ms step_avg:88.23ms
step:859/1670 train_time:75792ms step_avg:88.23ms
step:860/1670 train_time:75881ms step_avg:88.23ms
step:861/1670 train_time:75970ms step_avg:88.23ms
step:862/1670 train_time:76058ms step_avg:88.23ms
step:863/1670 train_time:76147ms step_avg:88.24ms
step:864/1670 train_time:76236ms step_avg:88.24ms
step:865/1670 train_time:76326ms step_avg:88.24ms
step:866/1670 train_time:76415ms step_avg:88.24ms
step:867/1670 train_time:76504ms step_avg:88.24ms
step:868/1670 train_time:76593ms step_avg:88.24ms
step:869/1670 train_time:76682ms step_avg:88.24ms
step:870/1670 train_time:76771ms step_avg:88.24ms
step:871/1670 train_time:76861ms step_avg:88.24ms
step:872/1670 train_time:76951ms step_avg:88.25ms
step:873/1670 train_time:77040ms step_avg:88.25ms
step:874/1670 train_time:77129ms step_avg:88.25ms
step:875/1670 train_time:77218ms step_avg:88.25ms
step:875/1670 val_loss:3.5175 train_time:77308ms step_avg:88.35ms
step:876/1670 train_time:77328ms step_avg:88.27ms
step:877/1670 train_time:77400ms step_avg:88.26ms
step:878/1670 train_time:77493ms step_avg:88.26ms
step:879/1670 train_time:77581ms step_avg:88.26ms
step:880/1670 train_time:77669ms step_avg:88.26ms
step:881/1670 train_time:77757ms step_avg:88.26ms
step:882/1670 train_time:77844ms step_avg:88.26ms
step:883/1670 train_time:77932ms step_avg:88.26ms
step:884/1670 train_time:78021ms step_avg:88.26ms
step:885/1670 train_time:78109ms step_avg:88.26ms
step:886/1670 train_time:78197ms step_avg:88.26ms
step:887/1670 train_time:78288ms step_avg:88.26ms
step:888/1670 train_time:78379ms step_avg:88.26ms
step:889/1670 train_time:78469ms step_avg:88.27ms
step:890/1670 train_time:78559ms step_avg:88.27ms
step:891/1670 train_time:78648ms step_avg:88.27ms
step:892/1670 train_time:78737ms step_avg:88.27ms
step:893/1670 train_time:78825ms step_avg:88.27ms
step:894/1670 train_time:78912ms step_avg:88.27ms
step:895/1670 train_time:79001ms step_avg:88.27ms
step:896/1670 train_time:79089ms step_avg:88.27ms
step:897/1670 train_time:79178ms step_avg:88.27ms
step:898/1670 train_time:79267ms step_avg:88.27ms
step:899/1670 train_time:79358ms step_avg:88.27ms
step:900/1670 train_time:79448ms step_avg:88.28ms
step:901/1670 train_time:79538ms step_avg:88.28ms
step:902/1670 train_time:79628ms step_avg:88.28ms
step:903/1670 train_time:79718ms step_avg:88.28ms
step:904/1670 train_time:79806ms step_avg:88.28ms
step:905/1670 train_time:79895ms step_avg:88.28ms
step:906/1670 train_time:79984ms step_avg:88.28ms
step:907/1670 train_time:80072ms step_avg:88.28ms
step:908/1670 train_time:80161ms step_avg:88.28ms
step:909/1670 train_time:80250ms step_avg:88.28ms
step:910/1670 train_time:80340ms step_avg:88.29ms
step:911/1670 train_time:80429ms step_avg:88.29ms
step:912/1670 train_time:80519ms step_avg:88.29ms
step:913/1670 train_time:80609ms step_avg:88.29ms
step:914/1670 train_time:80698ms step_avg:88.29ms
step:915/1670 train_time:80786ms step_avg:88.29ms
step:916/1670 train_time:80875ms step_avg:88.29ms
step:917/1670 train_time:80963ms step_avg:88.29ms
step:918/1670 train_time:81052ms step_avg:88.29ms
step:919/1670 train_time:81140ms step_avg:88.29ms
step:920/1670 train_time:81230ms step_avg:88.29ms
step:921/1670 train_time:81319ms step_avg:88.29ms
step:922/1670 train_time:81409ms step_avg:88.30ms
step:923/1670 train_time:81499ms step_avg:88.30ms
step:924/1670 train_time:81588ms step_avg:88.30ms
step:925/1670 train_time:81677ms step_avg:88.30ms
step:926/1670 train_time:81766ms step_avg:88.30ms
step:927/1670 train_time:81856ms step_avg:88.30ms
step:928/1670 train_time:81944ms step_avg:88.30ms
step:929/1670 train_time:82032ms step_avg:88.30ms
step:930/1670 train_time:82122ms step_avg:88.30ms
step:931/1670 train_time:82211ms step_avg:88.30ms
step:932/1670 train_time:82300ms step_avg:88.31ms
step:933/1670 train_time:82390ms step_avg:88.31ms
step:934/1670 train_time:82480ms step_avg:88.31ms
step:935/1670 train_time:82570ms step_avg:88.31ms
step:936/1670 train_time:82659ms step_avg:88.31ms
step:937/1670 train_time:82748ms step_avg:88.31ms
step:938/1670 train_time:82836ms step_avg:88.31ms
step:939/1670 train_time:82925ms step_avg:88.31ms
step:940/1670 train_time:83014ms step_avg:88.31ms
step:941/1670 train_time:83102ms step_avg:88.31ms
step:942/1670 train_time:83191ms step_avg:88.31ms
step:943/1670 train_time:83281ms step_avg:88.31ms
step:944/1670 train_time:83369ms step_avg:88.31ms
step:945/1670 train_time:83458ms step_avg:88.32ms
step:946/1670 train_time:83547ms step_avg:88.32ms
step:947/1670 train_time:83637ms step_avg:88.32ms
step:948/1670 train_time:83726ms step_avg:88.32ms
step:949/1670 train_time:83815ms step_avg:88.32ms
step:950/1670 train_time:83905ms step_avg:88.32ms
step:951/1670 train_time:83994ms step_avg:88.32ms
step:952/1670 train_time:84085ms step_avg:88.32ms
step:953/1670 train_time:84173ms step_avg:88.32ms
step:954/1670 train_time:84263ms step_avg:88.33ms
step:955/1670 train_time:84351ms step_avg:88.33ms
step:956/1670 train_time:84440ms step_avg:88.33ms
step:957/1670 train_time:84529ms step_avg:88.33ms
step:958/1670 train_time:84618ms step_avg:88.33ms
step:959/1670 train_time:84708ms step_avg:88.33ms
step:960/1670 train_time:84797ms step_avg:88.33ms
step:961/1670 train_time:84887ms step_avg:88.33ms
step:962/1670 train_time:84976ms step_avg:88.33ms
step:963/1670 train_time:85065ms step_avg:88.33ms
step:964/1670 train_time:85154ms step_avg:88.33ms
step:965/1670 train_time:85244ms step_avg:88.34ms
step:966/1670 train_time:85332ms step_avg:88.34ms
step:967/1670 train_time:85421ms step_avg:88.34ms
step:968/1670 train_time:85510ms step_avg:88.34ms
step:969/1670 train_time:85601ms step_avg:88.34ms
step:970/1670 train_time:85690ms step_avg:88.34ms
step:971/1670 train_time:85779ms step_avg:88.34ms
step:972/1670 train_time:85867ms step_avg:88.34ms
step:973/1670 train_time:85956ms step_avg:88.34ms
step:974/1670 train_time:86045ms step_avg:88.34ms
step:975/1670 train_time:86134ms step_avg:88.34ms
step:976/1670 train_time:86224ms step_avg:88.34ms
step:977/1670 train_time:86312ms step_avg:88.34ms
step:978/1670 train_time:86401ms step_avg:88.34ms
step:979/1670 train_time:86490ms step_avg:88.35ms
step:980/1670 train_time:86580ms step_avg:88.35ms
step:981/1670 train_time:86669ms step_avg:88.35ms
step:982/1670 train_time:86758ms step_avg:88.35ms
step:983/1670 train_time:86847ms step_avg:88.35ms
step:984/1670 train_time:86936ms step_avg:88.35ms
step:985/1670 train_time:87025ms step_avg:88.35ms
step:986/1670 train_time:87113ms step_avg:88.35ms
step:987/1670 train_time:87203ms step_avg:88.35ms
step:988/1670 train_time:87291ms step_avg:88.35ms
step:989/1670 train_time:87381ms step_avg:88.35ms
step:990/1670 train_time:87470ms step_avg:88.35ms
step:991/1670 train_time:87560ms step_avg:88.36ms
step:992/1670 train_time:87649ms step_avg:88.36ms
step:993/1670 train_time:87738ms step_avg:88.36ms
step:994/1670 train_time:87828ms step_avg:88.36ms
step:995/1670 train_time:87917ms step_avg:88.36ms
step:996/1670 train_time:88006ms step_avg:88.36ms
step:997/1670 train_time:88095ms step_avg:88.36ms
step:998/1670 train_time:88185ms step_avg:88.36ms
step:999/1670 train_time:88273ms step_avg:88.36ms
step:1000/1670 train_time:88363ms step_avg:88.36ms
step:1000/1670 val_loss:3.4676 train_time:88454ms step_avg:88.45ms
step:1001/1670 train_time:88473ms step_avg:88.38ms
step:1002/1670 train_time:88547ms step_avg:88.37ms
step:1003/1670 train_time:88641ms step_avg:88.38ms
step:1004/1670 train_time:88732ms step_avg:88.38ms
step:1005/1670 train_time:88819ms step_avg:88.38ms
step:1006/1670 train_time:88907ms step_avg:88.38ms
step:1007/1670 train_time:88995ms step_avg:88.38ms
step:1008/1670 train_time:89082ms step_avg:88.37ms
step:1009/1670 train_time:89169ms step_avg:88.37ms
step:1010/1670 train_time:89257ms step_avg:88.37ms
step:1011/1670 train_time:89345ms step_avg:88.37ms
step:1012/1670 train_time:89436ms step_avg:88.38ms
step:1013/1670 train_time:89528ms step_avg:88.38ms
step:1014/1670 train_time:89620ms step_avg:88.38ms
step:1015/1670 train_time:89711ms step_avg:88.38ms
step:1016/1670 train_time:89800ms step_avg:88.39ms
step:1017/1670 train_time:89888ms step_avg:88.39ms
step:1018/1670 train_time:89976ms step_avg:88.39ms
step:1019/1670 train_time:90064ms step_avg:88.38ms
step:1020/1670 train_time:90152ms step_avg:88.38ms
step:1021/1670 train_time:90239ms step_avg:88.38ms
step:1022/1670 train_time:90327ms step_avg:88.38ms
step:1023/1670 train_time:90416ms step_avg:88.38ms
step:1024/1670 train_time:90507ms step_avg:88.39ms
step:1025/1670 train_time:90599ms step_avg:88.39ms
step:1026/1670 train_time:90690ms step_avg:88.39ms
step:1027/1670 train_time:90780ms step_avg:88.39ms
step:1028/1670 train_time:90869ms step_avg:88.39ms
step:1029/1670 train_time:90958ms step_avg:88.39ms
step:1030/1670 train_time:91046ms step_avg:88.39ms
step:1031/1670 train_time:91134ms step_avg:88.39ms
step:1032/1670 train_time:91222ms step_avg:88.39ms
step:1033/1670 train_time:91310ms step_avg:88.39ms
step:1034/1670 train_time:91399ms step_avg:88.39ms
step:1035/1670 train_time:91489ms step_avg:88.39ms
step:1036/1670 train_time:91578ms step_avg:88.40ms
step:1037/1670 train_time:91668ms step_avg:88.40ms
step:1038/1670 train_time:91759ms step_avg:88.40ms
step:1039/1670 train_time:91849ms step_avg:88.40ms
step:1040/1670 train_time:91937ms step_avg:88.40ms
step:1041/1670 train_time:92026ms step_avg:88.40ms
step:1042/1670 train_time:92116ms step_avg:88.40ms
step:1043/1670 train_time:92204ms step_avg:88.40ms
step:1044/1670 train_time:92292ms step_avg:88.40ms
step:1045/1670 train_time:92381ms step_avg:88.40ms
step:1046/1670 train_time:92470ms step_avg:88.40ms
step:1047/1670 train_time:92561ms step_avg:88.41ms
step:1048/1670 train_time:92650ms step_avg:88.41ms
step:1049/1670 train_time:92740ms step_avg:88.41ms
step:1050/1670 train_time:92830ms step_avg:88.41ms
step:1051/1670 train_time:92919ms step_avg:88.41ms
step:1052/1670 train_time:93009ms step_avg:88.41ms
step:1053/1670 train_time:93097ms step_avg:88.41ms
step:1054/1670 train_time:93186ms step_avg:88.41ms
step:1055/1670 train_time:93274ms step_avg:88.41ms
step:1056/1670 train_time:93362ms step_avg:88.41ms
step:1057/1670 train_time:93452ms step_avg:88.41ms
step:1058/1670 train_time:93541ms step_avg:88.41ms
step:1059/1670 train_time:93631ms step_avg:88.41ms
step:1060/1670 train_time:93720ms step_avg:88.42ms
step:1061/1670 train_time:93810ms step_avg:88.42ms
step:1062/1670 train_time:93899ms step_avg:88.42ms
step:1063/1670 train_time:93989ms step_avg:88.42ms
step:1064/1670 train_time:94078ms step_avg:88.42ms
step:1065/1670 train_time:94167ms step_avg:88.42ms
step:1066/1670 train_time:94256ms step_avg:88.42ms
step:1067/1670 train_time:94344ms step_avg:88.42ms
step:1068/1670 train_time:94434ms step_avg:88.42ms
step:1069/1670 train_time:94524ms step_avg:88.42ms
step:1070/1670 train_time:94615ms step_avg:88.42ms
step:1071/1670 train_time:94702ms step_avg:88.42ms
step:1072/1670 train_time:94792ms step_avg:88.43ms
step:1073/1670 train_time:94881ms step_avg:88.43ms
step:1074/1670 train_time:94970ms step_avg:88.43ms
step:1075/1670 train_time:95059ms step_avg:88.43ms
step:1076/1670 train_time:95148ms step_avg:88.43ms
step:1077/1670 train_time:95237ms step_avg:88.43ms
step:1078/1670 train_time:95326ms step_avg:88.43ms
step:1079/1670 train_time:95416ms step_avg:88.43ms
step:1080/1670 train_time:95505ms step_avg:88.43ms
step:1081/1670 train_time:95595ms step_avg:88.43ms
step:1082/1670 train_time:95683ms step_avg:88.43ms
step:1083/1670 train_time:95773ms step_avg:88.43ms
step:1084/1670 train_time:95861ms step_avg:88.43ms
step:1085/1670 train_time:95951ms step_avg:88.43ms
step:1086/1670 train_time:96039ms step_avg:88.43ms
step:1087/1670 train_time:96128ms step_avg:88.43ms
step:1088/1670 train_time:96216ms step_avg:88.43ms
step:1089/1670 train_time:96306ms step_avg:88.43ms
step:1090/1670 train_time:96396ms step_avg:88.44ms
step:1091/1670 train_time:96486ms step_avg:88.44ms
step:1092/1670 train_time:96577ms step_avg:88.44ms
step:1093/1670 train_time:96666ms step_avg:88.44ms
step:1094/1670 train_time:96756ms step_avg:88.44ms
step:1095/1670 train_time:96846ms step_avg:88.44ms
step:1096/1670 train_time:96936ms step_avg:88.45ms
step:1097/1670 train_time:97026ms step_avg:88.45ms
step:1098/1670 train_time:97116ms step_avg:88.45ms
step:1099/1670 train_time:97205ms step_avg:88.45ms
step:1100/1670 train_time:97295ms step_avg:88.45ms
step:1101/1670 train_time:97385ms step_avg:88.45ms
step:1102/1670 train_time:97475ms step_avg:88.45ms
step:1103/1670 train_time:97564ms step_avg:88.45ms
step:1104/1670 train_time:97655ms step_avg:88.46ms
step:1105/1670 train_time:97744ms step_avg:88.46ms
step:1106/1670 train_time:97834ms step_avg:88.46ms
step:1107/1670 train_time:97924ms step_avg:88.46ms
step:1108/1670 train_time:98014ms step_avg:88.46ms
step:1109/1670 train_time:98103ms step_avg:88.46ms
step:1110/1670 train_time:98193ms step_avg:88.46ms
step:1111/1670 train_time:98282ms step_avg:88.46ms
step:1112/1670 train_time:98372ms step_avg:88.46ms
step:1113/1670 train_time:98462ms step_avg:88.46ms
step:1114/1670 train_time:98552ms step_avg:88.47ms
step:1115/1670 train_time:98641ms step_avg:88.47ms
step:1116/1670 train_time:98732ms step_avg:88.47ms
step:1117/1670 train_time:98822ms step_avg:88.47ms
step:1118/1670 train_time:98913ms step_avg:88.47ms
step:1119/1670 train_time:99002ms step_avg:88.47ms
step:1120/1670 train_time:99092ms step_avg:88.47ms
step:1121/1670 train_time:99180ms step_avg:88.47ms
step:1122/1670 train_time:99270ms step_avg:88.48ms
step:1123/1670 train_time:99360ms step_avg:88.48ms
step:1124/1670 train_time:99450ms step_avg:88.48ms
step:1125/1670 train_time:99540ms step_avg:88.48ms
step:1125/1670 val_loss:3.4131 train_time:99631ms step_avg:88.56ms
step:1126/1670 train_time:99650ms step_avg:88.50ms
step:1127/1670 train_time:99723ms step_avg:88.49ms
step:1128/1670 train_time:99812ms step_avg:88.49ms
step:1129/1670 train_time:99903ms step_avg:88.49ms
step:1130/1670 train_time:99992ms step_avg:88.49ms
step:1131/1670 train_time:100082ms step_avg:88.49ms
step:1132/1670 train_time:100170ms step_avg:88.49ms
step:1133/1670 train_time:100259ms step_avg:88.49ms
step:1134/1670 train_time:100348ms step_avg:88.49ms
step:1135/1670 train_time:100436ms step_avg:88.49ms
step:1136/1670 train_time:100527ms step_avg:88.49ms
step:1137/1670 train_time:100619ms step_avg:88.50ms
step:1138/1670 train_time:100711ms step_avg:88.50ms
step:1139/1670 train_time:100801ms step_avg:88.50ms
step:1140/1670 train_time:100891ms step_avg:88.50ms
step:1141/1670 train_time:100981ms step_avg:88.50ms
step:1142/1670 train_time:101071ms step_avg:88.50ms
step:1143/1670 train_time:101160ms step_avg:88.50ms
step:1144/1670 train_time:101249ms step_avg:88.50ms
step:1145/1670 train_time:101338ms step_avg:88.51ms
step:1146/1670 train_time:101427ms step_avg:88.51ms
step:1147/1670 train_time:101517ms step_avg:88.51ms
step:1148/1670 train_time:101607ms step_avg:88.51ms
step:1149/1670 train_time:101698ms step_avg:88.51ms
step:1150/1670 train_time:101788ms step_avg:88.51ms
step:1151/1670 train_time:101878ms step_avg:88.51ms
step:1152/1670 train_time:101968ms step_avg:88.51ms
step:1153/1670 train_time:102058ms step_avg:88.52ms
step:1154/1670 train_time:102148ms step_avg:88.52ms
step:1155/1670 train_time:102237ms step_avg:88.52ms
step:1156/1670 train_time:102326ms step_avg:88.52ms
step:1157/1670 train_time:102415ms step_avg:88.52ms
step:1158/1670 train_time:102505ms step_avg:88.52ms
step:1159/1670 train_time:102595ms step_avg:88.52ms
step:1160/1670 train_time:102686ms step_avg:88.52ms
step:1161/1670 train_time:102775ms step_avg:88.52ms
step:1162/1670 train_time:102867ms step_avg:88.53ms
step:1163/1670 train_time:102957ms step_avg:88.53ms
step:1164/1670 train_time:103047ms step_avg:88.53ms
step:1165/1670 train_time:103136ms step_avg:88.53ms
step:1166/1670 train_time:103226ms step_avg:88.53ms
step:1167/1670 train_time:103315ms step_avg:88.53ms
step:1168/1670 train_time:103404ms step_avg:88.53ms
step:1169/1670 train_time:103493ms step_avg:88.53ms
step:1170/1670 train_time:103583ms step_avg:88.53ms
step:1171/1670 train_time:103672ms step_avg:88.53ms
step:1172/1670 train_time:103763ms step_avg:88.53ms
step:1173/1670 train_time:103852ms step_avg:88.54ms
step:1174/1670 train_time:103943ms step_avg:88.54ms
step:1175/1670 train_time:104032ms step_avg:88.54ms
step:1176/1670 train_time:104123ms step_avg:88.54ms
step:1177/1670 train_time:104212ms step_avg:88.54ms
step:1178/1670 train_time:104301ms step_avg:88.54ms
step:1179/1670 train_time:104390ms step_avg:88.54ms
step:1180/1670 train_time:104479ms step_avg:88.54ms
step:1181/1670 train_time:104569ms step_avg:88.54ms
step:1182/1670 train_time:104659ms step_avg:88.54ms
step:1183/1670 train_time:104749ms step_avg:88.55ms
step:1184/1670 train_time:104841ms step_avg:88.55ms
step:1185/1670 train_time:104931ms step_avg:88.55ms
step:1186/1670 train_time:105020ms step_avg:88.55ms
step:1187/1670 train_time:105110ms step_avg:88.55ms
step:1188/1670 train_time:105200ms step_avg:88.55ms
step:1189/1670 train_time:105289ms step_avg:88.55ms
step:1190/1670 train_time:105380ms step_avg:88.55ms
step:1191/1670 train_time:105469ms step_avg:88.55ms
step:1192/1670 train_time:105559ms step_avg:88.56ms
step:1193/1670 train_time:105650ms step_avg:88.56ms
step:1194/1670 train_time:105741ms step_avg:88.56ms
step:1195/1670 train_time:105831ms step_avg:88.56ms
step:1196/1670 train_time:105920ms step_avg:88.56ms
step:1197/1670 train_time:106009ms step_avg:88.56ms
step:1198/1670 train_time:106098ms step_avg:88.56ms
step:1199/1670 train_time:106188ms step_avg:88.56ms
step:1200/1670 train_time:106278ms step_avg:88.57ms
step:1201/1670 train_time:106368ms step_avg:88.57ms
step:1202/1670 train_time:106457ms step_avg:88.57ms
step:1203/1670 train_time:106547ms step_avg:88.57ms
step:1204/1670 train_time:106637ms step_avg:88.57ms
step:1205/1670 train_time:106727ms step_avg:88.57ms
step:1206/1670 train_time:106817ms step_avg:88.57ms
step:1207/1670 train_time:106908ms step_avg:88.57ms
step:1208/1670 train_time:106997ms step_avg:88.57ms
step:1209/1670 train_time:107088ms step_avg:88.58ms
step:1210/1670 train_time:107178ms step_avg:88.58ms
step:1211/1670 train_time:107268ms step_avg:88.58ms
step:1212/1670 train_time:107359ms step_avg:88.58ms
step:1213/1670 train_time:107449ms step_avg:88.58ms
step:1214/1670 train_time:107539ms step_avg:88.58ms
step:1215/1670 train_time:107628ms step_avg:88.58ms
step:1216/1670 train_time:107718ms step_avg:88.58ms
step:1217/1670 train_time:107807ms step_avg:88.58ms
step:1218/1670 train_time:107896ms step_avg:88.58ms
step:1219/1670 train_time:107987ms step_avg:88.59ms
step:1220/1670 train_time:108077ms step_avg:88.59ms
step:1221/1670 train_time:108166ms step_avg:88.59ms
step:1222/1670 train_time:108256ms step_avg:88.59ms
step:1223/1670 train_time:108346ms step_avg:88.59ms
step:1224/1670 train_time:108436ms step_avg:88.59ms
step:1225/1670 train_time:108525ms step_avg:88.59ms
step:1226/1670 train_time:108614ms step_avg:88.59ms
step:1227/1670 train_time:108704ms step_avg:88.59ms
step:1228/1670 train_time:108793ms step_avg:88.59ms
step:1229/1670 train_time:108883ms step_avg:88.60ms
step:1230/1670 train_time:108972ms step_avg:88.60ms
step:1231/1670 train_time:109062ms step_avg:88.60ms
step:1232/1670 train_time:109151ms step_avg:88.60ms
step:1233/1670 train_time:109241ms step_avg:88.60ms
step:1234/1670 train_time:109331ms step_avg:88.60ms
step:1235/1670 train_time:109420ms step_avg:88.60ms
step:1236/1670 train_time:109509ms step_avg:88.60ms
step:1237/1670 train_time:109599ms step_avg:88.60ms
step:1238/1670 train_time:109689ms step_avg:88.60ms
step:1239/1670 train_time:109780ms step_avg:88.60ms
step:1240/1670 train_time:109870ms step_avg:88.60ms
step:1241/1670 train_time:109960ms step_avg:88.61ms
step:1242/1670 train_time:110050ms step_avg:88.61ms
step:1243/1670 train_time:110140ms step_avg:88.61ms
step:1244/1670 train_time:110229ms step_avg:88.61ms
step:1245/1670 train_time:110319ms step_avg:88.61ms
step:1246/1670 train_time:110408ms step_avg:88.61ms
step:1247/1670 train_time:110497ms step_avg:88.61ms
step:1248/1670 train_time:110587ms step_avg:88.61ms
step:1249/1670 train_time:110677ms step_avg:88.61ms
step:1250/1670 train_time:110767ms step_avg:88.61ms
step:1250/1670 val_loss:3.3752 train_time:110858ms step_avg:88.69ms
step:1251/1670 train_time:110879ms step_avg:88.63ms
step:1252/1670 train_time:110952ms step_avg:88.62ms
step:1253/1670 train_time:111043ms step_avg:88.62ms
step:1254/1670 train_time:111134ms step_avg:88.62ms
step:1255/1670 train_time:111222ms step_avg:88.62ms
step:1256/1670 train_time:111311ms step_avg:88.62ms
step:1257/1670 train_time:111399ms step_avg:88.62ms
step:1258/1670 train_time:111488ms step_avg:88.62ms
step:1259/1670 train_time:111576ms step_avg:88.62ms
step:1260/1670 train_time:111666ms step_avg:88.62ms
step:1261/1670 train_time:111754ms step_avg:88.62ms
step:1262/1670 train_time:111845ms step_avg:88.63ms
step:1263/1670 train_time:111936ms step_avg:88.63ms
step:1264/1670 train_time:112029ms step_avg:88.63ms
step:1265/1670 train_time:112120ms step_avg:88.63ms
step:1266/1670 train_time:112210ms step_avg:88.63ms
step:1267/1670 train_time:112300ms step_avg:88.63ms
step:1268/1670 train_time:112389ms step_avg:88.63ms
step:1269/1670 train_time:112478ms step_avg:88.64ms
step:1270/1670 train_time:112567ms step_avg:88.64ms
step:1271/1670 train_time:112656ms step_avg:88.64ms
step:1272/1670 train_time:112746ms step_avg:88.64ms
step:1273/1670 train_time:112835ms step_avg:88.64ms
step:1274/1670 train_time:112927ms step_avg:88.64ms
step:1275/1670 train_time:113017ms step_avg:88.64ms
step:1276/1670 train_time:113108ms step_avg:88.64ms
step:1277/1670 train_time:113197ms step_avg:88.64ms
step:1278/1670 train_time:113287ms step_avg:88.64ms
step:1279/1670 train_time:113376ms step_avg:88.64ms
step:1280/1670 train_time:113465ms step_avg:88.64ms
step:1281/1670 train_time:113553ms step_avg:88.64ms
step:1282/1670 train_time:113643ms step_avg:88.65ms
step:1283/1670 train_time:113732ms step_avg:88.65ms
step:1284/1670 train_time:113823ms step_avg:88.65ms
step:1285/1670 train_time:113913ms step_avg:88.65ms
step:1286/1670 train_time:114004ms step_avg:88.65ms
step:1287/1670 train_time:114093ms step_avg:88.65ms
step:1288/1670 train_time:114184ms step_avg:88.65ms
step:1289/1670 train_time:114275ms step_avg:88.65ms
step:1290/1670 train_time:114366ms step_avg:88.66ms
step:1291/1670 train_time:114454ms step_avg:88.66ms
step:1292/1670 train_time:114543ms step_avg:88.66ms
step:1293/1670 train_time:114632ms step_avg:88.66ms
step:1294/1670 train_time:114721ms step_avg:88.66ms
step:1295/1670 train_time:114812ms step_avg:88.66ms
step:1296/1670 train_time:114902ms step_avg:88.66ms
step:1297/1670 train_time:114992ms step_avg:88.66ms
step:1298/1670 train_time:115081ms step_avg:88.66ms
step:1299/1670 train_time:115171ms step_avg:88.66ms
step:1300/1670 train_time:115261ms step_avg:88.66ms
step:1301/1670 train_time:115351ms step_avg:88.66ms
step:1302/1670 train_time:115441ms step_avg:88.66ms
step:1303/1670 train_time:115531ms step_avg:88.67ms
step:1304/1670 train_time:115620ms step_avg:88.67ms
step:1305/1670 train_time:115709ms step_avg:88.67ms
step:1306/1670 train_time:115800ms step_avg:88.67ms
step:1307/1670 train_time:115890ms step_avg:88.67ms
step:1308/1670 train_time:115980ms step_avg:88.67ms
step:1309/1670 train_time:116069ms step_avg:88.67ms
step:1310/1670 train_time:116159ms step_avg:88.67ms
step:1311/1670 train_time:116249ms step_avg:88.67ms
step:1312/1670 train_time:116339ms step_avg:88.67ms
step:1313/1670 train_time:116429ms step_avg:88.67ms
step:1314/1670 train_time:116518ms step_avg:88.67ms
step:1315/1670 train_time:116608ms step_avg:88.68ms
step:1316/1670 train_time:116697ms step_avg:88.68ms
step:1317/1670 train_time:116788ms step_avg:88.68ms
step:1318/1670 train_time:116878ms step_avg:88.68ms
step:1319/1670 train_time:116969ms step_avg:88.68ms
step:1320/1670 train_time:117059ms step_avg:88.68ms
step:1321/1670 train_time:117148ms step_avg:88.68ms
step:1322/1670 train_time:117237ms step_avg:88.68ms
step:1323/1670 train_time:117328ms step_avg:88.68ms
step:1324/1670 train_time:117418ms step_avg:88.68ms
step:1325/1670 train_time:117508ms step_avg:88.69ms
step:1326/1670 train_time:117598ms step_avg:88.69ms
step:1327/1670 train_time:117689ms step_avg:88.69ms
step:1328/1670 train_time:117778ms step_avg:88.69ms
step:1329/1670 train_time:117870ms step_avg:88.69ms
step:1330/1670 train_time:117959ms step_avg:88.69ms
step:1331/1670 train_time:118049ms step_avg:88.69ms
step:1332/1670 train_time:118139ms step_avg:88.69ms
step:1333/1670 train_time:118229ms step_avg:88.69ms
step:1334/1670 train_time:118318ms step_avg:88.69ms
step:1335/1670 train_time:118409ms step_avg:88.70ms
step:1336/1670 train_time:118498ms step_avg:88.70ms
step:1337/1670 train_time:118588ms step_avg:88.70ms
step:1338/1670 train_time:118677ms step_avg:88.70ms
step:1339/1670 train_time:118768ms step_avg:88.70ms
step:1340/1670 train_time:118857ms step_avg:88.70ms
step:1341/1670 train_time:118947ms step_avg:88.70ms
step:1342/1670 train_time:119036ms step_avg:88.70ms
step:1343/1670 train_time:119127ms step_avg:88.70ms
step:1344/1670 train_time:119216ms step_avg:88.70ms
step:1345/1670 train_time:119306ms step_avg:88.70ms
step:1346/1670 train_time:119395ms step_avg:88.70ms
step:1347/1670 train_time:119485ms step_avg:88.70ms
step:1348/1670 train_time:119575ms step_avg:88.71ms
step:1349/1670 train_time:119666ms step_avg:88.71ms
step:1350/1670 train_time:119755ms step_avg:88.71ms
step:1351/1670 train_time:119846ms step_avg:88.71ms
step:1352/1670 train_time:119935ms step_avg:88.71ms
step:1353/1670 train_time:120026ms step_avg:88.71ms
step:1354/1670 train_time:120116ms step_avg:88.71ms
step:1355/1670 train_time:120206ms step_avg:88.71ms
step:1356/1670 train_time:120295ms step_avg:88.71ms
step:1357/1670 train_time:120385ms step_avg:88.71ms
step:1358/1670 train_time:120475ms step_avg:88.71ms
step:1359/1670 train_time:120565ms step_avg:88.72ms
step:1360/1670 train_time:120655ms step_avg:88.72ms
step:1361/1670 train_time:120745ms step_avg:88.72ms
step:1362/1670 train_time:120834ms step_avg:88.72ms
step:1363/1670 train_time:120924ms step_avg:88.72ms
step:1364/1670 train_time:121013ms step_avg:88.72ms
step:1365/1670 train_time:121103ms step_avg:88.72ms
step:1366/1670 train_time:121192ms step_avg:88.72ms
step:1367/1670 train_time:121282ms step_avg:88.72ms
step:1368/1670 train_time:121371ms step_avg:88.72ms
step:1369/1670 train_time:121463ms step_avg:88.72ms
step:1370/1670 train_time:121552ms step_avg:88.72ms
step:1371/1670 train_time:121642ms step_avg:88.73ms
step:1372/1670 train_time:121732ms step_avg:88.73ms
step:1373/1670 train_time:121822ms step_avg:88.73ms
step:1374/1670 train_time:121912ms step_avg:88.73ms
step:1375/1670 train_time:122001ms step_avg:88.73ms
step:1375/1670 val_loss:3.3403 train_time:122093ms step_avg:88.79ms
step:1376/1670 train_time:122112ms step_avg:88.74ms
step:1377/1670 train_time:122188ms step_avg:88.74ms
step:1378/1670 train_time:122282ms step_avg:88.74ms
step:1379/1670 train_time:122372ms step_avg:88.74ms
step:1380/1670 train_time:122460ms step_avg:88.74ms
step:1381/1670 train_time:122550ms step_avg:88.74ms
step:1382/1670 train_time:122637ms step_avg:88.74ms
step:1383/1670 train_time:122726ms step_avg:88.74ms
step:1384/1670 train_time:122815ms step_avg:88.74ms
step:1385/1670 train_time:122904ms step_avg:88.74ms
step:1386/1670 train_time:122994ms step_avg:88.74ms
step:1387/1670 train_time:123086ms step_avg:88.74ms
step:1388/1670 train_time:123178ms step_avg:88.74ms
step:1389/1670 train_time:123270ms step_avg:88.75ms
step:1390/1670 train_time:123361ms step_avg:88.75ms
step:1391/1670 train_time:123450ms step_avg:88.75ms
step:1392/1670 train_time:123539ms step_avg:88.75ms
step:1393/1670 train_time:123628ms step_avg:88.75ms
step:1394/1670 train_time:123717ms step_avg:88.75ms
step:1395/1670 train_time:123806ms step_avg:88.75ms
step:1396/1670 train_time:123895ms step_avg:88.75ms
step:1397/1670 train_time:123984ms step_avg:88.75ms
step:1398/1670 train_time:124074ms step_avg:88.75ms
step:1399/1670 train_time:124165ms step_avg:88.75ms
step:1400/1670 train_time:124256ms step_avg:88.75ms
step:1401/1670 train_time:124346ms step_avg:88.76ms
step:1402/1670 train_time:124436ms step_avg:88.76ms
step:1403/1670 train_time:124526ms step_avg:88.76ms
step:1404/1670 train_time:124616ms step_avg:88.76ms
step:1405/1670 train_time:124705ms step_avg:88.76ms
step:1406/1670 train_time:124794ms step_avg:88.76ms
step:1407/1670 train_time:124884ms step_avg:88.76ms
step:1408/1670 train_time:124973ms step_avg:88.76ms
step:1409/1670 train_time:125063ms step_avg:88.76ms
step:1410/1670 train_time:125153ms step_avg:88.76ms
step:1411/1670 train_time:125244ms step_avg:88.76ms
step:1412/1670 train_time:125335ms step_avg:88.76ms
step:1413/1670 train_time:125426ms step_avg:88.77ms
step:1414/1670 train_time:125516ms step_avg:88.77ms
step:1415/1670 train_time:125606ms step_avg:88.77ms
step:1416/1670 train_time:125695ms step_avg:88.77ms
step:1417/1670 train_time:125786ms step_avg:88.77ms
step:1418/1670 train_time:125874ms step_avg:88.77ms
step:1419/1670 train_time:125964ms step_avg:88.77ms
step:1420/1670 train_time:126053ms step_avg:88.77ms
step:1421/1670 train_time:126143ms step_avg:88.77ms
step:1422/1670 train_time:126233ms step_avg:88.77ms
step:1423/1670 train_time:126324ms step_avg:88.77ms
step:1424/1670 train_time:126415ms step_avg:88.77ms
step:1425/1670 train_time:126505ms step_avg:88.78ms
step:1426/1670 train_time:126594ms step_avg:88.78ms
step:1427/1670 train_time:126684ms step_avg:88.78ms
step:1428/1670 train_time:126773ms step_avg:88.78ms
step:1429/1670 train_time:126863ms step_avg:88.78ms
step:1430/1670 train_time:126952ms step_avg:88.78ms
step:1431/1670 train_time:127041ms step_avg:88.78ms
step:1432/1670 train_time:127132ms step_avg:88.78ms
step:1433/1670 train_time:127223ms step_avg:88.78ms
step:1434/1670 train_time:127314ms step_avg:88.78ms
step:1435/1670 train_time:127405ms step_avg:88.78ms
step:1436/1670 train_time:127496ms step_avg:88.79ms
step:1437/1670 train_time:127585ms step_avg:88.79ms
step:1438/1670 train_time:127674ms step_avg:88.79ms
step:1439/1670 train_time:127764ms step_avg:88.79ms
step:1440/1670 train_time:127854ms step_avg:88.79ms
step:1441/1670 train_time:127943ms step_avg:88.79ms
step:1442/1670 train_time:128033ms step_avg:88.79ms
step:1443/1670 train_time:128123ms step_avg:88.79ms
step:1444/1670 train_time:128214ms step_avg:88.79ms
step:1445/1670 train_time:128306ms step_avg:88.79ms
step:1446/1670 train_time:128395ms step_avg:88.79ms
step:1447/1670 train_time:128485ms step_avg:88.79ms
step:1448/1670 train_time:128575ms step_avg:88.79ms
step:1449/1670 train_time:128665ms step_avg:88.80ms
step:1450/1670 train_time:128754ms step_avg:88.80ms
step:1451/1670 train_time:128843ms step_avg:88.80ms
step:1452/1670 train_time:128933ms step_avg:88.80ms
step:1453/1670 train_time:129023ms step_avg:88.80ms
step:1454/1670 train_time:129114ms step_avg:88.80ms
step:1455/1670 train_time:129204ms step_avg:88.80ms
step:1456/1670 train_time:129296ms step_avg:88.80ms
step:1457/1670 train_time:129386ms step_avg:88.80ms
step:1458/1670 train_time:129475ms step_avg:88.80ms
step:1459/1670 train_time:129564ms step_avg:88.80ms
step:1460/1670 train_time:129654ms step_avg:88.80ms
step:1461/1670 train_time:129743ms step_avg:88.80ms
step:1462/1670 train_time:129833ms step_avg:88.80ms
step:1463/1670 train_time:129923ms step_avg:88.81ms
step:1464/1670 train_time:130014ms step_avg:88.81ms
step:1465/1670 train_time:130104ms step_avg:88.81ms
step:1466/1670 train_time:130195ms step_avg:88.81ms
step:1467/1670 train_time:130286ms step_avg:88.81ms
step:1468/1670 train_time:130376ms step_avg:88.81ms
step:1469/1670 train_time:130465ms step_avg:88.81ms
step:1470/1670 train_time:130554ms step_avg:88.81ms
step:1471/1670 train_time:130643ms step_avg:88.81ms
step:1472/1670 train_time:130734ms step_avg:88.81ms
step:1473/1670 train_time:130824ms step_avg:88.81ms
step:1474/1670 train_time:130913ms step_avg:88.81ms
step:1475/1670 train_time:131003ms step_avg:88.82ms
step:1476/1670 train_time:131094ms step_avg:88.82ms
step:1477/1670 train_time:131185ms step_avg:88.82ms
step:1478/1670 train_time:131275ms step_avg:88.82ms
step:1479/1670 train_time:131366ms step_avg:88.82ms
step:1480/1670 train_time:131455ms step_avg:88.82ms
step:1481/1670 train_time:131545ms step_avg:88.82ms
step:1482/1670 train_time:131634ms step_avg:88.82ms
step:1483/1670 train_time:131724ms step_avg:88.82ms
step:1484/1670 train_time:131814ms step_avg:88.82ms
step:1485/1670 train_time:131906ms step_avg:88.83ms
step:1486/1670 train_time:131995ms step_avg:88.83ms
step:1487/1670 train_time:132085ms step_avg:88.83ms
step:1488/1670 train_time:132176ms step_avg:88.83ms
step:1489/1670 train_time:132267ms step_avg:88.83ms
step:1490/1670 train_time:132356ms step_avg:88.83ms
step:1491/1670 train_time:132446ms step_avg:88.83ms
step:1492/1670 train_time:132535ms step_avg:88.83ms
step:1493/1670 train_time:132624ms step_avg:88.83ms
step:1494/1670 train_time:132713ms step_avg:88.83ms
step:1495/1670 train_time:132803ms step_avg:88.83ms
step:1496/1670 train_time:132894ms step_avg:88.83ms
step:1497/1670 train_time:132985ms step_avg:88.83ms
step:1498/1670 train_time:133074ms step_avg:88.83ms
step:1499/1670 train_time:133166ms step_avg:88.84ms
step:1500/1670 train_time:133255ms step_avg:88.84ms
step:1500/1670 val_loss:3.3102 train_time:133346ms step_avg:88.90ms
step:1501/1670 train_time:133365ms step_avg:88.85ms
step:1502/1670 train_time:133439ms step_avg:88.84ms
step:1503/1670 train_time:133533ms step_avg:88.84ms
step:1504/1670 train_time:133624ms step_avg:88.85ms
step:1505/1670 train_time:133713ms step_avg:88.85ms
step:1506/1670 train_time:133802ms step_avg:88.85ms
step:1507/1670 train_time:133891ms step_avg:88.85ms
step:1508/1670 train_time:133979ms step_avg:88.85ms
step:1509/1670 train_time:134067ms step_avg:88.84ms
step:1510/1670 train_time:134156ms step_avg:88.85ms
step:1511/1670 train_time:134245ms step_avg:88.85ms
step:1512/1670 train_time:134338ms step_avg:88.85ms
step:1513/1670 train_time:134430ms step_avg:88.85ms
step:1514/1670 train_time:134523ms step_avg:88.85ms
step:1515/1670 train_time:134613ms step_avg:88.85ms
step:1516/1670 train_time:134703ms step_avg:88.85ms
step:1517/1670 train_time:134792ms step_avg:88.85ms
step:1518/1670 train_time:134881ms step_avg:88.85ms
step:1519/1670 train_time:134969ms step_avg:88.85ms
step:1520/1670 train_time:135058ms step_avg:88.85ms
step:1521/1670 train_time:135146ms step_avg:88.85ms
step:1522/1670 train_time:135236ms step_avg:88.85ms
step:1523/1670 train_time:135327ms step_avg:88.86ms
step:1524/1670 train_time:135420ms step_avg:88.86ms
step:1525/1670 train_time:135510ms step_avg:88.86ms
step:1526/1670 train_time:135600ms step_avg:88.86ms
step:1527/1670 train_time:135690ms step_avg:88.86ms
step:1528/1670 train_time:135779ms step_avg:88.86ms
step:1529/1670 train_time:135868ms step_avg:88.86ms
step:1530/1670 train_time:135957ms step_avg:88.86ms
step:1531/1670 train_time:136046ms step_avg:88.86ms
step:1532/1670 train_time:136135ms step_avg:88.86ms
step:1533/1670 train_time:136226ms step_avg:88.86ms
step:1534/1670 train_time:136316ms step_avg:88.86ms
step:1535/1670 train_time:136407ms step_avg:88.86ms
step:1536/1670 train_time:136498ms step_avg:88.87ms
step:1537/1670 train_time:136589ms step_avg:88.87ms
step:1538/1670 train_time:136680ms step_avg:88.87ms
step:1539/1670 train_time:136769ms step_avg:88.87ms
step:1540/1670 train_time:136859ms step_avg:88.87ms
step:1541/1670 train_time:136949ms step_avg:88.87ms
step:1542/1670 train_time:137039ms step_avg:88.87ms
step:1543/1670 train_time:137128ms step_avg:88.87ms
step:1544/1670 train_time:137218ms step_avg:88.87ms
step:1545/1670 train_time:137308ms step_avg:88.87ms
step:1546/1670 train_time:137398ms step_avg:88.87ms
step:1547/1670 train_time:137488ms step_avg:88.87ms
step:1548/1670 train_time:137579ms step_avg:88.88ms
step:1549/1670 train_time:137668ms step_avg:88.88ms
step:1550/1670 train_time:137758ms step_avg:88.88ms
step:1551/1670 train_time:137847ms step_avg:88.88ms
step:1552/1670 train_time:137937ms step_avg:88.88ms
step:1553/1670 train_time:138027ms step_avg:88.88ms
step:1554/1670 train_time:138117ms step_avg:88.88ms
step:1555/1670 train_time:138207ms step_avg:88.88ms
step:1556/1670 train_time:138297ms step_avg:88.88ms
step:1557/1670 train_time:138387ms step_avg:88.88ms
step:1558/1670 train_time:138477ms step_avg:88.88ms
step:1559/1670 train_time:138567ms step_avg:88.88ms
step:1560/1670 train_time:138658ms step_avg:88.88ms
step:1561/1670 train_time:138747ms step_avg:88.88ms
step:1562/1670 train_time:138838ms step_avg:88.88ms
step:1563/1670 train_time:138927ms step_avg:88.88ms
step:1564/1670 train_time:139017ms step_avg:88.89ms
step:1565/1670 train_time:139105ms step_avg:88.89ms
step:1566/1670 train_time:139195ms step_avg:88.89ms
step:1567/1670 train_time:139285ms step_avg:88.89ms
step:1568/1670 train_time:139375ms step_avg:88.89ms
step:1569/1670 train_time:139466ms step_avg:88.89ms
step:1570/1670 train_time:139557ms step_avg:88.89ms
step:1571/1670 train_time:139647ms step_avg:88.89ms
step:1572/1670 train_time:139737ms step_avg:88.89ms
step:1573/1670 train_time:139826ms step_avg:88.89ms
step:1574/1670 train_time:139916ms step_avg:88.89ms
step:1575/1670 train_time:140006ms step_avg:88.89ms
step:1576/1670 train_time:140096ms step_avg:88.89ms
step:1577/1670 train_time:140186ms step_avg:88.89ms
step:1578/1670 train_time:140275ms step_avg:88.89ms
step:1579/1670 train_time:140365ms step_avg:88.89ms
step:1580/1670 train_time:140455ms step_avg:88.90ms
step:1581/1670 train_time:140545ms step_avg:88.90ms
step:1582/1670 train_time:140636ms step_avg:88.90ms
step:1583/1670 train_time:140726ms step_avg:88.90ms
step:1584/1670 train_time:140816ms step_avg:88.90ms
step:1585/1670 train_time:140905ms step_avg:88.90ms
step:1586/1670 train_time:140995ms step_avg:88.90ms
step:1587/1670 train_time:141085ms step_avg:88.90ms
step:1588/1670 train_time:141175ms step_avg:88.90ms
step:1589/1670 train_time:141264ms step_avg:88.90ms
step:1590/1670 train_time:141354ms step_avg:88.90ms
step:1591/1670 train_time:141444ms step_avg:88.90ms
step:1592/1670 train_time:141535ms step_avg:88.90ms
step:1593/1670 train_time:141625ms step_avg:88.90ms
step:1594/1670 train_time:141715ms step_avg:88.91ms
step:1595/1670 train_time:141805ms step_avg:88.91ms
step:1596/1670 train_time:141894ms step_avg:88.91ms
step:1597/1670 train_time:141984ms step_avg:88.91ms
step:1598/1670 train_time:142074ms step_avg:88.91ms
step:1599/1670 train_time:142165ms step_avg:88.91ms
step:1600/1670 train_time:142255ms step_avg:88.91ms
step:1601/1670 train_time:142345ms step_avg:88.91ms
step:1602/1670 train_time:142434ms step_avg:88.91ms
step:1603/1670 train_time:142525ms step_avg:88.91ms
step:1604/1670 train_time:142616ms step_avg:88.91ms
step:1605/1670 train_time:142705ms step_avg:88.91ms
step:1606/1670 train_time:142795ms step_avg:88.91ms
step:1607/1670 train_time:142885ms step_avg:88.91ms
step:1608/1670 train_time:142975ms step_avg:88.91ms
step:1609/1670 train_time:143064ms step_avg:88.91ms
step:1610/1670 train_time:143154ms step_avg:88.92ms
step:1611/1670 train_time:143244ms step_avg:88.92ms
step:1612/1670 train_time:143333ms step_avg:88.92ms
step:1613/1670 train_time:143424ms step_avg:88.92ms
step:1614/1670 train_time:143514ms step_avg:88.92ms
step:1615/1670 train_time:143604ms step_avg:88.92ms
step:1616/1670 train_time:143694ms step_avg:88.92ms
step:1617/1670 train_time:143783ms step_avg:88.92ms
step:1618/1670 train_time:143873ms step_avg:88.92ms
step:1619/1670 train_time:143963ms step_avg:88.92ms
step:1620/1670 train_time:144053ms step_avg:88.92ms
step:1621/1670 train_time:144144ms step_avg:88.92ms
step:1622/1670 train_time:144234ms step_avg:88.92ms
step:1623/1670 train_time:144324ms step_avg:88.92ms
step:1624/1670 train_time:144415ms step_avg:88.93ms
step:1625/1670 train_time:144505ms step_avg:88.93ms
step:1625/1670 val_loss:3.2869 train_time:144596ms step_avg:88.98ms
step:1626/1670 train_time:144615ms step_avg:88.94ms
step:1627/1670 train_time:144690ms step_avg:88.93ms
step:1628/1670 train_time:144787ms step_avg:88.94ms
step:1629/1670 train_time:144878ms step_avg:88.94ms
step:1630/1670 train_time:144967ms step_avg:88.94ms
step:1631/1670 train_time:145055ms step_avg:88.94ms
step:1632/1670 train_time:145145ms step_avg:88.94ms
step:1633/1670 train_time:145233ms step_avg:88.94ms
step:1634/1670 train_time:145321ms step_avg:88.94ms
step:1635/1670 train_time:145410ms step_avg:88.94ms
step:1636/1670 train_time:145499ms step_avg:88.94ms
step:1637/1670 train_time:145592ms step_avg:88.94ms
step:1638/1670 train_time:145685ms step_avg:88.94ms
step:1639/1670 train_time:145776ms step_avg:88.94ms
step:1640/1670 train_time:145867ms step_avg:88.94ms
step:1641/1670 train_time:145957ms step_avg:88.94ms
step:1642/1670 train_time:146046ms step_avg:88.94ms
step:1643/1670 train_time:146135ms step_avg:88.94ms
step:1644/1670 train_time:146224ms step_avg:88.94ms
step:1645/1670 train_time:146313ms step_avg:88.94ms
step:1646/1670 train_time:146402ms step_avg:88.94ms
step:1647/1670 train_time:146492ms step_avg:88.94ms
step:1648/1670 train_time:146583ms step_avg:88.95ms
step:1649/1670 train_time:146674ms step_avg:88.95ms
step:1650/1670 train_time:146765ms step_avg:88.95ms
step:1651/1670 train_time:146855ms step_avg:88.95ms
step:1652/1670 train_time:146947ms step_avg:88.95ms
step:1653/1670 train_time:147036ms step_avg:88.95ms
step:1654/1670 train_time:147125ms step_avg:88.95ms
step:1655/1670 train_time:147214ms step_avg:88.95ms
step:1656/1670 train_time:147303ms step_avg:88.95ms
step:1657/1670 train_time:147392ms step_avg:88.95ms
step:1658/1670 train_time:147481ms step_avg:88.95ms
step:1659/1670 train_time:147571ms step_avg:88.95ms
step:1660/1670 train_time:147661ms step_avg:88.95ms
step:1661/1670 train_time:147751ms step_avg:88.95ms
step:1662/1670 train_time:147842ms step_avg:88.95ms
step:1663/1670 train_time:147932ms step_avg:88.95ms
step:1664/1670 train_time:148022ms step_avg:88.96ms
step:1665/1670 train_time:148112ms step_avg:88.96ms
step:1666/1670 train_time:148201ms step_avg:88.96ms
step:1667/1670 train_time:148290ms step_avg:88.96ms
step:1668/1670 train_time:148379ms step_avg:88.96ms
step:1669/1670 train_time:148469ms step_avg:88.96ms
step:1670/1670 train_time:148559ms step_avg:88.96ms
step:1670/1670 val_loss:3.2776 train_time:148653ms step_avg:89.01ms
peak memory allocated: 30760 MiB reserved: 45914 MiB

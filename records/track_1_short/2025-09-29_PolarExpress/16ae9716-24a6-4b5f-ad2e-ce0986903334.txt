import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:35:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:142ms step_avg:141.53ms
step:2/1670 train_time:163ms step_avg:81.29ms
step:3/1670 train_time:226ms step_avg:75.35ms
step:4/1670 train_time:311ms step_avg:77.87ms
step:5/1670 train_time:398ms step_avg:79.55ms
step:6/1670 train_time:484ms step_avg:80.73ms
step:7/1670 train_time:571ms step_avg:81.52ms
step:8/1670 train_time:658ms step_avg:82.20ms
step:9/1670 train_time:744ms step_avg:82.70ms
step:10/1670 train_time:831ms step_avg:83.09ms
step:11/1670 train_time:918ms step_avg:83.41ms
step:12/1670 train_time:1008ms step_avg:83.98ms
step:13/1670 train_time:1101ms step_avg:84.71ms
step:14/1670 train_time:1191ms step_avg:85.09ms
step:15/1670 train_time:1280ms step_avg:85.36ms
step:16/1670 train_time:1368ms step_avg:85.53ms
step:17/1670 train_time:1456ms step_avg:85.64ms
step:18/1670 train_time:1543ms step_avg:85.73ms
step:19/1670 train_time:1630ms step_avg:85.81ms
step:20/1670 train_time:1717ms step_avg:85.87ms
step:21/1670 train_time:1805ms step_avg:85.95ms
step:22/1670 train_time:1892ms step_avg:86.00ms
step:23/1670 train_time:1980ms step_avg:86.07ms
step:24/1670 train_time:2070ms step_avg:86.23ms
step:25/1670 train_time:2159ms step_avg:86.37ms
step:26/1670 train_time:2248ms step_avg:86.46ms
step:27/1670 train_time:2337ms step_avg:86.54ms
step:28/1670 train_time:2424ms step_avg:86.58ms
step:29/1670 train_time:2512ms step_avg:86.63ms
step:30/1670 train_time:2600ms step_avg:86.65ms
step:31/1670 train_time:2687ms step_avg:86.68ms
step:32/1670 train_time:2774ms step_avg:86.69ms
step:33/1670 train_time:2861ms step_avg:86.70ms
step:34/1670 train_time:2949ms step_avg:86.72ms
step:35/1670 train_time:3037ms step_avg:86.78ms
step:36/1670 train_time:3126ms step_avg:86.84ms
step:37/1670 train_time:3215ms step_avg:86.90ms
step:38/1670 train_time:3304ms step_avg:86.94ms
step:39/1670 train_time:3391ms step_avg:86.95ms
step:40/1670 train_time:3479ms step_avg:86.97ms
step:41/1670 train_time:3567ms step_avg:86.99ms
step:42/1670 train_time:3655ms step_avg:87.01ms
step:43/1670 train_time:3742ms step_avg:87.03ms
step:44/1670 train_time:3831ms step_avg:87.06ms
step:45/1670 train_time:3918ms step_avg:87.07ms
step:46/1670 train_time:4006ms step_avg:87.09ms
step:47/1670 train_time:4095ms step_avg:87.13ms
step:48/1670 train_time:4184ms step_avg:87.16ms
step:49/1670 train_time:4273ms step_avg:87.20ms
step:50/1670 train_time:4360ms step_avg:87.21ms
step:51/1670 train_time:4448ms step_avg:87.22ms
step:52/1670 train_time:4536ms step_avg:87.23ms
step:53/1670 train_time:4623ms step_avg:87.23ms
step:54/1670 train_time:4711ms step_avg:87.24ms
step:55/1670 train_time:4798ms step_avg:87.24ms
step:56/1670 train_time:4886ms step_avg:87.25ms
step:57/1670 train_time:4974ms step_avg:87.26ms
step:58/1670 train_time:5062ms step_avg:87.27ms
step:59/1670 train_time:5150ms step_avg:87.29ms
step:60/1670 train_time:5239ms step_avg:87.32ms
step:61/1670 train_time:5327ms step_avg:87.33ms
step:62/1670 train_time:5415ms step_avg:87.34ms
step:63/1670 train_time:5503ms step_avg:87.35ms
step:64/1670 train_time:5591ms step_avg:87.36ms
step:65/1670 train_time:5679ms step_avg:87.37ms
step:66/1670 train_time:5766ms step_avg:87.37ms
step:67/1670 train_time:5854ms step_avg:87.37ms
step:68/1670 train_time:5941ms step_avg:87.36ms
step:69/1670 train_time:6029ms step_avg:87.37ms
step:70/1670 train_time:6117ms step_avg:87.39ms
step:71/1670 train_time:6205ms step_avg:87.40ms
step:72/1670 train_time:6295ms step_avg:87.43ms
step:73/1670 train_time:6382ms step_avg:87.42ms
step:74/1670 train_time:6470ms step_avg:87.44ms
step:75/1670 train_time:6558ms step_avg:87.44ms
step:76/1670 train_time:6646ms step_avg:87.45ms
step:77/1670 train_time:6734ms step_avg:87.45ms
step:78/1670 train_time:6821ms step_avg:87.45ms
step:79/1670 train_time:6909ms step_avg:87.46ms
step:80/1670 train_time:6997ms step_avg:87.47ms
step:81/1670 train_time:7084ms step_avg:87.46ms
step:82/1670 train_time:7173ms step_avg:87.47ms
step:83/1670 train_time:7261ms step_avg:87.48ms
step:84/1670 train_time:7348ms step_avg:87.48ms
step:85/1670 train_time:7437ms step_avg:87.49ms
step:86/1670 train_time:7525ms step_avg:87.50ms
step:87/1670 train_time:7612ms step_avg:87.50ms
step:88/1670 train_time:7700ms step_avg:87.50ms
step:89/1670 train_time:7787ms step_avg:87.49ms
step:90/1670 train_time:7875ms step_avg:87.50ms
step:91/1670 train_time:7962ms step_avg:87.50ms
step:92/1670 train_time:8051ms step_avg:87.51ms
step:93/1670 train_time:8139ms step_avg:87.51ms
step:94/1670 train_time:8227ms step_avg:87.52ms
step:95/1670 train_time:8315ms step_avg:87.53ms
step:96/1670 train_time:8403ms step_avg:87.53ms
step:97/1670 train_time:8491ms step_avg:87.53ms
step:98/1670 train_time:8579ms step_avg:87.54ms
step:99/1670 train_time:8667ms step_avg:87.54ms
step:100/1670 train_time:8754ms step_avg:87.54ms
step:101/1670 train_time:8842ms step_avg:87.54ms
step:102/1670 train_time:8930ms step_avg:87.55ms
step:103/1670 train_time:9018ms step_avg:87.56ms
step:104/1670 train_time:9106ms step_avg:87.56ms
step:105/1670 train_time:9195ms step_avg:87.57ms
step:106/1670 train_time:9282ms step_avg:87.56ms
step:107/1670 train_time:9370ms step_avg:87.57ms
step:108/1670 train_time:9458ms step_avg:87.57ms
step:109/1670 train_time:9545ms step_avg:87.57ms
step:110/1670 train_time:9633ms step_avg:87.57ms
step:111/1670 train_time:9720ms step_avg:87.57ms
step:112/1670 train_time:9808ms step_avg:87.57ms
step:113/1670 train_time:9896ms step_avg:87.57ms
step:114/1670 train_time:9983ms step_avg:87.57ms
step:115/1670 train_time:10071ms step_avg:87.57ms
step:116/1670 train_time:10159ms step_avg:87.58ms
step:117/1670 train_time:10246ms step_avg:87.58ms
step:118/1670 train_time:10335ms step_avg:87.59ms
step:119/1670 train_time:10423ms step_avg:87.58ms
step:120/1670 train_time:10510ms step_avg:87.59ms
step:121/1670 train_time:10598ms step_avg:87.59ms
step:122/1670 train_time:10686ms step_avg:87.59ms
step:123/1670 train_time:10773ms step_avg:87.58ms
step:124/1670 train_time:10860ms step_avg:87.58ms
step:125/1670 train_time:10947ms step_avg:87.58ms
step:125/1670 val_loss:4.3607 train_time:11037ms step_avg:88.30ms
step:126/1670 train_time:11057ms step_avg:87.75ms
step:127/1670 train_time:11128ms step_avg:87.62ms
step:128/1670 train_time:11224ms step_avg:87.69ms
step:129/1670 train_time:11315ms step_avg:87.71ms
step:130/1670 train_time:11402ms step_avg:87.71ms
step:131/1670 train_time:11489ms step_avg:87.70ms
step:132/1670 train_time:11576ms step_avg:87.70ms
step:133/1670 train_time:11662ms step_avg:87.69ms
step:134/1670 train_time:11749ms step_avg:87.68ms
step:135/1670 train_time:11835ms step_avg:87.67ms
step:136/1670 train_time:11922ms step_avg:87.66ms
step:137/1670 train_time:12009ms step_avg:87.66ms
step:138/1670 train_time:12099ms step_avg:87.67ms
step:139/1670 train_time:12189ms step_avg:87.69ms
step:140/1670 train_time:12279ms step_avg:87.71ms
step:141/1670 train_time:12367ms step_avg:87.71ms
step:142/1670 train_time:12454ms step_avg:87.71ms
step:143/1670 train_time:12541ms step_avg:87.70ms
step:144/1670 train_time:12629ms step_avg:87.70ms
step:145/1670 train_time:12716ms step_avg:87.70ms
step:146/1670 train_time:12802ms step_avg:87.69ms
step:147/1670 train_time:12890ms step_avg:87.68ms
step:148/1670 train_time:12977ms step_avg:87.68ms
step:149/1670 train_time:13065ms step_avg:87.69ms
step:150/1670 train_time:13156ms step_avg:87.70ms
step:151/1670 train_time:13245ms step_avg:87.72ms
step:152/1670 train_time:13333ms step_avg:87.72ms
step:153/1670 train_time:13420ms step_avg:87.72ms
step:154/1670 train_time:13509ms step_avg:87.72ms
step:155/1670 train_time:13596ms step_avg:87.71ms
step:156/1670 train_time:13683ms step_avg:87.71ms
step:157/1670 train_time:13770ms step_avg:87.71ms
step:158/1670 train_time:13857ms step_avg:87.70ms
step:159/1670 train_time:13944ms step_avg:87.70ms
step:160/1670 train_time:14032ms step_avg:87.70ms
step:161/1670 train_time:14120ms step_avg:87.70ms
step:162/1670 train_time:14209ms step_avg:87.71ms
step:163/1670 train_time:14297ms step_avg:87.71ms
step:164/1670 train_time:14385ms step_avg:87.71ms
step:165/1670 train_time:14473ms step_avg:87.72ms
step:166/1670 train_time:14560ms step_avg:87.71ms
step:167/1670 train_time:14647ms step_avg:87.71ms
step:168/1670 train_time:14735ms step_avg:87.71ms
step:169/1670 train_time:14823ms step_avg:87.71ms
step:170/1670 train_time:14911ms step_avg:87.71ms
step:171/1670 train_time:14998ms step_avg:87.71ms
step:172/1670 train_time:15086ms step_avg:87.71ms
step:173/1670 train_time:15174ms step_avg:87.71ms
step:174/1670 train_time:15262ms step_avg:87.71ms
step:175/1670 train_time:15351ms step_avg:87.72ms
step:176/1670 train_time:15438ms step_avg:87.72ms
step:177/1670 train_time:15527ms step_avg:87.72ms
step:178/1670 train_time:15614ms step_avg:87.72ms
step:179/1670 train_time:15702ms step_avg:87.72ms
step:180/1670 train_time:15789ms step_avg:87.72ms
step:181/1670 train_time:15876ms step_avg:87.72ms
step:182/1670 train_time:15964ms step_avg:87.71ms
step:183/1670 train_time:16051ms step_avg:87.71ms
step:184/1670 train_time:16138ms step_avg:87.71ms
step:185/1670 train_time:16226ms step_avg:87.71ms
step:186/1670 train_time:16314ms step_avg:87.71ms
step:187/1670 train_time:16401ms step_avg:87.71ms
step:188/1670 train_time:16489ms step_avg:87.71ms
step:189/1670 train_time:16577ms step_avg:87.71ms
step:190/1670 train_time:16665ms step_avg:87.71ms
step:191/1670 train_time:16752ms step_avg:87.71ms
step:192/1670 train_time:16839ms step_avg:87.70ms
step:193/1670 train_time:16927ms step_avg:87.70ms
step:194/1670 train_time:17014ms step_avg:87.70ms
step:195/1670 train_time:17102ms step_avg:87.70ms
step:196/1670 train_time:17191ms step_avg:87.71ms
step:197/1670 train_time:17278ms step_avg:87.71ms
step:198/1670 train_time:17366ms step_avg:87.71ms
step:199/1670 train_time:17454ms step_avg:87.71ms
step:200/1670 train_time:17541ms step_avg:87.71ms
step:201/1670 train_time:17628ms step_avg:87.70ms
step:202/1670 train_time:17716ms step_avg:87.70ms
step:203/1670 train_time:17803ms step_avg:87.70ms
step:204/1670 train_time:17891ms step_avg:87.70ms
step:205/1670 train_time:17977ms step_avg:87.70ms
step:206/1670 train_time:18065ms step_avg:87.70ms
step:207/1670 train_time:18154ms step_avg:87.70ms
step:208/1670 train_time:18242ms step_avg:87.70ms
step:209/1670 train_time:18330ms step_avg:87.70ms
step:210/1670 train_time:18417ms step_avg:87.70ms
step:211/1670 train_time:18505ms step_avg:87.70ms
step:212/1670 train_time:18592ms step_avg:87.70ms
step:213/1670 train_time:18680ms step_avg:87.70ms
step:214/1670 train_time:18768ms step_avg:87.70ms
step:215/1670 train_time:18856ms step_avg:87.70ms
step:216/1670 train_time:18943ms step_avg:87.70ms
step:217/1670 train_time:19030ms step_avg:87.69ms
step:218/1670 train_time:19117ms step_avg:87.69ms
step:219/1670 train_time:19205ms step_avg:87.69ms
step:220/1670 train_time:19292ms step_avg:87.69ms
step:221/1670 train_time:19380ms step_avg:87.69ms
step:222/1670 train_time:19469ms step_avg:87.70ms
step:223/1670 train_time:19556ms step_avg:87.70ms
step:224/1670 train_time:19643ms step_avg:87.69ms
step:225/1670 train_time:19731ms step_avg:87.69ms
step:226/1670 train_time:19818ms step_avg:87.69ms
step:227/1670 train_time:19905ms step_avg:87.69ms
step:228/1670 train_time:19993ms step_avg:87.69ms
step:229/1670 train_time:20080ms step_avg:87.68ms
step:230/1670 train_time:20167ms step_avg:87.68ms
step:231/1670 train_time:20255ms step_avg:87.69ms
step:232/1670 train_time:20343ms step_avg:87.69ms
step:233/1670 train_time:20431ms step_avg:87.69ms
step:234/1670 train_time:20518ms step_avg:87.68ms
step:235/1670 train_time:20605ms step_avg:87.68ms
step:236/1670 train_time:20693ms step_avg:87.68ms
step:237/1670 train_time:20780ms step_avg:87.68ms
step:238/1670 train_time:20868ms step_avg:87.68ms
step:239/1670 train_time:20956ms step_avg:87.68ms
step:240/1670 train_time:21043ms step_avg:87.68ms
step:241/1670 train_time:21130ms step_avg:87.68ms
step:242/1670 train_time:21217ms step_avg:87.67ms
step:243/1670 train_time:21305ms step_avg:87.67ms
step:244/1670 train_time:21393ms step_avg:87.67ms
step:245/1670 train_time:21480ms step_avg:87.67ms
step:246/1670 train_time:21567ms step_avg:87.67ms
step:247/1670 train_time:21656ms step_avg:87.67ms
step:248/1670 train_time:21744ms step_avg:87.68ms
step:249/1670 train_time:21833ms step_avg:87.68ms
step:250/1670 train_time:21919ms step_avg:87.68ms
step:250/1670 val_loss:3.9859 train_time:22009ms step_avg:88.04ms
step:251/1670 train_time:22028ms step_avg:87.76ms
step:252/1670 train_time:22098ms step_avg:87.69ms
step:253/1670 train_time:22192ms step_avg:87.72ms
step:254/1670 train_time:22281ms step_avg:87.72ms
step:255/1670 train_time:22368ms step_avg:87.72ms
step:256/1670 train_time:22455ms step_avg:87.72ms
step:257/1670 train_time:22541ms step_avg:87.71ms
step:258/1670 train_time:22629ms step_avg:87.71ms
step:259/1670 train_time:22715ms step_avg:87.70ms
step:260/1670 train_time:22801ms step_avg:87.70ms
step:261/1670 train_time:22887ms step_avg:87.69ms
step:262/1670 train_time:22976ms step_avg:87.69ms
step:263/1670 train_time:23066ms step_avg:87.70ms
step:264/1670 train_time:23157ms step_avg:87.72ms
step:265/1670 train_time:23245ms step_avg:87.72ms
step:266/1670 train_time:23333ms step_avg:87.72ms
step:267/1670 train_time:23420ms step_avg:87.72ms
step:268/1670 train_time:23507ms step_avg:87.71ms
step:269/1670 train_time:23594ms step_avg:87.71ms
step:270/1670 train_time:23680ms step_avg:87.71ms
step:271/1670 train_time:23768ms step_avg:87.70ms
step:272/1670 train_time:23854ms step_avg:87.70ms
step:273/1670 train_time:23942ms step_avg:87.70ms
step:274/1670 train_time:24031ms step_avg:87.70ms
step:275/1670 train_time:24120ms step_avg:87.71ms
step:276/1670 train_time:24210ms step_avg:87.72ms
step:277/1670 train_time:24298ms step_avg:87.72ms
step:278/1670 train_time:24385ms step_avg:87.72ms
step:279/1670 train_time:24472ms step_avg:87.71ms
step:280/1670 train_time:24560ms step_avg:87.71ms
step:281/1670 train_time:24647ms step_avg:87.71ms
step:282/1670 train_time:24733ms step_avg:87.71ms
step:283/1670 train_time:24820ms step_avg:87.70ms
step:284/1670 train_time:24908ms step_avg:87.70ms
step:285/1670 train_time:24995ms step_avg:87.70ms
step:286/1670 train_time:25082ms step_avg:87.70ms
step:287/1670 train_time:25170ms step_avg:87.70ms
step:288/1670 train_time:25259ms step_avg:87.70ms
step:289/1670 train_time:25347ms step_avg:87.71ms
step:290/1670 train_time:25435ms step_avg:87.71ms
step:291/1670 train_time:25522ms step_avg:87.70ms
step:292/1670 train_time:25610ms step_avg:87.70ms
step:293/1670 train_time:25696ms step_avg:87.70ms
step:294/1670 train_time:25784ms step_avg:87.70ms
step:295/1670 train_time:25871ms step_avg:87.70ms
step:296/1670 train_time:25958ms step_avg:87.70ms
step:297/1670 train_time:26046ms step_avg:87.70ms
step:298/1670 train_time:26134ms step_avg:87.70ms
step:299/1670 train_time:26222ms step_avg:87.70ms
step:300/1670 train_time:26311ms step_avg:87.70ms
step:301/1670 train_time:26399ms step_avg:87.71ms
step:302/1670 train_time:26488ms step_avg:87.71ms
step:303/1670 train_time:26575ms step_avg:87.71ms
step:304/1670 train_time:26663ms step_avg:87.71ms
step:305/1670 train_time:26750ms step_avg:87.70ms
step:306/1670 train_time:26837ms step_avg:87.70ms
step:307/1670 train_time:26923ms step_avg:87.70ms
step:308/1670 train_time:27012ms step_avg:87.70ms
step:309/1670 train_time:27100ms step_avg:87.70ms
step:310/1670 train_time:27189ms step_avg:87.71ms
step:311/1670 train_time:27276ms step_avg:87.71ms
step:312/1670 train_time:27364ms step_avg:87.70ms
step:313/1670 train_time:27451ms step_avg:87.70ms
step:314/1670 train_time:27539ms step_avg:87.70ms
step:315/1670 train_time:27626ms step_avg:87.70ms
step:316/1670 train_time:27713ms step_avg:87.70ms
step:317/1670 train_time:27800ms step_avg:87.70ms
step:318/1670 train_time:27887ms step_avg:87.69ms
step:319/1670 train_time:27974ms step_avg:87.69ms
step:320/1670 train_time:28061ms step_avg:87.69ms
step:321/1670 train_time:28151ms step_avg:87.70ms
step:322/1670 train_time:28238ms step_avg:87.70ms
step:323/1670 train_time:28327ms step_avg:87.70ms
step:324/1670 train_time:28414ms step_avg:87.70ms
step:325/1670 train_time:28501ms step_avg:87.69ms
step:326/1670 train_time:28590ms step_avg:87.70ms
step:327/1670 train_time:28677ms step_avg:87.70ms
step:328/1670 train_time:28764ms step_avg:87.69ms
step:329/1670 train_time:28851ms step_avg:87.69ms
step:330/1670 train_time:28938ms step_avg:87.69ms
step:331/1670 train_time:29026ms step_avg:87.69ms
step:332/1670 train_time:29113ms step_avg:87.69ms
step:333/1670 train_time:29202ms step_avg:87.69ms
step:334/1670 train_time:29290ms step_avg:87.70ms
step:335/1670 train_time:29378ms step_avg:87.70ms
step:336/1670 train_time:29465ms step_avg:87.69ms
step:337/1670 train_time:29552ms step_avg:87.69ms
step:338/1670 train_time:29639ms step_avg:87.69ms
step:339/1670 train_time:29727ms step_avg:87.69ms
step:340/1670 train_time:29814ms step_avg:87.69ms
step:341/1670 train_time:29901ms step_avg:87.69ms
step:342/1670 train_time:29989ms step_avg:87.69ms
step:343/1670 train_time:30077ms step_avg:87.69ms
step:344/1670 train_time:30165ms step_avg:87.69ms
step:345/1670 train_time:30253ms step_avg:87.69ms
step:346/1670 train_time:30340ms step_avg:87.69ms
step:347/1670 train_time:30428ms step_avg:87.69ms
step:348/1670 train_time:30515ms step_avg:87.69ms
step:349/1670 train_time:30602ms step_avg:87.68ms
step:350/1670 train_time:30690ms step_avg:87.68ms
step:351/1670 train_time:30777ms step_avg:87.68ms
step:352/1670 train_time:30864ms step_avg:87.68ms
step:353/1670 train_time:30951ms step_avg:87.68ms
step:354/1670 train_time:31038ms step_avg:87.68ms
step:355/1670 train_time:31126ms step_avg:87.68ms
step:356/1670 train_time:31214ms step_avg:87.68ms
step:357/1670 train_time:31302ms step_avg:87.68ms
step:358/1670 train_time:31391ms step_avg:87.68ms
step:359/1670 train_time:31479ms step_avg:87.68ms
step:360/1670 train_time:31566ms step_avg:87.68ms
step:361/1670 train_time:31653ms step_avg:87.68ms
step:362/1670 train_time:31740ms step_avg:87.68ms
step:363/1670 train_time:31828ms step_avg:87.68ms
step:364/1670 train_time:31915ms step_avg:87.68ms
step:365/1670 train_time:32002ms step_avg:87.68ms
step:366/1670 train_time:32090ms step_avg:87.68ms
step:367/1670 train_time:32177ms step_avg:87.68ms
step:368/1670 train_time:32265ms step_avg:87.68ms
step:369/1670 train_time:32353ms step_avg:87.68ms
step:370/1670 train_time:32440ms step_avg:87.68ms
step:371/1670 train_time:32528ms step_avg:87.68ms
step:372/1670 train_time:32615ms step_avg:87.67ms
step:373/1670 train_time:32702ms step_avg:87.67ms
step:374/1670 train_time:32791ms step_avg:87.68ms
step:375/1670 train_time:32877ms step_avg:87.67ms
step:375/1670 val_loss:3.8308 train_time:32966ms step_avg:87.91ms
step:376/1670 train_time:32986ms step_avg:87.73ms
step:377/1670 train_time:33057ms step_avg:87.68ms
step:378/1670 train_time:33147ms step_avg:87.69ms
step:379/1670 train_time:33236ms step_avg:87.69ms
step:380/1670 train_time:33323ms step_avg:87.69ms
step:381/1670 train_time:33410ms step_avg:87.69ms
step:382/1670 train_time:33496ms step_avg:87.69ms
step:383/1670 train_time:33582ms step_avg:87.68ms
step:384/1670 train_time:33669ms step_avg:87.68ms
step:385/1670 train_time:33757ms step_avg:87.68ms
step:386/1670 train_time:33844ms step_avg:87.68ms
step:387/1670 train_time:33933ms step_avg:87.68ms
step:388/1670 train_time:34023ms step_avg:87.69ms
step:389/1670 train_time:34112ms step_avg:87.69ms
step:390/1670 train_time:34200ms step_avg:87.69ms
step:391/1670 train_time:34287ms step_avg:87.69ms
step:392/1670 train_time:34375ms step_avg:87.69ms
step:393/1670 train_time:34462ms step_avg:87.69ms
step:394/1670 train_time:34548ms step_avg:87.69ms
step:395/1670 train_time:34636ms step_avg:87.69ms
step:396/1670 train_time:34722ms step_avg:87.68ms
step:397/1670 train_time:34809ms step_avg:87.68ms
step:398/1670 train_time:34896ms step_avg:87.68ms
step:399/1670 train_time:34985ms step_avg:87.68ms
step:400/1670 train_time:35075ms step_avg:87.69ms
step:401/1670 train_time:35162ms step_avg:87.69ms
step:402/1670 train_time:35250ms step_avg:87.69ms
step:403/1670 train_time:35338ms step_avg:87.69ms
step:404/1670 train_time:35425ms step_avg:87.68ms
step:405/1670 train_time:35512ms step_avg:87.68ms
step:406/1670 train_time:35598ms step_avg:87.68ms
step:407/1670 train_time:35685ms step_avg:87.68ms
step:408/1670 train_time:35772ms step_avg:87.68ms
step:409/1670 train_time:35859ms step_avg:87.68ms
step:410/1670 train_time:35947ms step_avg:87.68ms
step:411/1670 train_time:36036ms step_avg:87.68ms
step:412/1670 train_time:36125ms step_avg:87.68ms
step:413/1670 train_time:36212ms step_avg:87.68ms
step:414/1670 train_time:36300ms step_avg:87.68ms
step:415/1670 train_time:36388ms step_avg:87.68ms
step:416/1670 train_time:36476ms step_avg:87.68ms
step:417/1670 train_time:36563ms step_avg:87.68ms
step:418/1670 train_time:36650ms step_avg:87.68ms
step:419/1670 train_time:36737ms step_avg:87.68ms
step:420/1670 train_time:36826ms step_avg:87.68ms
step:421/1670 train_time:36914ms step_avg:87.68ms
step:422/1670 train_time:37001ms step_avg:87.68ms
step:423/1670 train_time:37090ms step_avg:87.68ms
step:424/1670 train_time:37178ms step_avg:87.68ms
step:425/1670 train_time:37266ms step_avg:87.68ms
step:426/1670 train_time:37353ms step_avg:87.68ms
step:427/1670 train_time:37440ms step_avg:87.68ms
step:428/1670 train_time:37528ms step_avg:87.68ms
step:429/1670 train_time:37615ms step_avg:87.68ms
step:430/1670 train_time:37702ms step_avg:87.68ms
step:431/1670 train_time:37790ms step_avg:87.68ms
step:432/1670 train_time:37878ms step_avg:87.68ms
step:433/1670 train_time:37965ms step_avg:87.68ms
step:434/1670 train_time:38053ms step_avg:87.68ms
step:435/1670 train_time:38140ms step_avg:87.68ms
step:436/1670 train_time:38228ms step_avg:87.68ms
step:437/1670 train_time:38316ms step_avg:87.68ms
step:438/1670 train_time:38404ms step_avg:87.68ms
step:439/1670 train_time:38492ms step_avg:87.68ms
step:440/1670 train_time:38579ms step_avg:87.68ms
step:441/1670 train_time:38667ms step_avg:87.68ms
step:442/1670 train_time:38756ms step_avg:87.68ms
step:443/1670 train_time:38843ms step_avg:87.68ms
step:444/1670 train_time:38931ms step_avg:87.68ms
step:445/1670 train_time:39018ms step_avg:87.68ms
step:446/1670 train_time:39106ms step_avg:87.68ms
step:447/1670 train_time:39193ms step_avg:87.68ms
step:448/1670 train_time:39281ms step_avg:87.68ms
step:449/1670 train_time:39369ms step_avg:87.68ms
step:450/1670 train_time:39458ms step_avg:87.68ms
step:451/1670 train_time:39545ms step_avg:87.68ms
step:452/1670 train_time:39633ms step_avg:87.68ms
step:453/1670 train_time:39720ms step_avg:87.68ms
step:454/1670 train_time:39807ms step_avg:87.68ms
step:455/1670 train_time:39895ms step_avg:87.68ms
step:456/1670 train_time:39982ms step_avg:87.68ms
step:457/1670 train_time:40071ms step_avg:87.68ms
step:458/1670 train_time:40158ms step_avg:87.68ms
step:459/1670 train_time:40247ms step_avg:87.68ms
step:460/1670 train_time:40335ms step_avg:87.68ms
step:461/1670 train_time:40422ms step_avg:87.68ms
step:462/1670 train_time:40510ms step_avg:87.68ms
step:463/1670 train_time:40597ms step_avg:87.68ms
step:464/1670 train_time:40685ms step_avg:87.68ms
step:465/1670 train_time:40772ms step_avg:87.68ms
step:466/1670 train_time:40860ms step_avg:87.68ms
step:467/1670 train_time:40947ms step_avg:87.68ms
step:468/1670 train_time:41035ms step_avg:87.68ms
step:469/1670 train_time:41122ms step_avg:87.68ms
step:470/1670 train_time:41210ms step_avg:87.68ms
step:471/1670 train_time:41298ms step_avg:87.68ms
step:472/1670 train_time:41385ms step_avg:87.68ms
step:473/1670 train_time:41474ms step_avg:87.68ms
step:474/1670 train_time:41561ms step_avg:87.68ms
step:475/1670 train_time:41649ms step_avg:87.68ms
step:476/1670 train_time:41738ms step_avg:87.68ms
step:477/1670 train_time:41825ms step_avg:87.68ms
step:478/1670 train_time:41912ms step_avg:87.68ms
step:479/1670 train_time:42000ms step_avg:87.68ms
step:480/1670 train_time:42087ms step_avg:87.68ms
step:481/1670 train_time:42175ms step_avg:87.68ms
step:482/1670 train_time:42263ms step_avg:87.68ms
step:483/1670 train_time:42351ms step_avg:87.68ms
step:484/1670 train_time:42438ms step_avg:87.68ms
step:485/1670 train_time:42526ms step_avg:87.68ms
step:486/1670 train_time:42614ms step_avg:87.68ms
step:487/1670 train_time:42701ms step_avg:87.68ms
step:488/1670 train_time:42789ms step_avg:87.68ms
step:489/1670 train_time:42877ms step_avg:87.68ms
step:490/1670 train_time:42964ms step_avg:87.68ms
step:491/1670 train_time:43051ms step_avg:87.68ms
step:492/1670 train_time:43139ms step_avg:87.68ms
step:493/1670 train_time:43227ms step_avg:87.68ms
step:494/1670 train_time:43315ms step_avg:87.68ms
step:495/1670 train_time:43403ms step_avg:87.68ms
step:496/1670 train_time:43490ms step_avg:87.68ms
step:497/1670 train_time:43578ms step_avg:87.68ms
step:498/1670 train_time:43665ms step_avg:87.68ms
step:499/1670 train_time:43753ms step_avg:87.68ms
step:500/1670 train_time:43841ms step_avg:87.68ms
step:500/1670 val_loss:3.7255 train_time:43930ms step_avg:87.86ms
step:501/1670 train_time:43949ms step_avg:87.72ms
step:502/1670 train_time:44021ms step_avg:87.69ms
step:503/1670 train_time:44112ms step_avg:87.70ms
step:504/1670 train_time:44200ms step_avg:87.70ms
step:505/1670 train_time:44287ms step_avg:87.70ms
step:506/1670 train_time:44374ms step_avg:87.70ms
step:507/1670 train_time:44461ms step_avg:87.69ms
step:508/1670 train_time:44548ms step_avg:87.69ms
step:509/1670 train_time:44634ms step_avg:87.69ms
step:510/1670 train_time:44722ms step_avg:87.69ms
step:511/1670 train_time:44808ms step_avg:87.69ms
step:512/1670 train_time:44896ms step_avg:87.69ms
step:513/1670 train_time:44987ms step_avg:87.69ms
step:514/1670 train_time:45077ms step_avg:87.70ms
step:515/1670 train_time:45166ms step_avg:87.70ms
step:516/1670 train_time:45254ms step_avg:87.70ms
step:517/1670 train_time:45341ms step_avg:87.70ms
step:518/1670 train_time:45428ms step_avg:87.70ms
step:519/1670 train_time:45515ms step_avg:87.70ms
step:520/1670 train_time:45603ms step_avg:87.70ms
step:521/1670 train_time:45691ms step_avg:87.70ms
step:522/1670 train_time:45778ms step_avg:87.70ms
step:523/1670 train_time:45866ms step_avg:87.70ms
step:524/1670 train_time:45954ms step_avg:87.70ms
step:525/1670 train_time:46044ms step_avg:87.70ms
step:526/1670 train_time:46132ms step_avg:87.70ms
step:527/1670 train_time:46220ms step_avg:87.70ms
step:528/1670 train_time:46308ms step_avg:87.70ms
step:529/1670 train_time:46395ms step_avg:87.70ms
step:530/1670 train_time:46483ms step_avg:87.70ms
step:531/1670 train_time:46569ms step_avg:87.70ms
step:532/1670 train_time:46657ms step_avg:87.70ms
step:533/1670 train_time:46744ms step_avg:87.70ms
step:534/1670 train_time:46831ms step_avg:87.70ms
step:535/1670 train_time:46920ms step_avg:87.70ms
step:536/1670 train_time:47008ms step_avg:87.70ms
step:537/1670 train_time:47096ms step_avg:87.70ms
step:538/1670 train_time:47184ms step_avg:87.70ms
step:539/1670 train_time:47272ms step_avg:87.70ms
step:540/1670 train_time:47359ms step_avg:87.70ms
step:541/1670 train_time:47447ms step_avg:87.70ms
step:542/1670 train_time:47533ms step_avg:87.70ms
step:543/1670 train_time:47620ms step_avg:87.70ms
step:544/1670 train_time:47707ms step_avg:87.70ms
step:545/1670 train_time:47795ms step_avg:87.70ms
step:546/1670 train_time:47885ms step_avg:87.70ms
step:547/1670 train_time:47974ms step_avg:87.70ms
step:548/1670 train_time:48063ms step_avg:87.71ms
step:549/1670 train_time:48151ms step_avg:87.71ms
step:550/1670 train_time:48240ms step_avg:87.71ms
step:551/1670 train_time:48329ms step_avg:87.71ms
step:552/1670 train_time:48418ms step_avg:87.71ms
step:553/1670 train_time:48506ms step_avg:87.71ms
step:554/1670 train_time:48595ms step_avg:87.72ms
step:555/1670 train_time:48684ms step_avg:87.72ms
step:556/1670 train_time:48772ms step_avg:87.72ms
step:557/1670 train_time:48861ms step_avg:87.72ms
step:558/1670 train_time:48950ms step_avg:87.72ms
step:559/1670 train_time:49038ms step_avg:87.72ms
step:560/1670 train_time:49128ms step_avg:87.73ms
step:561/1670 train_time:49217ms step_avg:87.73ms
step:562/1670 train_time:49307ms step_avg:87.73ms
step:563/1670 train_time:49396ms step_avg:87.74ms
step:564/1670 train_time:49486ms step_avg:87.74ms
step:565/1670 train_time:49575ms step_avg:87.74ms
step:566/1670 train_time:49664ms step_avg:87.75ms
step:567/1670 train_time:49752ms step_avg:87.75ms
step:568/1670 train_time:49843ms step_avg:87.75ms
step:569/1670 train_time:49930ms step_avg:87.75ms
step:570/1670 train_time:50019ms step_avg:87.75ms
step:571/1670 train_time:50109ms step_avg:87.76ms
step:572/1670 train_time:50199ms step_avg:87.76ms
step:573/1670 train_time:50287ms step_avg:87.76ms
step:574/1670 train_time:50376ms step_avg:87.76ms
step:575/1670 train_time:50465ms step_avg:87.76ms
step:576/1670 train_time:50554ms step_avg:87.77ms
step:577/1670 train_time:50643ms step_avg:87.77ms
step:578/1670 train_time:50731ms step_avg:87.77ms
step:579/1670 train_time:50820ms step_avg:87.77ms
step:580/1670 train_time:50908ms step_avg:87.77ms
step:581/1670 train_time:50998ms step_avg:87.78ms
step:582/1670 train_time:51087ms step_avg:87.78ms
step:583/1670 train_time:51177ms step_avg:87.78ms
step:584/1670 train_time:51265ms step_avg:87.78ms
step:585/1670 train_time:51354ms step_avg:87.78ms
step:586/1670 train_time:51443ms step_avg:87.79ms
step:587/1670 train_time:51531ms step_avg:87.79ms
step:588/1670 train_time:51620ms step_avg:87.79ms
step:589/1670 train_time:51709ms step_avg:87.79ms
step:590/1670 train_time:51799ms step_avg:87.80ms
step:591/1670 train_time:51888ms step_avg:87.80ms
step:592/1670 train_time:51977ms step_avg:87.80ms
step:593/1670 train_time:52067ms step_avg:87.80ms
step:594/1670 train_time:52156ms step_avg:87.80ms
step:595/1670 train_time:52245ms step_avg:87.81ms
step:596/1670 train_time:52333ms step_avg:87.81ms
step:597/1670 train_time:52423ms step_avg:87.81ms
step:598/1670 train_time:52512ms step_avg:87.81ms
step:599/1670 train_time:52601ms step_avg:87.81ms
step:600/1670 train_time:52690ms step_avg:87.82ms
step:601/1670 train_time:52779ms step_avg:87.82ms
step:602/1670 train_time:52867ms step_avg:87.82ms
step:603/1670 train_time:52956ms step_avg:87.82ms
step:604/1670 train_time:53045ms step_avg:87.82ms
step:605/1670 train_time:53135ms step_avg:87.83ms
step:606/1670 train_time:53224ms step_avg:87.83ms
step:607/1670 train_time:53313ms step_avg:87.83ms
step:608/1670 train_time:53402ms step_avg:87.83ms
step:609/1670 train_time:53491ms step_avg:87.83ms
step:610/1670 train_time:53581ms step_avg:87.84ms
step:611/1670 train_time:53670ms step_avg:87.84ms
step:612/1670 train_time:53759ms step_avg:87.84ms
step:613/1670 train_time:53847ms step_avg:87.84ms
step:614/1670 train_time:53936ms step_avg:87.84ms
step:615/1670 train_time:54025ms step_avg:87.85ms
step:616/1670 train_time:54115ms step_avg:87.85ms
step:617/1670 train_time:54205ms step_avg:87.85ms
step:618/1670 train_time:54294ms step_avg:87.85ms
step:619/1670 train_time:54383ms step_avg:87.86ms
step:620/1670 train_time:54472ms step_avg:87.86ms
step:621/1670 train_time:54561ms step_avg:87.86ms
step:622/1670 train_time:54649ms step_avg:87.86ms
step:623/1670 train_time:54738ms step_avg:87.86ms
step:624/1670 train_time:54827ms step_avg:87.86ms
step:625/1670 train_time:54916ms step_avg:87.87ms
step:625/1670 val_loss:3.6211 train_time:55006ms step_avg:88.01ms
step:626/1670 train_time:55026ms step_avg:87.90ms
step:627/1670 train_time:55097ms step_avg:87.87ms
step:628/1670 train_time:55186ms step_avg:87.88ms
step:629/1670 train_time:55275ms step_avg:87.88ms
step:630/1670 train_time:55363ms step_avg:87.88ms
step:631/1670 train_time:55450ms step_avg:87.88ms
step:632/1670 train_time:55538ms step_avg:87.88ms
step:633/1670 train_time:55626ms step_avg:87.88ms
step:634/1670 train_time:55714ms step_avg:87.88ms
step:635/1670 train_time:55803ms step_avg:87.88ms
step:636/1670 train_time:55892ms step_avg:87.88ms
step:637/1670 train_time:55985ms step_avg:87.89ms
step:638/1670 train_time:56074ms step_avg:87.89ms
step:639/1670 train_time:56164ms step_avg:87.89ms
step:640/1670 train_time:56252ms step_avg:87.89ms
step:641/1670 train_time:56341ms step_avg:87.90ms
step:642/1670 train_time:56428ms step_avg:87.89ms
step:643/1670 train_time:56516ms step_avg:87.89ms
step:644/1670 train_time:56605ms step_avg:87.90ms
step:645/1670 train_time:56692ms step_avg:87.89ms
step:646/1670 train_time:56781ms step_avg:87.90ms
step:647/1670 train_time:56871ms step_avg:87.90ms
step:648/1670 train_time:56961ms step_avg:87.90ms
step:649/1670 train_time:57051ms step_avg:87.91ms
step:650/1670 train_time:57141ms step_avg:87.91ms
step:651/1670 train_time:57230ms step_avg:87.91ms
step:652/1670 train_time:57318ms step_avg:87.91ms
step:653/1670 train_time:57407ms step_avg:87.91ms
step:654/1670 train_time:57495ms step_avg:87.91ms
step:655/1670 train_time:57583ms step_avg:87.91ms
step:656/1670 train_time:57671ms step_avg:87.91ms
step:657/1670 train_time:57759ms step_avg:87.91ms
step:658/1670 train_time:57850ms step_avg:87.92ms
step:659/1670 train_time:57941ms step_avg:87.92ms
step:660/1670 train_time:58031ms step_avg:87.93ms
step:661/1670 train_time:58121ms step_avg:87.93ms
step:662/1670 train_time:58210ms step_avg:87.93ms
step:663/1670 train_time:58299ms step_avg:87.93ms
step:664/1670 train_time:58387ms step_avg:87.93ms
step:665/1670 train_time:58476ms step_avg:87.93ms
step:666/1670 train_time:58564ms step_avg:87.93ms
step:667/1670 train_time:58652ms step_avg:87.93ms
step:668/1670 train_time:58740ms step_avg:87.93ms
step:669/1670 train_time:58829ms step_avg:87.94ms
step:670/1670 train_time:58920ms step_avg:87.94ms
step:671/1670 train_time:59009ms step_avg:87.94ms
step:672/1670 train_time:59099ms step_avg:87.94ms
step:673/1670 train_time:59189ms step_avg:87.95ms
step:674/1670 train_time:59278ms step_avg:87.95ms
step:675/1670 train_time:59367ms step_avg:87.95ms
step:676/1670 train_time:59455ms step_avg:87.95ms
step:677/1670 train_time:59544ms step_avg:87.95ms
step:678/1670 train_time:59632ms step_avg:87.95ms
step:679/1670 train_time:59720ms step_avg:87.95ms
step:680/1670 train_time:59809ms step_avg:87.95ms
step:681/1670 train_time:59898ms step_avg:87.96ms
step:682/1670 train_time:59988ms step_avg:87.96ms
step:683/1670 train_time:60078ms step_avg:87.96ms
step:684/1670 train_time:60168ms step_avg:87.97ms
step:685/1670 train_time:60257ms step_avg:87.97ms
step:686/1670 train_time:60346ms step_avg:87.97ms
step:687/1670 train_time:60434ms step_avg:87.97ms
step:688/1670 train_time:60523ms step_avg:87.97ms
step:689/1670 train_time:60611ms step_avg:87.97ms
step:690/1670 train_time:60701ms step_avg:87.97ms
step:691/1670 train_time:60789ms step_avg:87.97ms
step:692/1670 train_time:60878ms step_avg:87.97ms
step:693/1670 train_time:60967ms step_avg:87.98ms
step:694/1670 train_time:61056ms step_avg:87.98ms
step:695/1670 train_time:61146ms step_avg:87.98ms
step:696/1670 train_time:61234ms step_avg:87.98ms
step:697/1670 train_time:61323ms step_avg:87.98ms
step:698/1670 train_time:61412ms step_avg:87.98ms
step:699/1670 train_time:61502ms step_avg:87.98ms
step:700/1670 train_time:61590ms step_avg:87.99ms
step:701/1670 train_time:61679ms step_avg:87.99ms
step:702/1670 train_time:61767ms step_avg:87.99ms
step:703/1670 train_time:61855ms step_avg:87.99ms
step:704/1670 train_time:61945ms step_avg:87.99ms
step:705/1670 train_time:62034ms step_avg:87.99ms
step:706/1670 train_time:62123ms step_avg:87.99ms
step:707/1670 train_time:62211ms step_avg:87.99ms
step:708/1670 train_time:62301ms step_avg:88.00ms
step:709/1670 train_time:62390ms step_avg:88.00ms
step:710/1670 train_time:62479ms step_avg:88.00ms
step:711/1670 train_time:62567ms step_avg:88.00ms
step:712/1670 train_time:62656ms step_avg:88.00ms
step:713/1670 train_time:62746ms step_avg:88.00ms
step:714/1670 train_time:62834ms step_avg:88.00ms
step:715/1670 train_time:62923ms step_avg:88.00ms
step:716/1670 train_time:63012ms step_avg:88.01ms
step:717/1670 train_time:63102ms step_avg:88.01ms
step:718/1670 train_time:63190ms step_avg:88.01ms
step:719/1670 train_time:63279ms step_avg:88.01ms
step:720/1670 train_time:63368ms step_avg:88.01ms
step:721/1670 train_time:63456ms step_avg:88.01ms
step:722/1670 train_time:63546ms step_avg:88.01ms
step:723/1670 train_time:63635ms step_avg:88.01ms
step:724/1670 train_time:63724ms step_avg:88.02ms
step:725/1670 train_time:63813ms step_avg:88.02ms
step:726/1670 train_time:63902ms step_avg:88.02ms
step:727/1670 train_time:63991ms step_avg:88.02ms
step:728/1670 train_time:64079ms step_avg:88.02ms
step:729/1670 train_time:64168ms step_avg:88.02ms
step:730/1670 train_time:64257ms step_avg:88.02ms
step:731/1670 train_time:64346ms step_avg:88.02ms
step:732/1670 train_time:64434ms step_avg:88.02ms
step:733/1670 train_time:64523ms step_avg:88.03ms
step:734/1670 train_time:64612ms step_avg:88.03ms
step:735/1670 train_time:64702ms step_avg:88.03ms
step:736/1670 train_time:64790ms step_avg:88.03ms
step:737/1670 train_time:64879ms step_avg:88.03ms
step:738/1670 train_time:64968ms step_avg:88.03ms
step:739/1670 train_time:65057ms step_avg:88.03ms
step:740/1670 train_time:65147ms step_avg:88.04ms
step:741/1670 train_time:65236ms step_avg:88.04ms
step:742/1670 train_time:65325ms step_avg:88.04ms
step:743/1670 train_time:65413ms step_avg:88.04ms
step:744/1670 train_time:65502ms step_avg:88.04ms
step:745/1670 train_time:65590ms step_avg:88.04ms
step:746/1670 train_time:65679ms step_avg:88.04ms
step:747/1670 train_time:65769ms step_avg:88.04ms
step:748/1670 train_time:65858ms step_avg:88.05ms
step:749/1670 train_time:65948ms step_avg:88.05ms
step:750/1670 train_time:66038ms step_avg:88.05ms
step:750/1670 val_loss:3.5708 train_time:66130ms step_avg:88.17ms
step:751/1670 train_time:66150ms step_avg:88.08ms
step:752/1670 train_time:66223ms step_avg:88.06ms
step:753/1670 train_time:66316ms step_avg:88.07ms
step:754/1670 train_time:66406ms step_avg:88.07ms
step:755/1670 train_time:66494ms step_avg:88.07ms
step:756/1670 train_time:66582ms step_avg:88.07ms
step:757/1670 train_time:66670ms step_avg:88.07ms
step:758/1670 train_time:66757ms step_avg:88.07ms
step:759/1670 train_time:66845ms step_avg:88.07ms
step:760/1670 train_time:66933ms step_avg:88.07ms
step:761/1670 train_time:67021ms step_avg:88.07ms
step:762/1670 train_time:67113ms step_avg:88.07ms
step:763/1670 train_time:67204ms step_avg:88.08ms
step:764/1670 train_time:67294ms step_avg:88.08ms
step:765/1670 train_time:67386ms step_avg:88.09ms
step:766/1670 train_time:67475ms step_avg:88.09ms
step:767/1670 train_time:67564ms step_avg:88.09ms
step:768/1670 train_time:67652ms step_avg:88.09ms
step:769/1670 train_time:67740ms step_avg:88.09ms
step:770/1670 train_time:67828ms step_avg:88.09ms
step:771/1670 train_time:67916ms step_avg:88.09ms
step:772/1670 train_time:68005ms step_avg:88.09ms
step:773/1670 train_time:68094ms step_avg:88.09ms
step:774/1670 train_time:68184ms step_avg:88.09ms
step:775/1670 train_time:68274ms step_avg:88.10ms
step:776/1670 train_time:68365ms step_avg:88.10ms
step:777/1670 train_time:68454ms step_avg:88.10ms
step:778/1670 train_time:68543ms step_avg:88.10ms
step:779/1670 train_time:68631ms step_avg:88.10ms
step:780/1670 train_time:68719ms step_avg:88.10ms
step:781/1670 train_time:68807ms step_avg:88.10ms
step:782/1670 train_time:68896ms step_avg:88.10ms
step:783/1670 train_time:68985ms step_avg:88.10ms
step:784/1670 train_time:69073ms step_avg:88.10ms
step:785/1670 train_time:69163ms step_avg:88.11ms
step:786/1670 train_time:69252ms step_avg:88.11ms
step:787/1670 train_time:69341ms step_avg:88.11ms
step:788/1670 train_time:69430ms step_avg:88.11ms
step:789/1670 train_time:69520ms step_avg:88.11ms
step:790/1670 train_time:69609ms step_avg:88.11ms
step:791/1670 train_time:69699ms step_avg:88.11ms
step:792/1670 train_time:69786ms step_avg:88.11ms
step:793/1670 train_time:69875ms step_avg:88.11ms
step:794/1670 train_time:69963ms step_avg:88.11ms
step:795/1670 train_time:70051ms step_avg:88.12ms
step:796/1670 train_time:70141ms step_avg:88.12ms
step:797/1670 train_time:70231ms step_avg:88.12ms
step:798/1670 train_time:70320ms step_avg:88.12ms
step:799/1670 train_time:70409ms step_avg:88.12ms
step:800/1670 train_time:70497ms step_avg:88.12ms
step:801/1670 train_time:70587ms step_avg:88.12ms
step:802/1670 train_time:70676ms step_avg:88.12ms
step:803/1670 train_time:70764ms step_avg:88.12ms
step:804/1670 train_time:70853ms step_avg:88.13ms
step:805/1670 train_time:70942ms step_avg:88.13ms
step:806/1670 train_time:71030ms step_avg:88.13ms
step:807/1670 train_time:71120ms step_avg:88.13ms
step:808/1670 train_time:71209ms step_avg:88.13ms
step:809/1670 train_time:71298ms step_avg:88.13ms
step:810/1670 train_time:71388ms step_avg:88.13ms
step:811/1670 train_time:71476ms step_avg:88.13ms
step:812/1670 train_time:71565ms step_avg:88.13ms
step:813/1670 train_time:71654ms step_avg:88.14ms
step:814/1670 train_time:71743ms step_avg:88.14ms
step:815/1670 train_time:71831ms step_avg:88.14ms
step:816/1670 train_time:71921ms step_avg:88.14ms
step:817/1670 train_time:72010ms step_avg:88.14ms
step:818/1670 train_time:72099ms step_avg:88.14ms
step:819/1670 train_time:72187ms step_avg:88.14ms
step:820/1670 train_time:72276ms step_avg:88.14ms
step:821/1670 train_time:72366ms step_avg:88.14ms
step:822/1670 train_time:72455ms step_avg:88.15ms
step:823/1670 train_time:72545ms step_avg:88.15ms
step:824/1670 train_time:72634ms step_avg:88.15ms
step:825/1670 train_time:72722ms step_avg:88.15ms
step:826/1670 train_time:72811ms step_avg:88.15ms
step:827/1670 train_time:72900ms step_avg:88.15ms
step:828/1670 train_time:72989ms step_avg:88.15ms
step:829/1670 train_time:73079ms step_avg:88.15ms
step:830/1670 train_time:73168ms step_avg:88.15ms
step:831/1670 train_time:73258ms step_avg:88.16ms
step:832/1670 train_time:73347ms step_avg:88.16ms
step:833/1670 train_time:73437ms step_avg:88.16ms
step:834/1670 train_time:73525ms step_avg:88.16ms
step:835/1670 train_time:73614ms step_avg:88.16ms
step:836/1670 train_time:73703ms step_avg:88.16ms
step:837/1670 train_time:73792ms step_avg:88.16ms
step:838/1670 train_time:73880ms step_avg:88.16ms
step:839/1670 train_time:73969ms step_avg:88.16ms
step:840/1670 train_time:74059ms step_avg:88.17ms
step:841/1670 train_time:74149ms step_avg:88.17ms
step:842/1670 train_time:74238ms step_avg:88.17ms
step:843/1670 train_time:74327ms step_avg:88.17ms
step:844/1670 train_time:74417ms step_avg:88.17ms
step:845/1670 train_time:74506ms step_avg:88.17ms
step:846/1670 train_time:74595ms step_avg:88.17ms
step:847/1670 train_time:74685ms step_avg:88.18ms
step:848/1670 train_time:74772ms step_avg:88.17ms
step:849/1670 train_time:74861ms step_avg:88.18ms
step:850/1670 train_time:74950ms step_avg:88.18ms
step:851/1670 train_time:75039ms step_avg:88.18ms
step:852/1670 train_time:75128ms step_avg:88.18ms
step:853/1670 train_time:75218ms step_avg:88.18ms
step:854/1670 train_time:75308ms step_avg:88.18ms
step:855/1670 train_time:75397ms step_avg:88.18ms
step:856/1670 train_time:75486ms step_avg:88.18ms
step:857/1670 train_time:75576ms step_avg:88.19ms
step:858/1670 train_time:75665ms step_avg:88.19ms
step:859/1670 train_time:75753ms step_avg:88.19ms
step:860/1670 train_time:75843ms step_avg:88.19ms
step:861/1670 train_time:75931ms step_avg:88.19ms
step:862/1670 train_time:76020ms step_avg:88.19ms
step:863/1670 train_time:76109ms step_avg:88.19ms
step:864/1670 train_time:76198ms step_avg:88.19ms
step:865/1670 train_time:76287ms step_avg:88.19ms
step:866/1670 train_time:76375ms step_avg:88.19ms
step:867/1670 train_time:76464ms step_avg:88.19ms
step:868/1670 train_time:76553ms step_avg:88.19ms
step:869/1670 train_time:76643ms step_avg:88.20ms
step:870/1670 train_time:76731ms step_avg:88.20ms
step:871/1670 train_time:76820ms step_avg:88.20ms
step:872/1670 train_time:76908ms step_avg:88.20ms
step:873/1670 train_time:76997ms step_avg:88.20ms
step:874/1670 train_time:77086ms step_avg:88.20ms
step:875/1670 train_time:77175ms step_avg:88.20ms
step:875/1670 val_loss:3.5191 train_time:77266ms step_avg:88.30ms
step:876/1670 train_time:77285ms step_avg:88.22ms
step:877/1670 train_time:77358ms step_avg:88.21ms
step:878/1670 train_time:77452ms step_avg:88.21ms
step:879/1670 train_time:77542ms step_avg:88.22ms
step:880/1670 train_time:77631ms step_avg:88.22ms
step:881/1670 train_time:77719ms step_avg:88.22ms
step:882/1670 train_time:77807ms step_avg:88.22ms
step:883/1670 train_time:77895ms step_avg:88.22ms
step:884/1670 train_time:77982ms step_avg:88.22ms
step:885/1670 train_time:78070ms step_avg:88.21ms
step:886/1670 train_time:78158ms step_avg:88.21ms
step:887/1670 train_time:78248ms step_avg:88.22ms
step:888/1670 train_time:78339ms step_avg:88.22ms
step:889/1670 train_time:78430ms step_avg:88.22ms
step:890/1670 train_time:78521ms step_avg:88.23ms
step:891/1670 train_time:78610ms step_avg:88.23ms
step:892/1670 train_time:78699ms step_avg:88.23ms
step:893/1670 train_time:78789ms step_avg:88.23ms
step:894/1670 train_time:78877ms step_avg:88.23ms
step:895/1670 train_time:78965ms step_avg:88.23ms
step:896/1670 train_time:79053ms step_avg:88.23ms
step:897/1670 train_time:79141ms step_avg:88.23ms
step:898/1670 train_time:79230ms step_avg:88.23ms
step:899/1670 train_time:79319ms step_avg:88.23ms
step:900/1670 train_time:79409ms step_avg:88.23ms
step:901/1670 train_time:79499ms step_avg:88.23ms
step:902/1670 train_time:79590ms step_avg:88.24ms
step:903/1670 train_time:79679ms step_avg:88.24ms
step:904/1670 train_time:79768ms step_avg:88.24ms
step:905/1670 train_time:79856ms step_avg:88.24ms
step:906/1670 train_time:79944ms step_avg:88.24ms
step:907/1670 train_time:80032ms step_avg:88.24ms
step:908/1670 train_time:80120ms step_avg:88.24ms
step:909/1670 train_time:80209ms step_avg:88.24ms
step:910/1670 train_time:80299ms step_avg:88.24ms
step:911/1670 train_time:80389ms step_avg:88.24ms
step:912/1670 train_time:80479ms step_avg:88.24ms
step:913/1670 train_time:80569ms step_avg:88.25ms
step:914/1670 train_time:80658ms step_avg:88.25ms
step:915/1670 train_time:80747ms step_avg:88.25ms
step:916/1670 train_time:80835ms step_avg:88.25ms
step:917/1670 train_time:80924ms step_avg:88.25ms
step:918/1670 train_time:81012ms step_avg:88.25ms
step:919/1670 train_time:81101ms step_avg:88.25ms
step:920/1670 train_time:81190ms step_avg:88.25ms
step:921/1670 train_time:81278ms step_avg:88.25ms
step:922/1670 train_time:81367ms step_avg:88.25ms
step:923/1670 train_time:81456ms step_avg:88.25ms
step:924/1670 train_time:81546ms step_avg:88.25ms
step:925/1670 train_time:81634ms step_avg:88.25ms
step:926/1670 train_time:81724ms step_avg:88.25ms
step:927/1670 train_time:81813ms step_avg:88.26ms
step:928/1670 train_time:81902ms step_avg:88.26ms
step:929/1670 train_time:81991ms step_avg:88.26ms
step:930/1670 train_time:82080ms step_avg:88.26ms
step:931/1670 train_time:82169ms step_avg:88.26ms
step:932/1670 train_time:82258ms step_avg:88.26ms
step:933/1670 train_time:82347ms step_avg:88.26ms
step:934/1670 train_time:82436ms step_avg:88.26ms
step:935/1670 train_time:82526ms step_avg:88.26ms
step:936/1670 train_time:82614ms step_avg:88.26ms
step:937/1670 train_time:82703ms step_avg:88.26ms
step:938/1670 train_time:82791ms step_avg:88.26ms
step:939/1670 train_time:82881ms step_avg:88.26ms
step:940/1670 train_time:82970ms step_avg:88.27ms
step:941/1670 train_time:83058ms step_avg:88.27ms
step:942/1670 train_time:83147ms step_avg:88.27ms
step:943/1670 train_time:83236ms step_avg:88.27ms
step:944/1670 train_time:83325ms step_avg:88.27ms
step:945/1670 train_time:83414ms step_avg:88.27ms
step:946/1670 train_time:83503ms step_avg:88.27ms
step:947/1670 train_time:83592ms step_avg:88.27ms
step:948/1670 train_time:83681ms step_avg:88.27ms
step:949/1670 train_time:83770ms step_avg:88.27ms
step:950/1670 train_time:83858ms step_avg:88.27ms
step:951/1670 train_time:83948ms step_avg:88.27ms
step:952/1670 train_time:84037ms step_avg:88.27ms
step:953/1670 train_time:84126ms step_avg:88.27ms
step:954/1670 train_time:84215ms step_avg:88.28ms
step:955/1670 train_time:84304ms step_avg:88.28ms
step:956/1670 train_time:84392ms step_avg:88.28ms
step:957/1670 train_time:84480ms step_avg:88.28ms
step:958/1670 train_time:84569ms step_avg:88.28ms
step:959/1670 train_time:84658ms step_avg:88.28ms
step:960/1670 train_time:84747ms step_avg:88.28ms
step:961/1670 train_time:84835ms step_avg:88.28ms
step:962/1670 train_time:84925ms step_avg:88.28ms
step:963/1670 train_time:85013ms step_avg:88.28ms
step:964/1670 train_time:85102ms step_avg:88.28ms
step:965/1670 train_time:85191ms step_avg:88.28ms
step:966/1670 train_time:85280ms step_avg:88.28ms
step:967/1670 train_time:85368ms step_avg:88.28ms
step:968/1670 train_time:85457ms step_avg:88.28ms
step:969/1670 train_time:85546ms step_avg:88.28ms
step:970/1670 train_time:85634ms step_avg:88.28ms
step:971/1670 train_time:85724ms step_avg:88.28ms
step:972/1670 train_time:85812ms step_avg:88.28ms
step:973/1670 train_time:85902ms step_avg:88.29ms
step:974/1670 train_time:85991ms step_avg:88.29ms
step:975/1670 train_time:86080ms step_avg:88.29ms
step:976/1670 train_time:86169ms step_avg:88.29ms
step:977/1670 train_time:86258ms step_avg:88.29ms
step:978/1670 train_time:86347ms step_avg:88.29ms
step:979/1670 train_time:86435ms step_avg:88.29ms
step:980/1670 train_time:86524ms step_avg:88.29ms
step:981/1670 train_time:86613ms step_avg:88.29ms
step:982/1670 train_time:86702ms step_avg:88.29ms
step:983/1670 train_time:86791ms step_avg:88.29ms
step:984/1670 train_time:86881ms step_avg:88.29ms
step:985/1670 train_time:86970ms step_avg:88.29ms
step:986/1670 train_time:87060ms step_avg:88.30ms
step:987/1670 train_time:87148ms step_avg:88.30ms
step:988/1670 train_time:87237ms step_avg:88.30ms
step:989/1670 train_time:87327ms step_avg:88.30ms
step:990/1670 train_time:87415ms step_avg:88.30ms
step:991/1670 train_time:87505ms step_avg:88.30ms
step:992/1670 train_time:87593ms step_avg:88.30ms
step:993/1670 train_time:87683ms step_avg:88.30ms
step:994/1670 train_time:87771ms step_avg:88.30ms
step:995/1670 train_time:87861ms step_avg:88.30ms
step:996/1670 train_time:87950ms step_avg:88.30ms
step:997/1670 train_time:88038ms step_avg:88.30ms
step:998/1670 train_time:88128ms step_avg:88.30ms
step:999/1670 train_time:88217ms step_avg:88.31ms
step:1000/1670 train_time:88307ms step_avg:88.31ms
step:1000/1670 val_loss:3.4687 train_time:88396ms step_avg:88.40ms
step:1001/1670 train_time:88416ms step_avg:88.33ms
step:1002/1670 train_time:88488ms step_avg:88.31ms
step:1003/1670 train_time:88582ms step_avg:88.32ms
step:1004/1670 train_time:88670ms step_avg:88.32ms
step:1005/1670 train_time:88759ms step_avg:88.32ms
step:1006/1670 train_time:88848ms step_avg:88.32ms
step:1007/1670 train_time:88936ms step_avg:88.32ms
step:1008/1670 train_time:89024ms step_avg:88.32ms
step:1009/1670 train_time:89112ms step_avg:88.32ms
step:1010/1670 train_time:89201ms step_avg:88.32ms
step:1011/1670 train_time:89289ms step_avg:88.32ms
step:1012/1670 train_time:89378ms step_avg:88.32ms
step:1013/1670 train_time:89470ms step_avg:88.32ms
step:1014/1670 train_time:89561ms step_avg:88.32ms
step:1015/1670 train_time:89651ms step_avg:88.33ms
step:1016/1670 train_time:89740ms step_avg:88.33ms
step:1017/1670 train_time:89829ms step_avg:88.33ms
step:1018/1670 train_time:89919ms step_avg:88.33ms
step:1019/1670 train_time:90007ms step_avg:88.33ms
step:1020/1670 train_time:90095ms step_avg:88.33ms
step:1021/1670 train_time:90183ms step_avg:88.33ms
step:1022/1670 train_time:90271ms step_avg:88.33ms
step:1023/1670 train_time:90361ms step_avg:88.33ms
step:1024/1670 train_time:90451ms step_avg:88.33ms
step:1025/1670 train_time:90542ms step_avg:88.33ms
step:1026/1670 train_time:90631ms step_avg:88.33ms
step:1027/1670 train_time:90721ms step_avg:88.34ms
step:1028/1670 train_time:90809ms step_avg:88.34ms
step:1029/1670 train_time:90898ms step_avg:88.34ms
step:1030/1670 train_time:90987ms step_avg:88.34ms
step:1031/1670 train_time:91074ms step_avg:88.34ms
step:1032/1670 train_time:91163ms step_avg:88.34ms
step:1033/1670 train_time:91251ms step_avg:88.34ms
step:1034/1670 train_time:91340ms step_avg:88.34ms
step:1035/1670 train_time:91430ms step_avg:88.34ms
step:1036/1670 train_time:91519ms step_avg:88.34ms
step:1037/1670 train_time:91609ms step_avg:88.34ms
step:1038/1670 train_time:91699ms step_avg:88.34ms
step:1039/1670 train_time:91787ms step_avg:88.34ms
step:1040/1670 train_time:91876ms step_avg:88.34ms
step:1041/1670 train_time:91965ms step_avg:88.34ms
step:1042/1670 train_time:92054ms step_avg:88.34ms
step:1043/1670 train_time:92143ms step_avg:88.34ms
step:1044/1670 train_time:92232ms step_avg:88.34ms
step:1045/1670 train_time:92322ms step_avg:88.35ms
step:1046/1670 train_time:92410ms step_avg:88.35ms
step:1047/1670 train_time:92500ms step_avg:88.35ms
step:1048/1670 train_time:92589ms step_avg:88.35ms
step:1049/1670 train_time:92679ms step_avg:88.35ms
step:1050/1670 train_time:92768ms step_avg:88.35ms
step:1051/1670 train_time:92857ms step_avg:88.35ms
step:1052/1670 train_time:92946ms step_avg:88.35ms
step:1053/1670 train_time:93036ms step_avg:88.35ms
step:1054/1670 train_time:93125ms step_avg:88.35ms
step:1055/1670 train_time:93213ms step_avg:88.35ms
step:1056/1670 train_time:93302ms step_avg:88.35ms
step:1057/1670 train_time:93391ms step_avg:88.35ms
step:1058/1670 train_time:93480ms step_avg:88.36ms
step:1059/1670 train_time:93569ms step_avg:88.36ms
step:1060/1670 train_time:93658ms step_avg:88.36ms
step:1061/1670 train_time:93747ms step_avg:88.36ms
step:1062/1670 train_time:93837ms step_avg:88.36ms
step:1063/1670 train_time:93925ms step_avg:88.36ms
step:1064/1670 train_time:94014ms step_avg:88.36ms
step:1065/1670 train_time:94104ms step_avg:88.36ms
step:1066/1670 train_time:94192ms step_avg:88.36ms
step:1067/1670 train_time:94281ms step_avg:88.36ms
step:1068/1670 train_time:94369ms step_avg:88.36ms
step:1069/1670 train_time:94460ms step_avg:88.36ms
step:1070/1670 train_time:94549ms step_avg:88.36ms
step:1071/1670 train_time:94637ms step_avg:88.36ms
step:1072/1670 train_time:94726ms step_avg:88.36ms
step:1073/1670 train_time:94816ms step_avg:88.37ms
step:1074/1670 train_time:94905ms step_avg:88.37ms
step:1075/1670 train_time:94993ms step_avg:88.37ms
step:1076/1670 train_time:95082ms step_avg:88.37ms
step:1077/1670 train_time:95171ms step_avg:88.37ms
step:1078/1670 train_time:95260ms step_avg:88.37ms
step:1079/1670 train_time:95349ms step_avg:88.37ms
step:1080/1670 train_time:95438ms step_avg:88.37ms
step:1081/1670 train_time:95528ms step_avg:88.37ms
step:1082/1670 train_time:95617ms step_avg:88.37ms
step:1083/1670 train_time:95706ms step_avg:88.37ms
step:1084/1670 train_time:95795ms step_avg:88.37ms
step:1085/1670 train_time:95884ms step_avg:88.37ms
step:1086/1670 train_time:95973ms step_avg:88.37ms
step:1087/1670 train_time:96062ms step_avg:88.37ms
step:1088/1670 train_time:96150ms step_avg:88.37ms
step:1089/1670 train_time:96239ms step_avg:88.37ms
step:1090/1670 train_time:96329ms step_avg:88.38ms
step:1091/1670 train_time:96420ms step_avg:88.38ms
step:1092/1670 train_time:96510ms step_avg:88.38ms
step:1093/1670 train_time:96599ms step_avg:88.38ms
step:1094/1670 train_time:96688ms step_avg:88.38ms
step:1095/1670 train_time:96778ms step_avg:88.38ms
step:1096/1670 train_time:96867ms step_avg:88.38ms
step:1097/1670 train_time:96957ms step_avg:88.38ms
step:1098/1670 train_time:97047ms step_avg:88.39ms
step:1099/1670 train_time:97138ms step_avg:88.39ms
step:1100/1670 train_time:97228ms step_avg:88.39ms
step:1101/1670 train_time:97319ms step_avg:88.39ms
step:1102/1670 train_time:97409ms step_avg:88.39ms
step:1103/1670 train_time:97499ms step_avg:88.39ms
step:1104/1670 train_time:97589ms step_avg:88.40ms
step:1105/1670 train_time:97678ms step_avg:88.40ms
step:1106/1670 train_time:97767ms step_avg:88.40ms
step:1107/1670 train_time:97857ms step_avg:88.40ms
step:1108/1670 train_time:97947ms step_avg:88.40ms
step:1109/1670 train_time:98037ms step_avg:88.40ms
step:1110/1670 train_time:98126ms step_avg:88.40ms
step:1111/1670 train_time:98216ms step_avg:88.40ms
step:1112/1670 train_time:98306ms step_avg:88.40ms
step:1113/1670 train_time:98395ms step_avg:88.41ms
step:1114/1670 train_time:98486ms step_avg:88.41ms
step:1115/1670 train_time:98576ms step_avg:88.41ms
step:1116/1670 train_time:98666ms step_avg:88.41ms
step:1117/1670 train_time:98756ms step_avg:88.41ms
step:1118/1670 train_time:98846ms step_avg:88.41ms
step:1119/1670 train_time:98935ms step_avg:88.41ms
step:1120/1670 train_time:99026ms step_avg:88.42ms
step:1121/1670 train_time:99115ms step_avg:88.42ms
step:1122/1670 train_time:99204ms step_avg:88.42ms
step:1123/1670 train_time:99294ms step_avg:88.42ms
step:1124/1670 train_time:99384ms step_avg:88.42ms
step:1125/1670 train_time:99473ms step_avg:88.42ms
step:1125/1670 val_loss:3.4156 train_time:99565ms step_avg:88.50ms
step:1126/1670 train_time:99585ms step_avg:88.44ms
step:1127/1670 train_time:99655ms step_avg:88.42ms
step:1128/1670 train_time:99745ms step_avg:88.43ms
step:1129/1670 train_time:99836ms step_avg:88.43ms
step:1130/1670 train_time:99925ms step_avg:88.43ms
step:1131/1670 train_time:100014ms step_avg:88.43ms
step:1132/1670 train_time:100102ms step_avg:88.43ms
step:1133/1670 train_time:100190ms step_avg:88.43ms
step:1134/1670 train_time:100279ms step_avg:88.43ms
step:1135/1670 train_time:100369ms step_avg:88.43ms
step:1136/1670 train_time:100458ms step_avg:88.43ms
step:1137/1670 train_time:100550ms step_avg:88.43ms
step:1138/1670 train_time:100641ms step_avg:88.44ms
step:1139/1670 train_time:100732ms step_avg:88.44ms
step:1140/1670 train_time:100822ms step_avg:88.44ms
step:1141/1670 train_time:100911ms step_avg:88.44ms
step:1142/1670 train_time:101002ms step_avg:88.44ms
step:1143/1670 train_time:101090ms step_avg:88.44ms
step:1144/1670 train_time:101179ms step_avg:88.44ms
step:1145/1670 train_time:101268ms step_avg:88.44ms
step:1146/1670 train_time:101358ms step_avg:88.45ms
step:1147/1670 train_time:101447ms step_avg:88.45ms
step:1148/1670 train_time:101538ms step_avg:88.45ms
step:1149/1670 train_time:101627ms step_avg:88.45ms
step:1150/1670 train_time:101718ms step_avg:88.45ms
step:1151/1670 train_time:101807ms step_avg:88.45ms
step:1152/1670 train_time:101897ms step_avg:88.45ms
step:1153/1670 train_time:101986ms step_avg:88.45ms
step:1154/1670 train_time:102076ms step_avg:88.45ms
step:1155/1670 train_time:102164ms step_avg:88.45ms
step:1156/1670 train_time:102253ms step_avg:88.45ms
step:1157/1670 train_time:102342ms step_avg:88.45ms
step:1158/1670 train_time:102431ms step_avg:88.46ms
step:1159/1670 train_time:102521ms step_avg:88.46ms
step:1160/1670 train_time:102611ms step_avg:88.46ms
step:1161/1670 train_time:102702ms step_avg:88.46ms
step:1162/1670 train_time:102793ms step_avg:88.46ms
step:1163/1670 train_time:102882ms step_avg:88.46ms
step:1164/1670 train_time:102972ms step_avg:88.46ms
step:1165/1670 train_time:103061ms step_avg:88.46ms
step:1166/1670 train_time:103151ms step_avg:88.47ms
step:1167/1670 train_time:103240ms step_avg:88.47ms
step:1168/1670 train_time:103330ms step_avg:88.47ms
step:1169/1670 train_time:103419ms step_avg:88.47ms
step:1170/1670 train_time:103508ms step_avg:88.47ms
step:1171/1670 train_time:103600ms step_avg:88.47ms
step:1172/1670 train_time:103690ms step_avg:88.47ms
step:1173/1670 train_time:103780ms step_avg:88.47ms
step:1174/1670 train_time:103870ms step_avg:88.48ms
step:1175/1670 train_time:103959ms step_avg:88.48ms
step:1176/1670 train_time:104049ms step_avg:88.48ms
step:1177/1670 train_time:104139ms step_avg:88.48ms
step:1178/1670 train_time:104228ms step_avg:88.48ms
step:1179/1670 train_time:104318ms step_avg:88.48ms
step:1180/1670 train_time:104406ms step_avg:88.48ms
step:1181/1670 train_time:104496ms step_avg:88.48ms
step:1182/1670 train_time:104586ms step_avg:88.48ms
step:1183/1670 train_time:104676ms step_avg:88.48ms
step:1184/1670 train_time:104765ms step_avg:88.48ms
step:1185/1670 train_time:104856ms step_avg:88.49ms
step:1186/1670 train_time:104945ms step_avg:88.49ms
step:1187/1670 train_time:105036ms step_avg:88.49ms
step:1188/1670 train_time:105125ms step_avg:88.49ms
step:1189/1670 train_time:105214ms step_avg:88.49ms
step:1190/1670 train_time:105303ms step_avg:88.49ms
step:1191/1670 train_time:105394ms step_avg:88.49ms
step:1192/1670 train_time:105483ms step_avg:88.49ms
step:1193/1670 train_time:105575ms step_avg:88.50ms
step:1194/1670 train_time:105665ms step_avg:88.50ms
step:1195/1670 train_time:105755ms step_avg:88.50ms
step:1196/1670 train_time:105844ms step_avg:88.50ms
step:1197/1670 train_time:105933ms step_avg:88.50ms
step:1198/1670 train_time:106023ms step_avg:88.50ms
step:1199/1670 train_time:106112ms step_avg:88.50ms
step:1200/1670 train_time:106201ms step_avg:88.50ms
step:1201/1670 train_time:106292ms step_avg:88.50ms
step:1202/1670 train_time:106382ms step_avg:88.50ms
step:1203/1670 train_time:106471ms step_avg:88.50ms
step:1204/1670 train_time:106561ms step_avg:88.51ms
step:1205/1670 train_time:106651ms step_avg:88.51ms
step:1206/1670 train_time:106741ms step_avg:88.51ms
step:1207/1670 train_time:106831ms step_avg:88.51ms
step:1208/1670 train_time:106921ms step_avg:88.51ms
step:1209/1670 train_time:107011ms step_avg:88.51ms
step:1210/1670 train_time:107101ms step_avg:88.51ms
step:1211/1670 train_time:107191ms step_avg:88.51ms
step:1212/1670 train_time:107281ms step_avg:88.52ms
step:1213/1670 train_time:107372ms step_avg:88.52ms
step:1214/1670 train_time:107462ms step_avg:88.52ms
step:1215/1670 train_time:107552ms step_avg:88.52ms
step:1216/1670 train_time:107642ms step_avg:88.52ms
step:1217/1670 train_time:107731ms step_avg:88.52ms
step:1218/1670 train_time:107822ms step_avg:88.52ms
step:1219/1670 train_time:107911ms step_avg:88.52ms
step:1220/1670 train_time:108002ms step_avg:88.53ms
step:1221/1670 train_time:108092ms step_avg:88.53ms
step:1222/1670 train_time:108181ms step_avg:88.53ms
step:1223/1670 train_time:108271ms step_avg:88.53ms
step:1224/1670 train_time:108361ms step_avg:88.53ms
step:1225/1670 train_time:108451ms step_avg:88.53ms
step:1226/1670 train_time:108542ms step_avg:88.53ms
step:1227/1670 train_time:108631ms step_avg:88.53ms
step:1228/1670 train_time:108721ms step_avg:88.54ms
step:1229/1670 train_time:108810ms step_avg:88.54ms
step:1230/1670 train_time:108900ms step_avg:88.54ms
step:1231/1670 train_time:108991ms step_avg:88.54ms
step:1232/1670 train_time:109080ms step_avg:88.54ms
step:1233/1670 train_time:109171ms step_avg:88.54ms
step:1234/1670 train_time:109262ms step_avg:88.54ms
step:1235/1670 train_time:109351ms step_avg:88.54ms
step:1236/1670 train_time:109441ms step_avg:88.54ms
step:1237/1670 train_time:109532ms step_avg:88.55ms
step:1238/1670 train_time:109621ms step_avg:88.55ms
step:1239/1670 train_time:109710ms step_avg:88.55ms
step:1240/1670 train_time:109800ms step_avg:88.55ms
step:1241/1670 train_time:109889ms step_avg:88.55ms
step:1242/1670 train_time:109980ms step_avg:88.55ms
step:1243/1670 train_time:110069ms step_avg:88.55ms
step:1244/1670 train_time:110160ms step_avg:88.55ms
step:1245/1670 train_time:110249ms step_avg:88.55ms
step:1246/1670 train_time:110339ms step_avg:88.55ms
step:1247/1670 train_time:110428ms step_avg:88.55ms
step:1248/1670 train_time:110519ms step_avg:88.56ms
step:1249/1670 train_time:110607ms step_avg:88.56ms
step:1250/1670 train_time:110697ms step_avg:88.56ms
step:1250/1670 val_loss:3.3774 train_time:110788ms step_avg:88.63ms
step:1251/1670 train_time:110807ms step_avg:88.58ms
step:1252/1670 train_time:110881ms step_avg:88.56ms
step:1253/1670 train_time:110973ms step_avg:88.57ms
step:1254/1670 train_time:111063ms step_avg:88.57ms
step:1255/1670 train_time:111151ms step_avg:88.57ms
step:1256/1670 train_time:111240ms step_avg:88.57ms
step:1257/1670 train_time:111329ms step_avg:88.57ms
step:1258/1670 train_time:111417ms step_avg:88.57ms
step:1259/1670 train_time:111506ms step_avg:88.57ms
step:1260/1670 train_time:111595ms step_avg:88.57ms
step:1261/1670 train_time:111684ms step_avg:88.57ms
step:1262/1670 train_time:111775ms step_avg:88.57ms
step:1263/1670 train_time:111867ms step_avg:88.57ms
step:1264/1670 train_time:111959ms step_avg:88.58ms
step:1265/1670 train_time:112049ms step_avg:88.58ms
step:1266/1670 train_time:112138ms step_avg:88.58ms
step:1267/1670 train_time:112228ms step_avg:88.58ms
step:1268/1670 train_time:112316ms step_avg:88.58ms
step:1269/1670 train_time:112405ms step_avg:88.58ms
step:1270/1670 train_time:112493ms step_avg:88.58ms
step:1271/1670 train_time:112582ms step_avg:88.58ms
step:1272/1670 train_time:112672ms step_avg:88.58ms
step:1273/1670 train_time:112763ms step_avg:88.58ms
step:1274/1670 train_time:112854ms step_avg:88.58ms
step:1275/1670 train_time:112946ms step_avg:88.58ms
step:1276/1670 train_time:113036ms step_avg:88.59ms
step:1277/1670 train_time:113125ms step_avg:88.59ms
step:1278/1670 train_time:113214ms step_avg:88.59ms
step:1279/1670 train_time:113304ms step_avg:88.59ms
step:1280/1670 train_time:113393ms step_avg:88.59ms
step:1281/1670 train_time:113483ms step_avg:88.59ms
step:1282/1670 train_time:113573ms step_avg:88.59ms
step:1283/1670 train_time:113662ms step_avg:88.59ms
step:1284/1670 train_time:113752ms step_avg:88.59ms
step:1285/1670 train_time:113843ms step_avg:88.59ms
step:1286/1670 train_time:113934ms step_avg:88.60ms
step:1287/1670 train_time:114025ms step_avg:88.60ms
step:1288/1670 train_time:114115ms step_avg:88.60ms
step:1289/1670 train_time:114205ms step_avg:88.60ms
step:1290/1670 train_time:114294ms step_avg:88.60ms
step:1291/1670 train_time:114384ms step_avg:88.60ms
step:1292/1670 train_time:114473ms step_avg:88.60ms
step:1293/1670 train_time:114563ms step_avg:88.60ms
step:1294/1670 train_time:114652ms step_avg:88.60ms
step:1295/1670 train_time:114741ms step_avg:88.60ms
step:1296/1670 train_time:114832ms step_avg:88.60ms
step:1297/1670 train_time:114922ms step_avg:88.61ms
step:1298/1670 train_time:115013ms step_avg:88.61ms
step:1299/1670 train_time:115103ms step_avg:88.61ms
step:1300/1670 train_time:115194ms step_avg:88.61ms
step:1301/1670 train_time:115284ms step_avg:88.61ms
step:1302/1670 train_time:115373ms step_avg:88.61ms
step:1303/1670 train_time:115462ms step_avg:88.61ms
step:1304/1670 train_time:115551ms step_avg:88.61ms
step:1305/1670 train_time:115640ms step_avg:88.61ms
step:1306/1670 train_time:115729ms step_avg:88.61ms
step:1307/1670 train_time:115819ms step_avg:88.61ms
step:1308/1670 train_time:115910ms step_avg:88.62ms
step:1309/1670 train_time:116000ms step_avg:88.62ms
step:1310/1670 train_time:116090ms step_avg:88.62ms
step:1311/1670 train_time:116181ms step_avg:88.62ms
step:1312/1670 train_time:116271ms step_avg:88.62ms
step:1313/1670 train_time:116360ms step_avg:88.62ms
step:1314/1670 train_time:116451ms step_avg:88.62ms
step:1315/1670 train_time:116540ms step_avg:88.62ms
step:1316/1670 train_time:116630ms step_avg:88.62ms
step:1317/1670 train_time:116720ms step_avg:88.63ms
step:1318/1670 train_time:116811ms step_avg:88.63ms
step:1319/1670 train_time:116902ms step_avg:88.63ms
step:1320/1670 train_time:116993ms step_avg:88.63ms
step:1321/1670 train_time:117085ms step_avg:88.63ms
step:1322/1670 train_time:117175ms step_avg:88.63ms
step:1323/1670 train_time:117266ms step_avg:88.64ms
step:1324/1670 train_time:117354ms step_avg:88.64ms
step:1325/1670 train_time:117443ms step_avg:88.64ms
step:1326/1670 train_time:117534ms step_avg:88.64ms
step:1327/1670 train_time:117623ms step_avg:88.64ms
step:1328/1670 train_time:117713ms step_avg:88.64ms
step:1329/1670 train_time:117803ms step_avg:88.64ms
step:1330/1670 train_time:117893ms step_avg:88.64ms
step:1331/1670 train_time:117982ms step_avg:88.64ms
step:1332/1670 train_time:118072ms step_avg:88.64ms
step:1333/1670 train_time:118162ms step_avg:88.64ms
step:1334/1670 train_time:118252ms step_avg:88.64ms
step:1335/1670 train_time:118341ms step_avg:88.65ms
step:1336/1670 train_time:118431ms step_avg:88.65ms
step:1337/1670 train_time:118521ms step_avg:88.65ms
step:1338/1670 train_time:118610ms step_avg:88.65ms
step:1339/1670 train_time:118699ms step_avg:88.65ms
step:1340/1670 train_time:118789ms step_avg:88.65ms
step:1341/1670 train_time:118879ms step_avg:88.65ms
step:1342/1670 train_time:118969ms step_avg:88.65ms
step:1343/1670 train_time:119059ms step_avg:88.65ms
step:1344/1670 train_time:119149ms step_avg:88.65ms
step:1345/1670 train_time:119238ms step_avg:88.65ms
step:1346/1670 train_time:119328ms step_avg:88.65ms
step:1347/1670 train_time:119417ms step_avg:88.65ms
step:1348/1670 train_time:119507ms step_avg:88.66ms
step:1349/1670 train_time:119597ms step_avg:88.66ms
step:1350/1670 train_time:119687ms step_avg:88.66ms
step:1351/1670 train_time:119777ms step_avg:88.66ms
step:1352/1670 train_time:119867ms step_avg:88.66ms
step:1353/1670 train_time:119957ms step_avg:88.66ms
step:1354/1670 train_time:120047ms step_avg:88.66ms
step:1355/1670 train_time:120136ms step_avg:88.66ms
step:1356/1670 train_time:120226ms step_avg:88.66ms
step:1357/1670 train_time:120315ms step_avg:88.66ms
step:1358/1670 train_time:120406ms step_avg:88.66ms
step:1359/1670 train_time:120495ms step_avg:88.66ms
step:1360/1670 train_time:120584ms step_avg:88.66ms
step:1361/1670 train_time:120673ms step_avg:88.66ms
step:1362/1670 train_time:120762ms step_avg:88.67ms
step:1363/1670 train_time:120852ms step_avg:88.67ms
step:1364/1670 train_time:120942ms step_avg:88.67ms
step:1365/1670 train_time:121033ms step_avg:88.67ms
step:1366/1670 train_time:121123ms step_avg:88.67ms
step:1367/1670 train_time:121213ms step_avg:88.67ms
step:1368/1670 train_time:121303ms step_avg:88.67ms
step:1369/1670 train_time:121393ms step_avg:88.67ms
step:1370/1670 train_time:121483ms step_avg:88.67ms
step:1371/1670 train_time:121572ms step_avg:88.67ms
step:1372/1670 train_time:121662ms step_avg:88.68ms
step:1373/1670 train_time:121751ms step_avg:88.68ms
step:1374/1670 train_time:121841ms step_avg:88.68ms
step:1375/1670 train_time:121930ms step_avg:88.68ms
step:1375/1670 val_loss:3.3423 train_time:122022ms step_avg:88.74ms
step:1376/1670 train_time:122041ms step_avg:88.69ms
step:1377/1670 train_time:122116ms step_avg:88.68ms
step:1378/1670 train_time:122207ms step_avg:88.68ms
step:1379/1670 train_time:122297ms step_avg:88.69ms
step:1380/1670 train_time:122385ms step_avg:88.69ms
step:1381/1670 train_time:122475ms step_avg:88.69ms
step:1382/1670 train_time:122563ms step_avg:88.69ms
step:1383/1670 train_time:122652ms step_avg:88.69ms
step:1384/1670 train_time:122741ms step_avg:88.69ms
step:1385/1670 train_time:122830ms step_avg:88.69ms
step:1386/1670 train_time:122920ms step_avg:88.69ms
step:1387/1670 train_time:123011ms step_avg:88.69ms
step:1388/1670 train_time:123104ms step_avg:88.69ms
step:1389/1670 train_time:123195ms step_avg:88.69ms
step:1390/1670 train_time:123285ms step_avg:88.69ms
step:1391/1670 train_time:123374ms step_avg:88.69ms
step:1392/1670 train_time:123463ms step_avg:88.69ms
step:1393/1670 train_time:123553ms step_avg:88.70ms
step:1394/1670 train_time:123641ms step_avg:88.70ms
step:1395/1670 train_time:123731ms step_avg:88.70ms
step:1396/1670 train_time:123821ms step_avg:88.70ms
step:1397/1670 train_time:123911ms step_avg:88.70ms
step:1398/1670 train_time:124002ms step_avg:88.70ms
step:1399/1670 train_time:124094ms step_avg:88.70ms
step:1400/1670 train_time:124184ms step_avg:88.70ms
step:1401/1670 train_time:124275ms step_avg:88.70ms
step:1402/1670 train_time:124364ms step_avg:88.70ms
step:1403/1670 train_time:124455ms step_avg:88.71ms
step:1404/1670 train_time:124543ms step_avg:88.71ms
step:1405/1670 train_time:124633ms step_avg:88.71ms
step:1406/1670 train_time:124722ms step_avg:88.71ms
step:1407/1670 train_time:124811ms step_avg:88.71ms
step:1408/1670 train_time:124900ms step_avg:88.71ms
step:1409/1670 train_time:124991ms step_avg:88.71ms
step:1410/1670 train_time:125082ms step_avg:88.71ms
step:1411/1670 train_time:125174ms step_avg:88.71ms
step:1412/1670 train_time:125264ms step_avg:88.71ms
step:1413/1670 train_time:125356ms step_avg:88.72ms
step:1414/1670 train_time:125446ms step_avg:88.72ms
step:1415/1670 train_time:125537ms step_avg:88.72ms
step:1416/1670 train_time:125625ms step_avg:88.72ms
step:1417/1670 train_time:125714ms step_avg:88.72ms
step:1418/1670 train_time:125803ms step_avg:88.72ms
step:1419/1670 train_time:125893ms step_avg:88.72ms
step:1420/1670 train_time:125983ms step_avg:88.72ms
step:1421/1670 train_time:126073ms step_avg:88.72ms
step:1422/1670 train_time:126163ms step_avg:88.72ms
step:1423/1670 train_time:126253ms step_avg:88.72ms
step:1424/1670 train_time:126343ms step_avg:88.72ms
step:1425/1670 train_time:126434ms step_avg:88.73ms
step:1426/1670 train_time:126524ms step_avg:88.73ms
step:1427/1670 train_time:126613ms step_avg:88.73ms
step:1428/1670 train_time:126702ms step_avg:88.73ms
step:1429/1670 train_time:126792ms step_avg:88.73ms
step:1430/1670 train_time:126882ms step_avg:88.73ms
step:1431/1670 train_time:126971ms step_avg:88.73ms
step:1432/1670 train_time:127062ms step_avg:88.73ms
step:1433/1670 train_time:127152ms step_avg:88.73ms
step:1434/1670 train_time:127241ms step_avg:88.73ms
step:1435/1670 train_time:127332ms step_avg:88.73ms
step:1436/1670 train_time:127421ms step_avg:88.73ms
step:1437/1670 train_time:127511ms step_avg:88.73ms
step:1438/1670 train_time:127601ms step_avg:88.73ms
step:1439/1670 train_time:127690ms step_avg:88.74ms
step:1440/1670 train_time:127780ms step_avg:88.74ms
step:1441/1670 train_time:127870ms step_avg:88.74ms
step:1442/1670 train_time:127960ms step_avg:88.74ms
step:1443/1670 train_time:128050ms step_avg:88.74ms
step:1444/1670 train_time:128140ms step_avg:88.74ms
step:1445/1670 train_time:128230ms step_avg:88.74ms
step:1446/1670 train_time:128320ms step_avg:88.74ms
step:1447/1670 train_time:128410ms step_avg:88.74ms
step:1448/1670 train_time:128499ms step_avg:88.74ms
step:1449/1670 train_time:128589ms step_avg:88.74ms
step:1450/1670 train_time:128678ms step_avg:88.74ms
step:1451/1670 train_time:128768ms step_avg:88.74ms
step:1452/1670 train_time:128858ms step_avg:88.74ms
step:1453/1670 train_time:128946ms step_avg:88.74ms
step:1454/1670 train_time:129037ms step_avg:88.75ms
step:1455/1670 train_time:129127ms step_avg:88.75ms
step:1456/1670 train_time:129218ms step_avg:88.75ms
step:1457/1670 train_time:129308ms step_avg:88.75ms
step:1458/1670 train_time:129398ms step_avg:88.75ms
step:1459/1670 train_time:129487ms step_avg:88.75ms
step:1460/1670 train_time:129578ms step_avg:88.75ms
step:1461/1670 train_time:129667ms step_avg:88.75ms
step:1462/1670 train_time:129757ms step_avg:88.75ms
step:1463/1670 train_time:129846ms step_avg:88.75ms
step:1464/1670 train_time:129936ms step_avg:88.75ms
step:1465/1670 train_time:130026ms step_avg:88.75ms
step:1466/1670 train_time:130115ms step_avg:88.76ms
step:1467/1670 train_time:130204ms step_avg:88.76ms
step:1468/1670 train_time:130295ms step_avg:88.76ms
step:1469/1670 train_time:130384ms step_avg:88.76ms
step:1470/1670 train_time:130474ms step_avg:88.76ms
step:1471/1670 train_time:130564ms step_avg:88.76ms
step:1472/1670 train_time:130653ms step_avg:88.76ms
step:1473/1670 train_time:130743ms step_avg:88.76ms
step:1474/1670 train_time:130833ms step_avg:88.76ms
step:1475/1670 train_time:130923ms step_avg:88.76ms
step:1476/1670 train_time:131013ms step_avg:88.76ms
step:1477/1670 train_time:131103ms step_avg:88.76ms
step:1478/1670 train_time:131193ms step_avg:88.76ms
step:1479/1670 train_time:131283ms step_avg:88.76ms
step:1480/1670 train_time:131373ms step_avg:88.77ms
step:1481/1670 train_time:131462ms step_avg:88.77ms
step:1482/1670 train_time:131553ms step_avg:88.77ms
step:1483/1670 train_time:131642ms step_avg:88.77ms
step:1484/1670 train_time:131732ms step_avg:88.77ms
step:1485/1670 train_time:131822ms step_avg:88.77ms
step:1486/1670 train_time:131912ms step_avg:88.77ms
step:1487/1670 train_time:132002ms step_avg:88.77ms
step:1488/1670 train_time:132093ms step_avg:88.77ms
step:1489/1670 train_time:132183ms step_avg:88.77ms
step:1490/1670 train_time:132272ms step_avg:88.77ms
step:1491/1670 train_time:132362ms step_avg:88.77ms
step:1492/1670 train_time:132452ms step_avg:88.77ms
step:1493/1670 train_time:132542ms step_avg:88.78ms
step:1494/1670 train_time:132632ms step_avg:88.78ms
step:1495/1670 train_time:132723ms step_avg:88.78ms
step:1496/1670 train_time:132813ms step_avg:88.78ms
step:1497/1670 train_time:132902ms step_avg:88.78ms
step:1498/1670 train_time:132992ms step_avg:88.78ms
step:1499/1670 train_time:133082ms step_avg:88.78ms
step:1500/1670 train_time:133170ms step_avg:88.78ms
step:1500/1670 val_loss:3.3122 train_time:133262ms step_avg:88.84ms
step:1501/1670 train_time:133281ms step_avg:88.79ms
step:1502/1670 train_time:133355ms step_avg:88.79ms
step:1503/1670 train_time:133449ms step_avg:88.79ms
step:1504/1670 train_time:133539ms step_avg:88.79ms
step:1505/1670 train_time:133628ms step_avg:88.79ms
step:1506/1670 train_time:133717ms step_avg:88.79ms
step:1507/1670 train_time:133805ms step_avg:88.79ms
step:1508/1670 train_time:133893ms step_avg:88.79ms
step:1509/1670 train_time:133982ms step_avg:88.79ms
step:1510/1670 train_time:134070ms step_avg:88.79ms
step:1511/1670 train_time:134161ms step_avg:88.79ms
step:1512/1670 train_time:134253ms step_avg:88.79ms
step:1513/1670 train_time:134346ms step_avg:88.79ms
step:1514/1670 train_time:134437ms step_avg:88.80ms
step:1515/1670 train_time:134529ms step_avg:88.80ms
step:1516/1670 train_time:134618ms step_avg:88.80ms
step:1517/1670 train_time:134708ms step_avg:88.80ms
step:1518/1670 train_time:134796ms step_avg:88.80ms
step:1519/1670 train_time:134885ms step_avg:88.80ms
step:1520/1670 train_time:134973ms step_avg:88.80ms
step:1521/1670 train_time:135062ms step_avg:88.80ms
step:1522/1670 train_time:135153ms step_avg:88.80ms
step:1523/1670 train_time:135242ms step_avg:88.80ms
step:1524/1670 train_time:135333ms step_avg:88.80ms
step:1525/1670 train_time:135425ms step_avg:88.80ms
step:1526/1670 train_time:135514ms step_avg:88.80ms
step:1527/1670 train_time:135604ms step_avg:88.80ms
step:1528/1670 train_time:135693ms step_avg:88.80ms
step:1529/1670 train_time:135782ms step_avg:88.80ms
step:1530/1670 train_time:135871ms step_avg:88.80ms
step:1531/1670 train_time:135961ms step_avg:88.81ms
step:1532/1670 train_time:136051ms step_avg:88.81ms
step:1533/1670 train_time:136141ms step_avg:88.81ms
step:1534/1670 train_time:136230ms step_avg:88.81ms
step:1535/1670 train_time:136321ms step_avg:88.81ms
step:1536/1670 train_time:136412ms step_avg:88.81ms
step:1537/1670 train_time:136503ms step_avg:88.81ms
step:1538/1670 train_time:136592ms step_avg:88.81ms
step:1539/1670 train_time:136682ms step_avg:88.81ms
step:1540/1670 train_time:136772ms step_avg:88.81ms
step:1541/1670 train_time:136861ms step_avg:88.81ms
step:1542/1670 train_time:136951ms step_avg:88.81ms
step:1543/1670 train_time:137041ms step_avg:88.81ms
step:1544/1670 train_time:137130ms step_avg:88.81ms
step:1545/1670 train_time:137220ms step_avg:88.82ms
step:1546/1670 train_time:137310ms step_avg:88.82ms
step:1547/1670 train_time:137401ms step_avg:88.82ms
step:1548/1670 train_time:137491ms step_avg:88.82ms
step:1549/1670 train_time:137581ms step_avg:88.82ms
step:1550/1670 train_time:137671ms step_avg:88.82ms
step:1551/1670 train_time:137761ms step_avg:88.82ms
step:1552/1670 train_time:137851ms step_avg:88.82ms
step:1553/1670 train_time:137940ms step_avg:88.82ms
step:1554/1670 train_time:138030ms step_avg:88.82ms
step:1555/1670 train_time:138120ms step_avg:88.82ms
step:1556/1670 train_time:138210ms step_avg:88.82ms
step:1557/1670 train_time:138301ms step_avg:88.83ms
step:1558/1670 train_time:138391ms step_avg:88.83ms
step:1559/1670 train_time:138482ms step_avg:88.83ms
step:1560/1670 train_time:138571ms step_avg:88.83ms
step:1561/1670 train_time:138661ms step_avg:88.83ms
step:1562/1670 train_time:138751ms step_avg:88.83ms
step:1563/1670 train_time:138841ms step_avg:88.83ms
step:1564/1670 train_time:138930ms step_avg:88.83ms
step:1565/1670 train_time:139019ms step_avg:88.83ms
step:1566/1670 train_time:139109ms step_avg:88.83ms
step:1567/1670 train_time:139198ms step_avg:88.83ms
step:1568/1670 train_time:139289ms step_avg:88.83ms
step:1569/1670 train_time:139380ms step_avg:88.83ms
step:1570/1670 train_time:139470ms step_avg:88.83ms
step:1571/1670 train_time:139561ms step_avg:88.84ms
step:1572/1670 train_time:139651ms step_avg:88.84ms
step:1573/1670 train_time:139741ms step_avg:88.84ms
step:1574/1670 train_time:139830ms step_avg:88.84ms
step:1575/1670 train_time:139921ms step_avg:88.84ms
step:1576/1670 train_time:140010ms step_avg:88.84ms
step:1577/1670 train_time:140099ms step_avg:88.84ms
step:1578/1670 train_time:140190ms step_avg:88.84ms
step:1579/1670 train_time:140279ms step_avg:88.84ms
step:1580/1670 train_time:140369ms step_avg:88.84ms
step:1581/1670 train_time:140459ms step_avg:88.84ms
step:1582/1670 train_time:140550ms step_avg:88.84ms
step:1583/1670 train_time:140639ms step_avg:88.84ms
step:1584/1670 train_time:140729ms step_avg:88.84ms
step:1585/1670 train_time:140819ms step_avg:88.84ms
step:1586/1670 train_time:140910ms step_avg:88.85ms
step:1587/1670 train_time:140999ms step_avg:88.85ms
step:1588/1670 train_time:141089ms step_avg:88.85ms
step:1589/1670 train_time:141179ms step_avg:88.85ms
step:1590/1670 train_time:141269ms step_avg:88.85ms
step:1591/1670 train_time:141359ms step_avg:88.85ms
step:1592/1670 train_time:141449ms step_avg:88.85ms
step:1593/1670 train_time:141540ms step_avg:88.85ms
step:1594/1670 train_time:141630ms step_avg:88.85ms
step:1595/1670 train_time:141719ms step_avg:88.85ms
step:1596/1670 train_time:141809ms step_avg:88.85ms
step:1597/1670 train_time:141898ms step_avg:88.85ms
step:1598/1670 train_time:141988ms step_avg:88.85ms
step:1599/1670 train_time:142078ms step_avg:88.85ms
step:1600/1670 train_time:142169ms step_avg:88.86ms
step:1601/1670 train_time:142260ms step_avg:88.86ms
step:1602/1670 train_time:142350ms step_avg:88.86ms
step:1603/1670 train_time:142440ms step_avg:88.86ms
step:1604/1670 train_time:142530ms step_avg:88.86ms
step:1605/1670 train_time:142621ms step_avg:88.86ms
step:1606/1670 train_time:142710ms step_avg:88.86ms
step:1607/1670 train_time:142800ms step_avg:88.86ms
step:1608/1670 train_time:142890ms step_avg:88.86ms
step:1609/1670 train_time:142980ms step_avg:88.86ms
step:1610/1670 train_time:143071ms step_avg:88.86ms
step:1611/1670 train_time:143162ms step_avg:88.87ms
step:1612/1670 train_time:143252ms step_avg:88.87ms
step:1613/1670 train_time:143342ms step_avg:88.87ms
step:1614/1670 train_time:143431ms step_avg:88.87ms
step:1615/1670 train_time:143521ms step_avg:88.87ms
step:1616/1670 train_time:143611ms step_avg:88.87ms
step:1617/1670 train_time:143701ms step_avg:88.87ms
step:1618/1670 train_time:143792ms step_avg:88.87ms
step:1619/1670 train_time:143882ms step_avg:88.87ms
step:1620/1670 train_time:143971ms step_avg:88.87ms
step:1621/1670 train_time:144062ms step_avg:88.87ms
step:1622/1670 train_time:144152ms step_avg:88.87ms
step:1623/1670 train_time:144242ms step_avg:88.87ms
step:1624/1670 train_time:144332ms step_avg:88.87ms
step:1625/1670 train_time:144421ms step_avg:88.87ms
step:1625/1670 val_loss:3.2890 train_time:144512ms step_avg:88.93ms
step:1626/1670 train_time:144531ms step_avg:88.89ms
step:1627/1670 train_time:144604ms step_avg:88.88ms
step:1628/1670 train_time:144696ms step_avg:88.88ms
step:1629/1670 train_time:144787ms step_avg:88.88ms
step:1630/1670 train_time:144876ms step_avg:88.88ms
step:1631/1670 train_time:144964ms step_avg:88.88ms
step:1632/1670 train_time:145053ms step_avg:88.88ms
step:1633/1670 train_time:145142ms step_avg:88.88ms
step:1634/1670 train_time:145232ms step_avg:88.88ms
step:1635/1670 train_time:145320ms step_avg:88.88ms
step:1636/1670 train_time:145411ms step_avg:88.88ms
step:1637/1670 train_time:145501ms step_avg:88.88ms
step:1638/1670 train_time:145594ms step_avg:88.89ms
step:1639/1670 train_time:145687ms step_avg:88.89ms
step:1640/1670 train_time:145777ms step_avg:88.89ms
step:1641/1670 train_time:145867ms step_avg:88.89ms
step:1642/1670 train_time:145956ms step_avg:88.89ms
step:1643/1670 train_time:146045ms step_avg:88.89ms
step:1644/1670 train_time:146134ms step_avg:88.89ms
step:1645/1670 train_time:146223ms step_avg:88.89ms
step:1646/1670 train_time:146312ms step_avg:88.89ms
step:1647/1670 train_time:146402ms step_avg:88.89ms
step:1648/1670 train_time:146494ms step_avg:88.89ms
step:1649/1670 train_time:146587ms step_avg:88.89ms
step:1650/1670 train_time:146678ms step_avg:88.90ms
step:1651/1670 train_time:146768ms step_avg:88.90ms
step:1652/1670 train_time:146857ms step_avg:88.90ms
step:1653/1670 train_time:146947ms step_avg:88.90ms
step:1654/1670 train_time:147036ms step_avg:88.90ms
step:1655/1670 train_time:147125ms step_avg:88.90ms
step:1656/1670 train_time:147214ms step_avg:88.90ms
step:1657/1670 train_time:147304ms step_avg:88.90ms
step:1658/1670 train_time:147394ms step_avg:88.90ms
step:1659/1670 train_time:147485ms step_avg:88.90ms
step:1660/1670 train_time:147576ms step_avg:88.90ms
step:1661/1670 train_time:147666ms step_avg:88.90ms
step:1662/1670 train_time:147756ms step_avg:88.90ms
step:1663/1670 train_time:147846ms step_avg:88.90ms
step:1664/1670 train_time:147934ms step_avg:88.90ms
step:1665/1670 train_time:148024ms step_avg:88.90ms
step:1666/1670 train_time:148114ms step_avg:88.90ms
step:1667/1670 train_time:148204ms step_avg:88.90ms
step:1668/1670 train_time:148293ms step_avg:88.90ms
step:1669/1670 train_time:148384ms step_avg:88.91ms
step:1670/1670 train_time:148476ms step_avg:88.91ms
step:1670/1670 val_loss:3.2796 train_time:148569ms step_avg:88.96ms
peak memory allocated: 30760 MiB reserved: 45994 MiB
